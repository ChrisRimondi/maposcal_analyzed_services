[
  {
    "content": "module github.com/nats-io/nats-server/v2\n\ngo 1.23.0\n\ntoolchain go1.23.10\n\nrequire (\n\tgithub.com/antithesishq/antithesis-sdk-go v0.4.3-default-no-op\n\tgithub.com/google/go-tpm v0.9.5\n\tgithub.com/klauspost/compress v1.18.0\n\tgithub.com/minio/highwayhash v1.0.3\n\tgithub.com/nats-io/jwt/v2 v2.7.4\n\tgithub.com/nats-io/nats.go v1.43.0\n\tgithub.com/nats-io/nkeys v0.4.11\n\tgithub.com/nats-io/nuid v1.0.1\n\tgo.uber.org/automaxprocs v1.6.0\n\tgolang.org/x/crypto v0.39.0\n\tgolang.org/x/sys v0.33.0\n\tgolang.org/x/time v0.12.0\n)\n",
    "source_file": "go.mod",
    "chunk_type": "unknown"
  },
  {
    "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n",
    "source_file": "LICENSE",
    "chunk_type": "unknown"
  },
  {
    "content": "## Community Code of Conduct\n\nNATS follows the [CNCF Code of Conduct](https://github.com/cncf/foundation/blob/master/code-of-conduct.md).",
    "source_file": "CODE-OF-CONDUCT.md",
    "chunk_type": "doc"
  },
  {
    "content": "# Maintainers\n\nMaintainership is on a per project basis. Reference [NATS Governance Model](https://github.com/nats-io/nats-general/blob/main/GOVERNANCE.md).\n",
    "source_file": "MAINTAINERS.md",
    "chunk_type": "doc"
  },
  {
    "content": "### Maintainers\n  - Derek Collison <derek@nats.io> [@derekcollison](https://github.com/derekcollison)\n  - Ivan Kozlovic <ivan@nats.io> [@kozlovic](https://github.com/kozlovic)\n  - Waldemar Quevedo <wally@nats.io> [@wallyqs](https://github.com/wallyqs)\n  - Oleg Shaldybin <olegsh@google.com> [@olegshaldybin](https://github.com/olegshaldybin)\n  - R.I. Pienaar <rip@devco.net> [@ripienaar](https://github.com/ripienaar)",
    "source_file": "MAINTAINERS.md",
    "chunk_type": "doc"
  },
  {
    "content": "# NATS Server Governance\n\nNATS Server is part of the NATS project and is subject to the [NATS Governance](https://github.com/nats-io/nats-general/blob/main/GOVERNANCE.md).",
    "source_file": "GOVERNANCE.md",
    "chunk_type": "doc"
  },
  {
    "content": "# External Dependencies\n\nThis file lists the dependencies used in this repository.\n\n| Dependency | License |\n|-|-|\n| Go | BSD 3-Clause \"New\" or \"Revised\" License |\n| github.com/nats-io/nats-server/v2 | Apache License 2.0 |\n| github.com/google/go-tpm | Apache License 2.0 |\n| github.com/klauspost/compress | BSD 3-Clause \"New\" or \"Revised\" License |\n| github.com/minio/highwayhash | Apache License 2.0 |\n| github.com/nats-io/jwt/v2 | Apache License 2.0 |\n| github.com/nats-io/nats.go | Apache License 2.0 |\n| github.com/nats-io/nkeys | Apache License 2.0 |\n| github.com/nats-io/nuid  | Apache License 2.0 |\n| go.uber.org/automaxprocs | MIT License |\n| golang.org/x/crypto | BSD 3-Clause \"New\" or \"Revised\" License |\n| golang.org/x/sys | BSD 3-Clause \"New\" or \"Revised\" License |\n| golang.org/x/time | BSD 3-Clause \"New\" or \"Revised\" License |",
    "source_file": "DEPENDENCIES.md",
    "chunk_type": "doc"
  },
  {
    "content": "github.com/antithesishq/antithesis-sdk-go v0.4.3-default-no-op h1:+OSa/t11TFhqfrX0EOSqQBDJ0YlpmK0rDSiB19dg9M0=\ngithub.com/antithesishq/antithesis-sdk-go v0.4.3-default-no-op/go.mod h1:IUpT2DPAKh6i/YhSbt6Gl3v2yvUZjmKncl7U91fup7E=\ngithub.com/davecgh/go-spew v1.1.1 h1:vj9j/u1bqnvCEfJOwUhtlOARqs3+rkHYY13jYWTU97c=\ngithub.com/davecgh/go-spew v1.1.1/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=\ngithub.com/google/go-tpm v0.9.5 h1:ocUmnDebX54dnW+MQWGQRbdaAcJELsa6PqZhJ48KwVU=\ngithub.com/google/go-tpm v0.9.5/go.mod h1:h9jEsEECg7gtLis0upRBQU+GhYVH6jMjrFxI8u6bVUY=\ngithub.com/klauspost/compress v1.18.0 h1:c/Cqfb0r+Yi+JtIEq73FWXVkRonBlf0CRNYc8Zttxdo=\ngithub.com/klauspost/compress v1.18.0/go.mod h1:2Pp+KzxcywXVXMr50+X0Q/Lsb43OQHYWRCY2AiWywWQ=\ngithub.com/minio/highwayhash v1.0.3 h1:kbnuUMoHYyVl7szWjSxJnxw11k2U709jqFPPmIUyD6Q=\ngithub.com/minio/highwayhash v1.0.3/go.mod h1:GGYsuwP/fPD6Y9hMiXuapVvlIUEhFhMTh0rxU3ik1LQ=\ngithub.com/nats-io/jwt/v2 v2.7.4 h1:jXFuDDxs/GQjGDZGhNgH4tXzSUK6WQi2rsj4xmsNOtI=\ngithub.com/nats-io/jwt/v2 v2.7.4/go.mod h1:me11pOkwObtcBNR8AiMrUbtVOUGkqYjMQZ6jnSdVUIA=\ngithub.com/nats-io/nats.go v1.43.0 h1:uRFZ2FEoRvP64+UUhaTokyS18XBCR/xM2vQZKO4i8ug=\ngithub.com/nats-io/nats.go v1.43.0/go.mod h1:iRWIPokVIFbVijxuMQq4y9ttaBTMe0SFdlZfMDd+33g=\ngithub.com/nats-io/nkeys v0.4.11 h1:q44qGV008kYd9W1b1nEBkNzvnWxtRSQ7A8BoqRrcfa0=\ngithub.com/nats-io/nkeys v0.4.11/go.mod h1:szDimtgmfOi9n25JpfIdGw12tZFYXqhGxjhVxsatHVE=\ngithub.com/nats-io/nuid v1.0.1 h1:5iA8DT8V7q8WK2EScv2padNa/rTESc1KdnPw4TC2paw=\ngithub.com/nats-io/nuid v1.0.1/go.mod h1:19wcPz3Ph3q0Jbyiqsd0kePYG7A95tJPxeL+1OSON2c=\ngithub.com/pmezard/go-difflib v1.0.0 h1:4DBwDE0NGyQoBHbLQYPwSUPoCMWR5BEzIk/f1lZbAQM=\ngithub.com/pmezard/go-difflib v1.0.0/go.mod h1:iKH77koFhYxTK1pcRnkKkqfTogsbg7gZNVY4sRDYZ/4=\ngithub.com/prashantv/gostub v1.1.0 h1:BTyx3RfQjRHnUWaGF9oQos79AlQ5k8WNktv7VGvVH4g=\ngithub.com/prashantv/gostub v1.1.0/go.mod h1:A5zLQHz7ieHGG7is6LLXLz7I8+3LZzsrV0P1IAHhP5U=\ngithub.com/stretchr/testify v1.7.1 h1:5TQK59W5E3v0r2duFAb7P95B6hEeOyEnHRa8MjYSMTY=\ngithub.com/stretchr/testify v1.7.1/go.mod h1:6Fq8oRcR53rry900zMqJjRRixrwX3KX962/h/Wwjteg=\ngo.uber.org/automaxprocs v1.6.0 h1:O3y2/QNTOdbF+e/dpXNNW7Rx2hZ4sTIPyybbxyNqTUs=\ngo.uber.org/automaxprocs v1.6.0/go.mod h1:ifeIMSnPZuznNm6jmdzmU3/bfk01Fe2fotchwEFJ8r8=\ngolang.org/x/crypto v0.39.0 h1:SHs+kF4LP+f+p14esP5jAoDpHU8Gu/v9lFRK6IT5imM=\ngolang.org/x/crypto v0.39.0/go.mod h1:L+Xg3Wf6HoL4Bn4238Z6ft6KfEpN0tJGo53AAPC632U=\ngolang.org/x/sys v0.21.0/go.mod h1:/VUhepiaJMQUp4+oa/7Zr1D23ma6VTLIYjOOTFZPUcA=\ngolang.org/x/sys v0.33.0 h1:q3i8TbbEz+JRD9ywIRlyRAQbM0qF7hu24q3teo2hbuw=\ngolang.org/x/sys v0.33.0/go.mod h1:BJP2sWEmIv4KK5OTEluFJCKSidICx8ciO85XgH3Ak8k=\ngolang.org/x/time v0.12.0 h1:ScB/8o8olJvc+CQPWrK3fPZNfh7qgwCrY0zJmoEQLSE=\ngolang.org/x/time v0.12.0/go.mod h1:CDIdPxbZBQxdj6cxyCIdrNogrJKMJ7pr37NYpMcMDSg=\ngopkg.in/yaml.v3 v3.0.1 h1:fxVm/GzAzEWqLHuvctI91KS9hhNmmWOoWu0XTYJS7CA=\ngopkg.in/yaml.v3 v3.0.1/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=\n",
    "source_file": "go.sum",
    "chunk_type": "unknown"
  },
  {
    "content": "",
    "source_file": "TODO.md",
    "chunk_type": "doc"
  },
  {
    "content": "# General\n\n- [ ] Auth for queue groups?\n- [ ] Blacklist or ERR escalation to close connection for auth/permissions\n- [ ] Protocol updates, MAP, MPUB, etc\n- [ ] Multiple listen endpoints\n- [ ] Websocket / HTTP2 strategy\n- [ ] T series reservations\n- [ ] _SYS. server events?\n- [ ] No downtime restart\n- [ ] Signal based reload of configuration\n- [ ] brew, apt-get, rpm, chocately (windows)\n- [ ] IOVec pools and writev for high fanout?\n- [ ] Modify cluster support for single message across routes between pub/sub and d-queue\n- [ ] Memory limits/warnings?\n- [ ] Limit number of subscriptions a client can have, total memory usage etc.\n- [ ] Multi-tenant accounts with isolation of subject space\n- [ ] Pedantic state\n- [X] _SYS.> reserved for server events?\n- [X] Listen configure key vs addr and port\n- [X] Add ENV and variable support to dconf? ucl?\n- [X] Buffer pools/sync pools?\n- [X] Multiple Authorization / Access\n- [X] Write dynamic socket buffer sizes\n- [X] Read dynamic socket buffer sizes\n- [X] Info updates contain other implicit route servers\n- [X] Sublist better at high concurrency, cache uses writelock always currently\n- [X] Switch to 1.4/1.5 and use maps vs hashmaps in sublist\n- [X] NewSource on Rand to lower lock contention on QueueSubs, or redesign!\n- [X] Default sort by cid on connz\n- [X] Track last activity time per connection?\n- [X] Add total connections to varz so we won't miss spikes, etc.\n- [X] Add starttime and uptime to connz list.\n- [X] Gossip Protocol for discovery for clustering\n- [X] Add in HTTP requests to varz?\n- [X] Add favico and help link for monitoring?\n- [X] Better user/pass support using bcrypt etc.\n- [X] SSL/TLS support\n- [X] Add support for / to point to varz, connz, etc..\n- [X] Support sort options for /connz via nats-top\n- [X] Dropped message statistics (slow consumers)\n- [X] Add current time to each monitoring endpoint\n- [X] varz uptime do days and only integer secs\n- [X] Place version in varz (same info sent to clients)\n- [X] Place server ID/UUID in varz\n- [X] nats-top equivalent, utils\n- [X] Connz report routes (/routez)\n- [X] Docker\n- [X] Remove reliance on `ps`\n- [X] Syslog support\n- [X] Client support for language and version\n- [X] Fix benchmarks on linux\n- [X] Daemon mode? Won't fix",
    "source_file": "TODO.md",
    "chunk_type": "doc"
  },
  {
    "content": "# Ambassadors\n\nThe NATS ambassador program recognizes community members that go above and beyond in their contributions to the community and the ecosystem. Learn more [here](https://nats.io/community#nats-ambassador-program).\n\n- [Maurice van Veen](https://nats.io/community#maurice-van-veen) <contact@mauricevanveen.com>",
    "source_file": "AMBASSADORS.md",
    "chunk_type": "doc"
  },
  {
    "content": "<p align=\"center\">\n  <img src=\"logos/nats-horizontal-color.png\" width=\"300\" alt=\"NATS Logo\">\n</p>\n\n[NATS](https://nats.io) is a simple, secure and performant communications system for digital systems, services and devices. NATS is part of the Cloud Native Computing Foundation ([CNCF](https://cncf.io)). NATS has over [40 client language implementations](https://nats.io/download/), and its server can run on-premise, in the cloud, at the edge, and even on a Raspberry Pi. NATS can secure and simplify design and operation of modern distributed systems.\n\n[![License][License-Image]][License-Url] [![Build][Build-Status-Image]][Build-Status-Url] [![Release][Release-Image]][Release-Url] [![Slack][Slack-Image]][Slack-Url] [![Coverage][Coverage-Image]][Coverage-Url] [![Docker Downloads][Docker-Image]][Docker-Url] [![GitHub Downloads][GitHub-Image]][Somsubhra-URL] [![CII Best Practices][CIIBestPractices-Image]][CIIBestPractices-Url] [![Artifact Hub][ArtifactHub-Image]][ArtifactHub-Url]\n",
    "source_file": "README.md",
    "chunk_type": "doc"
  },
  {
    "content": "## Documentation\n\n- [Official Website](https://nats.io)\n- [Official Documentation](https://docs.nats.io)\n- [FAQ](https://docs.nats.io/reference/faq)\n- Watch [a video overview](https://rethink.synadia.com/episodes/1/) of NATS.\n- Watch [this video from SCALE 13x](https://www.youtube.com/watch?v=sm63oAVPqAM) to learn more about its origin story and design philosophy.\n",
    "source_file": "README.md",
    "chunk_type": "doc"
  },
  {
    "content": "## Contact\n\n- [Twitter](https://twitter.com/nats_io): Follow us on Twitter!\n- [Google Groups](https://groups.google.com/forum/#!forum/natsio): Where you can ask questions\n- [Slack](https://natsio.slack.com): Click [here](https://slack.nats.io) to join. You can ask questions to our maintainers and to the rich and active community.\n",
    "source_file": "README.md",
    "chunk_type": "doc"
  },
  {
    "content": "## Contributing\n\nIf you are interested in contributing to NATS, read about our...\n\n- [Contributing guide](./CONTRIBUTING.md)\n- [Report issues or propose Pull Requests](https://github.com/nats-io)\n\n[License-Url]: https://www.apache.org/licenses/LICENSE-2.0\n[License-Image]: https://img.shields.io/badge/License-Apache2-blue.svg\n[Docker-Image]: https://img.shields.io/docker/pulls/_/nats.svg\n[Docker-Url]: https://hub.docker.com/_/nats\n[Slack-Image]: https://img.shields.io/badge/chat-on%20slack-green\n[Slack-Url]: https://slack.nats.io\n[Fossa-Url]: https://app.fossa.io/projects/git%2Bgithub.com%2Fnats-io%2Fnats-server?ref=badge_shield\n[Fossa-Image]: https://app.fossa.io/api/projects/git%2Bgithub.com%2Fnats-io%2Fnats-server.svg?type=shield\n[Build-Status-Url]: https://github.com/nats-io/nats-server/actions/workflows/tests.yaml\n[Build-Status-Image]: https://github.com/nats-io/nats-server/actions/workflows/tests.yaml/badge.svg?branch=main\n[Release-Url]: https://github.com/nats-io/nats-server/releases/latest\n[Release-Image]: https://img.shields.io/github/v/release/nats-io/nats-server\n[Coverage-Url]: https://coveralls.io/r/nats-io/nats-server?branch=main\n[Coverage-image]: https://coveralls.io/repos/github/nats-io/nats-server/badge.svg?branch=main\n[ReportCard-Url]: https://goreportcard.com/report/nats-io/nats-server\n[ReportCard-Image]: https://goreportcard.com/badge/github.com/nats-io/nats-server\n[CIIBestPractices-Url]: https://bestpractices.coreinfrastructure.org/projects/1895\n[CIIBestPractices-Image]: https://bestpractices.coreinfrastructure.org/projects/1895/badge\n[ArtifactHub-Url]: https://artifacthub.io/packages/helm/nats/nats\n[ArtifactHub-Image]: https://img.shields.io/endpoint?url=https://artifacthub.io/badge/repository/nats\n[GitHub-Release]: https://github.com/nats-io/nats-server/releases/\n[GitHub-Image]: https://img.shields.io/github/downloads/nats-io/nats-server/total.svg?logo=github\n[Somsubhra-url]: https://somsubhra.github.io/github-release-stats/?username=nats-io&repository=nats-server\n",
    "source_file": "README.md",
    "chunk_type": "doc"
  },
  {
    "content": "## Roadmap\n\nThe NATS product roadmap can be found [here](https://nats.io/about/#roadmap).\n",
    "source_file": "README.md",
    "chunk_type": "doc"
  },
  {
    "content": "## Adopters\n\nWho uses NATS? See our [list of users](https://nats.io/#who-uses-nats) on [https://nats.io](https://nats.io).\n",
    "source_file": "README.md",
    "chunk_type": "doc"
  },
  {
    "content": "## Security\n",
    "source_file": "README.md",
    "chunk_type": "doc"
  },
  {
    "content": "### Security Audit\n\nA third party security audit was performed by Trail of Bits following engagement by the Open Source Technology Improvement Fund (OSTIF). You can see the [full report from April 2025 here](https://github.com/trailofbits/publications/blob/master/reviews/2025-04-ostif-nats-securityreview.pdf).\n",
    "source_file": "README.md",
    "chunk_type": "doc"
  },
  {
    "content": "### Reporting Security Vulnerabilities\n\nIf you've found a vulnerability or a potential vulnerability in the NATS server, please let us know at\n[nats-security](mailto:security@nats.io).\n",
    "source_file": "README.md",
    "chunk_type": "doc"
  },
  {
    "content": "## License\n\nUnless otherwise noted, the NATS source files are distributed\nunder the Apache Version 2.0 license found in the LICENSE file.",
    "source_file": "README.md",
    "chunk_type": "doc"
  },
  {
    "content": "Here is the list of some established lock ordering.\n\nIn this list, A -> B means that you can have A.Lock() then B.Lock(), not the opposite.\n\njetStream -> jsAccount -> Server -> client -> Account\n\njetStream -> jsAccount -> stream -> consumer\n\nA lock to protect jetstream account's usage has been introduced: jsAccount.usageMu.\nThis lock is independent and can be invoked under any other lock: jsAccount -> jsa.usageMu, stream -> jsa.usageMu, etc...\n\nA lock to protect the account's leafnodes list was also introduced to\nallow that lock to be held and the acquire a client lock which is not\npossible with the normal account lock.\n\naccountLeafList -> client\n\nAccountResolver interface has various implementations, but assume: AccountResolver -> Server\n\nA reloadMu lock was added to prevent newly connecting clients racing with the configuration reload.\nThis must be taken out as soon as a reload is about to happen before any other locks:\n\n    reloadMu -> Server\n    reloadMu -> optsMu\n\nThe \"jscmMu\" lock in the Account is used to serialise calls to checkJetStreamMigrate and\nclearObserverState so that they cannot interleave which would leave Raft nodes in\ninconsistent observer states.\n\n    jscmMu -> Account -> jsAccount\n    jscmMu -> stream.clsMu\n    jscmMu -> RaftNode\n\nThe \"clsMu\" lock protects the consumer list on a stream, used for signalling consumer activity.\n\n    stream -> clsMu\n",
    "source_file": "locksordering.txt",
    "chunk_type": "doc"
  },
  {
    "content": "# Contributing\n\nThanks for your interest in contributing! This document contains `nats-io/nats-server` specific contributing details. If you are a first-time contributor, please refer to the general [NATS Contributor Guide](https://nats.io/contributing/) to get a comprehensive overview of contributing to the NATS project.\n",
    "source_file": "CONTRIBUTING.md",
    "chunk_type": "doc"
  },
  {
    "content": "## Getting started\n\nThere are three general ways you can contribute to this repo:\n\n- Proposing an enhancement or new feature\n- Reporting a bug or regression\n- Contributing changes to the source code\n\nFor the first two, refer to the [GitHub Issues](https://github.com/nats-io/nats-server/issues/new/choose) which guides you through the available options along with the needed information to collect.\n",
    "source_file": "CONTRIBUTING.md",
    "chunk_type": "doc"
  },
  {
    "content": "## Contributing changes\n\n_Prior to opening a pull request, it is recommended to open an issue first to ensure the maintainers can review intended changes. This saves time for both you and us. Exceptions to this rule include fixing non-functional source such as code comments, documentation or other supporting files._\n\nProposing source code changes is done through GitHub's standard pull request workflow.\n\nIf your branch is a work-in-progress then please start by creating your pull requests as draft, by clicking the down-arrow next to the `Create pull request` button and instead selecting `Create draft pull request`.\n\nThis will defer the automatic process of requesting a review from the NATS team and significantly reduces noise until you are ready. Once you are happy, you can click the `Ready for review` button.\n",
    "source_file": "CONTRIBUTING.md",
    "chunk_type": "doc"
  },
  {
    "content": "### Guidelines\n\nA good pull request includes:\n\n- A succinct yet descriptive title describing, in a few words, what is fixed/improved/optimised by this change.\n- A high-level description of the changes, including an overview of _why_ the changes are relevant or what problem you are trying to solve. Include links to any issues that are related by adding comments like `Resolves #NNN` to your description. See [Linking a Pull Request to an Issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/linking-a-pull-request-to-an-issue) for more information.\n- An up-to-date parent commit. Please make sure you are pulling in the latest `main` branch and rebasing your work on top of it, i.e. `git rebase main`.\n- Unit tests where appropriate. Bug fixes will benefit from the addition of regression tests and performance improvements will benefit from the addition of new benchmarks where possible. New features will **NOT** be accepted without suitable test coverage!\n- No more commits than necessary. Sometimes having multiple commits is useful for telling a story or isolating changes from one another, but please squash down any unnecessary commits that may just be for clean-up, comments or small changes.\n- No additional external dependencies that aren't absolutely essential. Please do everything you can to avoid pulling in additional libraries/dependencies into `go.mod` as we will be very critical of these.\n",
    "source_file": "CONTRIBUTING.md",
    "chunk_type": "doc"
  },
  {
    "content": "### Sign-off\n\nIn order to accept a contribution, you will first need to certify that the contribution is your original work and that you license the work to the project under the [Apache-2.0 license](https://github.com/nats-io/nats-server/blob/main/LICENSE).\n\nThis is done by using `Signed-off-by` statements, which should appear in **both** your commit messages and your PR description. Please note that we can only accept sign-offs under a legal name. Nicknames and aliases are not permitted.\n\nTo perform a sign-off when committing with `git`, use `git commit -s` (or `--signoff`) to add the `Signed-off-by:` trailer to your commit message.\n",
    "source_file": "CONTRIBUTING.md",
    "chunk_type": "doc"
  },
  {
    "content": "## Get help\n\nIf you have questions about the contribution process, please start a [GitHub discussion](https://github.com/nats-io/nats-server/discussions), join the [NATS Slack](https://slack.nats.io/), or send your question to the [NATS Google Group](https://groups.google.com/forum/#!forum/natsio).",
    "source_file": "CONTRIBUTING.md",
    "chunk_type": "doc"
  },
  {
    "content": "// Copyright 2012-2025 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage main\n\n//go:generate go run server/errors_gen.go\n\nimport (\n\t\"flag\"\n\t\"fmt\"\n\t\"os\"\n\n\t\"github.com/nats-io/nats-server/v2/server\"\n\t\"go.uber.org/automaxprocs/maxprocs\"\n)\n\nvar usageStr = `\nUsage: nats-server [options]\n\nServer Options:\n    -a, --addr, --net <host>         Bind to host address (default: 0.0.0.0)\n    -p, --port <port>                Use port for clients (default: 4222)\n    -n, --name\n        --server_name <server_name>  Server name (default: auto)\n    -P, --pid <file>                 File to store PID\n    -m, --http_port <port>           Use port for http monitoring\n    -ms,--https_port <port>          Use port for https monitoring\n    -c, --config <file>              Configuration file\n    -t                               Test configuration and exit\n    -sl,--signal <signal>[=<pid>]    Send signal to nats-server process (ldm, stop, quit, term, reopen, reload)\n                                     <pid> can be either a PID (e.g. 1) or the path to a PID file (e.g. /var/run/nats-server.pid)\n        --client_advertise <string>  Client URL to advertise to other servers\n        --ports_file_dir <dir>       Creates a ports file in the specified directory (<executable_name>_<pid>.ports).\n\nLogging Options:\n    -l, --log <file>                 File to redirect log output\n    -T, --logtime                    Timestamp log entries (default: true)\n    -s, --syslog                     Log to syslog or windows event log\n    -r, --remote_syslog <addr>       Syslog server addr (udp://localhost:514)\n    -D, --debug                      Enable debugging output\n    -V, --trace                      Trace the raw protocol\n    -VV                              Verbose trace (traces system account as well)\n    -DV                              Debug and trace\n    -DVV                             Debug and verbose trace (traces system account as well)\n        --log_size_limit <limit>     Logfile size limit (default: auto)\n        --max_traced_msg_len <len>   Maximum printable length for traced messages (default: unlimited)\n\nJetStream Options:\n    -js, --jetstream                 Enable JetStream functionality\n    -sd, --store_dir <dir>           Set the storage directory\n\nAuthorization Options:\n        --user <user>                User required for connections\n        --pass <password>            Password required for connections\n        --auth <token>               Authorization token required for connections\n\nTLS Options:\n        --tls                        Enable TLS, do not verify clients (default: false)\n        --tlscert <file>             Server certificate file\n        --tlskey <file>              Private key for server certificate\n        --tlsverify                  Enable TLS, verify client certificates\n        --tlscacert <file>           Client certificate CA for verification\n\nCluster Options:\n        --routes <rurl-1, rurl-2>    Routes to solicit and connect\n        --cluster <cluster-url>      Cluster URL for solicited routes\n        --cluster_name <string>      Cluster Name, if not set one will be dynamically generated\n        --no_advertise <bool>        Do not advertise known cluster information to clients\n        --cluster_advertise <string> Cluster URL to advertise to other servers\n        --connect_retries <number>   For implicit routes, number of connect retries\n        --cluster_listen <url>       Cluster url from which members can solicit routes\n\nProfiling Options:\n        --profile <port>             Profiling HTTP port\n\nCommon Options:\n    -h, --help                       Show this message\n    -v, --version                    Show version\n        --help_tls                   TLS help\n`\n\n// usage will print out the flag options for the server.\nfunc usage() {\n\tfmt.Printf(\"%s\\n\", usageStr)\n\tos.Exit(0)\n}\n\nfunc main() {\n\texe := \"nats-server\"\n\n\t// Create a FlagSet and sets the usage\n\tfs := flag.NewFlagSet(exe, flag.ExitOnError)\n\tfs.Usage = usage\n\n\t// Configure the options from the flags/config file\n\topts, err := server.ConfigureOptions(fs, os.Args[1:],\n\t\tserver.PrintServerAndExit,\n\t\tfs.Usage,\n\t\tserver.PrintTLSHelpAndDie)\n\tif err != nil {\n\t\tserver.PrintAndDie(fmt.Sprintf(\"%s: %s\", exe, err))\n\t} else if opts.CheckConfig {\n\t\tfmt.Fprintf(os.Stderr, \"%s: configuration file %s is valid (%s)\\n\", exe, opts.ConfigFile, opts.ConfigDigest())\n\t\tos.Exit(0)\n\t}\n\n\t// Create the server with appropriate options.\n\ts, err := server.NewServer(opts)\n\tif err != nil {\n\t\tserver.PrintAndDie(fmt.Sprintf(\"%s: %s\", exe, err))\n\t}\n\n\t// Configure the logger based on the flags.\n\ts.ConfigureLogger()\n\n\t// Start things up. Block here until done.\n\tif err := server.Run(s); err != nil {\n\t\tserver.PrintAndDie(err.Error())\n\t}\n\n\t// Adjust MAXPROCS if running under linux/cgroups quotas.\n\tundo, err := maxprocs.Set(maxprocs.Logger(s.Debugf))\n\tif err != nil {\n\t\ts.Warnf(\"Failed to set GOMAXPROCS: %v\", err)\n\t} else {\n\t\tdefer undo()\n\t}\n\n\ts.WaitForShutdown()\n}\n",
    "source_file": "main.go",
    "chunk_type": "code"
  },
  {
    "content": "FROM golang:alpine AS builder\n\nARG VERSION=\"nightly\"\n\nRUN apk add --update git\nRUN mkdir -p src/github.com/nats-io && \\\n    cd src/github.com/nats-io/ && \\\n    git clone https://github.com/nats-io/natscli.git && \\\n    cd natscli/nats && \\\n    go build -ldflags \"-w -X main.version=${VERSION}\" -o /nats\n\nRUN go install github.com/nats-io/nsc/v2@latest\n\nFROM alpine:latest\n\nRUN apk add --update ca-certificates && mkdir -p /nats/bin && mkdir /nats/conf\n\nCOPY docker/nats-server.conf /nats/conf/nats-server.conf\nCOPY nats-server /bin/nats-server\nCOPY --from=builder /nats /bin/nats\nCOPY --from=builder /go/bin/nsc /bin/nsc\n\nEXPOSE 4222 8222 6222 5222\n\nENTRYPOINT [\"/bin/nats-server\"]\nCMD [\"-c\", \"/nats/conf/nats-server.conf\"]\n",
    "source_file": "docker/Dockerfile.nightly",
    "chunk_type": "unknown"
  },
  {
    "content": "[Unit]\nDescription=NATS Server\nAfter=network-online.target ntp.service\n\n[Service]\nPrivateTmp=true\nType=simple\nExecStart=/usr/sbin/nats-server -c /etc/nats-server.conf\nExecReload=/bin/kill -s HUP $MAINPID\n\n# The nats-server uses SIGUSR2 to trigger Lame Duck Mode (LDM) shutdown\n# https://docs.nats.io/running-a-nats-service/nats_admin/lame_duck_mode\nExecStop=/bin/kill -s SIGUSR2  $MAINPID\n\n# This should be `lame_duck_duration` + some buffer to finish the shutdown.\n# By default, `lame_duck_duration` is 2 mins.\nTimeoutStopSec=150\n\nRestart=on-failure\n\nUser=nats\nGroup=nats\n\n[Install]\nWantedBy=multi-user.target\n",
    "source_file": "util/nats-server.service",
    "chunk_type": "unknown"
  },
  {
    "content": "[Unit]\nDescription=NATS Server\nAfter=network-online.target ntp.service\n\n# If you use a dedicated filesystem for JetStream data, then you might use something like:\n# ConditionPathIsMountPoint=/srv/jetstream\n# See also Service.ReadWritePaths\n\n[Service]\nType=simple\nEnvironmentFile=-/etc/default/nats-server\nExecStart=/usr/sbin/nats-server -c /etc/nats-server.conf\nExecReload=/bin/kill -s HUP $MAINPID\n\n# The nats-server uses SIGUSR2 to trigger Lame Duck Mode (LDM) shutdown\n# https://docs.nats.io/running-a-nats-service/nats_admin/lame_duck_mode\nExecStop=/bin/kill -s SIGUSR2  $MAINPID\n\nUser=nats\nGroup=nats\n\nRestart=on-failure\nRestartSec=5\n\n# This should be `lame_duck_duration` + some buffer to finish the shutdown.\n# By default, `lame_duck_duration` is 2 mins.\nTimeoutStopSec=150\n\n# Capacity Limits\n# JetStream requires 2 FDs open per stream.\nLimitNOFILE=800000\n# Environment=GOMEMLIMIT=12GiB\n# You might find it better to set GOMEMLIMIT via /etc/default/nats-server,\n# so that you can change limits without needing a systemd daemon-reload.\n\n# Hardening\nCapabilityBoundingSet=\nLockPersonality=true\nMemoryDenyWriteExecute=true\nNoNewPrivileges=true\nPrivateDevices=true\nPrivateTmp=true\nPrivateUsers=true\nProcSubset=pid\nProtectClock=true\nProtectControlGroups=true\nProtectHome=true\nProtectHostname=true\nProtectKernelLogs=true\nProtectKernelModules=true\nProtectKernelTunables=true\nProtectSystem=strict\nReadOnlyPaths=\nRestrictAddressFamilies=AF_INET AF_INET6\nRestrictNamespaces=true\nRestrictRealtime=true\nRestrictSUIDSGID=true\nSystemCallFilter=@system-service ~@privileged ~@resources\nUMask=0077\n\n# Consider locking down all areas of /etc which hold machine identity keys, etc\nInaccessiblePaths=/etc/ssh\n\n# If you have systemd >= 247\nProtectProc=invisible\n\n# If you have systemd >= 248\nPrivateIPC=true\n\n# Optional: writable directory for JetStream.\n# See also: Unit.ConditionPathIsMountPoint\nReadWritePaths=/var/lib/nats\n\n# Optional: resource control.\n# Replace weights by values that make sense for your situation.\n# For a list of all options see:\n# https://www.freedesktop.org/software/systemd/man/systemd.resource-control.html\n#CPUAccounting=true\n#CPUWeight=100 # of 10000\n#IOAccounting=true\n#IOWeight=100 # of 10000\n#MemoryAccounting=true\n#MemoryMax=1GB\n#IPAccounting=true\n\n[Install]\nWantedBy=multi-user.target\n# If you install this service as nats-server.service and want 'nats'\n# to work as an alias, then uncomment this next line:\n#Alias=nats.service\n",
    "source_file": "util/nats-server-hardened.service",
    "chunk_type": "unknown"
  },
  {
    "content": "// Copyright 2017-2025 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage server\n\nimport (\n\t\"cmp\"\n\t\"crypto/tls\"\n\t\"errors\"\n\t\"fmt\"\n\t\"net/url\"\n\t\"reflect\"\n\t\"slices\"\n\t\"strings\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t\"github.com/klauspost/compress/s2\"\n\n\t\"github.com/nats-io/jwt/v2\"\n\t\"github.com/nats-io/nuid\"\n)\n\n// FlagSnapshot captures the server options as specified by CLI flags at\n// startup. This should not be modified once the server has started.\nvar FlagSnapshot *Options\n\ntype reloadContext struct {\n\toldClusterPerms *RoutePermissions\n}\n\n// option is a hot-swappable configuration setting.\ntype option interface {\n\t// Apply the server option.\n\tApply(server *Server)\n\n\t// IsLoggingChange indicates if this option requires reloading the logger.\n\tIsLoggingChange() bool\n\n\t// IsTraceLevelChange indicates if this option requires reloading cached trace level.\n\t// Clients store trace level separately.\n\tIsTraceLevelChange() bool\n\n\t// IsAuthChange indicates if this option requires reloading authorization.\n\tIsAuthChange() bool\n\n\t// IsTLSChange indicates if this option requires reloading TLS.\n\tIsTLSChange() bool\n\n\t// IsClusterPermsChange indicates if this option requires reloading\n\t// cluster permissions.\n\tIsClusterPermsChange() bool\n\n\t// IsClusterPoolSizeOrAccountsChange indicates if this option requires\n\t// special handling for changes in cluster's pool size or accounts list.\n\tIsClusterPoolSizeOrAccountsChange() bool\n\n\t// IsJetStreamChange inidicates a change in the servers config for JetStream.\n\t// Account changes will be handled separately in reloadAuthorization.\n\tIsJetStreamChange() bool\n\n\t// Indicates a change in the server that requires publishing the server's statz\n\tIsStatszChange() bool\n}\n\n// noopOption is a base struct that provides default no-op behaviors.\ntype noopOption struct{}\n\nfunc (n noopOption) IsLoggingChange() bool {\n\treturn false\n}\n\nfunc (n noopOption) IsTraceLevelChange() bool {\n\treturn false\n}\n\nfunc (n noopOption) IsAuthChange() bool {\n\treturn false\n}\n\nfunc (n noopOption) IsTLSChange() bool {\n\treturn false\n}\n\nfunc (n noopOption) IsClusterPermsChange() bool {\n\treturn false\n}\n\nfunc (n noopOption) IsClusterPoolSizeOrAccountsChange() bool {\n\treturn false\n}\n\nfunc (n noopOption) IsJetStreamChange() bool {\n\treturn false\n}\n\nfunc (n noopOption) IsStatszChange() bool {\n\treturn false\n}\n\n// loggingOption is a base struct that provides default option behaviors for\n// logging-related options.\ntype loggingOption struct {\n\tnoopOption\n}\n\nfunc (l loggingOption) IsLoggingChange() bool {\n\treturn true\n}\n\n// traceLevelOption is a base struct that provides default option behaviors for\n// tracelevel-related options.\ntype traceLevelOption struct {\n\tloggingOption\n}\n\nfunc (l traceLevelOption) IsTraceLevelChange() bool {\n\treturn true\n}\n\n// traceOption implements the option interface for the `trace` setting.\ntype traceOption struct {\n\ttraceLevelOption\n\tnewValue bool\n}\n\n// Apply is a no-op because logging will be reloaded after options are applied.\nfunc (t *traceOption) Apply(server *Server) {\n\tserver.Noticef(\"Reloaded: trace = %v\", t.newValue)\n}\n\n// traceVersboseOption implements the option interface for the `trace_verbose` setting.\ntype traceVerboseOption struct {\n\ttraceLevelOption\n\tnewValue bool\n}\n\n// Apply is a no-op because logging will be reloaded after options are applied.\nfunc (t *traceVerboseOption) Apply(server *Server) {\n\tserver.Noticef(\"Reloaded: trace_verbose = %v\", t.newValue)\n}\n\n// traceHeadersOption implements the option interface for the `trace_headers` setting.\ntype traceHeadersOption struct {\n\ttraceLevelOption\n\tnewValue bool\n}\n\n// Apply is a no-op because logging will be reloaded after options are applied.\nfunc (t *traceHeadersOption) Apply(server *Server) {\n\tserver.Noticef(\"Reloaded: trace_headers = %v\", t.newValue)\n}\n\n// debugOption implements the option interface for the `debug` setting.\ntype debugOption struct {\n\tloggingOption\n\tnewValue bool\n}\n\n// Apply is mostly a no-op because logging will be reloaded after options are applied.\n// However we will kick the raft nodes if they exist to reload.\nfunc (d *debugOption) Apply(server *Server) {\n\tserver.Noticef(\"Reloaded: debug = %v\", d.newValue)\n\tserver.reloadDebugRaftNodes(d.newValue)\n}\n\n// logtimeOption implements the option interface for the `logtime` setting.\ntype logtimeOption struct {\n\tloggingOption\n\tnewValue bool\n}\n\n// Apply is a no-op because logging will be reloaded after options are applied.\nfunc (l *logtimeOption) Apply(server *Server) {\n\tserver.Noticef(\"Reloaded: logtime = %v\", l.newValue)\n}\n\n// logtimeUTCOption implements the option interface for the `logtime_utc` setting.\ntype logtimeUTCOption struct {\n\tloggingOption\n\tnewValue bool\n}\n\n// Apply is a no-op because logging will be reloaded after options are applied.\nfunc (l *logtimeUTCOption) Apply(server *Server) {\n\tserver.Noticef(\"Reloaded: logtime_utc = %v\", l.newValue)\n}\n\n// logfileOption implements the option interface for the `log_file` setting.\ntype logfileOption struct {\n\tloggingOption\n\tnewValue string\n}\n\n// Apply is a no-op because logging will be reloaded after options are applied.\nfunc (l *logfileOption) Apply(server *Server) {\n\tserver.Noticef(\"Reloaded: log_file = %v\", l.newValue)\n}\n\n// syslogOption implements the option interface for the `syslog` setting.\ntype syslogOption struct {\n\tloggingOption\n\tnewValue bool\n}\n\n// Apply is a no-op because logging will be reloaded after options are applied.\nfunc (s *syslogOption) Apply(server *Server) {\n\tserver.Noticef(\"Reloaded: syslog = %v\", s.newValue)\n}\n\n// remoteSyslogOption implements the option interface for the `remote_syslog`\n// setting.\ntype remoteSyslogOption struct {\n\tloggingOption\n\tnewValue string\n}\n\n// Apply is a no-op because logging will be reloaded after options are applied.\nfunc (r *remoteSyslogOption) Apply(server *Server) {\n\tserver.Noticef(\"Reloaded: remote_syslog = %v\", r.newValue)\n}\n\n// tlsOption implements the option interface for the `tls` setting.\ntype tlsOption struct {\n\tnoopOption\n\tnewValue *tls.Config\n}\n\n// Apply the tls change.\nfunc (t *tlsOption) Apply(server *Server) {\n\tserver.mu.Lock()\n\ttlsRequired := t.newValue != nil\n\tserver.info.TLSRequired = tlsRequired && !server.getOpts().AllowNonTLS\n\tmessage := \"disabled\"\n\tif tlsRequired {\n\t\tserver.info.TLSVerify = (t.newValue.ClientAuth == tls.RequireAndVerifyClientCert)\n\t\tmessage = \"enabled\"\n\t}\n\tserver.mu.Unlock()\n\tserver.Noticef(\"Reloaded: tls = %s\", message)\n}\n\nfunc (t *tlsOption) IsTLSChange() bool {\n\treturn true\n}\n\n// tlsTimeoutOption implements the option interface for the tls `timeout`\n// setting.\ntype tlsTimeoutOption struct {\n\tnoopOption\n\tnewValue float64\n}\n\n// Apply is a no-op because the timeout will be reloaded after options are\n// applied.\nfunc (t *tlsTimeoutOption) Apply(server *Server) {\n\tserver.Noticef(\"Reloaded: tls timeout = %v\", t.newValue)\n}\n\n// tlsPinnedCertOption implements the option interface for the tls `pinned_certs` setting.\ntype tlsPinnedCertOption struct {\n\tnoopOption\n\tnewValue PinnedCertSet\n}\n\n// Apply is a no-op because the pinned certs will be reloaded after options are  applied.\nfunc (t *tlsPinnedCertOption) Apply(server *Server) {\n\tserver.Noticef(\"Reloaded: %d pinned_certs\", len(t.newValue))\n}\n\n// tlsHandshakeFirst implements the option interface for the tls `handshake first` setting.\ntype tlsHandshakeFirst struct {\n\tnoopOption\n\tnewValue bool\n}\n\n// Apply is a no-op because the timeout will be reloaded after options are applied.\nfunc (t *tlsHandshakeFirst) Apply(server *Server) {\n\tserver.Noticef(\"Reloaded: Client TLS handshake first: %v\", t.newValue)\n}\n\n// tlsHandshakeFirstFallback implements the option interface for the tls `handshake first fallback delay` setting.\ntype tlsHandshakeFirstFallback struct {\n\tnoopOption\n\tnewValue time.Duration\n}\n\n// Apply is a no-op because the timeout will be reloaded after options are applied.\nfunc (t *tlsHandshakeFirstFallback) Apply(server *Server) {\n\tserver.Noticef(\"Reloaded: Client TLS handshake first fallback delay: %v\", t.newValue)\n}\n\n// authOption is a base struct that provides default option behaviors.\ntype authOption struct {\n\tnoopOption\n}\n\nfunc (o authOption) IsAuthChange() bool {\n\treturn true\n}\n\n// usernameOption implements the option interface for the `username` setting.\ntype usernameOption struct {\n\tauthOption\n}\n\n// Apply is a no-op because authorization will be reloaded after options are\n// applied.\nfunc (u *usernameOption) Apply(server *Server) {\n\tserver.Noticef(\"Reloaded: authorization username\")\n}\n\n// passwordOption implements the option interface for the `password` setting.\ntype passwordOption struct {\n\tauthOption\n}\n\n// Apply is a no-op because authorization will be reloaded after options are\n// applied.\nfunc (p *passwordOption) Apply(server *Server) {\n\tserver.Noticef(\"Reloaded: authorization password\")\n}\n\n// authorizationOption implements the option interface for the `token`\n// authorization setting.\ntype authorizationOption struct {\n\tauthOption\n}\n\n// Apply is a no-op because authorization will be reloaded after options are\n// applied.\nfunc (a *authorizationOption) Apply(server *Server) {\n\tserver.Noticef(\"Reloaded: authorization token\")\n}\n\n// authTimeoutOption implements the option interface for the authorization\n// `timeout` setting.\ntype authTimeoutOption struct {\n\tnoopOption // Not authOption because this is a no-op; will be reloaded with options.\n\tnewValue   float64\n}\n\n// Apply is a no-op because the timeout will be reloaded after options are\n// applied.\nfunc (a *authTimeoutOption) Apply(server *Server) {\n\tserver.Noticef(\"Reloaded: authorization timeout = %v\", a.newValue)\n}\n\n// tagsOption implements the option interface for the `tags` setting.\ntype tagsOption struct {\n\tnoopOption // Not authOption because this is a no-op; will be reloaded with options.\n}\n\nfunc (u *tagsOption) Apply(server *Server) {\n\tserver.Noticef(\"Reloaded: tags\")\n}\n\nfunc (u *tagsOption) IsStatszChange() bool {\n\treturn true\n}\n\n// usersOption implements the option interface for the authorization `users`\n// setting.\ntype usersOption struct {\n\tauthOption\n}\n\nfunc (u *usersOption) Apply(server *Server) {\n\tserver.Noticef(\"Reloaded: authorization users\")\n}\n\n// nkeysOption implements the option interface for the authorization `users`\n// setting.\ntype nkeysOption struct {\n\tauthOption\n}\n\nfunc (u *nkeysOption) Apply(server *Server) {\n\tserver.Noticef(\"Reloaded: authorization nkey users\")\n}\n\n// clusterOption implements the option interface for the `cluster` setting.\ntype clusterOption struct {\n\tauthOption\n\tnewValue        ClusterOpts\n\tpermsChanged    bool\n\taccsAdded       []string\n\taccsRemoved     []string\n\tpoolSizeChanged bool\n\tcompressChanged bool\n}\n\n// Apply the cluster change.\nfunc (c *clusterOption) Apply(s *Server) {\n\t// TODO: support enabling/disabling clustering.\n\ts.mu.Lock()\n\ttlsRequired := c.newValue.TLSConfig != nil\n\ts.routeInfo.TLSRequired = tlsRequired\n\ts.routeInfo.TLSVerify = tlsRequired\n\ts.routeInfo.AuthRequired = c.newValue.Username != \"\"\n\tif c.newValue.NoAdvertise {\n\t\ts.routeInfo.ClientConnectURLs = nil\n\t\ts.routeInfo.WSConnectURLs = nil\n\t} else {\n\t\ts.routeInfo.ClientConnectURLs = s.clientConnectURLs\n\t\ts.routeInfo.WSConnectURLs = s.websocket.connectURLs\n\t}\n\ts.setRouteInfoHostPortAndIP()\n\tvar routes []*client\n\tif c.compressChanged {\n\t\tco := &s.getOpts().Cluster.Compression\n\t\tnewMode := co.Mode\n\t\ts.forEachRoute(func(r *client) {\n\t\t\tr.mu.Lock()\n\t\t\t// Skip routes that are \"not supported\" (because they will never do\n\t\t\t// compression) or the routes that have already the new compression\n\t\t\t// mode.\n\t\t\tif r.route.compression == CompressionNotSupported || r.route.compression == newMode {\n\t\t\t\tr.mu.Unlock()\n\t\t\t\treturn\n\t\t\t}\n\t\t\t// We need to close the route if it had compression \"off\" or the new\n\t\t\t// mode is compression \"off\", or if the new mode is \"accept\", because\n\t\t\t// these require negotiation.\n\t\t\tif r.route.compression == CompressionOff || newMode == CompressionOff || newMode == CompressionAccept {\n\t\t\t\troutes = append(routes, r)\n\t\t\t} else if newMode == CompressionS2Auto {\n\t\t\t\t// If the mode is \"s2_auto\", we need to check if there is really\n\t\t\t\t// need to change, and at any rate, we want to save the actual\n\t\t\t\t// compression level here, not s2_auto.\n\t\t\t\tr.updateS2AutoCompressionLevel(co, &r.route.compression)\n\t\t\t} else {\n\t\t\t\t// Simply change the compression writer\n\t\t\t\tr.out.cw = s2.NewWriter(nil, s2WriterOptions(newMode)...)\n\t\t\t\tr.route.compression = newMode\n\t\t\t}\n\t\t\tr.mu.Unlock()\n\t\t})\n\t}\n\ts.mu.Unlock()\n\tif c.newValue.Name != \"\" && c.newValue.Name != s.ClusterName() {\n\t\ts.setClusterName(c.newValue.Name)\n\t}\n\tfor _, r := range routes {\n\t\tr.closeConnection(ClientClosed)\n\t}\n\ts.Noticef(\"Reloaded: cluster\")\n\tif tlsRequired && c.newValue.TLSConfig.InsecureSkipVerify {\n\t\ts.Warnf(clusterTLSInsecureWarning)\n\t}\n}\n\nfunc (c *clusterOption) IsClusterPermsChange() bool {\n\treturn c.permsChanged\n}\n\nfunc (c *clusterOption) IsClusterPoolSizeOrAccountsChange() bool {\n\treturn c.poolSizeChanged || len(c.accsAdded) > 0 || len(c.accsRemoved) > 0\n}\n\nfunc (c *clusterOption) diffPoolAndAccounts(old *ClusterOpts) {\n\tc.poolSizeChanged = c.newValue.PoolSize != old.PoolSize\naddLoop:\n\tfor _, na := range c.newValue.PinnedAccounts {\n\t\tfor _, oa := range old.PinnedAccounts {\n\t\t\tif na == oa {\n\t\t\t\tcontinue addLoop\n\t\t\t}\n\t\t}\n\t\tc.accsAdded = append(c.accsAdded, na)\n\t}\nremoveLoop:\n\tfor _, oa := range old.PinnedAccounts {\n\t\tfor _, na := range c.newValue.PinnedAccounts {\n\t\t\tif oa == na {\n\t\t\t\tcontinue removeLoop\n\t\t\t}\n\t\t}\n\t\tc.accsRemoved = append(c.accsRemoved, oa)\n\t}\n}\n\n// routesOption implements the option interface for the cluster `routes`\n// setting.\ntype routesOption struct {\n\tnoopOption\n\tadd    []*url.URL\n\tremove []*url.URL\n}\n\n// Apply the route changes by adding and removing the necessary routes.\nfunc (r *routesOption) Apply(server *Server) {\n\tserver.mu.Lock()\n\troutes := make([]*client, server.numRoutes())\n\ti := 0\n\tserver.forEachRoute(func(r *client) {\n\t\troutes[i] = r\n\t\ti++\n\t})\n\t// If there was a change, notify monitoring code that it should\n\t// update the route URLs if /varz endpoint is inspected.\n\tif len(r.add)+len(r.remove) > 0 {\n\t\tserver.varzUpdateRouteURLs = true\n\t}\n\tserver.mu.Unlock()\n\n\t// Remove routes.\n\tfor _, remove := range r.remove {\n\t\tfor _, client := range routes {\n\t\t\tvar url *url.URL\n\t\t\tclient.mu.Lock()\n\t\t\tif client.route != nil {\n\t\t\t\turl = client.route.url\n\t\t\t}\n\t\t\tclient.mu.Unlock()\n\t\t\tif url != nil && urlsAreEqual(url, remove) {\n\t\t\t\t// Do not attempt to reconnect when route is removed.\n\t\t\t\tclient.setNoReconnect()\n\t\t\t\tclient.closeConnection(RouteRemoved)\n\t\t\t\tserver.Noticef(\"Removed route %v\", remove)\n\t\t\t}\n\t\t}\n\t}\n\n\t// Add routes.\n\tserver.mu.Lock()\n\tserver.solicitRoutes(r.add, server.getOpts().Cluster.PinnedAccounts)\n\tserver.mu.Unlock()\n\n\tserver.Noticef(\"Reloaded: cluster routes\")\n}\n\n// maxConnOption implements the option interface for the `max_connections`\n// setting.\ntype maxConnOption struct {\n\tnoopOption\n\tnewValue int\n}\n\n// Apply the max connections change by closing random connections til we are\n// below the limit if necessary.\nfunc (m *maxConnOption) Apply(server *Server) {\n\tserver.mu.Lock()\n\tvar (\n\t\tclients = make([]*client, len(server.clients))\n\t\ti       = 0\n\t)\n\t// Map iteration is random, which allows us to close random connections.\n\tfor _, client := range server.clients {\n\t\tclients[i] = client\n\t\ti++\n\t}\n\tserver.mu.Unlock()\n\n\tif m.newValue > 0 && len(clients) > m.newValue {\n\t\t// Close connections til we are within the limit.\n\t\tvar (\n\t\t\tnumClose = len(clients) - m.newValue\n\t\t\tclosed   = 0\n\t\t)\n\t\tfor _, client := range clients {\n\t\t\tclient.maxConnExceeded()\n\t\t\tclosed++\n\t\t\tif closed >= numClose {\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\tserver.Noticef(\"Closed %d connections to fall within max_connections\", closed)\n\t}\n\tserver.Noticef(\"Reloaded: max_connections = %v\", m.newValue)\n}\n\n// pidFileOption implements the option interface for the `pid_file` setting.\ntype pidFileOption struct {\n\tnoopOption\n\tnewValue string\n}\n\n// Apply the setting by logging the pid to the new file.\nfunc (p *pidFileOption) Apply(server *Server) {\n\tif p.newValue == \"\" {\n\t\treturn\n\t}\n\tif err := server.logPid(); err != nil {\n\t\tserver.Errorf(\"Failed to write pidfile: %v\", err)\n\t}\n\tserver.Noticef(\"Reloaded: pid_file = %v\", p.newValue)\n}\n\n// portsFileDirOption implements the option interface for the `portFileDir` setting.\ntype portsFileDirOption struct {\n\tnoopOption\n\toldValue string\n\tnewValue string\n}\n\nfunc (p *portsFileDirOption) Apply(server *Server) {\n\tserver.deletePortsFile(p.oldValue)\n\tserver.logPorts()\n\tserver.Noticef(\"Reloaded: ports_file_dir = %v\", p.newValue)\n}\n\n// maxControlLineOption implements the option interface for the\n// `max_control_line` setting.\ntype maxControlLineOption struct {\n\tnoopOption\n\tnewValue int32\n}\n\n// Apply the setting by updating each client.\nfunc (m *maxControlLineOption) Apply(server *Server) {\n\tmcl := int32(m.newValue)\n\tserver.mu.Lock()\n\tfor _, client := range server.clients {\n\t\tatomic.StoreInt32(&client.mcl, mcl)\n\t}\n\tserver.mu.Unlock()\n\tserver.Noticef(\"Reloaded: max_control_line = %d\", mcl)\n}\n\n// maxPayloadOption implements the option interface for the `max_payload`\n// setting.\ntype maxPayloadOption struct {\n\tnoopOption\n\tnewValue int32\n}\n\n// Apply the setting by updating the server info and each client.\nfunc (m *maxPayloadOption) Apply(server *Server) {\n\tserver.mu.Lock()\n\tserver.info.MaxPayload = m.newValue\n\tfor _, client := range server.clients {\n\t\tatomic.StoreInt32(&client.mpay, int32(m.newValue))\n\t}\n\tserver.mu.Unlock()\n\tserver.Noticef(\"Reloaded: max_payload = %d\", m.newValue)\n}\n\n// pingIntervalOption implements the option interface for the `ping_interval`\n// setting.\ntype pingIntervalOption struct {\n\tnoopOption\n\tnewValue time.Duration\n}\n\n// Apply is a no-op because the ping interval will be reloaded after options\n// are applied.\nfunc (p *pingIntervalOption) Apply(server *Server) {\n\tserver.Noticef(\"Reloaded: ping_interval = %s\", p.newValue)\n}\n\n// maxPingsOutOption implements the option interface for the `ping_max`\n// setting.\ntype maxPingsOutOption struct {\n\tnoopOption\n\tnewValue int\n}\n\n// Apply is a no-op because the ping interval will be reloaded after options\n// are applied.\nfunc (m *maxPingsOutOption) Apply(server *Server) {\n\tserver.Noticef(\"Reloaded: ping_max = %d\", m.newValue)\n}\n\n// writeDeadlineOption implements the option interface for the `write_deadline`\n// setting.\ntype writeDeadlineOption struct {\n\tnoopOption\n\tnewValue time.Duration\n}\n\n// Apply is a no-op because the write deadline will be reloaded after options\n// are applied.\nfunc (w *writeDeadlineOption) Apply(server *Server) {\n\tserver.Noticef(\"Reloaded: write_deadline = %s\", w.newValue)\n}\n\n// clientAdvertiseOption implements the option interface for the `client_advertise` setting.\ntype clientAdvertiseOption struct {\n\tnoopOption\n\tnewValue string\n}\n\n// Apply the setting by updating the server info and regenerate the infoJSON byte array.\nfunc (c *clientAdvertiseOption) Apply(server *Server) {\n\tserver.mu.Lock()\n\tserver.setInfoHostPort()\n\tserver.mu.Unlock()\n\tserver.Noticef(\"Reload: client_advertise = %s\", c.newValue)\n}\n\n// accountsOption implements the option interface.\n// Ensure that authorization code is executed if any change in accounts\ntype accountsOption struct {\n\tauthOption\n}\n\n// Apply is a no-op. Changes will be applied in reloadAuthorization\nfunc (a *accountsOption) Apply(s *Server) {\n\ts.Noticef(\"Reloaded: accounts\")\n}\n\n// For changes to a server's config.\ntype jetStreamOption struct {\n\tnoopOption\n\tnewValue bool\n}\n\nfunc (a *jetStreamOption) Apply(s *Server) {\n\ts.Noticef(\"Reloaded: JetStream\")\n}\n\nfunc (jso jetStreamOption) IsJetStreamChange() bool {\n\treturn true\n}\n\nfunc (jso jetStreamOption) IsStatszChange() bool {\n\treturn true\n}\n\ntype defaultSentinelOption struct {\n\tnoopOption\n\tnewValue string\n}\n\nfunc (so *defaultSentinelOption) Apply(s *Server) {\n\ts.Noticef(\"Reloaded: default_sentinel = %s\", so.newValue)\n}\n\ntype ocspOption struct {\n\ttlsOption\n\tnewValue *OCSPConfig\n}\n\nfunc (a *ocspOption) Apply(s *Server) {\n\ts.Noticef(\"Reloaded: OCSP\")\n}\n\ntype ocspResponseCacheOption struct {\n\ttlsOption\n\tnewValue *OCSPResponseCacheConfig\n}\n\nfunc (a *ocspResponseCacheOption) Apply(s *Server) {\n\ts.Noticef(\"Reloaded OCSP peer cache\")\n}\n\n// connectErrorReports implements the option interface for the `connect_error_reports`\n// setting.\ntype connectErrorReports struct {\n\tnoopOption\n\tnewValue int\n}\n\n// Apply is a no-op because the value will be reloaded after options are applied.\nfunc (c *connectErrorReports) Apply(s *Server) {\n\ts.Noticef(\"Reloaded: connect_error_reports = %v\", c.newValue)\n}\n\n// connectErrorReports implements the option interface for the `connect_error_reports`\n// setting.\ntype reconnectErrorReports struct {\n\tnoopOption\n\tnewValue int\n}\n\n// Apply is a no-op because the value will be reloaded after options are applied.\nfunc (r *reconnectErrorReports) Apply(s *Server) {\n\ts.Noticef(\"Reloaded: reconnect_error_reports = %v\", r.newValue)\n}\n\n// maxTracedMsgLenOption implements the option interface for the `max_traced_msg_len` setting.\ntype maxTracedMsgLenOption struct {\n\tnoopOption\n\tnewValue int\n}\n\n// Apply the setting by updating the maximum traced message length.\nfunc (m *maxTracedMsgLenOption) Apply(server *Server) {\n\tserver.mu.Lock()\n\tdefer server.mu.Unlock()\n\tserver.opts.MaxTracedMsgLen = m.newValue\n\tserver.Noticef(\"Reloaded: max_traced_msg_len = %d\", m.newValue)\n}\n\ntype mqttAckWaitReload struct {\n\tnoopOption\n\tnewValue time.Duration\n}\n\nfunc (o *mqttAckWaitReload) Apply(s *Server) {\n\ts.Noticef(\"Reloaded: MQTT ack_wait = %v\", o.newValue)\n}\n\ntype mqttMaxAckPendingReload struct {\n\tnoopOption\n\tnewValue uint16\n}\n\nfunc (o *mqttMaxAckPendingReload) Apply(s *Server) {\n\ts.mqttUpdateMaxAckPending(o.newValue)\n\ts.Noticef(\"Reloaded: MQTT max_ack_pending = %v\", o.newValue)\n}\n\ntype mqttStreamReplicasReload struct {\n\tnoopOption\n\tnewValue int\n}\n\nfunc (o *mqttStreamReplicasReload) Apply(s *Server) {\n\ts.Noticef(\"Reloaded: MQTT stream_replicas = %v\", o.newValue)\n}\n\ntype mqttConsumerReplicasReload struct {\n\tnoopOption\n\tnewValue int\n}\n\nfunc (o *mqttConsumerReplicasReload) Apply(s *Server) {\n\ts.Noticef(\"Reloaded: MQTT consumer_replicas = %v\", o.newValue)\n}\n\ntype mqttConsumerMemoryStorageReload struct {\n\tnoopOption\n\tnewValue bool\n}\n\nfunc (o *mqttConsumerMemoryStorageReload) Apply(s *Server) {\n\ts.Noticef(\"Reloaded: MQTT consumer_memory_storage = %v\", o.newValue)\n}\n\ntype mqttInactiveThresholdReload struct {\n\tnoopOption\n\tnewValue time.Duration\n}\n\nfunc (o *mqttInactiveThresholdReload) Apply(s *Server) {\n\ts.Noticef(\"Reloaded: MQTT consumer_inactive_threshold = %v\", o.newValue)\n}\n\ntype profBlockRateReload struct {\n\tnoopOption\n\tnewValue int\n}\n\nfunc (o *profBlockRateReload) Apply(s *Server) {\n\ts.setBlockProfileRate(o.newValue)\n\ts.Noticef(\"Reloaded: prof_block_rate = %v\", o.newValue)\n}\n\ntype leafNodeOption struct {\n\tnoopOption\n\ttlsFirstChanged    bool\n\tcompressionChanged bool\n}\n\nfunc (l *leafNodeOption) Apply(s *Server) {\n\topts := s.getOpts()\n\tif l.tlsFirstChanged {\n\t\ts.Noticef(\"Reloaded: LeafNode TLS HandshakeFirst value is: %v\", opts.LeafNode.TLSHandshakeFirst)\n\t\ts.Noticef(\"Reloaded: LeafNode TLS HandshakeFirstFallback value is: %v\", opts.LeafNode.TLSHandshakeFirstFallback)\n\t\tfor _, r := range opts.LeafNode.Remotes {\n\t\t\ts.Noticef(\"Reloaded: LeafNode Remote to %v TLS HandshakeFirst value is: %v\", r.URLs, r.TLSHandshakeFirst)\n\t\t}\n\t}\n\tif l.compressionChanged {\n\t\tvar leafs []*client\n\t\tacceptSideCompOpts := &opts.LeafNode.Compression\n\n\t\ts.mu.RLock()\n\t\t// First, update our internal leaf remote configurations with the new\n\t\t// compress options.\n\t\t// Since changing the remotes (as in adding/removing) is currently not\n\t\t// supported, we know that we should have the same number in Options\n\t\t// than in leafRemoteCfgs, but to be sure, use the max size.\n\t\tmax := len(opts.LeafNode.Remotes)\n\t\tif l := len(s.leafRemoteCfgs); l < max {\n\t\t\tmax = l\n\t\t}\n\t\tfor i := 0; i < max; i++ {\n\t\t\tlr := s.leafRemoteCfgs[i]\n\t\t\tlr.Lock()\n\t\t\tlr.Compression = opts.LeafNode.Remotes[i].Compression\n\t\t\tlr.Unlock()\n\t\t}\n\n\t\tfor _, l := range s.leafs {\n\t\t\tvar co *CompressionOpts\n\n\t\t\tl.mu.Lock()\n\t\t\tif r := l.leaf.remote; r != nil {\n\t\t\t\tco = &r.Compression\n\t\t\t} else {\n\t\t\t\tco = acceptSideCompOpts\n\t\t\t}\n\t\t\tnewMode := co.Mode\n\t\t\t// Skip leaf connections that are \"not supported\" (because they\n\t\t\t// will never do compression) or the ones that have already the\n\t\t\t// new compression mode.\n\t\t\tif l.leaf.compression == CompressionNotSupported || l.leaf.compression == newMode {\n\t\t\t\tl.mu.Unlock()\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\t// We need to close the connections if it had compression \"off\" or the new\n\t\t\t// mode is compression \"off\", or if the new mode is \"accept\", because\n\t\t\t// these require negotiation.\n\t\t\tif l.leaf.compression == CompressionOff || newMode == CompressionOff || newMode == CompressionAccept {\n\t\t\t\tleafs = append(leafs, l)\n\t\t\t} else if newMode == CompressionS2Auto {\n\t\t\t\t// If the mode is \"s2_auto\", we need to check if there is really\n\t\t\t\t// need to change, and at any rate, we want to save the actual\n\t\t\t\t// compression level here, not s2_auto.\n\t\t\t\tl.updateS2AutoCompressionLevel(co, &l.leaf.compression)\n\t\t\t} else {\n\t\t\t\t// Simply change the compression writer\n\t\t\t\tl.out.cw = s2.NewWriter(nil, s2WriterOptions(newMode)...)\n\t\t\t\tl.leaf.compression = newMode\n\t\t\t}\n\t\t\tl.mu.Unlock()\n\t\t}\n\t\ts.mu.RUnlock()\n\t\t// Close the connections for which negotiation is required.\n\t\tfor _, l := range leafs {\n\t\t\tl.closeConnection(ClientClosed)\n\t\t}\n\t\ts.Noticef(\"Reloaded: LeafNode compression settings\")\n\t}\n}\n\ntype noFastProdStallReload struct {\n\tnoopOption\n\tnoStall bool\n}\n\nfunc (l *noFastProdStallReload) Apply(s *Server) {\n\tvar not string\n\tif l.noStall {\n\t\tnot = \"not \"\n\t}\n\ts.Noticef(\"Reloaded: fast producers will %sbe stalled\", not)\n}\n\n// Compares options and disconnects clients that are no longer listed in pinned certs. Lock must not be held.\nfunc (s *Server) recheckPinnedCerts(curOpts *Options, newOpts *Options) {\n\ts.mu.Lock()\n\tdisconnectClients := []*client{}\n\tprotoToPinned := map[int]PinnedCertSet{}\n\tif !reflect.DeepEqual(newOpts.TLSPinnedCerts, curOpts.TLSPinnedCerts) {\n\t\tprotoToPinned[NATS] = curOpts.TLSPinnedCerts\n\t}\n\tif !reflect.DeepEqual(newOpts.MQTT.TLSPinnedCerts, curOpts.MQTT.TLSPinnedCerts) {\n\t\tprotoToPinned[MQTT] = curOpts.MQTT.TLSPinnedCerts\n\t}\n\tif !reflect.DeepEqual(newOpts.Websocket.TLSPinnedCerts, curOpts.Websocket.TLSPinnedCerts) {\n\t\tprotoToPinned[WS] = curOpts.Websocket.TLSPinnedCerts\n\t}\n\tfor _, c := range s.clients {\n\t\tif c.kind != CLIENT {\n\t\t\tcontinue\n\t\t}\n\t\tif pinned, ok := protoToPinned[c.clientType()]; ok {\n\t\t\tif !c.matchesPinnedCert(pinned) {\n\t\t\t\tdisconnectClients = append(disconnectClients, c)\n\t\t\t}\n\t\t}\n\t}\n\tcheckClients := func(kind int, clients map[uint64]*client, set PinnedCertSet) {\n\t\tfor _, c := range clients {\n\t\t\tif c.kind == kind && !c.matchesPinnedCert(set) {\n\t\t\t\tdisconnectClients = append(disconnectClients, c)\n\t\t\t}\n\t\t}\n\t}\n\tif !reflect.DeepEqual(newOpts.LeafNode.TLSPinnedCerts, curOpts.LeafNode.TLSPinnedCerts) {\n\t\tcheckClients(LEAF, s.leafs, newOpts.LeafNode.TLSPinnedCerts)\n\t}\n\tif !reflect.DeepEqual(newOpts.Cluster.TLSPinnedCerts, curOpts.Cluster.TLSPinnedCerts) {\n\t\ts.forEachRoute(func(c *client) {\n\t\t\tif !c.matchesPinnedCert(newOpts.Cluster.TLSPinnedCerts) {\n\t\t\t\tdisconnectClients = append(disconnectClients, c)\n\t\t\t}\n\t\t})\n\t}\n\tif s.gateway.enabled && reflect.DeepEqual(newOpts.Gateway.TLSPinnedCerts, curOpts.Gateway.TLSPinnedCerts) {\n\t\tgw := s.gateway\n\t\tgw.RLock()\n\t\tfor _, c := range gw.out {\n\t\t\tif !c.matchesPinnedCert(newOpts.Gateway.TLSPinnedCerts) {\n\t\t\t\tdisconnectClients = append(disconnectClients, c)\n\t\t\t}\n\t\t}\n\t\tcheckClients(GATEWAY, gw.in, newOpts.Gateway.TLSPinnedCerts)\n\t\tgw.RUnlock()\n\t}\n\ts.mu.Unlock()\n\tif len(disconnectClients) > 0 {\n\t\ts.Noticef(\"Disconnect %d clients due to pinned certs reload\", len(disconnectClients))\n\t\tfor _, c := range disconnectClients {\n\t\t\tc.closeConnection(TLSHandshakeError)\n\t\t}\n\t}\n}\n\n// Reload reads the current configuration file and calls out to ReloadOptions\n// to apply the changes. This returns an error if the server was not started\n// with a config file or an option which doesn't support hot-swapping was changed.\nfunc (s *Server) Reload() error {\n\ts.mu.Lock()\n\tconfigFile := s.configFile\n\ts.mu.Unlock()\n\tif configFile == \"\" {\n\t\treturn errors.New(\"can only reload config when a file is provided using -c or --config\")\n\t}\n\n\tnewOpts, err := ProcessConfigFile(configFile)\n\tif err != nil {\n\t\t// TODO: Dump previous good config to a .bak file?\n\t\treturn err\n\t}\n\treturn s.ReloadOptions(newOpts)\n}\n\n// ReloadOptions applies any supported options from the provided Options\n// type. This returns an error if an option which doesn't support\n// hot-swapping was changed.\n// The provided Options type should not be re-used afterwards.\n// Either use Options.Clone() to pass a copy, or make a new one.\nfunc (s *Server) ReloadOptions(newOpts *Options) error {\n\ts.reloadMu.Lock()\n\tdefer s.reloadMu.Unlock()\n\n\ts.mu.Lock()\n\n\tcurOpts := s.getOpts()\n\n\t// Wipe trusted keys if needed when we have an operator.\n\tif len(curOpts.TrustedOperators) > 0 && len(curOpts.TrustedKeys) > 0 {\n\t\tcurOpts.TrustedKeys = nil\n\t}\n\n\tclientOrgPort := curOpts.Port\n\tclusterOrgPort := curOpts.Cluster.Port\n\tgatewayOrgPort := curOpts.Gateway.Port\n\tleafnodesOrgPort := curOpts.LeafNode.Port\n\twebsocketOrgPort := curOpts.Websocket.Port\n\tmqttOrgPort := curOpts.MQTT.Port\n\n\ts.mu.Unlock()\n\n\t// In case \"-cluster ...\" was provided through the command line, this will\n\t// properly set the Cluster.Host/Port etc...\n\tif l := curOpts.Cluster.ListenStr; l != _EMPTY_ {\n\t\tnewOpts.Cluster.ListenStr = l\n\t\toverrideCluster(newOpts)\n\t}\n\n\t// Apply flags over config file settings.\n\tnewOpts = MergeOptions(newOpts, FlagSnapshot)\n\n\t// Need more processing for boolean flags...\n\tif FlagSnapshot != nil {\n\t\tapplyBoolFlags(newOpts, FlagSnapshot)\n\t}\n\n\tsetBaselineOptions(newOpts)\n\n\t// setBaselineOptions sets Port to 0 if set to -1 (RANDOM port)\n\t// If that's the case, set it to the saved value when the accept loop was\n\t// created.\n\tif newOpts.Port == 0 {\n\t\tnewOpts.Port = clientOrgPort\n\t}\n\t// We don't do that for cluster, so check against -1.\n\tif newOpts.Cluster.Port == -1 {\n\t\tnewOpts.Cluster.Port = clusterOrgPort\n\t}\n\tif newOpts.Gateway.Port == -1 {\n\t\tnewOpts.Gateway.Port = gatewayOrgPort\n\t}\n\tif newOpts.LeafNode.Port == -1 {\n\t\tnewOpts.LeafNode.Port = leafnodesOrgPort\n\t}\n\tif newOpts.Websocket.Port == -1 {\n\t\tnewOpts.Websocket.Port = websocketOrgPort\n\t}\n\tif newOpts.MQTT.Port == -1 {\n\t\tnewOpts.MQTT.Port = mqttOrgPort\n\t}\n\n\tif err := s.reloadOptions(curOpts, newOpts); err != nil {\n\t\treturn err\n\t}\n\n\ts.recheckPinnedCerts(curOpts, newOpts)\n\n\ts.mu.Lock()\n\ts.configTime = time.Now().UTC()\n\ts.updateVarzConfigReloadableFields(s.varz)\n\ts.mu.Unlock()\n\treturn nil\n}\nfunc applyBoolFlags(newOpts, flagOpts *Options) {\n\t// Reset fields that may have been set to `true` in\n\t// MergeOptions() when some of the flags default to `true`\n\t// but have not been explicitly set and therefore value\n\t// from config file should take precedence.\n\tfor name, val := range newOpts.inConfig {\n\t\tf := reflect.ValueOf(newOpts).Elem()\n\t\tnames := strings.Split(name, \".\")\n\t\tfor _, name := range names {\n\t\t\tf = f.FieldByName(name)\n\t\t}\n\t\tf.SetBool(val)\n\t}\n\t// Now apply value (true or false) from flags that have\n\t// been explicitly set in command line\n\tfor name, val := range flagOpts.inCmdLine {\n\t\tf := reflect.ValueOf(newOpts).Elem()\n\t\tnames := strings.Split(name, \".\")\n\t\tfor _, name := range names {\n\t\t\tf = f.FieldByName(name)\n\t\t}\n\t\tf.SetBool(val)\n\t}\n}\n\n// reloadOptions reloads the server config with the provided options. If an\n// option that doesn't support hot-swapping is changed, this returns an error.\nfunc (s *Server) reloadOptions(curOpts, newOpts *Options) error {\n\t// Apply to the new options some of the options that may have been set\n\t// that can't be configured in the config file (this can happen in\n\t// applications starting NATS Server programmatically).\n\tnewOpts.CustomClientAuthentication = curOpts.CustomClientAuthentication\n\tnewOpts.CustomRouterAuthentication = curOpts.CustomRouterAuthentication\n\n\tchanged, err := s.diffOptions(newOpts)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif len(changed) != 0 {\n\t\tif err := validateOptions(newOpts); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\t// Create a context that is used to pass special info that we may need\n\t// while applying the new options.\n\tctx := reloadContext{oldClusterPerms: curOpts.Cluster.Permissions}\n\ts.setOpts(newOpts)\n\ts.applyOptions(&ctx, changed)\n\treturn nil\n}\n\n// For the purpose of comparing, impose a order on slice data types where order does not matter\nfunc imposeOrder(value any) error {\n\tswitch value := value.(type) {\n\tcase []*Account:\n\t\tslices.SortFunc(value, func(i, j *Account) int { return cmp.Compare(i.Name, j.Name) })\n\t\tfor _, a := range value {\n\t\t\tslices.SortFunc(a.imports.streams, func(i, j *streamImport) int { return cmp.Compare(i.acc.Name, j.acc.Name) })\n\t\t}\n\tcase []*User:\n\t\tslices.SortFunc(value, func(i, j *User) int { return cmp.Compare(i.Username, j.Username) })\n\tcase []*NkeyUser:\n\t\tslices.SortFunc(value, func(i, j *NkeyUser) int { return cmp.Compare(i.Nkey, j.Nkey) })\n\tcase []*url.URL:\n\t\tslices.SortFunc(value, func(i, j *url.URL) int { return cmp.Compare(i.String(), j.String()) })\n\tcase []string:\n\t\tslices.Sort(value)\n\tcase []*jwt.OperatorClaims:\n\t\tslices.SortFunc(value, func(i, j *jwt.OperatorClaims) int { return cmp.Compare(i.Issuer, j.Issuer) })\n\tcase GatewayOpts:\n\t\tslices.SortFunc(value.Gateways, func(i, j *RemoteGatewayOpts) int { return cmp.Compare(i.Name, j.Name) })\n\tcase WebsocketOpts:\n\t\tslices.Sort(value.AllowedOrigins)\n\tcase string, bool, uint8, uint16, int, int32, int64, time.Duration, float64, nil, LeafNodeOpts, ClusterOpts, *tls.Config, PinnedCertSet,\n\t\t*URLAccResolver, *MemAccResolver, *DirAccResolver, *CacheDirAccResolver, Authentication, MQTTOpts, jwt.TagList,\n\t\t*OCSPConfig, map[string]string, JSLimitOpts, StoreCipher, *OCSPResponseCacheConfig:\n\t\t// explicitly skipped types\n\tcase *AuthCallout:\n\tcase JSTpmOpts:\n\tdefault:\n\t\t// this will fail during unit tests\n\t\treturn fmt.Errorf(\"OnReload, sort or explicitly skip type: %s\",\n\t\t\treflect.TypeOf(value))\n\t}\n\treturn nil\n}\n\n// diffOptions returns a slice containing options which have been changed. If\n// an option that doesn't support hot-swapping is changed, this returns an\n// error.\nfunc (s *Server) diffOptions(newOpts *Options) ([]option, error) {\n\tvar (\n\t\toldConfig = reflect.ValueOf(s.getOpts()).Elem()\n\t\tnewConfig = reflect.ValueOf(newOpts).Elem()\n\t\tdiffOpts  = []option{}\n\n\t\t// Need to keep track of whether JS is being disabled\n\t\t// to prevent changing limits at runtime.\n\t\tjsEnabled           = s.JetStreamEnabled()\n\t\tdisableJS           bool\n\t\tjsMemLimitsChanged  bool\n\t\tjsFileLimitsChanged bool\n\t\tjsStoreDirChanged   bool\n\t)\n\tfor i := 0; i < oldConfig.NumField(); i++ {\n\t\tfield := oldConfig.Type().Field(i)\n\t\t// field.PkgPath is empty for exported fields, and is not for unexported ones.\n\t\t// We skip the unexported fields.\n\t\tif field.PkgPath != _EMPTY_ {\n\t\t\tcontinue\n\t\t}\n\t\tvar (\n\t\t\toldValue = oldConfig.Field(i).Interface()\n\t\t\tnewValue = newConfig.Field(i).Interface()\n\t\t)\n\t\tif err := imposeOrder(oldValue); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tif err := imposeOrder(newValue); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\toptName := strings.ToLower(field.Name)\n\t\t// accounts and users (referencing accounts) will always differ as accounts\n\t\t// contain internal state, say locks etc..., so we don't bother here.\n\t\t// This also avoids races with atomic stats counters\n\t\tif optName != \"accounts\" && optName != \"users\" {\n\t\t\tif changed := !reflect.DeepEqual(oldValue, newValue); !changed {\n\t\t\t\t// Check to make sure we are running JetStream if we think we should be.\n\t\t\t\tif optName == \"jetstream\" && newValue.(bool) {\n\t\t\t\t\tif !jsEnabled {\n\t\t\t\t\t\tdiffOpts = append(diffOpts, &jetStreamOption{newValue: true})\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tcontinue\n\t\t\t}\n\t\t}\n\t\tswitch optName {\n\t\tcase \"traceverbose\":\n\t\t\tdiffOpts = append(diffOpts, &traceVerboseOption{newValue: newValue.(bool)})\n\t\tcase \"traceheaders\":\n\t\t\tdiffOpts = append(diffOpts, &traceHeadersOption{newValue: newValue.(bool)})\n\t\tcase \"trace\":\n\t\t\tdiffOpts = append(diffOpts, &traceOption{newValue: newValue.(bool)})\n\t\tcase \"debug\":\n\t\t\tdiffOpts = append(diffOpts, &debugOption{newValue: newValue.(bool)})\n\t\tcase \"logtime\":\n\t\t\tdiffOpts = append(diffOpts, &logtimeOption{newValue: newValue.(bool)})\n\t\tcase \"logtimeutc\":\n\t\t\tdiffOpts = append(diffOpts, &logtimeUTCOption{newValue: newValue.(bool)})\n\t\tcase \"logfile\":\n\t\t\tdiffOpts = append(diffOpts, &logfileOption{newValue: newValue.(string)})\n\t\tcase \"syslog\":\n\t\t\tdiffOpts = append(diffOpts, &syslogOption{newValue: newValue.(bool)})\n\t\tcase \"remotesyslog\":\n\t\t\tdiffOpts = append(diffOpts, &remoteSyslogOption{newValue: newValue.(string)})\n\t\tcase \"tlsconfig\":\n\t\t\tdiffOpts = append(diffOpts, &tlsOption{newValue: newValue.(*tls.Config)})\n\t\tcase \"tlstimeout\":\n\t\t\tdiffOpts = append(diffOpts, &tlsTimeoutOption{newValue: newValue.(float64)})\n\t\tcase \"tlspinnedcerts\":\n\t\t\tdiffOpts = append(diffOpts, &tlsPinnedCertOption{newValue: newValue.(PinnedCertSet)})\n\t\tcase \"tlshandshakefirst\":\n\t\t\tdiffOpts = append(diffOpts, &tlsHandshakeFirst{newValue: newValue.(bool)})\n\t\tcase \"tlshandshakefirstfallback\":\n\t\t\tdiffOpts = append(diffOpts, &tlsHandshakeFirstFallback{newValue: newValue.(time.Duration)})\n\t\tcase \"username\":\n\t\t\tdiffOpts = append(diffOpts, &usernameOption{})\n\t\tcase \"password\":\n\t\t\tdiffOpts = append(diffOpts, &passwordOption{})\n\t\tcase \"tags\":\n\t\t\tdiffOpts = append(diffOpts, &tagsOption{})\n\t\tcase \"authorization\":\n\t\t\tdiffOpts = append(diffOpts, &authorizationOption{})\n\t\tcase \"authtimeout\":\n\t\t\tdiffOpts = append(diffOpts, &authTimeoutOption{newValue: newValue.(float64)})\n\t\tcase \"users\":\n\t\t\tdiffOpts = append(diffOpts, &usersOption{})\n\t\tcase \"nkeys\":\n\t\t\tdiffOpts = append(diffOpts, &nkeysOption{})\n\t\tcase \"cluster\":\n\t\t\tnewClusterOpts := newValue.(ClusterOpts)\n\t\t\toldClusterOpts := oldValue.(ClusterOpts)\n\t\t\tif err := validateClusterOpts(oldClusterOpts, newClusterOpts); err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tco := &clusterOption{\n\t\t\t\tnewValue:        newClusterOpts,\n\t\t\t\tpermsChanged:    !reflect.DeepEqual(newClusterOpts.Permissions, oldClusterOpts.Permissions),\n\t\t\t\tcompressChanged: !reflect.DeepEqual(oldClusterOpts.Compression, newClusterOpts.Compression),\n\t\t\t}\n\t\t\tco.diffPoolAndAccounts(&oldClusterOpts)\n\t\t\t// If there are added accounts, first make sure that we can look them up.\n\t\t\t// If we can't let's fail the reload.\n\t\t\tfor _, acc := range co.accsAdded {\n\t\t\t\tif _, err := s.LookupAccount(acc); err != nil {\n\t\t\t\t\treturn nil, fmt.Errorf(\"unable to add account %q to the list of dedicated routes: %v\", acc, err)\n\t\t\t\t}\n\t\t\t}\n\t\t\t// If pool_size has been set to negative (but was not before), then let's\n\t\t\t// add the system account to the list of removed accounts (we don't have\n\t\t\t// to check if already there, duplicates are ok in that case).\n\t\t\tif newClusterOpts.PoolSize < 0 && oldClusterOpts.PoolSize >= 0 {\n\t\t\t\tif sys := s.SystemAccount(); sys != nil {\n\t\t\t\t\tco.accsRemoved = append(co.accsRemoved, sys.GetName())\n\t\t\t\t}\n\t\t\t}\n\t\t\tdiffOpts = append(diffOpts, co)\n\t\tcase \"routes\":\n\t\t\tadd, remove := diffRoutes(oldValue.([]*url.URL), newValue.([]*url.URL))\n\t\t\tdiffOpts = append(diffOpts, &routesOption{add: add, remove: remove})\n\t\tcase \"maxconn\":\n\t\t\tdiffOpts = append(diffOpts, &maxConnOption{newValue: newValue.(int)})\n\t\tcase \"pidfile\":\n\t\t\tdiffOpts = append(diffOpts, &pidFileOption{newValue: newValue.(string)})\n\t\tcase \"portsfiledir\":\n\t\t\tdiffOpts = append(diffOpts, &portsFileDirOption{newValue: newValue.(string), oldValue: oldValue.(string)})\n\t\tcase \"maxcontrolline\":\n\t\t\tdiffOpts = append(diffOpts, &maxControlLineOption{newValue: newValue.(int32)})\n\t\tcase \"maxpayload\":\n\t\t\tdiffOpts = append(diffOpts, &maxPayloadOption{newValue: newValue.(int32)})\n\t\tcase \"pinginterval\":\n\t\t\tdiffOpts = append(diffOpts, &pingIntervalOption{newValue: newValue.(time.Duration)})\n\t\tcase \"maxpingsout\":\n\t\t\tdiffOpts = append(diffOpts, &maxPingsOutOption{newValue: newValue.(int)})\n\t\tcase \"writedeadline\":\n\t\t\tdiffOpts = append(diffOpts, &writeDeadlineOption{newValue: newValue.(time.Duration)})\n\t\tcase \"clientadvertise\":\n\t\t\tcliAdv := newValue.(string)\n\t\t\tif cliAdv != \"\" {\n\t\t\t\t// Validate ClientAdvertise syntax\n\t\t\t\tif _, _, err := parseHostPort(cliAdv, 0); err != nil {\n\t\t\t\t\treturn nil, fmt.Errorf(\"invalid ClientAdvertise value of %s, err=%v\", cliAdv, err)\n\t\t\t\t}\n\t\t\t}\n\t\t\tdiffOpts = append(diffOpts, &clientAdvertiseOption{newValue: cliAdv})\n\t\tcase \"accounts\":\n\t\t\tdiffOpts = append(diffOpts, &accountsOption{})\n\t\tcase \"resolver\", \"accountresolver\", \"accountsresolver\":\n\t\t\t// We can't move from no resolver to one. So check for that.\n\t\t\tif (oldValue == nil && newValue != nil) ||\n\t\t\t\t(oldValue != nil && newValue == nil) {\n\t\t\t\treturn nil, fmt.Errorf(\"config reload does not support moving to or from an account resolver\")\n\t\t\t}\n\t\t\tdiffOpts = append(diffOpts, &accountsOption{})\n\t\tcase \"accountresolvertlsconfig\":\n\t\t\tdiffOpts = append(diffOpts, &accountsOption{})\n\t\tcase \"gateway\":\n\t\t\t// Not supported for now, but report warning if configuration of gateway\n\t\t\t// is actually changed so that user knows that it won't take effect.\n\n\t\t\t// Any deep-equal is likely to fail for when there is a TLSConfig. so\n\t\t\t// remove for the test.\n\t\t\ttmpOld := oldValue.(GatewayOpts)\n\t\t\ttmpNew := newValue.(GatewayOpts)\n\t\t\ttmpOld.TLSConfig = nil\n\t\t\ttmpNew.TLSConfig = nil\n\t\t\ttmpOld.tlsConfigOpts = nil\n\t\t\ttmpNew.tlsConfigOpts = nil\n\n\t\t\t// Need to do the same for remote gateways' TLS configs.\n\t\t\t// But we can't just set remotes' TLSConfig to nil otherwise this\n\t\t\t// would lose the real TLS configuration.\n\t\t\ttmpOld.Gateways = copyRemoteGWConfigsWithoutTLSConfig(tmpOld.Gateways)\n\t\t\ttmpNew.Gateways = copyRemoteGWConfigsWithoutTLSConfig(tmpNew.Gateways)\n\n\t\t\t// If there is really a change prevents reload.\n\t\t\tif !reflect.DeepEqual(tmpOld, tmpNew) {\n\t\t\t\t// See TODO(ik) note below about printing old/new values.\n\t\t\t\treturn nil, fmt.Errorf(\"config reload not supported for %s: old=%v, new=%v\",\n\t\t\t\t\tfield.Name, oldValue, newValue)\n\t\t\t}\n\t\tcase \"leafnode\":\n\t\t\t// Similar to gateways\n\t\t\ttmpOld := oldValue.(LeafNodeOpts)\n\t\t\ttmpNew := newValue.(LeafNodeOpts)\n\t\t\ttmpOld.TLSConfig = nil\n\t\t\ttmpNew.TLSConfig = nil\n\t\t\ttmpOld.tlsConfigOpts = nil\n\t\t\ttmpNew.tlsConfigOpts = nil\n\t\t\t// We will allow TLSHandshakeFirst to be config reloaded. First,\n\t\t\t// we just want to detect if there was a change in the leafnodes{}\n\t\t\t// block, and if not, we will check the remotes.\n\t\t\thandshakeFirstChanged := tmpOld.TLSHandshakeFirst != tmpNew.TLSHandshakeFirst ||\n\t\t\t\ttmpOld.TLSHandshakeFirstFallback != tmpNew.TLSHandshakeFirstFallback\n\t\t\t// If changed, set them (in the temporary variables) to false so that the\n\t\t\t// rest of the comparison does not fail.\n\t\t\tif handshakeFirstChanged {\n\t\t\t\ttmpOld.TLSHandshakeFirst, tmpNew.TLSHandshakeFirst = false, false\n\t\t\t\ttmpOld.TLSHandshakeFirstFallback, tmpNew.TLSHandshakeFirstFallback = 0, 0\n\t\t\t} else if len(tmpOld.Remotes) == len(tmpNew.Remotes) {\n\t\t\t\t// Since we don't support changes in the remotes, we will do a\n\t\t\t\t// simple pass to see if there was a change of this field.\n\t\t\t\tfor i := 0; i < len(tmpOld.Remotes); i++ {\n\t\t\t\t\tif tmpOld.Remotes[i].TLSHandshakeFirst != tmpNew.Remotes[i].TLSHandshakeFirst {\n\t\t\t\t\t\thandshakeFirstChanged = true\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\t// We also support config reload for compression. Check if it changed before\n\t\t\t// blanking them out for the deep-equal check at the end.\n\t\t\tcompressionChanged := !reflect.DeepEqual(tmpOld.Compression, tmpNew.Compression)\n\t\t\tif compressionChanged {\n\t\t\t\ttmpOld.Compression, tmpNew.Compression = CompressionOpts{}, CompressionOpts{}\n\t\t\t} else if len(tmpOld.Remotes) == len(tmpNew.Remotes) {\n\t\t\t\t// Same that for tls first check, do the remotes now.\n\t\t\t\tfor i := 0; i < len(tmpOld.Remotes); i++ {\n\t\t\t\t\tif !reflect.DeepEqual(tmpOld.Remotes[i].Compression, tmpNew.Remotes[i].Compression) {\n\t\t\t\t\t\tcompressionChanged = true\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// Need to do the same for remote leafnodes' TLS configs.\n\t\t\t// But we can't just set remotes' TLSConfig to nil otherwise this\n\t\t\t// would lose the real TLS configuration.\n\t\t\ttmpOld.Remotes = copyRemoteLNConfigForReloadCompare(tmpOld.Remotes)\n\t\t\ttmpNew.Remotes = copyRemoteLNConfigForReloadCompare(tmpNew.Remotes)\n\n\t\t\t// Special check for leafnode remotes changes which are not supported right now.\n\t\t\tleafRemotesChanged := func(a, b LeafNodeOpts) bool {\n\t\t\t\tif len(a.Remotes) != len(b.Remotes) {\n\t\t\t\t\treturn true\n\t\t\t\t}\n\n\t\t\t\t// Check whether all remotes URLs are still the same.\n\t\t\t\tfor _, oldRemote := range a.Remotes {\n\t\t\t\t\tvar found bool\n\n\t\t\t\t\tif oldRemote.LocalAccount == _EMPTY_ {\n\t\t\t\t\t\toldRemote.LocalAccount = globalAccountName\n\t\t\t\t\t}\n\n\t\t\t\t\tfor _, newRemote := range b.Remotes {\n\t\t\t\t\t\t// Bind to global account in case not defined.\n\t\t\t\t\t\tif newRemote.LocalAccount == _EMPTY_ {\n\t\t\t\t\t\t\tnewRemote.LocalAccount = globalAccountName\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\tif reflect.DeepEqual(oldRemote, newRemote) {\n\t\t\t\t\t\t\tfound = true\n\t\t\t\t\t\t\tbreak\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tif !found {\n\t\t\t\t\t\treturn true\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\treturn false\n\t\t\t}\n\n\t\t\t// First check whether remotes changed at all. If they did not,\n\t\t\t// skip them in the complete equal check.\n\t\t\tif !leafRemotesChanged(tmpOld, tmpNew) {\n\t\t\t\ttmpOld.Remotes = nil\n\t\t\t\ttmpNew.Remotes = nil\n\t\t\t}\n\n\t\t\t// Special check for auth users to detect changes.\n\t\t\t// If anything is off will fall through and fail below.\n\t\t\t// If we detect they are semantically the same we nil them out\n\t\t\t// to pass the check below.\n\t\t\tif tmpOld.Users != nil || tmpNew.Users != nil {\n\t\t\t\tif len(tmpOld.Users) == len(tmpNew.Users) {\n\t\t\t\t\toua := make(map[string]*User, len(tmpOld.Users))\n\t\t\t\t\tnua := make(map[string]*User, len(tmpOld.Users))\n\t\t\t\t\tfor _, u := range tmpOld.Users {\n\t\t\t\t\t\toua[u.Username] = u\n\t\t\t\t\t}\n\t\t\t\t\tfor _, u := range tmpNew.Users {\n\t\t\t\t\t\tnua[u.Username] = u\n\t\t\t\t\t}\n\t\t\t\t\tsame := true\n\t\t\t\t\tfor uname, u := range oua {\n\t\t\t\t\t\t// If we can not find new one with same name, drop through to fail.\n\t\t\t\t\t\tnu, ok := nua[uname]\n\t\t\t\t\t\tif !ok {\n\t\t\t\t\t\t\tsame = false\n\t\t\t\t\t\t\tbreak\n\t\t\t\t\t\t}\n\t\t\t\t\t\t// If username or password or account different break.\n\t\t\t\t\t\tif u.Username != nu.Username || u.Password != nu.Password || u.Account.GetName() != nu.Account.GetName() {\n\t\t\t\t\t\t\tsame = false\n\t\t\t\t\t\t\tbreak\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\t// We can nil out here.\n\t\t\t\t\tif same {\n\t\t\t\t\t\ttmpOld.Users, tmpNew.Users = nil, nil\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// If there is really a change prevents reload.\n\t\t\tif !reflect.DeepEqual(tmpOld, tmpNew) {\n\t\t\t\t// See TODO(ik) note below about printing old/new values.\n\t\t\t\treturn nil, fmt.Errorf(\"config reload not supported for %s: old=%v, new=%v\",\n\t\t\t\t\tfield.Name, oldValue, newValue)\n\t\t\t}\n\n\t\t\tdiffOpts = append(diffOpts, &leafNodeOption{\n\t\t\t\ttlsFirstChanged:    handshakeFirstChanged,\n\t\t\t\tcompressionChanged: compressionChanged,\n\t\t\t})\n\t\tcase \"jetstream\":\n\t\t\tnew := newValue.(bool)\n\t\t\told := oldValue.(bool)\n\t\t\tif new != old {\n\t\t\t\tdiffOpts = append(diffOpts, &jetStreamOption{newValue: new})\n\t\t\t}\n\n\t\t\t// Mark whether JS will be disabled.\n\t\t\tdisableJS = !new\n\t\tcase \"storedir\":\n\t\t\tnew := newValue.(string)\n\t\t\told := oldValue.(string)\n\t\t\tmodified := new != old\n\n\t\t\t// Check whether JS is being disabled and/or storage dir attempted to change.\n\t\t\tif jsEnabled && modified {\n\t\t\t\tif new == _EMPTY_ {\n\t\t\t\t\t// This means that either JS is being disabled or it is using an temp dir.\n\t\t\t\t\t// Allow the change but error in case JS was not disabled.\n\t\t\t\t\tjsStoreDirChanged = true\n\t\t\t\t} else {\n\t\t\t\t\treturn nil, fmt.Errorf(\"config reload not supported for jetstream storage directory\")\n\t\t\t\t}\n\t\t\t}\n\t\tcase \"jetstreammaxmemory\", \"jetstreammaxstore\":\n\t\t\told := oldValue.(int64)\n\t\t\tnew := newValue.(int64)\n\n\t\t\t// Check whether JS is being disabled and/or limits are being changed.\n\t\t\tvar (\n\t\t\t\tmodified  = new != old\n\t\t\t\tfromUnset = old == -1\n\t\t\t\tfromSet   = !fromUnset\n\t\t\t\ttoUnset   = new == -1\n\t\t\t\ttoSet     = !toUnset\n\t\t\t)\n\t\t\tif jsEnabled && modified {\n\t\t\t\t// Cannot change limits from dynamic storage at runtime.\n\t\t\t\tswitch {\n\t\t\t\tcase fromSet && toUnset:\n\t\t\t\t\t// Limits changed but it may mean that JS is being disabled,\n\t\t\t\t\t// keep track of the change and error in case it is not.\n\t\t\t\t\tswitch optName {\n\t\t\t\t\tcase \"jetstreammaxmemory\":\n\t\t\t\t\t\tjsMemLimitsChanged = true\n\t\t\t\t\tcase \"jetstreammaxstore\":\n\t\t\t\t\t\tjsFileLimitsChanged = true\n\t\t\t\t\tdefault:\n\t\t\t\t\t\treturn nil, fmt.Errorf(\"config reload not supported for jetstream max memory and store\")\n\t\t\t\t\t}\n\t\t\t\tcase fromUnset && toSet:\n\t\t\t\t\t// Prevent changing from dynamic max memory / file at runtime.\n\t\t\t\t\treturn nil, fmt.Errorf(\"config reload not supported for jetstream dynamic max memory and store\")\n\t\t\t\tdefault:\n\t\t\t\t\treturn nil, fmt.Errorf(\"config reload not supported for jetstream max memory and store\")\n\t\t\t\t}\n\t\t\t}\n\t\tcase \"websocket\":\n\t\t\t// Similar to gateways\n\t\t\ttmpOld := oldValue.(WebsocketOpts)\n\t\t\ttmpNew := newValue.(WebsocketOpts)\n\t\t\ttmpOld.TLSConfig, tmpOld.tlsConfigOpts = nil, nil\n\t\t\ttmpNew.TLSConfig, tmpNew.tlsConfigOpts = nil, nil\n\t\t\t// If there is really a change prevents reload.\n\t\t\tif !reflect.DeepEqual(tmpOld, tmpNew) {\n\t\t\t\t// See TODO(ik) note below about printing old/new values.\n\t\t\t\treturn nil, fmt.Errorf(\"config reload not supported for %s: old=%v, new=%v\",\n\t\t\t\t\tfield.Name, oldValue, newValue)\n\t\t\t}\n\t\tcase \"mqtt\":\n\t\t\tdiffOpts = append(diffOpts, &mqttAckWaitReload{newValue: newValue.(MQTTOpts).AckWait})\n\t\t\tdiffOpts = append(diffOpts, &mqttMaxAckPendingReload{newValue: newValue.(MQTTOpts).MaxAckPending})\n\t\t\tdiffOpts = append(diffOpts, &mqttStreamReplicasReload{newValue: newValue.(MQTTOpts).StreamReplicas})\n\t\t\tdiffOpts = append(diffOpts, &mqttConsumerReplicasReload{newValue: newValue.(MQTTOpts).ConsumerReplicas})\n\t\t\tdiffOpts = append(diffOpts, &mqttConsumerMemoryStorageReload{newValue: newValue.(MQTTOpts).ConsumerMemoryStorage})\n\t\t\tdiffOpts = append(diffOpts, &mqttInactiveThresholdReload{newValue: newValue.(MQTTOpts).ConsumerInactiveThreshold})\n\n\t\t\t// Nil out/set to 0 the options that we allow to be reloaded so that\n\t\t\t// we only fail reload if some that we don't support are changed.\n\t\t\ttmpOld := oldValue.(MQTTOpts)\n\t\t\ttmpNew := newValue.(MQTTOpts)\n\t\t\ttmpOld.TLSConfig, tmpOld.tlsConfigOpts, tmpOld.AckWait, tmpOld.MaxAckPending, tmpOld.StreamReplicas, tmpOld.ConsumerReplicas, tmpOld.ConsumerMemoryStorage = nil, nil, 0, 0, 0, 0, false\n\t\t\ttmpOld.ConsumerInactiveThreshold = 0\n\t\t\ttmpNew.TLSConfig, tmpNew.tlsConfigOpts, tmpNew.AckWait, tmpNew.MaxAckPending, tmpNew.StreamReplicas, tmpNew.ConsumerReplicas, tmpNew.ConsumerMemoryStorage = nil, nil, 0, 0, 0, 0, false\n\t\t\ttmpNew.ConsumerInactiveThreshold = 0\n\n\t\t\tif !reflect.DeepEqual(tmpOld, tmpNew) {\n\t\t\t\t// See TODO(ik) note below about printing old/new values.\n\t\t\t\treturn nil, fmt.Errorf(\"config reload not supported for %s: old=%v, new=%v\",\n\t\t\t\t\tfield.Name, oldValue, newValue)\n\t\t\t}\n\t\t\ttmpNew.AckWait = newValue.(MQTTOpts).AckWait\n\t\t\ttmpNew.MaxAckPending = newValue.(MQTTOpts).MaxAckPending\n\t\t\ttmpNew.StreamReplicas = newValue.(MQTTOpts).StreamReplicas\n\t\t\ttmpNew.ConsumerReplicas = newValue.(MQTTOpts).ConsumerReplicas\n\t\t\ttmpNew.ConsumerMemoryStorage = newValue.(MQTTOpts).ConsumerMemoryStorage\n\t\t\ttmpNew.ConsumerInactiveThreshold = newValue.(MQTTOpts).ConsumerInactiveThreshold\n\t\tcase \"connecterrorreports\":\n\t\t\tdiffOpts = append(diffOpts, &connectErrorReports{newValue: newValue.(int)})\n\t\tcase \"reconnecterrorreports\":\n\t\t\tdiffOpts = append(diffOpts, &reconnectErrorReports{newValue: newValue.(int)})\n\t\tcase \"nolog\", \"nosigs\":\n\t\t\t// Ignore NoLog and NoSigs options since they are not parsed and only used in\n\t\t\t// testing.\n\t\t\tcontinue\n\t\tcase \"disableshortfirstping\":\n\t\t\tnewOpts.DisableShortFirstPing = oldValue.(bool)\n\t\t\tcontinue\n\t\tcase \"maxtracedmsglen\":\n\t\t\tdiffOpts = append(diffOpts, &maxTracedMsgLenOption{newValue: newValue.(int)})\n\t\tcase \"port\":\n\t\t\t// check to see if newValue == 0 and continue if so.\n\t\t\tif newValue == 0 {\n\t\t\t\t// ignore RANDOM_PORT\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tfallthrough\n\t\tcase \"noauthuser\":\n\t\t\tif oldValue != _EMPTY_ && newValue == _EMPTY_ {\n\t\t\t\tfor _, user := range newOpts.Users {\n\t\t\t\t\tif user.Username == oldValue {\n\t\t\t\t\t\treturn nil, fmt.Errorf(\"config reload not supported for %s: old=%v, new=%v\",\n\t\t\t\t\t\t\tfield.Name, oldValue, newValue)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\treturn nil, fmt.Errorf(\"config reload not supported for %s: old=%v, new=%v\",\n\t\t\t\t\tfield.Name, oldValue, newValue)\n\t\t\t}\n\t\tcase \"defaultsentinel\":\n\t\t\tdiffOpts = append(diffOpts, &defaultSentinelOption{newValue: newValue.(string)})\n\t\tcase \"systemaccount\":\n\t\t\tif oldValue != DEFAULT_SYSTEM_ACCOUNT || newValue != _EMPTY_ {\n\t\t\t\treturn nil, fmt.Errorf(\"config reload not supported for %s: old=%v, new=%v\",\n\t\t\t\t\tfield.Name, oldValue, newValue)\n\t\t\t}\n\t\tcase \"ocspconfig\":\n\t\t\tdiffOpts = append(diffOpts, &ocspOption{newValue: newValue.(*OCSPConfig)})\n\t\tcase \"ocspcacheconfig\":\n\t\t\tdiffOpts = append(diffOpts, &ocspResponseCacheOption{newValue: newValue.(*OCSPResponseCacheConfig)})\n\t\tcase \"profblockrate\":\n\t\t\tnew := newValue.(int)\n\t\t\told := oldValue.(int)\n\t\t\tif new != old {\n\t\t\t\tdiffOpts = append(diffOpts, &profBlockRateReload{newValue: new})\n\t\t\t}\n\t\tcase \"configdigest\":\n\t\t\t// skip changes in config digest, this is handled already while\n\t\t\t// processing the config.\n\t\t\tcontinue\n\t\tcase \"nofastproducerstall\":\n\t\t\tdiffOpts = append(diffOpts, &noFastProdStallReload{noStall: newValue.(bool)})\n\t\tdefault:\n\t\t\t// TODO(ik): Implement String() on those options to have a nice print.\n\t\t\t// %v is difficult to figure what's what, %+v print private fields and\n\t\t\t// would print passwords. Tried json.Marshal but it is too verbose for\n\t\t\t// the URL array.\n\n\t\t\t// Bail out if attempting to reload any unsupported options.\n\t\t\treturn nil, fmt.Errorf(\"config reload not supported for %s: old=%v, new=%v\",\n\t\t\t\tfield.Name, oldValue, newValue)\n\t\t}\n\t}\n\n\t// If not disabling JS but limits have changed then it is an error.\n\tif !disableJS {\n\t\tif jsMemLimitsChanged || jsFileLimitsChanged {\n\t\t\treturn nil, fmt.Errorf(\"config reload not supported for jetstream max memory and max store\")\n\t\t}\n\t\tif jsStoreDirChanged {\n\t\t\treturn nil, fmt.Errorf(\"config reload not supported for jetstream storage dir\")\n\t\t}\n\t}\n\n\treturn diffOpts, nil\n}\n\nfunc copyRemoteGWConfigsWithoutTLSConfig(current []*RemoteGatewayOpts) []*RemoteGatewayOpts {\n\tl := len(current)\n\tif l == 0 {\n\t\treturn nil\n\t}\n\trgws := make([]*RemoteGatewayOpts, 0, l)\n\tfor _, rcfg := range current {\n\t\tcp := *rcfg\n\t\tcp.TLSConfig = nil\n\t\tcp.tlsConfigOpts = nil\n\t\trgws = append(rgws, &cp)\n\t}\n\treturn rgws\n}\n\nfunc copyRemoteLNConfigForReloadCompare(current []*RemoteLeafOpts) []*RemoteLeafOpts {\n\tl := len(current)\n\tif l == 0 {\n\t\treturn nil\n\t}\n\trlns := make([]*RemoteLeafOpts, 0, l)\n\tfor _, rcfg := range current {\n\t\tcp := *rcfg\n\t\tcp.TLSConfig = nil\n\t\tcp.tlsConfigOpts = nil\n\t\tcp.TLSHandshakeFirst = false\n\t\t// This is set only when processing a CONNECT, so reset here so that we\n\t\t// don't fail the DeepEqual comparison.\n\t\tcp.TLS = false\n\t\t// For now, remove DenyImports/Exports since those get modified at runtime\n\t\t// to add JS APIs.\n\t\tcp.DenyImports, cp.DenyExports = nil, nil\n\t\t// Remove compression mode\n\t\tcp.Compression = CompressionOpts{}\n\t\trlns = append(rlns, &cp)\n\t}\n\treturn rlns\n}\n\nfunc (s *Server) applyOptions(ctx *reloadContext, opts []option) {\n\tvar (\n\t\treloadLogging      = false\n\t\treloadAuth         = false\n\t\treloadClusterPerms = false\n\t\treloadClientTrcLvl = false\n\t\treloadJetstream    = false\n\t\tjsEnabled          = false\n\t\tisStatszChange     = false\n\t\tco                 *clusterOption\n\t)\n\tfor _, opt := range opts {\n\t\topt.Apply(s)\n\t\tif opt.IsLoggingChange() {\n\t\t\treloadLogging = true\n\t\t}\n\t\tif opt.IsTraceLevelChange() {\n\t\t\treloadClientTrcLvl = true\n\t\t}\n\t\tif opt.IsAuthChange() {\n\t\t\treloadAuth = true\n\t\t}\n\t\tif opt.IsClusterPoolSizeOrAccountsChange() {\n\t\t\tco = opt.(*clusterOption)\n\t\t}\n\t\tif opt.IsClusterPermsChange() {\n\t\t\treloadClusterPerms = true\n\t\t}\n\t\tif opt.IsJetStreamChange() {\n\t\t\treloadJetstream = true\n\t\t\tjsEnabled = opt.(*jetStreamOption).newValue\n\t\t}\n\t\tif opt.IsStatszChange() {\n\t\t\tisStatszChange = true\n\t\t}\n\t}\n\n\tif reloadLogging {\n\t\ts.ConfigureLogger()\n\t}\n\tif reloadClientTrcLvl {\n\t\ts.reloadClientTraceLevel()\n\t}\n\tif reloadAuth {\n\t\ts.reloadAuthorization()\n\t}\n\tif reloadClusterPerms {\n\t\ts.reloadClusterPermissions(ctx.oldClusterPerms)\n\t}\n\tnewOpts := s.getOpts()\n\t// If we need to reload cluster pool/per-account, then co will be not nil\n\tif co != nil {\n\t\ts.reloadClusterPoolAndAccounts(co, newOpts)\n\t}\n\tif reloadJetstream {\n\t\tif !jsEnabled {\n\t\t\ts.DisableJetStream()\n\t\t} else if !s.JetStreamEnabled() {\n\t\t\tif err := s.restartJetStream(); err != nil {\n\t\t\t\ts.Warnf(\"Can't start JetStream: %v\", err)\n\t\t\t}\n\t\t}\n\t\t// Make sure to reset the internal loop's version of JS.\n\t\ts.resetInternalLoopInfo()\n\t}\n\tif isStatszChange {\n\t\ts.sendStatszUpdate()\n\t}\n\n\t// For remote gateways and leafnodes, make sure that their TLS configuration\n\t// is updated (since the config is \"captured\" early and changes would otherwise\n\t// not be visible).\n\tif s.gateway.enabled {\n\t\ts.gateway.updateRemotesTLSConfig(newOpts)\n\t}\n\tif len(newOpts.LeafNode.Remotes) > 0 {\n\t\ts.updateRemoteLeafNodesTLSConfig(newOpts)\n\t}\n\n\t// Always restart OCSP monitoring on reload.\n\tif err := s.reloadOCSP(); err != nil {\n\t\ts.Warnf(\"Can't restart OCSP features: %v\", err)\n\t}\n\tvar cd string\n\tif newOpts.configDigest != \"\" {\n\t\tcd = fmt.Sprintf(\"(%s)\", newOpts.configDigest)\n\t}\n\ts.Noticef(\"Reloaded server configuration %s\", cd)\n}\n\n// This will send a reset to the internal send loop.\nfunc (s *Server) resetInternalLoopInfo() {\n\tvar resetCh chan struct{}\n\ts.mu.Lock()\n\tif s.sys != nil {\n\t\t// can't hold the lock as go routine reading it may be waiting for lock as well\n\t\tresetCh = s.sys.resetCh\n\t}\n\ts.mu.Unlock()\n\n\tif resetCh != nil {\n\t\tresetCh <- struct{}{}\n\t}\n}\n\n// Update all cached debug and trace settings for every client\nfunc (s *Server) reloadClientTraceLevel() {\n\topts := s.getOpts()\n\n\tif opts.NoLog {\n\t\treturn\n\t}\n\n\t// Create a list of all clients.\n\t// Update their trace level when not holding server or gateway lock\n\n\ts.mu.Lock()\n\tclientCnt := 1 + len(s.clients) + len(s.grTmpClients) + s.numRoutes() + len(s.leafs)\n\ts.mu.Unlock()\n\n\ts.gateway.RLock()\n\tclientCnt += len(s.gateway.in) + len(s.gateway.outo)\n\ts.gateway.RUnlock()\n\n\tclients := make([]*client, 0, clientCnt)\n\n\ts.mu.Lock()\n\tif s.eventsEnabled() {\n\t\tclients = append(clients, s.sys.client)\n\t}\n\n\tcMaps := []map[uint64]*client{s.clients, s.grTmpClients, s.leafs}\n\tfor _, m := range cMaps {\n\t\tfor _, c := range m {\n\t\t\tclients = append(clients, c)\n\t\t}\n\t}\n\ts.forEachRoute(func(c *client) {\n\t\tclients = append(clients, c)\n\t})\n\ts.mu.Unlock()\n\n\ts.gateway.RLock()\n\tfor _, c := range s.gateway.in {\n\t\tclients = append(clients, c)\n\t}\n\tclients = append(clients, s.gateway.outo...)\n\ts.gateway.RUnlock()\n\n\tfor _, c := range clients {\n\t\t// client.trace is commonly read while holding the lock\n\t\tc.mu.Lock()\n\t\tc.setTraceLevel()\n\t\tc.mu.Unlock()\n\t}\n}\n\n// reloadAuthorization reconfigures the server authorization settings,\n// disconnects any clients who are no longer authorized, and removes any\n// unauthorized subscriptions.\nfunc (s *Server) reloadAuthorization() {\n\t// This map will contain the names of accounts that have their streams\n\t// import configuration changed.\n\tvar awcsti map[string]struct{}\n\tcheckJetStream := false\n\topts := s.getOpts()\n\ts.mu.Lock()\n\n\tdeletedAccounts := make(map[string]*Account)\n\n\t// This can not be changed for now so ok to check server's trustedKeys unlocked.\n\t// If plain configured accounts, process here.\n\tif s.trustedKeys == nil {\n\t\t// Make a map of the configured account names so we figure out the accounts\n\t\t// that should be removed later on.\n\t\tconfigAccs := make(map[string]struct{}, len(opts.Accounts))\n\t\tfor _, acc := range opts.Accounts {\n\t\t\tconfigAccs[acc.GetName()] = struct{}{}\n\t\t}\n\t\t// Now range over existing accounts and keep track of the ones deleted\n\t\t// so some cleanup can be made after releasing the server lock.\n\t\ts.accounts.Range(func(k, v any) bool {\n\t\t\tan, acc := k.(string), v.(*Account)\n\t\t\t// Exclude default and system account from this test since those\n\t\t\t// may not actually be in opts.Accounts.\n\t\t\tif an == DEFAULT_GLOBAL_ACCOUNT || an == DEFAULT_SYSTEM_ACCOUNT {\n\t\t\t\treturn true\n\t\t\t}\n\t\t\t// Check check if existing account is still in opts.Accounts.\n\t\t\tif _, ok := configAccs[an]; !ok {\n\t\t\t\tdeletedAccounts[an] = acc\n\t\t\t\ts.accounts.Delete(k)\n\t\t\t}\n\t\t\treturn true\n\t\t})\n\t\t// This will update existing and add new ones.\n\t\tawcsti, _ = s.configureAccounts(true)\n\t\ts.configureAuthorization()\n\t\t// Double check any JetStream configs.\n\t\tcheckJetStream = s.getJetStream() != nil\n\t} else if opts.AccountResolver != nil {\n\t\ts.configureResolver()\n\t\tif _, ok := s.accResolver.(*MemAccResolver); ok {\n\t\t\t// Check preloads so we can issue warnings etc if needed.\n\t\t\ts.checkResolvePreloads()\n\t\t\t// With a memory resolver we want to do something similar to configured accounts.\n\t\t\t// We will walk the accounts and delete them if they are no longer present via fetch.\n\t\t\t// If they are present we will force a claim update to process changes.\n\t\t\ts.accounts.Range(func(k, v any) bool {\n\t\t\t\tacc := v.(*Account)\n\t\t\t\t// Skip global account.\n\t\t\t\tif acc == s.gacc {\n\t\t\t\t\treturn true\n\t\t\t\t}\n\t\t\t\taccName := acc.GetName()\n\t\t\t\t// Release server lock for following actions\n\t\t\t\ts.mu.Unlock()\n\t\t\t\taccClaims, claimJWT, _ := s.fetchAccountClaims(accName)\n\t\t\t\tif accClaims != nil {\n\t\t\t\t\tif err := s.updateAccountWithClaimJWT(acc, claimJWT); err != nil {\n\t\t\t\t\t\ts.Noticef(\"Reloaded: deleting account [bad claims]: %q\", accName)\n\t\t\t\t\t\ts.accounts.Delete(k)\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\ts.Noticef(\"Reloaded: deleting account [removed]: %q\", accName)\n\t\t\t\t\ts.accounts.Delete(k)\n\t\t\t\t}\n\t\t\t\t// Regrab server lock.\n\t\t\t\ts.mu.Lock()\n\t\t\t\treturn true\n\t\t\t})\n\t\t}\n\t}\n\n\tvar (\n\t\tcclientsa [64]*client\n\t\tcclients  = cclientsa[:0]\n\t\tclientsa  [64]*client\n\t\tclients   = clientsa[:0]\n\t\troutesa   [64]*client\n\t\troutes    = routesa[:0]\n\t)\n\n\t// Gather clients that changed accounts. We will close them and they\n\t// will reconnect, doing the right thing.\n\tfor _, client := range s.clients {\n\t\tif s.clientHasMovedToDifferentAccount(client) {\n\t\t\tcclients = append(cclients, client)\n\t\t} else {\n\t\t\tclients = append(clients, client)\n\t\t}\n\t}\n\ts.forEachRoute(func(route *client) {\n\t\troutes = append(routes, route)\n\t})\n\t// Check here for any system/internal clients which will not be in the servers map of normal clients.\n\tif s.sys != nil && s.sys.account != nil && !opts.NoSystemAccount {\n\t\ts.accounts.Store(s.sys.account.Name, s.sys.account)\n\t}\n\n\ts.accounts.Range(func(k, v any) bool {\n\t\tacc := v.(*Account)\n\t\tacc.mu.RLock()\n\t\t// Check for sysclients accounting, ignore the system account.\n\t\tif acc.sysclients > 0 && (s.sys == nil || s.sys.account != acc) {\n\t\t\tfor c := range acc.clients {\n\t\t\t\tif c.kind != CLIENT && c.kind != LEAF {\n\t\t\t\t\tclients = append(clients, c)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tacc.mu.RUnlock()\n\t\treturn true\n\t})\n\n\tvar resetCh chan struct{}\n\tif s.sys != nil {\n\t\t// can't hold the lock as go routine reading it may be waiting for lock as well\n\t\tresetCh = s.sys.resetCh\n\t}\n\ts.mu.Unlock()\n\n\t// Clear some timers and remove service import subs for deleted accounts.\n\tfor _, acc := range deletedAccounts {\n\t\tacc.mu.Lock()\n\t\tclearTimer(&acc.etmr)\n\t\tclearTimer(&acc.ctmr)\n\t\tfor _, se := range acc.exports.services {\n\t\t\tse.clearResponseThresholdTimer()\n\t\t}\n\t\tacc.mu.Unlock()\n\t\tacc.removeAllServiceImportSubs()\n\t}\n\n\tif resetCh != nil {\n\t\tresetCh <- struct{}{}\n\t}\n\n\t// Check that publish retained messages sources are still allowed to publish.\n\ts.mqttCheckPubRetainedPerms()\n\n\t// Close clients that have moved accounts\n\tfor _, client := range cclients {\n\t\tclient.closeConnection(ClientClosed)\n\t}\n\n\tfor _, c := range clients {\n\t\t// Disconnect any unauthorized clients.\n\t\t// Ignore internal clients.\n\t\tif (c.kind == CLIENT || c.kind == LEAF) && !s.isClientAuthorized(c) {\n\t\t\tc.authViolation()\n\t\t\tcontinue\n\t\t}\n\t\t// Check to make sure account is correct.\n\t\tc.swapAccountAfterReload()\n\t\t// Remove any unauthorized subscriptions and check for account imports.\n\t\tc.processSubsOnConfigReload(awcsti)\n\t}\n\n\tfor _, route := range routes {\n\t\t// Disconnect any unauthorized routes.\n\t\t// Do this only for routes that were accepted, not initiated\n\t\t// because in the later case, we don't have the user name/password\n\t\t// of the remote server.\n\t\tif !route.isSolicitedRoute() && !s.isRouterAuthorized(route) {\n\t\t\troute.setNoReconnect()\n\t\t\troute.authViolation()\n\t\t}\n\t}\n\n\tif res := s.AccountResolver(); res != nil {\n\t\tres.Reload()\n\t}\n\n\t// We will double check all JetStream configs on a reload.\n\tif checkJetStream {\n\t\tif err := s.enableJetStreamAccounts(); err != nil {\n\t\t\ts.Errorf(err.Error())\n\t\t}\n\t}\n}\n\n// Returns true if given client current account has changed (or user\n// no longer exist) in the new config, false if the user did not\n// change accounts.\n// Server lock is held on entry.\nfunc (s *Server) clientHasMovedToDifferentAccount(c *client) bool {\n\tvar (\n\t\tnu *NkeyUser\n\t\tu  *User\n\t)\n\tc.mu.Lock()\n\tdefer c.mu.Unlock()\n\tif c.opts.Nkey != _EMPTY_ {\n\t\tif s.nkeys != nil {\n\t\t\tnu = s.nkeys[c.opts.Nkey]\n\t\t}\n\t} else if c.opts.Username != _EMPTY_ {\n\t\tif s.users != nil {\n\t\t\tu = s.users[c.opts.Username]\n\t\t}\n\t} else {\n\t\treturn false\n\t}\n\t// Get the current account name\n\tvar curAccName string\n\tif c.acc != nil {\n\t\tcurAccName = c.acc.Name\n\t}\n\tif nu != nil && nu.Account != nil {\n\t\treturn curAccName != nu.Account.Name\n\t} else if u != nil && u.Account != nil {\n\t\treturn curAccName != u.Account.Name\n\t}\n\t// user/nkey no longer exists.\n\treturn true\n}\n\n// reloadClusterPermissions reconfigures the cluster's permssions\n// and set the permissions to all existing routes, sending an\n// update INFO protocol so that remote can resend their local\n// subs if needed, and sending local subs matching cluster's\n// import subjects.\nfunc (s *Server) reloadClusterPermissions(oldPerms *RoutePermissions) {\n\ts.mu.Lock()\n\tnewPerms := s.getOpts().Cluster.Permissions\n\troutes := make(map[uint64]*client, s.numRoutes())\n\t// Get all connected routes\n\ts.forEachRoute(func(route *client) {\n\t\troute.mu.Lock()\n\t\troutes[route.cid] = route\n\t\troute.mu.Unlock()\n\t})\n\t// If new permissions is nil, then clear routeInfo import/export\n\tif newPerms == nil {\n\t\ts.routeInfo.Import = nil\n\t\ts.routeInfo.Export = nil\n\t} else {\n\t\ts.routeInfo.Import = newPerms.Import\n\t\ts.routeInfo.Export = newPerms.Export\n\t}\n\tinfoJSON := generateInfoJSON(&s.routeInfo)\n\ts.mu.Unlock()\n\n\t// Close connections for routes that don't understand async INFO.\n\tfor _, route := range routes {\n\t\troute.mu.Lock()\n\t\tclose := route.opts.Protocol < RouteProtoInfo\n\t\tcid := route.cid\n\t\troute.mu.Unlock()\n\t\tif close {\n\t\t\troute.closeConnection(RouteRemoved)\n\t\t\tdelete(routes, cid)\n\t\t}\n\t}\n\n\t// If there are no route left, we are done\n\tif len(routes) == 0 {\n\t\treturn\n\t}\n\n\t// Fake clients to test cluster permissions\n\toldPermsTester := &client{}\n\toldPermsTester.setRoutePermissions(oldPerms)\n\tnewPermsTester := &client{}\n\tnewPermsTester.setRoutePermissions(newPerms)\n\n\tvar (\n\t\t_localSubs       [4096]*subscription\n\t\tsubsNeedSUB      = map[*client][]*subscription{}\n\t\tsubsNeedUNSUB    = map[*client][]*subscription{}\n\t\tdeleteRoutedSubs []*subscription\n\t)\n\n\tgetRouteForAccount := func(accName string, poolIdx int) *client {\n\t\tfor _, r := range routes {\n\t\t\tr.mu.Lock()\n\t\t\tok := (poolIdx >= 0 && poolIdx == r.route.poolIdx) || (string(r.route.accName) == accName) || r.route.noPool\n\t\t\tr.mu.Unlock()\n\t\t\tif ok {\n\t\t\t\treturn r\n\t\t\t}\n\t\t}\n\t\treturn nil\n\t}\n\n\t// First set the new permissions on all routes.\n\tfor _, route := range routes {\n\t\troute.mu.Lock()\n\t\troute.setRoutePermissions(newPerms)\n\t\troute.mu.Unlock()\n\t}\n\n\t// Then, go over all accounts and gather local subscriptions that need to be\n\t// sent over as SUB or removed as UNSUB, and routed subscriptions that need\n\t// to be dropped due to export permissions.\n\ts.accounts.Range(func(_, v any) bool {\n\t\tacc := v.(*Account)\n\t\tacc.mu.RLock()\n\t\taccName, sl, poolIdx := acc.Name, acc.sl, acc.routePoolIdx\n\t\tacc.mu.RUnlock()\n\t\t// Get the route handling this account. If no route or sublist, bail out.\n\t\troute := getRouteForAccount(accName, poolIdx)\n\t\tif route == nil || sl == nil {\n\t\t\treturn true\n\t\t}\n\t\tlocalSubs := _localSubs[:0]\n\t\tsl.localSubs(&localSubs, false)\n\n\t\t// Go through all local subscriptions\n\t\tfor _, sub := range localSubs {\n\t\t\t// Get all subs that can now be imported\n\t\t\tsubj := string(sub.subject)\n\t\t\tcouldImportThen := oldPermsTester.canImport(subj)\n\t\t\tcanImportNow := newPermsTester.canImport(subj)\n\t\t\tif canImportNow {\n\t\t\t\t// If we could not before, then will need to send a SUB protocol.\n\t\t\t\tif !couldImportThen {\n\t\t\t\t\tsubsNeedSUB[route] = append(subsNeedSUB[route], sub)\n\t\t\t\t}\n\t\t\t} else if couldImportThen {\n\t\t\t\t// We were previously able to import this sub, but now\n\t\t\t\t// we can't so we need to send an UNSUB protocol\n\t\t\t\tsubsNeedUNSUB[route] = append(subsNeedUNSUB[route], sub)\n\t\t\t}\n\t\t}\n\t\tdeleteRoutedSubs = deleteRoutedSubs[:0]\n\t\troute.mu.Lock()\n\t\tpa, _, hasSubType := route.getRoutedSubKeyInfo()\n\t\tfor key, sub := range route.subs {\n\t\t\t// If this is not a pinned-account route, we need to get the\n\t\t\t// account name from the key to see if we collect this sub.\n\t\t\tif !pa {\n\t\t\t\tif an := getAccNameFromRoutedSubKey(sub, key, hasSubType); an != accName {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t}\n\t\t\t// If we can't export, we need to drop the subscriptions that\n\t\t\t// we have on behalf of this route.\n\t\t\t// Need to make a string cast here since canExport call sl.Match()\n\t\t\tsubj := string(sub.subject)\n\t\t\tif !route.canExport(subj) {\n\t\t\t\t// We can use bytesToString() here.\n\t\t\t\tdelete(route.subs, bytesToString(sub.sid))\n\t\t\t\tdeleteRoutedSubs = append(deleteRoutedSubs, sub)\n\t\t\t}\n\t\t}\n\t\troute.mu.Unlock()\n\t\t// Remove as a batch all the subs that we have removed from each route.\n\t\tsl.RemoveBatch(deleteRoutedSubs)\n\t\treturn true\n\t})\n\n\t// Send an update INFO, which will allow remote server to show\n\t// our current route config in monitoring and resend subscriptions\n\t// that we now possibly allow with a change of Export permissions.\n\tfor _, route := range routes {\n\t\troute.mu.Lock()\n\t\troute.enqueueProto(infoJSON)\n\t\t// Now send SUB and UNSUB protocols as needed.\n\t\tif subs, ok := subsNeedSUB[route]; ok && len(subs) > 0 {\n\t\t\troute.sendRouteSubProtos(subs, false, nil)\n\t\t}\n\t\tif unsubs, ok := subsNeedUNSUB[route]; ok && len(unsubs) > 0 {\n\t\t\troute.sendRouteUnSubProtos(unsubs, false, nil)\n\t\t}\n\t\troute.mu.Unlock()\n\t}\n}\n\nfunc (s *Server) reloadClusterPoolAndAccounts(co *clusterOption, opts *Options) {\n\ts.mu.Lock()\n\t// Prevent adding new routes until we are ready to do so.\n\ts.routesReject = true\n\tvar ch chan struct{}\n\t// For accounts that have been added to the list of dedicated routes,\n\t// send a protocol to their current assigned routes to allow the\n\t// other side to prepare for the changes.\n\tif len(co.accsAdded) > 0 {\n\t\tprotosSent := 0\n\t\ts.accAddedReqID = nuid.Next()\n\t\tfor _, an := range co.accsAdded {\n\t\t\tif s.accRoutes == nil {\n\t\t\t\ts.accRoutes = make(map[string]map[string]*client)\n\t\t\t}\n\t\t\t// In case a config reload was first done on another server,\n\t\t\t// we may have already switched this account to a dedicated route.\n\t\t\t// But we still want to send the protocol over the routes that\n\t\t\t// would have otherwise handled it.\n\t\t\tif _, ok := s.accRoutes[an]; !ok {\n\t\t\t\ts.accRoutes[an] = make(map[string]*client)\n\t\t\t}\n\t\t\tif a, ok := s.accounts.Load(an); ok {\n\t\t\t\tacc := a.(*Account)\n\t\t\t\tacc.mu.Lock()\n\t\t\t\tsl := acc.sl\n\t\t\t\t// Get the current route pool index before calling setRouteInfo.\n\t\t\t\trpi := acc.routePoolIdx\n\t\t\t\t// Switch to per-account route if not already done.\n\t\t\t\tif rpi >= 0 {\n\t\t\t\t\ts.setRouteInfo(acc)\n\t\t\t\t} else {\n\t\t\t\t\t// If it was transitioning, make sure we set it to the state\n\t\t\t\t\t// that indicates that it has a dedicated route\n\t\t\t\t\tif rpi == accTransitioningToDedicatedRoute {\n\t\t\t\t\t\tacc.routePoolIdx = accDedicatedRoute\n\t\t\t\t\t}\n\t\t\t\t\t// Otherwise get the route pool index it would have been before\n\t\t\t\t\t// the move so we can send the protocol to those routes.\n\t\t\t\t\trpi = computeRoutePoolIdx(s.routesPoolSize, acc.Name)\n\t\t\t\t}\n\t\t\t\tacc.mu.Unlock()\n\t\t\t\t// Generate the INFO protocol to send indicating that this account\n\t\t\t\t// is being moved to a dedicated route.\n\t\t\t\tri := Info{\n\t\t\t\t\tRoutePoolSize: s.routesPoolSize,\n\t\t\t\t\tRouteAccount:  an,\n\t\t\t\t\tRouteAccReqID: s.accAddedReqID,\n\t\t\t\t}\n\t\t\t\tproto := generateInfoJSON(&ri)\n\t\t\t\t// Since v2.11.0, we support remotes with a different pool size\n\t\t\t\t// (for rolling upgrades), so we need to use the remote route\n\t\t\t\t// pool index (based on the remote configured pool size) since\n\t\t\t\t// the remote subscriptions will be attached to the route at\n\t\t\t\t// that index, not at our account's route pool index. However,\n\t\t\t\t// we are going to send the protocol through the route that\n\t\t\t\t// handles this account from our pool size perspective (that\n\t\t\t\t// would be the route at index `rpi`).\n\t\t\t\tremoveSubsAndSendProto := func(r *client, doSubs, doProto bool) {\n\t\t\t\t\tr.mu.Lock()\n\t\t\t\t\tdefer r.mu.Unlock()\n\t\t\t\t\t// Exclude routes to servers that don't support pooling.\n\t\t\t\t\tif r.route.noPool {\n\t\t\t\t\t\treturn\n\t\t\t\t\t}\n\t\t\t\t\tif doSubs {\n\t\t\t\t\t\tif subs := r.removeRemoteSubsForAcc(an); len(subs) > 0 {\n\t\t\t\t\t\t\tsl.RemoveBatch(subs)\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tif doProto {\n\t\t\t\t\t\tr.enqueueProto(proto)\n\t\t\t\t\t\tprotosSent++\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tfor remote, conns := range s.routes {\n\t\t\t\t\tr := conns[rpi]\n\t\t\t\t\t// The route connection at this index is currently not up,\n\t\t\t\t\t// so we won't be able to send the protocol, so move to the\n\t\t\t\t\t// next remote.\n\t\t\t\t\tif r == nil {\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\t\t\t\t\tdoSubs := true\n\t\t\t\t\t// Check the remote's route pool size and if different than\n\t\t\t\t\t// ours, remove the subs on that other route.\n\t\t\t\t\tremotePoolSize, ok := s.remoteRoutePoolSize[remote]\n\t\t\t\t\tif ok && remotePoolSize != s.routesPoolSize {\n\t\t\t\t\t\t// This is the remote's route pool index for this account\n\t\t\t\t\t\trrpi := computeRoutePoolIdx(remotePoolSize, an)\n\t\t\t\t\t\tif rr := conns[rrpi]; rr != nil {\n\t\t\t\t\t\t\tremoveSubsAndSendProto(rr, true, false)\n\t\t\t\t\t\t\t// Indicate that we have already remove the subs.\n\t\t\t\t\t\t\tdoSubs = false\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\t// Now send the protocol from the route that handles the\n\t\t\t\t\t// account from this server perspective.\n\t\t\t\t\tremoveSubsAndSendProto(r, doSubs, true)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tif protosSent > 0 {\n\t\t\ts.accAddedCh = make(chan struct{}, protosSent)\n\t\t\tch = s.accAddedCh\n\t\t}\n\t}\n\t// Collect routes that need to be closed.\n\troutes := make(map[*client]struct{})\n\t// Collect the per-account routes that need to be closed.\n\tif len(co.accsRemoved) > 0 {\n\t\tfor _, an := range co.accsRemoved {\n\t\t\tif remotes, ok := s.accRoutes[an]; ok && remotes != nil {\n\t\t\t\tfor _, r := range remotes {\n\t\t\t\t\tif r != nil {\n\t\t\t\t\t\tr.setNoReconnect()\n\t\t\t\t\t\troutes[r] = struct{}{}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\t// If the pool size has changed, we need to close all pooled routes.\n\tif co.poolSizeChanged {\n\t\ts.forEachNonPerAccountRoute(func(r *client) {\n\t\t\troutes[r] = struct{}{}\n\t\t})\n\t}\n\t// If there are routes to close, we need to release the server lock.\n\t// Same if we need to wait on responses from the remotes when\n\t// processing new per-account routes.\n\tif len(routes) > 0 || len(ch) > 0 {\n\t\ts.mu.Unlock()\n\n\t\tfor done := false; !done && len(ch) > 0; {\n\t\t\tselect {\n\t\t\tcase <-ch:\n\t\t\tcase <-time.After(2 * time.Second):\n\t\t\t\ts.Warnf(\"Timed out waiting for confirmation from all routes regarding per-account routes changes\")\n\t\t\t\tdone = true\n\t\t\t}\n\t\t}\n\n\t\tfor r := range routes {\n\t\t\tr.closeConnection(RouteRemoved)\n\t\t}\n\n\t\ts.mu.Lock()\n\t}\n\t// Clear the accAddedCh/ReqID fields in case they were set.\n\ts.accAddedReqID, s.accAddedCh = _EMPTY_, nil\n\t// Now that per-account routes that needed to be closed are closed,\n\t// remove them from s.accRoutes. Doing so before would prevent\n\t// removeRoute() to do proper cleanup because the route would not\n\t// be found in s.accRoutes.\n\tfor _, an := range co.accsRemoved {\n\t\tdelete(s.accRoutes, an)\n\t\t// Do not lookup and call setRouteInfo() on the accounts here.\n\t\t// We need first to set the new s.routesPoolSize value and\n\t\t// anyway, there is no need to do here if the pool size has\n\t\t// changed (since it will be called for all accounts).\n\t}\n\t// We have already added the accounts to s.accRoutes that needed to\n\t// be added.\n\n\t// We should always have at least the system account with a dedicated route,\n\t// but in case we have a configuration that disables pooling and without\n\t// a system account, possibly set the accRoutes to nil.\n\tif len(opts.Cluster.PinnedAccounts) == 0 {\n\t\ts.accRoutes = nil\n\t}\n\t// Now deal with pool size updates.\n\tif ps := opts.Cluster.PoolSize; ps > 0 {\n\t\ts.routesPoolSize = ps\n\t\ts.routeInfo.RoutePoolSize = ps\n\t} else {\n\t\ts.routesPoolSize = 1\n\t\ts.routeInfo.RoutePoolSize = 0\n\t}\n\t// If the pool size has changed, we need to recompute all accounts' route\n\t// pool index. Note that the added/removed accounts will be reset there\n\t// too, but that's ok (we could use a map to exclude them, but not worth it).\n\tif co.poolSizeChanged {\n\t\ts.accounts.Range(func(_, v any) bool {\n\t\t\tacc := v.(*Account)\n\t\t\tacc.mu.Lock()\n\t\t\ts.setRouteInfo(acc)\n\t\t\tacc.mu.Unlock()\n\t\t\treturn true\n\t\t})\n\t} else if len(co.accsRemoved) > 0 {\n\t\t// For accounts that no longer have a dedicated route, we need to send\n\t\t// the subsriptions on the existing pooled routes for those accounts.\n\t\tfor _, an := range co.accsRemoved {\n\t\t\tif a, ok := s.accounts.Load(an); ok {\n\t\t\t\tacc := a.(*Account)\n\t\t\t\tacc.mu.Lock()\n\t\t\t\t// First call this which will assign a new route pool index.\n\t\t\t\ts.setRouteInfo(acc)\n\t\t\t\t// Get the value so we can send the subscriptions interest\n\t\t\t\t// on all routes with this pool index.\n\t\t\t\trpi := acc.routePoolIdx\n\t\t\t\tacc.mu.Unlock()\n\t\t\t\ts.forEachRouteIdx(rpi, func(r *client) bool {\n\t\t\t\t\t// We have the guarantee that if the route exists, it\n\t\t\t\t\t// is not a new one that would have been created when\n\t\t\t\t\t// we released the server lock if some routes needed\n\t\t\t\t\t// to be closed, because we have set s.routesReject\n\t\t\t\t\t// to `true` at the top of this function.\n\t\t\t\t\ts.sendSubsToRoute(r, rpi, an)\n\t\t\t\t\treturn true\n\t\t\t\t})\n\t\t\t}\n\t\t}\n\t}\n\t// Allow routes to be accepted now.\n\ts.routesReject = false\n\t// If there is a pool size change or added accounts, solicit routes now.\n\tif co.poolSizeChanged || len(co.accsAdded) > 0 {\n\t\ts.solicitRoutes(opts.Routes, co.accsAdded)\n\t}\n\ts.mu.Unlock()\n}\n\n// validateClusterOpts ensures the new ClusterOpts does not change some of the\n// fields that do not support reload.\nfunc validateClusterOpts(old, new ClusterOpts) error {\n\tif old.Host != new.Host {\n\t\treturn fmt.Errorf(\"config reload not supported for cluster host: old=%s, new=%s\",\n\t\t\told.Host, new.Host)\n\t}\n\tif old.Port != new.Port {\n\t\treturn fmt.Errorf(\"config reload not supported for cluster port: old=%d, new=%d\",\n\t\t\told.Port, new.Port)\n\t}\n\t// Validate Cluster.Advertise syntax\n\tif new.Advertise != \"\" {\n\t\tif _, _, err := parseHostPort(new.Advertise, 0); err != nil {\n\t\t\treturn fmt.Errorf(\"invalid Cluster.Advertise value of %s, err=%v\", new.Advertise, err)\n\t\t}\n\t}\n\treturn nil\n}\n\n// diffRoutes diffs the old routes and the new routes and returns the ones that\n// should be added and removed from the server.\nfunc diffRoutes(old, new []*url.URL) (add, remove []*url.URL) {\n\t// Find routes to remove.\nremoveLoop:\n\tfor _, oldRoute := range old {\n\t\tfor _, newRoute := range new {\n\t\t\tif urlsAreEqual(oldRoute, newRoute) {\n\t\t\t\tcontinue removeLoop\n\t\t\t}\n\t\t}\n\t\tremove = append(remove, oldRoute)\n\t}\n\n\t// Find routes to add.\naddLoop:\n\tfor _, newRoute := range new {\n\t\tfor _, oldRoute := range old {\n\t\t\tif urlsAreEqual(oldRoute, newRoute) {\n\t\t\t\tcontinue addLoop\n\t\t\t}\n\t\t}\n\t\tadd = append(add, newRoute)\n\t}\n\n\treturn add, remove\n}\n",
    "source_file": "server/reload.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2020-2025 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage server\n\nimport (\n\t\"bytes\"\n\t\"cmp\"\n\tcrand \"crypto/rand\"\n\t\"encoding/binary\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"fmt\"\n\t\"math\"\n\t\"math/rand\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"reflect\"\n\t\"slices\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t\"github.com/klauspost/compress/s2\"\n\t\"github.com/minio/highwayhash\"\n\t\"github.com/nats-io/nuid\"\n)\n\n// jetStreamCluster holds information about the meta group and stream assignments.\ntype jetStreamCluster struct {\n\t// The metacontroller raftNode.\n\tmeta RaftNode\n\t// For stream and consumer assignments. All servers will have this be the same.\n\t// ACCOUNT -> STREAM -> Stream Assignment -> Consumers\n\tstreams map[string]map[string]*streamAssignment\n\t// These are inflight proposals and used to apply limits when there are\n\t// concurrent requests that would otherwise be accepted.\n\t// We also record the group for the stream. This is needed since if we have\n\t// concurrent requests for same account and stream we need to let it process to get\n\t// a response but they need to be same group, peers etc. and sync subjects.\n\tinflight map[string]map[string]*inflightInfo\n\t// Signals meta-leader should check the stream assignments.\n\tstreamsCheck bool\n\t// Server.\n\ts *Server\n\t// Internal client.\n\tc *client\n\t// Processing assignment results.\n\tstreamResults   *subscription\n\tconsumerResults *subscription\n\t// System level request to have the leader stepdown.\n\tstepdown *subscription\n\t// System level requests to remove a peer.\n\tpeerRemove *subscription\n\t// System level request to move a stream\n\tpeerStreamMove *subscription\n\t// System level request to cancel a stream move\n\tpeerStreamCancelMove *subscription\n\t// To pop out the monitorCluster before the raft layer.\n\tqch chan struct{}\n}\n\n// Used to track inflight stream add requests to properly re-use same group and sync subject.\ntype inflightInfo struct {\n\trg   *raftGroup\n\tsync string\n}\n\n// Used to guide placement of streams and meta controllers in clustered JetStream.\ntype Placement struct {\n\tCluster   string   `json:\"cluster,omitempty\"`\n\tTags      []string `json:\"tags,omitempty\"`\n\tPreferred string   `json:\"preferred,omitempty\"`\n}\n\n// Define types of the entry.\ntype entryOp uint8\n\n// ONLY ADD TO THE END, DO NOT INSERT IN BETWEEN WILL BREAK SERVER INTEROP.\nconst (\n\t// Meta ops.\n\tassignStreamOp entryOp = iota\n\tassignConsumerOp\n\tremoveStreamOp\n\tremoveConsumerOp\n\t// Stream ops.\n\tstreamMsgOp\n\tpurgeStreamOp\n\tdeleteMsgOp\n\t// Consumer ops.\n\tupdateDeliveredOp\n\tupdateAcksOp\n\t// Compressed consumer assignments.\n\tassignCompressedConsumerOp\n\t// Filtered Consumer skip.\n\tupdateSkipOp\n\t// Update Stream.\n\tupdateStreamOp\n\t// For updating information on pending pull requests.\n\taddPendingRequest\n\tremovePendingRequest\n\t// For sending compressed streams, either through RAFT or catchup.\n\tcompressedStreamMsgOp\n\t// For sending deleted gaps on catchups for replicas.\n\tdeleteRangeOp\n)\n\n// raftGroups are controlled by the metagroup controller.\n// The raftGroups will house streams and consumers.\ntype raftGroup struct {\n\tName      string      `json:\"name\"`\n\tPeers     []string    `json:\"peers\"`\n\tStorage   StorageType `json:\"store\"`\n\tCluster   string      `json:\"cluster,omitempty\"`\n\tPreferred string      `json:\"preferred,omitempty\"`\n\t// Internal\n\tnode RaftNode\n}\n\n// streamAssignment is what the meta controller uses to assign streams to peers.\ntype streamAssignment struct {\n\tClient  *ClientInfo   `json:\"client,omitempty\"`\n\tCreated time.Time     `json:\"created\"`\n\tConfig  *StreamConfig `json:\"stream\"`\n\tGroup   *raftGroup    `json:\"group\"`\n\tSync    string        `json:\"sync\"`\n\tSubject string        `json:\"subject,omitempty\"`\n\tReply   string        `json:\"reply,omitempty\"`\n\tRestore *StreamState  `json:\"restore_state,omitempty\"`\n\t// Internal\n\tconsumers   map[string]*consumerAssignment\n\tresponded   bool\n\trecovering  bool\n\treassigning bool // i.e. due to placement issues, lack of resources, etc.\n\tresetting   bool // i.e. there was an error, and we're stopping and starting the stream\n\terr         error\n}\n\n// consumerAssignment is what the meta controller uses to assign consumers to streams.\ntype consumerAssignment struct {\n\tClient  *ClientInfo     `json:\"client,omitempty\"`\n\tCreated time.Time       `json:\"created\"`\n\tName    string          `json:\"name\"`\n\tStream  string          `json:\"stream\"`\n\tConfig  *ConsumerConfig `json:\"consumer\"`\n\tGroup   *raftGroup      `json:\"group\"`\n\tSubject string          `json:\"subject,omitempty\"`\n\tReply   string          `json:\"reply,omitempty\"`\n\tState   *ConsumerState  `json:\"state,omitempty\"`\n\t// Internal\n\tresponded  bool\n\trecovering bool\n\tpending    bool\n\tdeleted    bool\n\terr        error\n}\n\n// streamPurge is what the stream leader will replicate when purging a stream.\ntype streamPurge struct {\n\tClient  *ClientInfo              `json:\"client,omitempty\"`\n\tStream  string                   `json:\"stream\"`\n\tLastSeq uint64                   `json:\"last_seq\"`\n\tSubject string                   `json:\"subject\"`\n\tReply   string                   `json:\"reply\"`\n\tRequest *JSApiStreamPurgeRequest `json:\"request,omitempty\"`\n}\n\n// streamMsgDelete is what the stream leader will replicate when deleting a message.\ntype streamMsgDelete struct {\n\tClient  *ClientInfo `json:\"client,omitempty\"`\n\tStream  string      `json:\"stream\"`\n\tSeq     uint64      `json:\"seq\"`\n\tNoErase bool        `json:\"no_erase,omitempty\"`\n\tSubject string      `json:\"subject\"`\n\tReply   string      `json:\"reply\"`\n}\n\nconst (\n\tdefaultStoreDirName  = \"_js_\"\n\tdefaultMetaGroupName = \"_meta_\"\n\tdefaultMetaFSBlkSize = 1024 * 1024\n\tjsExcludePlacement   = \"!jetstream\"\n)\n\n// Returns information useful in mixed mode.\nfunc (s *Server) trackedJetStreamServers() (js, total int) {\n\ts.mu.RLock()\n\tdefer s.mu.RUnlock()\n\tif !s.isRunning() || !s.eventsEnabled() {\n\t\treturn -1, -1\n\t}\n\ts.nodeToInfo.Range(func(k, v any) bool {\n\t\tsi := v.(nodeInfo)\n\t\tif si.js {\n\t\t\tjs++\n\t\t}\n\t\ttotal++\n\t\treturn true\n\t})\n\treturn js, total\n}\n\nfunc (s *Server) getJetStreamCluster() (*jetStream, *jetStreamCluster) {\n\tif s.isShuttingDown() {\n\t\treturn nil, nil\n\t}\n\n\tjs := s.getJetStream()\n\tif js == nil {\n\t\treturn nil, nil\n\t}\n\n\t// Only set once, do not need a lock.\n\treturn js, js.cluster\n}\n\nfunc (s *Server) JetStreamIsClustered() bool {\n\treturn s.jsClustered.Load()\n}\n\nfunc (s *Server) JetStreamIsLeader() bool {\n\treturn s.isMetaLeader.Load()\n}\n\nfunc (s *Server) JetStreamIsCurrent() bool {\n\tjs := s.getJetStream()\n\tif js == nil {\n\t\treturn false\n\t}\n\t// Grab what we need and release js lock.\n\tjs.mu.RLock()\n\tvar meta RaftNode\n\tcc := js.cluster\n\tif cc != nil {\n\t\tmeta = cc.meta\n\t}\n\tjs.mu.RUnlock()\n\n\tif cc == nil {\n\t\t// Non-clustered mode\n\t\treturn true\n\t}\n\treturn meta.Current()\n}\n\nfunc (s *Server) JetStreamSnapshotMeta() error {\n\tjs := s.getJetStream()\n\tif js == nil {\n\t\treturn NewJSNotEnabledError()\n\t}\n\tjs.mu.RLock()\n\tcc := js.cluster\n\tisLeader := cc.isLeader()\n\tmeta := cc.meta\n\tjs.mu.RUnlock()\n\n\tif !isLeader {\n\t\treturn errNotLeader\n\t}\n\n\tsnap, err := js.metaSnapshot()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\treturn meta.InstallSnapshot(snap)\n}\n\nfunc (s *Server) JetStreamStepdownStream(account, stream string) error {\n\tjs, cc := s.getJetStreamCluster()\n\tif js == nil {\n\t\treturn NewJSNotEnabledError()\n\t}\n\tif cc == nil {\n\t\treturn NewJSClusterNotActiveError()\n\t}\n\t// Grab account\n\tacc, err := s.LookupAccount(account)\n\tif err != nil {\n\t\treturn err\n\t}\n\t// Grab stream\n\tmset, err := acc.lookupStream(stream)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif node := mset.raftNode(); node != nil {\n\t\tnode.StepDown()\n\t}\n\n\treturn nil\n}\n\nfunc (s *Server) JetStreamStepdownConsumer(account, stream, consumer string) error {\n\tjs, cc := s.getJetStreamCluster()\n\tif js == nil {\n\t\treturn NewJSNotEnabledError()\n\t}\n\tif cc == nil {\n\t\treturn NewJSClusterNotActiveError()\n\t}\n\t// Grab account\n\tacc, err := s.LookupAccount(account)\n\tif err != nil {\n\t\treturn err\n\t}\n\t// Grab stream\n\tmset, err := acc.lookupStream(stream)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\to := mset.lookupConsumer(consumer)\n\tif o == nil {\n\t\treturn NewJSConsumerNotFoundError()\n\t}\n\n\tif node := o.raftNode(); node != nil {\n\t\tnode.StepDown()\n\t}\n\n\treturn nil\n}\n\nfunc (s *Server) JetStreamSnapshotStream(account, stream string) error {\n\tjs, cc := s.getJetStreamCluster()\n\tif js == nil {\n\t\treturn NewJSNotEnabledForAccountError()\n\t}\n\tif cc == nil {\n\t\treturn NewJSClusterNotActiveError()\n\t}\n\t// Grab account\n\tacc, err := s.LookupAccount(account)\n\tif err != nil {\n\t\treturn err\n\t}\n\t// Grab stream\n\tmset, err := acc.lookupStream(stream)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Hold lock when installing snapshot.\n\tmset.mu.Lock()\n\tif mset.node == nil {\n\t\tmset.mu.Unlock()\n\t\treturn nil\n\t}\n\terr = mset.node.InstallSnapshot(mset.stateSnapshotLocked())\n\tmset.mu.Unlock()\n\n\treturn err\n}\n\nfunc (s *Server) JetStreamClusterPeers() []string {\n\tjs := s.getJetStream()\n\tif js == nil {\n\t\treturn nil\n\t}\n\tjs.mu.RLock()\n\tdefer js.mu.RUnlock()\n\n\tcc := js.cluster\n\tif !cc.isLeader() || cc.meta == nil {\n\t\treturn nil\n\t}\n\tpeers := cc.meta.Peers()\n\tvar nodes []string\n\tfor _, p := range peers {\n\t\tsi, ok := s.nodeToInfo.Load(p.ID)\n\t\tif !ok || si == nil {\n\t\t\tcontinue\n\t\t}\n\t\tni := si.(nodeInfo)\n\t\t// Ignore if offline, no JS, or no current stats have been received.\n\t\tif ni.offline || !ni.js || ni.stats == nil {\n\t\t\tcontinue\n\t\t}\n\t\tnodes = append(nodes, si.(nodeInfo).name)\n\t}\n\treturn nodes\n}\n\n// Read lock should be held.\nfunc (cc *jetStreamCluster) isLeader() bool {\n\tif cc == nil {\n\t\t// Non-clustered mode\n\t\treturn true\n\t}\n\treturn cc.meta != nil && cc.meta.Leader()\n}\n\n// isStreamCurrent will determine if the stream is up to date.\n// For R1 it will make sure the stream is present on this server.\n// Read lock should be held.\nfunc (cc *jetStreamCluster) isStreamCurrent(account, stream string) bool {\n\tif cc == nil {\n\t\t// Non-clustered mode\n\t\treturn true\n\t}\n\tas := cc.streams[account]\n\tif as == nil {\n\t\treturn false\n\t}\n\tsa := as[stream]\n\tif sa == nil {\n\t\treturn false\n\t}\n\trg := sa.Group\n\tif rg == nil {\n\t\treturn false\n\t}\n\n\tif rg.node == nil || rg.node.Current() {\n\t\t// Check if we are processing a snapshot and are catching up.\n\t\tacc, err := cc.s.LookupAccount(account)\n\t\tif err != nil {\n\t\t\treturn false\n\t\t}\n\t\tmset, err := acc.lookupStream(stream)\n\t\tif err != nil {\n\t\t\treturn false\n\t\t}\n\t\tif mset.isCatchingUp() {\n\t\t\treturn false\n\t\t}\n\t\t// Success.\n\t\treturn true\n\t}\n\n\treturn false\n}\n\n// isStreamHealthy will determine if the stream is up to date or very close.\n// For R1 it will make sure the stream is present on this server.\nfunc (js *jetStream) isStreamHealthy(acc *Account, sa *streamAssignment) error {\n\tjs.mu.RLock()\n\ts, cc := js.srv, js.cluster\n\tif cc == nil {\n\t\t// Non-clustered mode\n\t\tjs.mu.RUnlock()\n\t\treturn nil\n\t}\n\tif sa == nil || sa.Group == nil {\n\t\tjs.mu.RUnlock()\n\t\treturn errors.New(\"stream assignment or group missing\")\n\t}\n\tstreamName := sa.Config.Name\n\tnode := sa.Group.node\n\tjs.mu.RUnlock()\n\n\t// First lookup stream and make sure its there.\n\tmset, err := acc.lookupStream(streamName)\n\tif err != nil {\n\t\treturn errors.New(\"stream not found\")\n\t}\n\n\tmsetNode := mset.raftNode()\n\tswitch {\n\tcase mset.cfg.Replicas <= 1:\n\t\treturn nil // No further checks for R=1 streams\n\n\tcase node == nil:\n\t\treturn errors.New(\"group node missing\")\n\n\tcase msetNode == nil:\n\t\t// Can happen when the stream's node is not yet initialized.\n\t\treturn errors.New(\"stream node missing\")\n\n\tcase node != msetNode:\n\t\ts.Warnf(\"Detected stream cluster node skew '%s > %s'\", acc.GetName(), streamName)\n\t\tnode.Delete()\n\t\tmset.resetClusteredState(nil)\n\t\treturn errors.New(\"cluster node skew detected\")\n\n\tcase !mset.isMonitorRunning():\n\t\treturn errors.New(\"monitor goroutine not running\")\n\n\tcase !node.Healthy():\n\t\treturn errors.New(\"group node unhealthy\")\n\n\tcase mset.isCatchingUp():\n\t\treturn errors.New(\"stream catching up\")\n\n\tdefault:\n\t\treturn nil\n\t}\n}\n\n// isConsumerHealthy will determine if the consumer is up to date.\n// For R1 it will make sure the consunmer is present on this server.\nfunc (js *jetStream) isConsumerHealthy(mset *stream, consumer string, ca *consumerAssignment) error {\n\tif mset == nil {\n\t\treturn errors.New(\"stream missing\")\n\t}\n\tjs.mu.RLock()\n\ts, cc := js.srv, js.cluster\n\tif cc == nil {\n\t\t// Non-clustered mode\n\t\tjs.mu.RUnlock()\n\t\treturn nil\n\t}\n\tif ca == nil || ca.Group == nil {\n\t\tjs.mu.RUnlock()\n\t\treturn errors.New(\"consumer assignment or group missing\")\n\t}\n\tnode := ca.Group.node\n\tjs.mu.RUnlock()\n\n\t// Check if not running at all.\n\to := mset.lookupConsumer(consumer)\n\tif o == nil {\n\t\treturn errors.New(\"consumer not found\")\n\t}\n\n\toNode := o.raftNode()\n\trc, _ := o.replica()\n\tswitch {\n\tcase rc <= 1:\n\t\treturn nil // No further checks for R=1 consumers\n\n\tcase node == nil:\n\t\treturn errors.New(\"group node missing\")\n\n\tcase oNode == nil:\n\t\t// Can happen when the consumer's node is not yet initialized.\n\t\treturn errors.New(\"consumer node missing\")\n\n\tcase node != oNode:\n\t\tmset.mu.RLock()\n\t\taccName, streamName := mset.acc.GetName(), mset.cfg.Name\n\t\tmset.mu.RUnlock()\n\t\ts.Warnf(\"Detected consumer cluster node skew '%s > %s > %s'\", accName, streamName, consumer)\n\t\tnode.Delete()\n\t\to.deleteWithoutAdvisory()\n\n\t\t// When we try to restart we nil out the node and reprocess the consumer assignment.\n\t\tjs.mu.Lock()\n\t\tca.Group.node = nil\n\t\tjs.mu.Unlock()\n\t\tjs.processConsumerAssignment(ca)\n\t\treturn errors.New(\"cluster node skew detected\")\n\n\tcase !o.isMonitorRunning():\n\t\treturn errors.New(\"monitor goroutine not running\")\n\n\tcase !node.Healthy():\n\t\treturn errors.New(\"group node unhealthy\")\n\n\tdefault:\n\t\treturn nil\n\t}\n}\n\n// subjectsOverlap checks all existing stream assignments for the account cross-cluster for subject overlap\n// Use only for clustered JetStream\n// Read lock should be held.\nfunc (jsc *jetStreamCluster) subjectsOverlap(acc string, subjects []string, osa *streamAssignment) bool {\n\tasa := jsc.streams[acc]\n\tfor _, sa := range asa {\n\t\t// can't overlap yourself, assume osa pre-checked for deep equal if passed\n\t\tif osa != nil && sa == osa {\n\t\t\tcontinue\n\t\t}\n\t\tfor _, subj := range sa.Config.Subjects {\n\t\t\tfor _, tsubj := range subjects {\n\t\t\t\tif SubjectsCollide(tsubj, subj) {\n\t\t\t\t\treturn true\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn false\n}\n\nfunc (a *Account) getJetStreamFromAccount() (*Server, *jetStream, *jsAccount) {\n\ta.mu.RLock()\n\tjsa := a.js\n\ta.mu.RUnlock()\n\tif jsa == nil {\n\t\treturn nil, nil, nil\n\t}\n\tjsa.mu.RLock()\n\tjs := jsa.js\n\tjsa.mu.RUnlock()\n\tif js == nil {\n\t\treturn nil, nil, nil\n\t}\n\t// Lock not needed, set on creation.\n\ts := js.srv\n\treturn s, js, jsa\n}\n\nfunc (s *Server) JetStreamIsStreamLeader(account, stream string) bool {\n\tjs, cc := s.getJetStreamCluster()\n\tif js == nil || cc == nil {\n\t\treturn false\n\t}\n\tjs.mu.RLock()\n\tdefer js.mu.RUnlock()\n\treturn cc.isStreamLeader(account, stream)\n}\n\nfunc (a *Account) JetStreamIsStreamLeader(stream string) bool {\n\ts, js, jsa := a.getJetStreamFromAccount()\n\tif s == nil || js == nil || jsa == nil {\n\t\treturn false\n\t}\n\tjs.mu.RLock()\n\tdefer js.mu.RUnlock()\n\treturn js.cluster.isStreamLeader(a.Name, stream)\n}\n\nfunc (s *Server) JetStreamIsStreamCurrent(account, stream string) bool {\n\tjs, cc := s.getJetStreamCluster()\n\tif js == nil {\n\t\treturn false\n\t}\n\tjs.mu.RLock()\n\tdefer js.mu.RUnlock()\n\treturn cc.isStreamCurrent(account, stream)\n}\n\nfunc (a *Account) JetStreamIsConsumerLeader(stream, consumer string) bool {\n\ts, js, jsa := a.getJetStreamFromAccount()\n\tif s == nil || js == nil || jsa == nil {\n\t\treturn false\n\t}\n\tjs.mu.RLock()\n\tdefer js.mu.RUnlock()\n\treturn js.cluster.isConsumerLeader(a.Name, stream, consumer)\n}\n\nfunc (s *Server) JetStreamIsConsumerLeader(account, stream, consumer string) bool {\n\tjs, cc := s.getJetStreamCluster()\n\tif js == nil || cc == nil {\n\t\treturn false\n\t}\n\tjs.mu.RLock()\n\tdefer js.mu.RUnlock()\n\treturn cc.isConsumerLeader(account, stream, consumer)\n}\n\nfunc (s *Server) enableJetStreamClustering() error {\n\tif !s.isRunning() {\n\t\treturn nil\n\t}\n\tjs := s.getJetStream()\n\tif js == nil {\n\t\treturn NewJSNotEnabledForAccountError()\n\t}\n\t// Already set.\n\tif js.cluster != nil {\n\t\treturn nil\n\t}\n\n\ts.Noticef(\"Starting JetStream cluster\")\n\t// We need to determine if we have a stable cluster name and expected number of servers.\n\ts.Debugf(\"JetStream cluster checking for stable cluster name and peers\")\n\n\thasLeafNodeSystemShare := s.canExtendOtherDomain()\n\tif s.isClusterNameDynamic() && !hasLeafNodeSystemShare {\n\t\treturn errors.New(\"JetStream cluster requires cluster name\")\n\t}\n\tif s.configuredRoutes() == 0 && !hasLeafNodeSystemShare {\n\t\treturn errors.New(\"JetStream cluster requires configured routes or solicited leafnode for the system account\")\n\t}\n\n\treturn js.setupMetaGroup()\n}\n\n// isClustered returns if we are clustered.\n// Lock should not be held.\nfunc (js *jetStream) isClustered() bool {\n\t// This is only ever set, no need for lock here.\n\treturn js.cluster != nil\n}\n\n// isClusteredNoLock returns if we are clustered, but unlike isClustered() does\n// not use the jetstream's lock, instead, uses an atomic operation.\n// There are situations where some code wants to know if we are clustered but\n// can't use js.isClustered() without causing a lock inversion.\nfunc (js *jetStream) isClusteredNoLock() bool {\n\treturn atomic.LoadInt32(&js.clustered) == 1\n}\n\nfunc (js *jetStream) setupMetaGroup() error {\n\ts := js.srv\n\ts.Noticef(\"Creating JetStream metadata controller\")\n\n\t// Setup our WAL for the metagroup.\n\tsysAcc := s.SystemAccount()\n\tif sysAcc == nil {\n\t\treturn ErrNoSysAccount\n\t}\n\tstoreDir := filepath.Join(js.config.StoreDir, sysAcc.Name, defaultStoreDirName, defaultMetaGroupName)\n\n\tjs.srv.optsMu.RLock()\n\tsyncAlways := js.srv.opts.SyncAlways\n\tsyncInterval := js.srv.opts.SyncInterval\n\tjs.srv.optsMu.RUnlock()\n\tfs, err := newFileStoreWithCreated(\n\t\tFileStoreConfig{StoreDir: storeDir, BlockSize: defaultMetaFSBlkSize, AsyncFlush: false, SyncAlways: syncAlways, SyncInterval: syncInterval, srv: s},\n\t\tStreamConfig{Name: defaultMetaGroupName, Storage: FileStorage},\n\t\ttime.Now().UTC(),\n\t\ts.jsKeyGen(s.getOpts().JetStreamKey, defaultMetaGroupName),\n\t\ts.jsKeyGen(s.getOpts().JetStreamOldKey, defaultMetaGroupName),\n\t)\n\tif err != nil {\n\t\ts.Errorf(\"Error creating filestore: %v\", err)\n\t\treturn err\n\t}\n\n\tcfg := &RaftConfig{Name: defaultMetaGroupName, Store: storeDir, Log: fs}\n\n\t// If we are soliciting leafnode connections and we are sharing a system account and do not disable it with a hint,\n\t// we want to move to observer mode so that we extend the solicited cluster or supercluster but do not form our own.\n\tcfg.Observer = s.canExtendOtherDomain() && s.getOpts().JetStreamExtHint != jsNoExtend\n\n\tvar bootstrap bool\n\tif ps, err := readPeerState(storeDir); err != nil {\n\t\ts.Noticef(\"JetStream cluster bootstrapping\")\n\t\tbootstrap = true\n\t\tpeers := s.ActivePeers()\n\t\ts.Debugf(\"JetStream cluster initial peers: %+v\", peers)\n\t\tif err := s.bootstrapRaftNode(cfg, peers, false); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif cfg.Observer {\n\t\t\ts.Noticef(\"Turning JetStream metadata controller Observer Mode on\")\n\t\t}\n\t} else {\n\t\ts.Noticef(\"JetStream cluster recovering state\")\n\t\t// correlate the value of observer with observations from a previous run.\n\t\tif cfg.Observer {\n\t\t\tswitch ps.domainExt {\n\t\t\tcase extExtended:\n\t\t\t\ts.Noticef(\"Keeping JetStream metadata controller Observer Mode on - due to previous contact\")\n\t\t\tcase extNotExtended:\n\t\t\t\ts.Noticef(\"Turning JetStream metadata controller Observer Mode off - due to previous contact\")\n\t\t\t\tcfg.Observer = false\n\t\t\tcase extUndetermined:\n\t\t\t\ts.Noticef(\"Turning JetStream metadata controller Observer Mode on - no previous contact\")\n\t\t\t\ts.Noticef(\"In cases where JetStream will not be extended\")\n\t\t\t\ts.Noticef(\"and waiting for leader election until first contact is not acceptable,\")\n\t\t\t\ts.Noticef(`manually disable Observer Mode by setting the JetStream Option \"extension_hint: %s\"`, jsNoExtend)\n\t\t\t}\n\t\t} else {\n\t\t\t// To track possible configuration changes, responsible for an altered value of cfg.Observer,\n\t\t\t// set extension state to undetermined.\n\t\t\tps.domainExt = extUndetermined\n\t\t\tif err := writePeerState(storeDir, ps); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t}\n\n\t// Start up our meta node.\n\tn, err := s.startRaftNode(sysAcc.GetName(), cfg, pprofLabels{\n\t\t\"type\":    \"metaleader\",\n\t\t\"account\": sysAcc.Name,\n\t})\n\tif err != nil {\n\t\ts.Warnf(\"Could not start metadata controller: %v\", err)\n\t\treturn err\n\t}\n\n\t// If we are bootstrapped with no state, start campaign early.\n\tif bootstrap {\n\t\tn.Campaign()\n\t}\n\n\tc := s.createInternalJetStreamClient()\n\n\tjs.mu.Lock()\n\tdefer js.mu.Unlock()\n\tjs.cluster = &jetStreamCluster{\n\t\tmeta:    n,\n\t\tstreams: make(map[string]map[string]*streamAssignment),\n\t\ts:       s,\n\t\tc:       c,\n\t\tqch:     make(chan struct{}),\n\t}\n\tatomic.StoreInt32(&js.clustered, 1)\n\tc.registerWithAccount(sysAcc)\n\n\t// Set to true before we start.\n\tjs.metaRecovering = true\n\tjs.srv.startGoRoutine(\n\t\tjs.monitorCluster,\n\t\tpprofLabels{\n\t\t\t\"type\":    \"metaleader\",\n\t\t\t\"account\": sysAcc.Name,\n\t\t},\n\t)\n\treturn nil\n}\n\nfunc (js *jetStream) getMetaGroup() RaftNode {\n\tjs.mu.RLock()\n\tdefer js.mu.RUnlock()\n\tif js.cluster == nil {\n\t\treturn nil\n\t}\n\treturn js.cluster.meta\n}\n\nfunc (js *jetStream) server() *Server {\n\t// Lock not needed, only set once on creation.\n\treturn js.srv\n}\n\n// Will respond if we do not think we have a metacontroller leader.\nfunc (js *jetStream) isLeaderless() bool {\n\tjs.mu.RLock()\n\tcc := js.cluster\n\tif cc == nil || cc.meta == nil {\n\t\tjs.mu.RUnlock()\n\t\treturn false\n\t}\n\tmeta := cc.meta\n\tjs.mu.RUnlock()\n\n\t// If we don't have a leader.\n\t// Make sure we have been running for enough time.\n\tif meta.Leaderless() && time.Since(meta.Created()) > lostQuorumIntervalDefault {\n\t\treturn true\n\t}\n\treturn false\n}\n\n// Will respond iff we are a member and we know we have no leader.\nfunc (js *jetStream) isGroupLeaderless(rg *raftGroup) bool {\n\tif rg == nil || js == nil {\n\t\treturn false\n\t}\n\tjs.mu.RLock()\n\tcc := js.cluster\n\tstarted := js.started\n\n\t// If we are not a member we can not say..\n\tif cc.meta == nil {\n\t\tjs.mu.RUnlock()\n\t\treturn false\n\t}\n\tif !rg.isMember(cc.meta.ID()) {\n\t\tjs.mu.RUnlock()\n\t\treturn false\n\t}\n\t// Single peer groups always have a leader if we are here.\n\tif rg.node == nil {\n\t\tjs.mu.RUnlock()\n\t\treturn false\n\t}\n\tnode := rg.node\n\tjs.mu.RUnlock()\n\t// If we don't have a leader.\n\tif node.Leaderless() {\n\t\t// Threshold for jetstream startup.\n\t\tconst startupThreshold = 10 * time.Second\n\n\t\tif node.HadPreviousLeader() {\n\t\t\t// Make sure we have been running long enough to intelligently determine this.\n\t\t\tif time.Since(started) > startupThreshold {\n\t\t\t\treturn true\n\t\t\t}\n\t\t}\n\t\t// Make sure we have been running for enough time.\n\t\tif time.Since(node.Created()) > lostQuorumIntervalDefault {\n\t\t\treturn true\n\t\t}\n\t}\n\n\treturn false\n}\n\nfunc (s *Server) JetStreamIsStreamAssigned(account, stream string) bool {\n\tjs, cc := s.getJetStreamCluster()\n\tif js == nil || cc == nil {\n\t\treturn false\n\t}\n\tacc, _ := s.LookupAccount(account)\n\tif acc == nil {\n\t\treturn false\n\t}\n\tjs.mu.RLock()\n\tassigned := cc.isStreamAssigned(acc, stream)\n\tjs.mu.RUnlock()\n\treturn assigned\n}\n\n// streamAssigned informs us if this server has this stream assigned.\nfunc (jsa *jsAccount) streamAssigned(stream string) bool {\n\tjsa.mu.RLock()\n\tjs, acc := jsa.js, jsa.account\n\tjsa.mu.RUnlock()\n\n\tif js == nil {\n\t\treturn false\n\t}\n\tjs.mu.RLock()\n\tassigned := js.cluster.isStreamAssigned(acc, stream)\n\tjs.mu.RUnlock()\n\treturn assigned\n}\n\n// Read lock should be held.\nfunc (cc *jetStreamCluster) isStreamAssigned(a *Account, stream string) bool {\n\t// Non-clustered mode always return true.\n\tif cc == nil {\n\t\treturn true\n\t}\n\tif cc.meta == nil {\n\t\treturn false\n\t}\n\tas := cc.streams[a.Name]\n\tif as == nil {\n\t\treturn false\n\t}\n\tsa := as[stream]\n\tif sa == nil {\n\t\treturn false\n\t}\n\trg := sa.Group\n\tif rg == nil {\n\t\treturn false\n\t}\n\t// Check if we are the leader of this raftGroup assigned to the stream.\n\tourID := cc.meta.ID()\n\tfor _, peer := range rg.Peers {\n\t\tif peer == ourID {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\n// Read lock should be held.\nfunc (cc *jetStreamCluster) isStreamLeader(account, stream string) bool {\n\t// Non-clustered mode always return true.\n\tif cc == nil {\n\t\treturn true\n\t}\n\tif cc.meta == nil {\n\t\treturn false\n\t}\n\n\tvar sa *streamAssignment\n\tif as := cc.streams[account]; as != nil {\n\t\tsa = as[stream]\n\t}\n\tif sa == nil {\n\t\treturn false\n\t}\n\trg := sa.Group\n\tif rg == nil {\n\t\treturn false\n\t}\n\t// Check if we are the leader of this raftGroup assigned to the stream.\n\tourID := cc.meta.ID()\n\tfor _, peer := range rg.Peers {\n\t\tif peer == ourID {\n\t\t\tif len(rg.Peers) == 1 || (rg.node != nil && rg.node.Leader()) {\n\t\t\t\treturn true\n\t\t\t}\n\t\t}\n\t}\n\treturn false\n}\n\n// Read lock should be held.\nfunc (cc *jetStreamCluster) isConsumerLeader(account, stream, consumer string) bool {\n\t// Non-clustered mode always return true.\n\tif cc == nil {\n\t\treturn true\n\t}\n\tif cc.meta == nil {\n\t\treturn false\n\t}\n\n\tvar sa *streamAssignment\n\tif as := cc.streams[account]; as != nil {\n\t\tsa = as[stream]\n\t}\n\tif sa == nil {\n\t\treturn false\n\t}\n\t// Check if we are the leader of this raftGroup assigned to this consumer.\n\tca := sa.consumers[consumer]\n\tif ca == nil {\n\t\treturn false\n\t}\n\trg := ca.Group\n\tourID := cc.meta.ID()\n\tfor _, peer := range rg.Peers {\n\t\tif peer == ourID {\n\t\t\tif len(rg.Peers) == 1 || (rg.node != nil && rg.node.Leader()) {\n\t\t\t\treturn true\n\t\t\t}\n\t\t}\n\t}\n\treturn false\n}\n\n// Remove the stream `streamName` for the account `accName` from the inflight\n// proposals map. This is done on success (processStreamAssignment) or on\n// failure (processStreamAssignmentResults).\n// (Write) Lock held on entry.\nfunc (cc *jetStreamCluster) removeInflightProposal(accName, streamName string) {\n\tstreams, ok := cc.inflight[accName]\n\tif !ok {\n\t\treturn\n\t}\n\tdelete(streams, streamName)\n\tif len(streams) == 0 {\n\t\tdelete(cc.inflight, accName)\n\t}\n}\n\n// Return the cluster quit chan.\nfunc (js *jetStream) clusterQuitC() chan struct{} {\n\tjs.mu.RLock()\n\tdefer js.mu.RUnlock()\n\tif js.cluster != nil {\n\t\treturn js.cluster.qch\n\t}\n\treturn nil\n}\n\n// Mark that the meta layer is recovering.\nfunc (js *jetStream) setMetaRecovering() {\n\tjs.mu.Lock()\n\tdefer js.mu.Unlock()\n\tif js.cluster != nil {\n\t\t// metaRecovering\n\t\tjs.metaRecovering = true\n\t}\n}\n\n// Mark that the meta layer is no longer recovering.\nfunc (js *jetStream) clearMetaRecovering() {\n\tjs.mu.Lock()\n\tdefer js.mu.Unlock()\n\tjs.metaRecovering = false\n}\n\n// Return whether the meta layer is recovering.\nfunc (js *jetStream) isMetaRecovering() bool {\n\tjs.mu.RLock()\n\tdefer js.mu.RUnlock()\n\treturn js.metaRecovering\n}\n\n// During recovery track any stream and consumer delete and update operations.\ntype recoveryUpdates struct {\n\tremoveStreams   map[string]*streamAssignment\n\tremoveConsumers map[string]map[string]*consumerAssignment\n\taddStreams      map[string]*streamAssignment\n\tupdateStreams   map[string]*streamAssignment\n\tupdateConsumers map[string]map[string]*consumerAssignment\n}\n\n// Called after recovery of the cluster on startup to check for any orphans.\n// Streams and consumers are recovered from disk, and the meta layer's mappings\n// should clean them up, but under crash scenarios there could be orphans.\nfunc (js *jetStream) checkForOrphans() {\n\t// Can not hold jetstream lock while trying to delete streams or consumers.\n\tjs.mu.Lock()\n\ts, cc := js.srv, js.cluster\n\ts.Debugf(\"JetStream cluster checking for orphans\")\n\n\t// We only want to cleanup any orphans if we know we are current with the meta-leader.\n\tmeta := cc.meta\n\tif meta == nil || meta.Leaderless() {\n\t\tjs.mu.Unlock()\n\t\ts.Debugf(\"JetStream cluster skipping check for orphans, no meta-leader\")\n\t\treturn\n\t}\n\tif !meta.Healthy() {\n\t\tjs.mu.Unlock()\n\t\ts.Debugf(\"JetStream cluster skipping check for orphans, not current with the meta-leader\")\n\t\treturn\n\t}\n\n\tvar streams []*stream\n\tvar consumers []*consumer\n\n\tfor accName, jsa := range js.accounts {\n\t\tasa := cc.streams[accName]\n\t\tjsa.mu.RLock()\n\t\tfor stream, mset := range jsa.streams {\n\t\t\tif sa := asa[stream]; sa == nil {\n\t\t\t\tstreams = append(streams, mset)\n\t\t\t} else {\n\t\t\t\t// This one is good, check consumers now.\n\t\t\t\tfor _, o := range mset.getConsumers() {\n\t\t\t\t\tif sa.consumers[o.String()] == nil {\n\t\t\t\t\t\tconsumers = append(consumers, o)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tjsa.mu.RUnlock()\n\t}\n\tjs.mu.Unlock()\n\n\tfor _, mset := range streams {\n\t\tmset.mu.RLock()\n\t\taccName, stream := mset.acc.Name, mset.cfg.Name\n\t\tmset.mu.RUnlock()\n\t\ts.Warnf(\"Detected orphaned stream '%s > %s', will cleanup\", accName, stream)\n\t\tif err := mset.delete(); err != nil {\n\t\t\ts.Warnf(\"Deleting stream encountered an error: %v\", err)\n\t\t}\n\t}\n\tfor _, o := range consumers {\n\t\to.mu.RLock()\n\t\taccName, mset, consumer := o.acc.Name, o.mset, o.name\n\t\to.mu.RUnlock()\n\t\tstream := \"N/A\"\n\t\tif mset != nil {\n\t\t\tmset.mu.RLock()\n\t\t\tstream = mset.cfg.Name\n\t\t\tmset.mu.RUnlock()\n\t\t}\n\t\tif o.isDurable() {\n\t\t\ts.Warnf(\"Detected orphaned durable consumer '%s > %s > %s', will cleanup\", accName, stream, consumer)\n\t\t} else {\n\t\t\ts.Debugf(\"Detected orphaned consumer '%s > %s > %s', will cleanup\", accName, stream, consumer)\n\t\t}\n\n\t\tif err := o.delete(); err != nil {\n\t\t\ts.Warnf(\"Deleting consumer encountered an error: %v\", err)\n\t\t}\n\t}\n}\n\nfunc (js *jetStream) monitorCluster() {\n\ts, n := js.server(), js.getMetaGroup()\n\tqch, rqch, lch, aq := js.clusterQuitC(), n.QuitC(), n.LeadChangeC(), n.ApplyQ()\n\n\tdefer s.grWG.Done()\n\n\ts.Debugf(\"Starting metadata monitor\")\n\tdefer s.Debugf(\"Exiting metadata monitor\")\n\n\t// Make sure to stop the raft group on exit to prevent accidental memory bloat.\n\tdefer n.Stop()\n\tdefer s.isMetaLeader.Store(false)\n\n\tconst compactInterval = time.Minute\n\tt := time.NewTicker(compactInterval)\n\tdefer t.Stop()\n\n\t// Used to check cold boot cluster when possibly in mixed mode.\n\tconst leaderCheckInterval = time.Second\n\tlt := time.NewTicker(leaderCheckInterval)\n\tdefer lt.Stop()\n\n\t// Check the general health once an hour.\n\tconst healthCheckInterval = 1 * time.Hour\n\tht := time.NewTicker(healthCheckInterval)\n\tdefer ht.Stop()\n\n\t// Utility to check health.\n\tcheckHealth := func() {\n\t\tif hs := s.healthz(nil); hs.Error != _EMPTY_ {\n\t\t\ts.Warnf(\"%v\", hs.Error)\n\t\t}\n\t}\n\n\tvar (\n\t\tisLeader       bool\n\t\tlastSnapTime   time.Time\n\t\tcompactSizeMin = uint64(8 * 1024 * 1024) // 8MB\n\t\tminSnapDelta   = 30 * time.Second\n\t)\n\n\t// Highwayhash key for generating hashes.\n\tkey := make([]byte, 32)\n\tcrand.Read(key)\n\n\t// Set to true to start.\n\tjs.setMetaRecovering()\n\n\t// Snapshotting function.\n\tdoSnapshot := func() {\n\t\t// Suppress during recovery.\n\t\tif js.isMetaRecovering() {\n\t\t\treturn\n\t\t}\n\t\t// For the meta layer we want to snapshot when asked if we need one or have any entries that we can compact.\n\t\tif ne, _ := n.Size(); ne > 0 || n.NeedSnapshot() {\n\t\t\tsnap, err := js.metaSnapshot()\n\t\t\tif err != nil {\n\t\t\t\ts.Warnf(\"Error generating JetStream cluster snapshot: %v\", err)\n\t\t\t} else if err = n.InstallSnapshot(snap); err == nil {\n\t\t\t\tlastSnapTime = time.Now()\n\t\t\t} else if err != errNoSnapAvailable && err != errNodeClosed {\n\t\t\t\ts.Warnf(\"Error snapshotting JetStream cluster state: %v\", err)\n\t\t\t}\n\t\t}\n\t}\n\n\tru := &recoveryUpdates{\n\t\tremoveStreams:   make(map[string]*streamAssignment),\n\t\tremoveConsumers: make(map[string]map[string]*consumerAssignment),\n\t\taddStreams:      make(map[string]*streamAssignment),\n\t\tupdateStreams:   make(map[string]*streamAssignment),\n\t\tupdateConsumers: make(map[string]map[string]*consumerAssignment),\n\t}\n\n\t// Make sure to cancel any pending checkForOrphans calls if the\n\t// monitor goroutine exits.\n\tvar oc *time.Timer\n\tdefer stopAndClearTimer(&oc)\n\n\tfor {\n\t\tselect {\n\t\tcase <-s.quitCh:\n\t\t\t// Server shutting down, but we might receive this before qch, so try to snapshot.\n\t\t\tdoSnapshot()\n\t\t\treturn\n\t\tcase <-rqch:\n\t\t\treturn\n\t\tcase <-qch:\n\t\t\t// Clean signal from shutdown routine so do best effort attempt to snapshot meta layer.\n\t\t\tdoSnapshot()\n\t\t\t// Return the signal back since shutdown will be waiting.\n\t\t\tclose(qch)\n\t\t\treturn\n\t\tcase <-aq.ch:\n\t\t\tces := aq.pop()\n\t\t\tfor _, ce := range ces {\n\t\t\t\tif ce == nil {\n\t\t\t\t\t// Process any removes that are still valid after recovery.\n\t\t\t\t\tfor _, cas := range ru.removeConsumers {\n\t\t\t\t\t\tfor _, ca := range cas {\n\t\t\t\t\t\t\tjs.processConsumerRemoval(ca)\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tfor _, sa := range ru.removeStreams {\n\t\t\t\t\t\tjs.processStreamRemoval(sa)\n\t\t\t\t\t}\n\t\t\t\t\t// Process stream additions.\n\t\t\t\t\tfor _, sa := range ru.addStreams {\n\t\t\t\t\t\tjs.processStreamAssignment(sa)\n\t\t\t\t\t}\n\t\t\t\t\t// Process pending updates.\n\t\t\t\t\tfor _, sa := range ru.updateStreams {\n\t\t\t\t\t\tjs.processUpdateStreamAssignment(sa)\n\t\t\t\t\t}\n\t\t\t\t\t// Now consumers.\n\t\t\t\t\tfor _, cas := range ru.updateConsumers {\n\t\t\t\t\t\tfor _, ca := range cas {\n\t\t\t\t\t\t\tjs.processConsumerAssignment(ca)\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\t// Signals we have replayed all of our metadata.\n\t\t\t\t\tjs.clearMetaRecovering()\n\t\t\t\t\t// Clear.\n\t\t\t\t\tru = nil\n\t\t\t\t\ts.Debugf(\"Recovered JetStream cluster metadata\")\n\t\t\t\t\toc = time.AfterFunc(30*time.Second, js.checkForOrphans)\n\t\t\t\t\t// Do a health check here as well.\n\t\t\t\t\tgo checkHealth()\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tif didSnap, didStreamRemoval, _, err := js.applyMetaEntries(ce.Entries, ru); err == nil {\n\t\t\t\t\tvar nb uint64\n\t\t\t\t\t// Some entries can fail without an error when shutting down, don't move applied forward.\n\t\t\t\t\tif !js.isShuttingDown() {\n\t\t\t\t\t\t_, nb = n.Applied(ce.Index)\n\t\t\t\t\t}\n\t\t\t\t\tif js.hasPeerEntries(ce.Entries) || didStreamRemoval || (didSnap && !isLeader) {\n\t\t\t\t\t\tdoSnapshot()\n\t\t\t\t\t} else if nb > compactSizeMin && time.Since(lastSnapTime) > minSnapDelta {\n\t\t\t\t\t\tdoSnapshot()\n\t\t\t\t\t}\n\t\t\t\t\tce.ReturnToPool()\n\t\t\t\t} else {\n\t\t\t\t\ts.Warnf(\"Error applying JetStream cluster entries: %v\", err)\n\t\t\t\t}\n\t\t\t}\n\t\t\taq.recycle(&ces)\n\n\t\tcase isLeader = <-lch:\n\t\t\t// Process the change.\n\t\t\tjs.processLeaderChange(isLeader)\n\t\t\tif isLeader {\n\t\t\t\ts.sendInternalMsgLocked(serverStatsPingReqSubj, _EMPTY_, nil, nil)\n\t\t\t\t// Install a snapshot as we become leader.\n\t\t\t\tjs.checkClusterSize()\n\t\t\t\tdoSnapshot()\n\t\t\t}\n\n\t\tcase <-t.C:\n\t\t\tdoSnapshot()\n\t\t\t// Periodically check the cluster size.\n\t\t\tif n.Leader() {\n\t\t\t\tjs.checkClusterSize()\n\t\t\t}\n\t\tcase <-ht.C:\n\t\t\t// Do this in a separate go routine.\n\t\t\tgo checkHealth()\n\n\t\tcase <-lt.C:\n\t\t\ts.Debugf(\"Checking JetStream cluster state\")\n\t\t\t// If we have a current leader or had one in the past we can cancel this here since the metaleader\n\t\t\t// will be in charge of all peer state changes.\n\t\t\t// For cold boot only.\n\t\t\tif !n.Leaderless() || n.HadPreviousLeader() {\n\t\t\t\tlt.Stop()\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\t// If we are here we do not have a leader and we did not have a previous one, so cold start.\n\t\t\t// Check to see if we can adjust our cluster size down iff we are in mixed mode and we have\n\t\t\t// seen a total that is what our original estimate was.\n\t\t\tcs := n.ClusterSize()\n\t\t\tif js, total := s.trackedJetStreamServers(); js < total && total >= cs && js != cs {\n\t\t\t\ts.Noticef(\"Adjusting JetStream expected peer set size to %d from original %d\", js, cs)\n\t\t\t\tn.AdjustBootClusterSize(js)\n\t\t\t}\n\t\t}\n\t}\n}\n\n// This is called on first leader transition to double check the peers and cluster set size.\nfunc (js *jetStream) checkClusterSize() {\n\ts, n := js.server(), js.getMetaGroup()\n\tif n == nil {\n\t\treturn\n\t}\n\t// We will check that we have a correct cluster set size by checking for any non-js servers\n\t// which can happen in mixed mode.\n\tps := n.(*raft).currentPeerState()\n\tif len(ps.knownPeers) >= ps.clusterSize {\n\t\treturn\n\t}\n\n\t// Grab our active peers.\n\tpeers := s.ActivePeers()\n\n\t// If we have not registered all of our peers yet we can't do\n\t// any adjustments based on a mixed mode. We will periodically check back.\n\tif len(peers) < ps.clusterSize {\n\t\treturn\n\t}\n\n\ts.Debugf(\"Checking JetStream cluster size\")\n\n\t// If we are here our known set as the leader is not the same as the cluster size.\n\t// Check to see if we have a mixed mode setup.\n\tvar totalJS int\n\tfor _, p := range peers {\n\t\tif si, ok := s.nodeToInfo.Load(p); ok && si != nil {\n\t\t\tif si.(nodeInfo).js {\n\t\t\t\ttotalJS++\n\t\t\t}\n\t\t}\n\t}\n\t// If we have less then our cluster size adjust that here. Can not do individual peer removals since\n\t// they will not be in the tracked peers.\n\tif totalJS < ps.clusterSize {\n\t\ts.Debugf(\"Adjusting JetStream cluster size from %d to %d\", ps.clusterSize, totalJS)\n\t\tif err := n.AdjustClusterSize(totalJS); err != nil {\n\t\t\ts.Warnf(\"Error adjusting JetStream cluster size: %v\", err)\n\t\t}\n\t}\n}\n\n// Represents our stable meta state that we can write out.\ntype writeableStreamAssignment struct {\n\tClient    *ClientInfo   `json:\"client,omitempty\"`\n\tCreated   time.Time     `json:\"created\"`\n\tConfig    *StreamConfig `json:\"stream\"`\n\tGroup     *raftGroup    `json:\"group\"`\n\tSync      string        `json:\"sync\"`\n\tConsumers []*consumerAssignment\n}\n\nfunc (js *jetStream) clusterStreamConfig(accName, streamName string) (StreamConfig, bool) {\n\tjs.mu.RLock()\n\tdefer js.mu.RUnlock()\n\tif sa, ok := js.cluster.streams[accName][streamName]; ok {\n\t\treturn *sa.Config, true\n\t}\n\treturn StreamConfig{}, false\n}\n\nfunc (js *jetStream) metaSnapshot() ([]byte, error) {\n\tstart := time.Now()\n\tjs.mu.RLock()\n\ts := js.srv\n\tcc := js.cluster\n\tnsa := 0\n\tnca := 0\n\tfor _, asa := range cc.streams {\n\t\tnsa += len(asa)\n\t}\n\tstreams := make([]writeableStreamAssignment, 0, nsa)\n\tfor _, asa := range cc.streams {\n\t\tfor _, sa := range asa {\n\t\t\twsa := writeableStreamAssignment{\n\t\t\t\tClient:    sa.Client.forAssignmentSnap(),\n\t\t\t\tCreated:   sa.Created,\n\t\t\t\tConfig:    sa.Config,\n\t\t\t\tGroup:     sa.Group,\n\t\t\t\tSync:      sa.Sync,\n\t\t\t\tConsumers: make([]*consumerAssignment, 0, len(sa.consumers)),\n\t\t\t}\n\t\t\tfor _, ca := range sa.consumers {\n\t\t\t\t// Skip if the consumer is pending, we can't include it in our snapshot.\n\t\t\t\t// If the proposal fails after we marked it pending, it would result in a ghost consumer.\n\t\t\t\tif ca.pending {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tcca := *ca\n\t\t\t\tcca.Stream = wsa.Config.Name // Needed for safe roll-backs.\n\t\t\t\tcca.Client = cca.Client.forAssignmentSnap()\n\t\t\t\tcca.Subject, cca.Reply = _EMPTY_, _EMPTY_\n\t\t\t\twsa.Consumers = append(wsa.Consumers, &cca)\n\t\t\t\tnca++\n\t\t\t}\n\t\t\tstreams = append(streams, wsa)\n\t\t}\n\t}\n\n\tif len(streams) == 0 {\n\t\tjs.mu.RUnlock()\n\t\treturn nil, nil\n\t}\n\n\t// Track how long it took to marshal the JSON\n\tmstart := time.Now()\n\tb, err := json.Marshal(streams)\n\tmend := time.Since(mstart)\n\n\tjs.mu.RUnlock()\n\n\t// Must not be possible for a JSON marshaling error to result\n\t// in an empty snapshot.\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Track how long it took to compress the JSON\n\tcstart := time.Now()\n\tsnap := s2.Encode(nil, b)\n\tcend := time.Since(cstart)\n\n\tif took := time.Since(start); took > time.Second {\n\t\ts.rateLimitFormatWarnf(\"Metalayer snapshot took %.3fs (streams: %d, consumers: %d, marshal: %.3fs, s2: %.3fs, uncompressed: %d, compressed: %d)\",\n\t\t\ttook.Seconds(), nsa, nca, mend.Seconds(), cend.Seconds(), len(b), len(snap))\n\t}\n\treturn snap, nil\n}\n\nfunc (js *jetStream) applyMetaSnapshot(buf []byte, ru *recoveryUpdates, isRecovering bool) error {\n\tvar wsas []writeableStreamAssignment\n\tif len(buf) > 0 {\n\t\tjse, err := s2.Decode(nil, buf)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif err = json.Unmarshal(jse, &wsas); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\t// Build our new version here outside of js.\n\tstreams := make(map[string]map[string]*streamAssignment)\n\tfor _, wsa := range wsas {\n\t\tfixCfgMirrorWithDedupWindow(wsa.Config)\n\t\tas := streams[wsa.Client.serviceAccount()]\n\t\tif as == nil {\n\t\t\tas = make(map[string]*streamAssignment)\n\t\t\tstreams[wsa.Client.serviceAccount()] = as\n\t\t}\n\t\tsa := &streamAssignment{Client: wsa.Client, Created: wsa.Created, Config: wsa.Config, Group: wsa.Group, Sync: wsa.Sync}\n\t\tif len(wsa.Consumers) > 0 {\n\t\t\tsa.consumers = make(map[string]*consumerAssignment)\n\t\t\tfor _, ca := range wsa.Consumers {\n\t\t\t\tif ca.Stream == _EMPTY_ {\n\t\t\t\t\tca.Stream = sa.Config.Name // Rehydrate from the stream name.\n\t\t\t\t}\n\t\t\t\tsa.consumers[ca.Name] = ca\n\t\t\t}\n\t\t}\n\t\tas[wsa.Config.Name] = sa\n\t}\n\n\tjs.mu.Lock()\n\tcc := js.cluster\n\n\tvar saAdd, saDel, saChk []*streamAssignment\n\t// Walk through the old list to generate the delete list.\n\tfor account, asa := range cc.streams {\n\t\tnasa := streams[account]\n\t\tfor sn, sa := range asa {\n\t\t\tif nsa := nasa[sn]; nsa == nil {\n\t\t\t\tsaDel = append(saDel, sa)\n\t\t\t} else {\n\t\t\t\tsaChk = append(saChk, nsa)\n\t\t\t}\n\t\t}\n\t}\n\t// Walk through the new list to generate the add list.\n\tfor account, nasa := range streams {\n\t\tasa := cc.streams[account]\n\t\tfor sn, sa := range nasa {\n\t\t\tif asa[sn] == nil {\n\t\t\t\tsaAdd = append(saAdd, sa)\n\t\t\t}\n\t\t}\n\t}\n\n\t// Now walk the ones to check and process consumers.\n\tvar caAdd, caDel []*consumerAssignment\n\tfor _, sa := range saChk {\n\t\t// Make sure to add in all the new ones from sa.\n\t\tfor _, ca := range sa.consumers {\n\t\t\tcaAdd = append(caAdd, ca)\n\t\t}\n\t\tif osa := js.streamAssignment(sa.Client.serviceAccount(), sa.Config.Name); osa != nil {\n\t\t\tfor _, ca := range osa.consumers {\n\t\t\t\t// Consumer was either removed, or recreated with a different raft group.\n\t\t\t\tif nca := sa.consumers[ca.Name]; nca == nil {\n\t\t\t\t\tcaDel = append(caDel, ca)\n\t\t\t\t} else if nca.Group != nil && ca.Group != nil && nca.Group.Name != ca.Group.Name {\n\t\t\t\t\tcaDel = append(caDel, ca)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tjs.mu.Unlock()\n\n\t// Do removals first.\n\tfor _, sa := range saDel {\n\t\tjs.setStreamAssignmentRecovering(sa)\n\t\tif isRecovering {\n\t\t\tkey := sa.recoveryKey()\n\t\t\tru.removeStreams[key] = sa\n\t\t\tdelete(ru.addStreams, key)\n\t\t\tdelete(ru.updateStreams, key)\n\t\t\tdelete(ru.updateConsumers, key)\n\t\t\tdelete(ru.removeConsumers, key)\n\t\t} else {\n\t\t\tjs.processStreamRemoval(sa)\n\t\t}\n\t}\n\t// Now do add for the streams. Also add in all consumers.\n\tfor _, sa := range saAdd {\n\t\tjs.setStreamAssignmentRecovering(sa)\n\t\tjs.processStreamAssignment(sa)\n\n\t\t// We can simply process the consumers.\n\t\tfor _, ca := range sa.consumers {\n\t\t\tjs.setConsumerAssignmentRecovering(ca)\n\t\t\tjs.processConsumerAssignment(ca)\n\t\t}\n\t}\n\n\t// Perform updates on those in saChk. These were existing so make\n\t// sure to process any changes.\n\tfor _, sa := range saChk {\n\t\tjs.setStreamAssignmentRecovering(sa)\n\t\tif isRecovering {\n\t\t\tkey := sa.recoveryKey()\n\t\t\tru.updateStreams[key] = sa\n\t\t\tdelete(ru.addStreams, key)\n\t\t\tdelete(ru.removeStreams, key)\n\t\t} else {\n\t\t\tjs.processUpdateStreamAssignment(sa)\n\t\t}\n\t}\n\n\t// Now do the deltas for existing stream's consumers.\n\tfor _, ca := range caDel {\n\t\tjs.setConsumerAssignmentRecovering(ca)\n\t\tif isRecovering {\n\t\t\tkey := ca.recoveryKey()\n\t\t\tskey := ca.streamRecoveryKey()\n\t\t\tif _, ok := ru.removeConsumers[skey]; !ok {\n\t\t\t\tru.removeConsumers[skey] = map[string]*consumerAssignment{}\n\t\t\t}\n\t\t\tru.removeConsumers[skey][key] = ca\n\t\t\tif consumers, ok := ru.updateConsumers[skey]; ok {\n\t\t\t\tdelete(consumers, key)\n\t\t\t}\n\t\t} else {\n\t\t\tjs.processConsumerRemoval(ca)\n\t\t}\n\t}\n\tfor _, ca := range caAdd {\n\t\tjs.setConsumerAssignmentRecovering(ca)\n\t\tif isRecovering {\n\t\t\tkey := ca.recoveryKey()\n\t\t\tskey := ca.streamRecoveryKey()\n\t\t\tif consumers, ok := ru.removeConsumers[skey]; ok {\n\t\t\t\tdelete(consumers, key)\n\t\t\t}\n\t\t\tif _, ok := ru.updateConsumers[skey]; !ok {\n\t\t\t\tru.updateConsumers[skey] = map[string]*consumerAssignment{}\n\t\t\t}\n\t\t\tru.updateConsumers[skey][key] = ca\n\t\t} else {\n\t\t\tjs.processConsumerAssignment(ca)\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// Called on recovery to make sure we do not process like original.\nfunc (js *jetStream) setStreamAssignmentRecovering(sa *streamAssignment) {\n\tjs.mu.Lock()\n\tdefer js.mu.Unlock()\n\tsa.responded = true\n\tsa.recovering = true\n\tsa.Restore = nil\n\tif sa.Group != nil {\n\t\tsa.Group.Preferred = _EMPTY_\n\t}\n}\n\n// Called on recovery to make sure we do not process like original.\nfunc (js *jetStream) setConsumerAssignmentRecovering(ca *consumerAssignment) {\n\tjs.mu.Lock()\n\tdefer js.mu.Unlock()\n\tca.responded = true\n\tca.recovering = true\n\tif ca.Group != nil {\n\t\tca.Group.Preferred = _EMPTY_\n\t}\n}\n\n// Just copies over and changes out the group so it can be encoded.\n// Lock should be held.\nfunc (sa *streamAssignment) copyGroup() *streamAssignment {\n\tcsa, cg := *sa, *sa.Group\n\tcsa.Group = &cg\n\tcsa.Group.Peers = copyStrings(sa.Group.Peers)\n\treturn &csa\n}\n\n// Just copies over and changes out the group so it can be encoded.\n// Lock should be held.\nfunc (ca *consumerAssignment) copyGroup() *consumerAssignment {\n\tcca, cg := *ca, *ca.Group\n\tcca.Group = &cg\n\tcca.Group.Peers = copyStrings(ca.Group.Peers)\n\treturn &cca\n}\n\n// Lock should be held.\nfunc (sa *streamAssignment) missingPeers() bool {\n\treturn len(sa.Group.Peers) < sa.Config.Replicas\n}\n\n// Called when we detect a new peer. Only the leader will process checking\n// for any streams, and consequently any consumers.\nfunc (js *jetStream) processAddPeer(peer string) {\n\tjs.mu.Lock()\n\tdefer js.mu.Unlock()\n\n\ts, cc := js.srv, js.cluster\n\tif cc == nil || cc.meta == nil {\n\t\treturn\n\t}\n\tisLeader := cc.isLeader()\n\n\t// Now check if we are meta-leader. We will check for any re-assignments.\n\tif !isLeader {\n\t\treturn\n\t}\n\n\tsir, ok := s.nodeToInfo.Load(peer)\n\tif !ok || sir == nil {\n\t\treturn\n\t}\n\tsi := sir.(nodeInfo)\n\n\tfor _, asa := range cc.streams {\n\t\tfor _, sa := range asa {\n\t\t\tif sa.missingPeers() {\n\t\t\t\t// Make sure the right cluster etc.\n\t\t\t\tif si.cluster != sa.Client.Cluster {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\t// If we are here we can add in this peer.\n\t\t\t\tcsa := sa.copyGroup()\n\t\t\t\tcsa.Group.Peers = append(csa.Group.Peers, peer)\n\t\t\t\t// Send our proposal for this csa. Also use same group definition for all the consumers as well.\n\t\t\t\tcc.meta.Propose(encodeAddStreamAssignment(csa))\n\t\t\t\tfor _, ca := range sa.consumers {\n\t\t\t\t\t// Ephemerals are R=1, so only auto-remap durables, or R>1.\n\t\t\t\t\tif ca.Config.Durable != _EMPTY_ || len(ca.Group.Peers) > 1 {\n\t\t\t\t\t\tcca := ca.copyGroup()\n\t\t\t\t\t\tcca.Group.Peers = csa.Group.Peers\n\t\t\t\t\t\tcc.meta.Propose(encodeAddConsumerAssignment(cca))\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc (js *jetStream) processRemovePeer(peer string) {\n\t// We may be already disabled.\n\tif js == nil || js.disabled.Load() {\n\t\treturn\n\t}\n\n\tjs.mu.Lock()\n\ts, cc := js.srv, js.cluster\n\tif cc == nil || cc.meta == nil {\n\t\tjs.mu.Unlock()\n\t\treturn\n\t}\n\tisLeader := cc.isLeader()\n\t// All nodes will check if this is them.\n\tisUs := cc.meta.ID() == peer\n\tjs.mu.Unlock()\n\n\tif isUs {\n\t\ts.Errorf(\"JetStream being DISABLED, our server was removed from the cluster\")\n\t\tadv := &JSServerRemovedAdvisory{\n\t\t\tTypedEvent: TypedEvent{\n\t\t\t\tType: JSServerRemovedAdvisoryType,\n\t\t\t\tID:   nuid.Next(),\n\t\t\t\tTime: time.Now().UTC(),\n\t\t\t},\n\t\t\tServer:   s.Name(),\n\t\t\tServerID: s.ID(),\n\t\t\tCluster:  s.cachedClusterName(),\n\t\t\tDomain:   s.getOpts().JetStreamDomain,\n\t\t}\n\t\ts.publishAdvisory(nil, JSAdvisoryServerRemoved, adv)\n\n\t\tgo s.DisableJetStream()\n\t}\n\n\t// Now check if we are meta-leader. We will attempt re-assignment.\n\tif !isLeader {\n\t\treturn\n\t}\n\n\tjs.mu.Lock()\n\tdefer js.mu.Unlock()\n\n\tfor _, asa := range cc.streams {\n\t\tfor _, sa := range asa {\n\t\t\tif rg := sa.Group; rg.isMember(peer) {\n\t\t\t\tjs.removePeerFromStreamLocked(sa, peer)\n\t\t\t}\n\t\t}\n\t}\n}\n\n// Assumes all checks have already been done.\nfunc (js *jetStream) removePeerFromStream(sa *streamAssignment, peer string) bool {\n\tjs.mu.Lock()\n\tdefer js.mu.Unlock()\n\treturn js.removePeerFromStreamLocked(sa, peer)\n}\n\n// Lock should be held.\nfunc (js *jetStream) removePeerFromStreamLocked(sa *streamAssignment, peer string) bool {\n\tif rg := sa.Group; !rg.isMember(peer) {\n\t\treturn false\n\t}\n\n\ts, cc, csa := js.srv, js.cluster, sa.copyGroup()\n\tif cc == nil || cc.meta == nil {\n\t\treturn false\n\t}\n\treplaced := cc.remapStreamAssignment(csa, peer)\n\tif !replaced {\n\t\ts.Warnf(\"JetStream cluster could not replace peer for stream '%s > %s'\", sa.Client.serviceAccount(), sa.Config.Name)\n\t}\n\n\t// Send our proposal for this csa. Also use same group definition for all the consumers as well.\n\tcc.meta.Propose(encodeAddStreamAssignment(csa))\n\trg := csa.Group\n\tfor _, ca := range sa.consumers {\n\t\t// Ephemerals are R=1, so only auto-remap durables, or R>1.\n\t\tif ca.Config.Durable != _EMPTY_ {\n\t\t\tcca := ca.copyGroup()\n\t\t\tcca.Group.Peers, cca.Group.Preferred = rg.Peers, _EMPTY_\n\t\t\tcc.meta.Propose(encodeAddConsumerAssignment(cca))\n\t\t} else if ca.Group.isMember(peer) {\n\t\t\t// These are ephemerals. Check to see if we deleted this peer.\n\t\t\tcc.meta.Propose(encodeDeleteConsumerAssignment(ca))\n\t\t}\n\t}\n\treturn replaced\n}\n\n// Check if we have peer related entries.\nfunc (js *jetStream) hasPeerEntries(entries []*Entry) bool {\n\tfor _, e := range entries {\n\t\tif e.Type == EntryRemovePeer || e.Type == EntryAddPeer {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\nconst ksep = \":\"\n\nfunc (sa *streamAssignment) recoveryKey() string {\n\tif sa == nil {\n\t\treturn _EMPTY_\n\t}\n\treturn sa.Client.serviceAccount() + ksep + sa.Config.Name\n}\n\nfunc (ca *consumerAssignment) streamRecoveryKey() string {\n\tif ca == nil {\n\t\treturn _EMPTY_\n\t}\n\treturn ca.Client.serviceAccount() + ksep + ca.Stream\n}\n\nfunc (ca *consumerAssignment) recoveryKey() string {\n\tif ca == nil {\n\t\treturn _EMPTY_\n\t}\n\treturn ca.Client.serviceAccount() + ksep + ca.Stream + ksep + ca.Name\n}\n\nfunc (js *jetStream) applyMetaEntries(entries []*Entry, ru *recoveryUpdates) (bool, bool, bool, error) {\n\tvar didSnap, didRemoveStream, didRemoveConsumer bool\n\tisRecovering := js.isMetaRecovering()\n\n\tfor _, e := range entries {\n\t\tif e.Type == EntrySnapshot {\n\t\t\tjs.applyMetaSnapshot(e.Data, ru, isRecovering)\n\t\t\tdidSnap = true\n\t\t} else if e.Type == EntryRemovePeer {\n\t\t\tif !isRecovering {\n\t\t\t\tjs.processRemovePeer(string(e.Data))\n\t\t\t}\n\t\t} else if e.Type == EntryAddPeer {\n\t\t\tif !isRecovering {\n\t\t\t\tjs.processAddPeer(string(e.Data))\n\t\t\t}\n\t\t} else {\n\t\t\tbuf := e.Data\n\t\t\tswitch entryOp(buf[0]) {\n\t\t\tcase assignStreamOp:\n\t\t\t\tsa, err := decodeStreamAssignment(buf[1:])\n\t\t\t\tif err != nil {\n\t\t\t\t\tjs.srv.Errorf(\"JetStream cluster failed to decode stream assignment: %q\", buf[1:])\n\t\t\t\t\treturn didSnap, didRemoveStream, didRemoveConsumer, err\n\t\t\t\t}\n\t\t\t\tif isRecovering {\n\t\t\t\t\tjs.setStreamAssignmentRecovering(sa)\n\t\t\t\t\tkey := sa.recoveryKey()\n\t\t\t\t\tru.addStreams[key] = sa\n\t\t\t\t\tdelete(ru.removeStreams, key)\n\t\t\t\t} else if js.processStreamAssignment(sa) {\n\t\t\t\t\tdidRemoveStream = true\n\t\t\t\t}\n\t\t\tcase removeStreamOp:\n\t\t\t\tsa, err := decodeStreamAssignment(buf[1:])\n\t\t\t\tif err != nil {\n\t\t\t\t\tjs.srv.Errorf(\"JetStream cluster failed to decode stream assignment: %q\", buf[1:])\n\t\t\t\t\treturn didSnap, didRemoveStream, didRemoveConsumer, err\n\t\t\t\t}\n\t\t\t\tif isRecovering {\n\t\t\t\t\tjs.setStreamAssignmentRecovering(sa)\n\t\t\t\t\tkey := sa.recoveryKey()\n\t\t\t\t\tru.removeStreams[key] = sa\n\t\t\t\t\tdelete(ru.addStreams, key)\n\t\t\t\t\tdelete(ru.updateStreams, key)\n\t\t\t\t\tdelete(ru.updateConsumers, key)\n\t\t\t\t\tdelete(ru.removeConsumers, key)\n\t\t\t\t} else {\n\t\t\t\t\tjs.processStreamRemoval(sa)\n\t\t\t\t\tdidRemoveStream = true\n\t\t\t\t}\n\t\t\tcase assignConsumerOp:\n\t\t\t\tca, err := decodeConsumerAssignment(buf[1:])\n\t\t\t\tif err != nil {\n\t\t\t\t\tjs.srv.Errorf(\"JetStream cluster failed to decode consumer assignment: %q\", buf[1:])\n\t\t\t\t\treturn didSnap, didRemoveStream, didRemoveConsumer, err\n\t\t\t\t}\n\t\t\t\tif isRecovering {\n\t\t\t\t\tjs.setConsumerAssignmentRecovering(ca)\n\t\t\t\t\tkey := ca.recoveryKey()\n\t\t\t\t\tskey := ca.streamRecoveryKey()\n\t\t\t\t\tif consumers, ok := ru.removeConsumers[skey]; ok {\n\t\t\t\t\t\tdelete(consumers, key)\n\t\t\t\t\t}\n\t\t\t\t\tif _, ok := ru.updateConsumers[skey]; !ok {\n\t\t\t\t\t\tru.updateConsumers[skey] = map[string]*consumerAssignment{}\n\t\t\t\t\t}\n\t\t\t\t\tru.updateConsumers[skey][key] = ca\n\t\t\t\t} else {\n\t\t\t\t\tjs.processConsumerAssignment(ca)\n\t\t\t\t}\n\t\t\tcase assignCompressedConsumerOp:\n\t\t\t\tca, err := decodeConsumerAssignmentCompressed(buf[1:])\n\t\t\t\tif err != nil {\n\t\t\t\t\tjs.srv.Errorf(\"JetStream cluster failed to decode compressed consumer assignment: %q\", buf[1:])\n\t\t\t\t\treturn didSnap, didRemoveStream, didRemoveConsumer, err\n\t\t\t\t}\n\t\t\t\tif isRecovering {\n\t\t\t\t\tjs.setConsumerAssignmentRecovering(ca)\n\t\t\t\t\tkey := ca.recoveryKey()\n\t\t\t\t\tskey := ca.streamRecoveryKey()\n\t\t\t\t\tif consumers, ok := ru.removeConsumers[skey]; ok {\n\t\t\t\t\t\tdelete(consumers, key)\n\t\t\t\t\t}\n\t\t\t\t\tif _, ok := ru.updateConsumers[skey]; !ok {\n\t\t\t\t\t\tru.updateConsumers[skey] = map[string]*consumerAssignment{}\n\t\t\t\t\t}\n\t\t\t\t\tru.updateConsumers[skey][key] = ca\n\t\t\t\t} else {\n\t\t\t\t\tjs.processConsumerAssignment(ca)\n\t\t\t\t}\n\t\t\tcase removeConsumerOp:\n\t\t\t\tca, err := decodeConsumerAssignment(buf[1:])\n\t\t\t\tif err != nil {\n\t\t\t\t\tjs.srv.Errorf(\"JetStream cluster failed to decode consumer assignment: %q\", buf[1:])\n\t\t\t\t\treturn didSnap, didRemoveStream, didRemoveConsumer, err\n\t\t\t\t}\n\t\t\t\tif isRecovering {\n\t\t\t\t\tjs.setConsumerAssignmentRecovering(ca)\n\t\t\t\t\tkey := ca.recoveryKey()\n\t\t\t\t\tskey := ca.streamRecoveryKey()\n\t\t\t\t\tif _, ok := ru.removeConsumers[skey]; !ok {\n\t\t\t\t\t\tru.removeConsumers[skey] = map[string]*consumerAssignment{}\n\t\t\t\t\t}\n\t\t\t\t\tru.removeConsumers[skey][key] = ca\n\t\t\t\t\tif consumers, ok := ru.updateConsumers[skey]; ok {\n\t\t\t\t\t\tdelete(consumers, key)\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\tjs.processConsumerRemoval(ca)\n\t\t\t\t\tdidRemoveConsumer = true\n\t\t\t\t}\n\t\t\tcase updateStreamOp:\n\t\t\t\tsa, err := decodeStreamAssignment(buf[1:])\n\t\t\t\tif err != nil {\n\t\t\t\t\tjs.srv.Errorf(\"JetStream cluster failed to decode stream assignment: %q\", buf[1:])\n\t\t\t\t\treturn didSnap, didRemoveStream, didRemoveConsumer, err\n\t\t\t\t}\n\t\t\t\tif isRecovering {\n\t\t\t\t\tjs.setStreamAssignmentRecovering(sa)\n\t\t\t\t\tkey := sa.recoveryKey()\n\t\t\t\t\tru.updateStreams[key] = sa\n\t\t\t\t\tdelete(ru.addStreams, key)\n\t\t\t\t\tdelete(ru.removeStreams, key)\n\t\t\t\t} else {\n\t\t\t\t\tjs.processUpdateStreamAssignment(sa)\n\t\t\t\t\t// Since an update can be lowering replica count, we want upper layer to treat\n\t\t\t\t\t// similar to a removal and snapshot to collapse old entries.\n\t\t\t\t\tdidRemoveStream = true\n\t\t\t\t}\n\t\t\tdefault:\n\t\t\t\tpanic(fmt.Sprintf(\"JetStream Cluster Unknown meta entry op type: %v\", entryOp(buf[0])))\n\t\t\t}\n\t\t}\n\t}\n\treturn didSnap, didRemoveStream, didRemoveConsumer, nil\n}\n\nfunc (rg *raftGroup) isMember(id string) bool {\n\tif rg == nil {\n\t\treturn false\n\t}\n\tfor _, peer := range rg.Peers {\n\t\tif peer == id {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\nfunc (rg *raftGroup) setPreferred() {\n\tif rg == nil || len(rg.Peers) == 0 {\n\t\treturn\n\t}\n\tif len(rg.Peers) == 1 {\n\t\trg.Preferred = rg.Peers[0]\n\t} else {\n\t\t// For now just randomly select a peer for the preferred.\n\t\tpi := rand.Int31n(int32(len(rg.Peers)))\n\t\trg.Preferred = rg.Peers[pi]\n\t}\n}\n\n// createRaftGroup is called to spin up this raft group if needed.\nfunc (js *jetStream) createRaftGroup(accName string, rg *raftGroup, storage StorageType, labels pprofLabels) (RaftNode, error) {\n\t// Must hold JS lock throughout, otherwise two parallel calls for the same raft group could result\n\t// in duplicate instances for the same identifier, if the current Raft node is shutting down.\n\t// We can release the lock temporarily while waiting for the Raft node to shut down.\n\tjs.mu.Lock()\n\tdefer js.mu.Unlock()\n\n\ts, cc := js.srv, js.cluster\n\tif cc == nil || cc.meta == nil {\n\t\treturn nil, NewJSClusterNotActiveError()\n\t}\n\n\t// If this is a single peer raft group or we are not a member return.\n\tif len(rg.Peers) <= 1 || !rg.isMember(cc.meta.ID()) {\n\t\t// Nothing to do here.\n\t\treturn nil, nil\n\t}\n\n\t// Check if we already have this assigned.\nretry:\n\tif node := s.lookupRaftNode(rg.Name); node != nil {\n\t\tif node.State() == Closed {\n\t\t\t// We're waiting for this node to finish shutting down before we replace it.\n\t\t\tjs.mu.Unlock()\n\t\t\tnode.WaitForStop()\n\t\t\tjs.mu.Lock()\n\t\t\tgoto retry\n\t\t}\n\t\ts.Debugf(\"JetStream cluster already has raft group %q assigned\", rg.Name)\n\t\t// Check and see if the group has the same peers. If not then we\n\t\t// will update the known peers, which will send a peerstate if leader.\n\t\tgroupPeerIDs := append([]string{}, rg.Peers...)\n\t\tvar samePeers bool\n\t\tif nodePeers := node.Peers(); len(rg.Peers) == len(nodePeers) {\n\t\t\tnodePeerIDs := make([]string, 0, len(nodePeers))\n\t\t\tfor _, n := range nodePeers {\n\t\t\t\tnodePeerIDs = append(nodePeerIDs, n.ID)\n\t\t\t}\n\t\t\tslices.Sort(groupPeerIDs)\n\t\t\tslices.Sort(nodePeerIDs)\n\t\t\tsamePeers = slices.Equal(groupPeerIDs, nodePeerIDs)\n\t\t}\n\t\tif !samePeers {\n\t\t\t// At this point we have no way of knowing:\n\t\t\t// 1. Whether the group has lost enough nodes to cause a quorum\n\t\t\t//    loss, in which case a proposal may fail, therefore we will\n\t\t\t//    force a peerstate write;\n\t\t\t// 2. Whether nodes in the group have other applies queued up\n\t\t\t//    that could change the peerstate again, therefore the leader\n\t\t\t//    should send out a new proposal anyway too just to make sure\n\t\t\t//    that this change gets captured in the log.\n\t\t\tnode.UpdateKnownPeers(groupPeerIDs)\n\n\t\t\t// If the peers changed as a result of an update by the meta layer, we must reflect that in the log of\n\t\t\t// this group. Otherwise, a new peer would come up and instantly reset the peer state back to whatever is\n\t\t\t// in the log at that time, overwriting what the meta layer told it.\n\t\t\t// Will need to address this properly later on, by for example having the meta layer decide the new\n\t\t\t// placement, but have the leader of this group propose it through its own log instead.\n\t\t\tif node.Leader() {\n\t\t\t\tnode.ProposeKnownPeers(groupPeerIDs)\n\t\t\t}\n\t\t}\n\t\trg.node = node\n\t\treturn node, nil\n\t}\n\n\ts.Debugf(\"JetStream cluster creating raft group:%+v\", rg)\n\n\tsysAcc := s.SystemAccount()\n\tif sysAcc == nil {\n\t\ts.Debugf(\"JetStream cluster detected shutdown processing raft group: %+v\", rg)\n\t\treturn nil, errors.New(\"shutting down\")\n\t}\n\n\t// Check here to see if we have a max HA Assets limit set.\n\tif maxHaAssets := s.getOpts().JetStreamLimits.MaxHAAssets; maxHaAssets > 0 {\n\t\tif s.numRaftNodes() > maxHaAssets {\n\t\t\ts.Warnf(\"Maximum HA Assets limit reached: %d\", maxHaAssets)\n\t\t\t// Since the meta leader assigned this, send a statsz update to them to get them up to date.\n\t\t\tgo s.sendStatszUpdate()\n\t\t\treturn nil, errors.New(\"system limit reached\")\n\t\t}\n\t}\n\n\tstoreDir := filepath.Join(js.config.StoreDir, sysAcc.Name, defaultStoreDirName, rg.Name)\n\tvar store StreamStore\n\tif storage == FileStorage {\n\t\t// If the server is set to sync always, do the same for the Raft log.\n\t\tjs.srv.optsMu.RLock()\n\t\tsyncAlways := js.srv.opts.SyncAlways\n\t\tsyncInterval := js.srv.opts.SyncInterval\n\t\tjs.srv.optsMu.RUnlock()\n\t\tfs, err := newFileStoreWithCreated(\n\t\t\tFileStoreConfig{StoreDir: storeDir, BlockSize: defaultMediumBlockSize, AsyncFlush: false, SyncAlways: syncAlways, SyncInterval: syncInterval, srv: s},\n\t\t\tStreamConfig{Name: rg.Name, Storage: FileStorage, Metadata: labels},\n\t\t\ttime.Now().UTC(),\n\t\t\ts.jsKeyGen(s.getOpts().JetStreamKey, rg.Name),\n\t\t\ts.jsKeyGen(s.getOpts().JetStreamOldKey, rg.Name),\n\t\t)\n\t\tif err != nil {\n\t\t\ts.Errorf(\"Error creating filestore WAL: %v\", err)\n\t\t\treturn nil, err\n\t\t}\n\t\tstore = fs\n\t} else {\n\t\tms, err := newMemStore(&StreamConfig{Name: rg.Name, Storage: MemoryStorage})\n\t\tif err != nil {\n\t\t\ts.Errorf(\"Error creating memstore WAL: %v\", err)\n\t\t\treturn nil, err\n\t\t}\n\t\tstore = ms\n\t}\n\n\tcfg := &RaftConfig{Name: rg.Name, Store: storeDir, Log: store, Track: true}\n\n\tif _, err := readPeerState(storeDir); err != nil {\n\t\ts.bootstrapRaftNode(cfg, rg.Peers, true)\n\t}\n\n\tn, err := s.startRaftNode(accName, cfg, labels)\n\tif err != nil || n == nil {\n\t\ts.Debugf(\"Error creating raft group: %v\", err)\n\t\treturn nil, err\n\t}\n\t// Need JS lock to be held for the assignment to avoid data-race reports\n\trg.node = n\n\t// See if we are preferred and should start campaign immediately.\n\tif n.ID() == rg.Preferred && n.Term() == 0 {\n\t\tn.CampaignImmediately()\n\t}\n\treturn n, nil\n}\n\nfunc (mset *stream) raftGroup() *raftGroup {\n\tif mset == nil {\n\t\treturn nil\n\t}\n\tmset.mu.RLock()\n\tdefer mset.mu.RUnlock()\n\tif mset.sa == nil {\n\t\treturn nil\n\t}\n\treturn mset.sa.Group\n}\n\nfunc (mset *stream) raftNode() RaftNode {\n\tif mset == nil {\n\t\treturn nil\n\t}\n\tmset.mu.RLock()\n\tdefer mset.mu.RUnlock()\n\treturn mset.node\n}\n\nfunc (mset *stream) removeNode() {\n\tmset.mu.Lock()\n\tdefer mset.mu.Unlock()\n\tif n := mset.node; n != nil {\n\t\tn.Delete()\n\t\tmset.node = nil\n\t}\n}\n\n// Helper function to generate peer info.\n// lists and sets for old and new.\nfunc genPeerInfo(peers []string, split int) (newPeers, oldPeers []string, newPeerSet, oldPeerSet map[string]bool) {\n\tnewPeers = peers[split:]\n\toldPeers = peers[:split]\n\tnewPeerSet = make(map[string]bool, len(newPeers))\n\toldPeerSet = make(map[string]bool, len(oldPeers))\n\tfor i, peer := range peers {\n\t\tif i < split {\n\t\t\toldPeerSet[peer] = true\n\t\t} else {\n\t\t\tnewPeerSet[peer] = true\n\t\t}\n\t}\n\treturn\n}\n\n// This will wait for a period of time until all consumers are registered and have\n// their consumer assignments assigned.\n// Should only be called from monitorStream.\nfunc (mset *stream) waitOnConsumerAssignments() {\n\tmset.mu.RLock()\n\ts, js, acc, sa, name, replicas := mset.srv, mset.js, mset.acc, mset.sa, mset.cfg.Name, mset.cfg.Replicas\n\tmset.mu.RUnlock()\n\n\tif s == nil || js == nil || acc == nil || sa == nil {\n\t\treturn\n\t}\n\n\tjs.mu.RLock()\n\tnumExpectedConsumers := len(sa.consumers)\n\tjs.mu.RUnlock()\n\n\t// Max to wait.\n\tconst maxWaitTime = 10 * time.Second\n\tconst sleepTime = 500 * time.Millisecond\n\n\t// Wait up to 10s\n\ttimeout := time.Now().Add(maxWaitTime)\n\tfor time.Now().Before(timeout) {\n\t\tvar numReady int\n\t\tfor _, o := range mset.getConsumers() {\n\t\t\t// Make sure we are registered with our consumer assignment.\n\t\t\tif ca := o.consumerAssignment(); ca != nil {\n\t\t\t\tif replicas > 1 && !o.isMonitorRunning() {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t\tnumReady++\n\t\t\t} else {\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\t// Check if we are good.\n\t\tif numReady >= numExpectedConsumers {\n\t\t\tbreak\n\t\t}\n\n\t\ts.Debugf(\"Waiting for consumers for interest based stream '%s > %s'\", acc.Name, name)\n\t\tselect {\n\t\tcase <-s.quitCh:\n\t\t\treturn\n\t\tcase <-mset.monitorQuitC():\n\t\t\treturn\n\t\tcase <-time.After(sleepTime):\n\t\t}\n\t}\n\n\tif actual := mset.numConsumers(); actual < numExpectedConsumers {\n\t\ts.Warnf(\"All consumers not online for '%s > %s': expected %d but only have %d\", acc.Name, name, numExpectedConsumers, actual)\n\t}\n}\n\n// Monitor our stream node for this stream.\nfunc (js *jetStream) monitorStream(mset *stream, sa *streamAssignment, sendSnapshot bool) {\n\ts, cc := js.server(), js.cluster\n\tdefer s.grWG.Done()\n\tif mset != nil {\n\t\tdefer mset.monitorWg.Done()\n\t}\n\tjs.mu.RLock()\n\tn := sa.Group.node\n\tmeta := cc.meta\n\tjs.mu.RUnlock()\n\n\tif n == nil || meta == nil {\n\t\ts.Warnf(\"No RAFT group for '%s > %s'\", sa.Client.serviceAccount(), sa.Config.Name)\n\t\treturn\n\t}\n\n\t// Make sure only one is running.\n\tif mset != nil {\n\t\tif mset.checkInMonitor() {\n\t\t\treturn\n\t\t}\n\t\tdefer mset.clearMonitorRunning()\n\t}\n\n\t// Make sure to stop the raft group on exit to prevent accidental memory bloat.\n\t// This should be below the checkInMonitor call though to avoid stopping it out\n\t// from underneath the one that is running since it will be the same raft node.\n\tdefer func() {\n\t\t// We might be closing during shutdown, don't pre-emptively stop here since we'll still want to install snapshots.\n\t\tif mset != nil && !mset.closed.Load() {\n\t\t\tn.Stop()\n\t\t}\n\t}()\n\n\tqch, mqch, lch, aq, uch, ourPeerId := n.QuitC(), mset.monitorQuitC(), n.LeadChangeC(), n.ApplyQ(), mset.updateC(), meta.ID()\n\n\ts.Debugf(\"Starting stream monitor for '%s > %s' [%s]\", sa.Client.serviceAccount(), sa.Config.Name, n.Group())\n\tdefer s.Debugf(\"Exiting stream monitor for '%s > %s' [%s]\", sa.Client.serviceAccount(), sa.Config.Name, n.Group())\n\n\t// Make sure we do not leave the apply channel to fill up and block the raft layer.\n\tdefer func() {\n\t\tif n.State() == Closed {\n\t\t\treturn\n\t\t}\n\t\tn.StepDown()\n\t\t// Drain the commit queue...\n\t\taq.drain()\n\t}()\n\n\tconst (\n\t\tcompactInterval = 2 * time.Minute\n\t\tcompactSizeMin  = 8 * 1024 * 1024\n\t\tcompactNumMin   = 65536\n\t)\n\n\t// Spread these out for large numbers on server restart.\n\trci := time.Duration(rand.Int63n(int64(time.Minute)))\n\tt := time.NewTicker(compactInterval + rci)\n\tdefer t.Stop()\n\n\tjs.mu.RLock()\n\tisLeader := cc.isStreamLeader(sa.Client.serviceAccount(), sa.Config.Name)\n\tisRestore := sa.Restore != nil\n\tjs.mu.RUnlock()\n\n\tacc, err := s.LookupAccount(sa.Client.serviceAccount())\n\tif err != nil {\n\t\ts.Warnf(\"Could not retrieve account for stream '%s > %s'\", sa.Client.serviceAccount(), sa.Config.Name)\n\t\treturn\n\t}\n\taccName := acc.GetName()\n\n\t// Used to represent how we can detect a changed state quickly and without representing\n\t// a complete and detailed state which could be costly in terms of memory, cpu and GC.\n\t// This only entails how many messages, and the first and last sequence of the stream.\n\t// This is all that is needed to detect a change, and we can get this from FilteredState()\n\t// with an empty filter.\n\tvar lastState SimpleState\n\n\t// Don't allow the upper layer to install snapshots until we have\n\t// fully recovered from disk.\n\tisRecovering := true\n\n\tdoSnapshot := func() {\n\t\tif mset == nil || isRecovering || isRestore {\n\t\t\treturn\n\t\t}\n\n\t\t// Before we actually calculate the detailed state and encode it, let's check the\n\t\t// simple state to detect any changes.\n\t\tcurState := mset.store.FilteredState(0, _EMPTY_)\n\n\t\t// If the state hasn't changed but the log has gone way over\n\t\t// the compaction size then we will want to compact anyway.\n\t\t// This shouldn't happen for streams like it can for pull\n\t\t// consumers on idle streams but better to be safe than sorry!\n\t\tne, nb := n.Size()\n\t\tif curState == lastState && ne < compactNumMin && nb < compactSizeMin {\n\t\t\treturn\n\t\t}\n\n\t\tif err := n.InstallSnapshot(mset.stateSnapshot()); err == nil {\n\t\t\tlastState = curState\n\t\t} else if err != errNoSnapAvailable && err != errNodeClosed && err != errCatchupsRunning {\n\t\t\ts.RateLimitWarnf(\"Failed to install snapshot for '%s > %s' [%s]: %v\", mset.acc.Name, mset.name(), n.Group(), err)\n\t\t}\n\t}\n\n\t// We will establish a restoreDoneCh no matter what. Will never be triggered unless\n\t// we replace with the restore chan.\n\trestoreDoneCh := make(<-chan error)\n\n\t// For migration tracking.\n\tvar mmt *time.Ticker\n\tvar mmtc <-chan time.Time\n\n\tstartMigrationMonitoring := func() {\n\t\tif mmt == nil {\n\t\t\tmmt = time.NewTicker(500 * time.Millisecond)\n\t\t\tmmtc = mmt.C\n\t\t}\n\t}\n\n\tstopMigrationMonitoring := func() {\n\t\tif mmt != nil {\n\t\t\tmmt.Stop()\n\t\t\tmmt, mmtc = nil, nil\n\t\t}\n\t}\n\tdefer stopMigrationMonitoring()\n\n\t// This is to optionally track when we are ready as a non-leader for direct access participation.\n\t// Either direct or if we are a direct mirror, or both.\n\tvar dat *time.Ticker\n\tvar datc <-chan time.Time\n\n\tstartDirectAccessMonitoring := func() {\n\t\tif dat == nil {\n\t\t\tdat = time.NewTicker(2 * time.Second)\n\t\t\tdatc = dat.C\n\t\t}\n\t}\n\n\tstopDirectMonitoring := func() {\n\t\tif dat != nil {\n\t\t\tdat.Stop()\n\t\t\tdat, datc = nil, nil\n\t\t}\n\t}\n\tdefer stopDirectMonitoring()\n\n\t// For checking interest state if applicable.\n\tvar cist *time.Ticker\n\tvar cistc <-chan time.Time\n\n\tcheckInterestInterval := checkInterestStateT + time.Duration(rand.Intn(checkInterestStateJ))*time.Second\n\n\tif mset != nil && mset.isInterestRetention() {\n\t\t// Wait on our consumers to be assigned and running before proceeding.\n\t\t// This can become important when a server has lots of assets\n\t\t// since we process streams first then consumers as an asset class.\n\t\tmset.waitOnConsumerAssignments()\n\t\t// Setup our periodic check here. We will check once we have restored right away.\n\t\tcist = time.NewTicker(checkInterestInterval)\n\t\tcistc = cist.C\n\t}\n\n\t// This is triggered during a scale up from R1 to clustered mode. We need the new followers to catchup,\n\t// similar to how we trigger the catchup mechanism post a backup/restore.\n\t// We can arrive here NOT being the leader, so we send the snapshot only if we are, and in this case\n\t// reset the notion that we need to send the snapshot. If we are not, then the first time the server\n\t// will switch to leader (in the loop below), we will send the snapshot.\n\tif sendSnapshot && isLeader && mset != nil && n != nil && !isRecovering {\n\t\tn.SendSnapshot(mset.stateSnapshot())\n\t\tsendSnapshot = false\n\t}\n\n\tfor {\n\t\tselect {\n\t\tcase <-s.quitCh:\n\t\t\t// Server shutting down, but we might receive this before qch, so try to snapshot.\n\t\t\tdoSnapshot()\n\t\t\treturn\n\t\tcase <-mqch:\n\t\t\t// Clean signal from shutdown routine so do best effort attempt to snapshot.\n\t\t\tdoSnapshot()\n\t\t\treturn\n\t\tcase <-qch:\n\t\t\t// Clean signal from shutdown routine so do best effort attempt to snapshot.\n\t\t\tdoSnapshot()\n\t\t\treturn\n\t\tcase <-aq.ch:\n\t\t\tvar ne, nb uint64\n\t\t\t// If we bump clfs we will want to write out snapshot if within our time window.\n\t\t\tpclfs := mset.getCLFS()\n\n\t\t\tces := aq.pop()\n\t\t\tfor _, ce := range ces {\n\t\t\t\t// No special processing needed for when we are caught up on restart.\n\t\t\t\tif ce == nil {\n\t\t\t\t\tisRecovering = false\n\t\t\t\t\t// If we are interest based make sure to check consumers if interest retention policy.\n\t\t\t\t\t// This is to make sure we process any outstanding acks from all consumers.\n\t\t\t\t\tif mset != nil && mset.isInterestRetention() {\n\t\t\t\t\t\tfire := time.Duration(rand.Intn(5)+5) * time.Second\n\t\t\t\t\t\ttime.AfterFunc(fire, mset.checkInterestState)\n\t\t\t\t\t}\n\t\t\t\t\t// If we became leader during this time and we need to send a snapshot to our\n\t\t\t\t\t// followers, i.e. as a result of a scale-up from R1, do it now.\n\t\t\t\t\tif sendSnapshot && isLeader && mset != nil && n != nil {\n\t\t\t\t\t\tn.SendSnapshot(mset.stateSnapshot())\n\t\t\t\t\t\tsendSnapshot = false\n\t\t\t\t\t}\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\n\t\t\t\t// Apply our entries.\n\t\t\t\tif err := js.applyStreamEntries(mset, ce, isRecovering); err == nil {\n\t\t\t\t\t// Update our applied.\n\t\t\t\t\tne, nb = n.Applied(ce.Index)\n\t\t\t\t\tce.ReturnToPool()\n\t\t\t\t} else {\n\t\t\t\t\t// Our stream was closed out from underneath of us, simply return here.\n\t\t\t\t\tif err == errStreamClosed || err == errCatchupStreamStopped || err == ErrServerNotRunning {\n\t\t\t\t\t\taq.recycle(&ces)\n\t\t\t\t\t\treturn\n\t\t\t\t\t}\n\t\t\t\t\ts.Warnf(\"Error applying entries to '%s > %s': %v\", accName, sa.Config.Name, err)\n\t\t\t\t\tif isClusterResetErr(err) {\n\t\t\t\t\t\tif mset.isMirror() && mset.IsLeader() {\n\t\t\t\t\t\t\tmset.retryMirrorConsumer()\n\t\t\t\t\t\t\tcontinue\n\t\t\t\t\t\t}\n\t\t\t\t\t\t// We will attempt to reset our cluster state.\n\t\t\t\t\t\tif mset.resetClusteredState(err) {\n\t\t\t\t\t\t\taq.recycle(&ces)\n\t\t\t\t\t\t\treturn\n\t\t\t\t\t\t}\n\t\t\t\t\t} else if isOutOfSpaceErr(err) {\n\t\t\t\t\t\t// If applicable this will tear all of this down, but don't assume so and return.\n\t\t\t\t\t\ts.handleOutOfSpace(mset)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\taq.recycle(&ces)\n\n\t\t\t// Check about snapshotting\n\t\t\t// If we have at least min entries to compact, go ahead and try to snapshot/compact.\n\t\t\tif ne >= compactNumMin || nb > compactSizeMin || mset.getCLFS() > pclfs {\n\t\t\t\tdoSnapshot()\n\t\t\t}\n\n\t\tcase isLeader = <-lch:\n\t\t\tif isLeader {\n\t\t\t\tif mset != nil && n != nil && sendSnapshot && !isRecovering {\n\t\t\t\t\t// If we *are* recovering at the time then this will get done when the apply queue\n\t\t\t\t\t// handles the nil guard to show the catchup ended.\n\t\t\t\t\tn.SendSnapshot(mset.stateSnapshot())\n\t\t\t\t\tsendSnapshot = false\n\t\t\t\t}\n\t\t\t\tif isRestore {\n\t\t\t\t\tacc, _ := s.LookupAccount(sa.Client.serviceAccount())\n\t\t\t\t\trestoreDoneCh = s.processStreamRestore(sa.Client, acc, sa.Config, _EMPTY_, sa.Reply, _EMPTY_)\n\t\t\t\t\tcontinue\n\t\t\t\t} else if n != nil && n.NeedSnapshot() {\n\t\t\t\t\tdoSnapshot()\n\t\t\t\t}\n\t\t\t\t// Always cancel if this was running.\n\t\t\t\tstopDirectMonitoring()\n\n\t\t\t} else if !n.Leaderless() {\n\t\t\t\tjs.setStreamAssignmentRecovering(sa)\n\t\t\t}\n\n\t\t\t// Process our leader change.\n\t\t\tjs.processStreamLeaderChange(mset, isLeader)\n\n\t\t\t// We may receive a leader change after the stream assignment which would cancel us\n\t\t\t// monitoring for this closely. So re-assess our state here as well.\n\t\t\t// Or the old leader is no longer part of the set and transferred leadership\n\t\t\t// for this leader to resume with removal\n\t\t\tmigrating := mset.isMigrating()\n\n\t\t\t// Check for migrations here. We set the state on the stream assignment update below.\n\t\t\tif isLeader && migrating {\n\t\t\t\tstartMigrationMonitoring()\n\t\t\t}\n\n\t\t\t// Here we are checking if we are not the leader but we have been asked to allow\n\t\t\t// direct access. We now allow non-leaders to participate in the queue group.\n\t\t\tif !isLeader && mset != nil {\n\t\t\t\tmset.mu.RLock()\n\t\t\t\tad, md := mset.cfg.AllowDirect, mset.cfg.MirrorDirect\n\t\t\t\tmset.mu.RUnlock()\n\t\t\t\tif ad || md {\n\t\t\t\t\tstartDirectAccessMonitoring()\n\t\t\t\t}\n\t\t\t}\n\n\t\tcase <-cistc:\n\t\t\tcist.Reset(checkInterestInterval)\n\t\t\t// We may be adjusting some things with consumers so do this in its own go routine.\n\t\t\tgo mset.checkInterestState()\n\n\t\tcase <-datc:\n\t\t\tif mset == nil || isRecovering {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\t// If we are leader we can stop, we know this is setup now.\n\t\t\tif isLeader {\n\t\t\t\tstopDirectMonitoring()\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tmset.mu.Lock()\n\t\t\tad, md, current := mset.cfg.AllowDirect, mset.cfg.MirrorDirect, mset.isCurrent()\n\t\t\tif !current {\n\t\t\t\tconst syncThreshold = 90.0\n\t\t\t\t// We are not current, but current means exactly caught up. Under heavy publish\n\t\t\t\t// loads we may never reach this, so check if we are within 90% caught up.\n\t\t\t\t_, c, a := mset.node.Progress()\n\t\t\t\tif c == 0 {\n\t\t\t\t\tmset.mu.Unlock()\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tif p := float64(a) / float64(c) * 100.0; p < syncThreshold {\n\t\t\t\t\tmset.mu.Unlock()\n\t\t\t\t\tcontinue\n\t\t\t\t} else {\n\t\t\t\t\ts.Debugf(\"Stream '%s > %s' enabling direct gets at %.0f%% synchronized\",\n\t\t\t\t\t\tsa.Client.serviceAccount(), sa.Config.Name, p)\n\t\t\t\t}\n\t\t\t}\n\t\t\t// We are current, cancel monitoring and create the direct subs as needed.\n\t\t\tif ad {\n\t\t\t\tmset.subscribeToDirect()\n\t\t\t}\n\t\t\tif md {\n\t\t\t\tmset.subscribeToMirrorDirect()\n\t\t\t}\n\t\t\tmset.mu.Unlock()\n\t\t\t// Stop direct monitoring.\n\t\t\tstopDirectMonitoring()\n\n\t\tcase <-t.C:\n\t\t\tdoSnapshot()\n\n\t\tcase <-uch:\n\t\t\t// keep stream assignment current\n\t\t\tsa = mset.streamAssignment()\n\n\t\t\t// We get this when we have a new stream assignment caused by an update.\n\t\t\t// We want to know if we are migrating.\n\t\t\tif migrating := mset.isMigrating(); migrating {\n\t\t\t\tif isLeader && mmtc == nil {\n\t\t\t\t\tstartMigrationMonitoring()\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tstopMigrationMonitoring()\n\t\t\t}\n\t\tcase <-mmtc:\n\t\t\tif !isLeader {\n\t\t\t\t// We are no longer leader, so not our job.\n\t\t\t\tstopMigrationMonitoring()\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// Check to see where we are..\n\t\t\trg := mset.raftGroup()\n\n\t\t\t// Track the new peers and check the ones that are current.\n\t\t\tmset.mu.RLock()\n\t\t\treplicas := mset.cfg.Replicas\n\t\t\tmset.mu.RUnlock()\n\t\t\tif len(rg.Peers) <= replicas {\n\t\t\t\t// Migration no longer happening, so not our job anymore\n\t\t\t\tstopMigrationMonitoring()\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// Make sure we have correct cluster information on the other peers.\n\t\t\tci := js.clusterInfo(rg)\n\t\t\tmset.checkClusterInfo(ci)\n\n\t\t\tnewPeers, oldPeers, newPeerSet, oldPeerSet := genPeerInfo(rg.Peers, len(rg.Peers)-replicas)\n\n\t\t\t// If we are part of the new peerset and we have been passed the baton.\n\t\t\t// We will handle scale down.\n\t\t\tif newPeerSet[ourPeerId] {\n\t\t\t\t// First need to check on any consumers and make sure they have moved properly before scaling down ourselves.\n\t\t\t\tjs.mu.RLock()\n\t\t\t\tvar needToWait bool\n\t\t\t\tfor name, c := range sa.consumers {\n\t\t\t\t\tfor _, peer := range c.Group.Peers {\n\t\t\t\t\t\t// If we have peers still in the old set block.\n\t\t\t\t\t\tif oldPeerSet[peer] {\n\t\t\t\t\t\t\ts.Debugf(\"Scale down of '%s > %s' blocked by consumer '%s'\", accName, sa.Config.Name, name)\n\t\t\t\t\t\t\tneedToWait = true\n\t\t\t\t\t\t\tbreak\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tif needToWait {\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tjs.mu.RUnlock()\n\t\t\t\tif needToWait {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\n\t\t\t\t// We are good to go, can scale down here.\n\t\t\t\tfor _, p := range oldPeers {\n\t\t\t\t\tn.ProposeRemovePeer(p)\n\t\t\t\t}\n\n\t\t\t\tcsa := sa.copyGroup()\n\t\t\t\tcsa.Group.Peers = newPeers\n\t\t\t\tcsa.Group.Preferred = ourPeerId\n\t\t\t\tcsa.Group.Cluster = s.cachedClusterName()\n\t\t\t\tcc.meta.ForwardProposal(encodeUpdateStreamAssignment(csa))\n\t\t\t\ts.Noticef(\"Scaling down '%s > %s' to %+v\", accName, sa.Config.Name, s.peerSetToNames(newPeers))\n\t\t\t} else {\n\t\t\t\t// We are the old leader here, from the original peer set.\n\t\t\t\t// We are simply waiting on the new peerset to be caught up so we can transfer leadership.\n\t\t\t\tvar newLeaderPeer, newLeader string\n\t\t\t\tneededCurrent, current := replicas/2+1, 0\n\n\t\t\t\tfor _, r := range ci.Replicas {\n\t\t\t\t\tif r.Current && newPeerSet[r.Peer] {\n\t\t\t\t\t\tcurrent++\n\t\t\t\t\t\tif newLeader == _EMPTY_ {\n\t\t\t\t\t\t\tnewLeaderPeer, newLeader = r.Peer, r.Name\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\t// Check if we have a quorom.\n\t\t\t\tif current >= neededCurrent {\n\t\t\t\t\ts.Noticef(\"Transfer of stream leader for '%s > %s' to '%s'\", accName, sa.Config.Name, newLeader)\n\t\t\t\t\tn.ProposeKnownPeers(newPeers)\n\t\t\t\t\tn.StepDown(newLeaderPeer)\n\t\t\t\t}\n\t\t\t}\n\n\t\tcase err := <-restoreDoneCh:\n\t\t\t// We have completed a restore from snapshot on this server. The stream assignment has\n\t\t\t// already been assigned but the replicas will need to catch up out of band. Consumers\n\t\t\t// will need to be assigned by forwarding the proposal and stamping the initial state.\n\t\t\ts.Debugf(\"Stream restore for '%s > %s' completed\", sa.Client.serviceAccount(), sa.Config.Name)\n\t\t\tif err != nil {\n\t\t\t\ts.Debugf(\"Stream restore failed: %v\", err)\n\t\t\t}\n\t\t\tisRestore = false\n\t\t\tsa.Restore = nil\n\t\t\t// If we were successful lookup up our stream now.\n\t\t\tif err == nil {\n\t\t\t\tif mset, err = acc.lookupStream(sa.Config.Name); mset != nil {\n\t\t\t\t\tmset.monitorWg.Add(1)\n\t\t\t\t\tdefer mset.monitorWg.Done()\n\t\t\t\t\tmset.setStreamAssignment(sa)\n\t\t\t\t\t// Make sure to update our updateC which would have been nil.\n\t\t\t\t\tuch = mset.updateC()\n\t\t\t\t\t// Also update our mqch\n\t\t\t\t\tmqch = mset.monitorQuitC()\n\t\t\t\t\t// Setup a periodic check here if we are interest based as well.\n\t\t\t\t\tif mset.isInterestRetention() {\n\t\t\t\t\t\tcist = time.NewTicker(checkInterestInterval)\n\t\t\t\t\t\tcistc = cist.C\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tif err != nil {\n\t\t\t\tif mset != nil {\n\t\t\t\t\tmset.delete()\n\t\t\t\t}\n\t\t\t\tjs.mu.Lock()\n\t\t\t\tsa.err = err\n\t\t\t\tif n != nil {\n\t\t\t\t\tn.Delete()\n\t\t\t\t}\n\t\t\t\tresult := &streamAssignmentResult{\n\t\t\t\t\tAccount: sa.Client.serviceAccount(),\n\t\t\t\t\tStream:  sa.Config.Name,\n\t\t\t\t\tRestore: &JSApiStreamRestoreResponse{ApiResponse: ApiResponse{Type: JSApiStreamRestoreResponseType}},\n\t\t\t\t}\n\t\t\t\tresult.Restore.Error = NewJSStreamAssignmentError(err, Unless(err))\n\t\t\t\tjs.mu.Unlock()\n\t\t\t\t// Send response to the metadata leader. They will forward to the user as needed.\n\t\t\t\ts.sendInternalMsgLocked(streamAssignmentSubj, _EMPTY_, nil, result)\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\tif !isLeader {\n\t\t\t\tpanic(\"Finished restore but not leader\")\n\t\t\t}\n\t\t\t// Trigger the stream followers to catchup.\n\t\t\tif n = mset.raftNode(); n != nil {\n\t\t\t\tn.SendSnapshot(mset.stateSnapshot())\n\t\t\t}\n\t\t\tjs.processStreamLeaderChange(mset, isLeader)\n\n\t\t\t// Check to see if we have restored consumers here.\n\t\t\t// These are not currently assigned so we will need to do so here.\n\t\t\tif consumers := mset.getPublicConsumers(); len(consumers) > 0 {\n\t\t\t\tfor _, o := range consumers {\n\t\t\t\t\tname, cfg := o.String(), o.config()\n\t\t\t\t\trg := cc.createGroupForConsumer(&cfg, sa)\n\t\t\t\t\t// Pick a preferred leader.\n\t\t\t\t\trg.setPreferred()\n\n\t\t\t\t\t// Place our initial state here as well for assignment distribution.\n\t\t\t\t\tstate, _ := o.store.State()\n\t\t\t\t\tca := &consumerAssignment{\n\t\t\t\t\t\tGroup:   rg,\n\t\t\t\t\t\tStream:  sa.Config.Name,\n\t\t\t\t\t\tName:    name,\n\t\t\t\t\t\tConfig:  &cfg,\n\t\t\t\t\t\tClient:  sa.Client,\n\t\t\t\t\t\tCreated: o.createdTime(),\n\t\t\t\t\t\tState:   state,\n\t\t\t\t\t}\n\n\t\t\t\t\t// We make these compressed in case state is complex.\n\t\t\t\t\taddEntry := encodeAddConsumerAssignmentCompressed(ca)\n\t\t\t\t\tcc.meta.ForwardProposal(addEntry)\n\n\t\t\t\t\t// Check to make sure we see the assignment.\n\t\t\t\t\tgo func() {\n\t\t\t\t\t\tticker := time.NewTicker(time.Second)\n\t\t\t\t\t\tdefer ticker.Stop()\n\t\t\t\t\t\tfor range ticker.C {\n\t\t\t\t\t\t\tjs.mu.RLock()\n\t\t\t\t\t\t\tca, meta := js.consumerAssignment(ca.Client.serviceAccount(), sa.Config.Name, name), cc.meta\n\t\t\t\t\t\t\tjs.mu.RUnlock()\n\t\t\t\t\t\t\tif ca == nil {\n\t\t\t\t\t\t\t\ts.Warnf(\"Consumer assignment has not been assigned, retrying\")\n\t\t\t\t\t\t\t\tif meta != nil {\n\t\t\t\t\t\t\t\t\tmeta.ForwardProposal(addEntry)\n\t\t\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\t\t\treturn\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\t\treturn\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}()\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n\n// Determine if we are migrating\nfunc (mset *stream) isMigrating() bool {\n\tif mset == nil {\n\t\treturn false\n\t}\n\n\tmset.mu.RLock()\n\tjs, sa := mset.js, mset.sa\n\tmset.mu.RUnlock()\n\n\tjs.mu.RLock()\n\tdefer js.mu.RUnlock()\n\n\t// During migration we will always be R>1, even when we start R1.\n\t// So if we do not have a group or node we no we are not migrating.\n\tif sa == nil || sa.Group == nil || sa.Group.node == nil {\n\t\treturn false\n\t}\n\t// The sign of migration is if our group peer count != configured replica count.\n\tif sa.Config.Replicas == len(sa.Group.Peers) {\n\t\treturn false\n\t}\n\treturn true\n}\n\n// resetClusteredState is called when a clustered stream had an error (e.g sequence mismatch, bad snapshot) and needs to be reset.\nfunc (mset *stream) resetClusteredState(err error) bool {\n\tmset.mu.RLock()\n\ts, js, jsa, sa, acc, node := mset.srv, mset.js, mset.jsa, mset.sa, mset.acc, mset.node\n\tstype, tierName, replicas := mset.cfg.Storage, mset.tier, mset.cfg.Replicas\n\tmset.mu.RUnlock()\n\n\t// Stepdown regardless if we are the leader here.\n\tif node != nil {\n\t\tnode.StepDown()\n\t}\n\n\t// If we detect we are shutting down just return.\n\tif js != nil && js.isShuttingDown() {\n\t\ts.Debugf(\"Will not reset stream, JetStream shutting down\")\n\t\treturn false\n\t}\n\n\t// Server\n\tif js.limitsExceeded(stype) {\n\t\ts.Warnf(\"Will not reset stream, server resources exceeded\")\n\t\treturn false\n\t}\n\n\t// Account\n\tif exceeded, _ := jsa.limitsExceeded(stype, tierName, replicas); exceeded {\n\t\ts.Warnf(\"stream '%s > %s' errored, account resources exceeded\", acc, mset.name())\n\t\treturn false\n\t}\n\n\tif node != nil {\n\t\tif errors.Is(err, errCatchupAbortedNoLeader) || err == errCatchupTooManyRetries {\n\t\t\t// Don't delete all state, could've just been temporarily unable to reach the leader.\n\t\t\tnode.Stop()\n\t\t} else {\n\t\t\t// We delete our raft state. Will recreate.\n\t\t\tnode.Delete()\n\t\t}\n\t}\n\n\t// Preserve our current state and messages unless we have a first sequence mismatch.\n\tshouldDelete := err == errFirstSequenceMismatch\n\n\t// Need to do the rest in a separate Go routine.\n\tgo func() {\n\t\tmset.monitorWg.Wait()\n\t\tmset.resetAndWaitOnConsumers()\n\t\t// Stop our stream.\n\t\tmset.stop(shouldDelete, false)\n\n\t\tif sa != nil {\n\t\t\tjs.mu.Lock()\n\t\t\tif js.shuttingDown {\n\t\t\t\tjs.mu.Unlock()\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\ts.Warnf(\"Resetting stream cluster state for '%s > %s'\", sa.Client.serviceAccount(), sa.Config.Name)\n\t\t\t// Mark stream assignment as resetting, so we don't double-account reserved resources.\n\t\t\t// But only if we're not also releasing the resources as part of the delete.\n\t\t\tsa.resetting = !shouldDelete\n\t\t\t// Now wipe groups from assignments.\n\t\t\tsa.Group.node = nil\n\t\t\tvar consumers []*consumerAssignment\n\t\t\tif cc := js.cluster; cc != nil && cc.meta != nil {\n\t\t\t\tourID := cc.meta.ID()\n\t\t\t\tfor _, ca := range sa.consumers {\n\t\t\t\t\tif rg := ca.Group; rg != nil && rg.isMember(ourID) {\n\t\t\t\t\t\trg.node = nil // Erase group raft/node state.\n\t\t\t\t\t\tconsumers = append(consumers, ca)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tjs.mu.Unlock()\n\n\t\t\t// This will reset the stream and consumers.\n\t\t\t// Reset stream.\n\t\t\tjs.processClusterCreateStream(acc, sa)\n\t\t\t// Reset consumers.\n\t\t\tfor _, ca := range consumers {\n\t\t\t\tjs.processClusterCreateConsumer(ca, nil, false)\n\t\t\t}\n\t\t}\n\t}()\n\n\treturn true\n}\n\nfunc isControlHdr(hdr []byte) bool {\n\treturn bytes.HasPrefix(hdr, []byte(\"NATS/1.0 100 \"))\n}\n\n// Apply our stream entries.\nfunc (js *jetStream) applyStreamEntries(mset *stream, ce *CommittedEntry, isRecovering bool) error {\n\tfor _, e := range ce.Entries {\n\t\tif e.Type == EntryNormal {\n\t\t\tbuf, op := e.Data, entryOp(e.Data[0])\n\t\t\tswitch op {\n\t\t\tcase streamMsgOp, compressedStreamMsgOp:\n\t\t\t\tif mset == nil {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\ts := js.srv\n\n\t\t\t\tmbuf := buf[1:]\n\t\t\t\tif op == compressedStreamMsgOp {\n\t\t\t\t\tvar err error\n\t\t\t\t\tmbuf, err = s2.Decode(nil, mbuf)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\tpanic(err.Error())\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\tsubject, reply, hdr, msg, lseq, ts, sourced, err := decodeStreamMsg(mbuf)\n\t\t\t\tif err != nil {\n\t\t\t\t\tif node := mset.raftNode(); node != nil {\n\t\t\t\t\t\ts.Errorf(\"JetStream cluster could not decode stream msg for '%s > %s' [%s]\",\n\t\t\t\t\t\t\tmset.account(), mset.name(), node.Group())\n\t\t\t\t\t}\n\t\t\t\t\tpanic(err.Error())\n\t\t\t\t}\n\n\t\t\t\t// Check for flowcontrol here.\n\t\t\t\tif len(msg) == 0 && len(hdr) > 0 && reply != _EMPTY_ && isControlHdr(hdr) {\n\t\t\t\t\tif !isRecovering {\n\t\t\t\t\t\tmset.sendFlowControlReply(reply)\n\t\t\t\t\t}\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\n\t\t\t\t// Grab last sequence and CLFS.\n\t\t\t\tlast, clfs := mset.lastSeqAndCLFS()\n\n\t\t\t\t// We can skip if we know this is less than what we already have.\n\t\t\t\tif lseq-clfs < last {\n\t\t\t\t\ts.Debugf(\"Apply stream entries for '%s > %s' skipping message with sequence %d with last of %d\",\n\t\t\t\t\t\tmset.account(), mset.name(), lseq+1-clfs, last)\n\t\t\t\t\tmset.mu.Lock()\n\t\t\t\t\t// Check for any preAcks in case we are interest based.\n\t\t\t\t\tmset.clearAllPreAcks(lseq + 1 - clfs)\n\t\t\t\t\tmset.mu.Unlock()\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\n\t\t\t\t// Skip by hand here since first msg special case.\n\t\t\t\t// Reason is sequence is unsigned and for lseq being 0\n\t\t\t\t// the lseq under stream would have to be -1.\n\t\t\t\tif lseq == 0 && last != 0 {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\n\t\t\t\t// Messages to be skipped have no subject or timestamp or msg or hdr.\n\t\t\t\tif subject == _EMPTY_ && ts == 0 && len(msg) == 0 && len(hdr) == 0 {\n\t\t\t\t\t// Skip and update our lseq.\n\t\t\t\t\tlast := mset.store.SkipMsg()\n\t\t\t\t\tmset.mu.Lock()\n\t\t\t\t\tmset.setLastSeq(last)\n\t\t\t\t\tmset.clearAllPreAcks(last)\n\t\t\t\t\tmset.mu.Unlock()\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\n\t\t\t\tvar mt *msgTrace\n\t\t\t\t// If not recovering, see if we find a message trace object for this\n\t\t\t\t// sequence. Only the leader that has proposed this entry will have\n\t\t\t\t// stored the trace info.\n\t\t\t\tif !isRecovering {\n\t\t\t\t\tmt = mset.getAndDeleteMsgTrace(lseq)\n\t\t\t\t}\n\t\t\t\t// Process the actual message here.\n\t\t\t\terr = mset.processJetStreamMsg(subject, reply, hdr, msg, lseq, ts, mt, sourced)\n\n\t\t\t\t// If we have inflight make sure to clear after processing.\n\t\t\t\t// TODO(dlc) - technically check on inflight != nil could cause datarace.\n\t\t\t\t// But do not want to acquire lock since tracking this will be rare.\n\t\t\t\tif mset.inflight != nil {\n\t\t\t\t\tmset.clMu.Lock()\n\t\t\t\t\tdelete(mset.inflight, lseq)\n\t\t\t\t\tmset.clMu.Unlock()\n\t\t\t\t}\n\n\t\t\t\t// Clear expected per subject state after processing.\n\t\t\t\tif mset.expectedPerSubjectSequence != nil {\n\t\t\t\t\tmset.clMu.Lock()\n\t\t\t\t\tif subj, found := mset.expectedPerSubjectSequence[lseq]; found {\n\t\t\t\t\t\tdelete(mset.expectedPerSubjectSequence, lseq)\n\t\t\t\t\t\tdelete(mset.expectedPerSubjectInProcess, subj)\n\t\t\t\t\t}\n\t\t\t\t\tmset.clMu.Unlock()\n\t\t\t\t}\n\n\t\t\t\tif err != nil {\n\t\t\t\t\tif err == errLastSeqMismatch {\n\n\t\t\t\t\t\tvar state StreamState\n\t\t\t\t\t\tmset.store.FastState(&state)\n\n\t\t\t\t\t\t// If we have no msgs and the other side is delivering us a sequence past where we\n\t\t\t\t\t\t// should be reset. This is possible if the other side has a stale snapshot and no longer\n\t\t\t\t\t\t// has those messages. So compact and retry to reset.\n\t\t\t\t\t\tif state.Msgs == 0 {\n\t\t\t\t\t\t\tmset.store.Compact(lseq + 1)\n\t\t\t\t\t\t\t// Retry\n\t\t\t\t\t\t\terr = mset.processJetStreamMsg(subject, reply, hdr, msg, lseq, ts, mt, sourced)\n\t\t\t\t\t\t}\n\t\t\t\t\t\t// FIXME(dlc) - We could just run a catchup with a request defining the span between what we expected\n\t\t\t\t\t\t// and what we got.\n\t\t\t\t\t}\n\n\t\t\t\t\t// Only return in place if we are going to reset our stream or we are out of space, or we are closed.\n\t\t\t\t\tif isClusterResetErr(err) || isOutOfSpaceErr(err) || err == errStreamClosed {\n\t\t\t\t\t\treturn err\n\t\t\t\t\t}\n\t\t\t\t\ts.Debugf(\"Apply stream entries for '%s > %s' got error processing message: %v\",\n\t\t\t\t\t\tmset.account(), mset.name(), err)\n\t\t\t\t}\n\n\t\t\tcase deleteMsgOp:\n\t\t\t\tmd, err := decodeMsgDelete(buf[1:])\n\t\t\t\tif err != nil {\n\t\t\t\t\tif node := mset.raftNode(); node != nil {\n\t\t\t\t\t\ts := js.srv\n\t\t\t\t\t\ts.Errorf(\"JetStream cluster could not decode delete msg for '%s > %s' [%s]\",\n\t\t\t\t\t\t\tmset.account(), mset.name(), node.Group())\n\t\t\t\t\t}\n\t\t\t\t\tpanic(err.Error())\n\t\t\t\t}\n\t\t\t\ts, cc := js.server(), js.cluster\n\n\t\t\t\tvar removed bool\n\t\t\t\tif md.NoErase {\n\t\t\t\t\tremoved, err = mset.removeMsg(md.Seq)\n\t\t\t\t} else {\n\t\t\t\t\tremoved, err = mset.eraseMsg(md.Seq)\n\t\t\t\t}\n\n\t\t\t\t// Cluster reset error.\n\t\t\t\tif err == ErrStoreEOF {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\n\t\t\t\tif err != nil && !isRecovering {\n\t\t\t\t\ts.Debugf(\"JetStream cluster failed to delete stream msg %d from '%s > %s': %v\",\n\t\t\t\t\t\tmd.Seq, md.Client.serviceAccount(), md.Stream, err)\n\t\t\t\t}\n\n\t\t\t\tjs.mu.RLock()\n\t\t\t\tisLeader := cc.isStreamLeader(md.Client.serviceAccount(), md.Stream)\n\t\t\t\tjs.mu.RUnlock()\n\n\t\t\t\tif isLeader && !isRecovering {\n\t\t\t\t\tvar resp = JSApiMsgDeleteResponse{ApiResponse: ApiResponse{Type: JSApiMsgDeleteResponseType}}\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\tresp.Error = NewJSStreamMsgDeleteFailedError(err, Unless(err))\n\t\t\t\t\t\ts.sendAPIErrResponse(md.Client, mset.account(), md.Subject, md.Reply, _EMPTY_, s.jsonResponse(resp))\n\t\t\t\t\t} else if !removed {\n\t\t\t\t\t\tresp.Error = NewJSSequenceNotFoundError(md.Seq)\n\t\t\t\t\t\ts.sendAPIErrResponse(md.Client, mset.account(), md.Subject, md.Reply, _EMPTY_, s.jsonResponse(resp))\n\t\t\t\t\t} else {\n\t\t\t\t\t\tresp.Success = true\n\t\t\t\t\t\ts.sendAPIResponse(md.Client, mset.account(), md.Subject, md.Reply, _EMPTY_, s.jsonResponse(resp))\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\tcase purgeStreamOp:\n\t\t\t\tsp, err := decodeStreamPurge(buf[1:])\n\t\t\t\tif err != nil {\n\t\t\t\t\tif node := mset.raftNode(); node != nil {\n\t\t\t\t\t\ts := js.srv\n\t\t\t\t\t\ts.Errorf(\"JetStream cluster could not decode purge msg for '%s > %s' [%s]\",\n\t\t\t\t\t\t\tmset.account(), mset.name(), node.Group())\n\t\t\t\t\t}\n\t\t\t\t\tpanic(err.Error())\n\t\t\t\t}\n\t\t\t\t// If no explicit request, fill in with leader stamped last sequence to protect ourselves on replay during server start.\n\t\t\t\tif sp.Request == nil || sp.Request.Sequence == 0 {\n\t\t\t\t\tpurgeSeq := sp.LastSeq + 1\n\t\t\t\t\tif sp.Request == nil {\n\t\t\t\t\t\tsp.Request = &JSApiStreamPurgeRequest{Sequence: purgeSeq}\n\t\t\t\t\t} else if sp.Request.Keep == 0 {\n\t\t\t\t\t\tsp.Request.Sequence = purgeSeq\n\t\t\t\t\t} else if isRecovering {\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\ts := js.server()\n\t\t\t\tpurged, err := mset.purge(sp.Request)\n\t\t\t\tif err != nil {\n\t\t\t\t\ts.Warnf(\"JetStream cluster failed to purge stream %q for account %q: %v\", sp.Stream, sp.Client.serviceAccount(), err)\n\t\t\t\t}\n\n\t\t\t\tjs.mu.RLock()\n\t\t\t\tisLeader := js.cluster.isStreamLeader(sp.Client.serviceAccount(), sp.Stream)\n\t\t\t\tjs.mu.RUnlock()\n\n\t\t\t\tif isLeader && !isRecovering {\n\t\t\t\t\tvar resp = JSApiStreamPurgeResponse{ApiResponse: ApiResponse{Type: JSApiStreamPurgeResponseType}}\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\tresp.Error = NewJSStreamGeneralError(err, Unless(err))\n\t\t\t\t\t\ts.sendAPIErrResponse(sp.Client, mset.account(), sp.Subject, sp.Reply, _EMPTY_, s.jsonResponse(resp))\n\t\t\t\t\t} else {\n\t\t\t\t\t\tresp.Purged = purged\n\t\t\t\t\t\tresp.Success = true\n\t\t\t\t\t\ts.sendAPIResponse(sp.Client, mset.account(), sp.Subject, sp.Reply, _EMPTY_, s.jsonResponse(resp))\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\tdefault:\n\t\t\t\tpanic(fmt.Sprintf(\"JetStream Cluster Unknown group entry op type: %v\", op))\n\t\t\t}\n\t\t} else if e.Type == EntrySnapshot {\n\t\t\tif mset == nil {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// Everything operates on new replicated state. Will convert legacy snapshots to this for processing.\n\t\t\tvar ss *StreamReplicatedState\n\n\t\t\tonBadState := func(err error) {\n\t\t\t\t// If we are the leader or recovering, meaning we own the snapshot,\n\t\t\t\t// we should stepdown and clear our raft state since our snapshot is bad.\n\t\t\t\tif isRecovering || mset.IsLeader() {\n\t\t\t\t\tmset.mu.RLock()\n\t\t\t\t\ts, accName, streamName := mset.srv, mset.acc.GetName(), mset.cfg.Name\n\t\t\t\t\tmset.mu.RUnlock()\n\t\t\t\t\ts.Warnf(\"Detected bad stream state, resetting '%s > %s'\", accName, streamName)\n\t\t\t\t\tmset.resetClusteredState(err)\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// Check if we are the new binary encoding.\n\t\t\tif IsEncodedStreamState(e.Data) {\n\t\t\t\tvar err error\n\t\t\t\tss, err = DecodeStreamState(e.Data)\n\t\t\t\tif err != nil {\n\t\t\t\t\tonBadState(err)\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tvar snap streamSnapshot\n\t\t\t\tif err := json.Unmarshal(e.Data, &snap); err != nil {\n\t\t\t\t\tonBadState(err)\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\t// Convert over to StreamReplicatedState\n\t\t\t\tss = &StreamReplicatedState{\n\t\t\t\t\tMsgs:     snap.Msgs,\n\t\t\t\t\tBytes:    snap.Bytes,\n\t\t\t\t\tFirstSeq: snap.FirstSeq,\n\t\t\t\t\tLastSeq:  snap.LastSeq,\n\t\t\t\t\tFailed:   snap.Failed,\n\t\t\t\t}\n\t\t\t\tif len(snap.Deleted) > 0 {\n\t\t\t\t\tss.Deleted = append(ss.Deleted, DeleteSlice(snap.Deleted))\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif isRecovering || !mset.IsLeader() {\n\t\t\t\tif err := mset.processSnapshot(ss, ce.Index); err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t}\n\t\t} else if e.Type == EntryRemovePeer {\n\t\t\tjs.mu.RLock()\n\t\t\tvar ourID string\n\t\t\tif js.cluster != nil && js.cluster.meta != nil {\n\t\t\t\tourID = js.cluster.meta.ID()\n\t\t\t}\n\t\t\tjs.mu.RUnlock()\n\t\t\t// We only need to do processing if this is us.\n\t\t\tif peer := string(e.Data); peer == ourID && mset != nil {\n\t\t\t\t// Double check here with the registered stream assignment.\n\t\t\t\tshouldRemove := true\n\t\t\t\tif sa := mset.streamAssignment(); sa != nil && sa.Group != nil {\n\t\t\t\t\tjs.mu.RLock()\n\t\t\t\t\tshouldRemove = !sa.Group.isMember(ourID)\n\t\t\t\t\tjs.mu.RUnlock()\n\t\t\t\t}\n\t\t\t\tif shouldRemove {\n\t\t\t\t\tmset.stop(true, false)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn nil\n}\n\n// Returns the PeerInfo for all replicas of a raft node. This is different than node.Peers()\n// and is used for external facing advisories.\nfunc (s *Server) replicas(node RaftNode) []*PeerInfo {\n\tnow := time.Now()\n\tvar replicas []*PeerInfo\n\tfor _, rp := range node.Peers() {\n\t\tif sir, ok := s.nodeToInfo.Load(rp.ID); ok && sir != nil {\n\t\t\tsi := sir.(nodeInfo)\n\t\t\tpi := &PeerInfo{Peer: rp.ID, Name: si.name, Current: rp.Current, Active: now.Sub(rp.Last), Offline: si.offline, Lag: rp.Lag}\n\t\t\treplicas = append(replicas, pi)\n\t\t}\n\t}\n\treturn replicas\n}\n\n// Process a leader change for the clustered stream.\nfunc (js *jetStream) processStreamLeaderChange(mset *stream, isLeader bool) {\n\tif mset == nil {\n\t\treturn\n\t}\n\tsa := mset.streamAssignment()\n\tif sa == nil {\n\t\treturn\n\t}\n\n\t// Clear inflight dedupe IDs, where seq=0.\n\tmset.mu.Lock()\n\tvar removed int\n\tfor i := len(mset.ddarr) - 1; i >= mset.ddindex; i-- {\n\t\tdde := mset.ddarr[i]\n\t\tif dde.seq != 0 {\n\t\t\tbreak\n\t\t}\n\t\tremoved++\n\t\tdelete(mset.ddmap, dde.id)\n\t}\n\tif removed > 0 {\n\t\tif len(mset.ddmap) > 0 {\n\t\t\tmset.ddarr = mset.ddarr[:len(mset.ddarr)-removed]\n\t\t} else {\n\t\t\tmset.ddmap = nil\n\t\t\tmset.ddarr = nil\n\t\t\tmset.ddindex = 0\n\t\t}\n\t}\n\tmset.mu.Unlock()\n\n\tmset.clMu.Lock()\n\t// Clear inflight if we have it.\n\tmset.inflight = nil\n\t// Clear expected per subject state.\n\tmset.expectedPerSubjectSequence = nil\n\tmset.expectedPerSubjectInProcess = nil\n\tmset.clMu.Unlock()\n\n\tjs.mu.Lock()\n\ts, account, err := js.srv, sa.Client.serviceAccount(), sa.err\n\tclient, subject, reply := sa.Client, sa.Subject, sa.Reply\n\thasResponded := sa.responded\n\tsa.responded = true\n\tjs.mu.Unlock()\n\n\tstreamName := mset.name()\n\n\tif isLeader {\n\t\ts.Noticef(\"JetStream cluster new stream leader for '%s > %s'\", account, streamName)\n\t\ts.sendStreamLeaderElectAdvisory(mset)\n\t} else {\n\t\t// We are stepping down.\n\t\t// Make sure if we are doing so because we have lost quorum that we send the appropriate advisories.\n\t\tif node := mset.raftNode(); node != nil && !node.Quorum() && time.Since(node.Created()) > 5*time.Second {\n\t\t\ts.sendStreamLostQuorumAdvisory(mset)\n\t\t}\n\n\t\t// Clear clseq. If we become leader again, it will be fixed up\n\t\t// automatically on the next processClusteredInboundMsg call.\n\t\tmset.clMu.Lock()\n\t\tif mset.clseq > 0 {\n\t\t\tmset.clseq = 0\n\t\t}\n\t\tmset.clMu.Unlock()\n\t}\n\n\t// Tell stream to switch leader status.\n\tmset.setLeader(isLeader)\n\n\tif !isLeader || hasResponded {\n\t\treturn\n\t}\n\n\tacc, _ := s.LookupAccount(account)\n\tif acc == nil {\n\t\treturn\n\t}\n\n\t// Send our response.\n\tvar resp = JSApiStreamCreateResponse{ApiResponse: ApiResponse{Type: JSApiStreamCreateResponseType}}\n\tif err != nil {\n\t\tresp.Error = NewJSStreamCreateError(err, Unless(err))\n\t\ts.sendAPIErrResponse(client, acc, subject, reply, _EMPTY_, s.jsonResponse(&resp))\n\t} else {\n\t\tmsetCfg := mset.config()\n\t\tresp.StreamInfo = &StreamInfo{\n\t\t\tCreated:   mset.createdTime(),\n\t\t\tState:     mset.state(),\n\t\t\tConfig:    *setDynamicStreamMetadata(&msetCfg),\n\t\t\tCluster:   js.clusterInfo(mset.raftGroup()),\n\t\t\tSources:   mset.sourcesInfo(),\n\t\t\tMirror:    mset.mirrorInfo(),\n\t\t\tTimeStamp: time.Now().UTC(),\n\t\t}\n\t\tresp.DidCreate = true\n\t\ts.sendAPIResponse(client, acc, subject, reply, _EMPTY_, s.jsonResponse(&resp))\n\t\tif node := mset.raftNode(); node != nil {\n\t\t\tmset.sendCreateAdvisory()\n\t\t}\n\t}\n}\n\n// Fixed value ok for now.\nconst lostQuorumAdvInterval = 10 * time.Second\n\n// Determines if we should send lost quorum advisory. We throttle these after first one.\nfunc (mset *stream) shouldSendLostQuorum() bool {\n\tmset.mu.Lock()\n\tdefer mset.mu.Unlock()\n\tif time.Since(mset.lqsent) >= lostQuorumAdvInterval {\n\t\tmset.lqsent = time.Now()\n\t\treturn true\n\t}\n\treturn false\n}\n\nfunc (s *Server) sendStreamLostQuorumAdvisory(mset *stream) {\n\tif mset == nil {\n\t\treturn\n\t}\n\tnode, stream, acc := mset.raftNode(), mset.name(), mset.account()\n\tif node == nil {\n\t\treturn\n\t}\n\tif !mset.shouldSendLostQuorum() {\n\t\treturn\n\t}\n\n\ts.Warnf(\"JetStream cluster stream '%s > %s' has NO quorum, stalled\", acc.GetName(), stream)\n\n\tsubj := JSAdvisoryStreamQuorumLostPre + \".\" + stream\n\tadv := &JSStreamQuorumLostAdvisory{\n\t\tTypedEvent: TypedEvent{\n\t\t\tType: JSStreamQuorumLostAdvisoryType,\n\t\t\tID:   nuid.Next(),\n\t\t\tTime: time.Now().UTC(),\n\t\t},\n\t\tStream:   stream,\n\t\tReplicas: s.replicas(node),\n\t\tDomain:   s.getOpts().JetStreamDomain,\n\t}\n\n\t// Send to the user's account if not the system account.\n\tif acc != s.SystemAccount() {\n\t\ts.publishAdvisory(acc, subj, adv)\n\t}\n\t// Now do system level one. Place account info in adv, and nil account means system.\n\tadv.Account = acc.GetName()\n\ts.publishAdvisory(nil, subj, adv)\n}\n\nfunc (s *Server) sendStreamLeaderElectAdvisory(mset *stream) {\n\tif mset == nil {\n\t\treturn\n\t}\n\tnode, stream, acc := mset.raftNode(), mset.name(), mset.account()\n\tif node == nil {\n\t\treturn\n\t}\n\tsubj := JSAdvisoryStreamLeaderElectedPre + \".\" + stream\n\tadv := &JSStreamLeaderElectedAdvisory{\n\t\tTypedEvent: TypedEvent{\n\t\t\tType: JSStreamLeaderElectedAdvisoryType,\n\t\t\tID:   nuid.Next(),\n\t\t\tTime: time.Now().UTC(),\n\t\t},\n\t\tStream:   stream,\n\t\tLeader:   s.serverNameForNode(node.GroupLeader()),\n\t\tReplicas: s.replicas(node),\n\t\tDomain:   s.getOpts().JetStreamDomain,\n\t}\n\n\t// Send to the user's account if not the system account.\n\tif acc != s.SystemAccount() {\n\t\ts.publishAdvisory(acc, subj, adv)\n\t}\n\t// Now do system level one. Place account info in adv, and nil account means system.\n\tadv.Account = acc.GetName()\n\ts.publishAdvisory(nil, subj, adv)\n}\n\n// Will lookup a stream assignment.\n// Lock should be held.\nfunc (js *jetStream) streamAssignment(account, stream string) (sa *streamAssignment) {\n\tcc := js.cluster\n\tif cc == nil {\n\t\treturn nil\n\t}\n\n\tif as := cc.streams[account]; as != nil {\n\t\tsa = as[stream]\n\t}\n\treturn sa\n}\n\n// processStreamAssignment is called when followers have replicated an assignment.\nfunc (js *jetStream) processStreamAssignment(sa *streamAssignment) bool {\n\tjs.mu.Lock()\n\ts, cc := js.srv, js.cluster\n\taccName, stream := sa.Client.serviceAccount(), sa.Config.Name\n\tnoMeta := cc == nil || cc.meta == nil\n\tvar ourID string\n\tif !noMeta {\n\t\tourID = cc.meta.ID()\n\t}\n\tvar isMember bool\n\tif sa.Group != nil && ourID != _EMPTY_ {\n\t\tisMember = sa.Group.isMember(ourID)\n\t}\n\n\t// Remove this stream from the inflight proposals\n\tcc.removeInflightProposal(accName, sa.Config.Name)\n\n\tif s == nil || noMeta {\n\t\tjs.mu.Unlock()\n\t\treturn false\n\t}\n\n\taccStreams := cc.streams[accName]\n\tif accStreams == nil {\n\t\taccStreams = make(map[string]*streamAssignment)\n\t} else if osa := accStreams[stream]; osa != nil && osa != sa {\n\t\t// Copy over private existing state from former SA.\n\t\tif sa.Group != nil {\n\t\t\tsa.Group.node = osa.Group.node\n\t\t}\n\t\tsa.consumers = osa.consumers\n\t\tsa.responded = osa.responded\n\t\tsa.err = osa.err\n\t}\n\n\t// Update our state.\n\taccStreams[stream] = sa\n\tcc.streams[accName] = accStreams\n\thasResponded := sa.responded\n\tjs.mu.Unlock()\n\n\tacc, err := s.LookupAccount(accName)\n\tif err != nil {\n\t\tll := fmt.Sprintf(\"Account [%s] lookup for stream create failed: %v\", accName, err)\n\t\tif isMember {\n\t\t\tif !hasResponded {\n\t\t\t\t// If we can not lookup the account and we are a member, send this result back to the metacontroller leader.\n\t\t\t\tresult := &streamAssignmentResult{\n\t\t\t\t\tAccount:  accName,\n\t\t\t\t\tStream:   stream,\n\t\t\t\t\tResponse: &JSApiStreamCreateResponse{ApiResponse: ApiResponse{Type: JSApiStreamCreateResponseType}},\n\t\t\t\t}\n\t\t\t\tresult.Response.Error = NewJSNoAccountError()\n\t\t\t\ts.sendInternalMsgLocked(streamAssignmentSubj, _EMPTY_, nil, result)\n\t\t\t}\n\t\t\ts.Warnf(ll)\n\t\t} else {\n\t\t\ts.Debugf(ll)\n\t\t}\n\t\treturn false\n\t}\n\n\tvar didRemove bool\n\n\t// Check if this is for us..\n\tif isMember {\n\t\tjs.processClusterCreateStream(acc, sa)\n\t} else if mset, _ := acc.lookupStream(sa.Config.Name); mset != nil {\n\t\t// We have one here even though we are not a member. This can happen on re-assignment.\n\t\ts.removeStream(mset, sa)\n\t}\n\n\t// If this stream assignment does not have a sync subject (bug) set that the meta-leader should check when elected.\n\tif sa.Sync == _EMPTY_ {\n\t\tjs.mu.Lock()\n\t\tcc.streamsCheck = true\n\t\tjs.mu.Unlock()\n\t\treturn false\n\t}\n\n\treturn didRemove\n}\n\n// processUpdateStreamAssignment is called when followers have replicated an updated assignment.\nfunc (js *jetStream) processUpdateStreamAssignment(sa *streamAssignment) {\n\tjs.mu.RLock()\n\ts, cc := js.srv, js.cluster\n\tjs.mu.RUnlock()\n\tif s == nil || cc == nil {\n\t\t// TODO(dlc) - debug at least\n\t\treturn\n\t}\n\n\taccName := sa.Client.serviceAccount()\n\tstream := sa.Config.Name\n\n\tjs.mu.Lock()\n\tif cc.meta == nil {\n\t\tjs.mu.Unlock()\n\t\treturn\n\t}\n\tourID := cc.meta.ID()\n\n\tvar isMember bool\n\tif sa.Group != nil {\n\t\tisMember = sa.Group.isMember(ourID)\n\t}\n\n\taccStreams := cc.streams[accName]\n\tif accStreams == nil {\n\t\tjs.mu.Unlock()\n\t\treturn\n\t}\n\tosa := accStreams[stream]\n\tif osa == nil {\n\t\tjs.mu.Unlock()\n\t\treturn\n\t}\n\n\t// Copy over private existing state from former SA.\n\tif sa.Group != nil {\n\t\tsa.Group.node = osa.Group.node\n\t}\n\tsa.consumers = osa.consumers\n\tsa.err = osa.err\n\n\t// If we detect we are scaling down to 1, non-clustered, and we had a previous node, clear it here.\n\tif sa.Config.Replicas == 1 && sa.Group.node != nil {\n\t\tsa.Group.node = nil\n\t}\n\n\t// Update our state.\n\taccStreams[stream] = sa\n\tcc.streams[accName] = accStreams\n\n\t// Make sure we respond if we are a member.\n\tif isMember {\n\t\tsa.responded = false\n\t} else {\n\t\t// Make sure to clean up any old node in case this stream moves back here.\n\t\tif sa.Group != nil {\n\t\t\tsa.Group.node = nil\n\t\t}\n\t}\n\tjs.mu.Unlock()\n\n\tacc, err := s.LookupAccount(accName)\n\tif err != nil {\n\t\ts.Warnf(\"Update Stream Account %s, error on lookup: %v\", accName, err)\n\t\treturn\n\t}\n\n\t// Check if this is for us..\n\tif isMember {\n\t\tjs.processClusterUpdateStream(acc, osa, sa)\n\t} else if mset, _ := acc.lookupStream(sa.Config.Name); mset != nil {\n\t\t// We have one here even though we are not a member. This can happen on re-assignment.\n\t\ts.removeStream(mset, sa)\n\t}\n}\n\n// Common function to remove ourselves from this server.\n// This can happen on re-assignment, move, etc\nfunc (s *Server) removeStream(mset *stream, nsa *streamAssignment) {\n\tif mset == nil {\n\t\treturn\n\t}\n\t// Make sure to use the new stream assignment, not our own.\n\ts.Debugf(\"JetStream removing stream '%s > %s' from this server\", nsa.Client.serviceAccount(), nsa.Config.Name)\n\tif node := mset.raftNode(); node != nil {\n\t\tnode.StepDown(nsa.Group.Preferred)\n\t\t// shutdown monitor by shutting down raft.\n\t\tnode.Delete()\n\t}\n\n\tvar isShuttingDown bool\n\t// Make sure this node is no longer attached to our stream assignment.\n\tif js, _ := s.getJetStreamCluster(); js != nil {\n\t\tjs.mu.Lock()\n\t\tnsa.Group.node = nil\n\t\tisShuttingDown = js.shuttingDown\n\t\tjs.mu.Unlock()\n\t}\n\n\tif !isShuttingDown {\n\t\t// wait for monitor to be shutdown.\n\t\tmset.monitorWg.Wait()\n\t}\n\tmset.stop(true, false)\n}\n\n// processClusterUpdateStream is called when we have a stream assignment that\n// has been updated for an existing assignment and we are a member.\nfunc (js *jetStream) processClusterUpdateStream(acc *Account, osa, sa *streamAssignment) {\n\tif sa == nil {\n\t\treturn\n\t}\n\n\tjs.mu.Lock()\n\ts, rg := js.srv, sa.Group\n\tclient, subject, reply := sa.Client, sa.Subject, sa.Reply\n\talreadyRunning, numReplicas := osa.Group.node != nil, len(rg.Peers)\n\tneedsNode := rg.node == nil\n\tstorage, cfg := sa.Config.Storage, sa.Config\n\thasResponded := sa.responded\n\tsa.responded = true\n\trecovering := sa.recovering\n\tjs.mu.Unlock()\n\n\tmset, err := acc.lookupStream(cfg.Name)\n\tif err == nil && mset != nil {\n\t\t// Make sure we have not had a new group assigned to us.\n\t\tif osa.Group.Name != sa.Group.Name {\n\t\t\ts.Warnf(\"JetStream cluster detected stream remapping for '%s > %s' from %q to %q\",\n\t\t\t\tacc, cfg.Name, osa.Group.Name, sa.Group.Name)\n\t\t\tmset.removeNode()\n\t\t\talreadyRunning, needsNode = false, true\n\t\t\t// Make sure to clear from original.\n\t\t\tjs.mu.Lock()\n\t\t\tosa.Group.node = nil\n\t\t\tjs.mu.Unlock()\n\t\t}\n\n\t\tif !alreadyRunning && numReplicas > 1 {\n\t\t\tif needsNode {\n\t\t\t\t// Since we are scaling up we want to make sure our sync subject\n\t\t\t\t// is registered before we start our raft node.\n\t\t\t\tmset.mu.Lock()\n\t\t\t\tmset.startClusterSubs()\n\t\t\t\tmset.mu.Unlock()\n\n\t\t\t\tjs.createRaftGroup(acc.GetName(), rg, storage, pprofLabels{\n\t\t\t\t\t\"type\":    \"stream\",\n\t\t\t\t\t\"account\": mset.accName(),\n\t\t\t\t\t\"stream\":  mset.name(),\n\t\t\t\t})\n\t\t\t}\n\t\t\tmset.monitorWg.Add(1)\n\t\t\t// Start monitoring..\n\t\t\ts.startGoRoutine(\n\t\t\t\tfunc() { js.monitorStream(mset, sa, needsNode) },\n\t\t\t\tpprofLabels{\n\t\t\t\t\t\"type\":    \"stream\",\n\t\t\t\t\t\"account\": mset.accName(),\n\t\t\t\t\t\"stream\":  mset.name(),\n\t\t\t\t},\n\t\t\t)\n\t\t} else if numReplicas == 1 && alreadyRunning {\n\t\t\t// We downgraded to R1. Make sure we cleanup the raft node and the stream monitor.\n\t\t\tmset.removeNode()\n\t\t\t// In case we need to shutdown the cluster specific subs, etc.\n\t\t\tmset.mu.Lock()\n\t\t\t// Stop responding to sync requests.\n\t\t\tmset.stopClusterSubs()\n\t\t\t// Clear catchup state\n\t\t\tmset.clearAllCatchupPeers()\n\t\t\tmset.mu.Unlock()\n\t\t\t// Remove from meta layer.\n\t\t\tjs.mu.Lock()\n\t\t\trg.node = nil\n\t\t\tjs.mu.Unlock()\n\t\t}\n\t\t// Set the new stream assignment.\n\t\tmset.setStreamAssignment(sa)\n\n\t\t// Call update.\n\t\tif err = mset.updateWithAdvisory(cfg, !recovering, false); err != nil {\n\t\t\ts.Warnf(\"JetStream cluster error updating stream %q for account %q: %v\", cfg.Name, acc.Name, err)\n\t\t}\n\t}\n\n\t// If not found we must be expanding into this node since if we are here we know we are a member.\n\tif err == ErrJetStreamStreamNotFound {\n\t\tjs.processStreamAssignment(sa)\n\t\treturn\n\t}\n\n\tif err != nil {\n\t\tjs.mu.Lock()\n\t\tsa.err = err\n\t\tresult := &streamAssignmentResult{\n\t\t\tAccount:  sa.Client.serviceAccount(),\n\t\t\tStream:   sa.Config.Name,\n\t\t\tResponse: &JSApiStreamCreateResponse{ApiResponse: ApiResponse{Type: JSApiStreamCreateResponseType}},\n\t\t\tUpdate:   true,\n\t\t}\n\t\tresult.Response.Error = NewJSStreamGeneralError(err, Unless(err))\n\t\tjs.mu.Unlock()\n\n\t\t// Send response to the metadata leader. They will forward to the user as needed.\n\t\ts.sendInternalMsgLocked(streamAssignmentSubj, _EMPTY_, nil, result)\n\t\treturn\n\t}\n\n\tisLeader := mset.IsLeader()\n\n\t// Check for missing syncSubject bug.\n\tif isLeader && osa != nil && osa.Sync == _EMPTY_ {\n\t\tif node := mset.raftNode(); node != nil {\n\t\t\tnode.StepDown()\n\t\t}\n\t\treturn\n\t}\n\n\t// If we were a single node being promoted assume leadership role for purpose of responding.\n\tif !hasResponded && !isLeader && !alreadyRunning {\n\t\tisLeader = true\n\t}\n\n\t// Check if we should bail.\n\tif !isLeader || hasResponded || recovering {\n\t\treturn\n\t}\n\n\t// Send our response.\n\tvar resp = JSApiStreamUpdateResponse{ApiResponse: ApiResponse{Type: JSApiStreamUpdateResponseType}}\n\tmsetCfg := mset.config()\n\tresp.StreamInfo = &StreamInfo{\n\t\tCreated:   mset.createdTime(),\n\t\tState:     mset.state(),\n\t\tConfig:    *setDynamicStreamMetadata(&msetCfg),\n\t\tCluster:   js.clusterInfo(mset.raftGroup()),\n\t\tMirror:    mset.mirrorInfo(),\n\t\tSources:   mset.sourcesInfo(),\n\t\tTimeStamp: time.Now().UTC(),\n\t}\n\n\ts.sendAPIResponse(client, acc, subject, reply, _EMPTY_, s.jsonResponse(&resp))\n}\n\n// processClusterCreateStream is called when we have a stream assignment that\n// has been committed and this server is a member of the peer group.\nfunc (js *jetStream) processClusterCreateStream(acc *Account, sa *streamAssignment) {\n\tif sa == nil {\n\t\treturn\n\t}\n\n\tjs.mu.RLock()\n\ts, rg := js.srv, sa.Group\n\talreadyRunning := rg.node != nil\n\tstorage := sa.Config.Storage\n\trestore := sa.Restore\n\tjs.mu.RUnlock()\n\n\t// Process the raft group and make sure it's running if needed.\n\t_, err := js.createRaftGroup(acc.GetName(), rg, storage, pprofLabels{\n\t\t\"type\":    \"stream\",\n\t\t\"account\": acc.Name,\n\t\t\"stream\":  sa.Config.Name,\n\t})\n\n\t// If we are restoring, create the stream if we are R>1 and not the preferred who handles the\n\t// receipt of the snapshot itself.\n\tshouldCreate := true\n\tif restore != nil {\n\t\tif len(rg.Peers) == 1 || rg.node != nil && rg.node.ID() == rg.Preferred {\n\t\t\tshouldCreate = false\n\t\t} else {\n\t\t\tjs.mu.Lock()\n\t\t\tsa.Restore = nil\n\t\t\tjs.mu.Unlock()\n\t\t}\n\t}\n\n\t// Our stream.\n\tvar mset *stream\n\n\t// Process here if not restoring or not the leader.\n\tif shouldCreate && err == nil {\n\t\t// Go ahead and create or update the stream.\n\t\tmset, err = acc.lookupStream(sa.Config.Name)\n\t\tif err == nil && mset != nil {\n\t\t\tosa := mset.streamAssignment()\n\t\t\t// If we already have a stream assignment and they are the same exact config, short circuit here.\n\t\t\tif osa != nil {\n\t\t\t\tif reflect.DeepEqual(osa.Config, sa.Config) {\n\t\t\t\t\tif sa.Group.Name == osa.Group.Name && reflect.DeepEqual(sa.Group.Peers, osa.Group.Peers) {\n\t\t\t\t\t\t// Since this already exists we know it succeeded, just respond to this caller.\n\t\t\t\t\t\tjs.mu.RLock()\n\t\t\t\t\t\tclient, subject, reply, recovering := sa.Client, sa.Subject, sa.Reply, sa.recovering\n\t\t\t\t\t\tjs.mu.RUnlock()\n\n\t\t\t\t\t\tif !recovering {\n\t\t\t\t\t\t\tvar resp = JSApiStreamCreateResponse{ApiResponse: ApiResponse{Type: JSApiStreamCreateResponseType}}\n\t\t\t\t\t\t\tmsetCfg := mset.config()\n\t\t\t\t\t\t\tresp.StreamInfo = &StreamInfo{\n\t\t\t\t\t\t\t\tCreated:   mset.createdTime(),\n\t\t\t\t\t\t\t\tState:     mset.state(),\n\t\t\t\t\t\t\t\tConfig:    *setDynamicStreamMetadata(&msetCfg),\n\t\t\t\t\t\t\t\tCluster:   js.clusterInfo(mset.raftGroup()),\n\t\t\t\t\t\t\t\tSources:   mset.sourcesInfo(),\n\t\t\t\t\t\t\t\tMirror:    mset.mirrorInfo(),\n\t\t\t\t\t\t\t\tTimeStamp: time.Now().UTC(),\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\ts.sendAPIResponse(client, acc, subject, reply, _EMPTY_, s.jsonResponse(&resp))\n\t\t\t\t\t\t}\n\t\t\t\t\t\treturn\n\t\t\t\t\t} else {\n\t\t\t\t\t\t// We had a bug where we could have multiple assignments for the same\n\t\t\t\t\t\t// stream but with different group assignments, including multiple raft\n\t\t\t\t\t\t// groups. So check for that here. We can only bet on the last one being\n\t\t\t\t\t\t// consistent in the long run, so let it continue if we see this condition.\n\t\t\t\t\t\ts.Warnf(\"JetStream cluster detected duplicate assignment for stream %q for account %q\", sa.Config.Name, acc.Name)\n\t\t\t\t\t\tif osa.Group.node != nil && osa.Group.node != sa.Group.node {\n\t\t\t\t\t\t\tosa.Group.node.Delete()\n\t\t\t\t\t\t\tosa.Group.node = nil\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tmset.setStreamAssignment(sa)\n\t\t\t// Check if our config has really been updated.\n\t\t\tcfg := mset.config()\n\t\t\tif !reflect.DeepEqual(&cfg, sa.Config) {\n\t\t\t\tif err = mset.updateWithAdvisory(sa.Config, false, false); err != nil {\n\t\t\t\t\ts.Warnf(\"JetStream cluster error updating stream %q for account %q: %v\", sa.Config.Name, acc.Name, err)\n\t\t\t\t\tif osa != nil {\n\t\t\t\t\t\t// Process the raft group and make sure it's running if needed.\n\t\t\t\t\t\tjs.createRaftGroup(acc.GetName(), osa.Group, storage, pprofLabels{\n\t\t\t\t\t\t\t\"type\":    \"stream\",\n\t\t\t\t\t\t\t\"account\": mset.accName(),\n\t\t\t\t\t\t\t\"stream\":  mset.name(),\n\t\t\t\t\t\t})\n\t\t\t\t\t\tmset.setStreamAssignment(osa)\n\t\t\t\t\t}\n\t\t\t\t\tif rg.node != nil {\n\t\t\t\t\t\trg.node.Delete()\n\t\t\t\t\t\trg.node = nil\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t} else if err == NewJSStreamNotFoundError() {\n\t\t\t// Add in the stream here.\n\t\t\tmset, err = acc.addStreamWithAssignment(sa.Config, nil, sa, false)\n\t\t}\n\t\tif mset != nil {\n\t\t\tmset.setCreatedTime(sa.Created)\n\t\t}\n\t}\n\n\t// This is an error condition.\n\tif err != nil {\n\t\t// If we're shutting down we could get a variety of errors, for example:\n\t\t// 'JetStream not enabled for account' when looking up the stream.\n\t\t// Normally we can continue and delete state, but need to be careful when shutting down.\n\t\tif js.isShuttingDown() {\n\t\t\ts.Debugf(\"Could not create stream, JetStream shutting down\")\n\t\t\treturn\n\t\t}\n\n\t\tif IsNatsErr(err, JSStreamStoreFailedF) {\n\t\t\ts.Warnf(\"Stream create failed for '%s > %s': %v\", sa.Client.serviceAccount(), sa.Config.Name, err)\n\t\t\terr = errStreamStoreFailed\n\t\t}\n\t\tjs.mu.Lock()\n\n\t\tsa.err = err\n\t\thasResponded := sa.responded\n\n\t\t// If out of space do nothing for now.\n\t\tif isOutOfSpaceErr(err) {\n\t\t\thasResponded = true\n\t\t}\n\n\t\tif rg.node != nil {\n\t\t\trg.node.Delete()\n\t\t}\n\n\t\tvar result *streamAssignmentResult\n\t\tif !hasResponded {\n\t\t\tresult = &streamAssignmentResult{\n\t\t\t\tAccount:  sa.Client.serviceAccount(),\n\t\t\t\tStream:   sa.Config.Name,\n\t\t\t\tResponse: &JSApiStreamCreateResponse{ApiResponse: ApiResponse{Type: JSApiStreamCreateResponseType}},\n\t\t\t}\n\t\t\tresult.Response.Error = NewJSStreamCreateError(err, Unless(err))\n\t\t}\n\t\tjs.mu.Unlock()\n\n\t\t// Send response to the metadata leader. They will forward to the user as needed.\n\t\tif result != nil {\n\t\t\ts.sendInternalMsgLocked(streamAssignmentSubj, _EMPTY_, nil, result)\n\t\t}\n\t\treturn\n\t}\n\n\t// Re-capture node.\n\tjs.mu.RLock()\n\tnode := rg.node\n\tjs.mu.RUnlock()\n\n\t// Start our monitoring routine.\n\tif node != nil {\n\t\tif !alreadyRunning {\n\t\t\tif mset != nil {\n\t\t\t\tmset.monitorWg.Add(1)\n\t\t\t}\n\t\t\ts.startGoRoutine(\n\t\t\t\tfunc() { js.monitorStream(mset, sa, false) },\n\t\t\t\tpprofLabels{\n\t\t\t\t\t\"type\":    \"stream\",\n\t\t\t\t\t\"account\": mset.accName(),\n\t\t\t\t\t\"stream\":  mset.name(),\n\t\t\t\t},\n\t\t\t)\n\t\t}\n\t} else {\n\t\t// Single replica stream, process manually here.\n\t\t// If we are restoring, process that first.\n\t\tif sa.Restore != nil {\n\t\t\t// We are restoring a stream here.\n\t\t\trestoreDoneCh := s.processStreamRestore(sa.Client, acc, sa.Config, _EMPTY_, sa.Reply, _EMPTY_)\n\t\t\ts.startGoRoutine(func() {\n\t\t\t\tdefer s.grWG.Done()\n\t\t\t\tselect {\n\t\t\t\tcase err := <-restoreDoneCh:\n\t\t\t\t\tif err == nil {\n\t\t\t\t\t\tmset, err = acc.lookupStream(sa.Config.Name)\n\t\t\t\t\t\tif mset != nil {\n\t\t\t\t\t\t\tmset.setStreamAssignment(sa)\n\t\t\t\t\t\t\tmset.setCreatedTime(sa.Created)\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\tif mset != nil {\n\t\t\t\t\t\t\tmset.delete()\n\t\t\t\t\t\t}\n\t\t\t\t\t\tjs.mu.Lock()\n\t\t\t\t\t\tsa.err = err\n\t\t\t\t\t\tresult := &streamAssignmentResult{\n\t\t\t\t\t\t\tAccount: sa.Client.serviceAccount(),\n\t\t\t\t\t\t\tStream:  sa.Config.Name,\n\t\t\t\t\t\t\tRestore: &JSApiStreamRestoreResponse{ApiResponse: ApiResponse{Type: JSApiStreamRestoreResponseType}},\n\t\t\t\t\t\t}\n\t\t\t\t\t\tresult.Restore.Error = NewJSStreamRestoreError(err, Unless(err))\n\t\t\t\t\t\tjs.mu.Unlock()\n\t\t\t\t\t\t// Send response to the metadata leader. They will forward to the user as needed.\n\t\t\t\t\t\tb, _ := json.Marshal(result) // Avoids auto-processing and doing fancy json with newlines.\n\t\t\t\t\t\ts.sendInternalMsgLocked(streamAssignmentSubj, _EMPTY_, nil, b)\n\t\t\t\t\t\treturn\n\t\t\t\t\t}\n\t\t\t\t\tjs.processStreamLeaderChange(mset, true)\n\n\t\t\t\t\t// Check to see if we have restored consumers here.\n\t\t\t\t\t// These are not currently assigned so we will need to do so here.\n\t\t\t\t\tif consumers := mset.getPublicConsumers(); len(consumers) > 0 {\n\t\t\t\t\t\tjs.mu.RLock()\n\t\t\t\t\t\tcc := js.cluster\n\t\t\t\t\t\tjs.mu.RUnlock()\n\n\t\t\t\t\t\tfor _, o := range consumers {\n\t\t\t\t\t\t\tname, cfg := o.String(), o.config()\n\t\t\t\t\t\t\trg := cc.createGroupForConsumer(&cfg, sa)\n\n\t\t\t\t\t\t\t// Place our initial state here as well for assignment distribution.\n\t\t\t\t\t\t\tca := &consumerAssignment{\n\t\t\t\t\t\t\t\tGroup:   rg,\n\t\t\t\t\t\t\t\tStream:  sa.Config.Name,\n\t\t\t\t\t\t\t\tName:    name,\n\t\t\t\t\t\t\t\tConfig:  &cfg,\n\t\t\t\t\t\t\t\tClient:  sa.Client,\n\t\t\t\t\t\t\t\tCreated: o.createdTime(),\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\taddEntry := encodeAddConsumerAssignment(ca)\n\t\t\t\t\t\t\tcc.meta.ForwardProposal(addEntry)\n\n\t\t\t\t\t\t\t// Check to make sure we see the assignment.\n\t\t\t\t\t\t\tgo func() {\n\t\t\t\t\t\t\t\tticker := time.NewTicker(time.Second)\n\t\t\t\t\t\t\t\tdefer ticker.Stop()\n\t\t\t\t\t\t\t\tfor range ticker.C {\n\t\t\t\t\t\t\t\t\tjs.mu.RLock()\n\t\t\t\t\t\t\t\t\tca, meta := js.consumerAssignment(ca.Client.serviceAccount(), sa.Config.Name, name), cc.meta\n\t\t\t\t\t\t\t\t\tjs.mu.RUnlock()\n\t\t\t\t\t\t\t\t\tif ca == nil {\n\t\t\t\t\t\t\t\t\t\ts.Warnf(\"Consumer assignment has not been assigned, retrying\")\n\t\t\t\t\t\t\t\t\t\tif meta != nil {\n\t\t\t\t\t\t\t\t\t\t\tmeta.ForwardProposal(addEntry)\n\t\t\t\t\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\t\t\t\t\treturn\n\t\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\t\t\t\treturn\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}()\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\tcase <-s.quitCh:\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t})\n\t\t} else {\n\t\t\tjs.processStreamLeaderChange(mset, true)\n\t\t}\n\t}\n}\n\n// processStreamRemoval is called when followers have replicated an assignment.\nfunc (js *jetStream) processStreamRemoval(sa *streamAssignment) {\n\tjs.mu.Lock()\n\ts, cc := js.srv, js.cluster\n\tif s == nil || cc == nil || cc.meta == nil {\n\t\t// TODO(dlc) - debug at least\n\t\tjs.mu.Unlock()\n\t\treturn\n\t}\n\tstream := sa.Config.Name\n\tisMember := sa.Group.isMember(cc.meta.ID())\n\twasLeader := cc.isStreamLeader(sa.Client.serviceAccount(), stream)\n\n\t// Check if we already have this assigned.\n\taccStreams := cc.streams[sa.Client.serviceAccount()]\n\tneedDelete := accStreams != nil && accStreams[stream] != nil\n\tif needDelete {\n\t\tdelete(accStreams, stream)\n\t\tif len(accStreams) == 0 {\n\t\t\tdelete(cc.streams, sa.Client.serviceAccount())\n\t\t}\n\t}\n\tjs.mu.Unlock()\n\n\tif needDelete {\n\t\tjs.processClusterDeleteStream(sa, isMember, wasLeader)\n\t}\n}\n\nfunc (js *jetStream) processClusterDeleteStream(sa *streamAssignment, isMember, wasLeader bool) {\n\tif sa == nil {\n\t\treturn\n\t}\n\tjs.mu.RLock()\n\ts := js.srv\n\tnode := sa.Group.node\n\thadLeader := node == nil || !node.Leaderless()\n\toffline := s.allPeersOffline(sa.Group)\n\tvar isMetaLeader bool\n\tif cc := js.cluster; cc != nil {\n\t\tisMetaLeader = cc.isLeader()\n\t}\n\trecovering := sa.recovering\n\tjs.mu.RUnlock()\n\n\tstopped := false\n\tvar resp = JSApiStreamDeleteResponse{ApiResponse: ApiResponse{Type: JSApiStreamDeleteResponseType}}\n\tvar err error\n\tvar acc *Account\n\n\t// Go ahead and delete the stream if we have it and the account here.\n\tif acc, _ = s.LookupAccount(sa.Client.serviceAccount()); acc != nil {\n\t\tif mset, _ := acc.lookupStream(sa.Config.Name); mset != nil {\n\t\t\t// shut down monitor by shutting down raft\n\t\t\tif n := mset.raftNode(); n != nil {\n\t\t\t\tn.Delete()\n\t\t\t}\n\t\t\t// wait for monitor to be shut down\n\t\t\tmset.monitorWg.Wait()\n\t\t\terr = mset.stop(true, wasLeader)\n\t\t\tstopped = true\n\t\t} else if isMember {\n\t\t\ts.Warnf(\"JetStream failed to lookup running stream while removing stream '%s > %s' from this server\",\n\t\t\t\tsa.Client.serviceAccount(), sa.Config.Name)\n\t\t}\n\t} else if isMember {\n\t\ts.Warnf(\"JetStream failed to lookup account while removing stream '%s > %s' from this server\", sa.Client.serviceAccount(), sa.Config.Name)\n\t}\n\n\t// Always delete the node if present.\n\tif node != nil {\n\t\tnode.Delete()\n\t}\n\n\t// This is a stop gap cleanup in case\n\t// 1) the account does not exist (and mset couldn't be stopped) and/or\n\t// 2) node was nil (and couldn't be deleted)\n\tif !stopped || node == nil {\n\t\tif sacc := s.SystemAccount(); sacc != nil {\n\t\t\tsaccName := sacc.GetName()\n\t\t\tos.RemoveAll(filepath.Join(js.config.StoreDir, saccName, defaultStoreDirName, sa.Group.Name))\n\t\t\t// cleanup dependent consumer groups\n\t\t\tif !stopped {\n\t\t\t\tfor _, ca := range sa.consumers {\n\t\t\t\t\t// Make sure we cleanup any possible running nodes for the consumers.\n\t\t\t\t\tif isMember && ca.Group != nil && ca.Group.node != nil {\n\t\t\t\t\t\tca.Group.node.Delete()\n\t\t\t\t\t}\n\t\t\t\t\tos.RemoveAll(filepath.Join(js.config.StoreDir, saccName, defaultStoreDirName, ca.Group.Name))\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\taccDir := filepath.Join(js.config.StoreDir, sa.Client.serviceAccount())\n\tstreamDir := filepath.Join(accDir, streamsDir)\n\tos.RemoveAll(filepath.Join(streamDir, sa.Config.Name))\n\n\t// no op if not empty\n\tos.Remove(streamDir)\n\tos.Remove(accDir)\n\n\t// Normally we want only the leader to respond here, but if we had no leader then all members will respond to make\n\t// sure we get feedback to the user.\n\tif !isMember || (hadLeader && !wasLeader) {\n\t\t// If all the peers are offline and we are the meta leader we will also respond, so suppress returning here.\n\t\tif !(offline && isMetaLeader) {\n\t\t\treturn\n\t\t}\n\t}\n\n\t// Do not respond if the account does not exist any longer\n\tif acc == nil || recovering {\n\t\treturn\n\t}\n\n\tif err != nil {\n\t\tresp.Error = NewJSStreamGeneralError(err, Unless(err))\n\t\ts.sendAPIErrResponse(sa.Client, acc, sa.Subject, sa.Reply, _EMPTY_, s.jsonResponse(resp))\n\t} else {\n\t\tresp.Success = true\n\t\ts.sendAPIResponse(sa.Client, acc, sa.Subject, sa.Reply, _EMPTY_, s.jsonResponse(resp))\n\t}\n}\n\n// processConsumerAssignment is called when followers have replicated an assignment for a consumer.\nfunc (js *jetStream) processConsumerAssignment(ca *consumerAssignment) {\n\tjs.mu.RLock()\n\ts, cc := js.srv, js.cluster\n\taccName, stream, consumerName := ca.Client.serviceAccount(), ca.Stream, ca.Name\n\tnoMeta := cc == nil || cc.meta == nil\n\tshuttingDown := js.shuttingDown\n\tvar ourID string\n\tif !noMeta {\n\t\tourID = cc.meta.ID()\n\t}\n\tvar isMember bool\n\tif ca.Group != nil && ourID != _EMPTY_ {\n\t\tisMember = ca.Group.isMember(ourID)\n\t}\n\tjs.mu.RUnlock()\n\n\tif s == nil || noMeta || shuttingDown {\n\t\treturn\n\t}\n\n\tjs.mu.Lock()\n\tsa := js.streamAssignment(accName, stream)\n\tif sa == nil {\n\t\tjs.mu.Unlock()\n\t\ts.Debugf(\"Consumer create failed, could not locate stream '%s > %s'\", accName, stream)\n\t\treturn\n\t}\n\n\t// Might need this below.\n\tnumReplicas := sa.Config.Replicas\n\n\t// Track if this existed already.\n\tvar wasExisting bool\n\n\t// Check if we have an existing consumer assignment.\n\tif sa.consumers == nil {\n\t\tsa.consumers = make(map[string]*consumerAssignment)\n\t} else if oca := sa.consumers[ca.Name]; oca != nil {\n\t\twasExisting = true\n\t\t// Copy over private existing state from former CA.\n\t\tif ca.Group != nil {\n\t\t\tca.Group.node = oca.Group.node\n\t\t}\n\t\tca.responded = oca.responded\n\t\tca.err = oca.err\n\t}\n\n\t// Capture the optional state. We will pass it along if we are a member to apply.\n\t// This is only applicable when restoring a stream with consumers.\n\tstate := ca.State\n\tca.State = nil\n\n\t// Place into our internal map under the stream assignment.\n\t// Ok to replace an existing one, we check on process call below.\n\tsa.consumers[ca.Name] = ca\n\tca.pending = false\n\tjs.mu.Unlock()\n\n\tacc, err := s.LookupAccount(accName)\n\tif err != nil {\n\t\tll := fmt.Sprintf(\"Account [%s] lookup for consumer create failed: %v\", accName, err)\n\t\tif isMember {\n\t\t\tif !js.isMetaRecovering() {\n\t\t\t\t// If we can not lookup the account and we are a member, send this result back to the metacontroller leader.\n\t\t\t\tresult := &consumerAssignmentResult{\n\t\t\t\t\tAccount:  accName,\n\t\t\t\t\tStream:   stream,\n\t\t\t\t\tConsumer: consumerName,\n\t\t\t\t\tResponse: &JSApiConsumerCreateResponse{ApiResponse: ApiResponse{Type: JSApiConsumerCreateResponseType}},\n\t\t\t\t}\n\t\t\t\tresult.Response.Error = NewJSNoAccountError()\n\t\t\t\ts.sendInternalMsgLocked(consumerAssignmentSubj, _EMPTY_, nil, result)\n\t\t\t}\n\t\t\ts.Warnf(ll)\n\t\t} else {\n\t\t\ts.Debugf(ll)\n\t\t}\n\t\treturn\n\t}\n\n\t// Check if this is for us..\n\tif isMember {\n\t\tjs.processClusterCreateConsumer(ca, state, wasExisting)\n\t} else {\n\t\t// We need to be removed here, we are no longer assigned.\n\t\t// Grab consumer if we have it.\n\t\tvar o *consumer\n\t\tif mset, _ := acc.lookupStream(sa.Config.Name); mset != nil {\n\t\t\to = mset.lookupConsumer(ca.Name)\n\t\t}\n\n\t\t// Check if we have a raft node running, meaning we are no longer part of the group but were.\n\t\tjs.mu.Lock()\n\t\tif node := ca.Group.node; node != nil {\n\t\t\t// We have one here even though we are not a member. This can happen on re-assignment.\n\t\t\ts.Debugf(\"JetStream removing consumer '%s > %s > %s' from this server\", sa.Client.serviceAccount(), sa.Config.Name, ca.Name)\n\t\t\tif node.Leader() {\n\t\t\t\ts.Debugf(\"JetStream consumer '%s > %s > %s' is being removed and was the leader, will perform stepdown\",\n\t\t\t\t\tsa.Client.serviceAccount(), sa.Config.Name, ca.Name)\n\n\t\t\t\tpeers, cn := node.Peers(), s.cachedClusterName()\n\t\t\t\tmigrating := numReplicas != len(peers)\n\n\t\t\t\t// Select a new peer to transfer to. If we are a migrating make sure its from the new cluster.\n\t\t\t\tvar npeer string\n\t\t\t\tfor _, r := range peers {\n\t\t\t\t\tif !r.Current {\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\t\t\t\t\tif !migrating {\n\t\t\t\t\t\tnpeer = r.ID\n\t\t\t\t\t\tbreak\n\t\t\t\t\t} else if sir, ok := s.nodeToInfo.Load(r.ID); ok && sir != nil {\n\t\t\t\t\t\tsi := sir.(nodeInfo)\n\t\t\t\t\t\tif si.cluster != cn {\n\t\t\t\t\t\t\tnpeer = r.ID\n\t\t\t\t\t\t\tbreak\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\t// Clear the raftnode from our consumer so that a subsequent o.delete will not also issue a stepdown.\n\t\t\t\tif o != nil {\n\t\t\t\t\to.clearRaftNode()\n\t\t\t\t}\n\t\t\t\t// Manually handle the stepdown and deletion of the node.\n\t\t\t\tnode.UpdateKnownPeers(ca.Group.Peers)\n\t\t\t\tnode.StepDown(npeer)\n\t\t\t\tnode.Delete()\n\t\t\t} else {\n\t\t\t\tnode.UpdateKnownPeers(ca.Group.Peers)\n\t\t\t}\n\t\t}\n\t\t// Always clear the old node.\n\t\tca.Group.node = nil\n\t\tca.err = nil\n\t\tjs.mu.Unlock()\n\n\t\tif o != nil {\n\t\t\to.deleteWithoutAdvisory()\n\t\t}\n\t}\n}\n\nfunc (js *jetStream) processConsumerRemoval(ca *consumerAssignment) {\n\tjs.mu.Lock()\n\ts, cc := js.srv, js.cluster\n\tif s == nil || cc == nil || cc.meta == nil {\n\t\t// TODO(dlc) - debug at least\n\t\tjs.mu.Unlock()\n\t\treturn\n\t}\n\twasLeader := cc.isConsumerLeader(ca.Client.serviceAccount(), ca.Stream, ca.Name)\n\n\t// Delete from our state.\n\tvar needDelete bool\n\tif accStreams := cc.streams[ca.Client.serviceAccount()]; accStreams != nil {\n\t\tif sa := accStreams[ca.Stream]; sa != nil && sa.consumers != nil && sa.consumers[ca.Name] != nil {\n\t\t\toca := sa.consumers[ca.Name]\n\t\t\t// Make sure this removal is for what we have, otherwise ignore.\n\t\t\tif ca.Group != nil && oca.Group != nil && ca.Group.Name == oca.Group.Name {\n\t\t\t\tneedDelete = true\n\t\t\t\toca.deleted = true\n\t\t\t\tdelete(sa.consumers, ca.Name)\n\t\t\t}\n\t\t}\n\t}\n\tjs.mu.Unlock()\n\n\tif needDelete {\n\t\tjs.processClusterDeleteConsumer(ca, wasLeader)\n\t}\n}\n\ntype consumerAssignmentResult struct {\n\tAccount  string                       `json:\"account\"`\n\tStream   string                       `json:\"stream\"`\n\tConsumer string                       `json:\"consumer\"`\n\tResponse *JSApiConsumerCreateResponse `json:\"response,omitempty\"`\n}\n\n// processClusterCreateConsumer is when we are a member of the group and need to create the consumer.\nfunc (js *jetStream) processClusterCreateConsumer(ca *consumerAssignment, state *ConsumerState, wasExisting bool) {\n\tif ca == nil {\n\t\treturn\n\t}\n\tjs.mu.RLock()\n\ts := js.srv\n\trg := ca.Group\n\talreadyRunning := rg != nil && rg.node != nil\n\taccName, stream, consumer := ca.Client.serviceAccount(), ca.Stream, ca.Name\n\tjs.mu.RUnlock()\n\n\tacc, err := s.LookupAccount(accName)\n\tif err != nil {\n\t\ts.Warnf(\"JetStream cluster failed to lookup axccount %q: %v\", accName, err)\n\t\treturn\n\t}\n\n\t// Go ahead and create or update the consumer.\n\tmset, err := acc.lookupStream(stream)\n\tif err != nil {\n\t\tif !js.isMetaRecovering() {\n\t\t\tjs.mu.Lock()\n\t\t\ts.Warnf(\"Consumer create failed, could not locate stream '%s > %s > %s'\", ca.Client.serviceAccount(), ca.Stream, ca.Name)\n\t\t\tca.err = NewJSStreamNotFoundError()\n\t\t\tresult := &consumerAssignmentResult{\n\t\t\t\tAccount:  ca.Client.serviceAccount(),\n\t\t\t\tStream:   ca.Stream,\n\t\t\t\tConsumer: ca.Name,\n\t\t\t\tResponse: &JSApiConsumerCreateResponse{ApiResponse: ApiResponse{Type: JSApiConsumerCreateResponseType}},\n\t\t\t}\n\t\t\tresult.Response.Error = NewJSStreamNotFoundError()\n\t\t\ts.sendInternalMsgLocked(consumerAssignmentSubj, _EMPTY_, nil, result)\n\t\t\tjs.mu.Unlock()\n\t\t}\n\t\treturn\n\t}\n\n\t// Check if we already have this consumer running.\n\to := mset.lookupConsumer(consumer)\n\n\t// Process the raft group and make sure it's running if needed.\n\tstorage := mset.config().Storage\n\tif ca.Config.MemoryStorage {\n\t\tstorage = MemoryStorage\n\t}\n\t// No-op if R1.\n\tjs.createRaftGroup(accName, rg, storage, pprofLabels{\n\t\t\"type\":     \"consumer\",\n\t\t\"account\":  mset.accName(),\n\t\t\"stream\":   ca.Stream,\n\t\t\"consumer\": ca.Name,\n\t})\n\n\t// Check if we already have this consumer running.\n\tvar didCreate, isConfigUpdate, needsLocalResponse bool\n\tif o == nil {\n\t\t// Add in the consumer if needed.\n\t\tif o, err = mset.addConsumerWithAssignment(ca.Config, ca.Name, ca, js.isMetaRecovering(), ActionCreateOrUpdate, false); err == nil {\n\t\t\tdidCreate = true\n\t\t}\n\t} else {\n\t\t// This consumer exists.\n\t\t// Only update if config is really different.\n\t\tcfg := o.config()\n\t\tif isConfigUpdate = !reflect.DeepEqual(&cfg, ca.Config); isConfigUpdate {\n\t\t\t// Call into update, ignore consumer exists error here since this means an old deliver subject is bound\n\t\t\t// which can happen on restart etc.\n\t\t\t// JS lock needed as this can mutate the consumer assignments and race with updateInactivityThreshold.\n\t\t\tjs.mu.Lock()\n\t\t\terr := o.updateConfig(ca.Config)\n\t\t\tjs.mu.Unlock()\n\t\t\tif err != nil && err != NewJSConsumerNameExistError() {\n\t\t\t\t// This is essentially an update that has failed. Respond back to metaleader if we are not recovering.\n\t\t\t\tjs.mu.RLock()\n\t\t\t\tif !js.metaRecovering {\n\t\t\t\t\tresult := &consumerAssignmentResult{\n\t\t\t\t\t\tAccount:  accName,\n\t\t\t\t\t\tStream:   stream,\n\t\t\t\t\t\tConsumer: consumer,\n\t\t\t\t\t\tResponse: &JSApiConsumerCreateResponse{ApiResponse: ApiResponse{Type: JSApiConsumerCreateResponseType}},\n\t\t\t\t\t}\n\t\t\t\t\tresult.Response.Error = NewJSConsumerNameExistError()\n\t\t\t\t\ts.sendInternalMsgLocked(consumerAssignmentSubj, _EMPTY_, nil, result)\n\t\t\t\t}\n\t\t\t\ts.Warnf(\"Consumer create failed during update for '%s > %s > %s': %v\", ca.Client.serviceAccount(), ca.Stream, ca.Name, err)\n\t\t\t\tjs.mu.RUnlock()\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\n\t\tvar sendState bool\n\t\tjs.mu.RLock()\n\t\tn := rg.node\n\t\t// Check if we already had a consumer assignment and its still pending.\n\t\tcca, oca := ca, o.consumerAssignment()\n\t\tif oca != nil {\n\t\t\tif !oca.responded {\n\t\t\t\t// We can't override info for replying here otherwise leader once elected can not respond.\n\t\t\t\t// So copy over original client and the reply from the old ca.\n\t\t\t\tcac := *ca\n\t\t\t\tcac.Client = oca.Client\n\t\t\t\tcac.Reply = oca.Reply\n\t\t\t\tcca = &cac\n\t\t\t\tneedsLocalResponse = true\n\t\t\t}\n\t\t\t// If we look like we are scaling up, let's send our current state to the group.\n\t\t\tsendState = len(ca.Group.Peers) > len(oca.Group.Peers) && o.IsLeader() && n != nil\n\t\t\t// Signal that this is an update\n\t\t\tif ca.Reply != _EMPTY_ {\n\t\t\t\tisConfigUpdate = true\n\t\t\t}\n\t\t}\n\t\tjs.mu.RUnlock()\n\n\t\tif sendState {\n\t\t\tif snap, err := o.store.EncodedState(); err == nil {\n\t\t\t\tn.SendSnapshot(snap)\n\t\t\t}\n\t\t}\n\n\t\t// Set CA for our consumer.\n\t\to.setConsumerAssignment(cca)\n\t\ts.Debugf(\"JetStream cluster, consumer '%s > %s > %s' was already running\", ca.Client.serviceAccount(), ca.Stream, ca.Name)\n\t}\n\n\t// If we have an initial state set apply that now.\n\tif state != nil && o != nil {\n\t\to.mu.Lock()\n\t\terr = o.setStoreState(state)\n\t\to.mu.Unlock()\n\t}\n\n\tif err != nil {\n\t\t// If we're shutting down we could get a variety of errors.\n\t\t// Normally we can continue and delete state, but need to be careful when shutting down.\n\t\tif js.isShuttingDown() {\n\t\t\ts.Debugf(\"Could not create consumer, JetStream shutting down\")\n\t\t\treturn\n\t\t}\n\n\t\tif IsNatsErr(err, JSConsumerStoreFailedErrF) {\n\t\t\ts.Warnf(\"Consumer create failed for '%s > %s > %s': %v\", ca.Client.serviceAccount(), ca.Stream, ca.Name, err)\n\t\t\terr = errConsumerStoreFailed\n\t\t}\n\n\t\tjs.mu.Lock()\n\n\t\tca.err = err\n\t\thasResponded := ca.responded\n\n\t\t// If out of space do nothing for now.\n\t\tif isOutOfSpaceErr(err) {\n\t\t\thasResponded = true\n\t\t}\n\n\t\tif rg.node != nil {\n\t\t\trg.node.Delete()\n\t\t\t// Clear the node here.\n\t\t\trg.node = nil\n\t\t}\n\n\t\t// If we did seem to create a consumer make sure to stop it.\n\t\tif o != nil {\n\t\t\to.stop()\n\t\t}\n\n\t\tvar result *consumerAssignmentResult\n\t\tif !hasResponded && !js.metaRecovering {\n\t\t\tresult = &consumerAssignmentResult{\n\t\t\t\tAccount:  ca.Client.serviceAccount(),\n\t\t\t\tStream:   ca.Stream,\n\t\t\t\tConsumer: ca.Name,\n\t\t\t\tResponse: &JSApiConsumerCreateResponse{ApiResponse: ApiResponse{Type: JSApiConsumerCreateResponseType}},\n\t\t\t}\n\t\t\tresult.Response.Error = NewJSConsumerCreateError(err, Unless(err))\n\t\t} else if err == errNoInterest {\n\t\t\t// This is a stranded ephemeral, let's clean this one up.\n\t\t\tsubject := fmt.Sprintf(JSApiConsumerDeleteT, ca.Stream, ca.Name)\n\t\t\tmset.outq.send(newJSPubMsg(subject, _EMPTY_, _EMPTY_, nil, nil, nil, 0))\n\t\t}\n\t\tjs.mu.Unlock()\n\n\t\tif result != nil {\n\t\t\t// Send response to the metadata leader. They will forward to the user as needed.\n\t\t\tb, _ := json.Marshal(result) // Avoids auto-processing and doing fancy json with newlines.\n\t\t\ts.sendInternalMsgLocked(consumerAssignmentSubj, _EMPTY_, nil, b)\n\t\t}\n\t} else {\n\t\tjs.mu.RLock()\n\t\tnode := rg.node\n\t\tjs.mu.RUnlock()\n\n\t\tif didCreate {\n\t\t\to.setCreatedTime(ca.Created)\n\t\t} else {\n\t\t\t// Check for scale down to 1..\n\t\t\tif node != nil && len(rg.Peers) == 1 {\n\t\t\t\to.clearNode()\n\t\t\t\to.setLeader(true)\n\t\t\t\t// Need to clear from rg too.\n\t\t\t\tjs.mu.Lock()\n\t\t\t\trg.node = nil\n\t\t\t\tclient, subject, reply := ca.Client, ca.Subject, ca.Reply\n\t\t\t\tjs.mu.Unlock()\n\t\t\t\tvar resp = JSApiConsumerCreateResponse{ApiResponse: ApiResponse{Type: JSApiConsumerCreateResponseType}}\n\t\t\t\tresp.ConsumerInfo = setDynamicConsumerInfoMetadata(o.info())\n\t\t\t\ts.sendAPIResponse(client, acc, subject, reply, _EMPTY_, s.jsonResponse(&resp))\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\n\t\tif node == nil {\n\t\t\t// Single replica consumer, process manually here.\n\t\t\tjs.mu.Lock()\n\t\t\t// Force response in case we think this is an update.\n\t\t\tif !js.metaRecovering && isConfigUpdate {\n\t\t\t\tca.responded = false\n\t\t\t}\n\t\t\tjs.mu.Unlock()\n\t\t\tjs.processConsumerLeaderChange(o, true)\n\t\t} else {\n\t\t\t// Clustered consumer.\n\t\t\t// Start our monitoring routine if needed.\n\t\t\tif !alreadyRunning && o.shouldStartMonitor() {\n\t\t\t\ts.startGoRoutine(\n\t\t\t\t\tfunc() { js.monitorConsumer(o, ca) },\n\t\t\t\t\tpprofLabels{\n\t\t\t\t\t\t\"type\":     \"consumer\",\n\t\t\t\t\t\t\"account\":  mset.accName(),\n\t\t\t\t\t\t\"stream\":   mset.name(),\n\t\t\t\t\t\t\"consumer\": ca.Name,\n\t\t\t\t\t},\n\t\t\t\t)\n\t\t\t}\n\t\t\t// For existing consumer, only send response if not recovering.\n\t\t\tif wasExisting && !js.isMetaRecovering() {\n\t\t\t\tif o.IsLeader() || (!didCreate && needsLocalResponse) {\n\t\t\t\t\t// Process if existing as an update. Double check that this is not recovered.\n\t\t\t\t\tjs.mu.RLock()\n\t\t\t\t\tclient, subject, reply, recovering := ca.Client, ca.Subject, ca.Reply, ca.recovering\n\t\t\t\t\tjs.mu.RUnlock()\n\t\t\t\t\tif !recovering {\n\t\t\t\t\t\tvar resp = JSApiConsumerCreateResponse{ApiResponse: ApiResponse{Type: JSApiConsumerCreateResponseType}}\n\t\t\t\t\t\tresp.ConsumerInfo = setDynamicConsumerInfoMetadata(o.info())\n\t\t\t\t\t\ts.sendAPIResponse(client, acc, subject, reply, _EMPTY_, s.jsonResponse(&resp))\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc (js *jetStream) processClusterDeleteConsumer(ca *consumerAssignment, wasLeader bool) {\n\tif ca == nil {\n\t\treturn\n\t}\n\tjs.mu.RLock()\n\ts := js.srv\n\tnode := ca.Group.node\n\toffline := s.allPeersOffline(ca.Group)\n\tvar isMetaLeader bool\n\tif cc := js.cluster; cc != nil {\n\t\tisMetaLeader = cc.isLeader()\n\t}\n\trecovering := ca.recovering\n\tjs.mu.RUnlock()\n\n\tvar resp = JSApiConsumerDeleteResponse{ApiResponse: ApiResponse{Type: JSApiConsumerDeleteResponseType}}\n\tvar err error\n\tvar acc *Account\n\n\t// Go ahead and delete the consumer if we have it and the account.\n\tif acc, _ = s.LookupAccount(ca.Client.serviceAccount()); acc != nil {\n\t\tif mset, _ := acc.lookupStream(ca.Stream); mset != nil {\n\t\t\tif o := mset.lookupConsumer(ca.Name); o != nil {\n\t\t\t\terr = o.stopWithFlags(true, false, true, wasLeader)\n\t\t\t}\n\t\t}\n\t} else if ca.Group != nil {\n\t\t// We have a missing account, see if we can cleanup.\n\t\tif sacc := s.SystemAccount(); sacc != nil {\n\t\t\tos.RemoveAll(filepath.Join(js.config.StoreDir, sacc.GetName(), defaultStoreDirName, ca.Group.Name))\n\t\t}\n\t}\n\n\t// Always delete the node if present.\n\tif node != nil {\n\t\tnode.Delete()\n\t}\n\n\tif !wasLeader || ca.Reply == _EMPTY_ {\n\t\tif !(offline && isMetaLeader) {\n\t\t\treturn\n\t\t}\n\t}\n\n\t// Do not respond if the account does not exist any longer or this is during recovery.\n\tif acc == nil || recovering {\n\t\treturn\n\t}\n\n\tif err != nil {\n\t\tresp.Error = NewJSStreamNotFoundError(Unless(err))\n\t\ts.sendAPIErrResponse(ca.Client, acc, ca.Subject, ca.Reply, _EMPTY_, s.jsonResponse(resp))\n\t} else {\n\t\tresp.Success = true\n\t\ts.sendAPIResponse(ca.Client, acc, ca.Subject, ca.Reply, _EMPTY_, s.jsonResponse(resp))\n\t}\n}\n\n// Returns the consumer assignment, or nil if not present.\n// Lock should be held.\nfunc (js *jetStream) consumerAssignment(account, stream, consumer string) *consumerAssignment {\n\tif sa := js.streamAssignment(account, stream); sa != nil {\n\t\treturn sa.consumers[consumer]\n\t}\n\treturn nil\n}\n\n// consumerAssigned informs us if this server has this consumer assigned.\nfunc (jsa *jsAccount) consumerAssigned(stream, consumer string) bool {\n\tjsa.mu.RLock()\n\tjs, acc := jsa.js, jsa.account\n\tjsa.mu.RUnlock()\n\n\tif js == nil {\n\t\treturn false\n\t}\n\tjs.mu.RLock()\n\tdefer js.mu.RUnlock()\n\treturn js.cluster.isConsumerAssigned(acc, stream, consumer)\n}\n\n// Read lock should be held.\nfunc (cc *jetStreamCluster) isConsumerAssigned(a *Account, stream, consumer string) bool {\n\t// Non-clustered mode always return true.\n\tif cc == nil {\n\t\treturn true\n\t}\n\tif cc.meta == nil {\n\t\treturn false\n\t}\n\tvar sa *streamAssignment\n\taccStreams := cc.streams[a.Name]\n\tif accStreams != nil {\n\t\tsa = accStreams[stream]\n\t}\n\tif sa == nil {\n\t\t// TODO(dlc) - This should not happen.\n\t\treturn false\n\t}\n\tca := sa.consumers[consumer]\n\tif ca == nil {\n\t\treturn false\n\t}\n\trg := ca.Group\n\t// Check if we are the leader of this raftGroup assigned to the stream.\n\tourID := cc.meta.ID()\n\tfor _, peer := range rg.Peers {\n\t\tif peer == ourID {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\n// Returns our stream and underlying raft node.\nfunc (o *consumer) streamAndNode() (*stream, RaftNode) {\n\tif o == nil {\n\t\treturn nil, nil\n\t}\n\to.mu.RLock()\n\tdefer o.mu.RUnlock()\n\treturn o.mset, o.node\n}\n\n// Return the replica count for this consumer. If the consumer has been\n// stopped, this will return an error.\nfunc (o *consumer) replica() (int, error) {\n\to.mu.RLock()\n\toCfg := o.cfg\n\tmset := o.mset\n\to.mu.RUnlock()\n\tif mset == nil {\n\t\treturn 0, errBadConsumer\n\t}\n\tsCfg := mset.config()\n\treturn oCfg.replicas(&sCfg), nil\n}\n\nfunc (o *consumer) raftGroup() *raftGroup {\n\tif o == nil {\n\t\treturn nil\n\t}\n\to.mu.RLock()\n\tdefer o.mu.RUnlock()\n\tif o.ca == nil {\n\t\treturn nil\n\t}\n\treturn o.ca.Group\n}\n\nfunc (o *consumer) clearRaftNode() {\n\tif o == nil {\n\t\treturn\n\t}\n\to.mu.Lock()\n\tdefer o.mu.Unlock()\n\to.node = nil\n}\n\nfunc (o *consumer) raftNode() RaftNode {\n\tif o == nil {\n\t\treturn nil\n\t}\n\to.mu.RLock()\n\tdefer o.mu.RUnlock()\n\treturn o.node\n}\n\nfunc (js *jetStream) monitorConsumer(o *consumer, ca *consumerAssignment) {\n\ts, n, meta := js.server(), o.raftNode(), js.getMetaGroup()\n\tdefer s.grWG.Done()\n\n\tdefer o.clearMonitorRunning()\n\n\tif n == nil || meta == nil {\n\t\ts.Warnf(\"No RAFT group for '%s > %s > %s'\", o.acc.Name, ca.Stream, ca.Name)\n\t\treturn\n\t}\n\n\t// Make sure to stop the raft group on exit to prevent accidental memory bloat.\n\t// This should be below the checkInMonitor call though to avoid stopping it out\n\t// from underneath the one that is running since it will be the same raft node.\n\tdefer n.Stop()\n\n\tqch, lch, aq, uch, ourPeerId := n.QuitC(), n.LeadChangeC(), n.ApplyQ(), o.updateC(), meta.ID()\n\n\ts.Debugf(\"Starting consumer monitor for '%s > %s > %s' [%s]\", o.acc.Name, ca.Stream, ca.Name, n.Group())\n\tdefer s.Debugf(\"Exiting consumer monitor for '%s > %s > %s' [%s]\", o.acc.Name, ca.Stream, ca.Name, n.Group())\n\n\tconst (\n\t\tcompactInterval = 2 * time.Minute\n\t\tcompactSizeMin  = 64 * 1024 // What is stored here is always small for consumers.\n\t\tcompactNumMin   = 1024\n\t\tminSnapDelta    = 10 * time.Second\n\t)\n\n\t// Spread these out for large numbers on server restart.\n\trci := time.Duration(rand.Int63n(int64(time.Minute)))\n\tt := time.NewTicker(compactInterval + rci)\n\tdefer t.Stop()\n\n\t// Highwayhash key for generating hashes.\n\tkey := make([]byte, 32)\n\tcrand.Read(key)\n\n\t// Hash of the last snapshot (fixed size in memory).\n\tvar lastSnap []byte\n\tvar lastSnapTime time.Time\n\n\t// Don't allow the upper layer to install snapshots until we have\n\t// fully recovered from disk.\n\trecovering := true\n\n\tdoSnapshot := func(force bool) {\n\t\t// Bail if trying too fast and not in a forced situation.\n\t\tif recovering || (!force && time.Since(lastSnapTime) < minSnapDelta) {\n\t\t\treturn\n\t\t}\n\n\t\t// Check several things to see if we need a snapshot.\n\t\tne, nb := n.Size()\n\t\tif !n.NeedSnapshot() {\n\t\t\t// Check if we should compact etc. based on size of log.\n\t\t\tif !force && ne < compactNumMin && nb < compactSizeMin {\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\n\t\tif snap, err := o.store.EncodedState(); err == nil {\n\t\t\thash := highwayhash.Sum(snap, key)\n\t\t\t// If the state hasn't changed but the log has gone way over\n\t\t\t// the compaction size then we will want to compact anyway.\n\t\t\t// This can happen for example when a pull consumer fetches a\n\t\t\t// lot on an idle stream, log entries get distributed but the\n\t\t\t// state never changes, therefore the log never gets compacted.\n\t\t\tif !bytes.Equal(hash[:], lastSnap) || ne >= compactNumMin || nb >= compactSizeMin {\n\t\t\t\tif err := n.InstallSnapshot(snap); err == nil {\n\t\t\t\t\tlastSnap, lastSnapTime = hash[:], time.Now()\n\t\t\t\t} else if err != errNoSnapAvailable && err != errNodeClosed && err != errCatchupsRunning {\n\t\t\t\t\ts.RateLimitWarnf(\"Failed to install snapshot for '%s > %s > %s' [%s]: %v\", o.acc.Name, ca.Stream, ca.Name, n.Group(), err)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\t// For migration tracking.\n\tvar mmt *time.Ticker\n\tvar mmtc <-chan time.Time\n\n\tstartMigrationMonitoring := func() {\n\t\tif mmt == nil {\n\t\t\tmmt = time.NewTicker(500 * time.Millisecond)\n\t\t\tmmtc = mmt.C\n\t\t}\n\t}\n\n\tstopMigrationMonitoring := func() {\n\t\tif mmt != nil {\n\t\t\tmmt.Stop()\n\t\t\tmmt, mmtc = nil, nil\n\t\t}\n\t}\n\tdefer stopMigrationMonitoring()\n\n\t// Track if we are leader.\n\tvar isLeader bool\n\n\tfor {\n\t\tselect {\n\t\tcase <-s.quitCh:\n\t\t\t// Server shutting down, but we might receive this before qch, so try to snapshot.\n\t\t\tdoSnapshot(false)\n\t\t\treturn\n\t\tcase <-qch:\n\t\t\t// Clean signal from shutdown routine so do best effort attempt to snapshot.\n\t\t\tdoSnapshot(false)\n\t\t\treturn\n\t\tcase <-aq.ch:\n\t\t\tces := aq.pop()\n\t\t\tfor _, ce := range ces {\n\t\t\t\t// No special processing needed for when we are caught up on restart.\n\t\t\t\tif ce == nil {\n\t\t\t\t\trecovering = false\n\t\t\t\t\tif n.NeedSnapshot() {\n\t\t\t\t\t\tdoSnapshot(true)\n\t\t\t\t\t}\n\t\t\t\t} else if err := js.applyConsumerEntries(o, ce, isLeader); err == nil {\n\t\t\t\t\tvar ne, nb uint64\n\t\t\t\t\t// We can't guarantee writes are flushed while we're shutting down. Just rely on replay during recovery.\n\t\t\t\t\tif !js.isShuttingDown() {\n\t\t\t\t\t\tne, nb = n.Applied(ce.Index)\n\t\t\t\t\t}\n\t\t\t\t\tce.ReturnToPool()\n\t\t\t\t\t// If we have at least min entries to compact, go ahead and snapshot/compact.\n\t\t\t\t\tif nb > 0 && ne >= compactNumMin || nb > compactSizeMin {\n\t\t\t\t\t\tdoSnapshot(false)\n\t\t\t\t\t}\n\t\t\t\t} else if err != errConsumerClosed {\n\t\t\t\t\ts.Warnf(\"Error applying consumer entries to '%s > %s'\", ca.Client.serviceAccount(), ca.Name)\n\t\t\t\t}\n\t\t\t}\n\t\t\taq.recycle(&ces)\n\n\t\tcase isLeader = <-lch:\n\t\t\tif recovering && !isLeader {\n\t\t\t\tjs.setConsumerAssignmentRecovering(ca)\n\t\t\t}\n\n\t\t\t// Process the change.\n\t\t\tif err := js.processConsumerLeaderChange(o, isLeader); err == nil {\n\t\t\t\t// Check our state if we are under an interest based stream.\n\t\t\t\tif mset := o.getStream(); mset != nil {\n\t\t\t\t\tvar ss StreamState\n\t\t\t\t\tmset.store.FastState(&ss)\n\t\t\t\t\to.checkStateForInterestStream(&ss)\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// We may receive a leader change after the consumer assignment which would cancel us\n\t\t\t// monitoring for this closely. So re-assess our state here as well.\n\t\t\t// Or the old leader is no longer part of the set and transferred leadership\n\t\t\t// for this leader to resume with removal\n\t\t\trg := o.raftGroup()\n\n\t\t\t// Check for migrations (peer count and replica count differ) here.\n\t\t\t// We set the state on the stream assignment update below.\n\t\t\treplicas, err := o.replica()\n\t\t\tif err != nil {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif isLeader && len(rg.Peers) != replicas {\n\t\t\t\tstartMigrationMonitoring()\n\t\t\t} else {\n\t\t\t\tstopMigrationMonitoring()\n\t\t\t}\n\t\tcase <-uch:\n\t\t\t// keep consumer assignment current\n\t\t\tca = o.consumerAssignment()\n\t\t\t// We get this when we have a new consumer assignment caused by an update.\n\t\t\t// We want to know if we are migrating.\n\t\t\trg := o.raftGroup()\n\t\t\t// If we are migrating, monitor for the new peers to be caught up.\n\t\t\treplicas, err := o.replica()\n\t\t\tif err != nil {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif isLeader && len(rg.Peers) != replicas {\n\t\t\t\tstartMigrationMonitoring()\n\t\t\t} else {\n\t\t\t\tstopMigrationMonitoring()\n\t\t\t}\n\t\tcase <-mmtc:\n\t\t\tif !isLeader {\n\t\t\t\t// We are no longer leader, so not our job.\n\t\t\t\tstopMigrationMonitoring()\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\trg := o.raftGroup()\n\t\t\tci := js.clusterInfo(rg)\n\t\t\treplicas, err := o.replica()\n\t\t\tif err != nil {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif len(rg.Peers) <= replicas {\n\t\t\t\t// Migration no longer happening, so not our job anymore\n\t\t\t\tstopMigrationMonitoring()\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tnewPeers, oldPeers, newPeerSet, _ := genPeerInfo(rg.Peers, len(rg.Peers)-replicas)\n\n\t\t\t// If we are part of the new peerset and we have been passed the baton.\n\t\t\t// We will handle scale down.\n\t\t\tif newPeerSet[ourPeerId] {\n\t\t\t\tfor _, p := range oldPeers {\n\t\t\t\t\tn.ProposeRemovePeer(p)\n\t\t\t\t}\n\t\t\t\tcca := ca.copyGroup()\n\t\t\t\tcca.Group.Peers = newPeers\n\t\t\t\tcca.Group.Cluster = s.cachedClusterName()\n\t\t\t\tmeta.ForwardProposal(encodeAddConsumerAssignment(cca))\n\t\t\t\ts.Noticef(\"Scaling down '%s > %s > %s' to %+v\", ca.Client.serviceAccount(), ca.Stream, ca.Name, s.peerSetToNames(newPeers))\n\n\t\t\t} else {\n\t\t\t\tvar newLeaderPeer, newLeader, newCluster string\n\t\t\t\tneededCurrent, current := replicas/2+1, 0\n\t\t\t\tfor _, r := range ci.Replicas {\n\t\t\t\t\tif r.Current && newPeerSet[r.Peer] {\n\t\t\t\t\t\tcurrent++\n\t\t\t\t\t\tif newCluster == _EMPTY_ {\n\t\t\t\t\t\t\tnewLeaderPeer, newLeader, newCluster = r.Peer, r.Name, r.cluster\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\t// Check if we have a quorom\n\t\t\t\tif current >= neededCurrent {\n\t\t\t\t\ts.Noticef(\"Transfer of consumer leader for '%s > %s > %s' to '%s'\", ca.Client.serviceAccount(), ca.Stream, ca.Name, newLeader)\n\t\t\t\t\tn.StepDown(newLeaderPeer)\n\t\t\t\t}\n\t\t\t}\n\n\t\tcase <-t.C:\n\t\t\tdoSnapshot(false)\n\t\t}\n\t}\n}\n\nfunc (js *jetStream) applyConsumerEntries(o *consumer, ce *CommittedEntry, isLeader bool) error {\n\tfor _, e := range ce.Entries {\n\t\tif e.Type == EntrySnapshot {\n\t\t\tif !isLeader {\n\t\t\t\t// No-op needed?\n\t\t\t\tstate, err := decodeConsumerState(e.Data)\n\t\t\t\tif err != nil {\n\t\t\t\t\tif mset, node := o.streamAndNode(); mset != nil && node != nil {\n\t\t\t\t\t\ts := js.srv\n\t\t\t\t\t\ts.Errorf(\"JetStream cluster could not decode consumer snapshot for '%s > %s > %s' [%s]\",\n\t\t\t\t\t\t\tmset.account(), mset.name(), o, node.Group())\n\t\t\t\t\t}\n\t\t\t\t\tpanic(err.Error())\n\t\t\t\t}\n\n\t\t\t\tif err = o.store.Update(state); err != nil {\n\t\t\t\t\to.mu.RLock()\n\t\t\t\t\ts, acc, mset, name := o.srv, o.acc, o.mset, o.name\n\t\t\t\t\to.mu.RUnlock()\n\t\t\t\t\tif s != nil && mset != nil {\n\t\t\t\t\t\ts.Warnf(\"Consumer '%s > %s > %s' error on store update from snapshot entry: %v\", acc, mset.name(), name, err)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\t// Check our interest state if applicable.\n\t\t\t\tif mset := o.getStream(); mset != nil {\n\t\t\t\t\tvar ss StreamState\n\t\t\t\t\tmset.store.FastState(&ss)\n\t\t\t\t\t// We used to register preacks here if our ack floor was higher than the last sequence.\n\t\t\t\t\t// Now when streams catch up they properly call checkInterestState() and periodically run this as well.\n\t\t\t\t\t// If our states drift this could have allocated lots of pre-acks.\n\t\t\t\t\to.checkStateForInterestStream(&ss)\n\t\t\t\t}\n\t\t\t}\n\n\t\t} else if e.Type == EntryRemovePeer {\n\t\t\tjs.mu.RLock()\n\t\t\tvar ourID string\n\t\t\tif js.cluster != nil && js.cluster.meta != nil {\n\t\t\t\tourID = js.cluster.meta.ID()\n\t\t\t}\n\t\t\tjs.mu.RUnlock()\n\t\t\tif peer := string(e.Data); peer == ourID {\n\t\t\t\tshouldRemove := true\n\t\t\t\tif mset := o.getStream(); mset != nil {\n\t\t\t\t\tif sa := mset.streamAssignment(); sa != nil && sa.Group != nil {\n\t\t\t\t\t\tjs.mu.RLock()\n\t\t\t\t\t\tshouldRemove = !sa.Group.isMember(ourID)\n\t\t\t\t\t\tjs.mu.RUnlock()\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif shouldRemove {\n\t\t\t\t\to.stopWithFlags(true, false, false, false)\n\t\t\t\t}\n\t\t\t}\n\t\t} else if e.Type == EntryAddPeer {\n\t\t\t// Ignore for now.\n\t\t} else {\n\t\t\tbuf := e.Data\n\t\t\tswitch entryOp(buf[0]) {\n\t\t\tcase updateDeliveredOp:\n\t\t\t\tdseq, sseq, dc, ts, err := decodeDeliveredUpdate(buf[1:])\n\t\t\t\tif err != nil {\n\t\t\t\t\tif mset, node := o.streamAndNode(); mset != nil && node != nil {\n\t\t\t\t\t\ts := js.srv\n\t\t\t\t\t\ts.Errorf(\"JetStream cluster could not decode consumer delivered update for '%s > %s > %s' [%s]\",\n\t\t\t\t\t\t\tmset.account(), mset.name(), o, node.Group())\n\t\t\t\t\t}\n\t\t\t\t\tpanic(err.Error())\n\t\t\t\t}\n\t\t\t\t// Make sure to update delivered under the lock.\n\t\t\t\to.mu.Lock()\n\t\t\t\terr = o.store.UpdateDelivered(dseq, sseq, dc, ts)\n\t\t\t\to.ldt = time.Now()\n\t\t\t\t// Need to send message to the client, since we have quorum to do so now.\n\t\t\t\tif pmsg, ok := o.pendingDeliveries[sseq]; ok {\n\t\t\t\t\to.outq.send(pmsg)\n\t\t\t\t\tdelete(o.pendingDeliveries, sseq)\n\t\t\t\t}\n\t\t\t\to.mu.Unlock()\n\t\t\t\tif err != nil {\n\t\t\t\t\tpanic(err.Error())\n\t\t\t\t}\n\t\t\tcase updateAcksOp:\n\t\t\t\tdseq, sseq, err := decodeAckUpdate(buf[1:])\n\t\t\t\tif err != nil {\n\t\t\t\t\tif mset, node := o.streamAndNode(); mset != nil && node != nil {\n\t\t\t\t\t\ts := js.srv\n\t\t\t\t\t\ts.Errorf(\"JetStream cluster could not decode consumer ack update for '%s > %s > %s' [%s]\",\n\t\t\t\t\t\t\tmset.account(), mset.name(), o, node.Group())\n\t\t\t\t\t}\n\t\t\t\t\tpanic(err.Error())\n\t\t\t\t}\n\t\t\t\tif err := o.processReplicatedAck(dseq, sseq); err == errConsumerClosed {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\tcase updateSkipOp:\n\t\t\t\to.mu.Lock()\n\t\t\t\tvar le = binary.LittleEndian\n\t\t\t\tsseq := le.Uint64(buf[1:])\n\t\t\t\tif !o.isLeader() && sseq > o.sseq {\n\t\t\t\t\to.sseq = sseq\n\t\t\t\t}\n\t\t\t\tif o.store != nil {\n\t\t\t\t\to.store.UpdateStarting(sseq - 1)\n\t\t\t\t}\n\t\t\t\to.mu.Unlock()\n\t\t\tcase addPendingRequest:\n\t\t\t\to.mu.Lock()\n\t\t\t\tif !o.isLeader() {\n\t\t\t\t\tif o.prm == nil {\n\t\t\t\t\t\to.prm = make(map[string]struct{})\n\t\t\t\t\t}\n\t\t\t\t\to.prm[string(buf[1:])] = struct{}{}\n\t\t\t\t}\n\t\t\t\to.mu.Unlock()\n\t\t\tcase removePendingRequest:\n\t\t\t\to.mu.Lock()\n\t\t\t\tif !o.isLeader() {\n\t\t\t\t\tif o.prm != nil {\n\t\t\t\t\t\tdelete(o.prm, string(buf[1:]))\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\to.mu.Unlock()\n\t\t\tdefault:\n\t\t\t\tpanic(fmt.Sprintf(\"JetStream Cluster Unknown group entry op type: %v\", entryOp(buf[0])))\n\t\t\t}\n\t\t}\n\t}\n\treturn nil\n}\n\nvar errConsumerClosed = errors.New(\"consumer closed\")\n\nfunc (o *consumer) processReplicatedAck(dseq, sseq uint64) error {\n\to.mu.Lock()\n\t// Update activity.\n\to.lat = time.Now()\n\n\tvar sagap uint64\n\tif o.cfg.AckPolicy == AckAll {\n\t\t// Always use the store state, as o.asflr is skipped ahead already.\n\t\t// Capture before updating store.\n\t\tstate, err := o.store.BorrowState()\n\t\tif err == nil {\n\t\t\tsagap = sseq - state.AckFloor.Stream\n\t\t}\n\t}\n\n\t// Do actual ack update to store.\n\t// Always do this to have it recorded.\n\to.store.UpdateAcks(dseq, sseq)\n\n\tmset := o.mset\n\tif o.closed || mset == nil {\n\t\to.mu.Unlock()\n\t\treturn errConsumerClosed\n\t}\n\tif mset.closed.Load() {\n\t\to.mu.Unlock()\n\t\treturn errStreamClosed\n\t}\n\n\t// Check if we have a reply that was requested.\n\tif reply := o.replies[sseq]; reply != _EMPTY_ {\n\t\to.outq.sendMsg(reply, nil)\n\t\tdelete(o.replies, sseq)\n\t}\n\n\tif o.retention == LimitsPolicy {\n\t\to.mu.Unlock()\n\t\treturn nil\n\t}\n\to.mu.Unlock()\n\n\tif sagap > 1 {\n\t\t// FIXME(dlc) - This is very inefficient, will need to fix.\n\t\tfor seq := sseq; seq > sseq-sagap; seq-- {\n\t\t\tmset.ackMsg(o, seq)\n\t\t}\n\t} else {\n\t\tmset.ackMsg(o, sseq)\n\t}\n\treturn nil\n}\n\nvar errBadAckUpdate = errors.New(\"jetstream cluster bad replicated ack update\")\nvar errBadDeliveredUpdate = errors.New(\"jetstream cluster bad replicated delivered update\")\n\nfunc decodeAckUpdate(buf []byte) (dseq, sseq uint64, err error) {\n\tvar bi, n int\n\tif dseq, n = binary.Uvarint(buf); n < 0 {\n\t\treturn 0, 0, errBadAckUpdate\n\t}\n\tbi += n\n\tif sseq, n = binary.Uvarint(buf[bi:]); n < 0 {\n\t\treturn 0, 0, errBadAckUpdate\n\t}\n\treturn dseq, sseq, nil\n}\n\nfunc decodeDeliveredUpdate(buf []byte) (dseq, sseq, dc uint64, ts int64, err error) {\n\tvar bi, n int\n\tif dseq, n = binary.Uvarint(buf); n < 0 {\n\t\treturn 0, 0, 0, 0, errBadDeliveredUpdate\n\t}\n\tbi += n\n\tif sseq, n = binary.Uvarint(buf[bi:]); n < 0 {\n\t\treturn 0, 0, 0, 0, errBadDeliveredUpdate\n\t}\n\tbi += n\n\tif dc, n = binary.Uvarint(buf[bi:]); n < 0 {\n\t\treturn 0, 0, 0, 0, errBadDeliveredUpdate\n\t}\n\tbi += n\n\tif ts, n = binary.Varint(buf[bi:]); n < 0 {\n\t\treturn 0, 0, 0, 0, errBadDeliveredUpdate\n\t}\n\treturn dseq, sseq, dc, ts, nil\n}\n\nfunc (js *jetStream) processConsumerLeaderChange(o *consumer, isLeader bool) error {\n\tstepDownIfLeader := func() error {\n\t\tif node := o.raftNode(); node != nil && isLeader {\n\t\t\tnode.StepDown()\n\t\t}\n\t\treturn errors.New(\"failed to update consumer leader status\")\n\t}\n\n\tif o == nil || o.isClosed() {\n\t\treturn stepDownIfLeader()\n\t}\n\n\tca := o.consumerAssignment()\n\tif ca == nil {\n\t\treturn stepDownIfLeader()\n\t}\n\tjs.mu.Lock()\n\ts, account, err := js.srv, ca.Client.serviceAccount(), ca.err\n\tclient, subject, reply, streamName, consumerName := ca.Client, ca.Subject, ca.Reply, ca.Stream, ca.Name\n\thasResponded := ca.responded\n\tca.responded = true\n\tjs.mu.Unlock()\n\n\tacc, _ := s.LookupAccount(account)\n\tif acc == nil {\n\t\treturn stepDownIfLeader()\n\t}\n\n\tif isLeader {\n\t\ts.Noticef(\"JetStream cluster new consumer leader for '%s > %s > %s'\", ca.Client.serviceAccount(), streamName, consumerName)\n\t\ts.sendConsumerLeaderElectAdvisory(o)\n\t} else {\n\t\t// We are stepping down.\n\t\t// Make sure if we are doing so because we have lost quorum that we send the appropriate advisories.\n\t\tif node := o.raftNode(); node != nil && !node.Quorum() && time.Since(node.Created()) > 5*time.Second {\n\t\t\ts.sendConsumerLostQuorumAdvisory(o)\n\t\t}\n\t}\n\n\t// Tell consumer to switch leader status.\n\to.setLeader(isLeader)\n\n\tif !isLeader || hasResponded {\n\t\tif isLeader {\n\t\t\to.clearInitialInfo()\n\t\t}\n\t\treturn nil\n\t}\n\n\tvar resp = JSApiConsumerCreateResponse{ApiResponse: ApiResponse{Type: JSApiConsumerCreateResponseType}}\n\tif err != nil {\n\t\tresp.Error = NewJSConsumerCreateError(err, Unless(err))\n\t\ts.sendAPIErrResponse(client, acc, subject, reply, _EMPTY_, s.jsonResponse(&resp))\n\t} else {\n\t\tresp.ConsumerInfo = setDynamicConsumerInfoMetadata(o.initialInfo())\n\t\ts.sendAPIResponse(client, acc, subject, reply, _EMPTY_, s.jsonResponse(&resp))\n\t\to.sendCreateAdvisory()\n\t}\n\n\t// Only send a pause advisory on consumer create if we're\n\t// actually paused. The timer would have been kicked by now\n\t// by the call to o.setLeader() above.\n\tif isLeader && o.cfg.PauseUntil != nil && !o.cfg.PauseUntil.IsZero() && time.Now().Before(*o.cfg.PauseUntil) {\n\t\to.sendPauseAdvisoryLocked(&o.cfg)\n\t}\n\n\treturn nil\n}\n\n// Determines if we should send lost quorum advisory. We throttle these after first one.\nfunc (o *consumer) shouldSendLostQuorum() bool {\n\to.mu.Lock()\n\tdefer o.mu.Unlock()\n\tif time.Since(o.lqsent) >= lostQuorumAdvInterval {\n\t\to.lqsent = time.Now()\n\t\treturn true\n\t}\n\treturn false\n}\n\nfunc (s *Server) sendConsumerLostQuorumAdvisory(o *consumer) {\n\tif o == nil {\n\t\treturn\n\t}\n\tnode, stream, consumer, acc := o.raftNode(), o.streamName(), o.String(), o.account()\n\tif node == nil {\n\t\treturn\n\t}\n\tif !o.shouldSendLostQuorum() {\n\t\treturn\n\t}\n\n\ts.Warnf(\"JetStream cluster consumer '%s > %s > %s' has NO quorum, stalled.\", acc.GetName(), stream, consumer)\n\n\tsubj := JSAdvisoryConsumerQuorumLostPre + \".\" + stream + \".\" + consumer\n\tadv := &JSConsumerQuorumLostAdvisory{\n\t\tTypedEvent: TypedEvent{\n\t\t\tType: JSConsumerQuorumLostAdvisoryType,\n\t\t\tID:   nuid.Next(),\n\t\t\tTime: time.Now().UTC(),\n\t\t},\n\t\tStream:   stream,\n\t\tConsumer: consumer,\n\t\tReplicas: s.replicas(node),\n\t\tDomain:   s.getOpts().JetStreamDomain,\n\t}\n\n\t// Send to the user's account if not the system account.\n\tif acc != s.SystemAccount() {\n\t\ts.publishAdvisory(acc, subj, adv)\n\t}\n\t// Now do system level one. Place account info in adv, and nil account means system.\n\tadv.Account = acc.GetName()\n\ts.publishAdvisory(nil, subj, adv)\n}\n\nfunc (s *Server) sendConsumerLeaderElectAdvisory(o *consumer) {\n\tif o == nil {\n\t\treturn\n\t}\n\tnode, stream, consumer, acc := o.raftNode(), o.streamName(), o.String(), o.account()\n\tif node == nil {\n\t\treturn\n\t}\n\n\tsubj := JSAdvisoryConsumerLeaderElectedPre + \".\" + stream + \".\" + consumer\n\tadv := &JSConsumerLeaderElectedAdvisory{\n\t\tTypedEvent: TypedEvent{\n\t\t\tType: JSConsumerLeaderElectedAdvisoryType,\n\t\t\tID:   nuid.Next(),\n\t\t\tTime: time.Now().UTC(),\n\t\t},\n\t\tStream:   stream,\n\t\tConsumer: consumer,\n\t\tLeader:   s.serverNameForNode(node.GroupLeader()),\n\t\tReplicas: s.replicas(node),\n\t\tDomain:   s.getOpts().JetStreamDomain,\n\t}\n\n\t// Send to the user's account if not the system account.\n\tif acc != s.SystemAccount() {\n\t\ts.publishAdvisory(acc, subj, adv)\n\t}\n\t// Now do system level one. Place account info in adv, and nil account means system.\n\tadv.Account = acc.GetName()\n\ts.publishAdvisory(nil, subj, adv)\n}\n\ntype streamAssignmentResult struct {\n\tAccount  string                      `json:\"account\"`\n\tStream   string                      `json:\"stream\"`\n\tResponse *JSApiStreamCreateResponse  `json:\"create_response,omitempty\"`\n\tRestore  *JSApiStreamRestoreResponse `json:\"restore_response,omitempty\"`\n\tUpdate   bool                        `json:\"is_update,omitempty\"`\n}\n\n// Determine if this is an insufficient resources' error type.\nfunc isInsufficientResourcesErr(resp *JSApiStreamCreateResponse) bool {\n\treturn resp != nil && resp.Error != nil && IsNatsErr(resp.Error, JSInsufficientResourcesErr, JSMemoryResourcesExceededErr, JSStorageResourcesExceededErr)\n}\n\n// Process error results of stream and consumer assignments.\n// Success will be handled by stream leader.\nfunc (js *jetStream) processStreamAssignmentResults(sub *subscription, c *client, _ *Account, subject, reply string, msg []byte) {\n\tvar result streamAssignmentResult\n\tif err := json.Unmarshal(msg, &result); err != nil {\n\t\t// TODO(dlc) - log\n\t\treturn\n\t}\n\tacc, _ := js.srv.LookupAccount(result.Account)\n\tif acc == nil {\n\t\t// TODO(dlc) - log\n\t\treturn\n\t}\n\n\tjs.mu.Lock()\n\tdefer js.mu.Unlock()\n\n\ts, cc := js.srv, js.cluster\n\tif cc == nil || cc.meta == nil {\n\t\treturn\n\t}\n\n\t// This should have been done already in processStreamAssignment, but in\n\t// case we have a code path that gets here with no processStreamAssignment,\n\t// then we will do the proper thing. Otherwise will be a no-op.\n\tcc.removeInflightProposal(result.Account, result.Stream)\n\n\tif sa := js.streamAssignment(result.Account, result.Stream); sa != nil && !sa.reassigning {\n\t\tcanDelete := !result.Update && time.Since(sa.Created) < 5*time.Second\n\n\t\t// See if we should retry in case this cluster is full but there are others.\n\t\tif cfg, ci := sa.Config, sa.Client; cfg != nil && ci != nil && isInsufficientResourcesErr(result.Response) && canDelete {\n\t\t\t// If cluster is defined we can not retry.\n\t\t\tif cfg.Placement == nil || cfg.Placement.Cluster == _EMPTY_ {\n\t\t\t\t// If we have additional clusters to try we can retry.\n\t\t\t\t// We have already verified that ci != nil.\n\t\t\t\tif len(ci.Alternates) > 0 {\n\t\t\t\t\tif rg, err := js.createGroupForStream(ci, cfg); err != nil {\n\t\t\t\t\t\ts.Warnf(\"Retrying cluster placement for stream '%s > %s' failed due to placement error: %+v\", result.Account, result.Stream, err)\n\t\t\t\t\t} else {\n\t\t\t\t\t\tif org := sa.Group; org != nil && len(org.Peers) > 0 {\n\t\t\t\t\t\t\ts.Warnf(\"Retrying cluster placement for stream '%s > %s' due to insufficient resources in cluster %q\",\n\t\t\t\t\t\t\t\tresult.Account, result.Stream, s.clusterNameForNode(org.Peers[0]))\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\ts.Warnf(\"Retrying cluster placement for stream '%s > %s' due to insufficient resources\", result.Account, result.Stream)\n\t\t\t\t\t\t}\n\t\t\t\t\t\t// Pick a new preferred leader.\n\t\t\t\t\t\trg.setPreferred()\n\t\t\t\t\t\t// Get rid of previous attempt.\n\t\t\t\t\t\tcc.meta.Propose(encodeDeleteStreamAssignment(sa))\n\t\t\t\t\t\t// Propose new.\n\t\t\t\t\t\tsa.Group, sa.err = rg, nil\n\t\t\t\t\t\tcc.meta.Propose(encodeAddStreamAssignment(sa))\n\t\t\t\t\t\t// When the new stream assignment is processed, sa.reassigning will be\n\t\t\t\t\t\t// automatically set back to false. Until then, don't process any more\n\t\t\t\t\t\t// assignment results.\n\t\t\t\t\t\tsa.reassigning = true\n\t\t\t\t\t\treturn\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t// Respond to the user here.\n\t\tvar resp string\n\t\tif result.Response != nil {\n\t\t\tresp = s.jsonResponse(result.Response)\n\t\t} else if result.Restore != nil {\n\t\t\tresp = s.jsonResponse(result.Restore)\n\t\t}\n\t\tif !sa.responded || result.Update {\n\t\t\tsa.responded = true\n\t\t\tjs.srv.sendAPIErrResponse(sa.Client, acc, sa.Subject, sa.Reply, _EMPTY_, resp)\n\t\t}\n\t\t// Remove this assignment if possible.\n\t\tif canDelete {\n\t\t\tsa.err = NewJSClusterNotAssignedError()\n\t\t\tcc.meta.Propose(encodeDeleteStreamAssignment(sa))\n\t\t}\n\t}\n}\n\nfunc (js *jetStream) processConsumerAssignmentResults(sub *subscription, c *client, _ *Account, subject, reply string, msg []byte) {\n\tvar result consumerAssignmentResult\n\tif err := json.Unmarshal(msg, &result); err != nil {\n\t\t// TODO(dlc) - log\n\t\treturn\n\t}\n\tacc, _ := js.srv.LookupAccount(result.Account)\n\tif acc == nil {\n\t\t// TODO(dlc) - log\n\t\treturn\n\t}\n\n\tjs.mu.Lock()\n\tdefer js.mu.Unlock()\n\n\ts, cc := js.srv, js.cluster\n\tif cc == nil || cc.meta == nil {\n\t\treturn\n\t}\n\n\tif sa := js.streamAssignment(result.Account, result.Stream); sa != nil && sa.consumers != nil {\n\t\tif ca := sa.consumers[result.Consumer]; ca != nil && !ca.responded {\n\t\t\tjs.srv.sendAPIErrResponse(ca.Client, acc, ca.Subject, ca.Reply, _EMPTY_, s.jsonResponse(result.Response))\n\t\t\tca.responded = true\n\n\t\t\t// Check if this failed.\n\t\t\t// TODO(dlc) - Could have mixed results, should track per peer.\n\t\t\t// Make sure this is recent response, do not delete existing consumers.\n\t\t\tif result.Response.Error != nil && result.Response.Error != NewJSConsumerNameExistError() && time.Since(ca.Created) < 2*time.Second {\n\t\t\t\t// So while we are deleting we will not respond to list/names requests.\n\t\t\t\tca.err = NewJSClusterNotAssignedError()\n\t\t\t\tcc.meta.Propose(encodeDeleteConsumerAssignment(ca))\n\t\t\t\ts.Warnf(\"Proposing to delete consumer `%s > %s > %s' due to assignment response error: %v\",\n\t\t\t\t\tresult.Account, result.Stream, result.Consumer, result.Response.Error)\n\t\t\t}\n\t\t}\n\t}\n}\n\nconst (\n\tstreamAssignmentSubj   = \"$SYS.JSC.STREAM.ASSIGNMENT.RESULT\"\n\tconsumerAssignmentSubj = \"$SYS.JSC.CONSUMER.ASSIGNMENT.RESULT\"\n)\n\n// Lock should be held.\nfunc (js *jetStream) startUpdatesSub() {\n\tcc, s, c := js.cluster, js.srv, js.cluster.c\n\tif cc.streamResults == nil {\n\t\tcc.streamResults, _ = s.systemSubscribe(streamAssignmentSubj, _EMPTY_, false, c, js.processStreamAssignmentResults)\n\t}\n\tif cc.consumerResults == nil {\n\t\tcc.consumerResults, _ = s.systemSubscribe(consumerAssignmentSubj, _EMPTY_, false, c, js.processConsumerAssignmentResults)\n\t}\n\tif cc.stepdown == nil {\n\t\tcc.stepdown, _ = s.systemSubscribe(JSApiLeaderStepDown, _EMPTY_, false, c, s.jsLeaderStepDownRequest)\n\t}\n\tif cc.peerRemove == nil {\n\t\tcc.peerRemove, _ = s.systemSubscribe(JSApiRemoveServer, _EMPTY_, false, c, s.jsLeaderServerRemoveRequest)\n\t}\n\tif cc.peerStreamMove == nil {\n\t\tcc.peerStreamMove, _ = s.systemSubscribe(JSApiServerStreamMove, _EMPTY_, false, c, s.jsLeaderServerStreamMoveRequest)\n\t}\n\tif cc.peerStreamCancelMove == nil {\n\t\tcc.peerStreamCancelMove, _ = s.systemSubscribe(JSApiServerStreamCancelMove, _EMPTY_, false, c, s.jsLeaderServerStreamCancelMoveRequest)\n\t}\n\tif js.accountPurge == nil {\n\t\tjs.accountPurge, _ = s.systemSubscribe(JSApiAccountPurge, _EMPTY_, false, c, s.jsLeaderAccountPurgeRequest)\n\t}\n}\n\n// Lock should be held.\nfunc (js *jetStream) stopUpdatesSub() {\n\tcc := js.cluster\n\tif cc.streamResults != nil {\n\t\tcc.s.sysUnsubscribe(cc.streamResults)\n\t\tcc.streamResults = nil\n\t}\n\tif cc.consumerResults != nil {\n\t\tcc.s.sysUnsubscribe(cc.consumerResults)\n\t\tcc.consumerResults = nil\n\t}\n\tif cc.stepdown != nil {\n\t\tcc.s.sysUnsubscribe(cc.stepdown)\n\t\tcc.stepdown = nil\n\t}\n\tif cc.peerRemove != nil {\n\t\tcc.s.sysUnsubscribe(cc.peerRemove)\n\t\tcc.peerRemove = nil\n\t}\n\tif cc.peerStreamMove != nil {\n\t\tcc.s.sysUnsubscribe(cc.peerStreamMove)\n\t\tcc.peerStreamMove = nil\n\t}\n\tif cc.peerStreamCancelMove != nil {\n\t\tcc.s.sysUnsubscribe(cc.peerStreamCancelMove)\n\t\tcc.peerStreamCancelMove = nil\n\t}\n\tif js.accountPurge != nil {\n\t\tcc.s.sysUnsubscribe(js.accountPurge)\n\t\tjs.accountPurge = nil\n\t}\n}\n\nfunc (s *Server) sendDomainLeaderElectAdvisory() {\n\tjs, cc := s.getJetStreamCluster()\n\tif js == nil || cc == nil {\n\t\treturn\n\t}\n\n\tjs.mu.RLock()\n\tnode := cc.meta\n\tjs.mu.RUnlock()\n\n\tadv := &JSDomainLeaderElectedAdvisory{\n\t\tTypedEvent: TypedEvent{\n\t\t\tType: JSDomainLeaderElectedAdvisoryType,\n\t\t\tID:   nuid.Next(),\n\t\t\tTime: time.Now().UTC(),\n\t\t},\n\t\tLeader:   node.GroupLeader(),\n\t\tReplicas: s.replicas(node),\n\t\tCluster:  s.cachedClusterName(),\n\t\tDomain:   s.getOpts().JetStreamDomain,\n\t}\n\n\ts.publishAdvisory(nil, JSAdvisoryDomainLeaderElected, adv)\n}\n\nfunc (js *jetStream) processLeaderChange(isLeader bool) {\n\tif js == nil {\n\t\treturn\n\t}\n\ts := js.srv\n\tif s == nil {\n\t\treturn\n\t}\n\t// Update our server atomic.\n\ts.isMetaLeader.Store(isLeader)\n\n\tif isLeader {\n\t\ts.Noticef(\"Self is new JetStream cluster metadata leader\")\n\t\ts.sendDomainLeaderElectAdvisory()\n\t} else {\n\t\tvar node string\n\t\tif meta := js.getMetaGroup(); meta != nil {\n\t\t\tnode = meta.GroupLeader()\n\t\t}\n\t\tif node == _EMPTY_ {\n\t\t\ts.Noticef(\"JetStream cluster no metadata leader\")\n\t\t} else if srv := js.srv.serverNameForNode(node); srv == _EMPTY_ {\n\t\t\ts.Noticef(\"JetStream cluster new remote metadata leader\")\n\t\t} else if clst := js.srv.clusterNameForNode(node); clst == _EMPTY_ {\n\t\t\ts.Noticef(\"JetStream cluster new metadata leader: %s\", srv)\n\t\t} else {\n\t\t\ts.Noticef(\"JetStream cluster new metadata leader: %s/%s\", srv, clst)\n\t\t}\n\t}\n\n\tjs.mu.Lock()\n\tdefer js.mu.Unlock()\n\n\tif isLeader {\n\t\tif meta := js.cluster.meta; meta != nil && meta.IsObserver() {\n\t\t\tmeta.StepDown()\n\t\t\treturn\n\t\t}\n\t}\n\n\tif isLeader {\n\t\tjs.startUpdatesSub()\n\t} else {\n\t\tjs.stopUpdatesSub()\n\t\t// TODO(dlc) - stepdown.\n\t}\n\n\t// If we have been signaled to check the streams, this is for a bug that left stream\n\t// assignments with no sync subject after an update and no way to sync/catchup outside of the RAFT layer.\n\tif isLeader && js.cluster.streamsCheck {\n\t\tcc := js.cluster\n\t\tfor acc, asa := range cc.streams {\n\t\t\tfor _, sa := range asa {\n\t\t\t\tif sa.Sync == _EMPTY_ {\n\t\t\t\t\ts.Warnf(\"Stream assignment corrupt for stream '%s > %s'\", acc, sa.Config.Name)\n\t\t\t\t\tnsa := &streamAssignment{Group: sa.Group, Config: sa.Config, Subject: sa.Subject, Reply: sa.Reply, Client: sa.Client}\n\t\t\t\t\tnsa.Sync = syncSubjForStream()\n\t\t\t\t\tcc.meta.Propose(encodeUpdateStreamAssignment(nsa))\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t// Clear check.\n\t\tcc.streamsCheck = false\n\t}\n}\n\n// Lock should be held.\nfunc (cc *jetStreamCluster) remapStreamAssignment(sa *streamAssignment, removePeer string) bool {\n\t// Invoke placement algo passing RG peers that stay (existing) and the peer that is being removed (ignore)\n\tvar retain, ignore []string\n\tfor _, v := range sa.Group.Peers {\n\t\tif v == removePeer {\n\t\t\tignore = append(ignore, v)\n\t\t} else {\n\t\t\tretain = append(retain, v)\n\t\t}\n\t}\n\n\tnewPeers, placementError := cc.selectPeerGroup(len(sa.Group.Peers), sa.Group.Cluster, sa.Config, retain, 0, ignore)\n\n\tif placementError == nil {\n\t\tsa.Group.Peers = newPeers\n\t\t// Don't influence preferred leader.\n\t\tsa.Group.Preferred = _EMPTY_\n\t\treturn true\n\t}\n\n\t// If R1 just return to avoid bricking the stream.\n\tif sa.Group.node == nil || len(sa.Group.Peers) == 1 {\n\t\treturn false\n\t}\n\n\t// If we are here let's remove the peer at least, as long as we are R>1\n\tfor i, peer := range sa.Group.Peers {\n\t\tif peer == removePeer {\n\t\t\tsa.Group.Peers[i] = sa.Group.Peers[len(sa.Group.Peers)-1]\n\t\t\tsa.Group.Peers = sa.Group.Peers[:len(sa.Group.Peers)-1]\n\t\t\tbreak\n\t\t}\n\t}\n\treturn false\n}\n\ntype selectPeerError struct {\n\texcludeTag  bool\n\toffline     bool\n\tnoStorage   bool\n\tuniqueTag   bool\n\tmisc        bool\n\tnoJsClust   bool\n\tnoMatchTags map[string]struct{}\n\texcludeTags map[string]struct{}\n}\n\nfunc (e *selectPeerError) Error() string {\n\tb := strings.Builder{}\n\twriteBoolErrReason := func(hasErr bool, errMsg string) {\n\t\tif !hasErr {\n\t\t\treturn\n\t\t}\n\t\tb.WriteString(\", \")\n\t\tb.WriteString(errMsg)\n\t}\n\tb.WriteString(\"no suitable peers for placement\")\n\twriteBoolErrReason(e.offline, \"peer offline\")\n\twriteBoolErrReason(e.excludeTag, \"exclude tag set\")\n\twriteBoolErrReason(e.noStorage, \"insufficient storage\")\n\twriteBoolErrReason(e.uniqueTag, \"server tag not unique\")\n\twriteBoolErrReason(e.misc, \"miscellaneous issue\")\n\twriteBoolErrReason(e.noJsClust, \"jetstream not enabled in cluster\")\n\tif len(e.noMatchTags) != 0 {\n\t\tb.WriteString(\", tags not matched [\")\n\t\tvar firstTagWritten bool\n\t\tfor tag := range e.noMatchTags {\n\t\t\tif firstTagWritten {\n\t\t\t\tb.WriteString(\", \")\n\t\t\t}\n\t\t\tfirstTagWritten = true\n\t\t\tb.WriteRune('\\'')\n\t\t\tb.WriteString(tag)\n\t\t\tb.WriteRune('\\'')\n\t\t}\n\t\tb.WriteString(\"]\")\n\t}\n\tif len(e.excludeTags) != 0 {\n\t\tb.WriteString(\", tags excluded [\")\n\t\tvar firstTagWritten bool\n\t\tfor tag := range e.excludeTags {\n\t\t\tif firstTagWritten {\n\t\t\t\tb.WriteString(\", \")\n\t\t\t}\n\t\t\tfirstTagWritten = true\n\t\t\tb.WriteRune('\\'')\n\t\t\tb.WriteString(tag)\n\t\t\tb.WriteRune('\\'')\n\t\t}\n\t\tb.WriteString(\"]\")\n\t}\n\n\treturn b.String()\n}\n\nfunc (e *selectPeerError) addMissingTag(t string) {\n\tif e.noMatchTags == nil {\n\t\te.noMatchTags = map[string]struct{}{}\n\t}\n\te.noMatchTags[t] = struct{}{}\n}\n\nfunc (e *selectPeerError) addExcludeTag(t string) {\n\tif e.excludeTags == nil {\n\t\te.excludeTags = map[string]struct{}{}\n\t}\n\te.excludeTags[t] = struct{}{}\n}\n\nfunc (e *selectPeerError) accumulate(eAdd *selectPeerError) {\n\tif eAdd == nil {\n\t\treturn\n\t}\n\tacc := func(val *bool, valAdd bool) {\n\t\tif valAdd {\n\t\t\t*val = valAdd\n\t\t}\n\t}\n\tacc(&e.offline, eAdd.offline)\n\tacc(&e.excludeTag, eAdd.excludeTag)\n\tacc(&e.noStorage, eAdd.noStorage)\n\tacc(&e.uniqueTag, eAdd.uniqueTag)\n\tacc(&e.misc, eAdd.misc)\n\tacc(&e.noJsClust, eAdd.noJsClust)\n\tfor tag := range eAdd.noMatchTags {\n\t\te.addMissingTag(tag)\n\t}\n\tfor tag := range eAdd.excludeTags {\n\t\te.addExcludeTag(tag)\n\t}\n}\n\n// selectPeerGroup will select a group of peers to start a raft group.\n// when peers exist already the unique tag prefix check for the replaceFirstExisting will be skipped\n// js lock should be held.\nfunc (cc *jetStreamCluster) selectPeerGroup(r int, cluster string, cfg *StreamConfig, existing []string, replaceFirstExisting int, ignore []string) ([]string, *selectPeerError) {\n\tif cluster == _EMPTY_ || cfg == nil {\n\t\treturn nil, &selectPeerError{misc: true}\n\t}\n\n\tvar maxBytes uint64\n\tif cfg.MaxBytes > 0 {\n\t\tmaxBytes = uint64(cfg.MaxBytes)\n\t}\n\n\t// Check for tags.\n\ttype tagInfo struct {\n\t\ttag     string\n\t\texclude bool\n\t}\n\tvar ti []tagInfo\n\tif cfg.Placement != nil {\n\t\tti = make([]tagInfo, 0, len(cfg.Placement.Tags))\n\t\tfor _, t := range cfg.Placement.Tags {\n\t\t\tti = append(ti, tagInfo{\n\t\t\t\ttag:     strings.TrimPrefix(t, \"!\"),\n\t\t\t\texclude: strings.HasPrefix(t, \"!\"),\n\t\t\t})\n\t\t}\n\t}\n\n\t// Used for weighted sorting based on availability.\n\ttype wn struct {\n\t\tid    string\n\t\tavail uint64\n\t\tha    int\n\t\tns    int\n\t}\n\n\tvar nodes []wn\n\t// peers is a randomized list\n\ts, peers := cc.s, cc.meta.Peers()\n\n\tuniqueTagPrefix := s.getOpts().JetStreamUniqueTag\n\tif uniqueTagPrefix != _EMPTY_ {\n\t\tfor _, t := range ti {\n\t\t\tif strings.HasPrefix(t.tag, uniqueTagPrefix) {\n\t\t\t\t// disable uniqueness check if explicitly listed in tags\n\t\t\t\tuniqueTagPrefix = _EMPTY_\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t}\n\tvar uniqueTags = make(map[string]*nodeInfo)\n\n\tcheckUniqueTag := func(ni *nodeInfo) (bool, *nodeInfo) {\n\t\tfor _, t := range ni.tags {\n\t\t\tif strings.HasPrefix(t, uniqueTagPrefix) {\n\t\t\t\tif n, ok := uniqueTags[t]; !ok {\n\t\t\t\t\tuniqueTags[t] = ni\n\t\t\t\t\treturn true, ni\n\t\t\t\t} else {\n\t\t\t\t\treturn false, n\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t// default requires the unique prefix to be present\n\t\treturn false, nil\n\t}\n\n\t// Map existing.\n\tvar ep map[string]struct{}\n\tif le := len(existing); le > 0 {\n\t\tif le >= r {\n\t\t\treturn existing[:r], nil\n\t\t}\n\t\tep = make(map[string]struct{})\n\t\tfor i, p := range existing {\n\t\t\tep[p] = struct{}{}\n\t\t\tif uniqueTagPrefix == _EMPTY_ {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tsi, ok := s.nodeToInfo.Load(p)\n\t\t\tif !ok || si == nil || i < replaceFirstExisting {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tni := si.(nodeInfo)\n\t\t\t// collect unique tags, but do not require them as this node is already part of the peerset\n\t\t\tcheckUniqueTag(&ni)\n\t\t}\n\t}\n\n\t// Map ignore\n\tvar ip map[string]struct{}\n\tif li := len(ignore); li > 0 {\n\t\tip = make(map[string]struct{})\n\t\tfor _, p := range ignore {\n\t\t\tip[p] = struct{}{}\n\t\t}\n\t}\n\n\t// Grab the number of streams and HA assets currently assigned to each peer.\n\t// HAAssets under usage is async, so calculate here in realtime based on assignments.\n\tpeerStreams := make(map[string]int, len(peers))\n\tpeerHA := make(map[string]int, len(peers))\n\tfor _, asa := range cc.streams {\n\t\tfor _, sa := range asa {\n\t\t\tisHA := len(sa.Group.Peers) > 1\n\t\t\tfor _, peer := range sa.Group.Peers {\n\t\t\t\tpeerStreams[peer]++\n\t\t\t\tif isHA {\n\t\t\t\t\tpeerHA[peer]++\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tmaxHaAssets := s.getOpts().JetStreamLimits.MaxHAAssets\n\n\t// An error is a result of multiple individual placement decisions.\n\t// Which is why we keep taps on how often which one happened.\n\terr := selectPeerError{}\n\n\t// Shuffle them up.\n\trand.Shuffle(len(peers), func(i, j int) { peers[i], peers[j] = peers[j], peers[i] })\n\tfor _, p := range peers {\n\t\tsi, ok := s.nodeToInfo.Load(p.ID)\n\t\tif !ok || si == nil {\n\t\t\terr.misc = true\n\t\t\tcontinue\n\t\t}\n\t\tni := si.(nodeInfo)\n\t\t// Only select from the designated named cluster.\n\t\tif ni.cluster != cluster {\n\t\t\ts.Debugf(\"Peer selection: discard %s@%s reason: not target cluster %s\", ni.name, ni.cluster, cluster)\n\t\t\tcontinue\n\t\t}\n\n\t\t// If we know its offline or we do not have config or err don't consider.\n\t\tif ni.offline || ni.cfg == nil || ni.stats == nil {\n\t\t\ts.Debugf(\"Peer selection: discard %s@%s reason: offline\", ni.name, ni.cluster)\n\t\t\terr.offline = true\n\t\t\tcontinue\n\t\t}\n\n\t\t// If ignore skip\n\t\tif _, ok := ip[p.ID]; ok {\n\t\t\tcontinue\n\t\t}\n\n\t\t// If existing also skip, we will add back in to front of the list when done.\n\t\tif _, ok := ep[p.ID]; ok {\n\t\t\tcontinue\n\t\t}\n\n\t\tif ni.tags.Contains(jsExcludePlacement) {\n\t\t\ts.Debugf(\"Peer selection: discard %s@%s tags: %v reason: %s present\",\n\t\t\t\tni.name, ni.cluster, ni.tags, jsExcludePlacement)\n\t\t\terr.excludeTag = true\n\t\t\tcontinue\n\t\t}\n\n\t\tif len(ti) > 0 {\n\t\t\tmatched := true\n\t\t\tfor _, t := range ti {\n\t\t\t\tcontains := ni.tags.Contains(t.tag)\n\t\t\t\tif t.exclude && contains {\n\t\t\t\t\tmatched = false\n\t\t\t\t\ts.Debugf(\"Peer selection: discard %s@%s tags: %v reason: excluded tag %s present\",\n\t\t\t\t\t\tni.name, ni.cluster, ni.tags, t)\n\t\t\t\t\terr.addExcludeTag(t.tag)\n\t\t\t\t\tbreak\n\t\t\t\t} else if !t.exclude && !contains {\n\t\t\t\t\tmatched = false\n\t\t\t\t\ts.Debugf(\"Peer selection: discard %s@%s tags: %v reason: mandatory tag %s not present\",\n\t\t\t\t\t\tni.name, ni.cluster, ni.tags, t)\n\t\t\t\t\terr.addMissingTag(t.tag)\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t\tif !matched {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t}\n\n\t\tvar available uint64\n\t\tif ni.stats != nil {\n\t\t\tswitch cfg.Storage {\n\t\t\tcase MemoryStorage:\n\t\t\t\tused := ni.stats.ReservedMemory\n\t\t\t\tif ni.stats.Memory > used {\n\t\t\t\t\tused = ni.stats.Memory\n\t\t\t\t}\n\t\t\t\tif ni.cfg.MaxMemory > int64(used) {\n\t\t\t\t\tavailable = uint64(ni.cfg.MaxMemory) - used\n\t\t\t\t}\n\t\t\tcase FileStorage:\n\t\t\t\tused := ni.stats.ReservedStore\n\t\t\t\tif ni.stats.Store > used {\n\t\t\t\t\tused = ni.stats.Store\n\t\t\t\t}\n\t\t\t\tif ni.cfg.MaxStore > int64(used) {\n\t\t\t\t\tavailable = uint64(ni.cfg.MaxStore) - used\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t// Otherwise check if we have enough room if maxBytes set.\n\t\tif maxBytes > 0 && maxBytes > available {\n\t\t\ts.Warnf(\"Peer selection: discard %s@%s (Max Bytes: %d) exceeds available %s storage of %d bytes\",\n\t\t\t\tni.name, ni.cluster, maxBytes, cfg.Storage.String(), available)\n\t\t\terr.noStorage = true\n\t\t\tcontinue\n\t\t}\n\t\t// HAAssets contain _meta_ which we want to ignore, hence > and not >=.\n\t\tif maxHaAssets > 0 && ni.stats != nil && ni.stats.HAAssets > maxHaAssets {\n\t\t\ts.Warnf(\"Peer selection: discard %s@%s (HA Asset Count: %d) exceeds max ha asset limit of %d for stream placement\",\n\t\t\t\tni.name, ni.cluster, ni.stats.HAAssets, maxHaAssets)\n\t\t\terr.misc = true\n\t\t\tcontinue\n\t\t}\n\n\t\tif uniqueTagPrefix != _EMPTY_ {\n\t\t\tif unique, owner := checkUniqueTag(&ni); !unique {\n\t\t\t\tif owner != nil {\n\t\t\t\t\ts.Debugf(\"Peer selection: discard %s@%s tags:%v reason: unique prefix %s owned by %s@%s\",\n\t\t\t\t\t\tni.name, ni.cluster, ni.tags, owner.name, owner.cluster)\n\t\t\t\t} else {\n\t\t\t\t\ts.Debugf(\"Peer selection: discard %s@%s tags:%v reason: unique prefix %s not present\",\n\t\t\t\t\t\tni.name, ni.cluster, ni.tags)\n\t\t\t\t}\n\t\t\t\terr.uniqueTag = true\n\t\t\t\tcontinue\n\t\t\t}\n\t\t}\n\t\t// Add to our list of potential nodes.\n\t\tnodes = append(nodes, wn{p.ID, available, peerHA[p.ID], peerStreams[p.ID]})\n\t}\n\n\t// If we could not select enough peers, fail.\n\tif len(nodes) < (r - len(existing)) {\n\t\ts.Debugf(\"Peer selection: required %d nodes but found %d (cluster: %s replica: %d existing: %v/%d peers: %d result-peers: %d err: %+v)\",\n\t\t\t(r - len(existing)), len(nodes), cluster, r, existing, replaceFirstExisting, len(peers), len(nodes), err)\n\t\tif len(peers) == 0 {\n\t\t\terr.noJsClust = true\n\t\t}\n\t\treturn nil, &err\n\t}\n\t// Sort based on available from most to least, breaking ties by number of total streams assigned to the peer.\n\tslices.SortFunc(nodes, func(i, j wn) int {\n\t\tif i.avail == j.avail {\n\t\t\treturn cmp.Compare(i.ns, j.ns)\n\t\t}\n\t\treturn -cmp.Compare(i.avail, j.avail) // reverse\n\t})\n\t// If we are placing a replicated stream, let's sort based on HAAssets, as that is more important to balance.\n\tif cfg.Replicas > 1 {\n\t\tslices.SortStableFunc(nodes, func(i, j wn) int { return cmp.Compare(i.ha, j.ha) })\n\t}\n\n\tvar results []string\n\tif len(existing) > 0 {\n\t\tresults = append(results, existing...)\n\t\tr -= len(existing)\n\t}\n\tfor _, r := range nodes[:r] {\n\t\tresults = append(results, r.id)\n\t}\n\treturn results, nil\n}\n\nfunc groupNameForStream(peers []string, storage StorageType) string {\n\treturn groupName(\"S\", peers, storage)\n}\n\nfunc groupNameForConsumer(peers []string, storage StorageType) string {\n\treturn groupName(\"C\", peers, storage)\n}\n\nfunc groupName(prefix string, peers []string, storage StorageType) string {\n\tgns := getHash(nuid.Next())\n\treturn fmt.Sprintf(\"%s-R%d%s-%s\", prefix, len(peers), storage.String()[:1], gns)\n}\n\n// returns stream count for this tier as well as applicable reservation size (not including cfg)\n// jetStream read lock should be held\nfunc tieredStreamAndReservationCount(asa map[string]*streamAssignment, tier string, cfg *StreamConfig) (int, int64) {\n\tvar numStreams int\n\tvar reservation int64\n\tfor _, sa := range asa {\n\t\t// Don't count the stream toward the limit if it already exists.\n\t\tif (tier == _EMPTY_ || isSameTier(sa.Config, cfg)) && sa.Config.Name != cfg.Name {\n\t\t\tnumStreams++\n\t\t\tif sa.Config.MaxBytes > 0 && sa.Config.Storage == cfg.Storage {\n\t\t\t\t// If tier is empty, all storage is flat and we should adjust for replicas.\n\t\t\t\t// Otherwise if tiered, storage replication already taken into consideration.\n\t\t\t\tif tier == _EMPTY_ && cfg.Replicas > 1 {\n\t\t\t\t\treservation += sa.Config.MaxBytes * int64(cfg.Replicas)\n\t\t\t\t} else {\n\t\t\t\t\treservation += sa.Config.MaxBytes\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn numStreams, reservation\n}\n\n// createGroupForStream will create a group for assignment for the stream.\n// Lock should be held.\nfunc (js *jetStream) createGroupForStream(ci *ClientInfo, cfg *StreamConfig) (*raftGroup, *selectPeerError) {\n\treplicas := cfg.Replicas\n\tif replicas == 0 {\n\t\treplicas = 1\n\t}\n\n\t// Default connected cluster from the request origin.\n\tcc, cluster := js.cluster, ci.Cluster\n\t// If specified, override the default.\n\tclusterDefined := cfg.Placement != nil && cfg.Placement.Cluster != _EMPTY_\n\tif clusterDefined {\n\t\tcluster = cfg.Placement.Cluster\n\t}\n\tclusters := []string{cluster}\n\tif !clusterDefined {\n\t\tclusters = append(clusters, ci.Alternates...)\n\t}\n\n\t// Need to create a group here.\n\terrs := &selectPeerError{}\n\tfor _, cn := range clusters {\n\t\tpeers, err := cc.selectPeerGroup(replicas, cn, cfg, nil, 0, nil)\n\t\tif len(peers) < replicas {\n\t\t\terrs.accumulate(err)\n\t\t\tcontinue\n\t\t}\n\t\treturn &raftGroup{Name: groupNameForStream(peers, cfg.Storage), Storage: cfg.Storage, Peers: peers, Cluster: cn}, nil\n\t}\n\treturn nil, errs\n}\n\nfunc (acc *Account) selectLimits(replicas int) (*JetStreamAccountLimits, string, *jsAccount, *ApiError) {\n\t// Grab our jetstream account info.\n\tacc.mu.RLock()\n\tjsa := acc.js\n\tacc.mu.RUnlock()\n\n\tif jsa == nil {\n\t\treturn nil, _EMPTY_, nil, NewJSNotEnabledForAccountError()\n\t}\n\n\tjsa.usageMu.RLock()\n\tselectedLimits, tierName, ok := jsa.selectLimits(replicas)\n\tjsa.usageMu.RUnlock()\n\n\tif !ok {\n\t\treturn nil, _EMPTY_, nil, NewJSNoLimitsError()\n\t}\n\treturn &selectedLimits, tierName, jsa, nil\n}\n\n// Read lock needs to be held\nfunc (js *jetStream) jsClusteredStreamLimitsCheck(acc *Account, cfg *StreamConfig) *ApiError {\n\tvar replicas int\n\tif cfg != nil {\n\t\treplicas = cfg.Replicas\n\t}\n\tselectedLimits, tier, _, apiErr := acc.selectLimits(replicas)\n\tif apiErr != nil {\n\t\treturn apiErr\n\t}\n\n\tasa := js.cluster.streams[acc.Name]\n\tnumStreams, reservations := tieredStreamAndReservationCount(asa, tier, cfg)\n\t// Check for inflight proposals...\n\tif cc := js.cluster; cc != nil && cc.inflight != nil {\n\t\tstreams := cc.inflight[acc.Name]\n\t\tnumStreams += len(streams)\n\t\t// If inflight contains the same stream, don't count toward exceeding maximum.\n\t\tif cfg != nil {\n\t\t\tif _, ok := streams[cfg.Name]; ok {\n\t\t\t\tnumStreams--\n\t\t\t}\n\t\t}\n\t}\n\tif selectedLimits.MaxStreams > 0 && numStreams >= selectedLimits.MaxStreams {\n\t\treturn NewJSMaximumStreamsLimitError()\n\t}\n\t// Check for account limits here before proposing.\n\tif err := js.checkAccountLimits(selectedLimits, cfg, reservations); err != nil {\n\t\treturn NewJSStreamLimitsError(err, Unless(err))\n\t}\n\treturn nil\n}\n\nfunc (s *Server) jsClusteredStreamRequest(ci *ClientInfo, acc *Account, subject, reply string, rmsg []byte, config *StreamConfigRequest) {\n\tjs, cc := s.getJetStreamCluster()\n\tif js == nil || cc == nil {\n\t\treturn\n\t}\n\n\tvar resp = JSApiStreamCreateResponse{ApiResponse: ApiResponse{Type: JSApiStreamCreateResponseType}}\n\n\tccfg, apiErr := s.checkStreamCfg(&config.StreamConfig, acc, config.Pedantic)\n\tif apiErr != nil {\n\t\tresp.Error = apiErr\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(rmsg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\tcfg := &ccfg\n\n\t// Now process the request and proposal.\n\tjs.mu.Lock()\n\tdefer js.mu.Unlock()\n\n\tvar self *streamAssignment\n\tvar rg *raftGroup\n\tvar syncSubject string\n\n\t// Capture if we have existing assignment first.\n\tif osa := js.streamAssignment(acc.Name, cfg.Name); osa != nil {\n\t\tcopyStreamMetadata(cfg, osa.Config)\n\t\tif !reflect.DeepEqual(osa.Config, cfg) {\n\t\t\tresp.Error = NewJSStreamNameExistError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(rmsg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\t// This is an equal assignment.\n\t\tself, rg, syncSubject = osa, osa.Group, osa.Sync\n\t}\n\n\tif cfg.Sealed {\n\t\tresp.Error = NewJSStreamInvalidConfigError(fmt.Errorf(\"stream configuration for create can not be sealed\"))\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(rmsg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\t// Check for subject collisions here.\n\tif cc.subjectsOverlap(acc.Name, cfg.Subjects, self) {\n\t\tresp.Error = NewJSStreamSubjectOverlapError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(rmsg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tapiErr = js.jsClusteredStreamLimitsCheck(acc, cfg)\n\t// Check for stream limits here before proposing. These need to be tracked from meta layer, not jsa.\n\tif apiErr != nil {\n\t\tresp.Error = apiErr\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(rmsg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\t// Make sure inflight is setup properly.\n\tif cc.inflight == nil {\n\t\tcc.inflight = make(map[string]map[string]*inflightInfo)\n\t}\n\tstreams, ok := cc.inflight[acc.Name]\n\tif !ok {\n\t\tstreams = make(map[string]*inflightInfo)\n\t\tcc.inflight[acc.Name] = streams\n\t}\n\n\t// Raft group selection and placement.\n\tif rg == nil {\n\t\t// Check inflight before proposing in case we have an existing inflight proposal.\n\t\tif existing, ok := streams[cfg.Name]; ok {\n\t\t\t// We have existing for same stream. Re-use same group and syncSubject.\n\t\t\trg, syncSubject = existing.rg, existing.sync\n\t\t}\n\t}\n\t// Create a new one here if needed.\n\tif rg == nil {\n\t\tnrg, err := js.createGroupForStream(ci, cfg)\n\t\tif err != nil {\n\t\t\tresp.Error = NewJSClusterNoPeersError(err)\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(rmsg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\trg = nrg\n\t\t// Pick a preferred leader.\n\t\trg.setPreferred()\n\t}\n\n\tif syncSubject == _EMPTY_ {\n\t\tsyncSubject = syncSubjForStream()\n\t}\n\t// Sync subject for post snapshot sync.\n\tsa := &streamAssignment{Group: rg, Sync: syncSubject, Config: cfg, Subject: subject, Reply: reply, Client: ci, Created: time.Now().UTC()}\n\tif err := cc.meta.Propose(encodeAddStreamAssignment(sa)); err == nil {\n\t\t// On success, add this as an inflight proposal so we can apply limits\n\t\t// on concurrent create requests while this stream assignment has\n\t\t// possibly not been processed yet.\n\t\tif streams, ok := cc.inflight[acc.Name]; ok && self == nil {\n\t\t\tstreams[cfg.Name] = &inflightInfo{rg, syncSubject}\n\t\t}\n\t}\n}\n\nvar (\n\terrReqTimeout = errors.New(\"timeout while waiting for response\")\n\terrReqSrvExit = errors.New(\"server shutdown while waiting for response\")\n)\n\n// blocking utility call to perform requests on the system account\n// returns (synchronized) v or error\nfunc sysRequest[T any](s *Server, subjFormat string, args ...any) (*T, error) {\n\tisubj := fmt.Sprintf(subjFormat, args...)\n\n\ts.mu.Lock()\n\tif s.sys == nil {\n\t\ts.mu.Unlock()\n\t\treturn nil, ErrNoSysAccount\n\t}\n\tinbox := s.newRespInbox()\n\tresults := make(chan *T, 1)\n\ts.sys.replies[inbox] = func(_ *subscription, _ *client, _ *Account, _, _ string, msg []byte) {\n\t\tvar v T\n\t\tif err := json.Unmarshal(msg, &v); err != nil {\n\t\t\ts.Warnf(\"Error unmarshalling response for request '%s':%v\", isubj, err)\n\t\t\treturn\n\t\t}\n\t\tselect {\n\t\tcase results <- &v:\n\t\tdefault:\n\t\t\ts.Warnf(\"Failed placing request response on internal channel\")\n\t\t}\n\t}\n\ts.mu.Unlock()\n\n\ts.sendInternalMsgLocked(isubj, inbox, nil, nil)\n\n\tdefer func() {\n\t\ts.mu.Lock()\n\t\tdefer s.mu.Unlock()\n\t\tif s.sys != nil && s.sys.replies != nil {\n\t\t\tdelete(s.sys.replies, inbox)\n\t\t}\n\t}()\n\n\tttl := time.NewTimer(2 * time.Second)\n\tdefer ttl.Stop()\n\n\tselect {\n\tcase <-s.quitCh:\n\t\treturn nil, errReqSrvExit\n\tcase <-ttl.C:\n\t\treturn nil, errReqTimeout\n\tcase data := <-results:\n\t\treturn data, nil\n\t}\n}\n\nfunc (s *Server) jsClusteredStreamUpdateRequest(ci *ClientInfo, acc *Account, subject, reply string, rmsg []byte, cfg *StreamConfig, peerSet []string, pedantic bool) {\n\tjs, cc := s.getJetStreamCluster()\n\tif js == nil || cc == nil {\n\t\treturn\n\t}\n\n\t// Now process the request and proposal.\n\tjs.mu.Lock()\n\tdefer js.mu.Unlock()\n\tmeta := cc.meta\n\tif meta == nil {\n\t\treturn\n\t}\n\n\tvar resp = JSApiStreamUpdateResponse{ApiResponse: ApiResponse{Type: JSApiStreamUpdateResponseType}}\n\n\tosa := js.streamAssignment(acc.Name, cfg.Name)\n\tif osa == nil {\n\t\tresp.Error = NewJSStreamNotFoundError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(rmsg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\t// Don't allow updating if all peers are offline.\n\tif s.allPeersOffline(osa.Group) {\n\t\tresp.Error = NewJSStreamOfflineError()\n\t\ts.sendDelayedAPIErrResponse(ci, acc, subject, reply, string(rmsg), s.jsonResponse(&resp), nil, errRespDelay)\n\t\treturn\n\t}\n\n\t// Update asset version metadata.\n\tsetStaticStreamMetadata(cfg)\n\n\tvar newCfg *StreamConfig\n\tif jsa := js.accounts[acc.Name]; jsa != nil {\n\t\tjs.mu.Unlock()\n\t\tncfg, err := jsa.configUpdateCheck(osa.Config, cfg, s, pedantic)\n\t\tjs.mu.Lock()\n\t\tif err != nil {\n\t\t\tresp.Error = NewJSStreamUpdateError(err, Unless(err))\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(rmsg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t} else {\n\t\t\tnewCfg = ncfg\n\t\t}\n\t} else {\n\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(rmsg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\t// Check for mirror changes which are not allowed.\n\tif !reflect.DeepEqual(newCfg.Mirror, osa.Config.Mirror) {\n\t\tresp.Error = NewJSStreamMirrorNotUpdatableError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(rmsg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\t// Check for subject collisions here.\n\tif cc.subjectsOverlap(acc.Name, cfg.Subjects, osa) {\n\t\tresp.Error = NewJSStreamSubjectOverlapError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(rmsg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\t// Make copy so to not change original.\n\trg := osa.copyGroup().Group\n\n\t// Check for a move request.\n\tvar isMoveRequest, isMoveCancel bool\n\tif lPeerSet := len(peerSet); lPeerSet > 0 {\n\t\tisMoveRequest = true\n\t\t// check if this is a cancellation\n\t\tif lPeerSet == osa.Config.Replicas && lPeerSet <= len(rg.Peers) {\n\t\t\tisMoveCancel = true\n\t\t\t// can only be a cancellation if the peer sets overlap as expected\n\t\t\tfor i := 0; i < lPeerSet; i++ {\n\t\t\t\tif peerSet[i] != rg.Peers[i] {\n\t\t\t\t\tisMoveCancel = false\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t} else {\n\t\tisMoveRequest = newCfg.Placement != nil && !reflect.DeepEqual(osa.Config.Placement, newCfg.Placement)\n\t}\n\n\t// Check for replica changes.\n\tisReplicaChange := newCfg.Replicas != osa.Config.Replicas\n\n\t// We stage consumer updates and do them after the stream update.\n\tvar consumers []*consumerAssignment\n\n\t// Check if this is a move request, but no cancellation, and we are already moving this stream.\n\tif isMoveRequest && !isMoveCancel && osa.Config.Replicas != len(rg.Peers) {\n\t\t// obtain stats to include in error message\n\t\tmsg := _EMPTY_\n\t\tif s.allPeersOffline(rg) {\n\t\t\tmsg = fmt.Sprintf(\"all %d peers offline\", len(rg.Peers))\n\t\t} else {\n\t\t\t// Need to release js lock.\n\t\t\tjs.mu.Unlock()\n\t\t\tif si, err := sysRequest[StreamInfo](s, clusterStreamInfoT, ci.serviceAccount(), cfg.Name); err != nil {\n\t\t\t\tmsg = fmt.Sprintf(\"error retrieving info: %s\", err.Error())\n\t\t\t} else if si != nil {\n\t\t\t\tcurrentCount := 0\n\t\t\t\tif si.Cluster.Leader != _EMPTY_ {\n\t\t\t\t\tcurrentCount++\n\t\t\t\t}\n\t\t\t\tcombinedLag := uint64(0)\n\t\t\t\tfor _, r := range si.Cluster.Replicas {\n\t\t\t\t\tif r.Current {\n\t\t\t\t\t\tcurrentCount++\n\t\t\t\t\t}\n\t\t\t\t\tcombinedLag += r.Lag\n\t\t\t\t}\n\t\t\t\tmsg = fmt.Sprintf(\"total peers: %d, current peers: %d, combined lag: %d\",\n\t\t\t\t\tlen(rg.Peers), currentCount, combinedLag)\n\t\t\t}\n\t\t\t// Re-acquire here.\n\t\t\tjs.mu.Lock()\n\t\t}\n\t\tresp.Error = NewJSStreamMoveInProgressError(msg)\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(rmsg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\t// Can not move and scale at same time.\n\tif isMoveRequest && isReplicaChange {\n\t\tresp.Error = NewJSStreamMoveAndScaleError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(rmsg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tif isReplicaChange {\n\t\tisScaleUp := newCfg.Replicas > len(rg.Peers)\n\t\t// We are adding new peers here.\n\t\tif isScaleUp {\n\t\t\t// Check that we have the allocation available.\n\t\t\tif err := js.jsClusteredStreamLimitsCheck(acc, newCfg); err != nil {\n\t\t\t\tresp.Error = err\n\t\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(rmsg), s.jsonResponse(&resp))\n\t\t\t\treturn\n\t\t\t}\n\t\t\t// Check if we do not have a cluster assigned, and if we do not make sure we\n\t\t\t// try to pick one. This could happen with older streams that were assigned by\n\t\t\t// previous servers.\n\t\t\tif rg.Cluster == _EMPTY_ {\n\t\t\t\t// Prefer placement directrives if we have them.\n\t\t\t\tif newCfg.Placement != nil && newCfg.Placement.Cluster != _EMPTY_ {\n\t\t\t\t\trg.Cluster = newCfg.Placement.Cluster\n\t\t\t\t} else {\n\t\t\t\t\t// Fall back to the cluster assignment from the client.\n\t\t\t\t\trg.Cluster = ci.Cluster\n\t\t\t\t}\n\t\t\t}\n\t\t\tpeers, err := cc.selectPeerGroup(newCfg.Replicas, rg.Cluster, newCfg, rg.Peers, 0, nil)\n\t\t\tif err != nil {\n\t\t\t\tresp.Error = NewJSClusterNoPeersError(err)\n\t\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(rmsg), s.jsonResponse(&resp))\n\t\t\t\treturn\n\t\t\t}\n\t\t\t// Single nodes are not recorded by the NRG layer so we can rename.\n\t\t\tif len(peers) == 1 {\n\t\t\t\trg.Name = groupNameForStream(peers, rg.Storage)\n\t\t\t} else if len(rg.Peers) == 1 {\n\t\t\t\t// This is scale up from being a singelton, set preferred to that singelton.\n\t\t\t\trg.Preferred = rg.Peers[0]\n\t\t\t}\n\t\t\trg.Peers = peers\n\t\t} else {\n\t\t\t// We are deleting nodes here. We want to do our best to preserve the current leader.\n\t\t\t// We have support now from above that guarantees we are in our own Go routine, so can\n\t\t\t// ask for stream info from the stream leader to make sure we keep the leader in the new list.\n\t\t\tvar curLeader string\n\t\t\tif !s.allPeersOffline(rg) {\n\t\t\t\t// Need to release js lock.\n\t\t\t\tjs.mu.Unlock()\n\t\t\t\tif si, err := sysRequest[StreamInfo](s, clusterStreamInfoT, ci.serviceAccount(), cfg.Name); err != nil {\n\t\t\t\t\ts.Warnf(\"Did not receive stream info results for '%s > %s' due to: %s\", acc, cfg.Name, err)\n\t\t\t\t} else if si != nil {\n\t\t\t\t\tif cl := si.Cluster; cl != nil && cl.Leader != _EMPTY_ {\n\t\t\t\t\t\tcurLeader = getHash(cl.Leader)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\t// Re-acquire here.\n\t\t\t\tjs.mu.Lock()\n\t\t\t}\n\t\t\t// If we identified a leader make sure its part of the new group.\n\t\t\tselected := make([]string, 0, newCfg.Replicas)\n\n\t\t\tif curLeader != _EMPTY_ {\n\t\t\t\tselected = append(selected, curLeader)\n\t\t\t}\n\t\t\tfor _, peer := range rg.Peers {\n\t\t\t\tif len(selected) == newCfg.Replicas {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t\tif peer == curLeader {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tif si, ok := s.nodeToInfo.Load(peer); ok && si != nil {\n\t\t\t\t\tif si.(nodeInfo).offline {\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\t\t\t\t\tselected = append(selected, peer)\n\t\t\t\t}\n\t\t\t}\n\t\t\trg.Peers = selected\n\t\t}\n\n\t\t// Need to remap any consumers.\n\t\tfor _, ca := range osa.consumers {\n\t\t\t// Legacy ephemerals are R=1 but present as R=0, so only auto-remap named consumers, or if we are downsizing the consumer peers.\n\t\t\t// If stream is interest or workqueue policy always remaps since they require peer parity with stream.\n\t\t\tnumPeers := len(ca.Group.Peers)\n\t\t\tisAutoScale := ca.Config.Replicas == 0 && (ca.Config.Durable != _EMPTY_ || ca.Config.Name != _EMPTY_)\n\t\t\tif isAutoScale || numPeers > len(rg.Peers) || cfg.Retention != LimitsPolicy {\n\t\t\t\tcca := ca.copyGroup()\n\t\t\t\t// Adjust preferred as needed.\n\t\t\t\tif numPeers == 1 && isScaleUp {\n\t\t\t\t\tcca.Group.Preferred = ca.Group.Peers[0]\n\t\t\t\t} else {\n\t\t\t\t\tcca.Group.Preferred = _EMPTY_\n\t\t\t\t}\n\t\t\t\t// Assign new peers.\n\t\t\t\tcca.Group.Peers = rg.Peers\n\t\t\t\t// If the replicas was not 0 make sure it matches here.\n\t\t\t\tif cca.Config.Replicas != 0 {\n\t\t\t\t\tcca.Config.Replicas = len(rg.Peers)\n\t\t\t\t}\n\t\t\t\t// We can not propose here before the stream itself so we collect them.\n\t\t\t\tconsumers = append(consumers, cca)\n\n\t\t\t} else if !isScaleUp {\n\t\t\t\t// We decided to leave this consumer's peer group alone but we are also scaling down.\n\t\t\t\t// We need to make sure we do not have any peers that are no longer part of the stream.\n\t\t\t\t// Note we handle down scaling of a consumer above if its number of peers were > new stream peers.\n\t\t\t\tvar needReplace []string\n\t\t\t\tfor _, rp := range ca.Group.Peers {\n\t\t\t\t\t// Check if we have an orphaned peer now for this consumer.\n\t\t\t\t\tif !rg.isMember(rp) {\n\t\t\t\t\t\tneedReplace = append(needReplace, rp)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif len(needReplace) > 0 {\n\t\t\t\t\tnewPeers := copyStrings(rg.Peers)\n\t\t\t\t\trand.Shuffle(len(newPeers), func(i, j int) { newPeers[i], newPeers[j] = newPeers[j], newPeers[i] })\n\t\t\t\t\t// If we had a small size then the peer set, restrict to the same number.\n\t\t\t\t\tif lp := len(ca.Group.Peers); lp < len(newPeers) {\n\t\t\t\t\t\tnewPeers = newPeers[:lp]\n\t\t\t\t\t}\n\t\t\t\t\tcca := ca.copyGroup()\n\t\t\t\t\t// Assign new peers.\n\t\t\t\t\tcca.Group.Peers = newPeers\n\t\t\t\t\t// If the replicas was not 0 make sure it matches here.\n\t\t\t\t\tif cca.Config.Replicas != 0 {\n\t\t\t\t\t\tcca.Config.Replicas = len(newPeers)\n\t\t\t\t\t}\n\t\t\t\t\t// Check if all peers are invalid. This can happen with R1 under replicated streams that are being scaled down.\n\t\t\t\t\tif len(needReplace) == len(ca.Group.Peers) {\n\t\t\t\t\t\t// We have to transfer state to new peers.\n\t\t\t\t\t\t// we will grab our state and attach to the new assignment.\n\t\t\t\t\t\t// TODO(dlc) - In practice we would want to make sure the consumer is paused.\n\t\t\t\t\t\t// Need to release js lock.\n\t\t\t\t\t\tjs.mu.Unlock()\n\t\t\t\t\t\tif ci, err := sysRequest[ConsumerInfo](s, clusterConsumerInfoT, acc, osa.Config.Name, ca.Name); err != nil {\n\t\t\t\t\t\t\ts.Warnf(\"Did not receive consumer info results for '%s > %s > %s' due to: %s\", acc, osa.Config.Name, ca.Name, err)\n\t\t\t\t\t\t} else if ci != nil {\n\t\t\t\t\t\t\tcca.State = &ConsumerState{\n\t\t\t\t\t\t\t\tDelivered: SequencePair{\n\t\t\t\t\t\t\t\t\tConsumer: ci.Delivered.Consumer,\n\t\t\t\t\t\t\t\t\tStream:   ci.Delivered.Stream,\n\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\tAckFloor: SequencePair{\n\t\t\t\t\t\t\t\t\tConsumer: ci.AckFloor.Consumer,\n\t\t\t\t\t\t\t\t\tStream:   ci.AckFloor.Stream,\n\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t\t// Re-acquire here.\n\t\t\t\t\t\tjs.mu.Lock()\n\t\t\t\t\t}\n\t\t\t\t\t// We can not propose here before the stream itself so we collect them.\n\t\t\t\t\tconsumers = append(consumers, cca)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t} else if isMoveRequest {\n\t\tif len(peerSet) == 0 {\n\t\t\tnrg, err := js.createGroupForStream(ci, newCfg)\n\t\t\tif err != nil {\n\t\t\t\tresp.Error = NewJSClusterNoPeersError(err)\n\t\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(rmsg), s.jsonResponse(&resp))\n\t\t\t\treturn\n\t\t\t}\n\t\t\t// filter peers present in both sets\n\t\t\tfor _, peer := range rg.Peers {\n\t\t\t\tfound := false\n\t\t\t\tfor _, newPeer := range nrg.Peers {\n\t\t\t\t\tif peer == newPeer {\n\t\t\t\t\t\tfound = true\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif !found {\n\t\t\t\t\tpeerSet = append(peerSet, peer)\n\t\t\t\t}\n\t\t\t}\n\t\t\tpeerSet = append(peerSet, nrg.Peers...)\n\t\t}\n\t\tif len(rg.Peers) == 1 {\n\t\t\trg.Preferred = peerSet[0]\n\t\t}\n\t\trg.Peers = peerSet\n\n\t\tfor _, ca := range osa.consumers {\n\t\t\tcca := ca.copyGroup()\n\t\t\tr := cca.Config.replicas(osa.Config)\n\t\t\t// shuffle part of cluster peer set we will be keeping\n\t\t\trandPeerSet := copyStrings(peerSet[len(peerSet)-newCfg.Replicas:])\n\t\t\trand.Shuffle(newCfg.Replicas, func(i, j int) { randPeerSet[i], randPeerSet[j] = randPeerSet[j], randPeerSet[i] })\n\t\t\t// move overlapping peers at the end of randPeerSet and keep a tally of non overlapping peers\n\t\t\tdropPeerSet := make([]string, 0, len(cca.Group.Peers))\n\t\t\tfor _, p := range cca.Group.Peers {\n\t\t\t\tfound := false\n\t\t\t\tfor i, rp := range randPeerSet {\n\t\t\t\t\tif p == rp {\n\t\t\t\t\t\trandPeerSet[i] = randPeerSet[newCfg.Replicas-1]\n\t\t\t\t\t\trandPeerSet[newCfg.Replicas-1] = p\n\t\t\t\t\t\tfound = true\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif !found {\n\t\t\t\t\tdropPeerSet = append(dropPeerSet, p)\n\t\t\t\t}\n\t\t\t}\n\t\t\tcPeerSet := randPeerSet[newCfg.Replicas-r:]\n\t\t\t// In case of a set or cancel simply assign\n\t\t\tif len(peerSet) == newCfg.Replicas {\n\t\t\t\tcca.Group.Peers = cPeerSet\n\t\t\t} else {\n\t\t\t\tcca.Group.Peers = append(dropPeerSet, cPeerSet...)\n\t\t\t}\n\t\t\t// make sure it overlaps with peers and remove if not\n\t\t\tif cca.Group.Preferred != _EMPTY_ {\n\t\t\t\tfound := false\n\t\t\t\tfor _, p := range cca.Group.Peers {\n\t\t\t\t\tif p == cca.Group.Preferred {\n\t\t\t\t\t\tfound = true\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif !found {\n\t\t\t\t\tcca.Group.Preferred = _EMPTY_\n\t\t\t\t}\n\t\t\t}\n\t\t\t// We can not propose here before the stream itself so we collect them.\n\t\t\tconsumers = append(consumers, cca)\n\t\t}\n\t} else {\n\t\t// All other updates make sure no preferred is set.\n\t\trg.Preferred = _EMPTY_\n\t}\n\n\tsa := &streamAssignment{Group: rg, Sync: osa.Sync, Created: osa.Created, Config: newCfg, Subject: subject, Reply: reply, Client: ci}\n\tmeta.Propose(encodeUpdateStreamAssignment(sa))\n\n\t// Process any staged consumers.\n\tfor _, ca := range consumers {\n\t\tmeta.Propose(encodeAddConsumerAssignment(ca))\n\t}\n}\n\nfunc (s *Server) jsClusteredStreamDeleteRequest(ci *ClientInfo, acc *Account, stream, subject, reply string, rmsg []byte) {\n\tjs, cc := s.getJetStreamCluster()\n\tif js == nil || cc == nil {\n\t\treturn\n\t}\n\n\tjs.mu.Lock()\n\tdefer js.mu.Unlock()\n\n\tif cc.meta == nil {\n\t\treturn\n\t}\n\n\tosa := js.streamAssignment(acc.Name, stream)\n\tif osa == nil {\n\t\tvar resp = JSApiStreamDeleteResponse{ApiResponse: ApiResponse{Type: JSApiStreamDeleteResponseType}}\n\t\tresp.Error = NewJSStreamNotFoundError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(rmsg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tsa := &streamAssignment{Group: osa.Group, Config: osa.Config, Subject: subject, Reply: reply, Client: ci}\n\tcc.meta.Propose(encodeDeleteStreamAssignment(sa))\n}\n\n// Process a clustered purge request.\nfunc (s *Server) jsClusteredStreamPurgeRequest(\n\tci *ClientInfo,\n\tacc *Account,\n\tmset *stream,\n\tstream, subject, reply string,\n\trmsg []byte,\n\tpreq *JSApiStreamPurgeRequest,\n) {\n\tjs, cc := s.getJetStreamCluster()\n\tif js == nil || cc == nil {\n\t\treturn\n\t}\n\n\tjs.mu.Lock()\n\tsa := js.streamAssignment(acc.Name, stream)\n\tif sa == nil {\n\t\tresp := JSApiStreamPurgeResponse{ApiResponse: ApiResponse{Type: JSApiStreamPurgeResponseType}}\n\t\tresp.Error = NewJSStreamNotFoundError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(rmsg), s.jsonResponse(&resp))\n\t\tjs.mu.Unlock()\n\t\treturn\n\t}\n\n\tif n := sa.Group.node; n != nil {\n\t\tsp := &streamPurge{Stream: stream, LastSeq: mset.state().LastSeq, Subject: subject, Reply: reply, Client: ci, Request: preq}\n\t\tn.Propose(encodeStreamPurge(sp))\n\t\tjs.mu.Unlock()\n\t\treturn\n\t}\n\tjs.mu.Unlock()\n\n\tif mset == nil {\n\t\treturn\n\t}\n\n\tvar resp = JSApiStreamPurgeResponse{ApiResponse: ApiResponse{Type: JSApiStreamPurgeResponseType}}\n\tpurged, err := mset.purge(preq)\n\tif err != nil {\n\t\tresp.Error = NewJSStreamGeneralError(err, Unless(err))\n\t} else {\n\t\tresp.Purged = purged\n\t\tresp.Success = true\n\t}\n\ts.sendAPIResponse(ci, acc, subject, reply, string(rmsg), s.jsonResponse(resp))\n}\n\nfunc (s *Server) jsClusteredStreamRestoreRequest(\n\tci *ClientInfo,\n\tacc *Account,\n\treq *JSApiStreamRestoreRequest,\n\tsubject, reply string, rmsg []byte) {\n\n\tjs, cc := s.getJetStreamCluster()\n\tif js == nil || cc == nil {\n\t\treturn\n\t}\n\n\tjs.mu.Lock()\n\tdefer js.mu.Unlock()\n\n\tif cc.meta == nil {\n\t\treturn\n\t}\n\n\tcfg := &req.Config\n\tresp := JSApiStreamRestoreResponse{ApiResponse: ApiResponse{Type: JSApiStreamRestoreResponseType}}\n\n\tif err := js.jsClusteredStreamLimitsCheck(acc, cfg); err != nil {\n\t\tresp.Error = err\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(rmsg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tif sa := js.streamAssignment(ci.serviceAccount(), cfg.Name); sa != nil {\n\t\tresp.Error = NewJSStreamNameExistRestoreFailedError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(rmsg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\t// Raft group selection and placement.\n\trg, err := js.createGroupForStream(ci, cfg)\n\tif err != nil {\n\t\tresp.Error = NewJSClusterNoPeersError(err)\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(rmsg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\t// Pick a preferred leader.\n\trg.setPreferred()\n\tsa := &streamAssignment{Group: rg, Sync: syncSubjForStream(), Config: cfg, Subject: subject, Reply: reply, Client: ci, Created: time.Now().UTC()}\n\t// Now add in our restore state and pre-select a peer to handle the actual receipt of the snapshot.\n\tsa.Restore = &req.State\n\tcc.meta.Propose(encodeAddStreamAssignment(sa))\n}\n\n// Determine if all peers for this group are offline.\nfunc (s *Server) allPeersOffline(rg *raftGroup) bool {\n\tif rg == nil {\n\t\treturn false\n\t}\n\t// Check to see if this stream has any servers online to respond.\n\tfor _, peer := range rg.Peers {\n\t\tif si, ok := s.nodeToInfo.Load(peer); ok && si != nil {\n\t\t\tif !si.(nodeInfo).offline {\n\t\t\t\treturn false\n\t\t\t}\n\t\t}\n\t}\n\treturn true\n}\n\n// This will do a scatter and gather operation for all streams for this account. This is only called from metadata leader.\n// This will be running in a separate Go routine.\nfunc (s *Server) jsClusteredStreamListRequest(acc *Account, ci *ClientInfo, filter string, offset int, subject, reply string, rmsg []byte) {\n\tdefer s.grWG.Done()\n\n\tjs, cc := s.getJetStreamCluster()\n\tif js == nil || cc == nil {\n\t\treturn\n\t}\n\n\tjs.mu.RLock()\n\n\tvar streams []*streamAssignment\n\tfor _, sa := range cc.streams[acc.Name] {\n\t\tif IsNatsErr(sa.err, JSClusterNotAssignedErr) {\n\t\t\tcontinue\n\t\t}\n\n\t\tif filter != _EMPTY_ {\n\t\t\t// These could not have subjects auto-filled in since they are raw and unprocessed.\n\t\t\tif len(sa.Config.Subjects) == 0 {\n\t\t\t\tif SubjectsCollide(filter, sa.Config.Name) {\n\t\t\t\t\tstreams = append(streams, sa)\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tfor _, subj := range sa.Config.Subjects {\n\t\t\t\t\tif SubjectsCollide(filter, subj) {\n\t\t\t\t\t\tstreams = append(streams, sa)\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t} else {\n\t\t\tstreams = append(streams, sa)\n\t\t}\n\t}\n\n\t// Needs to be sorted for offsets etc.\n\tif len(streams) > 1 {\n\t\tslices.SortFunc(streams, func(i, j *streamAssignment) int { return cmp.Compare(i.Config.Name, j.Config.Name) })\n\t}\n\n\tscnt := len(streams)\n\tif offset > scnt {\n\t\toffset = scnt\n\t}\n\tif offset > 0 {\n\t\tstreams = streams[offset:]\n\t}\n\tif len(streams) > JSApiListLimit {\n\t\tstreams = streams[:JSApiListLimit]\n\t}\n\n\tvar resp = JSApiStreamListResponse{\n\t\tApiResponse: ApiResponse{Type: JSApiStreamListResponseType},\n\t\tStreams:     make([]*StreamInfo, 0, len(streams)),\n\t}\n\n\tjs.mu.RUnlock()\n\n\tif len(streams) == 0 {\n\t\tresp.Limit = JSApiListLimit\n\t\tresp.Offset = offset\n\t\ts.sendAPIResponse(ci, acc, subject, reply, string(rmsg), s.jsonResponse(resp))\n\t\treturn\n\t}\n\n\t// Create an inbox for our responses and send out our requests.\n\ts.mu.Lock()\n\tinbox := s.newRespInbox()\n\trc := make(chan *StreamInfo, len(streams))\n\n\t// Store our handler.\n\ts.sys.replies[inbox] = func(sub *subscription, _ *client, _ *Account, subject, _ string, msg []byte) {\n\t\tvar si StreamInfo\n\t\tif err := json.Unmarshal(msg, &si); err != nil {\n\t\t\ts.Warnf(\"Error unmarshalling clustered stream info response:%v\", err)\n\t\t\treturn\n\t\t}\n\t\tselect {\n\t\tcase rc <- &si:\n\t\tdefault:\n\t\t\ts.Warnf(\"Failed placing remote stream info result on internal channel\")\n\t\t}\n\t}\n\ts.mu.Unlock()\n\n\t// Cleanup after.\n\tdefer func() {\n\t\ts.mu.Lock()\n\t\tif s.sys != nil && s.sys.replies != nil {\n\t\t\tdelete(s.sys.replies, inbox)\n\t\t}\n\t\ts.mu.Unlock()\n\t}()\n\n\tvar missingNames []string\n\tsent := map[string]int{}\n\n\t// Send out our requests here.\n\tjs.mu.RLock()\n\tfor _, sa := range streams {\n\t\tif s.allPeersOffline(sa.Group) {\n\t\t\t// Place offline onto our results by hand here.\n\t\t\tsi := &StreamInfo{\n\t\t\t\tConfig:    *sa.Config,\n\t\t\t\tCreated:   sa.Created,\n\t\t\t\tCluster:   js.offlineClusterInfo(sa.Group),\n\t\t\t\tTimeStamp: time.Now().UTC(),\n\t\t\t}\n\t\t\tresp.Streams = append(resp.Streams, si)\n\t\t\tmissingNames = append(missingNames, sa.Config.Name)\n\t\t} else {\n\t\t\tisubj := fmt.Sprintf(clusterStreamInfoT, sa.Client.serviceAccount(), sa.Config.Name)\n\t\t\ts.sendInternalMsgLocked(isubj, inbox, nil, nil)\n\t\t\tsent[sa.Config.Name] = len(sa.consumers)\n\t\t}\n\t}\n\t// Don't hold lock.\n\tjs.mu.RUnlock()\n\n\tconst timeout = 4 * time.Second\n\tnotActive := time.NewTimer(timeout)\n\tdefer notActive.Stop()\n\nLOOP:\n\tfor len(sent) > 0 {\n\t\tselect {\n\t\tcase <-s.quitCh:\n\t\t\treturn\n\t\tcase <-notActive.C:\n\t\t\ts.Warnf(\"Did not receive all stream info results for %q\", acc)\n\t\t\tfor sName := range sent {\n\t\t\t\tmissingNames = append(missingNames, sName)\n\t\t\t}\n\t\t\tbreak LOOP\n\t\tcase si := <-rc:\n\t\t\tconsCount := sent[si.Config.Name]\n\t\t\tif consCount > 0 {\n\t\t\t\tsi.State.Consumers = consCount\n\t\t\t}\n\t\t\tdelete(sent, si.Config.Name)\n\t\t\tresp.Streams = append(resp.Streams, si)\n\t\t\t// Check to see if we are done.\n\t\t\tif len(resp.Streams) == len(streams) {\n\t\t\t\tbreak LOOP\n\t\t\t}\n\t\t}\n\t}\n\n\t// Needs to be sorted as well.\n\tif len(resp.Streams) > 1 {\n\t\tslices.SortFunc(resp.Streams, func(i, j *StreamInfo) int { return cmp.Compare(i.Config.Name, j.Config.Name) })\n\t}\n\n\tresp.Total = scnt\n\tresp.Limit = JSApiListLimit\n\tresp.Offset = offset\n\tresp.Missing = missingNames\n\ts.sendAPIResponse(ci, acc, subject, reply, string(rmsg), s.jsonResponse(resp))\n}\n\n// This will do a scatter and gather operation for all consumers for this stream and account.\n// This will be running in a separate Go routine.\nfunc (s *Server) jsClusteredConsumerListRequest(acc *Account, ci *ClientInfo, offset int, stream, subject, reply string, rmsg []byte) {\n\tdefer s.grWG.Done()\n\n\tjs, cc := s.getJetStreamCluster()\n\tif js == nil || cc == nil {\n\t\treturn\n\t}\n\n\tjs.mu.RLock()\n\n\tvar consumers []*consumerAssignment\n\tif sas := cc.streams[acc.Name]; sas != nil {\n\t\tif sa := sas[stream]; sa != nil {\n\t\t\t// Copy over since we need to sort etc.\n\t\t\tfor _, ca := range sa.consumers {\n\t\t\t\tconsumers = append(consumers, ca)\n\t\t\t}\n\t\t}\n\t}\n\t// Needs to be sorted.\n\tif len(consumers) > 1 {\n\t\tslices.SortFunc(consumers, func(i, j *consumerAssignment) int { return cmp.Compare(i.Config.Name, j.Config.Name) })\n\t}\n\n\tocnt := len(consumers)\n\tif offset > ocnt {\n\t\toffset = ocnt\n\t}\n\tif offset > 0 {\n\t\tconsumers = consumers[offset:]\n\t}\n\tif len(consumers) > JSApiListLimit {\n\t\tconsumers = consumers[:JSApiListLimit]\n\t}\n\n\t// Send out our requests here.\n\tvar resp = JSApiConsumerListResponse{\n\t\tApiResponse: ApiResponse{Type: JSApiConsumerListResponseType},\n\t\tConsumers:   []*ConsumerInfo{},\n\t}\n\n\tjs.mu.RUnlock()\n\n\tif len(consumers) == 0 {\n\t\tresp.Limit = JSApiListLimit\n\t\tresp.Offset = offset\n\t\ts.sendAPIResponse(ci, acc, subject, reply, string(rmsg), s.jsonResponse(resp))\n\t\treturn\n\t}\n\n\t// Create an inbox for our responses and send out requests.\n\ts.mu.Lock()\n\tinbox := s.newRespInbox()\n\trc := make(chan *ConsumerInfo, len(consumers))\n\n\t// Store our handler.\n\ts.sys.replies[inbox] = func(sub *subscription, _ *client, _ *Account, subject, _ string, msg []byte) {\n\t\tvar ci ConsumerInfo\n\t\tif err := json.Unmarshal(msg, &ci); err != nil {\n\t\t\ts.Warnf(\"Error unmarshaling clustered consumer info response:%v\", err)\n\t\t\treturn\n\t\t}\n\t\tselect {\n\t\tcase rc <- &ci:\n\t\tdefault:\n\t\t\ts.Warnf(\"Failed placing consumer info result on internal chan\")\n\t\t}\n\t}\n\ts.mu.Unlock()\n\n\t// Cleanup after.\n\tdefer func() {\n\t\ts.mu.Lock()\n\t\tif s.sys != nil && s.sys.replies != nil {\n\t\t\tdelete(s.sys.replies, inbox)\n\t\t}\n\t\ts.mu.Unlock()\n\t}()\n\n\tvar missingNames []string\n\tsent := map[string]struct{}{}\n\n\t// Send out our requests here.\n\tjs.mu.RLock()\n\tfor _, ca := range consumers {\n\t\tif s.allPeersOffline(ca.Group) {\n\t\t\t// Place offline onto our results by hand here.\n\t\t\tci := &ConsumerInfo{\n\t\t\t\tConfig:    ca.Config,\n\t\t\t\tCreated:   ca.Created,\n\t\t\t\tCluster:   js.offlineClusterInfo(ca.Group),\n\t\t\t\tTimeStamp: time.Now().UTC(),\n\t\t\t}\n\t\t\tresp.Consumers = append(resp.Consumers, ci)\n\t\t\tmissingNames = append(missingNames, ca.Name)\n\t\t} else {\n\t\t\tisubj := fmt.Sprintf(clusterConsumerInfoT, ca.Client.serviceAccount(), stream, ca.Name)\n\t\t\ts.sendInternalMsgLocked(isubj, inbox, nil, nil)\n\t\t\tsent[ca.Name] = struct{}{}\n\t\t}\n\t}\n\t// Don't hold lock.\n\tjs.mu.RUnlock()\n\n\tconst timeout = 4 * time.Second\n\tnotActive := time.NewTimer(timeout)\n\tdefer notActive.Stop()\n\nLOOP:\n\tfor len(sent) > 0 {\n\t\tselect {\n\t\tcase <-s.quitCh:\n\t\t\treturn\n\t\tcase <-notActive.C:\n\t\t\ts.Warnf(\"Did not receive all consumer info results for '%s > %s'\", acc, stream)\n\t\t\tfor cName := range sent {\n\t\t\t\tmissingNames = append(missingNames, cName)\n\t\t\t}\n\t\t\tbreak LOOP\n\t\tcase ci := <-rc:\n\t\t\tdelete(sent, ci.Name)\n\t\t\tresp.Consumers = append(resp.Consumers, ci)\n\t\t\t// Check to see if we are done.\n\t\t\tif len(resp.Consumers) == len(consumers) {\n\t\t\t\tbreak LOOP\n\t\t\t}\n\t\t}\n\t}\n\n\t// Needs to be sorted as well.\n\tif len(resp.Consumers) > 1 {\n\t\tslices.SortFunc(resp.Consumers, func(i, j *ConsumerInfo) int { return cmp.Compare(i.Name, j.Name) })\n\t}\n\n\tresp.Total = ocnt\n\tresp.Limit = JSApiListLimit\n\tresp.Offset = offset\n\tresp.Missing = missingNames\n\ts.sendAPIResponse(ci, acc, subject, reply, string(rmsg), s.jsonResponse(resp))\n}\n\nfunc encodeStreamPurge(sp *streamPurge) []byte {\n\tvar bb bytes.Buffer\n\tbb.WriteByte(byte(purgeStreamOp))\n\tjson.NewEncoder(&bb).Encode(sp)\n\treturn bb.Bytes()\n}\n\nfunc decodeStreamPurge(buf []byte) (*streamPurge, error) {\n\tvar sp streamPurge\n\terr := json.Unmarshal(buf, &sp)\n\treturn &sp, err\n}\n\nfunc (s *Server) jsClusteredConsumerDeleteRequest(ci *ClientInfo, acc *Account, stream, consumer, subject, reply string, rmsg []byte) {\n\tjs, cc := s.getJetStreamCluster()\n\tif js == nil || cc == nil {\n\t\treturn\n\t}\n\n\tjs.mu.Lock()\n\tdefer js.mu.Unlock()\n\n\tif cc.meta == nil {\n\t\treturn\n\t}\n\n\tvar resp = JSApiConsumerDeleteResponse{ApiResponse: ApiResponse{Type: JSApiConsumerDeleteResponseType}}\n\n\tsa := js.streamAssignment(acc.Name, stream)\n\tif sa == nil {\n\t\tresp.Error = NewJSStreamNotFoundError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(rmsg), s.jsonResponse(&resp))\n\t\treturn\n\n\t}\n\tif sa.consumers == nil {\n\t\tresp.Error = NewJSConsumerNotFoundError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(rmsg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\toca := sa.consumers[consumer]\n\tif oca == nil {\n\t\tresp.Error = NewJSConsumerNotFoundError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(rmsg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\toca.deleted = true\n\tca := &consumerAssignment{Group: oca.Group, Stream: stream, Name: consumer, Config: oca.Config, Subject: subject, Reply: reply, Client: ci}\n\tcc.meta.Propose(encodeDeleteConsumerAssignment(ca))\n}\n\nfunc encodeMsgDelete(md *streamMsgDelete) []byte {\n\tvar bb bytes.Buffer\n\tbb.WriteByte(byte(deleteMsgOp))\n\tjson.NewEncoder(&bb).Encode(md)\n\treturn bb.Bytes()\n}\n\nfunc decodeMsgDelete(buf []byte) (*streamMsgDelete, error) {\n\tvar md streamMsgDelete\n\terr := json.Unmarshal(buf, &md)\n\treturn &md, err\n}\n\nfunc (s *Server) jsClusteredMsgDeleteRequest(ci *ClientInfo, acc *Account, mset *stream, stream, subject, reply string, req *JSApiMsgDeleteRequest, rmsg []byte) {\n\tjs, cc := s.getJetStreamCluster()\n\tif js == nil || cc == nil {\n\t\treturn\n\t}\n\n\tjs.mu.Lock()\n\tsa := js.streamAssignment(acc.Name, stream)\n\tif sa == nil {\n\t\ts.Debugf(\"Message delete failed, could not locate stream '%s > %s'\", acc.Name, stream)\n\t\tjs.mu.Unlock()\n\t\treturn\n\t}\n\n\t// Check for single replica items.\n\tif n := sa.Group.node; n != nil {\n\t\tmd := streamMsgDelete{Seq: req.Seq, NoErase: req.NoErase, Stream: stream, Subject: subject, Reply: reply, Client: ci}\n\t\tn.Propose(encodeMsgDelete(&md))\n\t\tjs.mu.Unlock()\n\t\treturn\n\t}\n\tjs.mu.Unlock()\n\n\tif mset == nil {\n\t\treturn\n\t}\n\n\tvar err error\n\tvar removed bool\n\tif req.NoErase {\n\t\tremoved, err = mset.removeMsg(req.Seq)\n\t} else {\n\t\tremoved, err = mset.eraseMsg(req.Seq)\n\t}\n\tvar resp = JSApiMsgDeleteResponse{ApiResponse: ApiResponse{Type: JSApiMsgDeleteResponseType}}\n\tif err != nil {\n\t\tresp.Error = NewJSStreamMsgDeleteFailedError(err, Unless(err))\n\t} else if !removed {\n\t\tresp.Error = NewJSSequenceNotFoundError(req.Seq)\n\t} else {\n\t\tresp.Success = true\n\t}\n\ts.sendAPIResponse(ci, acc, subject, reply, string(rmsg), s.jsonResponse(resp))\n}\n\nfunc encodeAddStreamAssignment(sa *streamAssignment) []byte {\n\tcsa := *sa\n\tcsa.Client = csa.Client.forProposal()\n\tvar bb bytes.Buffer\n\tbb.WriteByte(byte(assignStreamOp))\n\tjson.NewEncoder(&bb).Encode(csa)\n\treturn bb.Bytes()\n}\n\nfunc encodeUpdateStreamAssignment(sa *streamAssignment) []byte {\n\tcsa := *sa\n\tcsa.Client = csa.Client.forProposal()\n\tvar bb bytes.Buffer\n\tbb.WriteByte(byte(updateStreamOp))\n\tjson.NewEncoder(&bb).Encode(csa)\n\treturn bb.Bytes()\n}\n\nfunc encodeDeleteStreamAssignment(sa *streamAssignment) []byte {\n\tcsa := *sa\n\tcsa.Client = csa.Client.forProposal()\n\tvar bb bytes.Buffer\n\tbb.WriteByte(byte(removeStreamOp))\n\tjson.NewEncoder(&bb).Encode(csa)\n\treturn bb.Bytes()\n}\n\nfunc decodeStreamAssignment(buf []byte) (*streamAssignment, error) {\n\tvar sa streamAssignment\n\terr := json.Unmarshal(buf, &sa)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tfixCfgMirrorWithDedupWindow(sa.Config)\n\treturn &sa, err\n}\n\nfunc encodeDeleteRange(dr *DeleteRange) []byte {\n\tvar bb bytes.Buffer\n\tbb.WriteByte(byte(deleteRangeOp))\n\tjson.NewEncoder(&bb).Encode(dr)\n\treturn bb.Bytes()\n}\n\nfunc decodeDeleteRange(buf []byte) (*DeleteRange, error) {\n\tvar dr DeleteRange\n\terr := json.Unmarshal(buf, &dr)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &dr, err\n}\n\n// createGroupForConsumer will create a new group from same peer set as the stream.\nfunc (cc *jetStreamCluster) createGroupForConsumer(cfg *ConsumerConfig, sa *streamAssignment) *raftGroup {\n\tif len(sa.Group.Peers) == 0 || cfg.Replicas > len(sa.Group.Peers) {\n\t\treturn nil\n\t}\n\n\tpeers := copyStrings(sa.Group.Peers)\n\tvar _ss [5]string\n\tactive := _ss[:0]\n\n\t// Calculate all active peers.\n\tfor _, peer := range peers {\n\t\tif sir, ok := cc.s.nodeToInfo.Load(peer); ok && sir != nil {\n\t\t\tif !sir.(nodeInfo).offline {\n\t\t\t\tactive = append(active, peer)\n\t\t\t}\n\t\t}\n\t}\n\tif quorum := cfg.Replicas/2 + 1; quorum > len(active) {\n\t\t// Not enough active to satisfy the request.\n\t\treturn nil\n\t}\n\n\t// If we want less then our parent stream, select from active.\n\tif cfg.Replicas > 0 && cfg.Replicas < len(peers) {\n\t\t// Pedantic in case stream is say R5 and consumer is R3 and 3 or more offline, etc.\n\t\tif len(active) < cfg.Replicas {\n\t\t\treturn nil\n\t\t}\n\t\t// First shuffle the active peers and then select to account for replica = 1.\n\t\trand.Shuffle(len(active), func(i, j int) { active[i], active[j] = active[j], active[i] })\n\t\tpeers = active[:cfg.Replicas]\n\t}\n\tstorage := sa.Config.Storage\n\tif cfg.MemoryStorage {\n\t\tstorage = MemoryStorage\n\t}\n\treturn &raftGroup{Name: groupNameForConsumer(peers, storage), Storage: storage, Peers: peers}\n}\n\n// jsClusteredConsumerRequest is first point of entry to create a consumer in clustered mode.\nfunc (s *Server) jsClusteredConsumerRequest(ci *ClientInfo, acc *Account, subject, reply string, rmsg []byte, stream string, cfg *ConsumerConfig, action ConsumerAction, pedantic bool) {\n\tjs, cc := s.getJetStreamCluster()\n\tif js == nil || cc == nil {\n\t\treturn\n\t}\n\n\tvar resp = JSApiConsumerCreateResponse{ApiResponse: ApiResponse{Type: JSApiConsumerCreateResponseType}}\n\n\tstreamCfg, ok := js.clusterStreamConfig(acc.Name, stream)\n\tif !ok {\n\t\tresp.Error = NewJSStreamNotFoundError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(rmsg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\tselectedLimits, _, _, apiErr := acc.selectLimits(cfg.replicas(&streamCfg))\n\tif apiErr != nil {\n\t\tresp.Error = apiErr\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(rmsg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\tsrvLim := &s.getOpts().JetStreamLimits\n\t// Make sure we have sane defaults\n\tif err := setConsumerConfigDefaults(cfg, &streamCfg, srvLim, selectedLimits, pedantic); err != nil {\n\t\tresp.Error = err\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(rmsg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tif err := checkConsumerCfg(cfg, srvLim, &streamCfg, acc, selectedLimits, false); err != nil {\n\t\tresp.Error = err\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(rmsg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tjs.mu.Lock()\n\tdefer js.mu.Unlock()\n\n\tif cc.meta == nil {\n\t\treturn\n\t}\n\n\t// Lookup the stream assignment.\n\tsa := js.streamAssignment(acc.Name, stream)\n\tif sa == nil {\n\t\tresp.Error = NewJSStreamNotFoundError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(rmsg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\t// Was a consumer name provided?\n\tvar oname string\n\tif isDurableConsumer(cfg) || cfg.Name != _EMPTY_ {\n\t\tif cfg.Name != _EMPTY_ {\n\t\t\toname = cfg.Name\n\t\t} else {\n\t\t\toname = cfg.Durable\n\t\t}\n\t}\n\n\t// Check for max consumers here to short circuit if possible.\n\t// Start with limit on a stream, but if one is defined at the level of the account\n\t// and is lower, use that limit.\n\tif action == ActionCreate || action == ActionCreateOrUpdate {\n\t\tmaxc := sa.Config.MaxConsumers\n\t\tif maxc <= 0 || (selectedLimits.MaxConsumers > 0 && selectedLimits.MaxConsumers < maxc) {\n\t\t\tmaxc = selectedLimits.MaxConsumers\n\t\t}\n\t\tif maxc > 0 {\n\t\t\t// Don't count DIRECTS.\n\t\t\ttotal := 0\n\t\t\tfor cn, ca := range sa.consumers {\n\t\t\t\t// If the consumer name is specified and we think it already exists, then\n\t\t\t\t// we're likely updating an existing consumer, so don't count it. Otherwise\n\t\t\t\t// we will incorrectly return NewJSMaximumConsumersLimitError for an update.\n\t\t\t\tif oname != _EMPTY_ && cn == oname && sa.consumers[oname] != nil {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tif ca.Config != nil && !ca.Config.Direct {\n\t\t\t\t\ttotal++\n\t\t\t\t}\n\t\t\t}\n\t\t\tif total >= maxc {\n\t\t\t\tresp.Error = NewJSMaximumConsumersLimitError()\n\t\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(rmsg), s.jsonResponse(&resp))\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}\n\n\t// Also short circuit if DeliverLastPerSubject is set with no FilterSubject.\n\tif cfg.DeliverPolicy == DeliverLastPerSubject {\n\t\tif cfg.FilterSubject == _EMPTY_ && len(cfg.FilterSubjects) == 0 {\n\t\t\tresp.Error = NewJSConsumerInvalidPolicyError(fmt.Errorf(\"consumer delivery policy is deliver last per subject, but FilterSubject is not set\"))\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(rmsg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t}\n\n\t// Setup proper default for ack wait if we are in explicit ack mode.\n\tif cfg.AckWait == 0 && (cfg.AckPolicy == AckExplicit || cfg.AckPolicy == AckAll) {\n\t\tcfg.AckWait = JsAckWaitDefault\n\t}\n\t// Setup default of -1, meaning no limit for MaxDeliver.\n\tif cfg.MaxDeliver == 0 {\n\t\tcfg.MaxDeliver = -1\n\t}\n\t// Set proper default for max ack pending if we are ack explicit and none has been set.\n\tif cfg.AckPolicy == AckExplicit && cfg.MaxAckPending == 0 {\n\t\tcfg.MaxAckPending = JsDefaultMaxAckPending\n\t}\n\n\tif cfg.PriorityPolicy == PriorityPinnedClient && cfg.PinnedTTL == 0 {\n\t\tcfg.PinnedTTL = JsDefaultPinnedTTL\n\t}\n\n\tvar ca *consumerAssignment\n\n\t// See if we have an existing one already under same durable name or\n\t// if name was set by the user.\n\tif oname != _EMPTY_ {\n\t\tif ca = sa.consumers[oname]; ca != nil && !ca.deleted {\n\t\t\t// Provided config might miss metadata, copy from existing config.\n\t\t\tcopyConsumerMetadata(cfg, ca.Config)\n\n\t\t\tif action == ActionCreate && !reflect.DeepEqual(cfg, ca.Config) {\n\t\t\t\tresp.Error = NewJSConsumerAlreadyExistsError()\n\t\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(rmsg), s.jsonResponse(&resp))\n\t\t\t\treturn\n\t\t\t}\n\t\t\t// Do quick sanity check on new cfg to prevent here if possible.\n\t\t\tif err := acc.checkNewConsumerConfig(ca.Config, cfg); err != nil {\n\t\t\t\tresp.Error = NewJSConsumerCreateError(err, Unless(err))\n\t\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(rmsg), s.jsonResponse(&resp))\n\t\t\t\treturn\n\t\t\t}\n\t\t\t// Don't allow updating if all peers are offline.\n\t\t\tif s.allPeersOffline(ca.Group) {\n\t\t\t\tresp.Error = NewJSConsumerOfflineError()\n\t\t\t\ts.sendDelayedAPIErrResponse(ci, acc, subject, reply, string(rmsg), s.jsonResponse(&resp), nil, errRespDelay)\n\t\t\t\treturn\n\t\t\t}\n\t\t} else {\n\t\t\t// Initialize/update asset version metadata.\n\t\t\t// First time creating this consumer, or updating.\n\t\t\tsetStaticConsumerMetadata(cfg)\n\t\t}\n\t}\n\n\t// Initialize/update asset version metadata.\n\t// But only if we're not creating, should only update it the first time\n\t// to be idempotent with versions where there's no versioning metadata.\n\tif action != ActionCreate {\n\t\tsetStaticConsumerMetadata(cfg)\n\t}\n\n\t// If this is new consumer.\n\tif ca == nil {\n\t\tif action == ActionUpdate {\n\t\t\tresp.Error = NewJSConsumerDoesNotExistError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(rmsg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\trg := cc.createGroupForConsumer(cfg, sa)\n\t\tif rg == nil {\n\t\t\tresp.Error = NewJSInsufficientResourcesError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(rmsg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\t// Pick a preferred leader.\n\t\trg.setPreferred()\n\n\t\t// Inherit cluster from stream.\n\t\trg.Cluster = sa.Group.Cluster\n\n\t\t// We need to set the ephemeral here before replicating.\n\t\tif !isDurableConsumer(cfg) {\n\t\t\t// We chose to have ephemerals be R=1 unless stream is interest or workqueue.\n\t\t\t// Consumer can override.\n\t\t\tif sa.Config.Retention == LimitsPolicy && cfg.Replicas <= 1 {\n\t\t\t\trg.Peers = []string{rg.Preferred}\n\t\t\t\trg.Name = groupNameForConsumer(rg.Peers, rg.Storage)\n\t\t\t}\n\t\t\tif cfg.Name != _EMPTY_ {\n\t\t\t\toname = cfg.Name\n\t\t\t} else {\n\t\t\t\t// Make sure name is unique.\n\t\t\t\tfor {\n\t\t\t\t\toname = createConsumerName()\n\t\t\t\t\tif sa.consumers != nil {\n\t\t\t\t\t\tif sa.consumers[oname] != nil {\n\t\t\t\t\t\t\tcontinue\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tif len(rg.Peers) > 1 {\n\t\t\tif maxHaAssets := s.getOpts().JetStreamLimits.MaxHAAssets; maxHaAssets != 0 {\n\t\t\t\tfor _, peer := range rg.Peers {\n\t\t\t\t\tif ni, ok := s.nodeToInfo.Load(peer); ok {\n\t\t\t\t\t\tni := ni.(nodeInfo)\n\t\t\t\t\t\tif stats := ni.stats; stats != nil && stats.HAAssets > maxHaAssets {\n\t\t\t\t\t\t\tresp.Error = NewJSInsufficientResourcesError()\n\t\t\t\t\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(rmsg), s.jsonResponse(&resp))\n\t\t\t\t\t\t\ts.Warnf(\"%s@%s (HA Asset Count: %d) exceeds max ha asset limit of %d\"+\n\t\t\t\t\t\t\t\t\" for (durable) consumer %s placement on stream %s\",\n\t\t\t\t\t\t\t\tni.name, ni.cluster, ni.stats.HAAssets, maxHaAssets, oname, stream)\n\t\t\t\t\t\t\treturn\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t// Check if we are work queue policy.\n\t\t// We will do pre-checks here to avoid thrashing meta layer.\n\t\tif sa.Config.Retention == WorkQueuePolicy && !cfg.Direct {\n\t\t\tif cfg.AckPolicy != AckExplicit {\n\t\t\t\tresp.Error = NewJSConsumerWQRequiresExplicitAckError()\n\t\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(rmsg), s.jsonResponse(&resp))\n\t\t\t\treturn\n\t\t\t}\n\t\t\tsubjects := gatherSubjectFilters(cfg.FilterSubject, cfg.FilterSubjects)\n\t\t\tif len(subjects) == 0 && len(sa.consumers) > 0 {\n\t\t\t\tresp.Error = NewJSConsumerWQMultipleUnfilteredError()\n\t\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(rmsg), s.jsonResponse(&resp))\n\t\t\t\treturn\n\t\t\t}\n\t\t\t// Check here to make sure we have not collided with another.\n\t\t\tif len(sa.consumers) > 0 {\n\t\t\t\tfor _, oca := range sa.consumers {\n\t\t\t\t\tif oca.Name == oname {\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\t\t\t\t\tfor _, psubj := range gatherSubjectFilters(oca.Config.FilterSubject, oca.Config.FilterSubjects) {\n\t\t\t\t\t\tfor _, subj := range subjects {\n\t\t\t\t\t\t\tif SubjectsCollide(subj, psubj) {\n\t\t\t\t\t\t\t\tresp.Error = NewJSConsumerWQConsumerNotUniqueError()\n\t\t\t\t\t\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(rmsg), s.jsonResponse(&resp))\n\t\t\t\t\t\t\t\treturn\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tca = &consumerAssignment{\n\t\t\tGroup:   rg,\n\t\t\tStream:  stream,\n\t\t\tName:    oname,\n\t\t\tConfig:  cfg,\n\t\t\tSubject: subject,\n\t\t\tReply:   reply,\n\t\t\tClient:  ci,\n\t\t\tCreated: time.Now().UTC(),\n\t\t}\n\t} else {\n\t\t// If the consumer already exists then don't allow updating the PauseUntil, just set\n\t\t// it back to whatever the current configured value is.\n\t\tcfg.PauseUntil = ca.Config.PauseUntil\n\n\t\tnca := ca.copyGroup()\n\n\t\trBefore := nca.Config.replicas(sa.Config)\n\t\trAfter := cfg.replicas(sa.Config)\n\n\t\tvar curLeader string\n\t\tif rBefore != rAfter {\n\t\t\t// We are modifying nodes here. We want to do our best to preserve the current leader.\n\t\t\t// We have support now from above that guarantees we are in our own Go routine, so can\n\t\t\t// ask for stream info from the stream leader to make sure we keep the leader in the new list.\n\t\t\tif !s.allPeersOffline(ca.Group) {\n\t\t\t\t// Need to release js lock.\n\t\t\t\tjs.mu.Unlock()\n\t\t\t\tif ci, err := sysRequest[ConsumerInfo](s, clusterConsumerInfoT, ci.serviceAccount(), sa.Config.Name, cfg.Durable); err != nil {\n\t\t\t\t\ts.Warnf(\"Did not receive consumer info results for '%s > %s > %s' due to: %s\", acc, sa.Config.Name, cfg.Durable, err)\n\t\t\t\t} else if ci != nil {\n\t\t\t\t\tif cl := ci.Cluster; cl != nil {\n\t\t\t\t\t\tcurLeader = getHash(cl.Leader)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\t// Re-acquire here.\n\t\t\t\tjs.mu.Lock()\n\t\t\t}\n\t\t}\n\n\t\tif rBefore < rAfter {\n\t\t\tnewPeerSet := nca.Group.Peers\n\t\t\t// scale up by adding new members from the stream peer set that are not yet in the consumer peer set\n\t\t\tstreamPeerSet := copyStrings(sa.Group.Peers)\n\t\t\trand.Shuffle(rAfter, func(i, j int) { streamPeerSet[i], streamPeerSet[j] = streamPeerSet[j], streamPeerSet[i] })\n\t\t\tfor _, p := range streamPeerSet {\n\t\t\t\tfound := false\n\t\t\t\tfor _, sp := range newPeerSet {\n\t\t\t\t\tif sp == p {\n\t\t\t\t\t\tfound = true\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif !found {\n\t\t\t\t\tnewPeerSet = append(newPeerSet, p)\n\t\t\t\t\tif len(newPeerSet) == rAfter {\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tnca.Group.Peers = newPeerSet\n\t\t\tnca.Group.Preferred = curLeader\n\t\t} else if rBefore > rAfter {\n\t\t\tnewPeerSet := nca.Group.Peers\n\t\t\t// mark leader preferred and move it to end\n\t\t\tnca.Group.Preferred = curLeader\n\t\t\tif nca.Group.Preferred != _EMPTY_ {\n\t\t\t\tfor i, p := range newPeerSet {\n\t\t\t\t\tif nca.Group.Preferred == p {\n\t\t\t\t\t\tnewPeerSet[i] = newPeerSet[len(newPeerSet)-1]\n\t\t\t\t\t\tnewPeerSet[len(newPeerSet)-1] = p\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\t// scale down by removing peers from the end\n\t\t\tnewPeerSet = newPeerSet[len(newPeerSet)-rAfter:]\n\t\t\tnca.Group.Peers = newPeerSet\n\t\t}\n\n\t\t// Update config and client info on copy of existing.\n\t\tnca.Config = cfg\n\t\tnca.Client = ci\n\t\tnca.Subject = subject\n\t\tnca.Reply = reply\n\t\tca = nca\n\t}\n\n\t// Do formal proposal.\n\tif err := cc.meta.Propose(encodeAddConsumerAssignment(ca)); err == nil {\n\t\t// Mark this as pending.\n\t\tif sa.consumers == nil {\n\t\t\tsa.consumers = make(map[string]*consumerAssignment)\n\t\t}\n\t\tca.pending = true\n\t\tsa.consumers[ca.Name] = ca\n\t}\n}\n\nfunc encodeAddConsumerAssignment(ca *consumerAssignment) []byte {\n\tcca := *ca\n\tcca.Client = cca.Client.forProposal()\n\tvar bb bytes.Buffer\n\tbb.WriteByte(byte(assignConsumerOp))\n\tjson.NewEncoder(&bb).Encode(cca)\n\treturn bb.Bytes()\n}\n\nfunc encodeDeleteConsumerAssignment(ca *consumerAssignment) []byte {\n\tcca := *ca\n\tcca.Client = cca.Client.forProposal()\n\tvar bb bytes.Buffer\n\tbb.WriteByte(byte(removeConsumerOp))\n\tjson.NewEncoder(&bb).Encode(cca)\n\treturn bb.Bytes()\n}\n\nfunc decodeConsumerAssignment(buf []byte) (*consumerAssignment, error) {\n\tvar ca consumerAssignment\n\terr := json.Unmarshal(buf, &ca)\n\treturn &ca, err\n}\n\nfunc encodeAddConsumerAssignmentCompressed(ca *consumerAssignment) []byte {\n\tcca := *ca\n\tcca.Client = cca.Client.forProposal()\n\tvar bb bytes.Buffer\n\tbb.WriteByte(byte(assignCompressedConsumerOp))\n\ts2e := s2.NewWriter(&bb)\n\tjson.NewEncoder(s2e).Encode(cca)\n\ts2e.Close()\n\treturn bb.Bytes()\n}\n\nfunc decodeConsumerAssignmentCompressed(buf []byte) (*consumerAssignment, error) {\n\tvar ca consumerAssignment\n\tbb := bytes.NewBuffer(buf)\n\ts2d := s2.NewReader(bb)\n\treturn &ca, json.NewDecoder(s2d).Decode(&ca)\n}\n\nvar errBadStreamMsg = errors.New(\"jetstream cluster bad replicated stream msg\")\n\nfunc decodeStreamMsg(buf []byte) (subject, reply string, hdr, msg []byte, lseq uint64, ts int64, sourced bool, err error) {\n\tvar le = binary.LittleEndian\n\tif len(buf) < 26 {\n\t\treturn _EMPTY_, _EMPTY_, nil, nil, 0, 0, false, errBadStreamMsg\n\t}\n\tlseq = le.Uint64(buf)\n\tbuf = buf[8:]\n\tts = int64(le.Uint64(buf))\n\tbuf = buf[8:]\n\tsl := int(le.Uint16(buf))\n\tbuf = buf[2:]\n\tif len(buf) < sl {\n\t\treturn _EMPTY_, _EMPTY_, nil, nil, 0, 0, false, errBadStreamMsg\n\t}\n\tsubject = string(buf[:sl])\n\tbuf = buf[sl:]\n\tif len(buf) < 2 {\n\t\treturn _EMPTY_, _EMPTY_, nil, nil, 0, 0, false, errBadStreamMsg\n\t}\n\trl := int(le.Uint16(buf))\n\tbuf = buf[2:]\n\tif len(buf) < rl {\n\t\treturn _EMPTY_, _EMPTY_, nil, nil, 0, 0, false, errBadStreamMsg\n\t}\n\treply = string(buf[:rl])\n\tbuf = buf[rl:]\n\tif len(buf) < 2 {\n\t\treturn _EMPTY_, _EMPTY_, nil, nil, 0, 0, false, errBadStreamMsg\n\t}\n\thl := int(le.Uint16(buf))\n\tbuf = buf[2:]\n\tif len(buf) < hl {\n\t\treturn _EMPTY_, _EMPTY_, nil, nil, 0, 0, false, errBadStreamMsg\n\t}\n\tif hdr = buf[:hl]; len(hdr) == 0 {\n\t\thdr = nil\n\t}\n\tbuf = buf[hl:]\n\tif len(buf) < 4 {\n\t\treturn _EMPTY_, _EMPTY_, nil, nil, 0, 0, false, errBadStreamMsg\n\t}\n\tml := int(le.Uint32(buf))\n\tbuf = buf[4:]\n\tif len(buf) < ml {\n\t\treturn _EMPTY_, _EMPTY_, nil, nil, 0, 0, false, errBadStreamMsg\n\t}\n\tif msg = buf[:ml]; len(msg) == 0 {\n\t\tmsg = nil\n\t}\n\tbuf = buf[ml:]\n\tif len(buf) > 0 {\n\t\tflags, _ := binary.Uvarint(buf)\n\t\tsourced = flags&msgFlagFromSourceOrMirror != 0\n\t}\n\treturn subject, reply, hdr, msg, lseq, ts, sourced, nil\n}\n\n// Flags for encodeStreamMsg/decodeStreamMsg.\nconst (\n\tmsgFlagFromSourceOrMirror uint64 = 1 << iota\n)\n\nfunc encodeStreamMsg(subject, reply string, hdr, msg []byte, lseq uint64, ts int64, sourced bool) []byte {\n\treturn encodeStreamMsgAllowCompress(subject, reply, hdr, msg, lseq, ts, sourced)\n}\n\n// Threshold for compression.\n// TODO(dlc) - Eventually make configurable.\nconst compressThreshold = 8192 // 8k\n\n// If allowed and contents over the threshold we will compress.\nfunc encodeStreamMsgAllowCompress(subject, reply string, hdr, msg []byte, lseq uint64, ts int64, sourced bool) []byte {\n\t// Clip the subject, reply, header and msgs down. Operate on\n\t// uint64 lengths to avoid overflowing.\n\tslen := min(uint64(len(subject)), math.MaxUint16)\n\trlen := min(uint64(len(reply)), math.MaxUint16)\n\thlen := min(uint64(len(hdr)), math.MaxUint16)\n\tmlen := min(uint64(len(msg)), math.MaxUint32)\n\ttotal := slen + rlen + hlen + mlen\n\n\tshouldCompress := total > compressThreshold\n\telen := int(1 + 8 + 8 + total)\n\telen += (2 + 2 + 2 + 4 + 8) // Encoded lengths, 4bytes, flags are up to 8 bytes\n\n\tvar flags uint64\n\tif sourced {\n\t\tflags |= msgFlagFromSourceOrMirror\n\t}\n\n\tbuf := make([]byte, 1, elen)\n\tbuf[0] = byte(streamMsgOp)\n\n\tvar le = binary.LittleEndian\n\tbuf = le.AppendUint64(buf, lseq)\n\tbuf = le.AppendUint64(buf, uint64(ts))\n\tbuf = le.AppendUint16(buf, uint16(slen))\n\tbuf = append(buf, subject[:slen]...)\n\tbuf = le.AppendUint16(buf, uint16(rlen))\n\tbuf = append(buf, reply[:rlen]...)\n\tbuf = le.AppendUint16(buf, uint16(hlen))\n\tbuf = append(buf, hdr[:hlen]...)\n\tbuf = le.AppendUint32(buf, uint32(mlen))\n\tbuf = append(buf, msg[:mlen]...)\n\tbuf = binary.AppendUvarint(buf, flags)\n\n\t// Check if we should compress.\n\tif shouldCompress {\n\t\tnbuf := make([]byte, s2.MaxEncodedLen(elen))\n\t\tnbuf[0] = byte(compressedStreamMsgOp)\n\t\tebuf := s2.Encode(nbuf[1:], buf[1:])\n\t\t// Only pay the cost of decode on the other side if we compressed.\n\t\t// S2 will allow us to try without major penalty for non-compressable data.\n\t\tif len(ebuf) < len(buf) {\n\t\t\tbuf = nbuf[:len(ebuf)+1]\n\t\t}\n\t}\n\n\treturn buf\n}\n\n// Determine if all peers in our set support the binary snapshot.\nfunc (mset *stream) supportsBinarySnapshot() bool {\n\tmset.mu.RLock()\n\tdefer mset.mu.RUnlock()\n\treturn mset.supportsBinarySnapshotLocked()\n}\n\n// Determine if all peers in our set support the binary snapshot.\n// Lock should be held.\nfunc (mset *stream) supportsBinarySnapshotLocked() bool {\n\ts, n := mset.srv, mset.node\n\tif s == nil || n == nil {\n\t\treturn false\n\t}\n\t// Grab our peers and walk them to make sure we can all support binary stream snapshots.\n\tid, peers := n.ID(), n.Peers()\n\tfor _, p := range peers {\n\t\tif p.ID == id {\n\t\t\t// We know we support ourselves.\n\t\t\tcontinue\n\t\t}\n\t\t// Since release 2.10.16 only deny if we know the other node does not support.\n\t\tif sir, ok := s.nodeToInfo.Load(p.ID); ok && sir != nil && !sir.(nodeInfo).binarySnapshots {\n\t\t\treturn false\n\t\t}\n\t}\n\treturn true\n}\n\n// StreamSnapshot is used for snapshotting and out of band catch up in clustered mode.\n// Legacy, replace with binary stream snapshots.\ntype streamSnapshot struct {\n\tMsgs     uint64   `json:\"messages\"`\n\tBytes    uint64   `json:\"bytes\"`\n\tFirstSeq uint64   `json:\"first_seq\"`\n\tLastSeq  uint64   `json:\"last_seq\"`\n\tFailed   uint64   `json:\"clfs\"`\n\tDeleted  []uint64 `json:\"deleted,omitempty\"`\n}\n\n// Grab a snapshot of a stream for clustered mode.\nfunc (mset *stream) stateSnapshot() []byte {\n\tmset.mu.RLock()\n\tdefer mset.mu.RUnlock()\n\treturn mset.stateSnapshotLocked()\n}\n\n// Grab a snapshot of a stream for clustered mode.\n// Lock should be held.\nfunc (mset *stream) stateSnapshotLocked() []byte {\n\t// Decide if we can support the new style of stream snapshots.\n\tif mset.supportsBinarySnapshotLocked() {\n\t\tsnap, err := mset.store.EncodedStreamState(mset.getCLFS())\n\t\tif err != nil {\n\t\t\treturn nil\n\t\t}\n\t\treturn snap\n\t}\n\n\t// Older v1 version with deleted as a sorted []uint64.\n\tstate := mset.store.State()\n\tsnap := &streamSnapshot{\n\t\tMsgs:     state.Msgs,\n\t\tBytes:    state.Bytes,\n\t\tFirstSeq: state.FirstSeq,\n\t\tLastSeq:  state.LastSeq,\n\t\tFailed:   mset.getCLFS(),\n\t\tDeleted:  state.Deleted,\n\t}\n\tb, _ := json.Marshal(snap)\n\treturn b\n}\n\n// To warn when we are getting too far behind from what has been proposed vs what has been committed.\nconst streamLagWarnThreshold = 10_000\n\n// processClusteredInboundMsg will propose the inbound message to the underlying raft group.\nfunc (mset *stream) processClusteredInboundMsg(subject, reply string, hdr, msg []byte, mt *msgTrace, sourced bool) (retErr error) {\n\t// For possible error response.\n\tvar response []byte\n\n\tmset.mu.RLock()\n\tcanRespond := !mset.cfg.NoAck && len(reply) > 0\n\tname, stype, store := mset.cfg.Name, mset.cfg.Storage, mset.store\n\ts, js, jsa, st, r, tierName, outq, node := mset.srv, mset.js, mset.jsa, mset.cfg.Storage, mset.cfg.Replicas, mset.tier, mset.outq, mset.node\n\tmaxMsgSize, lseq := int(mset.cfg.MaxMsgSize), mset.lseq\n\tinterestPolicy, discard, maxMsgs, maxBytes := mset.cfg.Retention != LimitsPolicy, mset.cfg.Discard, mset.cfg.MaxMsgs, mset.cfg.MaxBytes\n\tisLeader, isSealed, allowTTL := mset.isLeader(), mset.cfg.Sealed, mset.cfg.AllowMsgTTL\n\tmset.mu.RUnlock()\n\n\t// This should not happen but possible now that we allow scale up, and scale down where this could trigger.\n\t//\n\t// We also invoke this in clustering mode for message tracing when not\n\t// performing message delivery.\n\tif node == nil || mt.traceOnly() {\n\t\treturn mset.processJetStreamMsg(subject, reply, hdr, msg, 0, 0, mt, sourced)\n\t}\n\n\t// If message tracing (with message delivery), we will need to send the\n\t// event on exit in case there was an error (if message was not proposed).\n\t// Otherwise, the event will be sent from processJetStreamMsg when\n\t// invoked by the leader (from applyStreamEntries).\n\tif mt != nil {\n\t\tdefer func() {\n\t\t\tif retErr != nil {\n\t\t\t\tmt.sendEventFromJetStream(retErr)\n\t\t\t}\n\t\t}()\n\t}\n\n\t// Check that we are the leader. This can be false if we have scaled up from an R1 that had inbound queued messages.\n\tif !isLeader {\n\t\treturn NewJSClusterNotLeaderError()\n\t}\n\n\t// Bail here if sealed.\n\tif isSealed {\n\t\tvar resp = JSPubAckResponse{PubAck: &PubAck{Stream: mset.name()}, Error: NewJSStreamSealedError()}\n\t\tb, _ := json.Marshal(resp)\n\t\tmset.outq.sendMsg(reply, b)\n\t\treturn NewJSStreamSealedError()\n\t}\n\n\t// Check here pre-emptively if we have exceeded this server limits.\n\tif js.limitsExceeded(stype) {\n\t\ts.resourcesExceededError()\n\t\tif canRespond {\n\t\t\tb, _ := json.Marshal(&JSPubAckResponse{PubAck: &PubAck{Stream: name}, Error: NewJSInsufficientResourcesError()})\n\t\t\toutq.send(newJSPubMsg(reply, _EMPTY_, _EMPTY_, nil, b, nil, 0))\n\t\t}\n\t\t// Stepdown regardless.\n\t\tif node := mset.raftNode(); node != nil {\n\t\t\tnode.StepDown()\n\t\t}\n\t\treturn NewJSInsufficientResourcesError()\n\t}\n\n\t// Check here pre-emptively if we have exceeded our account limits.\n\tif exceeded, err := jsa.wouldExceedLimits(st, tierName, r, subject, hdr, msg); exceeded {\n\t\tif err == nil {\n\t\t\terr = NewJSAccountResourcesExceededError()\n\t\t}\n\t\ts.RateLimitWarnf(\"JetStream account limits exceeded for '%s': %s\", jsa.acc().GetName(), err.Error())\n\t\tif canRespond {\n\t\t\tvar resp = &JSPubAckResponse{PubAck: &PubAck{Stream: name}}\n\t\t\tresp.Error = err\n\t\t\tresponse, _ = json.Marshal(resp)\n\t\t\toutq.send(newJSPubMsg(reply, _EMPTY_, _EMPTY_, nil, response, nil, 0))\n\t\t}\n\t\treturn err\n\t}\n\n\t// Check msgSize if we have a limit set there. Again this works if it goes through but better to be pre-emptive.\n\tif maxMsgSize >= 0 && (len(hdr)+len(msg)) > maxMsgSize {\n\t\terr := fmt.Errorf(\"JetStream message size exceeds limits for '%s > %s'\", jsa.acc().Name, mset.cfg.Name)\n\t\ts.RateLimitWarnf(\"%s\", err.Error())\n\t\tif canRespond {\n\t\t\tvar resp = &JSPubAckResponse{PubAck: &PubAck{Stream: name}}\n\t\t\tresp.Error = NewJSStreamMessageExceedsMaximumError()\n\t\t\tresponse, _ = json.Marshal(resp)\n\t\t\toutq.send(newJSPubMsg(reply, _EMPTY_, _EMPTY_, nil, response, nil, 0))\n\t\t}\n\t\treturn err\n\t}\n\n\t// Some header checks can be checked pre proposal. Most can not.\n\tvar msgId string\n\tif len(hdr) > 0 {\n\t\t// Since we encode header len as u16 make sure we do not exceed.\n\t\t// Again this works if it goes through but better to be pre-emptive.\n\t\tif len(hdr) > math.MaxUint16 {\n\t\t\terr := fmt.Errorf(\"JetStream header size exceeds limits for '%s > %s'\", jsa.acc().Name, mset.cfg.Name)\n\t\t\ts.RateLimitWarnf(\"%s\", err.Error())\n\t\t\tif canRespond {\n\t\t\t\tvar resp = &JSPubAckResponse{PubAck: &PubAck{Stream: name}}\n\t\t\t\tresp.Error = NewJSStreamHeaderExceedsMaximumError()\n\t\t\t\tresponse, _ = json.Marshal(resp)\n\t\t\t\toutq.send(newJSPubMsg(reply, _EMPTY_, _EMPTY_, nil, response, nil, 0))\n\t\t\t}\n\t\t\treturn err\n\t\t}\n\t\t// Expected stream name can also be pre-checked.\n\t\tif sname := getExpectedStream(hdr); sname != _EMPTY_ && sname != name {\n\t\t\tif canRespond {\n\t\t\t\tvar resp = &JSPubAckResponse{PubAck: &PubAck{Stream: name}}\n\t\t\t\tresp.PubAck = &PubAck{Stream: name}\n\t\t\t\tresp.Error = NewJSStreamNotMatchError()\n\t\t\t\tb, _ := json.Marshal(resp)\n\t\t\t\toutq.sendMsg(reply, b)\n\t\t\t}\n\t\t\treturn errStreamMismatch\n\t\t}\n\t\t// TTL'd messages are rejected entirely if TTLs are not enabled on the stream, or if the TTL is invalid.\n\t\tif ttl, err := getMessageTTL(hdr); !sourced && (ttl != 0 || err != nil) {\n\t\t\tif !allowTTL {\n\t\t\t\tif canRespond {\n\t\t\t\t\tvar resp = &JSPubAckResponse{PubAck: &PubAck{Stream: name}}\n\t\t\t\t\tresp.Error = NewJSMessageTTLDisabledError()\n\t\t\t\t\tb, _ := json.Marshal(resp)\n\t\t\t\t\toutq.sendMsg(reply, b)\n\t\t\t\t}\n\t\t\t\treturn errMsgTTLDisabled\n\t\t\t} else if err != nil {\n\t\t\t\tif canRespond {\n\t\t\t\t\tvar resp = &JSPubAckResponse{PubAck: &PubAck{Stream: name}}\n\t\t\t\t\tresp.Error = NewJSMessageTTLInvalidError()\n\t\t\t\t\tb, _ := json.Marshal(resp)\n\t\t\t\t\toutq.sendMsg(reply, b)\n\t\t\t\t}\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t\t// Check for MsgIds here at the cluster level to avoid excessive CLFS accounting.\n\t\t// Will help during restarts.\n\t\tif msgId = getMsgId(hdr); msgId != _EMPTY_ {\n\t\t\tmset.mu.Lock()\n\t\t\tif dde := mset.checkMsgId(msgId); dde != nil {\n\t\t\t\tvar buf [256]byte\n\t\t\t\tpubAck := append(buf[:0], mset.pubAck...)\n\t\t\t\tseq := dde.seq\n\t\t\t\tmset.mu.Unlock()\n\t\t\t\t// Should not return an invalid sequence, in that case error.\n\t\t\t\tif canRespond {\n\t\t\t\t\tif seq > 0 {\n\t\t\t\t\t\tresponse := append(pubAck, strconv.FormatUint(seq, 10)...)\n\t\t\t\t\t\tresponse = append(response, \",\\\"duplicate\\\": true}\"...)\n\t\t\t\t\t\toutq.sendMsg(reply, response)\n\t\t\t\t\t} else {\n\t\t\t\t\t\tvar resp = &JSPubAckResponse{PubAck: &PubAck{Stream: name}}\n\t\t\t\t\t\tresp.Error = ApiErrors[JSStreamDuplicateMessageConflict]\n\t\t\t\t\t\tb, _ := json.Marshal(resp)\n\t\t\t\t\t\toutq.sendMsg(reply, b)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\treturn errMsgIdDuplicate\n\t\t\t}\n\t\t\t// FIXME(dlc) - locking conflict with accessing mset.clseq\n\t\t\t// For now we stage with zero, and will update in processStreamMsg.\n\t\t\tmset.storeMsgIdLocked(&ddentry{msgId, 0, time.Now().UnixNano()})\n\t\t\tmset.mu.Unlock()\n\t\t}\n\t}\n\n\t// Proceed with proposing this message.\n\n\t// We only use mset.clseq for clustering and in case we run ahead of actual commits.\n\t// Check if we need to set initial value here\n\tmset.clMu.Lock()\n\tif mset.clseq == 0 || mset.clseq < lseq+mset.clfs {\n\t\t// Re-capture\n\t\tlseq = mset.lastSeq()\n\t\tmset.clseq = lseq + mset.clfs\n\t}\n\n\t// Check if we have an interest policy and discard new with max msgs or bytes.\n\t// We need to deny here otherwise it could succeed on some peers and not others\n\t// depending on consumer ack state. So we deny here, if we allow that means we know\n\t// it would succeed on every peer.\n\tif interestPolicy && discard == DiscardNew && (maxMsgs > 0 || maxBytes > 0) {\n\t\t// Track inflight.\n\t\tif mset.inflight == nil {\n\t\t\tmset.inflight = make(map[uint64]uint64)\n\t\t}\n\t\tif stype == FileStorage {\n\t\t\tmset.inflight[mset.clseq] = fileStoreMsgSize(subject, hdr, msg)\n\t\t} else {\n\t\t\tmset.inflight[mset.clseq] = memStoreMsgSize(subject, hdr, msg)\n\t\t}\n\n\t\tvar state StreamState\n\t\tmset.store.FastState(&state)\n\n\t\tvar err error\n\t\tif maxMsgs > 0 && state.Msgs+uint64(len(mset.inflight)) > uint64(maxMsgs) {\n\t\t\terr = ErrMaxMsgs\n\t\t} else if maxBytes > 0 {\n\t\t\t// TODO(dlc) - Could track this rollup independently.\n\t\t\tvar bytesPending uint64\n\t\t\tfor _, nb := range mset.inflight {\n\t\t\t\tbytesPending += nb\n\t\t\t}\n\t\t\tif state.Bytes+bytesPending > uint64(maxBytes) {\n\t\t\t\terr = ErrMaxBytes\n\t\t\t}\n\t\t}\n\t\tif err != nil {\n\t\t\tdelete(mset.inflight, mset.clseq)\n\t\t\tmset.clMu.Unlock()\n\t\t\tif canRespond {\n\t\t\t\tvar resp = &JSPubAckResponse{PubAck: &PubAck{Stream: name}}\n\t\t\t\tresp.Error = NewJSStreamStoreFailedError(err, Unless(err))\n\t\t\t\tresponse, _ = json.Marshal(resp)\n\t\t\t\toutq.send(newJSPubMsg(reply, _EMPTY_, _EMPTY_, nil, response, nil, 0))\n\t\t\t}\n\t\t\treturn err\n\t\t}\n\t}\n\n\tif len(hdr) > 0 {\n\t\t// Expected last sequence per subject.\n\t\tif seq, exists := getExpectedLastSeqPerSubject(hdr); exists && store != nil {\n\t\t\t// Allow override of the subject used for the check.\n\t\t\tseqSubj := subject\n\t\t\tif optSubj := getExpectedLastSeqPerSubjectForSubject(hdr); optSubj != _EMPTY_ {\n\t\t\t\tseqSubj = optSubj\n\t\t\t}\n\n\t\t\t// If subject is already in process, block as otherwise we could have multiple messages inflight with same subject.\n\t\t\tif _, found := mset.expectedPerSubjectInProcess[seqSubj]; found {\n\t\t\t\t// Could have set inflight above, cleanup here.\n\t\t\t\tdelete(mset.inflight, mset.clseq)\n\t\t\t\tmset.clMu.Unlock()\n\t\t\t\tif canRespond {\n\t\t\t\t\tvar resp = &JSPubAckResponse{PubAck: &PubAck{Stream: name}}\n\t\t\t\t\tresp.PubAck = &PubAck{Stream: name}\n\t\t\t\t\tresp.Error = NewJSStreamWrongLastSequenceConstantError()\n\t\t\t\t\tb, _ := json.Marshal(resp)\n\t\t\t\t\toutq.sendMsg(reply, b)\n\t\t\t\t}\n\t\t\t\treturn fmt.Errorf(\"last sequence by subject mismatch\")\n\t\t\t}\n\n\t\t\tvar smv StoreMsg\n\t\t\tvar fseq uint64\n\t\t\tsm, err := store.LoadLastMsg(seqSubj, &smv)\n\t\t\tif sm != nil {\n\t\t\t\tfseq = sm.seq\n\t\t\t}\n\t\t\tif err == ErrStoreMsgNotFound && seq == 0 {\n\t\t\t\tfseq, err = 0, nil\n\t\t\t}\n\t\t\tif err != nil || fseq != seq {\n\t\t\t\t// Could have set inflight above, cleanup here.\n\t\t\t\tdelete(mset.inflight, mset.clseq)\n\t\t\t\tmset.clMu.Unlock()\n\t\t\t\tif canRespond {\n\t\t\t\t\tvar resp = &JSPubAckResponse{PubAck: &PubAck{Stream: name}}\n\t\t\t\t\tresp.PubAck = &PubAck{Stream: name}\n\t\t\t\t\tresp.Error = NewJSStreamWrongLastSequenceError(fseq)\n\t\t\t\t\tb, _ := json.Marshal(resp)\n\t\t\t\t\toutq.sendMsg(reply, b)\n\t\t\t\t}\n\t\t\t\treturn fmt.Errorf(\"last sequence by subject mismatch: %d vs %d\", seq, fseq)\n\t\t\t}\n\n\t\t\t// Track sequence and subject.\n\t\t\tif mset.expectedPerSubjectSequence == nil {\n\t\t\t\tmset.expectedPerSubjectSequence = make(map[uint64]string)\n\t\t\t}\n\t\t\tif mset.expectedPerSubjectInProcess == nil {\n\t\t\t\tmset.expectedPerSubjectInProcess = make(map[string]struct{})\n\t\t\t}\n\t\t\tmset.expectedPerSubjectSequence[mset.clseq] = seqSubj\n\t\t\tmset.expectedPerSubjectInProcess[seqSubj] = struct{}{}\n\t\t}\n\t}\n\n\tesm := encodeStreamMsgAllowCompress(subject, reply, hdr, msg, mset.clseq, time.Now().UnixNano(), sourced)\n\tvar mtKey uint64\n\tif mt != nil {\n\t\tmtKey = mset.clseq\n\t\tif mset.mt == nil {\n\t\t\tmset.mt = make(map[uint64]*msgTrace)\n\t\t}\n\t\tmset.mt[mtKey] = mt\n\t}\n\n\t// Do proposal.\n\terr := node.Propose(esm)\n\tif err == nil {\n\t\tmset.clseq++\n\t\t// If we are using the system account for NRG, add in the extra sent msgs and bytes to our account\n\t\t// so that the end user / account owner has visibility.\n\t\tif node.IsSystemAccount() && mset.acc != nil && r > 1 {\n\t\t\toutMsgs := int64(r - 1)\n\t\t\toutBytes := int64(len(esm) * (r - 1))\n\n\t\t\tmset.acc.stats.Lock()\n\t\t\tmset.acc.stats.outMsgs += outMsgs\n\t\t\tmset.acc.stats.outBytes += outBytes\n\t\t\tmset.acc.stats.rt.outMsgs += outMsgs\n\t\t\tmset.acc.stats.rt.outBytes += outBytes\n\t\t\tmset.acc.stats.Unlock()\n\t\t}\n\t}\n\n\t// Check to see if we are being overrun.\n\t// TODO(dlc) - Make this a limit where we drop messages to protect ourselves, but allow to be configured.\n\tif mset.clseq-(lseq+mset.clfs) > streamLagWarnThreshold {\n\t\tlerr := fmt.Errorf(\"JetStream stream '%s > %s' has high message lag\", jsa.acc().Name, name)\n\t\ts.RateLimitWarnf(\"%s\", lerr.Error())\n\t}\n\tmset.clMu.Unlock()\n\n\tif err != nil {\n\t\tif mt != nil {\n\t\t\tmset.getAndDeleteMsgTrace(mtKey)\n\t\t}\n\t\tif canRespond {\n\t\t\tvar resp = &JSPubAckResponse{PubAck: &PubAck{Stream: mset.cfg.Name}}\n\t\t\tresp.Error = &ApiError{Code: 503, Description: err.Error()}\n\t\t\tresponse, _ = json.Marshal(resp)\n\t\t\t// If we errored out respond here.\n\t\t\toutq.send(newJSPubMsg(reply, _EMPTY_, _EMPTY_, nil, response, nil, 0))\n\t\t}\n\t\tif isOutOfSpaceErr(err) {\n\t\t\ts.handleOutOfSpace(mset)\n\t\t}\n\t}\n\n\treturn err\n}\n\nfunc (mset *stream) getAndDeleteMsgTrace(lseq uint64) *msgTrace {\n\tif mset == nil {\n\t\treturn nil\n\t}\n\tmset.clMu.Lock()\n\tmt, ok := mset.mt[lseq]\n\tif ok {\n\t\tdelete(mset.mt, lseq)\n\t}\n\tmset.clMu.Unlock()\n\treturn mt\n}\n\n// For requesting messages post raft snapshot to catch up streams post server restart.\n// Any deleted msgs etc will be handled inline on catchup.\ntype streamSyncRequest struct {\n\tPeer           string `json:\"peer,omitempty\"`\n\tFirstSeq       uint64 `json:\"first_seq\"`\n\tLastSeq        uint64 `json:\"last_seq\"`\n\tDeleteRangesOk bool   `json:\"delete_ranges\"`\n\tMinApplied     uint64 `json:\"min_applied\"`\n}\n\n// Given a stream state that represents a snapshot, calculate the sync request based on our current state.\n// Stream lock must be held.\nfunc (mset *stream) calculateSyncRequest(state *StreamState, snap *StreamReplicatedState, index uint64) *streamSyncRequest {\n\t// Shouldn't happen, but consequences are pretty bad if we have the lock held and\n\t// our caller tries to take the lock again on panic defer, as in processSnapshot.\n\tif state == nil || snap == nil || mset.node == nil {\n\t\treturn nil\n\t}\n\t// Quick check if we are already caught up.\n\tif state.LastSeq >= snap.LastSeq {\n\t\treturn nil\n\t}\n\treturn &streamSyncRequest{FirstSeq: state.LastSeq + 1, LastSeq: snap.LastSeq, Peer: mset.node.ID(), DeleteRangesOk: true, MinApplied: index}\n}\n\n// processSnapshotDeletes will update our current store based on the snapshot\n// but only processing deletes and new FirstSeq / purges.\nfunc (mset *stream) processSnapshotDeletes(snap *StreamReplicatedState) {\n\tmset.mu.Lock()\n\tvar state StreamState\n\tmset.store.FastState(&state)\n\t// Always adjust if FirstSeq has moved beyond our state.\n\tvar didReset bool\n\tif snap.FirstSeq > state.FirstSeq {\n\t\tmset.store.Compact(snap.FirstSeq)\n\t\tmset.store.FastState(&state)\n\t\tmset.lseq = state.LastSeq\n\t\tmset.clearAllPreAcksBelowFloor(state.FirstSeq)\n\t\tdidReset = true\n\t}\n\ts := mset.srv\n\tmset.mu.Unlock()\n\n\tif didReset {\n\t\ts.Warnf(\"Catchup for stream '%s > %s' resetting first sequence: %d on catchup request\",\n\t\t\tmset.account(), mset.name(), snap.FirstSeq)\n\t}\n\n\tif len(snap.Deleted) > 0 {\n\t\tmset.store.SyncDeleted(snap.Deleted)\n\t}\n}\n\nfunc (mset *stream) setCatchupPeer(peer string, lag uint64) {\n\tif peer == _EMPTY_ {\n\t\treturn\n\t}\n\tmset.mu.Lock()\n\tif mset.catchups == nil {\n\t\tmset.catchups = make(map[string]uint64)\n\t}\n\tmset.catchups[peer] = lag\n\tmset.mu.Unlock()\n}\n\n// Will decrement by one.\nfunc (mset *stream) updateCatchupPeer(peer string) {\n\tif peer == _EMPTY_ {\n\t\treturn\n\t}\n\tmset.mu.Lock()\n\tif lag := mset.catchups[peer]; lag > 0 {\n\t\tmset.catchups[peer] = lag - 1\n\t}\n\tmset.mu.Unlock()\n}\n\nfunc (mset *stream) decrementCatchupPeer(peer string, num uint64) {\n\tif peer == _EMPTY_ {\n\t\treturn\n\t}\n\tmset.mu.Lock()\n\tif lag := mset.catchups[peer]; lag > 0 {\n\t\tif lag >= num {\n\t\t\tlag -= num\n\t\t} else {\n\t\t\tlag = 0\n\t\t}\n\t\tmset.catchups[peer] = lag\n\t}\n\tmset.mu.Unlock()\n}\n\nfunc (mset *stream) clearCatchupPeer(peer string) {\n\tmset.mu.Lock()\n\tif mset.catchups != nil {\n\t\tdelete(mset.catchups, peer)\n\t}\n\tmset.mu.Unlock()\n}\n\n// Lock should be held.\nfunc (mset *stream) clearAllCatchupPeers() {\n\tif mset.catchups != nil {\n\t\tmset.catchups = nil\n\t}\n}\n\nfunc (mset *stream) lagForCatchupPeer(peer string) uint64 {\n\tmset.mu.RLock()\n\tdefer mset.mu.RUnlock()\n\tif mset.catchups == nil {\n\t\treturn 0\n\t}\n\treturn mset.catchups[peer]\n}\n\nfunc (mset *stream) hasCatchupPeers() bool {\n\tmset.mu.RLock()\n\tdefer mset.mu.RUnlock()\n\treturn len(mset.catchups) > 0\n}\n\nfunc (mset *stream) setCatchingUp() {\n\tmset.catchup.Store(true)\n}\n\nfunc (mset *stream) clearCatchingUp() {\n\tmset.catchup.Store(false)\n}\n\nfunc (mset *stream) isCatchingUp() bool {\n\treturn mset.catchup.Load()\n}\n\n// Determine if a non-leader is current.\n// Lock should be held.\nfunc (mset *stream) isCurrent() bool {\n\tif mset.node == nil {\n\t\treturn true\n\t}\n\treturn mset.node.Current() && !mset.catchup.Load()\n}\n\n// Maximum requests for the whole server that can be in flight at the same time.\nconst maxConcurrentSyncRequests = 32\n\nvar (\n\terrCatchupCorruptSnapshot = errors.New(\"corrupt stream snapshot detected\")\n\terrCatchupStalled         = errors.New(\"catchup stalled\")\n\terrCatchupStreamStopped   = errors.New(\"stream has been stopped\") // when a catchup is terminated due to the stream going away.\n\terrCatchupBadMsg          = errors.New(\"bad catchup msg\")\n\terrCatchupWrongSeqForSkip = errors.New(\"wrong sequence for skipped msg\")\n\terrCatchupAbortedNoLeader = errors.New(\"catchup aborted, no leader\")\n\terrCatchupTooManyRetries  = errors.New(\"catchup failed, too many retries\")\n)\n\n// Process a stream snapshot.\nfunc (mset *stream) processSnapshot(snap *StreamReplicatedState, index uint64) (e error) {\n\t// Update any deletes, etc.\n\tmset.processSnapshotDeletes(snap)\n\tmset.setCLFS(snap.Failed)\n\n\tmset.mu.Lock()\n\tvar state StreamState\n\tmset.store.FastState(&state)\n\tsreq := mset.calculateSyncRequest(&state, snap, index)\n\n\ts, js, subject, n, st := mset.srv, mset.js, mset.sa.Sync, mset.node, mset.cfg.Storage\n\tqname := fmt.Sprintf(\"[ACC:%s] stream '%s' snapshot\", mset.acc.Name, mset.cfg.Name)\n\tmset.mu.Unlock()\n\n\t// Bug that would cause this to be empty on stream update.\n\tif subject == _EMPTY_ {\n\t\treturn errCatchupCorruptSnapshot\n\t}\n\n\t// Just return if up to date or already exceeded limits.\n\tif sreq == nil || js.limitsExceeded(st) {\n\t\treturn nil\n\t}\n\n\t// Pause the apply channel for our raft group while we catch up.\n\tif err := n.PauseApply(); err != nil {\n\t\treturn err\n\t}\n\n\tdefer func() {\n\t\t// Don't bother resuming if server or stream is gone.\n\t\tif e != errCatchupStreamStopped && e != ErrServerNotRunning {\n\t\t\tn.ResumeApply()\n\t\t}\n\t}()\n\n\t// Set our catchup state.\n\tmset.setCatchingUp()\n\tdefer mset.clearCatchingUp()\n\n\tvar sub *subscription\n\tvar err error\n\n\tconst (\n\t\tstartInterval    = 5 * time.Second\n\t\tactivityInterval = 30 * time.Second\n\t)\n\tnotActive := time.NewTimer(startInterval)\n\tdefer notActive.Stop()\n\n\tdefer func() {\n\t\tif sub != nil {\n\t\t\ts.sysUnsubscribe(sub)\n\t\t}\n\t\t// Make sure any consumers are updated for the pending amounts.\n\t\tmset.mu.Lock()\n\t\tfor _, o := range mset.consumers {\n\t\t\to.mu.Lock()\n\t\t\tif o.isLeader() {\n\t\t\t\to.streamNumPending()\n\t\t\t}\n\t\t\to.mu.Unlock()\n\t\t}\n\t\tmset.mu.Unlock()\n\n\t\t// If we are interest based make sure to check our ack floor state.\n\t\t// We will delay a bit to allow consumer states to also catchup.\n\t\tif mset.isInterestRetention() {\n\t\t\tfire := time.Duration(rand.Intn(10)+5) * time.Second\n\t\t\ttime.AfterFunc(fire, mset.checkInterestState)\n\t\t}\n\t}()\n\n\tvar releaseSem bool\n\treleaseSyncOutSem := func() {\n\t\tif !releaseSem {\n\t\t\treturn\n\t\t}\n\t\t// Need to use select for the server shutdown case.\n\t\tselect {\n\t\tcase s.syncOutSem <- struct{}{}:\n\t\tdefault:\n\t\t}\n\t\treleaseSem = false\n\t}\n\t// On exit, we will release our semaphore if we acquired it.\n\tdefer releaseSyncOutSem()\n\n\t// Do not let this go on forever.\n\tconst maxRetries = 3\n\tvar numRetries int\n\nRETRY:\n\t// On retry, we need to release the semaphore we got. Call will be no-op\n\t// if releaseSem boolean has not been set to true on successfully getting\n\t// the semaphore.\n\treleaseSyncOutSem()\n\n\tif n.Leaderless() {\n\t\t// Prevent us from spinning if we've installed a snapshot from a leader but there's no leader online.\n\t\t// We wait a bit to check if a leader has come online in the meantime, if so we can continue.\n\t\tvar canContinue bool\n\t\tif numRetries == 0 {\n\t\t\ttime.Sleep(startInterval)\n\t\t\tcanContinue = !n.Leaderless()\n\t\t}\n\t\tif !canContinue {\n\t\t\treturn fmt.Errorf(\"%w for stream '%s > %s'\", errCatchupAbortedNoLeader, mset.account(), mset.name())\n\t\t}\n\t}\n\n\t// If we have a sub clear that here.\n\tif sub != nil {\n\t\ts.sysUnsubscribe(sub)\n\t\tsub = nil\n\t}\n\n\tif !s.isRunning() {\n\t\treturn ErrServerNotRunning\n\t}\n\n\tnumRetries++\n\tif numRetries > maxRetries {\n\t\t// Force a hard reset here.\n\t\treturn errCatchupTooManyRetries\n\t}\n\n\t// Block here if we have too many requests in flight.\n\t<-s.syncOutSem\n\treleaseSem = true\n\n\t// We may have been blocked for a bit, so the reset needs to ensure that we\n\t// consume the already fired timer.\n\tif !notActive.Stop() {\n\t\tselect {\n\t\tcase <-notActive.C:\n\t\tdefault:\n\t\t}\n\t}\n\tnotActive.Reset(startInterval)\n\n\t// Grab sync request again on failures.\n\tif sreq == nil {\n\t\tmset.mu.RLock()\n\t\tvar state StreamState\n\t\tmset.store.FastState(&state)\n\t\tsreq = mset.calculateSyncRequest(&state, snap, index)\n\t\tmset.mu.RUnlock()\n\t\tif sreq == nil {\n\t\t\treturn nil\n\t\t}\n\t}\n\n\t// Used to transfer message from the wire to another Go routine internally.\n\ttype im struct {\n\t\tmsg   []byte\n\t\treply string\n\t}\n\t// This is used to notify the leader that it should stop the runCatchup\n\t// because we are either bailing out or going to retry due to an error.\n\tnotifyLeaderStopCatchup := func(mrec *im, err error) {\n\t\tif mrec.reply == _EMPTY_ {\n\t\t\treturn\n\t\t}\n\t\ts.sendInternalMsgLocked(mrec.reply, _EMPTY_, nil, err.Error())\n\t}\n\n\tmsgsQ := newIPQueue[*im](s, qname)\n\tdefer msgsQ.unregister()\n\n\t// Send our catchup request here.\n\treply := syncReplySubject()\n\tsub, err = s.sysSubscribe(reply, func(_ *subscription, _ *client, _ *Account, _, reply string, msg []byte) {\n\t\t// Make copy since we are using a buffer from the inbound client/route.\n\t\tmsgsQ.push(&im{copyBytes(msg), reply})\n\t})\n\tif err != nil {\n\t\ts.Errorf(\"Could not subscribe to stream catchup: %v\", err)\n\t\tgoto RETRY\n\t}\n\n\t// Send our sync request.\n\tb, _ := json.Marshal(sreq)\n\ts.sendInternalMsgLocked(subject, reply, nil, b)\n\n\t// Remember when we sent this out to avoid loop spins on errors below.\n\treqSendTime := time.Now()\n\n\t// Clear our sync request.\n\tsreq = nil\n\n\t// Run our own select loop here.\n\tfor qch, lch := n.QuitC(), n.LeadChangeC(); ; {\n\t\tselect {\n\t\tcase <-msgsQ.ch:\n\t\t\tnotActive.Reset(activityInterval)\n\n\t\t\tmrecs := msgsQ.pop()\n\t\t\tfor _, mrec := range mrecs {\n\t\t\t\tmsg := mrec.msg\n\t\t\t\t// Check for eof signaling.\n\t\t\t\tif len(msg) == 0 {\n\t\t\t\t\tmsgsQ.recycle(&mrecs)\n\n\t\t\t\t\t// Sanity check that we've received all data expected by the snapshot.\n\t\t\t\t\tmset.mu.RLock()\n\t\t\t\t\tlseq := mset.lseq\n\t\t\t\t\tmset.mu.RUnlock()\n\t\t\t\t\tif lseq >= snap.LastSeq {\n\t\t\t\t\t\treturn nil\n\t\t\t\t\t}\n\n\t\t\t\t\t// Make sure we do not spin and make things worse.\n\t\t\t\t\tconst minRetryWait = 2 * time.Second\n\t\t\t\t\telapsed := time.Since(reqSendTime)\n\t\t\t\t\tif elapsed < minRetryWait {\n\t\t\t\t\t\tselect {\n\t\t\t\t\t\tcase <-s.quitCh:\n\t\t\t\t\t\t\treturn ErrServerNotRunning\n\t\t\t\t\t\tcase <-qch:\n\t\t\t\t\t\t\treturn errCatchupStreamStopped\n\t\t\t\t\t\tcase <-time.After(minRetryWait - elapsed):\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tgoto RETRY\n\t\t\t\t}\n\t\t\t\tif _, err := mset.processCatchupMsg(msg); err == nil {\n\t\t\t\t\tif mrec.reply != _EMPTY_ {\n\t\t\t\t\t\ts.sendInternalMsgLocked(mrec.reply, _EMPTY_, nil, nil)\n\t\t\t\t\t}\n\t\t\t\t} else if isOutOfSpaceErr(err) {\n\t\t\t\t\tnotifyLeaderStopCatchup(mrec, err)\n\t\t\t\t\tmsgsQ.recycle(&mrecs)\n\t\t\t\t\treturn err\n\t\t\t\t} else if err == NewJSInsufficientResourcesError() {\n\t\t\t\t\tnotifyLeaderStopCatchup(mrec, err)\n\t\t\t\t\tif mset.js.limitsExceeded(mset.cfg.Storage) {\n\t\t\t\t\t\ts.resourcesExceededError()\n\t\t\t\t\t} else {\n\t\t\t\t\t\ts.Warnf(\"Catchup for stream '%s > %s' errored, account resources exceeded: %v\", mset.account(), mset.name(), err)\n\t\t\t\t\t}\n\t\t\t\t\tmsgsQ.recycle(&mrecs)\n\t\t\t\t\treturn err\n\t\t\t\t} else {\n\t\t\t\t\tnotifyLeaderStopCatchup(mrec, err)\n\t\t\t\t\ts.Warnf(\"Catchup for stream '%s > %s' errored, will retry: %v\", mset.account(), mset.name(), err)\n\t\t\t\t\tmsgsQ.recycle(&mrecs)\n\n\t\t\t\t\t// Make sure we do not spin and make things worse.\n\t\t\t\t\tconst minRetryWait = 2 * time.Second\n\t\t\t\t\telapsed := time.Since(reqSendTime)\n\t\t\t\t\tif elapsed < minRetryWait {\n\t\t\t\t\t\tselect {\n\t\t\t\t\t\tcase <-s.quitCh:\n\t\t\t\t\t\t\treturn ErrServerNotRunning\n\t\t\t\t\t\tcase <-qch:\n\t\t\t\t\t\t\treturn errCatchupStreamStopped\n\t\t\t\t\t\tcase <-time.After(minRetryWait - elapsed):\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tgoto RETRY\n\t\t\t\t}\n\t\t\t}\n\t\t\tnotActive.Reset(activityInterval)\n\t\t\tmsgsQ.recycle(&mrecs)\n\t\tcase <-notActive.C:\n\t\t\tif mrecs := msgsQ.pop(); len(mrecs) > 0 {\n\t\t\t\tmrec := mrecs[0]\n\t\t\t\tnotifyLeaderStopCatchup(mrec, errCatchupStalled)\n\t\t\t\tmsgsQ.recycle(&mrecs)\n\t\t\t}\n\t\t\ts.Warnf(\"Catchup for stream '%s > %s' stalled\", mset.account(), mset.name())\n\t\t\tgoto RETRY\n\t\tcase <-s.quitCh:\n\t\t\treturn ErrServerNotRunning\n\t\tcase <-qch:\n\t\t\treturn errCatchupStreamStopped\n\t\tcase isLeader := <-lch:\n\t\t\tif isLeader {\n\t\t\t\tn.StepDown()\n\t\t\t\tgoto RETRY\n\t\t\t}\n\t\t}\n\t}\n}\n\n// processCatchupMsg will be called to process out of band catchup msgs from a sync request.\nfunc (mset *stream) processCatchupMsg(msg []byte) (uint64, error) {\n\tif len(msg) == 0 {\n\t\treturn 0, errCatchupBadMsg\n\t}\n\top := entryOp(msg[0])\n\tif op != streamMsgOp && op != compressedStreamMsgOp && op != deleteRangeOp {\n\t\treturn 0, errCatchupBadMsg\n\t}\n\n\tmbuf := msg[1:]\n\tif op == deleteRangeOp {\n\t\tdr, err := decodeDeleteRange(mbuf)\n\t\tif err != nil {\n\t\t\treturn 0, errCatchupBadMsg\n\t\t}\n\t\t// Handle the delete range.\n\t\t// Make sure the sequences match up properly.\n\t\tmset.mu.Lock()\n\t\tif len(mset.preAcks) > 0 {\n\t\t\tfor seq := dr.First; seq < dr.First+dr.Num; seq++ {\n\t\t\t\tmset.clearAllPreAcks(seq)\n\t\t\t}\n\t\t}\n\t\tif err = mset.store.SkipMsgs(dr.First, dr.Num); err != nil {\n\t\t\tmset.mu.Unlock()\n\t\t\treturn 0, errCatchupWrongSeqForSkip\n\t\t}\n\t\tmset.lseq = dr.First + dr.Num - 1\n\t\tlseq := mset.lseq\n\t\tmset.mu.Unlock()\n\t\treturn lseq, nil\n\t}\n\n\tif op == compressedStreamMsgOp {\n\t\tvar err error\n\t\tmbuf, err = s2.Decode(nil, mbuf)\n\t\tif err != nil {\n\t\t\tpanic(err.Error())\n\t\t}\n\t}\n\n\tsubj, _, hdr, msg, seq, ts, _, err := decodeStreamMsg(mbuf)\n\tif err != nil {\n\t\treturn 0, errCatchupBadMsg\n\t}\n\n\tmset.mu.Lock()\n\tst := mset.cfg.Storage\n\tddloaded := mset.ddloaded\n\tif mset.hasAllPreAcks(seq, subj) {\n\t\tmset.clearAllPreAcks(seq)\n\t\t// Mark this to be skipped\n\t\tsubj, ts = _EMPTY_, 0\n\t}\n\tmset.mu.Unlock()\n\n\t// Since we're clustered we do not want to check limits based on tier here and possibly introduce skew.\n\tif mset.js.limitsExceeded(st) {\n\t\treturn 0, NewJSInsufficientResourcesError()\n\t}\n\n\t// Find the message TTL if any.\n\t// TODO(nat): If the TTL isn't valid by this stage then there isn't really a\n\t// lot we can do about it, as we'd break the catchup if we reject the message.\n\tttl, _ := getMessageTTL(hdr)\n\n\t// Put into our store\n\t// Messages to be skipped have no subject or timestamp.\n\t// TODO(dlc) - formalize with skipMsgOp\n\tif subj == _EMPTY_ && ts == 0 {\n\t\tif lseq := mset.store.SkipMsg(); lseq != seq {\n\t\t\treturn 0, errCatchupWrongSeqForSkip\n\t\t}\n\t} else if err := mset.store.StoreRawMsg(subj, hdr, msg, seq, ts, ttl); err != nil {\n\t\treturn 0, err\n\t}\n\n\tmset.mu.Lock()\n\tdefer mset.mu.Unlock()\n\t// Update our lseq.\n\tmset.setLastSeq(seq)\n\n\t// Check for MsgId and if we have one here make sure to update our internal map.\n\tif len(hdr) > 0 {\n\t\tif msgId := getMsgId(hdr); msgId != _EMPTY_ {\n\t\t\tif !ddloaded {\n\t\t\t\tmset.rebuildDedupe()\n\t\t\t}\n\t\t\tmset.storeMsgIdLocked(&ddentry{msgId, seq, ts})\n\t\t}\n\t}\n\n\treturn seq, nil\n}\n\nfunc (mset *stream) handleClusterSyncRequest(sub *subscription, c *client, _ *Account, subject, reply string, msg []byte) {\n\tvar sreq streamSyncRequest\n\tif err := json.Unmarshal(msg, &sreq); err != nil {\n\t\t// Log error.\n\t\treturn\n\t}\n\tmset.srv.startGoRoutine(func() { mset.runCatchup(reply, &sreq) })\n}\n\n// Lock should be held.\nfunc (js *jetStream) offlineClusterInfo(rg *raftGroup) *ClusterInfo {\n\ts := js.srv\n\n\tci := &ClusterInfo{Name: s.ClusterName(), RaftGroup: rg.Name}\n\tfor _, peer := range rg.Peers {\n\t\tif sir, ok := s.nodeToInfo.Load(peer); ok && sir != nil {\n\t\t\tsi := sir.(nodeInfo)\n\t\t\tpi := &PeerInfo{Peer: peer, Name: si.name, Current: false, Offline: true}\n\t\t\tci.Replicas = append(ci.Replicas, pi)\n\t\t}\n\t}\n\treturn ci\n}\n\n// clusterInfo will report on the status of the raft group.\nfunc (js *jetStream) clusterInfo(rg *raftGroup) *ClusterInfo {\n\tif js == nil {\n\t\treturn nil\n\t}\n\tjs.mu.RLock()\n\tdefer js.mu.RUnlock()\n\n\ts := js.srv\n\tif rg == nil || rg.node == nil {\n\t\treturn &ClusterInfo{\n\t\t\tName:   s.cachedClusterName(),\n\t\t\tLeader: s.Name(),\n\t\t}\n\t}\n\n\tn := rg.node\n\tci := &ClusterInfo{\n\t\tName:      s.cachedClusterName(),\n\t\tLeader:    s.serverNameForNode(n.GroupLeader()),\n\t\tRaftGroup: rg.Name,\n\t}\n\n\tnow := time.Now()\n\tid, peers := n.ID(), n.Peers()\n\n\t// If we are leaderless, do not suppress putting us in the peer list.\n\tif ci.Leader == _EMPTY_ {\n\t\tid = _EMPTY_\n\t}\n\n\tfor _, rp := range peers {\n\t\tif rp.ID != id && rg.isMember(rp.ID) {\n\t\t\tvar lastSeen time.Duration\n\t\t\tif now.After(rp.Last) && rp.Last.Unix() != 0 {\n\t\t\t\tlastSeen = now.Sub(rp.Last)\n\t\t\t}\n\t\t\tcurrent := rp.Current\n\t\t\tif current && lastSeen > lostQuorumInterval {\n\t\t\t\tcurrent = false\n\t\t\t}\n\t\t\t// Create a peer info with common settings if the peer has not been seen\n\t\t\t// yet (which can happen after the whole cluster is stopped and only some\n\t\t\t// of the nodes are restarted).\n\t\t\tpi := &PeerInfo{\n\t\t\t\tCurrent: current,\n\t\t\t\tOffline: true,\n\t\t\t\tActive:  lastSeen,\n\t\t\t\tLag:     rp.Lag,\n\t\t\t\tPeer:    rp.ID,\n\t\t\t}\n\t\t\t// If node is found, complete/update the settings.\n\t\t\tif sir, ok := s.nodeToInfo.Load(rp.ID); ok && sir != nil {\n\t\t\t\tsi := sir.(nodeInfo)\n\t\t\t\tpi.Name, pi.Offline, pi.cluster = si.name, si.offline, si.cluster\n\t\t\t} else {\n\t\t\t\t// If not, then add a name that indicates that the server name\n\t\t\t\t// is unknown at this time, and clear the lag since it is misleading\n\t\t\t\t// (the node may not have that much lag).\n\t\t\t\t// Note: We return now the Peer ID in PeerInfo, so the \"(peerID: %s)\"\n\t\t\t\t// would technically not be required, but keeping it for now.\n\t\t\t\tpi.Name, pi.Lag = fmt.Sprintf(\"Server name unknown at this time (peerID: %s)\", rp.ID), 0\n\t\t\t}\n\t\t\tci.Replicas = append(ci.Replicas, pi)\n\t\t}\n\t}\n\t// Order the result based on the name so that we get something consistent\n\t// when doing repeated stream info in the CLI, etc...\n\tslices.SortFunc(ci.Replicas, func(i, j *PeerInfo) int { return cmp.Compare(i.Name, j.Name) })\n\treturn ci\n}\n\nfunc (mset *stream) checkClusterInfo(ci *ClusterInfo) {\n\tfor _, r := range ci.Replicas {\n\t\tpeer := getHash(r.Name)\n\t\tif lag := mset.lagForCatchupPeer(peer); lag > 0 {\n\t\t\tr.Current = false\n\t\t\tr.Lag = lag\n\t\t}\n\t}\n}\n\n// Return a list of alternates, ranked by preference order to the request, of stream mirrors.\n// This allows clients to select or get more information about read replicas that could be a\n// better option to connect to versus the original source.\nfunc (js *jetStream) streamAlternates(ci *ClientInfo, stream string) []StreamAlternate {\n\tif js == nil {\n\t\treturn nil\n\t}\n\n\tjs.mu.RLock()\n\tdefer js.mu.RUnlock()\n\n\ts, cc := js.srv, js.cluster\n\t// Track our domain.\n\tdomain := s.getOpts().JetStreamDomain\n\n\t// No clustering just return nil.\n\tif cc == nil {\n\t\treturn nil\n\t}\n\tacc, _ := s.LookupAccount(ci.serviceAccount())\n\tif acc == nil {\n\t\treturn nil\n\t}\n\n\t// Collect our ordering first for clusters.\n\tweights := make(map[string]int)\n\tall := []string{ci.Cluster}\n\tall = append(all, ci.Alternates...)\n\n\tfor i := 0; i < len(all); i++ {\n\t\tweights[all[i]] = len(all) - i\n\t}\n\n\tvar alts []StreamAlternate\n\tfor _, sa := range cc.streams[acc.Name] {\n\t\t// Add in ourselves and any mirrors.\n\t\tif sa.Config.Name == stream || (sa.Config.Mirror != nil && sa.Config.Mirror.Name == stream) {\n\t\t\talts = append(alts, StreamAlternate{Name: sa.Config.Name, Domain: domain, Cluster: sa.Group.Cluster})\n\t\t}\n\t}\n\t// If just us don't fill in.\n\tif len(alts) == 1 {\n\t\treturn nil\n\t}\n\n\t// Sort based on our weights that originate from the request itself.\n\t// reverse sort\n\tslices.SortFunc(alts, func(i, j StreamAlternate) int { return -cmp.Compare(weights[i.Cluster], weights[j.Cluster]) })\n\n\treturn alts\n}\n\n// Internal request for stream info, this is coming on the wire so do not block here.\nfunc (mset *stream) handleClusterStreamInfoRequest(_ *subscription, c *client, _ *Account, subject, reply string, _ []byte) {\n\tgo mset.processClusterStreamInfoRequest(reply)\n}\n\nfunc (mset *stream) processClusterStreamInfoRequest(reply string) {\n\tmset.mu.RLock()\n\tsysc, js, sa, config := mset.sysc, mset.srv.js.Load(), mset.sa, mset.cfg\n\tisLeader := mset.isLeader()\n\tmset.mu.RUnlock()\n\n\t// By design all members will receive this. Normally we only want the leader answering.\n\t// But if we have stalled and lost quorom all can respond.\n\tif sa != nil && !js.isGroupLeaderless(sa.Group) && !isLeader {\n\t\treturn\n\t}\n\n\t// If we are not the leader let someone else possibly respond first.\n\tif !isLeader {\n\t\ttime.Sleep(500 * time.Millisecond)\n\t}\n\n\tsi := &StreamInfo{\n\t\tCreated:   mset.createdTime(),\n\t\tState:     mset.state(),\n\t\tConfig:    config,\n\t\tCluster:   js.clusterInfo(mset.raftGroup()),\n\t\tSources:   mset.sourcesInfo(),\n\t\tMirror:    mset.mirrorInfo(),\n\t\tTimeStamp: time.Now().UTC(),\n\t}\n\n\t// Check for out of band catchups.\n\tif mset.hasCatchupPeers() {\n\t\tmset.checkClusterInfo(si.Cluster)\n\t}\n\n\tsysc.sendInternalMsg(reply, _EMPTY_, nil, si)\n}\n\n// 64MB for now, for the total server. This is max we will blast out if asked to\n// do so to another server for purposes of catchups.\n// This number should be ok on 1Gbit interface.\nconst defaultMaxTotalCatchupOutBytes = int64(64 * 1024 * 1024)\n\n// Current total outstanding catchup bytes.\nfunc (s *Server) gcbTotal() int64 {\n\ts.gcbMu.RLock()\n\tdefer s.gcbMu.RUnlock()\n\treturn s.gcbOut\n}\n\n// Returns true if Current total outstanding catchup bytes is below\n// the maximum configured.\nfunc (s *Server) gcbBelowMax() bool {\n\ts.gcbMu.RLock()\n\tdefer s.gcbMu.RUnlock()\n\treturn s.gcbOut <= s.gcbOutMax\n}\n\n// Adds `sz` to the server's total outstanding catchup bytes and to `localsz`\n// under the gcbMu lock. The `localsz` points to the local outstanding catchup\n// bytes of the runCatchup go routine of a given stream.\nfunc (s *Server) gcbAdd(localsz *int64, sz int64) {\n\ts.gcbMu.Lock()\n\tatomic.AddInt64(localsz, sz)\n\ts.gcbOut += sz\n\tif s.gcbOut >= s.gcbOutMax && s.gcbKick == nil {\n\t\ts.gcbKick = make(chan struct{})\n\t}\n\ts.gcbMu.Unlock()\n}\n\n// Removes `sz` from the server's total outstanding catchup bytes and from\n// `localsz`, but only if `localsz` is non 0, which would signal that gcSubLast\n// has already been invoked. See that function for details.\n// Must be invoked under the gcbMu lock.\nfunc (s *Server) gcbSubLocked(localsz *int64, sz int64) {\n\tif atomic.LoadInt64(localsz) == 0 {\n\t\treturn\n\t}\n\tatomic.AddInt64(localsz, -sz)\n\ts.gcbOut -= sz\n\tif s.gcbKick != nil && s.gcbOut < s.gcbOutMax {\n\t\tclose(s.gcbKick)\n\t\ts.gcbKick = nil\n\t}\n}\n\n// Locked version of gcbSubLocked()\nfunc (s *Server) gcbSub(localsz *int64, sz int64) {\n\ts.gcbMu.Lock()\n\ts.gcbSubLocked(localsz, sz)\n\ts.gcbMu.Unlock()\n}\n\n// Similar to gcbSub() but reset `localsz` to 0 at the end under the gcbMu lock.\n// This will signal further calls to gcbSub() for this `localsz` pointer that\n// nothing should be done because runCatchup() has exited and any remaining\n// outstanding bytes value has already been decremented.\nfunc (s *Server) gcbSubLast(localsz *int64) {\n\ts.gcbMu.Lock()\n\ts.gcbSubLocked(localsz, *localsz)\n\t*localsz = 0\n\ts.gcbMu.Unlock()\n}\n\n// Returns our kick chan, or nil if it does not exist.\nfunc (s *Server) cbKickChan() <-chan struct{} {\n\ts.gcbMu.RLock()\n\tdefer s.gcbMu.RUnlock()\n\treturn s.gcbKick\n}\n\nfunc (mset *stream) runCatchup(sendSubject string, sreq *streamSyncRequest) {\n\ts := mset.srv\n\tdefer s.grWG.Done()\n\n\tconst maxOutBytes = int64(64 * 1024 * 1024) // 64MB for now, these are all internal, from server to server\n\tconst maxOutMsgs = int32(256 * 1024)        // 256k in case we have lots of small messages or skip msgs.\n\toutb := int64(0)\n\toutm := int32(0)\n\n\t// On abnormal exit make sure to update global total.\n\tdefer s.gcbSubLast(&outb)\n\n\t// Flow control processing.\n\tackReplySize := func(subj string) int64 {\n\t\tif li := strings.LastIndexByte(subj, btsep); li > 0 && li < len(subj) {\n\t\t\treturn parseAckReplyNum(subj[li+1:])\n\t\t}\n\t\treturn 0\n\t}\n\n\tnextBatchC := make(chan struct{}, 4)\n\tnextBatchC <- struct{}{}\n\tremoteQuitCh := make(chan struct{})\n\n\tconst activityInterval = 30 * time.Second\n\tnotActive := time.NewTimer(activityInterval)\n\tdefer notActive.Stop()\n\n\t// Setup ackReply for flow control.\n\tackReply := syncAckSubject()\n\tackSub, _ := s.sysSubscribe(ackReply, func(sub *subscription, c *client, _ *Account, subject, reply string, msg []byte) {\n\t\tif len(msg) > 0 {\n\t\t\ts.Warnf(\"Catchup for stream '%s > %s' was aborted on the remote due to: %q\",\n\t\t\t\tmset.account(), mset.name(), msg)\n\t\t\ts.sysUnsubscribe(sub)\n\t\t\tclose(remoteQuitCh)\n\t\t\treturn\n\t\t}\n\t\tsz := ackReplySize(subject)\n\t\ts.gcbSub(&outb, sz)\n\t\tatomic.AddInt32(&outm, -1)\n\t\tmset.updateCatchupPeer(sreq.Peer)\n\t\t// Kick ourselves and anyone else who might have stalled on global state.\n\t\tselect {\n\t\tcase nextBatchC <- struct{}{}:\n\t\tdefault:\n\t\t}\n\t\t// Reset our activity\n\t\tnotActive.Reset(activityInterval)\n\t})\n\tdefer s.sysUnsubscribe(ackSub)\n\tackReplyT := strings.ReplaceAll(ackReply, \".*\", \".%d\")\n\n\t// Grab our state.\n\tvar state StreamState\n\t// mset.store never changes after being set, don't need lock.\n\tmset.store.FastState(&state)\n\n\t// Setup sequences to walk through.\n\tseq, last := sreq.FirstSeq, sreq.LastSeq\n\n\t// The follower received a snapshot from another leader, and we've become leader since.\n\t// We have an up-to-date log but could be behind on applies. We must wait until we've reached the minimum required.\n\t// The follower will automatically retry after a timeout, so we can safely return here.\n\tif node := mset.raftNode(); node != nil {\n\t\tindex, _, applied := node.Progress()\n\t\t// Only skip if our log has enough entries, and they could be applied in the future.\n\t\tif index >= sreq.MinApplied && applied < sreq.MinApplied {\n\t\t\treturn\n\t\t}\n\t\t// We know here we've either applied enough entries, or our log doesn't have enough entries.\n\t\t// In the latter case the request expects us to have more. Just continue and value availability here.\n\t\t// This should only be possible if the logs have already desynced, and we shouldn't have become leader\n\t\t// in the first place. Not much we can do here in this (hypothetical) scenario.\n\n\t\t// Do another quick sanity check that we actually have enough data to satisfy the request.\n\t\t// If not, let's step down and hope a new leader can correct this.\n\t\tif state.LastSeq < last {\n\t\t\ts.Warnf(\"Catchup for stream '%s > %s' skipped, requested sequence %d was larger than current state: %+v\",\n\t\t\t\tmset.account(), mset.name(), seq, state)\n\t\t\tnode.StepDown()\n\t\t\treturn\n\t\t}\n\t}\n\n\tmset.setCatchupPeer(sreq.Peer, last-seq)\n\n\tvar spb int\n\tconst minWait = 5 * time.Second\n\n\tsendNextBatchAndContinue := func(qch chan struct{}) bool {\n\t\t// Check if we know we will not enter the loop because we are done.\n\t\tif seq > last {\n\t\t\ts.Noticef(\"Catchup for stream '%s > %s' complete\", mset.account(), mset.name())\n\t\t\t// EOF\n\t\t\ts.sendInternalMsgLocked(sendSubject, _EMPTY_, nil, nil)\n\t\t\treturn false\n\t\t}\n\n\t\t// If we already sent a batch, we will try to make sure we can at least send a minimum\n\t\t// batch before sending the next batch.\n\t\tif spb > 0 {\n\t\t\t// Wait til we can send at least 4k\n\t\t\tconst minBatchWait = int32(4 * 1024)\n\t\t\tmw := time.NewTimer(minWait)\n\t\t\tfor done := maxOutMsgs-atomic.LoadInt32(&outm) > minBatchWait; !done; {\n\t\t\t\tselect {\n\t\t\t\tcase <-nextBatchC:\n\t\t\t\t\tdone = maxOutMsgs-atomic.LoadInt32(&outm) > minBatchWait\n\t\t\t\t\tif !done {\n\t\t\t\t\t\t// Wait for a small bit.\n\t\t\t\t\t\ttime.Sleep(100 * time.Millisecond)\n\t\t\t\t\t} else {\n\t\t\t\t\t\t// GC friendly.\n\t\t\t\t\t\tmw.Stop()\n\t\t\t\t\t}\n\t\t\t\tcase <-mw.C:\n\t\t\t\t\tdone = true\n\t\t\t\tcase <-s.quitCh:\n\t\t\t\t\treturn false\n\t\t\t\tcase <-qch:\n\t\t\t\t\treturn false\n\t\t\t\tcase <-remoteQuitCh:\n\t\t\t\t\treturn false\n\t\t\t\t}\n\t\t\t}\n\t\t\tspb = 0\n\t\t}\n\n\t\t// Send an encoded msg.\n\t\tsendEM := func(em []byte) {\n\t\t\t// Place size in reply subject for flow control.\n\t\t\tl := int64(len(em))\n\t\t\treply := fmt.Sprintf(ackReplyT, l)\n\t\t\ts.gcbAdd(&outb, l)\n\t\t\tatomic.AddInt32(&outm, 1)\n\t\t\ts.sendInternalMsgLocked(sendSubject, reply, nil, em)\n\t\t\tspb++\n\t\t}\n\n\t\t// If we support gap markers.\n\t\tvar dr DeleteRange\n\t\tdrOk := sreq.DeleteRangesOk\n\n\t\t// Will send our delete range.\n\t\t// Should already be checked for being valid.\n\t\tsendDR := func() {\n\t\t\tif dr.Num == 1 {\n\t\t\t\t// Send like a normal skip msg.\n\t\t\t\tsendEM(encodeStreamMsg(_EMPTY_, _EMPTY_, nil, nil, dr.First, 0, false))\n\t\t\t} else {\n\t\t\t\t// We have a run, send a gap record. We send these without reply or tracking.\n\t\t\t\ts.sendInternalMsgLocked(sendSubject, _EMPTY_, nil, encodeDeleteRange(&dr))\n\t\t\t\t// Clear out the pending for catchup.\n\t\t\t\tmset.decrementCatchupPeer(sreq.Peer, dr.Num)\n\t\t\t}\n\t\t\t// Reset always.\n\t\t\tdr.First, dr.Num = 0, 0\n\t\t}\n\n\t\t// See if we should use LoadNextMsg instead of walking sequence by sequence if we have an order magnitude more interior deletes.\n\t\t// Only makes sense with delete range capabilities.\n\t\tuseLoadNext := drOk && (uint64(state.NumDeleted) > 10*state.Msgs)\n\n\t\tvar smv StoreMsg\n\t\tfor ; seq <= last && atomic.LoadInt64(&outb) <= maxOutBytes && atomic.LoadInt32(&outm) <= maxOutMsgs && s.gcbBelowMax(); seq++ {\n\t\t\tvar sm *StoreMsg\n\t\t\tvar err error\n\t\t\t// If we should use load next do so here.\n\t\t\tif useLoadNext {\n\t\t\t\tvar nseq uint64\n\t\t\t\tsm, nseq, err = mset.store.LoadNextMsg(fwcs, true, seq, &smv)\n\t\t\t\tif err == nil && nseq > seq {\n\t\t\t\t\t// If we jumped over the requested last sequence, clamp it down.\n\t\t\t\t\t// Otherwise, we would send too much to the follower.\n\t\t\t\t\tif nseq > last {\n\t\t\t\t\t\tnseq = last\n\t\t\t\t\t\tsm = nil\n\t\t\t\t\t}\n\t\t\t\t\tdr.First, dr.Num = seq, nseq-seq\n\t\t\t\t\t// Jump ahead\n\t\t\t\t\tseq = nseq\n\t\t\t\t} else if err == ErrStoreEOF {\n\t\t\t\t\tdr.First, dr.Num = seq, last-seq\n\t\t\t\t\t// Clear EOF here for normal processing.\n\t\t\t\t\terr = nil\n\t\t\t\t\t// Jump ahead\n\t\t\t\t\tseq = last\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tsm, err = mset.store.LoadMsg(seq, &smv)\n\t\t\t}\n\n\t\t\t// if this is not a deleted msg, bail out.\n\t\t\tif err != nil && err != ErrStoreMsgNotFound && err != errDeletedMsg {\n\t\t\t\tif err == ErrStoreEOF {\n\t\t\t\t\tvar state StreamState\n\t\t\t\t\tmset.store.FastState(&state)\n\t\t\t\t\tif seq > state.LastSeq {\n\t\t\t\t\t\t// The snapshot has a larger last sequence then we have. This could be due to a truncation\n\t\t\t\t\t\t// when trying to recover after corruption, still not 100% sure. Could be off by 1 too somehow,\n\t\t\t\t\t\t// but tested a ton of those with no success.\n\t\t\t\t\t\ts.Warnf(\"Catchup for stream '%s > %s' completed, but requested sequence %d was larger than current state: %+v\",\n\t\t\t\t\t\t\tmset.account(), mset.name(), seq, state)\n\t\t\t\t\t\t// Try our best to redo our invalidated snapshot as well.\n\t\t\t\t\t\tif n := mset.raftNode(); n != nil {\n\t\t\t\t\t\t\tif snap := mset.stateSnapshot(); snap != nil {\n\t\t\t\t\t\t\t\tn.InstallSnapshot(snap)\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t\t// If we allow gap markers check if we have one pending.\n\t\t\t\t\t\tif drOk && dr.First > 0 {\n\t\t\t\t\t\t\tsendDR()\n\t\t\t\t\t\t}\n\t\t\t\t\t\t// Signal EOF\n\t\t\t\t\t\ts.sendInternalMsgLocked(sendSubject, _EMPTY_, nil, nil)\n\t\t\t\t\t\treturn false\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\ts.Warnf(\"Error loading message for catchup '%s > %s': %v\", mset.account(), mset.name(), err)\n\t\t\t\treturn false\n\t\t\t}\n\n\t\t\tif sm != nil {\n\t\t\t\t// If we allow gap markers check if we have one pending.\n\t\t\t\tif drOk && dr.First > 0 {\n\t\t\t\t\tsendDR()\n\t\t\t\t}\n\t\t\t\t// Send the normal message now.\n\t\t\t\tsendEM(encodeStreamMsgAllowCompress(sm.subj, _EMPTY_, sm.hdr, sm.msg, sm.seq, sm.ts, false))\n\t\t\t} else {\n\t\t\t\tif drOk {\n\t\t\t\t\tif dr.First == 0 {\n\t\t\t\t\t\tdr.First, dr.Num = seq, 1\n\t\t\t\t\t} else {\n\t\t\t\t\t\tdr.Num++\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\t// Skip record for deleted msg.\n\t\t\t\t\tsendEM(encodeStreamMsg(_EMPTY_, _EMPTY_, nil, nil, seq, 0, false))\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// Check if we are done.\n\t\t\tif seq == last {\n\t\t\t\t// Need to see if we have a pending delete range.\n\t\t\t\tif drOk && dr.First > 0 {\n\t\t\t\t\tsendDR()\n\t\t\t\t}\n\t\t\t\ts.Noticef(\"Catchup for stream '%s > %s' complete\", mset.account(), mset.name())\n\t\t\t\t// EOF\n\t\t\t\ts.sendInternalMsgLocked(sendSubject, _EMPTY_, nil, nil)\n\t\t\t\treturn false\n\t\t\t}\n\t\t\tselect {\n\t\t\tcase <-remoteQuitCh:\n\t\t\t\treturn false\n\t\t\tdefault:\n\t\t\t}\n\t\t}\n\t\tif drOk && dr.First > 0 {\n\t\t\tsendDR()\n\t\t}\n\t\treturn true\n\t}\n\n\t// Check is this stream got closed.\n\tmset.mu.RLock()\n\tqch := mset.qch\n\tmset.mu.RUnlock()\n\tif qch == nil {\n\t\treturn\n\t}\n\n\t// Run as long as we are still active and need catchup.\n\t// FIXME(dlc) - Purge event? Stream delete?\n\tfor {\n\t\t// Get this each time, will be non-nil if globally blocked and we will close to wake everyone up.\n\t\tcbKick := s.cbKickChan()\n\n\t\tselect {\n\t\tcase <-s.quitCh:\n\t\t\treturn\n\t\tcase <-qch:\n\t\t\treturn\n\t\tcase <-remoteQuitCh:\n\t\t\tmset.clearCatchupPeer(sreq.Peer)\n\t\t\treturn\n\t\tcase <-notActive.C:\n\t\t\ts.Warnf(\"Catchup for stream '%s > %s' stalled\", mset.account(), mset.name())\n\t\t\tmset.clearCatchupPeer(sreq.Peer)\n\t\t\treturn\n\t\tcase <-nextBatchC:\n\t\t\tif !sendNextBatchAndContinue(qch) {\n\t\t\t\tmset.clearCatchupPeer(sreq.Peer)\n\t\t\t\treturn\n\t\t\t}\n\t\tcase <-cbKick:\n\t\t\tif !sendNextBatchAndContinue(qch) {\n\t\t\t\tmset.clearCatchupPeer(sreq.Peer)\n\t\t\t\treturn\n\t\t\t}\n\t\tcase <-time.After(500 * time.Millisecond):\n\t\t\tif !sendNextBatchAndContinue(qch) {\n\t\t\t\tmset.clearCatchupPeer(sreq.Peer)\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}\n}\n\nconst jscAllSubj = \"$JSC.>\"\n\nfunc syncSubjForStream() string {\n\treturn syncSubject(\"$JSC.SYNC\")\n}\n\nfunc syncReplySubject() string {\n\treturn syncSubject(\"$JSC.R\")\n}\n\nfunc infoReplySubject() string {\n\treturn syncSubject(\"$JSC.R\")\n}\n\nfunc syncAckSubject() string {\n\treturn syncSubject(\"$JSC.ACK\") + \".*\"\n}\n\nfunc syncSubject(pre string) string {\n\tvar sb strings.Builder\n\tsb.WriteString(pre)\n\tsb.WriteByte(btsep)\n\n\tvar b [replySuffixLen]byte\n\trn := rand.Int63()\n\tfor i, l := 0, rn; i < len(b); i++ {\n\t\tb[i] = digits[l%base]\n\t\tl /= base\n\t}\n\n\tsb.Write(b[:])\n\treturn sb.String()\n}\n\nconst (\n\tclusterStreamInfoT   = \"$JSC.SI.%s.%s\"\n\tclusterConsumerInfoT = \"$JSC.CI.%s.%s.%s\"\n\tjsaUpdatesSubT       = \"$JSC.ARU.%s.*\"\n\tjsaUpdatesPubT       = \"$JSC.ARU.%s.%s\"\n)\n",
    "source_file": "server/jetstream_cluster.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2018-2025 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage server\n\nimport (\n\t\"bytes\"\n\t\"cmp\"\n\t\"encoding/hex\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"io/fs\"\n\t\"math\"\n\t\"math/rand\"\n\t\"net/http\"\n\t\"net/textproto\"\n\t\"reflect\"\n\t\"slices\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t\"github.com/nats-io/jwt/v2\"\n\t\"github.com/nats-io/nats-server/v2/internal/fastrand\"\n\t\"github.com/nats-io/nkeys\"\n\t\"github.com/nats-io/nuid\"\n)\n\n// For backwards compatibility with NATS < 2.0, users who are not explicitly defined into an\n// account will be grouped in the default global account.\nconst globalAccountName = DEFAULT_GLOBAL_ACCOUNT\n\nconst defaultMaxSubLimitReportThreshold = int64(2 * time.Second)\n\nvar maxSubLimitReportThreshold = defaultMaxSubLimitReportThreshold\n\n// Account are subject namespace definitions. By default no messages are shared between accounts.\n// You can share via Exports and Imports of Streams and Services.\ntype Account struct {\n\t// Total stats for the account.\n\tstats struct {\n\t\tsync.Mutex\n\t\tstats       // Totals\n\t\tgw    stats // Gateways\n\t\trt    stats // Routes\n\t\tln    stats // Leafnodes\n\t}\n\n\tgwReplyMapping\n\tName         string\n\tNkey         string\n\tIssuer       string\n\tclaimJWT     string\n\tupdated      time.Time\n\tmu           sync.RWMutex\n\tsqmu         sync.Mutex\n\tsl           *Sublist\n\tic           *client\n\tsq           *sendq\n\tisid         uint64\n\tetmr         *time.Timer\n\tctmr         *time.Timer\n\tstrack       map[string]sconns\n\tnrclients    int32\n\tsysclients   int32\n\tnleafs       int32\n\tnrleafs      int32\n\tclients      map[*client]struct{}\n\trm           map[string]int32\n\tlqws         map[string]int32\n\tusersRevoked map[string]int64\n\tmappings     []*mapping\n\thasMapped    atomic.Bool\n\tlmu          sync.RWMutex\n\tlleafs       []*client\n\tleafClusters map[string]uint64\n\timports      importMap\n\texports      exportMap\n\tjs           *jsAccount\n\tjsLimits     map[string]JetStreamAccountLimits\n\tnrgAccount   string\n\tlimits\n\texpired      atomic.Bool\n\tincomplete   bool\n\tsigningKeys  map[string]jwt.Scope\n\textAuth      *jwt.ExternalAuthorization\n\tsrv          *Server // server this account is registered with (possibly nil)\n\tlds          string  // loop detection subject for leaf nodes\n\tsiReply      []byte  // service reply prefix, will form wildcard subscription.\n\teventIds     *nuid.NUID\n\teventIdsMu   sync.Mutex\n\tdefaultPerms *Permissions\n\ttags         jwt.TagList\n\tnameTag      string\n\tlastLimErr   int64\n\troutePoolIdx int\n\t// If the trace destination is specified and a message with a traceParentHdr\n\t// is received, and has the least significant bit of the last token set to 1,\n\t// then if traceDestSampling is > 0 and < 100, a random value will be selected\n\t// and if it falls between 0 and that value, message tracing will be triggered.\n\ttraceDest         string\n\ttraceDestSampling int\n\t// Guarantee that only one goroutine can be running either checkJetStreamMigrate\n\t// or clearObserverState at a given time for this account to prevent interleaving.\n\tjscmMu sync.Mutex\n}\n\nconst (\n\taccDedicatedRoute                = -1\n\taccTransitioningToDedicatedRoute = -2\n)\n\n// Account based limits.\ntype limits struct {\n\tmpay           int32\n\tmsubs          int32\n\tmconns         int32\n\tmleafs         int32\n\tdisallowBearer bool\n}\n\n// Used to track remote clients and leafnodes per remote server.\ntype sconns struct {\n\tconns int32\n\tleafs int32\n}\n\n// Import stream mapping struct\ntype streamImport struct {\n\tacc     *Account\n\tfrom    string\n\tto      string\n\ttr      *subjectTransform\n\trtr     *subjectTransform\n\tclaim   *jwt.Import\n\tusePub  bool\n\tinvalid bool\n\t// This is `allow_trace` and when true and message tracing is happening,\n\t// we will trace egresses past the account boundary, if `false`, we stop\n\t// at the account boundary.\n\tatrc bool\n}\n\nconst ClientInfoHdr = \"Nats-Request-Info\"\n\n// Import service mapping struct\ntype serviceImport struct {\n\tacc         *Account\n\tclaim       *jwt.Import\n\tse          *serviceExport\n\tsid         []byte\n\tfrom        string\n\tto          string\n\ttr          *subjectTransform\n\tts          int64\n\trt          ServiceRespType\n\tlatency     *serviceLatency\n\tm1          *ServiceLatency\n\trc          *client\n\tusePub      bool\n\tresponse    bool\n\tinvalid     bool\n\tshare       bool\n\ttracking    bool\n\tdidDeliver  bool\n\tatrc        bool        // allow trace (got from service export)\n\ttrackingHdr http.Header // header from request\n}\n\n// This is used to record when we create a mapping for implicit service\n// imports. We use this to clean up entries that are not singletons when\n// we detect that interest is no longer present. The key to the map will\n// be the actual interest. We record the mapped subject and the account.\ntype serviceRespEntry struct {\n\tacc  *Account\n\tmsub string\n}\n\n// ServiceRespType represents the types of service request response types.\ntype ServiceRespType uint8\n\n// Service response types. Defaults to a singleton.\nconst (\n\tSingleton ServiceRespType = iota\n\tStreamed\n\tChunked\n)\n\n// String helper.\nfunc (rt ServiceRespType) String() string {\n\tswitch rt {\n\tcase Singleton:\n\t\treturn \"Singleton\"\n\tcase Streamed:\n\t\treturn \"Streamed\"\n\tcase Chunked:\n\t\treturn \"Chunked\"\n\t}\n\treturn \"Unknown ServiceResType\"\n}\n\n// exportAuth holds configured approvals or boolean indicating an\n// auth token is required for import.\ntype exportAuth struct {\n\ttokenReq    bool\n\taccountPos  uint\n\tapproved    map[string]*Account\n\tactsRevoked map[string]int64\n}\n\n// streamExport\ntype streamExport struct {\n\texportAuth\n}\n\n// serviceExport holds additional information for exported services.\ntype serviceExport struct {\n\texportAuth\n\tacc        *Account\n\trespType   ServiceRespType\n\tlatency    *serviceLatency\n\trtmr       *time.Timer\n\trespThresh time.Duration\n\t// This is `allow_trace` and when true and message tracing is happening,\n\t// when processing a service import we will go through account boundary\n\t// and trace egresses on that other account. If `false`, we stop at the\n\t// account boundary.\n\tatrc bool\n}\n\n// Used to track service latency.\ntype serviceLatency struct {\n\tsampling int8 // percentage from 1-100 or 0 to indicate triggered by header\n\tsubject  string\n}\n\n// exportMap tracks the exported streams and services.\ntype exportMap struct {\n\tstreams   map[string]*streamExport\n\tservices  map[string]*serviceExport\n\tresponses map[string]*serviceImport\n}\n\n// importMap tracks the imported streams and services.\n// For services we will also track the response mappings as well.\ntype importMap struct {\n\tstreams  []*streamImport\n\tservices map[string][]*serviceImport\n\trrMap    map[string][]*serviceRespEntry\n}\n\n// NewAccount creates a new unlimited account with the given name.\nfunc NewAccount(name string) *Account {\n\ta := &Account{\n\t\tName:     name,\n\t\tlimits:   limits{-1, -1, -1, -1, false},\n\t\teventIds: nuid.New(),\n\t}\n\treturn a\n}\n\nfunc (a *Account) String() string {\n\treturn a.Name\n}\n\nfunc (a *Account) setTraceDest(dest string) {\n\ta.mu.Lock()\n\ta.traceDest = dest\n\ta.mu.Unlock()\n}\n\nfunc (a *Account) getTraceDestAndSampling() (string, int) {\n\ta.mu.RLock()\n\tdest := a.traceDest\n\tsampling := a.traceDestSampling\n\ta.mu.RUnlock()\n\treturn dest, sampling\n}\n\n// Used to create shallow copies of accounts for transfer\n// from opts to real accounts in server struct.\n// Account `na` write lock is expected to be held on entry\n// while account `a` is the one from the Options struct\n// being loaded/reloaded and do not need locking.\nfunc (a *Account) shallowCopy(na *Account) {\n\tna.Nkey = a.Nkey\n\tna.Issuer = a.Issuer\n\tna.traceDest, na.traceDestSampling = a.traceDest, a.traceDestSampling\n\n\tif a.imports.streams != nil {\n\t\tna.imports.streams = make([]*streamImport, 0, len(a.imports.streams))\n\t\tfor _, v := range a.imports.streams {\n\t\t\tsi := *v\n\t\t\tna.imports.streams = append(na.imports.streams, &si)\n\t\t}\n\t}\n\tif a.imports.services != nil {\n\t\tna.imports.services = make(map[string][]*serviceImport)\n\t\tfor k, v := range a.imports.services {\n\t\t\tsis := make([]*serviceImport, 0, len(v))\n\t\t\tfor _, si := range v {\n\t\t\t\tcsi := *si\n\t\t\t\tsis = append(sis, &csi)\n\t\t\t}\n\t\t\tna.imports.services[k] = sis\n\t\t}\n\t}\n\tif a.exports.streams != nil {\n\t\tna.exports.streams = make(map[string]*streamExport)\n\t\tfor k, v := range a.exports.streams {\n\t\t\tif v != nil {\n\t\t\t\tse := *v\n\t\t\t\tna.exports.streams[k] = &se\n\t\t\t} else {\n\t\t\t\tna.exports.streams[k] = nil\n\t\t\t}\n\t\t}\n\t}\n\tif a.exports.services != nil {\n\t\tna.exports.services = make(map[string]*serviceExport)\n\t\tfor k, v := range a.exports.services {\n\t\t\tif v != nil {\n\t\t\t\tse := *v\n\t\t\t\tna.exports.services[k] = &se\n\t\t\t} else {\n\t\t\t\tna.exports.services[k] = nil\n\t\t\t}\n\t\t}\n\t}\n\tna.mappings = a.mappings\n\tna.hasMapped.Store(len(na.mappings) > 0)\n\n\t// JetStream\n\tna.jsLimits = a.jsLimits\n\t// Server config account limits.\n\tna.limits = a.limits\n}\n\n// nextEventID uses its own lock for better concurrency.\nfunc (a *Account) nextEventID() string {\n\ta.eventIdsMu.Lock()\n\tid := a.eventIds.Next()\n\ta.eventIdsMu.Unlock()\n\treturn id\n}\n\n// Returns a slice of clients stored in the account, or nil if none is present.\n// Lock is held on entry.\nfunc (a *Account) getClientsLocked() []*client {\n\tif len(a.clients) == 0 {\n\t\treturn nil\n\t}\n\tclients := make([]*client, 0, len(a.clients))\n\tfor c := range a.clients {\n\t\tclients = append(clients, c)\n\t}\n\treturn clients\n}\n\n// Returns a slice of clients stored in the account, or nil if none is present.\nfunc (a *Account) getClients() []*client {\n\ta.mu.RLock()\n\tclients := a.getClientsLocked()\n\ta.mu.RUnlock()\n\treturn clients\n}\n\n// Called to track a remote server and connections and leafnodes it\n// has for this account.\nfunc (a *Account) updateRemoteServer(m *AccountNumConns) []*client {\n\ta.mu.Lock()\n\tif a.strack == nil {\n\t\ta.strack = make(map[string]sconns)\n\t}\n\t// This does not depend on receiving all updates since each one is idempotent.\n\t// FIXME(dlc) - We should cleanup when these both go to zero.\n\tprev := a.strack[m.Server.ID]\n\ta.strack[m.Server.ID] = sconns{conns: int32(m.Conns), leafs: int32(m.LeafNodes)}\n\ta.nrclients += int32(m.Conns) - prev.conns\n\ta.nrleafs += int32(m.LeafNodes) - prev.leafs\n\n\tmtce := a.mconns != jwt.NoLimit && (len(a.clients)-int(a.sysclients)+int(a.nrclients) > int(a.mconns))\n\t// If we are over here some have snuck in and we need to rebalance.\n\t// All others will probably be doing the same thing but better to be\n\t// conservative and bit harsh here. Clients will reconnect if we over compensate.\n\tvar clients []*client\n\tif mtce {\n\t\tclients = a.getClientsLocked()\n\t\tslices.SortFunc(clients, func(i, j *client) int { return -i.start.Compare(j.start) }) // reserve\n\t\tover := (len(a.clients) - int(a.sysclients) + int(a.nrclients)) - int(a.mconns)\n\t\tif over < len(clients) {\n\t\t\tclients = clients[:over]\n\t\t}\n\t}\n\t// Now check leafnodes.\n\tmtlce := a.mleafs != jwt.NoLimit && (a.nleafs+a.nrleafs > a.mleafs)\n\tif mtlce {\n\t\t// Take ones from the end.\n\t\ta.lmu.RLock()\n\t\tleafs := a.lleafs\n\t\tover := int(a.nleafs + a.nrleafs - a.mleafs)\n\t\tif over < len(leafs) {\n\t\t\tleafs = leafs[len(leafs)-over:]\n\t\t}\n\t\tclients = append(clients, leafs...)\n\t\ta.lmu.RUnlock()\n\t}\n\ta.mu.Unlock()\n\n\t// If we have exceeded our max clients this will be populated.\n\treturn clients\n}\n\n// Removes tracking for a remote server that has shutdown.\nfunc (a *Account) removeRemoteServer(sid string) {\n\ta.mu.Lock()\n\tif a.strack != nil {\n\t\tprev := a.strack[sid]\n\t\tdelete(a.strack, sid)\n\t\ta.nrclients -= prev.conns\n\t\ta.nrleafs -= prev.leafs\n\t}\n\ta.mu.Unlock()\n}\n\n// When querying for subject interest this is the number of\n// expected responses. We need to actually check that the entry\n// has active connections.\nfunc (a *Account) expectedRemoteResponses() (expected int32) {\n\ta.mu.RLock()\n\tfor _, sc := range a.strack {\n\t\tif sc.conns > 0 || sc.leafs > 0 {\n\t\t\texpected++\n\t\t}\n\t}\n\ta.mu.RUnlock()\n\treturn\n}\n\n// Clears eventing and tracking for this account.\nfunc (a *Account) clearEventing() {\n\ta.mu.Lock()\n\ta.nrclients = 0\n\t// Now clear state\n\tclearTimer(&a.etmr)\n\tclearTimer(&a.ctmr)\n\ta.clients = nil\n\ta.strack = nil\n\ta.mu.Unlock()\n}\n\n// GetName will return the accounts name.\nfunc (a *Account) GetName() string {\n\tif a == nil {\n\t\treturn \"n/a\"\n\t}\n\ta.mu.RLock()\n\tname := a.Name\n\ta.mu.RUnlock()\n\treturn name\n}\n\n// getNameTag will return the name tag or the account name if not set.\nfunc (a *Account) getNameTag() string {\n\tif a == nil {\n\t\treturn _EMPTY_\n\t}\n\ta.mu.RLock()\n\tdefer a.mu.RUnlock()\n\treturn a.getNameTagLocked()\n}\n\n// getNameTagLocked will return the name tag or the account name if not set.\n// Lock should be held.\nfunc (a *Account) getNameTagLocked() string {\n\tif a == nil {\n\t\treturn _EMPTY_\n\t}\n\tnameTag := a.nameTag\n\tif nameTag == _EMPTY_ {\n\t\tnameTag = a.Name\n\t}\n\treturn nameTag\n}\n\n// NumConnections returns active number of clients for this account for\n// all known servers.\nfunc (a *Account) NumConnections() int {\n\ta.mu.RLock()\n\tnc := len(a.clients) - int(a.sysclients) + int(a.nrclients)\n\ta.mu.RUnlock()\n\treturn nc\n}\n\n// NumRemoteConnections returns the number of client or leaf connections that\n// are not on this server.\nfunc (a *Account) NumRemoteConnections() int {\n\ta.mu.RLock()\n\tnc := int(a.nrclients + a.nrleafs)\n\ta.mu.RUnlock()\n\treturn nc\n}\n\n// NumLocalConnections returns active number of clients for this account\n// on this server.\nfunc (a *Account) NumLocalConnections() int {\n\ta.mu.RLock()\n\tnlc := a.numLocalConnections()\n\ta.mu.RUnlock()\n\treturn nlc\n}\n\n// Do not account for the system accounts.\nfunc (a *Account) numLocalConnections() int {\n\treturn len(a.clients) - int(a.sysclients) - int(a.nleafs)\n}\n\n// This is for extended local interest.\n// Lock should not be held.\nfunc (a *Account) numLocalAndLeafConnections() int {\n\ta.mu.RLock()\n\tnlc := len(a.clients) - int(a.sysclients)\n\ta.mu.RUnlock()\n\treturn nlc\n}\n\nfunc (a *Account) numLocalLeafNodes() int {\n\treturn int(a.nleafs)\n}\n\n// MaxTotalConnectionsReached returns if we have reached our limit for number of connections.\nfunc (a *Account) MaxTotalConnectionsReached() bool {\n\tvar mtce bool\n\ta.mu.RLock()\n\tif a.mconns != jwt.NoLimit {\n\t\tmtce = len(a.clients)-int(a.sysclients)+int(a.nrclients) >= int(a.mconns)\n\t}\n\ta.mu.RUnlock()\n\treturn mtce\n}\n\n// MaxActiveConnections return the set limit for the account system\n// wide for total number of active connections.\nfunc (a *Account) MaxActiveConnections() int {\n\ta.mu.RLock()\n\tmconns := int(a.mconns)\n\ta.mu.RUnlock()\n\treturn mconns\n}\n\n// MaxTotalLeafNodesReached returns if we have reached our limit for number of leafnodes.\nfunc (a *Account) MaxTotalLeafNodesReached() bool {\n\ta.mu.RLock()\n\tmtc := a.maxTotalLeafNodesReached()\n\ta.mu.RUnlock()\n\treturn mtc\n}\n\nfunc (a *Account) maxTotalLeafNodesReached() bool {\n\tif a.mleafs != jwt.NoLimit {\n\t\treturn a.nleafs+a.nrleafs >= a.mleafs\n\t}\n\treturn false\n}\n\n// NumLeafNodes returns the active number of local and remote\n// leaf node connections.\nfunc (a *Account) NumLeafNodes() int {\n\ta.mu.RLock()\n\tnln := int(a.nleafs + a.nrleafs)\n\ta.mu.RUnlock()\n\treturn nln\n}\n\n// NumRemoteLeafNodes returns the active number of remote\n// leaf node connections.\nfunc (a *Account) NumRemoteLeafNodes() int {\n\ta.mu.RLock()\n\tnrn := int(a.nrleafs)\n\ta.mu.RUnlock()\n\treturn nrn\n}\n\n// MaxActiveLeafNodes return the set limit for the account system\n// wide for total number of leavenode connections.\n// NOTE: these are tracked separately.\nfunc (a *Account) MaxActiveLeafNodes() int {\n\ta.mu.RLock()\n\tmleafs := int(a.mleafs)\n\ta.mu.RUnlock()\n\treturn mleafs\n}\n\n// RoutedSubs returns how many subjects we would send across a route when first\n// connected or expressing interest. Local client subs.\nfunc (a *Account) RoutedSubs() int {\n\ta.mu.RLock()\n\tdefer a.mu.RUnlock()\n\treturn len(a.rm)\n}\n\n// TotalSubs returns total number of Subscriptions for this account.\nfunc (a *Account) TotalSubs() int {\n\ta.mu.RLock()\n\tdefer a.mu.RUnlock()\n\tif a.sl == nil {\n\t\treturn 0\n\t}\n\treturn int(a.sl.Count())\n}\n\nfunc (a *Account) shouldLogMaxSubErr() bool {\n\tif a == nil {\n\t\treturn true\n\t}\n\ta.mu.RLock()\n\tlast := a.lastLimErr\n\ta.mu.RUnlock()\n\tif now := time.Now().UnixNano(); now-last >= maxSubLimitReportThreshold {\n\t\ta.mu.Lock()\n\t\ta.lastLimErr = now\n\t\ta.mu.Unlock()\n\t\treturn true\n\t}\n\treturn false\n}\n\n// MapDest is for mapping published subjects for clients.\ntype MapDest struct {\n\tSubject string `json:\"subject\"`\n\tWeight  uint8  `json:\"weight\"`\n\tCluster string `json:\"cluster,omitempty\"`\n}\n\nfunc NewMapDest(subject string, weight uint8) *MapDest {\n\treturn &MapDest{subject, weight, _EMPTY_}\n}\n\n// destination is for internal representation for a weighted mapped destination.\ntype destination struct {\n\ttr     *subjectTransform\n\tweight uint8\n}\n\n// mapping is an internal entry for mapping subjects.\ntype mapping struct {\n\tsrc    string\n\twc     bool\n\tdests  []*destination\n\tcdests map[string][]*destination\n}\n\n// AddMapping adds in a simple route mapping from src subject to dest subject\n// for inbound client messages.\nfunc (a *Account) AddMapping(src, dest string) error {\n\treturn a.AddWeightedMappings(src, NewMapDest(dest, 100))\n}\n\n// AddWeightedMappings will add in a weighted mappings for the destinations.\nfunc (a *Account) AddWeightedMappings(src string, dests ...*MapDest) error {\n\ta.mu.Lock()\n\tdefer a.mu.Unlock()\n\n\tif !IsValidSubject(src) {\n\t\treturn ErrBadSubject\n\t}\n\n\tm := &mapping{src: src, wc: subjectHasWildcard(src), dests: make([]*destination, 0, len(dests)+1)}\n\tseen := make(map[string]struct{})\n\n\tvar tw = make(map[string]uint8)\n\tfor _, d := range dests {\n\t\tif _, ok := seen[d.Subject]; ok {\n\t\t\treturn fmt.Errorf(\"duplicate entry for %q\", d.Subject)\n\t\t}\n\t\tseen[d.Subject] = struct{}{}\n\t\tif d.Weight > 100 {\n\t\t\treturn fmt.Errorf(\"individual weights need to be <= 100\")\n\t\t}\n\t\ttw[d.Cluster] += d.Weight\n\t\tif tw[d.Cluster] > 100 {\n\t\t\treturn fmt.Errorf(\"total weight needs to be <= 100\")\n\t\t}\n\t\terr := ValidateMapping(src, d.Subject)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\ttr, err := NewSubjectTransform(src, d.Subject)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif d.Cluster == _EMPTY_ {\n\t\t\tm.dests = append(m.dests, &destination{tr, d.Weight})\n\t\t} else {\n\t\t\t// We have a cluster scoped filter.\n\t\t\tif m.cdests == nil {\n\t\t\t\tm.cdests = make(map[string][]*destination)\n\t\t\t}\n\t\t\tad := m.cdests[d.Cluster]\n\t\t\tad = append(ad, &destination{tr, d.Weight})\n\t\t\tm.cdests[d.Cluster] = ad\n\t\t}\n\t}\n\n\tprocessDestinations := func(dests []*destination) ([]*destination, error) {\n\t\tvar ltw uint8\n\t\tfor _, d := range dests {\n\t\t\tltw += d.weight\n\t\t}\n\t\t// Auto add in original at weight difference if all entries weight does not total to 100.\n\t\t// Iff the src was not already added in explicitly, meaning they want loss.\n\t\t_, haveSrc := seen[src]\n\t\tif ltw != 100 && !haveSrc {\n\t\t\tdest := src\n\t\t\tif m.wc {\n\t\t\t\t// We need to make the appropriate markers for the wildcards etc.\n\t\t\t\tdest = transformTokenize(dest)\n\t\t\t}\n\t\t\ttr, err := NewSubjectTransform(src, dest)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\taw := 100 - ltw\n\t\t\tif len(dests) == 0 {\n\t\t\t\taw = 100\n\t\t\t}\n\t\t\tdests = append(dests, &destination{tr, aw})\n\t\t}\n\t\tslices.SortFunc(dests, func(i, j *destination) int { return cmp.Compare(i.weight, j.weight) })\n\n\t\tvar lw uint8\n\t\tfor _, d := range dests {\n\t\t\td.weight += lw\n\t\t\tlw = d.weight\n\t\t}\n\t\treturn dests, nil\n\t}\n\n\tvar err error\n\tif m.dests, err = processDestinations(m.dests); err != nil {\n\t\treturn err\n\t}\n\n\t// Option cluster scoped destinations\n\tfor cluster, dests := range m.cdests {\n\t\tif dests, err = processDestinations(dests); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tm.cdests[cluster] = dests\n\t}\n\n\t// Replace an old one if it exists.\n\tfor i, em := range a.mappings {\n\t\tif em.src == src {\n\t\t\ta.mappings[i] = m\n\t\t\treturn nil\n\t\t}\n\t}\n\t// If we did not replace add to the end.\n\ta.mappings = append(a.mappings, m)\n\ta.hasMapped.Store(len(a.mappings) > 0)\n\n\t// If we have connected leafnodes make sure to update.\n\tif a.nleafs > 0 {\n\t\t// Need to release because lock ordering is client -> account\n\t\ta.mu.Unlock()\n\t\t// Now grab the leaf list lock. We can hold client lock under this one.\n\t\ta.lmu.RLock()\n\t\tfor _, lc := range a.lleafs {\n\t\t\tlc.forceAddToSmap(src)\n\t\t}\n\t\ta.lmu.RUnlock()\n\t\ta.mu.Lock()\n\t}\n\treturn nil\n}\n\n// RemoveMapping will remove an existing mapping.\nfunc (a *Account) RemoveMapping(src string) bool {\n\ta.mu.Lock()\n\tdefer a.mu.Unlock()\n\tfor i, m := range a.mappings {\n\t\tif m.src == src {\n\t\t\t// Swap last one into this spot. Its ok to change order.\n\t\t\ta.mappings[i] = a.mappings[len(a.mappings)-1]\n\t\t\ta.mappings[len(a.mappings)-1] = nil // gc\n\t\t\ta.mappings = a.mappings[:len(a.mappings)-1]\n\t\t\ta.hasMapped.Store(len(a.mappings) > 0)\n\t\t\t// If we have connected leafnodes make sure to update.\n\t\t\tif a.nleafs > 0 {\n\t\t\t\t// Need to release because lock ordering is client -> account\n\t\t\t\ta.mu.Unlock()\n\t\t\t\t// Now grab the leaf list lock. We can hold client lock under this one.\n\t\t\t\ta.lmu.RLock()\n\t\t\t\tfor _, lc := range a.lleafs {\n\t\t\t\t\tlc.forceRemoveFromSmap(src)\n\t\t\t\t}\n\t\t\t\ta.lmu.RUnlock()\n\t\t\t\ta.mu.Lock()\n\t\t\t}\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\n// Indicates we have mapping entries.\nfunc (a *Account) hasMappings() bool {\n\tif a == nil {\n\t\treturn false\n\t}\n\treturn a.hasMapped.Load()\n}\n\n// This performs the logic to map to a new dest subject based on mappings.\n// Should only be called from processInboundClientMsg or service import processing.\nfunc (a *Account) selectMappedSubject(dest string) (string, bool) {\n\tif !a.hasMappings() {\n\t\treturn dest, false\n\t}\n\n\ta.mu.RLock()\n\t// In case we have to tokenize for subset matching.\n\ttsa := [32]string{}\n\ttts := tsa[:0]\n\n\tvar m *mapping\n\tfor _, rm := range a.mappings {\n\t\tif !rm.wc && rm.src == dest {\n\t\t\tm = rm\n\t\t\tbreak\n\t\t} else {\n\t\t\t// tokenize and reuse for subset matching.\n\t\t\tif len(tts) == 0 {\n\t\t\t\tstart := 0\n\t\t\t\tsubject := dest\n\t\t\t\tfor i := 0; i < len(subject); i++ {\n\t\t\t\t\tif subject[i] == btsep {\n\t\t\t\t\t\ttts = append(tts, subject[start:i])\n\t\t\t\t\t\tstart = i + 1\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\ttts = append(tts, subject[start:])\n\t\t\t}\n\t\t\tif isSubsetMatch(tts, rm.src) {\n\t\t\t\tm = rm\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t}\n\n\tif m == nil {\n\t\ta.mu.RUnlock()\n\t\treturn dest, false\n\t}\n\n\t// The selected destination for the mapping.\n\tvar d *destination\n\tvar ndest string\n\n\tdests := m.dests\n\tif len(m.cdests) > 0 {\n\t\tcn := a.srv.cachedClusterName()\n\t\tdests = m.cdests[cn]\n\t\tif dests == nil {\n\t\t\t// Fallback to main if we do not match the cluster.\n\t\t\tdests = m.dests\n\t\t}\n\t}\n\n\t// Optimize for single entry case.\n\tif len(dests) == 1 && dests[0].weight == 100 {\n\t\td = dests[0]\n\t} else {\n\t\tw := uint8(fastrand.Uint32n(100))\n\t\tfor _, rm := range dests {\n\t\t\tif w < rm.weight {\n\t\t\t\td = rm\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t}\n\n\tif d != nil {\n\t\tif len(d.tr.dtokmftokindexesargs) == 0 {\n\t\t\tndest = d.tr.dest\n\t\t} else {\n\t\t\tndest = d.tr.TransformTokenizedSubject(tts)\n\t\t}\n\t}\n\n\ta.mu.RUnlock()\n\treturn ndest, true\n}\n\n// SubscriptionInterest returns true if this account has a matching subscription\n// for the given `subject`.\nfunc (a *Account) SubscriptionInterest(subject string) bool {\n\treturn a.Interest(subject) > 0\n}\n\n// Interest returns the number of subscriptions for a given subject that match.\nfunc (a *Account) Interest(subject string) int {\n\tvar nms int\n\ta.mu.RLock()\n\tif a.sl != nil {\n\t\tnp, nq := a.sl.NumInterest(subject)\n\t\tnms = np + nq\n\t}\n\ta.mu.RUnlock()\n\treturn nms\n}\n\n// addClient keeps our accounting of local active clients or leafnodes updated.\n// Returns previous total.\nfunc (a *Account) addClient(c *client) int {\n\ta.mu.Lock()\n\tn := len(a.clients)\n\n\t// Could come here earlier than the account is registered with the server.\n\t// Make sure we can still track clients.\n\tif a.clients == nil {\n\t\ta.clients = make(map[*client]struct{})\n\t}\n\ta.clients[c] = struct{}{}\n\n\t// If we did not add it, we are done\n\tif n == len(a.clients) {\n\t\ta.mu.Unlock()\n\t\treturn n\n\t}\n\tif c.kind != CLIENT && c.kind != LEAF {\n\t\ta.sysclients++\n\t} else if c.kind == LEAF {\n\t\ta.nleafs++\n\t}\n\ta.mu.Unlock()\n\n\t// If we added a new leaf use the list lock and add it to the list.\n\tif c.kind == LEAF {\n\t\ta.lmu.Lock()\n\t\ta.lleafs = append(a.lleafs, c)\n\t\ta.lmu.Unlock()\n\t}\n\n\tif c != nil && c.srv != nil {\n\t\tc.srv.accConnsUpdate(a)\n\t}\n\n\treturn n\n}\n\n// For registering clusters for remote leafnodes.\n// We only register as the hub.\nfunc (a *Account) registerLeafNodeCluster(cluster string) {\n\ta.mu.Lock()\n\tdefer a.mu.Unlock()\n\tif a.leafClusters == nil {\n\t\ta.leafClusters = make(map[string]uint64)\n\t}\n\ta.leafClusters[cluster]++\n}\n\n// Check to see if we already have this cluster registered.\nfunc (a *Account) hasLeafNodeCluster(cluster string) bool {\n\ta.mu.RLock()\n\tdefer a.mu.RUnlock()\n\treturn a.leafClusters[cluster] > 0\n}\n\n// Check to see if this cluster is isolated, meaning the only one.\n// Read Lock should be held.\nfunc (a *Account) isLeafNodeClusterIsolated(cluster string) bool {\n\tif cluster == _EMPTY_ {\n\t\treturn false\n\t}\n\tif len(a.leafClusters) > 1 {\n\t\treturn false\n\t}\n\treturn a.leafClusters[cluster] == uint64(a.nleafs)\n}\n\n// Helper function to remove leaf nodes. If number of leafnodes gets large\n// this may need to be optimized out of linear search but believe number\n// of active leafnodes per account scope to be small and therefore cache friendly.\n// Lock should not be held on general account lock.\nfunc (a *Account) removeLeafNode(c *client) {\n\t// Make sure we hold the list lock as well.\n\ta.lmu.Lock()\n\tdefer a.lmu.Unlock()\n\n\tll := len(a.lleafs)\n\tfor i, l := range a.lleafs {\n\t\tif l == c {\n\t\t\ta.lleafs[i] = a.lleafs[ll-1]\n\t\t\tif ll == 1 {\n\t\t\t\ta.lleafs = nil\n\t\t\t} else {\n\t\t\t\ta.lleafs = a.lleafs[:ll-1]\n\t\t\t}\n\t\t\treturn\n\t\t}\n\t}\n}\n\n// removeClient keeps our accounting of local active clients updated.\nfunc (a *Account) removeClient(c *client) int {\n\ta.mu.Lock()\n\tn := len(a.clients)\n\tdelete(a.clients, c)\n\t// If we did not actually remove it, we are done.\n\tif n == len(a.clients) {\n\t\ta.mu.Unlock()\n\t\treturn n\n\t}\n\tif c.kind != CLIENT && c.kind != LEAF {\n\t\ta.sysclients--\n\t} else if c.kind == LEAF {\n\t\ta.nleafs--\n\t\t// Need to do cluster accounting here.\n\t\t// Do cluster accounting if we are a hub.\n\t\tif c.isHubLeafNode() {\n\t\t\tcluster := c.remoteCluster()\n\t\t\tif count := a.leafClusters[cluster]; count > 1 {\n\t\t\t\ta.leafClusters[cluster]--\n\t\t\t} else if count == 1 {\n\t\t\t\tdelete(a.leafClusters, cluster)\n\t\t\t}\n\t\t}\n\t}\n\ta.mu.Unlock()\n\n\tif c.kind == LEAF {\n\t\ta.removeLeafNode(c)\n\t}\n\n\tif c != nil && c.srv != nil {\n\t\tc.srv.accConnsUpdate(a)\n\t}\n\n\treturn n\n}\n\nfunc setExportAuth(ea *exportAuth, subject string, accounts []*Account, accountPos uint) error {\n\tif accountPos > 0 {\n\t\ttoken := strings.Split(subject, tsep)\n\t\tif len(token) < int(accountPos) || token[accountPos-1] != \"*\" {\n\t\t\treturn ErrInvalidSubject\n\t\t}\n\t}\n\tea.accountPos = accountPos\n\t// empty means auth required but will be import token.\n\tif accounts == nil {\n\t\treturn nil\n\t}\n\tif len(accounts) == 0 {\n\t\tea.tokenReq = true\n\t\treturn nil\n\t}\n\tif ea.approved == nil {\n\t\tea.approved = make(map[string]*Account, len(accounts))\n\t}\n\tfor _, acc := range accounts {\n\t\tea.approved[acc.Name] = acc\n\t}\n\treturn nil\n}\n\n// AddServiceExport will configure the account with the defined export.\nfunc (a *Account) AddServiceExport(subject string, accounts []*Account) error {\n\treturn a.addServiceExportWithResponseAndAccountPos(subject, Singleton, accounts, 0)\n}\n\n// AddServiceExport will configure the account with the defined export.\nfunc (a *Account) addServiceExportWithAccountPos(subject string, accounts []*Account, accountPos uint) error {\n\treturn a.addServiceExportWithResponseAndAccountPos(subject, Singleton, accounts, accountPos)\n}\n\n// AddServiceExportWithResponse will configure the account with the defined export and response type.\nfunc (a *Account) AddServiceExportWithResponse(subject string, respType ServiceRespType, accounts []*Account) error {\n\treturn a.addServiceExportWithResponseAndAccountPos(subject, respType, accounts, 0)\n}\n\n// AddServiceExportWithresponse will configure the account with the defined export and response type.\nfunc (a *Account) addServiceExportWithResponseAndAccountPos(\n\tsubject string, respType ServiceRespType, accounts []*Account, accountPos uint) error {\n\tif a == nil {\n\t\treturn ErrMissingAccount\n\t}\n\n\ta.mu.Lock()\n\tif a.exports.services == nil {\n\t\ta.exports.services = make(map[string]*serviceExport)\n\t}\n\n\tse := a.exports.services[subject]\n\t// Always  create a service export\n\tif se == nil {\n\t\tse = &serviceExport{}\n\t}\n\n\tif respType != Singleton {\n\t\tse.respType = respType\n\t}\n\n\tif accounts != nil || accountPos > 0 {\n\t\tif err := setExportAuth(&se.exportAuth, subject, accounts, accountPos); err != nil {\n\t\t\ta.mu.Unlock()\n\t\t\treturn err\n\t\t}\n\t}\n\tlrt := a.lowestServiceExportResponseTime()\n\tse.acc = a\n\tse.respThresh = DEFAULT_SERVICE_EXPORT_RESPONSE_THRESHOLD\n\ta.exports.services[subject] = se\n\n\tvar clients []*client\n\tnlrt := a.lowestServiceExportResponseTime()\n\tif nlrt != lrt && len(a.clients) > 0 {\n\t\tclients = a.getClientsLocked()\n\t}\n\t// Need to release because lock ordering is client -> Account\n\ta.mu.Unlock()\n\tif len(clients) > 0 {\n\t\tupdateAllClientsServiceExportResponseTime(clients, nlrt)\n\t}\n\treturn nil\n}\n\n// TrackServiceExport will enable latency tracking of the named service.\n// Results will be published in this account to the given results subject.\nfunc (a *Account) TrackServiceExport(service, results string) error {\n\treturn a.TrackServiceExportWithSampling(service, results, DEFAULT_SERVICE_LATENCY_SAMPLING)\n}\n\n// TrackServiceExportWithSampling will enable latency tracking of the named service for the given\n// sampling rate (1-100). Results will be published in this account to the given results subject.\nfunc (a *Account) TrackServiceExportWithSampling(service, results string, sampling int) error {\n\tif a == nil {\n\t\treturn ErrMissingAccount\n\t}\n\n\tif sampling != 0 { // 0 means triggered by header\n\t\tif sampling < 1 || sampling > 100 {\n\t\t\treturn ErrBadSampling\n\t\t}\n\t}\n\tif !IsValidPublishSubject(results) {\n\t\treturn ErrBadPublishSubject\n\t}\n\t// Don't loop back on outselves.\n\tif a.IsExportService(results) {\n\t\treturn ErrBadPublishSubject\n\t}\n\n\tif a.srv != nil && !a.srv.EventsEnabled() {\n\t\treturn ErrNoSysAccount\n\t}\n\n\ta.mu.Lock()\n\tif a.exports.services == nil {\n\t\ta.mu.Unlock()\n\t\treturn ErrMissingService\n\t}\n\tea, ok := a.exports.services[service]\n\tif !ok {\n\t\ta.mu.Unlock()\n\t\treturn ErrMissingService\n\t}\n\tif ea == nil {\n\t\tea = &serviceExport{}\n\t\ta.exports.services[service] = ea\n\t} else if ea.respType != Singleton {\n\t\ta.mu.Unlock()\n\t\treturn ErrBadServiceType\n\t}\n\tea.latency = &serviceLatency{\n\t\tsampling: int8(sampling),\n\t\tsubject:  results,\n\t}\n\ts := a.srv\n\ta.mu.Unlock()\n\n\tif s == nil {\n\t\treturn nil\n\t}\n\n\t// Now track down the imports and add in latency as needed to enable.\n\ts.accounts.Range(func(k, v any) bool {\n\t\tacc := v.(*Account)\n\t\tacc.mu.Lock()\n\t\tfor _, ims := range acc.imports.services {\n\t\t\tfor _, im := range ims {\n\t\t\t\tif im != nil && im.acc.Name == a.Name && subjectIsSubsetMatch(im.to, service) {\n\t\t\t\t\tim.latency = ea.latency\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tacc.mu.Unlock()\n\t\treturn true\n\t})\n\n\treturn nil\n}\n\n// UnTrackServiceExport will disable latency tracking of the named service.\nfunc (a *Account) UnTrackServiceExport(service string) {\n\tif a == nil || (a.srv != nil && !a.srv.EventsEnabled()) {\n\t\treturn\n\t}\n\n\ta.mu.Lock()\n\tif a.exports.services == nil {\n\t\ta.mu.Unlock()\n\t\treturn\n\t}\n\tea, ok := a.exports.services[service]\n\tif !ok || ea == nil || ea.latency == nil {\n\t\ta.mu.Unlock()\n\t\treturn\n\t}\n\t// We have latency here.\n\tea.latency = nil\n\ts := a.srv\n\ta.mu.Unlock()\n\n\tif s == nil {\n\t\treturn\n\t}\n\n\t// Now track down the imports and clean them up.\n\ts.accounts.Range(func(k, v any) bool {\n\t\tacc := v.(*Account)\n\t\tacc.mu.Lock()\n\t\tfor _, ims := range acc.imports.services {\n\t\t\tfor _, im := range ims {\n\t\t\t\tif im != nil && im.acc.Name == a.Name {\n\t\t\t\t\tif subjectIsSubsetMatch(im.to, service) {\n\t\t\t\t\t\tim.latency, im.m1 = nil, nil\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tacc.mu.Unlock()\n\t\treturn true\n\t})\n}\n\n// IsExportService will indicate if this service exists. Will check wildcard scenarios.\nfunc (a *Account) IsExportService(service string) bool {\n\ta.mu.RLock()\n\tdefer a.mu.RUnlock()\n\t_, ok := a.exports.services[service]\n\tif ok {\n\t\treturn true\n\t}\n\ttokens := strings.Split(service, tsep)\n\tfor subj := range a.exports.services {\n\t\tif isSubsetMatch(tokens, subj) {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\n// IsExportServiceTracking will indicate if given publish subject is an export service with tracking enabled.\nfunc (a *Account) IsExportServiceTracking(service string) bool {\n\ta.mu.RLock()\n\tea, ok := a.exports.services[service]\n\tif ok && ea == nil {\n\t\ta.mu.RUnlock()\n\t\treturn false\n\t}\n\tif ok && ea != nil && ea.latency != nil {\n\t\ta.mu.RUnlock()\n\t\treturn true\n\t}\n\t// FIXME(dlc) - Might want to cache this is in the hot path checking for latency tracking.\n\ttokens := strings.Split(service, tsep)\n\tfor subj, ea := range a.exports.services {\n\t\tif isSubsetMatch(tokens, subj) && ea != nil && ea.latency != nil {\n\t\t\ta.mu.RUnlock()\n\t\t\treturn true\n\t\t}\n\t}\n\ta.mu.RUnlock()\n\treturn false\n}\n\n// ServiceLatency is the JSON message sent out in response to latency tracking for\n// an accounts exported services. Additional client info is available in requestor\n// and responder. Note that for a requestor, the only information shared by default\n// is the RTT used to calculate the total latency. The requestor's account can\n// designate to share the additional information in the service import.\ntype ServiceLatency struct {\n\tTypedEvent\n\tStatus         int           `json:\"status\"`\n\tError          string        `json:\"description,omitempty\"`\n\tRequestor      *ClientInfo   `json:\"requestor,omitempty\"`\n\tResponder      *ClientInfo   `json:\"responder,omitempty\"`\n\tRequestHeader  http.Header   `json:\"header,omitempty\"` // only contains header(s) triggering the measurement\n\tRequestStart   time.Time     `json:\"start\"`\n\tServiceLatency time.Duration `json:\"service\"`\n\tSystemLatency  time.Duration `json:\"system\"`\n\tTotalLatency   time.Duration `json:\"total\"`\n}\n\n// ServiceLatencyType is the NATS Event Type for ServiceLatency\nconst ServiceLatencyType = \"io.nats.server.metric.v1.service_latency\"\n\n// NATSTotalTime is a helper function that totals the NATS latencies.\nfunc (m1 *ServiceLatency) NATSTotalTime() time.Duration {\n\treturn m1.Requestor.RTT + m1.Responder.RTT + m1.SystemLatency\n}\n\n// Merge function to merge m1 and m2 (requestor and responder) measurements\n// when there are two samples. This happens when the requestor and responder\n// are on different servers.\n//\n// m2 ServiceLatency is correct, so use that.\n// m1 TotalLatency is correct, so use that.\n// Will use those to back into NATS latency.\nfunc (m1 *ServiceLatency) merge(m2 *ServiceLatency) {\n\trtt := time.Duration(0)\n\tif m2.Responder != nil {\n\t\trtt = m2.Responder.RTT\n\t}\n\tm1.SystemLatency = m1.ServiceLatency - (m2.ServiceLatency + rtt)\n\tm1.ServiceLatency = m2.ServiceLatency\n\tm1.Responder = m2.Responder\n\tsanitizeLatencyMetric(m1)\n}\n\n// sanitizeLatencyMetric adjusts latency metric values that could go\n// negative in some edge conditions since we estimate client RTT\n// for both requestor and responder.\n// These numbers are never meant to be negative, it just could be\n// how we back into the values based on estimated RTT.\nfunc sanitizeLatencyMetric(sl *ServiceLatency) {\n\tif sl.ServiceLatency < 0 {\n\t\tsl.ServiceLatency = 0\n\t}\n\tif sl.SystemLatency < 0 {\n\t\tsl.SystemLatency = 0\n\t}\n}\n\n// Used for transporting remote latency measurements.\ntype remoteLatency struct {\n\tAccount    string         `json:\"account\"`\n\tReqId      string         `json:\"req_id\"`\n\tM2         ServiceLatency `json:\"m2\"`\n\trespThresh time.Duration\n}\n\n// sendLatencyResult will send a latency result and clear the si of the requestor(rc).\nfunc (a *Account) sendLatencyResult(si *serviceImport, sl *ServiceLatency) {\n\tsl.Type = ServiceLatencyType\n\tsl.ID = a.nextEventID()\n\tsl.Time = time.Now().UTC()\n\ta.mu.Lock()\n\tlsubj := si.latency.subject\n\tsi.rc = nil\n\ta.mu.Unlock()\n\n\ta.srv.sendInternalAccountMsg(a, lsubj, sl)\n}\n\n// Used to send a bad request metric when we do not have a reply subject\nfunc (a *Account) sendBadRequestTrackingLatency(si *serviceImport, requestor *client, header http.Header) {\n\tsl := &ServiceLatency{\n\t\tStatus:    400,\n\t\tError:     \"Bad Request\",\n\t\tRequestor: requestor.getClientInfo(si.share),\n\t}\n\tsl.RequestHeader = header\n\tsl.RequestStart = time.Now().Add(-sl.Requestor.RTT).UTC()\n\ta.sendLatencyResult(si, sl)\n}\n\n// Used to send a latency result when the requestor interest was lost before the\n// response could be delivered.\nfunc (a *Account) sendReplyInterestLostTrackLatency(si *serviceImport) {\n\tsl := &ServiceLatency{\n\t\tStatus: 408,\n\t\tError:  \"Request Timeout\",\n\t}\n\ta.mu.RLock()\n\trc, share, ts := si.rc, si.share, si.ts\n\tsl.RequestHeader = si.trackingHdr\n\ta.mu.RUnlock()\n\tif rc != nil {\n\t\tsl.Requestor = rc.getClientInfo(share)\n\t}\n\tsl.RequestStart = time.Unix(0, ts-int64(sl.Requestor.RTT)).UTC()\n\ta.sendLatencyResult(si, sl)\n}\n\nfunc (a *Account) sendBackendErrorTrackingLatency(si *serviceImport, reason rsiReason) {\n\tsl := &ServiceLatency{}\n\ta.mu.RLock()\n\trc, share, ts := si.rc, si.share, si.ts\n\tsl.RequestHeader = si.trackingHdr\n\ta.mu.RUnlock()\n\tif rc != nil {\n\t\tsl.Requestor = rc.getClientInfo(share)\n\t}\n\tvar reqRTT time.Duration\n\tif sl.Requestor != nil {\n\t\treqRTT = sl.Requestor.RTT\n\t}\n\tsl.RequestStart = time.Unix(0, ts-int64(reqRTT)).UTC()\n\tif reason == rsiNoDelivery {\n\t\tsl.Status = 503\n\t\tsl.Error = \"Service Unavailable\"\n\t} else if reason == rsiTimeout {\n\t\tsl.Status = 504\n\t\tsl.Error = \"Service Timeout\"\n\t}\n\ta.sendLatencyResult(si, sl)\n}\n\n// sendTrackingLatency will send out the appropriate tracking information for the\n// service request/response latency. This is called when the requestor's server has\n// received the response.\n// TODO(dlc) - holding locks for RTTs may be too much long term. Should revisit.\nfunc (a *Account) sendTrackingLatency(si *serviceImport, responder *client) bool {\n\ta.mu.RLock()\n\trc := si.rc\n\ta.mu.RUnlock()\n\tif rc == nil {\n\t\treturn true\n\t}\n\n\tts := time.Now()\n\tserviceRTT := time.Duration(ts.UnixNano() - si.ts)\n\trequestor := si.rc\n\n\tsl := &ServiceLatency{\n\t\tStatus:    200,\n\t\tRequestor: requestor.getClientInfo(si.share),\n\t\tResponder: responder.getClientInfo(true),\n\t}\n\tvar respRTT, reqRTT time.Duration\n\tif sl.Responder != nil {\n\t\trespRTT = sl.Responder.RTT\n\t}\n\tif sl.Requestor != nil {\n\t\treqRTT = sl.Requestor.RTT\n\t}\n\tsl.RequestStart = time.Unix(0, si.ts-int64(reqRTT)).UTC()\n\tsl.ServiceLatency = serviceRTT - respRTT\n\tsl.TotalLatency = reqRTT + serviceRTT\n\tif respRTT > 0 {\n\t\tsl.SystemLatency = time.Since(ts)\n\t\tsl.TotalLatency += sl.SystemLatency\n\t}\n\tsl.RequestHeader = si.trackingHdr\n\tsanitizeLatencyMetric(sl)\n\n\tsl.Type = ServiceLatencyType\n\tsl.ID = a.nextEventID()\n\tsl.Time = time.Now().UTC()\n\n\t// If we are expecting a remote measurement, store our sl here.\n\t// We need to account for the race between this and us receiving the\n\t// remote measurement.\n\t// FIXME(dlc) - We need to clean these up but this should happen\n\t// already with the auto-expire logic.\n\tif responder != nil && responder.kind != CLIENT {\n\t\tsi.acc.mu.Lock()\n\t\tif si.m1 != nil {\n\t\t\tm1, m2 := sl, si.m1\n\t\t\tm1.merge(m2)\n\t\t\tsi.acc.mu.Unlock()\n\t\t\ta.srv.sendInternalAccountMsg(a, si.latency.subject, m1)\n\t\t\ta.mu.Lock()\n\t\t\tsi.rc = nil\n\t\t\ta.mu.Unlock()\n\t\t\treturn true\n\t\t}\n\t\tsi.m1 = sl\n\t\tsi.acc.mu.Unlock()\n\t\treturn false\n\t} else {\n\t\ta.srv.sendInternalAccountMsg(a, si.latency.subject, sl)\n\t\ta.mu.Lock()\n\t\tsi.rc = nil\n\t\ta.mu.Unlock()\n\t}\n\treturn true\n}\n\n// This will check to make sure our response lower threshold is set\n// properly in any clients doing rrTracking.\nfunc updateAllClientsServiceExportResponseTime(clients []*client, lrt time.Duration) {\n\tfor _, c := range clients {\n\t\tc.mu.Lock()\n\t\tif c.rrTracking != nil && lrt != c.rrTracking.lrt {\n\t\t\tc.rrTracking.lrt = lrt\n\t\t\tif c.rrTracking.ptmr.Stop() {\n\t\t\t\tc.rrTracking.ptmr.Reset(lrt)\n\t\t\t}\n\t\t}\n\t\tc.mu.Unlock()\n\t}\n}\n\n// Will select the lowest respThresh from all service exports.\n// Read lock should be held.\nfunc (a *Account) lowestServiceExportResponseTime() time.Duration {\n\t// Lowest we will allow is 5 minutes. Its an upper bound for this function.\n\tlrt := 5 * time.Minute\n\tfor _, se := range a.exports.services {\n\t\tif se.respThresh < lrt {\n\t\t\tlrt = se.respThresh\n\t\t}\n\t}\n\treturn lrt\n}\n\n// AddServiceImportWithClaim will add in the service import via the jwt claim.\nfunc (a *Account) AddServiceImportWithClaim(destination *Account, from, to string, imClaim *jwt.Import) error {\n\treturn a.addServiceImportWithClaim(destination, from, to, imClaim, false)\n}\n\n// addServiceImportWithClaim will add in the service import via the jwt claim.\n// It will also skip the authorization check in cases where internal is true\nfunc (a *Account) addServiceImportWithClaim(destination *Account, from, to string, imClaim *jwt.Import, internal bool) error {\n\tif destination == nil {\n\t\treturn ErrMissingAccount\n\t}\n\t// Empty means use from.\n\tif to == _EMPTY_ {\n\t\tto = from\n\t}\n\tif !IsValidSubject(from) || !IsValidSubject(to) {\n\t\treturn ErrInvalidSubject\n\t}\n\n\t// First check to see if the account has authorized us to route to the \"to\" subject.\n\tif !internal && !destination.checkServiceImportAuthorized(a, to, imClaim) {\n\t\treturn ErrServiceImportAuthorization\n\t}\n\n\t// Check if this introduces a cycle before proceeding.\n\t// From will be the mapped subject.\n\t// If the 'to' has a wildcard make sure we pre-transform the 'from' before we check for cycles, e.g. '$1'\n\tfromT := from\n\tif subjectHasWildcard(to) {\n\t\tfromT, _ = transformUntokenize(from)\n\t}\n\tif err := a.serviceImportFormsCycle(destination, fromT); err != nil {\n\t\treturn err\n\t}\n\n\t_, err := a.addServiceImport(destination, from, to, imClaim)\n\n\treturn err\n}\n\nconst MaxAccountCycleSearchDepth = 1024\n\nfunc (a *Account) serviceImportFormsCycle(dest *Account, from string) error {\n\treturn dest.checkServiceImportsForCycles(from, map[string]bool{a.Name: true})\n}\n\nfunc (a *Account) checkServiceImportsForCycles(from string, visited map[string]bool) error {\n\tif len(visited) >= MaxAccountCycleSearchDepth {\n\t\treturn ErrCycleSearchDepth\n\t}\n\ta.mu.RLock()\n\tfor _, sis := range a.imports.services {\n\t\tfor _, si := range sis {\n\t\t\tif SubjectsCollide(from, si.to) {\n\t\t\t\ta.mu.RUnlock()\n\t\t\t\tif visited[si.acc.Name] {\n\t\t\t\t\treturn ErrImportFormsCycle\n\t\t\t\t}\n\t\t\t\t// Push ourselves and check si.acc\n\t\t\t\tvisited[a.Name] = true\n\t\t\t\tif subjectIsSubsetMatch(si.from, from) {\n\t\t\t\t\tfrom = si.from\n\t\t\t\t}\n\t\t\t\tif err := si.acc.checkServiceImportsForCycles(from, visited); err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\ta.mu.RLock()\n\t\t\t}\n\t\t}\n\t}\n\ta.mu.RUnlock()\n\treturn nil\n}\n\nfunc (a *Account) streamImportFormsCycle(dest *Account, to string) error {\n\treturn dest.checkStreamImportsForCycles(to, map[string]bool{a.Name: true})\n}\n\n// Lock should be held.\nfunc (a *Account) hasServiceExportMatching(to string) bool {\n\tfor subj := range a.exports.services {\n\t\tif subjectIsSubsetMatch(to, subj) {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\n// Lock should be held.\nfunc (a *Account) hasStreamExportMatching(to string) bool {\n\tfor subj := range a.exports.streams {\n\t\tif subjectIsSubsetMatch(to, subj) {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\nfunc (a *Account) checkStreamImportsForCycles(to string, visited map[string]bool) error {\n\tif len(visited) >= MaxAccountCycleSearchDepth {\n\t\treturn ErrCycleSearchDepth\n\t}\n\n\ta.mu.RLock()\n\n\tif !a.hasStreamExportMatching(to) {\n\t\ta.mu.RUnlock()\n\t\treturn nil\n\t}\n\n\tfor _, si := range a.imports.streams {\n\t\tif SubjectsCollide(to, si.to) {\n\t\t\ta.mu.RUnlock()\n\t\t\tif visited[si.acc.Name] {\n\t\t\t\treturn ErrImportFormsCycle\n\t\t\t}\n\t\t\t// Push ourselves and check si.acc\n\t\t\tvisited[a.Name] = true\n\t\t\tif subjectIsSubsetMatch(si.to, to) {\n\t\t\t\tto = si.to\n\t\t\t}\n\t\t\tif err := si.acc.checkStreamImportsForCycles(to, visited); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\ta.mu.RLock()\n\t\t}\n\t}\n\ta.mu.RUnlock()\n\treturn nil\n}\n\n// SetServiceImportSharing will allow sharing of information about requests with the export account.\n// Used for service latency tracking at the moment.\nfunc (a *Account) SetServiceImportSharing(destination *Account, to string, allow bool) error {\n\treturn a.setServiceImportSharing(destination, to, true, allow)\n}\n\n// setServiceImportSharing will allow sharing of information about requests with the export account.\nfunc (a *Account) setServiceImportSharing(destination *Account, to string, check, allow bool) error {\n\ta.mu.Lock()\n\tdefer a.mu.Unlock()\n\tif check && a.isClaimAccount() {\n\t\treturn fmt.Errorf(\"claim based accounts can not be updated directly\")\n\t}\n\t// We can't use getServiceImportForAccountLocked() here since we are looking\n\t// for the service import with the si.to == to, which may not be the key\n\t// for the service import in the map.\n\tfor _, sis := range a.imports.services {\n\t\tfor _, si := range sis {\n\t\t\tif si.acc.Name == destination.Name && si.to == to {\n\t\t\t\tsi.share = allow\n\t\t\t\treturn nil\n\t\t\t}\n\t\t}\n\t}\n\treturn fmt.Errorf(\"service import not found\")\n}\n\n// AddServiceImport will add a route to an account to send published messages / requests\n// to the destination account. From is the local subject to map, To is the\n// subject that will appear on the destination account. Destination will need\n// to have an import rule to allow access via addService.\nfunc (a *Account) AddServiceImport(destination *Account, from, to string) error {\n\treturn a.AddServiceImportWithClaim(destination, from, to, nil)\n}\n\n// NumPendingReverseResponses returns the number of response mappings we have for all outstanding\n// requests for service imports.\nfunc (a *Account) NumPendingReverseResponses() int {\n\ta.mu.RLock()\n\tdefer a.mu.RUnlock()\n\treturn len(a.imports.rrMap)\n}\n\n// NumPendingAllResponses return the number of all responses outstanding for service exports.\nfunc (a *Account) NumPendingAllResponses() int {\n\treturn a.NumPendingResponses(_EMPTY_)\n}\n\n// NumPendingResponses returns the number of responses outstanding for service exports\n// on this account. An empty filter string returns all responses regardless of which export.\n// If you specify the filter we will only return ones that are for that export.\n// NOTE this is only for what this server is tracking.\nfunc (a *Account) NumPendingResponses(filter string) int {\n\ta.mu.RLock()\n\tdefer a.mu.RUnlock()\n\tif filter == _EMPTY_ {\n\t\treturn len(a.exports.responses)\n\t}\n\tse := a.getServiceExport(filter)\n\tif se == nil {\n\t\treturn 0\n\t}\n\tvar nre int\n\tfor _, si := range a.exports.responses {\n\t\tif si.se == se {\n\t\t\tnre++\n\t\t}\n\t}\n\treturn nre\n}\n\n// NumServiceImports returns the number of service imports we have configured.\nfunc (a *Account) NumServiceImports() int {\n\ta.mu.RLock()\n\tdefer a.mu.RUnlock()\n\treturn len(a.imports.services)\n}\n\n// Reason why we are removing this response serviceImport.\ntype rsiReason int\n\nconst (\n\trsiOk = rsiReason(iota)\n\trsiNoDelivery\n\trsiTimeout\n)\n\n// removeRespServiceImport removes a response si mapping and the reverse entries for interest detection.\nfunc (a *Account) removeRespServiceImport(si *serviceImport, reason rsiReason) {\n\tif si == nil {\n\t\treturn\n\t}\n\n\ta.mu.Lock()\n\tc := a.ic\n\tdelete(a.exports.responses, si.from)\n\tdest, to, tracking, rc, didDeliver := si.acc, si.to, si.tracking, si.rc, si.didDeliver\n\ta.mu.Unlock()\n\n\t// If we have a sid make sure to unsub.\n\tif len(si.sid) > 0 && c != nil {\n\t\tc.processUnsub(si.sid)\n\t}\n\n\tif tracking && rc != nil && !didDeliver {\n\t\ta.sendBackendErrorTrackingLatency(si, reason)\n\t}\n\n\tdest.checkForReverseEntry(to, si, false)\n}\n\nfunc (a *Account) getServiceImportForAccountLocked(dstAccName, subject string) *serviceImport {\n\tsis, ok := a.imports.services[subject]\n\tif !ok {\n\t\treturn nil\n\t}\n\tif len(sis) == 1 && sis[0].acc.Name == dstAccName {\n\t\treturn sis[0]\n\t}\n\tfor _, si := range sis {\n\t\tif si.acc.Name == dstAccName {\n\t\t\treturn si\n\t\t}\n\t}\n\treturn nil\n}\n\n// removeServiceImport will remove the route by subject.\nfunc (a *Account) removeServiceImport(dstAccName, subject string) {\n\ta.mu.Lock()\n\tsis, ok := a.imports.services[subject]\n\tif !ok {\n\t\ta.mu.Unlock()\n\t\treturn\n\t}\n\tvar si *serviceImport\n\tif len(sis) == 1 {\n\t\tsi = sis[0]\n\t\tif si.acc.Name != dstAccName {\n\t\t\tsi = nil\n\t\t} else {\n\t\t\tdelete(a.imports.services, subject)\n\t\t}\n\t} else {\n\t\tfor i, esi := range sis {\n\t\t\tif esi.acc.Name == dstAccName {\n\t\t\t\tsi = esi\n\t\t\t\tlast := len(sis) - 1\n\t\t\t\tif i != last {\n\t\t\t\t\tsis[i] = sis[last]\n\t\t\t\t}\n\t\t\t\tsis = sis[:last]\n\t\t\t\ta.imports.services[subject] = sis\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t}\n\tif si == nil {\n\t\ta.mu.Unlock()\n\t\treturn\n\t}\n\tvar sid []byte\n\tc := a.ic\n\tif c != nil && si.sid != nil {\n\t\tsid = si.sid\n\t}\n\ta.mu.Unlock()\n\n\tif sid != nil {\n\t\tc.processUnsub(sid)\n\t}\n}\n\n// This tracks responses to service requests mappings. This is used for cleanup.\nfunc (a *Account) addReverseRespMapEntry(acc *Account, reply, from string) {\n\ta.mu.Lock()\n\tif a.imports.rrMap == nil {\n\t\ta.imports.rrMap = make(map[string][]*serviceRespEntry)\n\t}\n\tsre := &serviceRespEntry{acc, from}\n\tsra := a.imports.rrMap[reply]\n\ta.imports.rrMap[reply] = append(sra, sre)\n\ta.mu.Unlock()\n}\n\n// checkForReverseEntries is for when we are trying to match reverse entries to a wildcard.\n// This will be called from checkForReverseEntry when the reply arg is a wildcard subject.\n// This will usually be called in a go routine since we need to walk all the entries.\nfunc (a *Account) checkForReverseEntries(reply string, checkInterest, recursed bool) {\n\tif subjectIsLiteral(reply) {\n\t\ta._checkForReverseEntry(reply, nil, checkInterest, recursed)\n\t\treturn\n\t}\n\n\ta.mu.RLock()\n\tif len(a.imports.rrMap) == 0 {\n\t\ta.mu.RUnlock()\n\t\treturn\n\t}\n\n\tvar _rs [64]string\n\trs := _rs[:0]\n\tif n := len(a.imports.rrMap); n > cap(rs) {\n\t\trs = make([]string, 0, n)\n\t}\n\n\tfor k := range a.imports.rrMap {\n\t\trs = append(rs, k)\n\t}\n\ta.mu.RUnlock()\n\n\ttsa := [32]string{}\n\ttts := tokenizeSubjectIntoSlice(tsa[:0], reply)\n\n\trsa := [32]string{}\n\tfor _, r := range rs {\n\t\trts := tokenizeSubjectIntoSlice(rsa[:0], r)\n\t\t//  isSubsetMatchTokenized is heavy so make sure we do this without the lock.\n\t\tif isSubsetMatchTokenized(rts, tts) {\n\t\t\ta._checkForReverseEntry(r, nil, checkInterest, recursed)\n\t\t}\n\t}\n}\n\n// This checks for any response map entries. If you specify an si we will only match and\n// clean up for that one, otherwise we remove them all.\nfunc (a *Account) checkForReverseEntry(reply string, si *serviceImport, checkInterest bool) {\n\ta._checkForReverseEntry(reply, si, checkInterest, false)\n}\n\n// Callers should use checkForReverseEntry instead. This function exists to help prevent\n// infinite recursion.\nfunc (a *Account) _checkForReverseEntry(reply string, si *serviceImport, checkInterest, recursed bool) {\n\ta.mu.RLock()\n\tif len(a.imports.rrMap) == 0 {\n\t\ta.mu.RUnlock()\n\t\treturn\n\t}\n\n\tif subjectHasWildcard(reply) {\n\t\tif recursed {\n\t\t\t// If we have reached this condition then it is because the reverse entries also\n\t\t\t// contain wildcards (that shouldn't happen but a client *could* provide an inbox\n\t\t\t// prefix that is illegal because it ends in a wildcard character), at which point\n\t\t\t// we will end up with infinite recursion between this func and checkForReverseEntries.\n\t\t\t// To avoid a stack overflow panic, we'll give up instead.\n\t\t\ta.mu.RUnlock()\n\t\t\treturn\n\t\t}\n\n\t\tdoInline := len(a.imports.rrMap) <= 64\n\t\ta.mu.RUnlock()\n\n\t\tif doInline {\n\t\t\ta.checkForReverseEntries(reply, checkInterest, true)\n\t\t} else {\n\t\t\tgo a.checkForReverseEntries(reply, checkInterest, true)\n\t\t}\n\t\treturn\n\t}\n\n\tif sres := a.imports.rrMap[reply]; sres == nil {\n\t\ta.mu.RUnlock()\n\t\treturn\n\t}\n\n\t// If we are here we have an entry we should check.\n\t// If requested we will first check if there is any\n\t// interest for this subject for the entire account.\n\t// If there is we can not delete any entries yet.\n\t// Note that if we are here reply has to be a literal subject.\n\tif checkInterest {\n\t\t// If interest still exists we can not clean these up yet.\n\t\tif a.sl.HasInterest(reply) {\n\t\t\ta.mu.RUnlock()\n\t\t\treturn\n\t\t}\n\t}\n\ta.mu.RUnlock()\n\n\t// Delete the appropriate entries here based on optional si.\n\ta.mu.Lock()\n\t// We need a new lookup here because we have released the lock.\n\tsres := a.imports.rrMap[reply]\n\tif si == nil {\n\t\tdelete(a.imports.rrMap, reply)\n\t} else if sres != nil {\n\t\t// Find the one we are looking for..\n\t\tfor i, sre := range sres {\n\t\t\tif sre.msub == si.from {\n\t\t\t\tsres = append(sres[:i], sres[i+1:]...)\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\tif len(sres) > 0 {\n\t\t\ta.imports.rrMap[si.to] = sres\n\t\t} else {\n\t\t\tdelete(a.imports.rrMap, si.to)\n\t\t}\n\t}\n\ta.mu.Unlock()\n\n\t// If we are here we no longer have interest and we have\n\t// response entries that we should clean up.\n\tif si == nil {\n\t\t// sres is now known to have been removed from a.imports.rrMap, so we\n\t\t// can safely (data race wise) iterate through.\n\t\tfor _, sre := range sres {\n\t\t\tacc := sre.acc\n\t\t\tvar trackingCleanup bool\n\t\t\tvar rsi *serviceImport\n\t\t\tacc.mu.Lock()\n\t\t\tc := acc.ic\n\t\t\tif rsi = acc.exports.responses[sre.msub]; rsi != nil && !rsi.didDeliver {\n\t\t\t\tdelete(acc.exports.responses, rsi.from)\n\t\t\t\ttrackingCleanup = rsi.tracking && rsi.rc != nil\n\t\t\t}\n\t\t\tacc.mu.Unlock()\n\t\t\t// If we are doing explicit subs for all responses (e.g. bound to leafnode)\n\t\t\t// we will have a non-empty sid here.\n\t\t\tif rsi != nil && len(rsi.sid) > 0 && c != nil {\n\t\t\t\tc.processUnsub(rsi.sid)\n\t\t\t}\n\t\t\tif trackingCleanup {\n\t\t\t\tacc.sendReplyInterestLostTrackLatency(rsi)\n\t\t\t}\n\t\t}\n\t}\n}\n\n// Checks to see if a potential service import subject is already overshadowed.\nfunc (a *Account) serviceImportShadowed(from string) bool {\n\ta.mu.RLock()\n\tdefer a.mu.RUnlock()\n\tif a.imports.services[from] != nil {\n\t\treturn true\n\t}\n\t// We did not find a direct match, so check individually.\n\tfor subj := range a.imports.services {\n\t\tif subjectIsSubsetMatch(from, subj) {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\n// Internal check to see if a service import exists.\nfunc (a *Account) serviceImportExists(dstAccName, from string) bool {\n\ta.mu.RLock()\n\tdup := a.getServiceImportForAccountLocked(dstAccName, from)\n\ta.mu.RUnlock()\n\treturn dup != nil\n}\n\n// Add a service import.\n// This does no checks and should only be called by the msg processing code.\n// Use AddServiceImport from above if responding to user input or config changes, etc.\nfunc (a *Account) addServiceImport(dest *Account, from, to string, claim *jwt.Import) (*serviceImport, error) {\n\trt := Singleton\n\tvar lat *serviceLatency\n\n\tif dest == nil {\n\t\treturn nil, ErrMissingAccount\n\t}\n\n\tvar atrc bool\n\tdest.mu.RLock()\n\tse := dest.getServiceExport(to)\n\tif se != nil {\n\t\trt = se.respType\n\t\tlat = se.latency\n\t\tatrc = se.atrc\n\t}\n\tdestAccName := dest.Name\n\tdest.mu.RUnlock()\n\n\ta.mu.Lock()\n\tif a.imports.services == nil {\n\t\ta.imports.services = make(map[string][]*serviceImport)\n\t} else if dup := a.getServiceImportForAccountLocked(destAccName, from); dup != nil {\n\t\ta.mu.Unlock()\n\t\treturn nil, fmt.Errorf(\"duplicate service import subject %q, previously used in import for account %q, subject %q\",\n\t\t\tfrom, dup.acc.Name, dup.to)\n\t}\n\n\tif to == _EMPTY_ {\n\t\tto = from\n\t}\n\t// Check to see if we have a wildcard\n\tvar (\n\t\tusePub bool\n\t\ttr     *subjectTransform\n\t\terr    error\n\t)\n\n\tif subjectHasWildcard(to) {\n\t\t// If to and from match, then we use the published subject.\n\t\tif to == from {\n\t\t\tusePub = true\n\t\t} else {\n\t\t\tto, _ = transformUntokenize(to)\n\t\t\t// Create a transform. Do so in reverse such that $ symbols only exist in to\n\t\t\tif tr, err = NewSubjectTransformStrict(to, transformTokenize(from)); err != nil {\n\t\t\t\ta.mu.Unlock()\n\t\t\t\treturn nil, fmt.Errorf(\"failed to create mapping transform for service import subject from %q to %q: %v\",\n\t\t\t\t\tfrom, to, err)\n\t\t\t} else {\n\t\t\t\t// un-tokenize and reverse transform so we get the transform needed\n\t\t\t\tfrom, _ = transformUntokenize(from)\n\t\t\t\ttr = tr.reverse()\n\t\t\t}\n\t\t}\n\t}\n\tvar share bool\n\tif claim != nil {\n\t\tshare = claim.Share\n\t}\n\tsi := &serviceImport{dest, claim, se, nil, from, to, tr, 0, rt, lat, nil, nil, usePub, false, false, share, false, false, atrc, nil}\n\tsis := a.imports.services[from]\n\tsis = append(sis, si)\n\ta.imports.services[from] = sis\n\ta.mu.Unlock()\n\n\tif err := a.addServiceImportSub(si); err != nil {\n\t\ta.removeServiceImport(destAccName, si.from)\n\t\treturn nil, err\n\t}\n\treturn si, nil\n}\n\n// Returns the internal client, will create one if not present.\n// Lock should be held.\nfunc (a *Account) internalClient() *client {\n\tif a.ic == nil && a.srv != nil {\n\t\ta.ic = a.srv.createInternalAccountClient()\n\t\ta.ic.acc = a\n\t}\n\treturn a.ic\n}\n\n// Internal account scoped subscriptions.\nfunc (a *Account) subscribeInternal(subject string, cb msgHandler) (*subscription, error) {\n\treturn a.subscribeInternalEx(subject, cb, false)\n}\n\n// Unsubscribe from an internal account subscription.\nfunc (a *Account) unsubscribeInternal(sub *subscription) {\n\tif ic := a.internalClient(); ic != nil {\n\t\tic.processUnsub(sub.sid)\n\t}\n}\n\n// Creates internal subscription for service import responses.\nfunc (a *Account) subscribeServiceImportResponse(subject string) (*subscription, error) {\n\treturn a.subscribeInternalEx(subject, a.processServiceImportResponse, true)\n}\n\nfunc (a *Account) subscribeInternalEx(subject string, cb msgHandler, ri bool) (*subscription, error) {\n\ta.mu.Lock()\n\ta.isid++\n\tc, sid := a.internalClient(), strconv.FormatUint(a.isid, 10)\n\ta.mu.Unlock()\n\n\t// This will happen in parsing when the account has not been properly setup.\n\tif c == nil {\n\t\treturn nil, fmt.Errorf(\"no internal account client\")\n\t}\n\n\treturn c.processSubEx([]byte(subject), nil, []byte(sid), cb, false, false, ri)\n}\n\n// This will add an account subscription that matches the \"from\" from a service import entry.\nfunc (a *Account) addServiceImportSub(si *serviceImport) error {\n\ta.mu.Lock()\n\tc := a.internalClient()\n\t// This will happen in parsing when the account has not been properly setup.\n\tif c == nil {\n\t\ta.mu.Unlock()\n\t\treturn nil\n\t}\n\tif si.sid != nil {\n\t\ta.mu.Unlock()\n\t\treturn fmt.Errorf(\"duplicate call to create subscription for service import\")\n\t}\n\ta.isid++\n\tsid := strconv.FormatUint(a.isid, 10)\n\tsi.sid = []byte(sid)\n\tsubject := si.from\n\ta.mu.Unlock()\n\n\tcb := func(sub *subscription, c *client, acc *Account, subject, reply string, msg []byte) {\n\t\tc.pa.delivered = c.processServiceImport(si, acc, msg)\n\t}\n\tsub, err := c.processSubEx([]byte(subject), nil, []byte(sid), cb, true, true, false)\n\tif err != nil {\n\t\treturn err\n\t}\n\t// Leafnodes introduce a new way to introduce messages into the system. Therefore forward import subscription\n\t// This is similar to what initLeafNodeSmapAndSendSubs does\n\t// TODO we need to consider performing this update as we get client subscriptions.\n\t//      This behavior would result in subscription propagation only where actually used.\n\ta.updateLeafNodes(sub, 1)\n\treturn nil\n}\n\n// Remove all the subscriptions associated with service imports.\nfunc (a *Account) removeAllServiceImportSubs() {\n\ta.mu.RLock()\n\tvar sids [][]byte\n\tfor _, sis := range a.imports.services {\n\t\tfor _, si := range sis {\n\t\t\tif si.sid != nil {\n\t\t\t\tsids = append(sids, si.sid)\n\t\t\t\tsi.sid = nil\n\t\t\t}\n\t\t}\n\t}\n\tc := a.ic\n\ta.ic = nil\n\ta.mu.RUnlock()\n\n\tif c == nil {\n\t\treturn\n\t}\n\tfor _, sid := range sids {\n\t\tc.processUnsub(sid)\n\t}\n\tc.closeConnection(InternalClient)\n}\n\n// Add in subscriptions for all registered service imports.\nfunc (a *Account) addAllServiceImportSubs() {\n\tvar sis [32]*serviceImport\n\tserviceImports := sis[:0]\n\ta.mu.RLock()\n\tfor _, sis := range a.imports.services {\n\t\tserviceImports = append(serviceImports, sis...)\n\t}\n\ta.mu.RUnlock()\n\tfor _, si := range serviceImports {\n\t\ta.addServiceImportSub(si)\n\t}\n}\n\nvar (\n\t// header where all information is encoded in one value.\n\ttrcUber = textproto.CanonicalMIMEHeaderKey(\"Uber-Trace-Id\")\n\ttrcCtx  = textproto.CanonicalMIMEHeaderKey(\"Traceparent\")\n\ttrcB3   = textproto.CanonicalMIMEHeaderKey(\"B3\")\n\t// openzipkin header to check\n\ttrcB3Sm = textproto.CanonicalMIMEHeaderKey(\"X-B3-Sampled\")\n\ttrcB3Id = textproto.CanonicalMIMEHeaderKey(\"X-B3-TraceId\")\n\t// additional header needed to include when present\n\ttrcB3PSId        = textproto.CanonicalMIMEHeaderKey(\"X-B3-ParentSpanId\")\n\ttrcB3SId         = textproto.CanonicalMIMEHeaderKey(\"X-B3-SpanId\")\n\ttrcCtxSt         = textproto.CanonicalMIMEHeaderKey(\"Tracestate\")\n\ttrcUberCtxPrefix = textproto.CanonicalMIMEHeaderKey(\"Uberctx-\")\n)\n\nfunc newB3Header(h http.Header) http.Header {\n\tretHdr := http.Header{}\n\tif v, ok := h[trcB3Sm]; ok {\n\t\tretHdr[trcB3Sm] = v\n\t}\n\tif v, ok := h[trcB3Id]; ok {\n\t\tretHdr[trcB3Id] = v\n\t}\n\tif v, ok := h[trcB3PSId]; ok {\n\t\tretHdr[trcB3PSId] = v\n\t}\n\tif v, ok := h[trcB3SId]; ok {\n\t\tretHdr[trcB3SId] = v\n\t}\n\treturn retHdr\n}\n\nfunc newUberHeader(h http.Header, tId []string) http.Header {\n\tretHdr := http.Header{trcUber: tId}\n\tfor k, v := range h {\n\t\tif strings.HasPrefix(k, trcUberCtxPrefix) {\n\t\t\tretHdr[k] = v\n\t\t}\n\t}\n\treturn retHdr\n}\n\nfunc newTraceCtxHeader(h http.Header, tId []string) http.Header {\n\tretHdr := http.Header{trcCtx: tId}\n\tif v, ok := h[trcCtxSt]; ok {\n\t\tretHdr[trcCtxSt] = v\n\t}\n\treturn retHdr\n}\n\n// Helper to determine when to sample. When header has a value, sampling is driven by header\nfunc shouldSample(l *serviceLatency, c *client) (bool, http.Header) {\n\tif l == nil {\n\t\treturn false, nil\n\t}\n\tif l.sampling < 0 {\n\t\treturn false, nil\n\t}\n\tif l.sampling >= 100 {\n\t\treturn true, nil\n\t}\n\tif l.sampling > 0 && rand.Int31n(100) <= int32(l.sampling) {\n\t\treturn true, nil\n\t}\n\th := c.parseState.getHeader()\n\tif len(h) == 0 {\n\t\treturn false, nil\n\t}\n\tif tId := h[trcUber]; len(tId) != 0 {\n\t\t// sample 479fefe9525eddb:5adb976bfc1f95c1:479fefe9525eddb:1\n\t\ttk := strings.Split(tId[0], \":\")\n\t\tif len(tk) == 4 && len(tk[3]) > 0 && len(tk[3]) <= 2 {\n\t\t\tdst := [2]byte{}\n\t\t\tsrc := [2]byte{'0', tk[3][0]}\n\t\t\tif len(tk[3]) == 2 {\n\t\t\t\tsrc[1] = tk[3][1]\n\t\t\t}\n\t\t\tif _, err := hex.Decode(dst[:], src[:]); err == nil && dst[0]&1 == 1 {\n\t\t\t\treturn true, newUberHeader(h, tId)\n\t\t\t}\n\t\t}\n\t\treturn false, nil\n\t} else if sampled := h[trcB3Sm]; len(sampled) != 0 && sampled[0] == \"1\" {\n\t\treturn true, newB3Header(h) // allowed\n\t} else if len(sampled) != 0 && sampled[0] == \"0\" {\n\t\treturn false, nil // denied\n\t} else if _, ok := h[trcB3Id]; ok {\n\t\t// sample 80f198ee56343ba864fe8b2a57d3eff7\n\t\t// presence (with X-B3-Sampled not being 0) means sampling left to recipient\n\t\treturn true, newB3Header(h)\n\t} else if b3 := h[trcB3]; len(b3) != 0 {\n\t\t// sample 80f198ee56343ba864fe8b2a57d3eff7-e457b5a2e4d86bd1-1-05e3ac9a4f6e3b90\n\t\t// sample 0\n\t\ttk := strings.Split(b3[0], \"-\")\n\t\tif len(tk) > 2 && tk[2] == \"0\" {\n\t\t\treturn false, nil // denied\n\t\t} else if len(tk) == 1 && tk[0] == \"0\" {\n\t\t\treturn false, nil // denied\n\t\t}\n\t\treturn true, http.Header{trcB3: b3} // sampling allowed or left to recipient of header\n\t} else if tId := h[trcCtx]; len(tId) != 0 {\n\t\tvar sample bool\n\t\t// sample 00-4bf92f3577b34da6a3ce929d0e0e4736-00f067aa0ba902b7-01\n\t\ttk := strings.Split(tId[0], \"-\")\n\t\tif len(tk) == 4 && len([]byte(tk[3])) == 2 {\n\t\t\tif hexVal, err := strconv.ParseInt(tk[3], 16, 8); err == nil {\n\t\t\t\tsample = hexVal&0x1 == 0x1\n\t\t\t}\n\t\t}\n\t\tif sample {\n\t\t\treturn true, newTraceCtxHeader(h, tId)\n\t\t} else {\n\t\t\treturn false, nil\n\t\t}\n\t}\n\treturn false, nil\n}\n\n// Used to mimic client like replies.\nconst (\n\treplyPrefix    = \"_R_.\"\n\treplyPrefixLen = len(replyPrefix)\n\tbaseServerLen  = 10\n\treplyLen       = 6\n\tminReplyLen    = 15\n\tdigits         = \"0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\"\n\tbase           = 62\n)\n\n// This is where all service export responses are handled.\nfunc (a *Account) processServiceImportResponse(sub *subscription, c *client, _ *Account, subject, reply string, msg []byte) {\n\ta.mu.RLock()\n\tif a.expired.Load() || len(a.exports.responses) == 0 {\n\t\ta.mu.RUnlock()\n\t\treturn\n\t}\n\tsi := a.exports.responses[subject]\n\n\tif si == nil || si.invalid {\n\t\ta.mu.RUnlock()\n\t\treturn\n\t}\n\ta.mu.RUnlock()\n\n\t// Send for normal processing.\n\tc.processServiceImport(si, a, msg)\n}\n\n// Will create the response prefix for fast generation of responses.\n// A wildcard subscription may be used handle interest graph propagation\n// for all service replies, unless we are bound to a leafnode.\n// Lock should be held.\nfunc (a *Account) createRespWildcard() {\n\tvar b = [baseServerLen]byte{'_', 'R', '_', '.'}\n\trn := fastrand.Uint64()\n\tfor i, l := replyPrefixLen, rn; i < len(b); i++ {\n\t\tb[i] = digits[l%base]\n\t\tl /= base\n\t}\n\ta.siReply = append(b[:], '.')\n}\n\n// Test whether this is a tracked reply.\nfunc isTrackedReply(reply []byte) bool {\n\tlreply := len(reply) - 1\n\treturn lreply > 3 && reply[lreply-1] == '.' && reply[lreply] == 'T'\n}\n\n// Generate a new service reply from the wildcard prefix.\n// FIXME(dlc) - probably do not have to use rand here. about 25ns per.\nfunc (a *Account) newServiceReply(tracking bool) []byte {\n\ta.mu.Lock()\n\ts := a.srv\n\trn := fastrand.Uint64()\n\n\t// Check if we need to create the reply here.\n\tvar createdSiReply bool\n\tif a.siReply == nil {\n\t\ta.createRespWildcard()\n\t\tcreatedSiReply = true\n\t}\n\treplyPre := a.siReply\n\ta.mu.Unlock()\n\n\t// If we created the siReply and we are not bound to a leafnode\n\t// we need to do the wildcard subscription.\n\tif createdSiReply {\n\t\ta.subscribeServiceImportResponse(string(append(replyPre, '>')))\n\t}\n\n\tvar b [replyLen]byte\n\tfor i, l := 0, rn; i < len(b); i++ {\n\t\tb[i] = digits[l%base]\n\t\tl /= base\n\t}\n\t// Make sure to copy.\n\treply := make([]byte, 0, len(replyPre)+len(b))\n\treply = append(reply, replyPre...)\n\treply = append(reply, b[:]...)\n\n\tif tracking && s.sys != nil {\n\t\t// Add in our tracking identifier. This allows the metrics to get back to only\n\t\t// this server without needless SUBS/UNSUBS.\n\t\treply = append(reply, '.')\n\t\treply = append(reply, s.sys.shash...)\n\t\treply = append(reply, '.', 'T')\n\t}\n\n\treturn reply\n}\n\n// Checks if a serviceImport was created to map responses.\nfunc (si *serviceImport) isRespServiceImport() bool {\n\treturn si != nil && si.response\n}\n\n// Sets the response threshold timer for a service export.\n// Account lock should be held\nfunc (se *serviceExport) setResponseThresholdTimer() {\n\tif se.rtmr != nil {\n\t\treturn // Already set\n\t}\n\tse.rtmr = time.AfterFunc(se.respThresh, se.checkExpiredResponses)\n}\n\n// Account lock should be held\nfunc (se *serviceExport) clearResponseThresholdTimer() bool {\n\tif se.rtmr == nil {\n\t\treturn true\n\t}\n\tstopped := se.rtmr.Stop()\n\tse.rtmr = nil\n\treturn stopped\n}\n\n// checkExpiredResponses will check for any pending responses that need to\n// be cleaned up.\nfunc (se *serviceExport) checkExpiredResponses() {\n\tacc := se.acc\n\tif acc == nil {\n\t\tse.clearResponseThresholdTimer()\n\t\treturn\n\t}\n\n\tvar expired []*serviceImport\n\tmints := time.Now().UnixNano() - int64(se.respThresh)\n\n\t// TODO(dlc) - Should we release lock while doing this? Or only do these in batches?\n\t// Should we break this up for responses only from this service export?\n\t// Responses live on acc directly for fast inbound processsing for the _R_ wildcard.\n\t// We could do another indirection at this level but just to get to the service export?\n\tvar totalResponses int\n\tacc.mu.RLock()\n\tfor _, si := range acc.exports.responses {\n\t\tif si.se == se {\n\t\t\ttotalResponses++\n\t\t\tif si.ts <= mints {\n\t\t\t\texpired = append(expired, si)\n\t\t\t}\n\t\t}\n\t}\n\tacc.mu.RUnlock()\n\n\tfor _, si := range expired {\n\t\tacc.removeRespServiceImport(si, rsiTimeout)\n\t}\n\n\t// Pull out expired to determine if we have any left for timer.\n\ttotalResponses -= len(expired)\n\n\t// Redo timer as needed.\n\tacc.mu.Lock()\n\tif totalResponses > 0 && se.rtmr != nil {\n\t\tse.rtmr.Stop()\n\t\tse.rtmr.Reset(se.respThresh)\n\t} else {\n\t\tse.clearResponseThresholdTimer()\n\t}\n\tacc.mu.Unlock()\n}\n\n// ServiceExportResponseThreshold returns the current threshold.\nfunc (a *Account) ServiceExportResponseThreshold(export string) (time.Duration, error) {\n\ta.mu.Lock()\n\tdefer a.mu.Unlock()\n\tse := a.getServiceExport(export)\n\tif se == nil {\n\t\treturn 0, fmt.Errorf(\"no export defined for %q\", export)\n\t}\n\treturn se.respThresh, nil\n}\n\n// SetServiceExportResponseThreshold sets the maximum time the system will a response to be delivered\n// from a service export responder.\nfunc (a *Account) SetServiceExportResponseThreshold(export string, maxTime time.Duration) error {\n\ta.mu.Lock()\n\tif a.isClaimAccount() {\n\t\ta.mu.Unlock()\n\t\treturn fmt.Errorf(\"claim based accounts can not be updated directly\")\n\t}\n\tlrt := a.lowestServiceExportResponseTime()\n\tse := a.getServiceExport(export)\n\tif se == nil {\n\t\ta.mu.Unlock()\n\t\treturn fmt.Errorf(\"no export defined for %q\", export)\n\t}\n\tse.respThresh = maxTime\n\n\tvar clients []*client\n\tnlrt := a.lowestServiceExportResponseTime()\n\tif nlrt != lrt && len(a.clients) > 0 {\n\t\tclients = a.getClientsLocked()\n\t}\n\t// Need to release because lock ordering is client -> Account\n\ta.mu.Unlock()\n\tif len(clients) > 0 {\n\t\tupdateAllClientsServiceExportResponseTime(clients, nlrt)\n\t}\n\treturn nil\n}\n\nfunc (a *Account) SetServiceExportAllowTrace(export string, allowTrace bool) error {\n\ta.mu.Lock()\n\tse := a.getServiceExport(export)\n\tif se == nil {\n\t\ta.mu.Unlock()\n\t\treturn fmt.Errorf(\"no export defined for %q\", export)\n\t}\n\tse.atrc = allowTrace\n\ta.mu.Unlock()\n\treturn nil\n}\n\n// This is for internal service import responses.\nfunc (a *Account) addRespServiceImport(dest *Account, to string, osi *serviceImport, tracking bool, header http.Header) *serviceImport {\n\tnrr := string(osi.acc.newServiceReply(tracking))\n\n\ta.mu.Lock()\n\trt := osi.rt\n\n\t// dest is the requestor's account. a is the service responder with the export.\n\t// Marked as internal here, that is how we distinguish.\n\tsi := &serviceImport{dest, nil, osi.se, nil, nrr, to, nil, 0, rt, nil, nil, nil, false, true, false, osi.share, false, false, false, nil}\n\n\tif a.exports.responses == nil {\n\t\ta.exports.responses = make(map[string]*serviceImport)\n\t}\n\ta.exports.responses[nrr] = si\n\n\t// Always grab time and make sure response threshold timer is running.\n\tsi.ts = time.Now().UnixNano()\n\tif osi.se != nil {\n\t\tosi.se.setResponseThresholdTimer()\n\t}\n\n\tif rt == Singleton && tracking {\n\t\tsi.latency = osi.latency\n\t\tsi.tracking = true\n\t\tsi.trackingHdr = header\n\t}\n\ta.mu.Unlock()\n\n\t// We do add in the reverse map such that we can detect loss of interest and do proper\n\t// cleanup of this si as interest goes away.\n\tdest.addReverseRespMapEntry(a, to, nrr)\n\n\treturn si\n}\n\n// AddStreamImportWithClaim will add in the stream import from a specific account with optional token.\nfunc (a *Account) AddStreamImportWithClaim(account *Account, from, prefix string, imClaim *jwt.Import) error {\n\treturn a.addStreamImportWithClaim(account, from, prefix, false, imClaim)\n}\n\nfunc (a *Account) addStreamImportWithClaim(account *Account, from, prefix string, allowTrace bool, imClaim *jwt.Import) error {\n\tif account == nil {\n\t\treturn ErrMissingAccount\n\t}\n\n\t// First check to see if the account has authorized export of the subject.\n\tif !account.checkStreamImportAuthorized(a, from, imClaim) {\n\t\treturn ErrStreamImportAuthorization\n\t}\n\n\t// Check prefix if it exists and make sure its a literal.\n\t// Append token separator if not already present.\n\tif prefix != _EMPTY_ {\n\t\t// Make sure there are no wildcards here, this prefix needs to be a literal\n\t\t// since it will be prepended to a publish subject.\n\t\tif !subjectIsLiteral(prefix) {\n\t\t\treturn ErrStreamImportBadPrefix\n\t\t}\n\t\tif prefix[len(prefix)-1] != btsep {\n\t\t\tprefix = prefix + string(btsep)\n\t\t}\n\t}\n\n\treturn a.addMappedStreamImportWithClaim(account, from, prefix+from, allowTrace, imClaim)\n}\n\n// AddMappedStreamImport helper for AddMappedStreamImportWithClaim\nfunc (a *Account) AddMappedStreamImport(account *Account, from, to string) error {\n\treturn a.AddMappedStreamImportWithClaim(account, from, to, nil)\n}\n\n// AddMappedStreamImportWithClaim will add in the stream import from a specific account with optional token.\nfunc (a *Account) AddMappedStreamImportWithClaim(account *Account, from, to string, imClaim *jwt.Import) error {\n\treturn a.addMappedStreamImportWithClaim(account, from, to, false, imClaim)\n}\n\nfunc (a *Account) addMappedStreamImportWithClaim(account *Account, from, to string, allowTrace bool, imClaim *jwt.Import) error {\n\tif account == nil {\n\t\treturn ErrMissingAccount\n\t}\n\n\t// First check to see if the account has authorized export of the subject.\n\tif !account.checkStreamImportAuthorized(a, from, imClaim) {\n\t\treturn ErrStreamImportAuthorization\n\t}\n\n\tif to == _EMPTY_ {\n\t\tto = from\n\t}\n\n\t// Check if this forms a cycle.\n\tif err := a.streamImportFormsCycle(account, to); err != nil {\n\t\treturn err\n\t}\n\n\tif err := a.streamImportFormsCycle(account, from); err != nil {\n\t\treturn err\n\t}\n\n\tvar (\n\t\tusePub bool\n\t\ttr     *subjectTransform\n\t\terr    error\n\t)\n\tif subjectHasWildcard(from) {\n\t\tif to == from {\n\t\t\tusePub = true\n\t\t} else {\n\t\t\t// Create a transform\n\t\t\tif tr, err = NewSubjectTransformStrict(from, transformTokenize(to)); err != nil {\n\t\t\t\treturn fmt.Errorf(\"failed to create mapping transform for stream import subject from %q to %q: %v\",\n\t\t\t\t\tfrom, to, err)\n\t\t\t}\n\t\t\tto, _ = transformUntokenize(to)\n\t\t}\n\t}\n\n\ta.mu.Lock()\n\tif a.isStreamImportDuplicate(account, from) {\n\t\ta.mu.Unlock()\n\t\treturn ErrStreamImportDuplicate\n\t}\n\tif imClaim != nil {\n\t\tallowTrace = imClaim.AllowTrace\n\t}\n\ta.imports.streams = append(a.imports.streams, &streamImport{account, from, to, tr, nil, imClaim, usePub, false, allowTrace})\n\ta.mu.Unlock()\n\treturn nil\n}\n\n// isStreamImportDuplicate checks for duplicate.\n// Lock should be held.\nfunc (a *Account) isStreamImportDuplicate(acc *Account, from string) bool {\n\tfor _, si := range a.imports.streams {\n\t\tif si.acc == acc && si.from == from {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\n// AddStreamImport will add in the stream import from a specific account.\nfunc (a *Account) AddStreamImport(account *Account, from, prefix string) error {\n\treturn a.addStreamImportWithClaim(account, from, prefix, false, nil)\n}\n\n// IsPublicExport is a placeholder to denote a public export.\nvar IsPublicExport = []*Account(nil)\n\n// AddStreamExport will add an export to the account. If accounts is nil\n// it will signify a public export, meaning anyone can import.\nfunc (a *Account) AddStreamExport(subject string, accounts []*Account) error {\n\treturn a.addStreamExportWithAccountPos(subject, accounts, 0)\n}\n\n// AddStreamExport will add an export to the account. If accounts is nil\n// it will signify a public export, meaning anyone can import.\n// if accountPos is > 0, all imports will be granted where the following holds:\n// strings.Split(subject, tsep)[accountPos] == account id will be granted.\nfunc (a *Account) addStreamExportWithAccountPos(subject string, accounts []*Account, accountPos uint) error {\n\tif a == nil {\n\t\treturn ErrMissingAccount\n\t}\n\n\ta.mu.Lock()\n\tdefer a.mu.Unlock()\n\n\tif a.exports.streams == nil {\n\t\ta.exports.streams = make(map[string]*streamExport)\n\t}\n\tea := a.exports.streams[subject]\n\tif accounts != nil || accountPos > 0 {\n\t\tif ea == nil {\n\t\t\tea = &streamExport{}\n\t\t}\n\t\tif err := setExportAuth(&ea.exportAuth, subject, accounts, accountPos); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\ta.exports.streams[subject] = ea\n\treturn nil\n}\n\n// Check if another account is authorized to import from us.\nfunc (a *Account) checkStreamImportAuthorized(account *Account, subject string, imClaim *jwt.Import) bool {\n\t// Find the subject in the exports list.\n\ta.mu.RLock()\n\tauth := a.checkStreamImportAuthorizedNoLock(account, subject, imClaim)\n\ta.mu.RUnlock()\n\treturn auth\n}\n\nfunc (a *Account) checkStreamImportAuthorizedNoLock(account *Account, subject string, imClaim *jwt.Import) bool {\n\tif a.exports.streams == nil || !IsValidSubject(subject) {\n\t\treturn false\n\t}\n\treturn a.checkStreamExportApproved(account, subject, imClaim)\n}\n\nfunc (a *Account) checkAuth(ea *exportAuth, account *Account, imClaim *jwt.Import, tokens []string) bool {\n\t// if ea is nil or ea.approved is nil, that denotes a public export\n\tif ea == nil || (len(ea.approved) == 0 && !ea.tokenReq && ea.accountPos == 0) {\n\t\treturn true\n\t}\n\t// Check if the export is protected and enforces presence of importing account identity\n\tif ea.accountPos > 0 {\n\t\treturn ea.accountPos <= uint(len(tokens)) && tokens[ea.accountPos-1] == account.Name\n\t}\n\t// Check if token required\n\tif ea.tokenReq {\n\t\treturn a.checkActivation(account, imClaim, ea, true)\n\t}\n\tif ea.approved == nil {\n\t\treturn false\n\t}\n\t// If we have a matching account we are authorized\n\t_, ok := ea.approved[account.Name]\n\treturn ok\n}\n\nfunc (a *Account) checkStreamExportApproved(account *Account, subject string, imClaim *jwt.Import) bool {\n\t// Check direct match of subject first\n\tea, ok := a.exports.streams[subject]\n\tif ok {\n\t\t// if ea is nil or eq.approved is nil, that denotes a public export\n\t\tif ea == nil {\n\t\t\treturn true\n\t\t}\n\t\treturn a.checkAuth(&ea.exportAuth, account, imClaim, nil)\n\t}\n\n\t// ok if we are here we did not match directly so we need to test each one.\n\t// The import subject arg has to take precedence, meaning the export\n\t// has to be a true subset of the import claim. We already checked for\n\t// exact matches above.\n\ttokens := strings.Split(subject, tsep)\n\tfor subj, ea := range a.exports.streams {\n\t\tif isSubsetMatch(tokens, subj) {\n\t\t\tif ea == nil {\n\t\t\t\treturn true\n\t\t\t}\n\t\t\treturn a.checkAuth(&ea.exportAuth, account, imClaim, tokens)\n\t\t}\n\t}\n\treturn false\n}\n\nfunc (a *Account) checkServiceExportApproved(account *Account, subject string, imClaim *jwt.Import) bool {\n\t// Check direct match of subject first\n\tse, ok := a.exports.services[subject]\n\tif ok {\n\t\t// if se is nil or eq.approved is nil, that denotes a public export\n\t\tif se == nil {\n\t\t\treturn true\n\t\t}\n\t\treturn a.checkAuth(&se.exportAuth, account, imClaim, nil)\n\t}\n\t// ok if we are here we did not match directly so we need to test each one.\n\t// The import subject arg has to take precedence, meaning the export\n\t// has to be a true subset of the import claim. We already checked for\n\t// exact matches above.\n\ttokens := strings.Split(subject, tsep)\n\tfor subj, se := range a.exports.services {\n\t\tif isSubsetMatch(tokens, subj) {\n\t\t\tif se == nil {\n\t\t\t\treturn true\n\t\t\t}\n\t\t\treturn a.checkAuth(&se.exportAuth, account, imClaim, tokens)\n\t\t}\n\t}\n\treturn false\n}\n\n// Helper function to get a serviceExport.\n// Lock should be held on entry.\nfunc (a *Account) getServiceExport(subj string) *serviceExport {\n\tse, ok := a.exports.services[subj]\n\t// The export probably has a wildcard, so lookup that up.\n\tif !ok {\n\t\tse = a.getWildcardServiceExport(subj)\n\t}\n\treturn se\n}\n\n// This helper is used when trying to match a serviceExport record that is\n// represented by a wildcard.\n// Lock should be held on entry.\nfunc (a *Account) getWildcardServiceExport(from string) *serviceExport {\n\ttokens := strings.Split(from, tsep)\n\tfor subj, se := range a.exports.services {\n\t\tif isSubsetMatch(tokens, subj) {\n\t\t\treturn se\n\t\t}\n\t}\n\treturn nil\n}\n\n// These are import stream specific versions for when an activation expires.\nfunc (a *Account) streamActivationExpired(exportAcc *Account, subject string) {\n\ta.mu.RLock()\n\tif a.expired.Load() || a.imports.streams == nil {\n\t\ta.mu.RUnlock()\n\t\treturn\n\t}\n\tvar si *streamImport\n\tfor _, si = range a.imports.streams {\n\t\tif si.acc == exportAcc && si.from == subject {\n\t\t\tbreak\n\t\t}\n\t}\n\n\tif si == nil || si.invalid {\n\t\ta.mu.RUnlock()\n\t\treturn\n\t}\n\ta.mu.RUnlock()\n\n\tif si.acc.checkActivation(a, si.claim, nil, false) {\n\t\t// The token has been updated most likely and we are good to go.\n\t\treturn\n\t}\n\n\ta.mu.Lock()\n\tsi.invalid = true\n\tclients := a.getClientsLocked()\n\tawcsti := map[string]struct{}{a.Name: {}}\n\ta.mu.Unlock()\n\tfor _, c := range clients {\n\t\tc.processSubsOnConfigReload(awcsti)\n\t}\n}\n\n// These are import service specific versions for when an activation expires.\nfunc (a *Account) serviceActivationExpired(dstAcc *Account, subject string) {\n\ta.mu.RLock()\n\tif a.expired.Load() || a.imports.services == nil {\n\t\ta.mu.RUnlock()\n\t\treturn\n\t}\n\tsi := a.getServiceImportForAccountLocked(dstAcc.Name, subject)\n\tif si == nil || si.invalid {\n\t\ta.mu.RUnlock()\n\t\treturn\n\t}\n\ta.mu.RUnlock()\n\n\tif si.acc.checkActivation(a, si.claim, nil, false) {\n\t\t// The token has been updated most likely and we are good to go.\n\t\treturn\n\t}\n\n\ta.mu.Lock()\n\tsi.invalid = true\n\ta.mu.Unlock()\n}\n\n// Fires for expired activation tokens. We could track this with timers etc.\n// Instead we just re-analyze where we are and if we need to act.\nfunc (a *Account) activationExpired(exportAcc *Account, subject string, kind jwt.ExportType) {\n\tswitch kind {\n\tcase jwt.Stream:\n\t\ta.streamActivationExpired(exportAcc, subject)\n\tcase jwt.Service:\n\t\ta.serviceActivationExpired(exportAcc, subject)\n\t}\n}\n\nfunc isRevoked(revocations map[string]int64, subject string, issuedAt int64) bool {\n\tif len(revocations) == 0 {\n\t\treturn false\n\t}\n\tif t, ok := revocations[subject]; !ok || t < issuedAt {\n\t\tif t, ok := revocations[jwt.All]; !ok || t < issuedAt {\n\t\t\treturn false\n\t\t}\n\t}\n\treturn true\n}\n\n// checkActivation will check the activation token for validity.\n// ea may only be nil in cases where revocation may not be checked, say triggered by expiration timer.\nfunc (a *Account) checkActivation(importAcc *Account, claim *jwt.Import, ea *exportAuth, expTimer bool) bool {\n\tif claim == nil || claim.Token == _EMPTY_ {\n\t\treturn false\n\t}\n\t// Create a quick clone so we can inline Token JWT.\n\tclone := *claim\n\n\tvr := jwt.CreateValidationResults()\n\tclone.Validate(importAcc.Name, vr)\n\tif vr.IsBlocking(true) {\n\t\treturn false\n\t}\n\tact, err := jwt.DecodeActivationClaims(clone.Token)\n\tif err != nil {\n\t\treturn false\n\t}\n\tif !a.isIssuerClaimTrusted(act) {\n\t\treturn false\n\t}\n\tvr = jwt.CreateValidationResults()\n\tact.Validate(vr)\n\tif vr.IsBlocking(true) {\n\t\treturn false\n\t}\n\tif act.Expires != 0 {\n\t\ttn := time.Now().Unix()\n\t\tif act.Expires <= tn {\n\t\t\treturn false\n\t\t}\n\t\tif expTimer {\n\t\t\texpiresAt := time.Duration(act.Expires - tn)\n\t\t\ttime.AfterFunc(expiresAt*time.Second, func() {\n\t\t\t\timportAcc.activationExpired(a, string(act.ImportSubject), claim.Type)\n\t\t\t})\n\t\t}\n\t}\n\tif ea == nil {\n\t\treturn true\n\t}\n\t// Check for token revocation..\n\treturn !isRevoked(ea.actsRevoked, act.Subject, act.IssuedAt)\n}\n\n// Returns true if the activation claim is trusted. That is the issuer matches\n// the account or is an entry in the signing keys.\nfunc (a *Account) isIssuerClaimTrusted(claims *jwt.ActivationClaims) bool {\n\t// if no issuer account, issuer is the account\n\tif claims.IssuerAccount == _EMPTY_ {\n\t\treturn true\n\t}\n\t// If the IssuerAccount is not us, then this is considered an error.\n\tif a.Name != claims.IssuerAccount {\n\t\tif a.srv != nil {\n\t\t\ta.srv.Errorf(\"Invalid issuer account %q in activation claim (subject: %q - type: %q) for account %q\",\n\t\t\t\tclaims.IssuerAccount, claims.Activation.ImportSubject, claims.Activation.ImportType, a.Name)\n\t\t}\n\t\treturn false\n\t}\n\t_, ok := a.hasIssuerNoLock(claims.Issuer)\n\treturn ok\n}\n\n// Returns true if `a` and `b` stream imports are the same. Note that the\n// check is done with the account's name, not the pointer. This is used\n// during config reload where we are comparing current and new config\n// in which pointers are different.\n// Acquires `a` read lock, but `b` is assumed to not be accessed\n// by anyone but the caller (`b` is not registered anywhere).\nfunc (a *Account) checkStreamImportsEqual(b *Account) bool {\n\ta.mu.RLock()\n\tdefer a.mu.RUnlock()\n\n\tif len(a.imports.streams) != len(b.imports.streams) {\n\t\treturn false\n\t}\n\t// Load the b imports into a map index by what we are looking for.\n\tbm := make(map[string]*streamImport, len(b.imports.streams))\n\tfor _, bim := range b.imports.streams {\n\t\tbm[bim.acc.Name+bim.from+bim.to] = bim\n\t}\n\tfor _, aim := range a.imports.streams {\n\t\tif bim, ok := bm[aim.acc.Name+aim.from+aim.to]; !ok {\n\t\t\treturn false\n\t\t} else if aim.atrc != bim.atrc {\n\t\t\treturn false\n\t\t}\n\t}\n\treturn true\n}\n\n// Returns true if `a` and `b` stream exports are the same.\n// Acquires `a` read lock, but `b` is assumed to not be accessed\n// by anyone but the caller (`b` is not registered anywhere).\nfunc (a *Account) checkStreamExportsEqual(b *Account) bool {\n\ta.mu.RLock()\n\tdefer a.mu.RUnlock()\n\tif len(a.exports.streams) != len(b.exports.streams) {\n\t\treturn false\n\t}\n\tfor subj, aea := range a.exports.streams {\n\t\tbea, ok := b.exports.streams[subj]\n\t\tif !ok {\n\t\t\treturn false\n\t\t}\n\t\tif !isStreamExportEqual(aea, bea) {\n\t\t\treturn false\n\t\t}\n\t}\n\treturn true\n}\n\nfunc isStreamExportEqual(a, b *streamExport) bool {\n\tif a == nil && b == nil {\n\t\treturn true\n\t}\n\tif (a == nil && b != nil) || (a != nil && b == nil) {\n\t\treturn false\n\t}\n\treturn isExportAuthEqual(&a.exportAuth, &b.exportAuth)\n}\n\n// Returns true if `a` and `b` service exports are the same.\n// Acquires `a` read lock, but `b` is assumed to not be accessed\n// by anyone but the caller (`b` is not registered anywhere).\nfunc (a *Account) checkServiceExportsEqual(b *Account) bool {\n\ta.mu.RLock()\n\tdefer a.mu.RUnlock()\n\tif len(a.exports.services) != len(b.exports.services) {\n\t\treturn false\n\t}\n\tfor subj, aea := range a.exports.services {\n\t\tbea, ok := b.exports.services[subj]\n\t\tif !ok {\n\t\t\treturn false\n\t\t}\n\t\tif !isServiceExportEqual(aea, bea) {\n\t\t\treturn false\n\t\t}\n\t}\n\treturn true\n}\n\nfunc isServiceExportEqual(a, b *serviceExport) bool {\n\tif a == nil && b == nil {\n\t\treturn true\n\t}\n\tif (a == nil && b != nil) || (a != nil && b == nil) {\n\t\treturn false\n\t}\n\tif !isExportAuthEqual(&a.exportAuth, &b.exportAuth) {\n\t\treturn false\n\t}\n\tif a.acc.Name != b.acc.Name {\n\t\treturn false\n\t}\n\tif a.respType != b.respType {\n\t\treturn false\n\t}\n\tif a.latency != nil || b.latency != nil {\n\t\tif (a.latency != nil && b.latency == nil) || (a.latency == nil && b.latency != nil) {\n\t\t\treturn false\n\t\t}\n\t\tif a.latency.sampling != b.latency.sampling {\n\t\t\treturn false\n\t\t}\n\t\tif a.latency.subject != b.latency.subject {\n\t\t\treturn false\n\t\t}\n\t}\n\tif a.atrc != b.atrc {\n\t\treturn false\n\t}\n\treturn true\n}\n\n// Returns true if `a` and `b` exportAuth structures are equal.\n// Both `a` and `b` are guaranteed to be non-nil.\n// Locking is handled by the caller.\nfunc isExportAuthEqual(a, b *exportAuth) bool {\n\tif a.tokenReq != b.tokenReq {\n\t\treturn false\n\t}\n\tif a.accountPos != b.accountPos {\n\t\treturn false\n\t}\n\tif len(a.approved) != len(b.approved) {\n\t\treturn false\n\t}\n\tfor ak, av := range a.approved {\n\t\tif bv, ok := b.approved[ak]; !ok || av.Name != bv.Name {\n\t\t\treturn false\n\t\t}\n\t}\n\tif len(a.actsRevoked) != len(b.actsRevoked) {\n\t\treturn false\n\t}\n\tfor ak, av := range a.actsRevoked {\n\t\tif bv, ok := b.actsRevoked[ak]; !ok || av != bv {\n\t\t\treturn false\n\t\t}\n\t}\n\treturn true\n}\n\n// Check if another account is authorized to route requests to this service.\nfunc (a *Account) checkServiceImportAuthorized(account *Account, subject string, imClaim *jwt.Import) bool {\n\ta.mu.RLock()\n\tauthorized := a.checkServiceImportAuthorizedNoLock(account, subject, imClaim)\n\ta.mu.RUnlock()\n\treturn authorized\n}\n\n// Check if another account is authorized to route requests to this service.\nfunc (a *Account) checkServiceImportAuthorizedNoLock(account *Account, subject string, imClaim *jwt.Import) bool {\n\t// Find the subject in the services list.\n\tif a.exports.services == nil {\n\t\treturn false\n\t}\n\treturn a.checkServiceExportApproved(account, subject, imClaim)\n}\n\n// IsExpired returns expiration status.\nfunc (a *Account) IsExpired() bool {\n\treturn a.expired.Load()\n}\n\n// Called when an account has expired.\nfunc (a *Account) expiredTimeout() {\n\t// Mark expired first.\n\ta.expired.Store(true)\n\n\t// Collect the clients and expire them.\n\tcs := a.getClients()\n\tfor _, c := range cs {\n\t\tif !isInternalClient(c.kind) {\n\t\t\tc.accountAuthExpired()\n\t\t}\n\t}\n}\n\n// Sets the expiration timer for an account JWT that has it set.\nfunc (a *Account) setExpirationTimer(d time.Duration) {\n\ta.etmr = time.AfterFunc(d, a.expiredTimeout)\n}\n\n// Lock should be held\nfunc (a *Account) clearExpirationTimer() bool {\n\tif a.etmr == nil {\n\t\treturn true\n\t}\n\tstopped := a.etmr.Stop()\n\ta.etmr = nil\n\treturn stopped\n}\n\n// checkUserRevoked will check if a user has been revoked.\nfunc (a *Account) checkUserRevoked(nkey string, issuedAt int64) bool {\n\ta.mu.RLock()\n\tdefer a.mu.RUnlock()\n\treturn isRevoked(a.usersRevoked, nkey, issuedAt)\n}\n\n// failBearer will return if bearer token are allowed (false) or disallowed (true)\nfunc (a *Account) failBearer() bool {\n\ta.mu.RLock()\n\tdefer a.mu.RUnlock()\n\treturn a.disallowBearer\n}\n\n// Check expiration and set the proper state as needed.\nfunc (a *Account) checkExpiration(claims *jwt.ClaimsData) {\n\ta.mu.Lock()\n\tdefer a.mu.Unlock()\n\n\ta.clearExpirationTimer()\n\tif claims.Expires == 0 {\n\t\ta.expired.Store(false)\n\t\treturn\n\t}\n\ttn := time.Now().Unix()\n\tif claims.Expires <= tn {\n\t\ta.expired.Store(true)\n\t\treturn\n\t}\n\texpiresAt := time.Duration(claims.Expires - tn)\n\ta.setExpirationTimer(expiresAt * time.Second)\n\ta.expired.Store(false)\n}\n\n// hasIssuer returns true if the issuer matches the account\n// If the issuer is a scoped signing key, the scope will be returned as well\n// issuer or it is a signing key for the account.\nfunc (a *Account) hasIssuer(issuer string) (jwt.Scope, bool) {\n\ta.mu.RLock()\n\tscope, ok := a.hasIssuerNoLock(issuer)\n\ta.mu.RUnlock()\n\treturn scope, ok\n}\n\n// hasIssuerNoLock is the unlocked version of hasIssuer\nfunc (a *Account) hasIssuerNoLock(issuer string) (jwt.Scope, bool) {\n\tscope, ok := a.signingKeys[issuer]\n\treturn scope, ok\n}\n\n// Returns the loop detection subject used for leafnodes\nfunc (a *Account) getLDSubject() string {\n\ta.mu.RLock()\n\tlds := a.lds\n\ta.mu.RUnlock()\n\treturn lds\n}\n\n// Placeholder for signaling token auth required.\nvar tokenAuthReq = []*Account{}\n\nfunc authAccounts(tokenReq bool) []*Account {\n\tif tokenReq {\n\t\treturn tokenAuthReq\n\t}\n\treturn nil\n}\n\n// SetAccountResolver will assign the account resolver.\nfunc (s *Server) SetAccountResolver(ar AccountResolver) {\n\ts.mu.Lock()\n\ts.accResolver = ar\n\ts.mu.Unlock()\n}\n\n// AccountResolver returns the registered account resolver.\nfunc (s *Server) AccountResolver() AccountResolver {\n\ts.mu.RLock()\n\tar := s.accResolver\n\ts.mu.RUnlock()\n\treturn ar\n}\n\n// isClaimAccount returns if this account is backed by a JWT claim.\n// Lock should be held.\nfunc (a *Account) isClaimAccount() bool {\n\treturn a.claimJWT != _EMPTY_\n}\n\n// UpdateAccountClaims will update an existing account with new claims.\n// This will replace any exports or imports previously defined.\n// Lock MUST NOT be held upon entry.\nfunc (s *Server) UpdateAccountClaims(a *Account, ac *jwt.AccountClaims) {\n\ts.updateAccountClaimsWithRefresh(a, ac, true)\n}\n\nfunc (a *Account) traceLabel() string {\n\tif a == nil {\n\t\treturn _EMPTY_\n\t}\n\tif a.nameTag != _EMPTY_ {\n\t\treturn fmt.Sprintf(\"%s/%s\", a.Name, a.nameTag)\n\t}\n\treturn a.Name\n}\n\n// Check if an account has external auth set.\n// Operator/Account Resolver only.\nfunc (a *Account) hasExternalAuth() bool {\n\tif a == nil {\n\t\treturn false\n\t}\n\ta.mu.RLock()\n\tdefer a.mu.RUnlock()\n\treturn a.extAuth != nil\n}\n\n// Deterimine if this is an external auth user.\nfunc (a *Account) isExternalAuthUser(userID string) bool {\n\tif a == nil {\n\t\treturn false\n\t}\n\ta.mu.RLock()\n\tdefer a.mu.RUnlock()\n\tif a.extAuth != nil {\n\t\tfor _, u := range a.extAuth.AuthUsers {\n\t\t\tif userID == u {\n\t\t\t\treturn true\n\t\t\t}\n\t\t}\n\t}\n\treturn false\n}\n\n// Return the external authorization xkey if external authorization is enabled and the xkey is set.\n// Operator/Account Resolver only.\nfunc (a *Account) externalAuthXKey() string {\n\tif a == nil {\n\t\treturn _EMPTY_\n\t}\n\ta.mu.RLock()\n\tdefer a.mu.RUnlock()\n\tif a.extAuth != nil && a.extAuth.XKey != _EMPTY_ {\n\t\treturn a.extAuth.XKey\n\t}\n\treturn _EMPTY_\n}\n\n// Check if an account switch for external authorization is allowed.\nfunc (a *Account) isAllowedAcount(acc string) bool {\n\tif a == nil {\n\t\treturn false\n\t}\n\ta.mu.RLock()\n\tdefer a.mu.RUnlock()\n\tif a.extAuth != nil {\n\t\t// if we have a single allowed account, and we have a wildcard\n\t\t// we accept it\n\t\tif len(a.extAuth.AllowedAccounts) == 1 &&\n\t\t\ta.extAuth.AllowedAccounts[0] == jwt.AnyAccount {\n\t\t\treturn true\n\t\t}\n\t\t// otherwise must match exactly\n\t\tfor _, a := range a.extAuth.AllowedAccounts {\n\t\t\tif a == acc {\n\t\t\t\treturn true\n\t\t\t}\n\t\t}\n\t}\n\treturn false\n}\n\n// updateAccountClaimsWithRefresh will update an existing account with new claims.\n// If refreshImportingAccounts is true it will also update incomplete dependent accounts\n// This will replace any exports or imports previously defined.\n// Lock MUST NOT be held upon entry.\nfunc (s *Server) updateAccountClaimsWithRefresh(a *Account, ac *jwt.AccountClaims, refreshImportingAccounts bool) {\n\tif a == nil {\n\t\treturn\n\t}\n\ts.Debugf(\"Updating account claims: %s/%s\", a.Name, ac.Name)\n\ta.checkExpiration(ac.Claims())\n\n\ta.mu.Lock()\n\t// Clone to update, only select certain fields.\n\told := &Account{Name: a.Name, exports: a.exports, limits: a.limits, signingKeys: a.signingKeys}\n\n\t// overwrite claim meta data\n\ta.nameTag = ac.Name\n\ta.tags = ac.Tags\n\n\t// Grab trace label under lock.\n\ttl := a.traceLabel()\n\n\tvar td string\n\tvar tds int\n\tif ac.Trace != nil {\n\t\t// Update trace destination and sampling\n\t\ttd, tds = string(ac.Trace.Destination), ac.Trace.Sampling\n\t\tif !IsValidPublishSubject(td) {\n\t\t\ttd, tds = _EMPTY_, 0\n\t\t} else if tds <= 0 || tds > 100 {\n\t\t\ttds = 100\n\t\t}\n\t}\n\ta.traceDest, a.traceDestSampling = td, tds\n\n\t// Check for external authorization.\n\tif ac.HasExternalAuthorization() {\n\t\ta.extAuth = &jwt.ExternalAuthorization{}\n\t\ta.extAuth.AuthUsers.Add(ac.Authorization.AuthUsers...)\n\t\ta.extAuth.AllowedAccounts.Add(ac.Authorization.AllowedAccounts...)\n\t\ta.extAuth.XKey = ac.Authorization.XKey\n\t}\n\n\t// Reset exports and imports here.\n\n\t// Exports is creating a whole new map.\n\ta.exports = exportMap{}\n\n\t// Imports are checked unlocked in processInbound, so we can't change out the struct here. Need to process inline.\n\tif a.imports.streams != nil {\n\t\told.imports.streams = a.imports.streams\n\t\ta.imports.streams = nil\n\t}\n\tif a.imports.services != nil {\n\t\told.imports.services = make(map[string][]*serviceImport, len(a.imports.services))\n\t\tfor k, v := range a.imports.services {\n\t\t\tsis := append([]*serviceImport(nil), v...)\n\t\t\told.imports.services[k] = sis\n\t\t\tdelete(a.imports.services, k)\n\t\t}\n\t}\n\n\talteredScope := map[string]struct{}{}\n\n\t// update account signing keys\n\ta.signingKeys = nil\n\t_, strict := s.strictSigningKeyUsage[a.Issuer]\n\tif len(ac.SigningKeys) > 0 || !strict {\n\t\ta.signingKeys = make(map[string]jwt.Scope)\n\t}\n\tsignersChanged := false\n\tfor k, scope := range ac.SigningKeys {\n\t\ta.signingKeys[k] = scope\n\t}\n\tif !strict {\n\t\ta.signingKeys[a.Name] = nil\n\t}\n\tif len(a.signingKeys) != len(old.signingKeys) {\n\t\tsignersChanged = true\n\t}\n\tfor k, scope := range a.signingKeys {\n\t\tif oldScope, ok := old.signingKeys[k]; !ok {\n\t\t\tsignersChanged = true\n\t\t} else if !reflect.DeepEqual(scope, oldScope) {\n\t\t\tsignersChanged = true\n\t\t\talteredScope[k] = struct{}{}\n\t\t}\n\t}\n\t// collect mappings that need to be removed\n\tremoveList := []string{}\n\tfor _, m := range a.mappings {\n\t\tif _, ok := ac.Mappings[jwt.Subject(m.src)]; !ok {\n\t\t\tremoveList = append(removeList, m.src)\n\t\t}\n\t}\n\ta.mu.Unlock()\n\n\tfor sub, wm := range ac.Mappings {\n\t\tmappings := make([]*MapDest, len(wm))\n\t\tfor i, m := range wm {\n\t\t\tmappings[i] = &MapDest{\n\t\t\t\tSubject: string(m.Subject),\n\t\t\t\tWeight:  m.GetWeight(),\n\t\t\t\tCluster: m.Cluster,\n\t\t\t}\n\t\t}\n\t\t// This will overwrite existing entries\n\t\ta.AddWeightedMappings(string(sub), mappings...)\n\t}\n\t// remove mappings\n\tfor _, rmMapping := range removeList {\n\t\ta.RemoveMapping(rmMapping)\n\t}\n\n\t// Re-register system exports/imports.\n\tif a == s.SystemAccount() {\n\t\ts.addSystemAccountExports(a)\n\t} else {\n\t\ts.registerSystemImports(a)\n\t}\n\n\tjsEnabled := s.JetStreamEnabled()\n\n\tstreamTokenExpirationChanged := false\n\tserviceTokenExpirationChanged := false\n\n\tfor _, e := range ac.Exports {\n\t\tswitch e.Type {\n\t\tcase jwt.Stream:\n\t\t\ts.Debugf(\"Adding stream export %q for %s\", e.Subject, tl)\n\t\t\tif err := a.addStreamExportWithAccountPos(\n\t\t\t\tstring(e.Subject), authAccounts(e.TokenReq), e.AccountTokenPosition); err != nil {\n\t\t\t\ts.Debugf(\"Error adding stream export to account [%s]: %v\", tl, err.Error())\n\t\t\t}\n\t\tcase jwt.Service:\n\t\t\ts.Debugf(\"Adding service export %q for %s\", e.Subject, tl)\n\t\t\trt := Singleton\n\t\t\tswitch e.ResponseType {\n\t\t\tcase jwt.ResponseTypeStream:\n\t\t\t\trt = Streamed\n\t\t\tcase jwt.ResponseTypeChunked:\n\t\t\t\trt = Chunked\n\t\t\t}\n\t\t\tif err := a.addServiceExportWithResponseAndAccountPos(\n\t\t\t\tstring(e.Subject), rt, authAccounts(e.TokenReq), e.AccountTokenPosition); err != nil {\n\t\t\t\ts.Debugf(\"Error adding service export to account [%s]: %v\", tl, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tsub := string(e.Subject)\n\t\t\tif e.Latency != nil {\n\t\t\t\tif err := a.TrackServiceExportWithSampling(sub, string(e.Latency.Results), int(e.Latency.Sampling)); err != nil {\n\t\t\t\t\thdrNote := _EMPTY_\n\t\t\t\t\tif e.Latency.Sampling == jwt.Headers {\n\t\t\t\t\t\thdrNote = \" (using headers)\"\n\t\t\t\t\t}\n\t\t\t\t\ts.Debugf(\"Error adding latency tracking%s for service export to account [%s]: %v\", hdrNote, tl, err)\n\t\t\t\t}\n\t\t\t}\n\t\t\tif e.ResponseThreshold != 0 {\n\t\t\t\t// Response threshold was set in options.\n\t\t\t\tif err := a.SetServiceExportResponseThreshold(sub, e.ResponseThreshold); err != nil {\n\t\t\t\t\ts.Debugf(\"Error adding service export response threshold for [%s]: %v\", tl, err)\n\t\t\t\t}\n\t\t\t}\n\t\t\tif err := a.SetServiceExportAllowTrace(sub, e.AllowTrace); err != nil {\n\t\t\t\ts.Debugf(\"Error adding allow_trace for %q: %v\", sub, err)\n\t\t\t}\n\t\t}\n\n\t\tvar revocationChanged *bool\n\t\tvar ea *exportAuth\n\n\t\ta.mu.Lock()\n\t\tswitch e.Type {\n\t\tcase jwt.Stream:\n\t\t\trevocationChanged = &streamTokenExpirationChanged\n\t\t\tif se, ok := a.exports.streams[string(e.Subject)]; ok && se != nil {\n\t\t\t\tea = &se.exportAuth\n\t\t\t}\n\t\tcase jwt.Service:\n\t\t\trevocationChanged = &serviceTokenExpirationChanged\n\t\t\tif se, ok := a.exports.services[string(e.Subject)]; ok && se != nil {\n\t\t\t\tea = &se.exportAuth\n\t\t\t}\n\t\t}\n\t\tif ea != nil {\n\t\t\toldRevocations := ea.actsRevoked\n\t\t\tif len(e.Revocations) == 0 {\n\t\t\t\t// remove all, no need to evaluate existing imports\n\t\t\t\tea.actsRevoked = nil\n\t\t\t} else if len(oldRevocations) == 0 {\n\t\t\t\t// add all, existing imports need to be re evaluated\n\t\t\t\tea.actsRevoked = e.Revocations\n\t\t\t\t*revocationChanged = true\n\t\t\t} else {\n\t\t\t\tea.actsRevoked = e.Revocations\n\t\t\t\t// diff, existing imports need to be conditionally re evaluated, depending on:\n\t\t\t\t// if a key was added, or it's timestamp increased\n\t\t\t\tfor k, t := range e.Revocations {\n\t\t\t\t\tif tOld, ok := oldRevocations[k]; !ok || tOld < t {\n\t\t\t\t\t\t*revocationChanged = true\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\ta.mu.Unlock()\n\t}\n\tvar incompleteImports []*jwt.Import\n\tfor _, i := range ac.Imports {\n\t\tacc, err := s.lookupAccount(i.Account)\n\t\tif acc == nil || err != nil {\n\t\t\ts.Errorf(\"Can't locate account [%s] for import of [%v] %s (err=%v)\", i.Account, i.Subject, i.Type, err)\n\t\t\tincompleteImports = append(incompleteImports, i)\n\t\t\tcontinue\n\t\t}\n\t\t// Capture trace labels.\n\t\tacc.mu.RLock()\n\t\tatl := acc.traceLabel()\n\t\tacc.mu.RUnlock()\n\t\t// Grab from and to\n\t\tfrom, to := string(i.Subject), i.GetTo()\n\t\tswitch i.Type {\n\t\tcase jwt.Stream:\n\t\t\tif i.LocalSubject != _EMPTY_ {\n\t\t\t\t// set local subject implies to is empty\n\t\t\t\tto = string(i.LocalSubject)\n\t\t\t\ts.Debugf(\"Adding stream import %s:%q for %s:%q\", atl, from, tl, to)\n\t\t\t\terr = a.AddMappedStreamImportWithClaim(acc, from, to, i)\n\t\t\t} else {\n\t\t\t\ts.Debugf(\"Adding stream import %s:%q for %s:%q\", atl, from, tl, to)\n\t\t\t\terr = a.AddStreamImportWithClaim(acc, from, to, i)\n\t\t\t}\n\t\t\tif err != nil {\n\t\t\t\ts.Debugf(\"Error adding stream import to account [%s]: %v\", tl, err.Error())\n\t\t\t\tincompleteImports = append(incompleteImports, i)\n\t\t\t}\n\t\tcase jwt.Service:\n\t\t\tif i.LocalSubject != _EMPTY_ {\n\t\t\t\tfrom = string(i.LocalSubject)\n\t\t\t\tto = string(i.Subject)\n\t\t\t}\n\t\t\ts.Debugf(\"Adding service import %s:%q for %s:%q\", atl, from, tl, to)\n\t\t\tif err := a.AddServiceImportWithClaim(acc, from, to, i); err != nil {\n\t\t\t\ts.Debugf(\"Error adding service import to account [%s]: %v\", tl, err.Error())\n\t\t\t\tincompleteImports = append(incompleteImports, i)\n\t\t\t}\n\t\t}\n\t}\n\t// Now let's apply any needed changes from import/export changes.\n\tif !a.checkStreamImportsEqual(old) {\n\t\tawcsti := map[string]struct{}{a.Name: {}}\n\t\tfor _, c := range a.getClients() {\n\t\t\tc.processSubsOnConfigReload(awcsti)\n\t\t}\n\t}\n\t// Now check if stream exports have changed.\n\tif !a.checkStreamExportsEqual(old) || signersChanged || streamTokenExpirationChanged {\n\t\tclients := map[*client]struct{}{}\n\t\t// We need to check all accounts that have an import claim from this account.\n\t\tawcsti := map[string]struct{}{}\n\n\t\t// We must only allow one goroutine to go through here, otherwise we could deadlock\n\t\t// due to locking two accounts in succession.\n\t\ts.mu.Lock()\n\t\ts.accounts.Range(func(k, v any) bool {\n\t\t\tacc := v.(*Account)\n\t\t\t// Move to the next if this account is actually account \"a\".\n\t\t\tif acc.Name == a.Name {\n\t\t\t\treturn true\n\t\t\t}\n\t\t\tacc.mu.Lock()\n\t\t\tfor _, im := range acc.imports.streams {\n\t\t\t\tif im != nil && im.acc.Name == a.Name {\n\t\t\t\t\t// Check for if we are still authorized for an import.\n\t\t\t\t\tim.invalid = !a.checkStreamImportAuthorized(acc, im.from, im.claim)\n\t\t\t\t\tawcsti[acc.Name] = struct{}{}\n\t\t\t\t\tfor c := range acc.clients {\n\t\t\t\t\t\tclients[c] = struct{}{}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tacc.mu.Unlock()\n\t\t\treturn true\n\t\t})\n\t\ts.mu.Unlock()\n\t\t// Now walk clients.\n\t\tfor c := range clients {\n\t\t\tc.processSubsOnConfigReload(awcsti)\n\t\t}\n\t}\n\t// Now check if service exports have changed.\n\tif !a.checkServiceExportsEqual(old) || signersChanged || serviceTokenExpirationChanged {\n\t\t// We must only allow one goroutine to go through here, otherwise we could deadlock\n\t\t// due to locking two accounts in succession.\n\t\ts.mu.Lock()\n\t\ts.accounts.Range(func(k, v any) bool {\n\t\t\tacc := v.(*Account)\n\t\t\t// Move to the next if this account is actually account \"a\".\n\t\t\tif acc.Name == a.Name {\n\t\t\t\treturn true\n\t\t\t}\n\t\t\tacc.mu.Lock()\n\t\t\tfor _, sis := range acc.imports.services {\n\t\t\t\tfor _, si := range sis {\n\t\t\t\t\tif si != nil && si.acc.Name == a.Name {\n\t\t\t\t\t\t// Check for if we are still authorized for an import.\n\t\t\t\t\t\tsi.invalid = !a.checkServiceImportAuthorized(acc, si.to, si.claim)\n\t\t\t\t\t\t// Make sure we should still be tracking latency and if we\n\t\t\t\t\t\t// are allowed to trace.\n\t\t\t\t\t\tif !si.response {\n\t\t\t\t\t\t\ta.mu.RLock()\n\t\t\t\t\t\t\tif se := a.getServiceExport(si.to); se != nil {\n\t\t\t\t\t\t\t\tif si.latency != nil {\n\t\t\t\t\t\t\t\t\tsi.latency = se.latency\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t// Update allow trace.\n\t\t\t\t\t\t\t\tsi.atrc = se.atrc\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\ta.mu.RUnlock()\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tacc.mu.Unlock()\n\t\t\treturn true\n\t\t})\n\t\ts.mu.Unlock()\n\t}\n\n\t// Now make sure we shutdown the old service import subscriptions.\n\tvar sids [][]byte\n\ta.mu.RLock()\n\tc := a.ic\n\tif c != nil {\n\t\tfor _, sis := range old.imports.services {\n\t\t\tfor _, si := range sis {\n\t\t\t\tif si.sid != nil {\n\t\t\t\t\tsids = append(sids, si.sid)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\ta.mu.RUnlock()\n\tfor _, sid := range sids {\n\t\tc.processUnsub(sid)\n\t}\n\n\t// Now do limits if they are present.\n\ta.mu.Lock()\n\ta.msubs = int32(ac.Limits.Subs)\n\ta.mpay = int32(ac.Limits.Payload)\n\ta.mconns = int32(ac.Limits.Conn)\n\ta.mleafs = int32(ac.Limits.LeafNodeConn)\n\ta.disallowBearer = ac.Limits.DisallowBearer\n\t// Check for any revocations\n\tif len(ac.Revocations) > 0 {\n\t\t// We will always replace whatever we had with most current, so no\n\t\t// need to look at what we have.\n\t\ta.usersRevoked = make(map[string]int64, len(ac.Revocations))\n\t\tfor pk, t := range ac.Revocations {\n\t\t\ta.usersRevoked[pk] = t\n\t\t}\n\t} else {\n\t\ta.usersRevoked = nil\n\t}\n\ta.defaultPerms = buildPermissionsFromJwt(&ac.DefaultPermissions)\n\ta.incomplete = len(incompleteImports) != 0\n\tfor _, i := range incompleteImports {\n\t\ts.incompleteAccExporterMap.Store(i.Account, struct{}{})\n\t}\n\tif a.srv == nil {\n\t\ta.srv = s\n\t}\n\n\tif ac.Limits.IsJSEnabled() {\n\t\ttoUnlimited := func(value int64) int64 {\n\t\t\tif value > 0 {\n\t\t\t\treturn value\n\t\t\t}\n\t\t\treturn -1\n\t\t}\n\t\tif ac.Limits.JetStreamLimits.DiskStorage != 0 || ac.Limits.JetStreamLimits.MemoryStorage != 0 {\n\t\t\t// JetStreamAccountLimits and jwt.JetStreamLimits use same value for unlimited\n\t\t\ta.jsLimits = map[string]JetStreamAccountLimits{\n\t\t\t\t_EMPTY_: {\n\t\t\t\t\tMaxMemory:            ac.Limits.JetStreamLimits.MemoryStorage,\n\t\t\t\t\tMaxStore:             ac.Limits.JetStreamLimits.DiskStorage,\n\t\t\t\t\tMaxStreams:           int(ac.Limits.JetStreamLimits.Streams),\n\t\t\t\t\tMaxConsumers:         int(ac.Limits.JetStreamLimits.Consumer),\n\t\t\t\t\tMemoryMaxStreamBytes: toUnlimited(ac.Limits.JetStreamLimits.MemoryMaxStreamBytes),\n\t\t\t\t\tStoreMaxStreamBytes:  toUnlimited(ac.Limits.JetStreamLimits.DiskMaxStreamBytes),\n\t\t\t\t\tMaxBytesRequired:     ac.Limits.JetStreamLimits.MaxBytesRequired,\n\t\t\t\t\tMaxAckPending:        int(toUnlimited(ac.Limits.JetStreamLimits.MaxAckPending)),\n\t\t\t\t},\n\t\t\t}\n\t\t} else {\n\t\t\ta.jsLimits = map[string]JetStreamAccountLimits{}\n\t\t\tfor t, l := range ac.Limits.JetStreamTieredLimits {\n\t\t\t\ta.jsLimits[t] = JetStreamAccountLimits{\n\t\t\t\t\tMaxMemory:            l.MemoryStorage,\n\t\t\t\t\tMaxStore:             l.DiskStorage,\n\t\t\t\t\tMaxStreams:           int(l.Streams),\n\t\t\t\t\tMaxConsumers:         int(l.Consumer),\n\t\t\t\t\tMemoryMaxStreamBytes: toUnlimited(l.MemoryMaxStreamBytes),\n\t\t\t\t\tStoreMaxStreamBytes:  toUnlimited(l.DiskMaxStreamBytes),\n\t\t\t\t\tMaxBytesRequired:     l.MaxBytesRequired,\n\t\t\t\t\tMaxAckPending:        int(toUnlimited(l.MaxAckPending)),\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t} else if a.jsLimits != nil {\n\t\t// covers failed update followed by disable\n\t\ta.jsLimits = nil\n\t}\n\n\ta.updated = time.Now()\n\tclients := a.getClientsLocked()\n\tajs := a.js\n\ta.mu.Unlock()\n\n\t// Sort if we are over the limit.\n\tif a.MaxTotalConnectionsReached() {\n\t\tslices.SortFunc(clients, func(i, j *client) int { return -i.start.Compare(j.start) }) // sort in reverse order\n\t}\n\n\t// If JetStream is enabled for this server we will call into configJetStream for the account\n\t// regardless of enabled or disabled. It handles both cases.\n\tif jsEnabled {\n\t\tif err := s.configJetStream(a); err != nil {\n\t\t\ts.Errorf(\"Error configuring jetstream for account [%s]: %v\", tl, err.Error())\n\t\t\ta.mu.Lock()\n\t\t\t// Absent reload of js server cfg, this is going to be broken until js is disabled\n\t\t\ta.incomplete = true\n\t\t\ta.mu.Unlock()\n\t\t}\n\t} else if a.jsLimits != nil {\n\t\t// We do not have JS enabled for this server, but the account has it enabled so setup\n\t\t// our imports properly. This allows this server to proxy JS traffic correctly.\n\t\ts.checkJetStreamExports()\n\t\ta.enableAllJetStreamServiceImportsAndMappings()\n\t}\n\n\tif ajs != nil {\n\t\t// Check whether the account NRG status changed. If it has then we need to notify the\n\t\t// Raft groups running on the system so that they can move their subs if needed.\n\t\ta.mu.Lock()\n\t\tprevious := a.nrgAccount\n\t\tswitch ac.ClusterTraffic {\n\t\tcase \"system\", _EMPTY_:\n\t\t\ta.nrgAccount = _EMPTY_\n\t\tcase \"owner\":\n\t\t\ta.nrgAccount = a.Name\n\t\tdefault:\n\t\t\ts.Errorf(\"Account claim for %q has invalid value %q for cluster traffic account\", a.Name, ac.ClusterTraffic)\n\t\t}\n\t\tchanged := a.nrgAccount != previous\n\t\ta.mu.Unlock()\n\t\tif changed {\n\t\t\ts.updateNRGAccountStatus()\n\t\t}\n\t}\n\n\tcount := 0\n\tfor _, c := range clients {\n\t\ta.mu.RLock()\n\t\texceeded := a.mconns != jwt.NoLimit && count >= int(a.mconns)\n\t\ta.mu.RUnlock()\n\t\t// Only kick non-internal clients.\n\t\tif !isInternalClient(c.kind) {\n\t\t\tif exceeded {\n\t\t\t\tc.maxAccountConnExceeded()\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tcount++\n\t\t}\n\t\tc.mu.Lock()\n\t\tc.applyAccountLimits()\n\t\t// if we have an nkey user we are a callout user - save\n\t\t// the issuedAt, and nkey user id to honor revocations\n\t\tvar nkeyUserID string\n\t\tvar issuedAt int64\n\t\tif c.user != nil {\n\t\t\tissuedAt = c.user.Issued\n\t\t\tnkeyUserID = c.user.Nkey\n\t\t}\n\t\ttheJWT := c.opts.JWT\n\t\tc.mu.Unlock()\n\t\t// Check for being revoked here. We use ac one to avoid the account lock.\n\t\tif (ac.Revocations != nil || ac.Limits.DisallowBearer) && theJWT != _EMPTY_ {\n\t\t\tif juc, err := jwt.DecodeUserClaims(theJWT); err != nil {\n\t\t\t\tc.Debugf(\"User JWT not valid: %v\", err)\n\t\t\t\tc.authViolation()\n\t\t\t\tcontinue\n\t\t\t} else if juc.BearerToken && ac.Limits.DisallowBearer {\n\t\t\t\tc.Debugf(\"Bearer User JWT not allowed\")\n\t\t\t\tc.authViolation()\n\t\t\t\tcontinue\n\t\t\t} else if ok := ac.IsClaimRevoked(juc); ok {\n\t\t\t\tc.sendErrAndDebug(\"User Authentication Revoked\")\n\t\t\t\tc.closeConnection(Revocation)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t}\n\n\t\t// if we extracted nkeyUserID and issuedAt we are a callout type\n\t\t// calloutIAT should only be set if we are in callout scenario as\n\t\t// the user JWT is _NOT_ associated with the client for callouts,\n\t\t// so we rely on the calloutIAT to know when the JWT was issued\n\t\t// revocations simply state that JWT issued before or by that date\n\t\t// are not valid\n\t\tif ac.Revocations != nil && nkeyUserID != _EMPTY_ && issuedAt > 0 {\n\t\t\tseconds, ok := ac.Revocations[jwt.All]\n\t\t\tif ok && seconds >= issuedAt {\n\t\t\t\tc.sendErrAndDebug(\"User Authentication Revoked\")\n\t\t\t\tc.closeConnection(Revocation)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tseconds, ok = ac.Revocations[nkeyUserID]\n\t\t\tif ok && seconds >= issuedAt {\n\t\t\t\tc.sendErrAndDebug(\"User Authentication Revoked\")\n\t\t\t\tc.closeConnection(Revocation)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t}\n\t}\n\n\t// Check if the signing keys changed, might have to evict\n\tif signersChanged {\n\t\tfor _, c := range clients {\n\t\t\tc.mu.Lock()\n\t\t\tif c.user == nil {\n\t\t\t\tc.mu.Unlock()\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tsk := c.user.SigningKey\n\t\t\tc.mu.Unlock()\n\t\t\tif sk == _EMPTY_ {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif _, ok := alteredScope[sk]; ok {\n\t\t\t\tc.closeConnection(AuthenticationViolation)\n\t\t\t} else if _, ok := a.hasIssuer(sk); !ok {\n\t\t\t\tc.closeConnection(AuthenticationViolation)\n\t\t\t}\n\t\t}\n\t}\n\n\tif _, ok := s.incompleteAccExporterMap.Load(old.Name); ok && refreshImportingAccounts {\n\t\ts.incompleteAccExporterMap.Delete(old.Name)\n\t\ts.accounts.Range(func(key, value any) bool {\n\t\t\tacc := value.(*Account)\n\t\t\tacc.mu.RLock()\n\t\t\tincomplete := acc.incomplete\n\t\t\tname := acc.Name\n\t\t\tlabel := acc.traceLabel()\n\t\t\t// Must use jwt in account or risk failing on fetch\n\t\t\t// This jwt may not be the same that caused exportingAcc to be in incompleteAccExporterMap\n\t\t\tclaimJWT := acc.claimJWT\n\t\t\tacc.mu.RUnlock()\n\t\t\tif incomplete && name != old.Name {\n\t\t\t\tif accClaims, _, err := s.verifyAccountClaims(claimJWT); err == nil {\n\t\t\t\t\t// Since claimJWT has not changed, acc can become complete\n\t\t\t\t\t// but it won't alter incomplete for it's dependents accounts.\n\t\t\t\t\ts.updateAccountClaimsWithRefresh(acc, accClaims, false)\n\t\t\t\t\t// old.Name was deleted before ranging over accounts\n\t\t\t\t\t// If it exists again, UpdateAccountClaims set it for failed imports of acc.\n\t\t\t\t\t// So there was one import of acc that imported this account and failed again.\n\t\t\t\t\t// Since this account just got updated, the import itself may be in error. So trace that.\n\t\t\t\t\tif _, ok := s.incompleteAccExporterMap.Load(old.Name); ok {\n\t\t\t\t\t\ts.incompleteAccExporterMap.Delete(old.Name)\n\t\t\t\t\t\ts.Errorf(\"Account %s has issues importing account %s\", label, old.Name)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn true\n\t\t})\n\t}\n}\n\n// Helper to build an internal account structure from a jwt.AccountClaims.\n// Lock MUST NOT be held upon entry.\nfunc (s *Server) buildInternalAccount(ac *jwt.AccountClaims) *Account {\n\tacc := NewAccount(ac.Subject)\n\tacc.Issuer = ac.Issuer\n\t// Set this here since we are placing in s.tmpAccounts below and may be\n\t// referenced by an route RS+, etc.\n\ts.setAccountSublist(acc)\n\n\t// We don't want to register an account that is in the process of\n\t// being built, however, to solve circular import dependencies, we\n\t// need to store it here.\n\tif v, loaded := s.tmpAccounts.LoadOrStore(ac.Subject, acc); loaded {\n\t\treturn v.(*Account)\n\t}\n\n\t// Update based on claims.\n\ts.UpdateAccountClaims(acc, ac)\n\n\treturn acc\n}\n\n// Helper to build Permissions from jwt.Permissions\n// or return nil if none were specified\nfunc buildPermissionsFromJwt(uc *jwt.Permissions) *Permissions {\n\tif uc == nil {\n\t\treturn nil\n\t}\n\tvar p *Permissions\n\tif len(uc.Pub.Allow) > 0 || len(uc.Pub.Deny) > 0 {\n\t\tp = &Permissions{}\n\t\tp.Publish = &SubjectPermission{}\n\t\tp.Publish.Allow = uc.Pub.Allow\n\t\tp.Publish.Deny = uc.Pub.Deny\n\t}\n\tif len(uc.Sub.Allow) > 0 || len(uc.Sub.Deny) > 0 {\n\t\tif p == nil {\n\t\t\tp = &Permissions{}\n\t\t}\n\t\tp.Subscribe = &SubjectPermission{}\n\t\tp.Subscribe.Allow = uc.Sub.Allow\n\t\tp.Subscribe.Deny = uc.Sub.Deny\n\t}\n\tif uc.Resp != nil {\n\t\tif p == nil {\n\t\t\tp = &Permissions{}\n\t\t}\n\t\tp.Response = &ResponsePermission{\n\t\t\tMaxMsgs: uc.Resp.MaxMsgs,\n\t\t\tExpires: uc.Resp.Expires,\n\t\t}\n\t\tvalidateResponsePermissions(p)\n\t}\n\treturn p\n}\n\n// Helper to build internal NKeyUser.\nfunc buildInternalNkeyUser(uc *jwt.UserClaims, acts map[string]struct{}, acc *Account) *NkeyUser {\n\tnu := &NkeyUser{Nkey: uc.Subject, Account: acc, AllowedConnectionTypes: acts, Issued: uc.IssuedAt}\n\tif uc.IssuerAccount != _EMPTY_ {\n\t\tnu.SigningKey = uc.Issuer\n\t}\n\n\t// Now check for permissions.\n\tvar p = buildPermissionsFromJwt(&uc.Permissions)\n\tif p == nil && acc.defaultPerms != nil {\n\t\tp = acc.defaultPerms.clone()\n\t}\n\tnu.Permissions = p\n\treturn nu\n}\n\nfunc fetchAccount(res AccountResolver, name string) (string, error) {\n\tif !nkeys.IsValidPublicAccountKey(name) {\n\t\treturn _EMPTY_, fmt.Errorf(\"will only fetch valid account keys\")\n\t}\n\treturn res.Fetch(copyString(name))\n}\n\n// AccountResolver interface. This is to fetch Account JWTs by public nkeys\ntype AccountResolver interface {\n\tFetch(name string) (string, error)\n\tStore(name, jwt string) error\n\tIsReadOnly() bool\n\tStart(server *Server) error\n\tIsTrackingUpdate() bool\n\tReload() error\n\tClose()\n}\n\n// Default implementations of IsReadOnly/Start so only need to be written when changed\ntype resolverDefaultsOpsImpl struct{}\n\nfunc (*resolverDefaultsOpsImpl) IsReadOnly() bool {\n\treturn true\n}\n\nfunc (*resolverDefaultsOpsImpl) IsTrackingUpdate() bool {\n\treturn false\n}\n\nfunc (*resolverDefaultsOpsImpl) Start(*Server) error {\n\treturn nil\n}\n\nfunc (*resolverDefaultsOpsImpl) Reload() error {\n\treturn nil\n}\n\nfunc (*resolverDefaultsOpsImpl) Close() {\n}\n\nfunc (*resolverDefaultsOpsImpl) Store(_, _ string) error {\n\treturn fmt.Errorf(\"store operation not supported for URL Resolver\")\n}\n\n// MemAccResolver is a memory only resolver.\n// Mostly for testing.\ntype MemAccResolver struct {\n\tsm sync.Map\n\tresolverDefaultsOpsImpl\n}\n\n// Fetch will fetch the account jwt claims from the internal sync.Map.\nfunc (m *MemAccResolver) Fetch(name string) (string, error) {\n\tif j, ok := m.sm.Load(name); ok {\n\t\treturn j.(string), nil\n\t}\n\treturn _EMPTY_, ErrMissingAccount\n}\n\n// Store will store the account jwt claims in the internal sync.Map.\nfunc (m *MemAccResolver) Store(name, jwt string) error {\n\tm.sm.Store(name, jwt)\n\treturn nil\n}\n\nfunc (m *MemAccResolver) IsReadOnly() bool {\n\treturn false\n}\n\n// URLAccResolver implements an http fetcher.\ntype URLAccResolver struct {\n\turl string\n\tc   *http.Client\n\tresolverDefaultsOpsImpl\n}\n\n// NewURLAccResolver returns a new resolver for the given base URL.\nfunc NewURLAccResolver(url string) (*URLAccResolver, error) {\n\tif !strings.HasSuffix(url, \"/\") {\n\t\turl += \"/\"\n\t}\n\t// FIXME(dlc) - Make timeout and others configurable.\n\t// We create our own transport to amortize TLS.\n\ttr := &http.Transport{\n\t\tMaxIdleConns:    10,\n\t\tIdleConnTimeout: 30 * time.Second,\n\t}\n\tur := &URLAccResolver{\n\t\turl: url,\n\t\tc:   &http.Client{Timeout: DEFAULT_ACCOUNT_FETCH_TIMEOUT, Transport: tr},\n\t}\n\treturn ur, nil\n}\n\n// Fetch will fetch the account jwt claims from the base url, appending the\n// account name onto the end.\nfunc (ur *URLAccResolver) Fetch(name string) (string, error) {\n\turl := ur.url + name\n\tresp, err := ur.c.Get(url)\n\tif err != nil {\n\t\treturn _EMPTY_, fmt.Errorf(\"could not fetch <%q>: %v\", redactURLString(url), err)\n\t} else if resp == nil {\n\t\treturn _EMPTY_, fmt.Errorf(\"could not fetch <%q>: no response\", redactURLString(url))\n\t}\n\tdefer resp.Body.Close()\n\tif resp.StatusCode != http.StatusOK {\n\t\treturn _EMPTY_, fmt.Errorf(\"could not fetch <%q>: %v\", redactURLString(url), resp.Status)\n\t}\n\tbody, err := io.ReadAll(resp.Body)\n\tif err != nil {\n\t\treturn _EMPTY_, err\n\t}\n\treturn string(body), nil\n}\n\n// Resolver based on nats for synchronization and backing directory for storage.\ntype DirAccResolver struct {\n\t*DirJWTStore\n\t*Server\n\tsyncInterval time.Duration\n\tfetchTimeout time.Duration\n}\n\nfunc (dr *DirAccResolver) IsTrackingUpdate() bool {\n\treturn true\n}\n\nfunc (dr *DirAccResolver) Reload() error {\n\treturn dr.DirJWTStore.Reload()\n}\n\n// ServerAPIClaimUpdateResponse is the response to $SYS.REQ.ACCOUNT.<id>.CLAIMS.UPDATE and $SYS.REQ.CLAIMS.UPDATE\ntype ServerAPIClaimUpdateResponse struct {\n\tServer *ServerInfo        `json:\"server\"`\n\tData   *ClaimUpdateStatus `json:\"data,omitempty\"`\n\tError  *ClaimUpdateError  `json:\"error,omitempty\"`\n}\n\ntype ClaimUpdateError struct {\n\tAccount     string `json:\"account,omitempty\"`\n\tCode        int    `json:\"code\"`\n\tDescription string `json:\"description,omitempty\"`\n}\n\ntype ClaimUpdateStatus struct {\n\tAccount string `json:\"account,omitempty\"`\n\tCode    int    `json:\"code,omitempty\"`\n\tMessage string `json:\"message,omitempty\"`\n}\n\nfunc respondToUpdate(s *Server, respSubj string, acc string, message string, err error) {\n\tif err == nil {\n\t\tif acc == _EMPTY_ {\n\t\t\ts.Debugf(\"%s\", message)\n\t\t} else {\n\t\t\ts.Debugf(\"%s - %s\", message, acc)\n\t\t}\n\t} else {\n\t\tif acc == _EMPTY_ {\n\t\t\ts.Errorf(\"%s - %s\", message, err)\n\t\t} else {\n\t\t\ts.Errorf(\"%s - %s - %s\", message, acc, err)\n\t\t}\n\t}\n\tif respSubj == _EMPTY_ {\n\t\treturn\n\t}\n\n\tresponse := ServerAPIClaimUpdateResponse{\n\t\tServer: &ServerInfo{},\n\t}\n\n\tif err == nil {\n\t\tresponse.Data = &ClaimUpdateStatus{\n\t\t\tAccount: acc,\n\t\t\tCode:    http.StatusOK,\n\t\t\tMessage: message,\n\t\t}\n\t} else {\n\t\tresponse.Error = &ClaimUpdateError{\n\t\t\tAccount:     acc,\n\t\t\tCode:        http.StatusInternalServerError,\n\t\t\tDescription: fmt.Sprintf(\"%s - %v\", message, err),\n\t\t}\n\t}\n\n\ts.sendInternalMsgLocked(respSubj, _EMPTY_, response.Server, response)\n}\n\nfunc handleListRequest(store *DirJWTStore, s *Server, reply string) {\n\tif reply == _EMPTY_ {\n\t\treturn\n\t}\n\taccIds := make([]string, 0, 1024)\n\tif err := store.PackWalk(1, func(partialPackMsg string) {\n\t\tif tk := strings.Split(partialPackMsg, \"|\"); len(tk) == 2 {\n\t\t\taccIds = append(accIds, tk[0])\n\t\t}\n\t}); err != nil {\n\t\t// let them timeout\n\t\ts.Errorf(\"list request error: %v\", err)\n\t} else {\n\t\ts.Debugf(\"list request responded with %d account ids\", len(accIds))\n\t\tserver := &ServerInfo{}\n\t\tresponse := map[string]any{\"server\": server, \"data\": accIds}\n\t\ts.sendInternalMsgLocked(reply, _EMPTY_, server, response)\n\t}\n}\n\nfunc handleDeleteRequest(store *DirJWTStore, s *Server, msg []byte, reply string) {\n\tvar accIds []any\n\tvar subj, sysAccName string\n\tif sysAcc := s.SystemAccount(); sysAcc != nil {\n\t\tsysAccName = sysAcc.GetName()\n\t}\n\t// Only operator and operator signing key are allowed to delete\n\tgk, err := jwt.DecodeGeneric(string(msg))\n\tif err == nil {\n\t\tsubj = gk.Subject\n\t\tif store.deleteType == NoDelete {\n\t\t\terr = fmt.Errorf(\"delete must be enabled in server config\")\n\t\t} else if subj != gk.Issuer {\n\t\t\terr = fmt.Errorf(\"not self signed\")\n\t\t} else if _, ok := store.operator[gk.Issuer]; !ok {\n\t\t\terr = fmt.Errorf(\"not trusted\")\n\t\t} else if list, ok := gk.Data[\"accounts\"]; !ok {\n\t\t\terr = fmt.Errorf(\"malformed request\")\n\t\t} else if accIds, ok = list.([]any); !ok {\n\t\t\terr = fmt.Errorf(\"malformed request\")\n\t\t} else {\n\t\t\tfor _, entry := range accIds {\n\t\t\t\tif acc, ok := entry.(string); !ok ||\n\t\t\t\t\tacc == _EMPTY_ || !nkeys.IsValidPublicAccountKey(acc) {\n\t\t\t\t\terr = fmt.Errorf(\"malformed request\")\n\t\t\t\t\tbreak\n\t\t\t\t} else if acc == sysAccName {\n\t\t\t\t\terr = fmt.Errorf(\"not allowed to delete system account\")\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tif err != nil {\n\t\trespondToUpdate(s, reply, _EMPTY_, fmt.Sprintf(\"delete accounts request by %s failed\", subj), err)\n\t\treturn\n\t}\n\terrs := []string{}\n\tpassCnt := 0\n\tfor _, acc := range accIds {\n\t\tif err := store.delete(acc.(string)); err != nil {\n\t\t\terrs = append(errs, err.Error())\n\t\t} else {\n\t\t\tpassCnt++\n\t\t}\n\t}\n\tif len(errs) == 0 {\n\t\trespondToUpdate(s, reply, _EMPTY_, fmt.Sprintf(\"deleted %d accounts\", passCnt), nil)\n\t} else {\n\t\trespondToUpdate(s, reply, _EMPTY_, fmt.Sprintf(\"deleted %d accounts, failed for %d\", passCnt, len(errs)),\n\t\t\terrors.New(strings.Join(errs, \"\\n\")))\n\t}\n}\n\nfunc getOperatorKeys(s *Server) (string, map[string]struct{}, bool, error) {\n\tvar op string\n\tvar strict bool\n\tkeys := make(map[string]struct{})\n\tif opts := s.getOpts(); opts != nil && len(opts.TrustedOperators) > 0 {\n\t\top = opts.TrustedOperators[0].Subject\n\t\tstrict = opts.TrustedOperators[0].StrictSigningKeyUsage\n\t\tif !strict {\n\t\t\tkeys[opts.TrustedOperators[0].Subject] = struct{}{}\n\t\t}\n\t\tfor _, key := range opts.TrustedOperators[0].SigningKeys {\n\t\t\tkeys[key] = struct{}{}\n\t\t}\n\t}\n\tif len(keys) == 0 {\n\t\treturn _EMPTY_, nil, false, fmt.Errorf(\"no operator key found\")\n\t}\n\treturn op, keys, strict, nil\n}\n\nfunc claimValidate(claim *jwt.AccountClaims) error {\n\tvr := &jwt.ValidationResults{}\n\tclaim.Validate(vr)\n\tif vr.IsBlocking(false) {\n\t\treturn fmt.Errorf(\"validation errors: %v\", vr.Errors())\n\t}\n\treturn nil\n}\n\nfunc removeCb(s *Server, pubKey string) {\n\tv, ok := s.accounts.Load(pubKey)\n\tif !ok {\n\t\treturn\n\t}\n\ta := v.(*Account)\n\ts.Debugf(\"Disable account %s due to remove\", pubKey)\n\ta.mu.Lock()\n\t// lock out new clients\n\ta.msubs = 0\n\ta.mpay = 0\n\ta.mconns = 0\n\ta.mleafs = 0\n\ta.updated = time.Now()\n\tjsa := a.js\n\ta.mu.Unlock()\n\t// set the account to be expired and disconnect clients\n\ta.expiredTimeout()\n\t// For JS, we need also to disable it.\n\tif js := s.getJetStream(); js != nil && jsa != nil {\n\t\tjs.disableJetStream(jsa)\n\t\t// Remove JetStream state in memory, this will be reset\n\t\t// on the changed callback from the account in case it is\n\t\t// enabled again.\n\t\ta.js = nil\n\t}\n\t// We also need to remove all ServerImport subscriptions\n\ta.removeAllServiceImportSubs()\n\ta.mu.Lock()\n\ta.clearExpirationTimer()\n\ta.mu.Unlock()\n}\n\nfunc (dr *DirAccResolver) Start(s *Server) error {\n\top, opKeys, strict, err := getOperatorKeys(s)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdr.Lock()\n\tdefer dr.Unlock()\n\tdr.Server = s\n\tdr.operator = opKeys\n\tdr.DirJWTStore.changed = func(pubKey string) {\n\t\tif v, ok := s.accounts.Load(pubKey); ok {\n\t\t\tif theJwt, err := dr.LoadAcc(pubKey); err != nil {\n\t\t\t\ts.Errorf(\"DirResolver - Update got error on load: %v\", err)\n\t\t\t} else {\n\t\t\t\tacc := v.(*Account)\n\t\t\t\tif err = s.updateAccountWithClaimJWT(acc, theJwt); err != nil {\n\t\t\t\t\ts.Errorf(\"DirResolver - Update for account %q resulted in error %v\", pubKey, err)\n\t\t\t\t} else {\n\t\t\t\t\tif _, jsa, err := acc.checkForJetStream(); err != nil {\n\t\t\t\t\t\tif !IsNatsErr(err, JSNotEnabledForAccountErr) {\n\t\t\t\t\t\t\ts.Warnf(\"DirResolver - Error checking for JetStream support for account %q: %v\", pubKey, err)\n\t\t\t\t\t\t}\n\t\t\t\t\t} else if jsa == nil {\n\t\t\t\t\t\tif err = s.configJetStream(acc); err != nil {\n\t\t\t\t\t\t\ts.Errorf(\"DirResolver - Error configuring JetStream for account %q: %v\", pubKey, err)\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tdr.DirJWTStore.deleted = func(pubKey string) {\n\t\tremoveCb(s, pubKey)\n\t}\n\tpackRespIb := s.newRespInbox()\n\tfor _, reqSub := range []string{accUpdateEventSubjOld, accUpdateEventSubjNew} {\n\t\t// subscribe to account jwt update requests\n\t\tif _, err := s.sysSubscribe(fmt.Sprintf(reqSub, \"*\"), func(_ *subscription, _ *client, _ *Account, subj, resp string, msg []byte) {\n\t\t\tvar pubKey string\n\t\t\ttk := strings.Split(subj, tsep)\n\t\t\tif len(tk) == accUpdateTokensNew {\n\t\t\t\tpubKey = tk[accReqAccIndex]\n\t\t\t} else if len(tk) == accUpdateTokensOld {\n\t\t\t\tpubKey = tk[accUpdateAccIdxOld]\n\t\t\t} else {\n\t\t\t\ts.Debugf(\"DirResolver - jwt update skipped due to bad subject %q\", subj)\n\t\t\t\treturn\n\t\t\t}\n\t\t\tif claim, err := jwt.DecodeAccountClaims(string(msg)); err != nil {\n\t\t\t\trespondToUpdate(s, resp, \"n/a\", \"jwt update resulted in error\", err)\n\t\t\t} else if err := claimValidate(claim); err != nil {\n\t\t\t\trespondToUpdate(s, resp, claim.Subject, \"jwt validation failed\", err)\n\t\t\t} else if claim.Subject != pubKey {\n\t\t\t\terr := errors.New(\"subject does not match jwt content\")\n\t\t\t\trespondToUpdate(s, resp, pubKey, \"jwt update resulted in error\", err)\n\t\t\t} else if claim.Issuer == op && strict {\n\t\t\t\terr := errors.New(\"operator requires issuer to be a signing key\")\n\t\t\t\trespondToUpdate(s, resp, pubKey, \"jwt update resulted in error\", err)\n\t\t\t} else if err := dr.save(pubKey, string(msg)); err != nil {\n\t\t\t\trespondToUpdate(s, resp, pubKey, \"jwt update resulted in error\", err)\n\t\t\t} else {\n\t\t\t\trespondToUpdate(s, resp, pubKey, \"jwt updated\", nil)\n\t\t\t}\n\t\t}); err != nil {\n\t\t\treturn fmt.Errorf(\"error setting up update handling: %v\", err)\n\t\t}\n\t}\n\tif _, err := s.sysSubscribe(accClaimsReqSubj, func(_ *subscription, c *client, _ *Account, _, resp string, msg []byte) {\n\t\t// As this is a raw message, we need to extract payload and only decode claims from it,\n\t\t// in case request is sent with headers.\n\t\t_, msg = c.msgParts(msg)\n\t\tif claim, err := jwt.DecodeAccountClaims(string(msg)); err != nil {\n\t\t\trespondToUpdate(s, resp, \"n/a\", \"jwt update resulted in error\", err)\n\t\t} else if claim.Issuer == op && strict {\n\t\t\terr := errors.New(\"operator requires issuer to be a signing key\")\n\t\t\trespondToUpdate(s, resp, claim.Subject, \"jwt update resulted in error\", err)\n\t\t} else if err := claimValidate(claim); err != nil {\n\t\t\trespondToUpdate(s, resp, claim.Subject, \"jwt validation failed\", err)\n\t\t} else if err := dr.save(claim.Subject, string(msg)); err != nil {\n\t\t\trespondToUpdate(s, resp, claim.Subject, \"jwt update resulted in error\", err)\n\t\t} else {\n\t\t\trespondToUpdate(s, resp, claim.Subject, \"jwt updated\", nil)\n\t\t}\n\t}); err != nil {\n\t\treturn fmt.Errorf(\"error setting up update handling: %v\", err)\n\t}\n\t// respond to lookups with our version\n\tif _, err := s.sysSubscribe(fmt.Sprintf(accLookupReqSubj, \"*\"), func(_ *subscription, _ *client, _ *Account, subj, reply string, msg []byte) {\n\t\tif reply == _EMPTY_ {\n\t\t\treturn\n\t\t}\n\t\ttk := strings.Split(subj, tsep)\n\t\tif len(tk) != accLookupReqTokens {\n\t\t\treturn\n\t\t}\n\t\taccName := tk[accReqAccIndex]\n\t\tif theJWT, err := dr.DirJWTStore.LoadAcc(accName); err != nil {\n\t\t\tif errors.Is(err, fs.ErrNotExist) {\n\t\t\t\ts.Debugf(\"DirResolver - Could not find account %q\", accName)\n\t\t\t\t// Reply with empty response to signal absence of JWT to others.\n\t\t\t\ts.sendInternalMsgLocked(reply, _EMPTY_, nil, nil)\n\t\t\t} else {\n\t\t\t\ts.Errorf(\"DirResolver - Error looking up account %q: %v\", accName, err)\n\t\t\t}\n\t\t} else {\n\t\t\ts.sendInternalMsgLocked(reply, _EMPTY_, nil, []byte(theJWT))\n\t\t}\n\t}); err != nil {\n\t\treturn fmt.Errorf(\"error setting up lookup request handling: %v\", err)\n\t}\n\t// respond to pack requests with one or more pack messages\n\t// an empty message signifies the end of the response responder.\n\tif _, err := s.sysSubscribeQ(accPackReqSubj, \"responder\", func(_ *subscription, _ *client, _ *Account, _, reply string, theirHash []byte) {\n\t\tif reply == _EMPTY_ {\n\t\t\treturn\n\t\t}\n\t\tourHash := dr.DirJWTStore.Hash()\n\t\tif bytes.Equal(theirHash, ourHash[:]) {\n\t\t\ts.sendInternalMsgLocked(reply, _EMPTY_, nil, []byte{})\n\t\t\ts.Debugf(\"DirResolver - Pack request matches hash %x\", ourHash[:])\n\t\t} else if err := dr.DirJWTStore.PackWalk(1, func(partialPackMsg string) {\n\t\t\ts.sendInternalMsgLocked(reply, _EMPTY_, nil, []byte(partialPackMsg))\n\t\t}); err != nil {\n\t\t\t// let them timeout\n\t\t\ts.Errorf(\"DirResolver - Pack request error: %v\", err)\n\t\t} else {\n\t\t\ts.Debugf(\"DirResolver - Pack request hash %x - finished responding with hash %x\", theirHash, ourHash)\n\t\t\ts.sendInternalMsgLocked(reply, _EMPTY_, nil, []byte{})\n\t\t}\n\t}); err != nil {\n\t\treturn fmt.Errorf(\"error setting up pack request handling: %v\", err)\n\t}\n\t// respond to list requests with one message containing all account ids\n\tif _, err := s.sysSubscribe(accListReqSubj, func(_ *subscription, _ *client, _ *Account, _, reply string, _ []byte) {\n\t\thandleListRequest(dr.DirJWTStore, s, reply)\n\t}); err != nil {\n\t\treturn fmt.Errorf(\"error setting up list request handling: %v\", err)\n\t}\n\tif _, err := s.sysSubscribe(accDeleteReqSubj, func(_ *subscription, _ *client, _ *Account, _, reply string, msg []byte) {\n\t\thandleDeleteRequest(dr.DirJWTStore, s, msg, reply)\n\t}); err != nil {\n\t\treturn fmt.Errorf(\"error setting up delete request handling: %v\", err)\n\t}\n\t// embed pack responses into store\n\tif _, err := s.sysSubscribe(packRespIb, func(_ *subscription, _ *client, _ *Account, _, _ string, msg []byte) {\n\t\thash := dr.DirJWTStore.Hash()\n\t\tif len(msg) == 0 { // end of response stream\n\t\t\ts.Debugf(\"DirResolver - Merging finished and resulting in: %x\", dr.DirJWTStore.Hash())\n\t\t\treturn\n\t\t} else if err := dr.DirJWTStore.Merge(string(msg)); err != nil {\n\t\t\ts.Errorf(\"DirResolver - Merging resulted in error: %v\", err)\n\t\t} else {\n\t\t\ts.Debugf(\"DirResolver - Merging succeeded and changed %x to %x\", hash, dr.DirJWTStore.Hash())\n\t\t}\n\t}); err != nil {\n\t\treturn fmt.Errorf(\"error setting up pack response handling: %v\", err)\n\t}\n\t// periodically send out pack message\n\tquit := s.quitCh\n\ts.startGoRoutine(func() {\n\t\tdefer s.grWG.Done()\n\t\tticker := time.NewTicker(dr.syncInterval)\n\t\tfor {\n\t\t\tselect {\n\t\t\tcase <-quit:\n\t\t\t\tticker.Stop()\n\t\t\t\treturn\n\t\t\tcase <-ticker.C:\n\t\t\t}\n\t\t\tourHash := dr.DirJWTStore.Hash()\n\t\t\ts.Debugf(\"DirResolver - Checking store state: %x\", ourHash)\n\t\t\ts.sendInternalMsgLocked(accPackReqSubj, packRespIb, nil, ourHash[:])\n\t\t}\n\t})\n\ts.Noticef(\"Managing all jwt in exclusive directory %s\", dr.directory)\n\treturn nil\n}\n\nfunc (dr *DirAccResolver) Fetch(name string) (string, error) {\n\tif theJWT, err := dr.LoadAcc(name); theJWT != _EMPTY_ {\n\t\treturn theJWT, nil\n\t} else {\n\t\tdr.Lock()\n\t\tsrv := dr.Server\n\t\tto := dr.fetchTimeout\n\t\tdr.Unlock()\n\t\tif srv == nil {\n\t\t\treturn _EMPTY_, err\n\t\t}\n\t\treturn srv.fetch(dr, name, to) // lookup from other server\n\t}\n}\n\nfunc (dr *DirAccResolver) Store(name, jwt string) error {\n\treturn dr.saveIfNewer(name, jwt)\n}\n\ntype DirResOption func(s *DirAccResolver) error\n\n// limits the amount of time spent waiting for an account fetch to complete\nfunc FetchTimeout(to time.Duration) DirResOption {\n\treturn func(r *DirAccResolver) error {\n\t\tif to <= time.Duration(0) {\n\t\t\treturn fmt.Errorf(\"Fetch timeout %v is too smal\", to)\n\t\t}\n\t\tr.fetchTimeout = to\n\t\treturn nil\n\t}\n}\n\nfunc (dr *DirAccResolver) apply(opts ...DirResOption) error {\n\tfor _, o := range opts {\n\t\tif err := o(dr); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc NewDirAccResolver(path string, limit int64, syncInterval time.Duration, delete deleteType, opts ...DirResOption) (*DirAccResolver, error) {\n\tif limit == 0 {\n\t\tlimit = math.MaxInt64\n\t}\n\tif syncInterval <= 0 {\n\t\tsyncInterval = time.Minute\n\t}\n\tstore, err := NewExpiringDirJWTStore(path, false, true, delete, 0, limit, false, 0, nil)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tres := &DirAccResolver{store, nil, syncInterval, DEFAULT_ACCOUNT_FETCH_TIMEOUT}\n\tif err := res.apply(opts...); err != nil {\n\t\treturn nil, err\n\t}\n\treturn res, nil\n}\n\n// Caching resolver using nats for lookups and making use of a directory for storage\ntype CacheDirAccResolver struct {\n\tDirAccResolver\n\tttl time.Duration\n}\n\nfunc (s *Server) fetch(res AccountResolver, name string, timeout time.Duration) (string, error) {\n\tif s == nil {\n\t\treturn _EMPTY_, ErrNoAccountResolver\n\t}\n\trespC := make(chan []byte, 1)\n\taccountLookupRequest := fmt.Sprintf(accLookupReqSubj, name)\n\ts.mu.Lock()\n\tif s.sys == nil || s.sys.replies == nil {\n\t\ts.mu.Unlock()\n\t\treturn _EMPTY_, fmt.Errorf(\"eventing shut down\")\n\t}\n\t// Resolver will wait for detected active servers to reply\n\t// before serving an error in case there weren't any found.\n\texpectedServers := len(s.sys.servers)\n\treplySubj := s.newRespInbox()\n\treplies := s.sys.replies\n\n\t// Store our handler.\n\treplies[replySubj] = func(sub *subscription, _ *client, _ *Account, subject, _ string, msg []byte) {\n\t\tvar clone []byte\n\t\tisEmpty := len(msg) == 0\n\t\tif !isEmpty {\n\t\t\tclone = make([]byte, len(msg))\n\t\t\tcopy(clone, msg)\n\t\t}\n\t\ts.mu.Lock()\n\t\tdefer s.mu.Unlock()\n\t\texpectedServers--\n\t\t// Skip empty responses until getting all the available servers.\n\t\tif isEmpty && expectedServers > 0 {\n\t\t\treturn\n\t\t}\n\t\t// Use the first valid response if there is still interest or\n\t\t// one of the empty responses to signal that it was not found.\n\t\tif _, ok := replies[replySubj]; ok {\n\t\t\tselect {\n\t\t\tcase respC <- clone:\n\t\t\tdefault:\n\t\t\t}\n\t\t}\n\t}\n\ts.sendInternalMsg(accountLookupRequest, replySubj, nil, []byte{})\n\tquit := s.quitCh\n\ts.mu.Unlock()\n\tvar err error\n\tvar theJWT string\n\tselect {\n\tcase <-quit:\n\t\terr = errors.New(\"fetching jwt failed due to shutdown\")\n\tcase <-time.After(timeout):\n\t\terr = errors.New(\"fetching jwt timed out\")\n\tcase m := <-respC:\n\t\tif len(m) == 0 {\n\t\t\terr = errors.New(\"account jwt not found\")\n\t\t} else if err = res.Store(name, string(m)); err == nil {\n\t\t\ttheJWT = string(m)\n\t\t}\n\t}\n\ts.mu.Lock()\n\tdelete(replies, replySubj)\n\ts.mu.Unlock()\n\tclose(respC)\n\treturn theJWT, err\n}\n\nfunc NewCacheDirAccResolver(path string, limit int64, ttl time.Duration, opts ...DirResOption) (*CacheDirAccResolver, error) {\n\tif limit <= 0 {\n\t\tlimit = 1_000\n\t}\n\tstore, err := NewExpiringDirJWTStore(path, false, true, HardDelete, 0, limit, true, ttl, nil)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tres := &CacheDirAccResolver{DirAccResolver{store, nil, 0, DEFAULT_ACCOUNT_FETCH_TIMEOUT}, ttl}\n\tif err := res.apply(opts...); err != nil {\n\t\treturn nil, err\n\t}\n\treturn res, nil\n}\n\nfunc (dr *CacheDirAccResolver) Start(s *Server) error {\n\top, opKeys, strict, err := getOperatorKeys(s)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdr.Lock()\n\tdefer dr.Unlock()\n\tdr.Server = s\n\tdr.operator = opKeys\n\tdr.DirJWTStore.changed = func(pubKey string) {\n\t\tif v, ok := s.accounts.Load(pubKey); !ok {\n\t\t} else if theJwt, err := dr.LoadAcc(pubKey); err != nil {\n\t\t\ts.Errorf(\"DirResolver - Update got error on load: %v\", err)\n\t\t} else if err := s.updateAccountWithClaimJWT(v.(*Account), theJwt); err != nil {\n\t\t\ts.Errorf(\"DirResolver - Update resulted in error %v\", err)\n\t\t}\n\t}\n\tdr.DirJWTStore.deleted = func(pubKey string) {\n\t\tremoveCb(s, pubKey)\n\t}\n\tfor _, reqSub := range []string{accUpdateEventSubjOld, accUpdateEventSubjNew} {\n\t\t// subscribe to account jwt update requests\n\t\tif _, err := s.sysSubscribe(fmt.Sprintf(reqSub, \"*\"), func(_ *subscription, _ *client, _ *Account, subj, resp string, msg []byte) {\n\t\t\tvar pubKey string\n\t\t\ttk := strings.Split(subj, tsep)\n\t\t\tif len(tk) == accUpdateTokensNew {\n\t\t\t\tpubKey = tk[accReqAccIndex]\n\t\t\t} else if len(tk) == accUpdateTokensOld {\n\t\t\t\tpubKey = tk[accUpdateAccIdxOld]\n\t\t\t} else {\n\t\t\t\ts.Debugf(\"DirResolver - jwt update cache skipped due to bad subject %q\", subj)\n\t\t\t\treturn\n\t\t\t}\n\t\t\tif claim, err := jwt.DecodeAccountClaims(string(msg)); err != nil {\n\t\t\t\trespondToUpdate(s, resp, pubKey, \"jwt update cache resulted in error\", err)\n\t\t\t} else if claim.Subject != pubKey {\n\t\t\t\terr := errors.New(\"subject does not match jwt content\")\n\t\t\t\trespondToUpdate(s, resp, pubKey, \"jwt update cache resulted in error\", err)\n\t\t\t} else if claim.Issuer == op && strict {\n\t\t\t\terr := errors.New(\"operator requires issuer to be a signing key\")\n\t\t\t\trespondToUpdate(s, resp, pubKey, \"jwt update cache resulted in error\", err)\n\t\t\t} else if _, ok := s.accounts.Load(pubKey); !ok {\n\t\t\t\trespondToUpdate(s, resp, pubKey, \"jwt update cache skipped\", nil)\n\t\t\t} else if err := claimValidate(claim); err != nil {\n\t\t\t\trespondToUpdate(s, resp, claim.Subject, \"jwt update cache validation failed\", err)\n\t\t\t} else if err := dr.save(pubKey, string(msg)); err != nil {\n\t\t\t\trespondToUpdate(s, resp, pubKey, \"jwt update cache resulted in error\", err)\n\t\t\t} else {\n\t\t\t\trespondToUpdate(s, resp, pubKey, \"jwt updated cache\", nil)\n\t\t\t}\n\t\t}); err != nil {\n\t\t\treturn fmt.Errorf(\"error setting up update handling: %v\", err)\n\t\t}\n\t}\n\tif _, err := s.sysSubscribe(accClaimsReqSubj, func(_ *subscription, c *client, _ *Account, _, resp string, msg []byte) {\n\t\t// As this is a raw message, we need to extract payload and only decode claims from it,\n\t\t// in case request is sent with headers.\n\t\t_, msg = c.msgParts(msg)\n\t\tif claim, err := jwt.DecodeAccountClaims(string(msg)); err != nil {\n\t\t\trespondToUpdate(s, resp, \"n/a\", \"jwt update cache resulted in error\", err)\n\t\t} else if claim.Issuer == op && strict {\n\t\t\terr := errors.New(\"operator requires issuer to be a signing key\")\n\t\t\trespondToUpdate(s, resp, claim.Subject, \"jwt update cache resulted in error\", err)\n\t\t} else if _, ok := s.accounts.Load(claim.Subject); !ok {\n\t\t\trespondToUpdate(s, resp, claim.Subject, \"jwt update cache skipped\", nil)\n\t\t} else if err := claimValidate(claim); err != nil {\n\t\t\trespondToUpdate(s, resp, claim.Subject, \"jwt update cache validation failed\", err)\n\t\t} else if err := dr.save(claim.Subject, string(msg)); err != nil {\n\t\t\trespondToUpdate(s, resp, claim.Subject, \"jwt update cache resulted in error\", err)\n\t\t} else {\n\t\t\trespondToUpdate(s, resp, claim.Subject, \"jwt updated cache\", nil)\n\t\t}\n\t}); err != nil {\n\t\treturn fmt.Errorf(\"error setting up update handling: %v\", err)\n\t}\n\t// respond to list requests with one message containing all account ids\n\tif _, err := s.sysSubscribe(accListReqSubj, func(_ *subscription, _ *client, _ *Account, _, reply string, _ []byte) {\n\t\thandleListRequest(dr.DirJWTStore, s, reply)\n\t}); err != nil {\n\t\treturn fmt.Errorf(\"error setting up list request handling: %v\", err)\n\t}\n\tif _, err := s.sysSubscribe(accDeleteReqSubj, func(_ *subscription, _ *client, _ *Account, _, reply string, msg []byte) {\n\t\thandleDeleteRequest(dr.DirJWTStore, s, msg, reply)\n\t}); err != nil {\n\t\treturn fmt.Errorf(\"error setting up list request handling: %v\", err)\n\t}\n\ts.Noticef(\"Managing some jwt in exclusive directory %s\", dr.directory)\n\treturn nil\n}\n\nfunc (dr *CacheDirAccResolver) Reload() error {\n\treturn dr.DirAccResolver.Reload()\n}\n",
    "source_file": "server/accounts.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2022-2025 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n//go:build netbsd\n\npackage server\n\n// TODO - See if there is a version of this for NetBSD.\nfunc diskAvailable(storeDir string) int64 {\n\treturn JetStreamMaxStoreDefault\n}\n",
    "source_file": "server/disk_avail_netbsd.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2024-2025 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage server\n\nimport (\n\t\"bytes\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"math/rand\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync/atomic\"\n\t\"time\"\n)\n\nconst (\n\tMsgTraceDest          = \"Nats-Trace-Dest\"\n\tMsgTraceHop           = \"Nats-Trace-Hop\"\n\tMsgTraceOriginAccount = \"Nats-Trace-Origin-Account\"\n\tMsgTraceOnly          = \"Nats-Trace-Only\"\n\n\t// External trace header. Note that this header is normally in lower\n\t// case (https://www.w3.org/TR/trace-context/#header-name). Vendors\n\t// MUST expect the header in any case (upper, lower, mixed), and\n\t// SHOULD send the header name in lowercase.\n\ttraceParentHdr = \"traceparent\"\n)\n\ntype MsgTraceType string\n\n// Type of message trace events in the MsgTraceEvents list.\n// This is needed to unmarshal the list.\nconst (\n\tMsgTraceIngressType        = \"in\"\n\tMsgTraceSubjectMappingType = \"sm\"\n\tMsgTraceStreamExportType   = \"se\"\n\tMsgTraceServiceImportType  = \"si\"\n\tMsgTraceJetStreamType      = \"js\"\n\tMsgTraceEgressType         = \"eg\"\n)\n\ntype MsgTraceEvent struct {\n\tServer  ServerInfo      `json:\"server\"`\n\tRequest MsgTraceRequest `json:\"request\"`\n\tHops    int             `json:\"hops,omitempty\"`\n\tEvents  MsgTraceEvents  `json:\"events\"`\n}\n\ntype MsgTraceRequest struct {\n\t// We are not making this an http.Header so that header name case is preserved.\n\tHeader  map[string][]string `json:\"header,omitempty\"`\n\tMsgSize int                 `json:\"msgsize,omitempty\"`\n}\n\ntype MsgTraceEvents []MsgTrace\n\ntype MsgTrace interface {\n\tnew() MsgTrace\n\ttyp() MsgTraceType\n}\n\ntype MsgTraceBase struct {\n\tType      MsgTraceType `json:\"type\"`\n\tTimestamp time.Time    `json:\"ts\"`\n}\n\ntype MsgTraceIngress struct {\n\tMsgTraceBase\n\tKind    int    `json:\"kind\"`\n\tCID     uint64 `json:\"cid\"`\n\tName    string `json:\"name,omitempty\"`\n\tAccount string `json:\"acc\"`\n\tSubject string `json:\"subj\"`\n\tError   string `json:\"error,omitempty\"`\n}\n\ntype MsgTraceSubjectMapping struct {\n\tMsgTraceBase\n\tMappedTo string `json:\"to\"`\n}\n\ntype MsgTraceStreamExport struct {\n\tMsgTraceBase\n\tAccount string `json:\"acc\"`\n\tTo      string `json:\"to\"`\n}\n\ntype MsgTraceServiceImport struct {\n\tMsgTraceBase\n\tAccount string `json:\"acc\"`\n\tFrom    string `json:\"from\"`\n\tTo      string `json:\"to\"`\n}\n\ntype MsgTraceJetStream struct {\n\tMsgTraceBase\n\tStream     string `json:\"stream\"`\n\tSubject    string `json:\"subject,omitempty\"`\n\tNoInterest bool   `json:\"nointerest,omitempty\"`\n\tError      string `json:\"error,omitempty\"`\n}\n\ntype MsgTraceEgress struct {\n\tMsgTraceBase\n\tKind         int    `json:\"kind\"`\n\tCID          uint64 `json:\"cid\"`\n\tName         string `json:\"name,omitempty\"`\n\tHop          string `json:\"hop,omitempty\"`\n\tAccount      string `json:\"acc,omitempty\"`\n\tSubscription string `json:\"sub,omitempty\"`\n\tQueue        string `json:\"queue,omitempty\"`\n\tError        string `json:\"error,omitempty\"`\n\n\t// This is for applications that unmarshal the trace events\n\t// and want to link an egress to route/leaf/gateway with\n\t// the MsgTraceEvent from that server.\n\tLink *MsgTraceEvent `json:\"-\"`\n}\n\n// -------------------------------------------------------------\n\nfunc (t MsgTraceBase) typ() MsgTraceType     { return t.Type }\nfunc (MsgTraceIngress) new() MsgTrace        { return &MsgTraceIngress{} }\nfunc (MsgTraceSubjectMapping) new() MsgTrace { return &MsgTraceSubjectMapping{} }\nfunc (MsgTraceStreamExport) new() MsgTrace   { return &MsgTraceStreamExport{} }\nfunc (MsgTraceServiceImport) new() MsgTrace  { return &MsgTraceServiceImport{} }\nfunc (MsgTraceJetStream) new() MsgTrace      { return &MsgTraceJetStream{} }\nfunc (MsgTraceEgress) new() MsgTrace         { return &MsgTraceEgress{} }\n\nvar msgTraceInterfaces = map[MsgTraceType]MsgTrace{\n\tMsgTraceIngressType:        MsgTraceIngress{},\n\tMsgTraceSubjectMappingType: MsgTraceSubjectMapping{},\n\tMsgTraceStreamExportType:   MsgTraceStreamExport{},\n\tMsgTraceServiceImportType:  MsgTraceServiceImport{},\n\tMsgTraceJetStreamType:      MsgTraceJetStream{},\n\tMsgTraceEgressType:         MsgTraceEgress{},\n}\n\nfunc (t *MsgTraceEvents) UnmarshalJSON(data []byte) error {\n\tvar raw []json.RawMessage\n\terr := json.Unmarshal(data, &raw)\n\tif err != nil {\n\t\treturn err\n\t}\n\t*t = make(MsgTraceEvents, len(raw))\n\tvar tt MsgTraceBase\n\tfor i, r := range raw {\n\t\tif err = json.Unmarshal(r, &tt); err != nil {\n\t\t\treturn err\n\t\t}\n\t\ttr, ok := msgTraceInterfaces[tt.Type]\n\t\tif !ok {\n\t\t\treturn fmt.Errorf(\"unknown trace type %v\", tt.Type)\n\t\t}\n\t\tte := tr.new()\n\t\tif err := json.Unmarshal(r, te); err != nil {\n\t\t\treturn err\n\t\t}\n\t\t(*t)[i] = te\n\t}\n\treturn nil\n}\n\nfunc getTraceAs[T MsgTrace](e any) *T {\n\tv, ok := e.(*T)\n\tif ok {\n\t\treturn v\n\t}\n\treturn nil\n}\n\nfunc (t *MsgTraceEvent) Ingress() *MsgTraceIngress {\n\tif len(t.Events) < 1 {\n\t\treturn nil\n\t}\n\treturn getTraceAs[MsgTraceIngress](t.Events[0])\n}\n\nfunc (t *MsgTraceEvent) SubjectMapping() *MsgTraceSubjectMapping {\n\tfor _, e := range t.Events {\n\t\tif e.typ() == MsgTraceSubjectMappingType {\n\t\t\treturn getTraceAs[MsgTraceSubjectMapping](e)\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc (t *MsgTraceEvent) StreamExports() []*MsgTraceStreamExport {\n\tvar se []*MsgTraceStreamExport\n\tfor _, e := range t.Events {\n\t\tif e.typ() == MsgTraceStreamExportType {\n\t\t\tse = append(se, getTraceAs[MsgTraceStreamExport](e))\n\t\t}\n\t}\n\treturn se\n}\n\nfunc (t *MsgTraceEvent) ServiceImports() []*MsgTraceServiceImport {\n\tvar si []*MsgTraceServiceImport\n\tfor _, e := range t.Events {\n\t\tif e.typ() == MsgTraceServiceImportType {\n\t\t\tsi = append(si, getTraceAs[MsgTraceServiceImport](e))\n\t\t}\n\t}\n\treturn si\n}\n\nfunc (t *MsgTraceEvent) JetStream() *MsgTraceJetStream {\n\tfor _, e := range t.Events {\n\t\tif e.typ() == MsgTraceJetStreamType {\n\t\t\treturn getTraceAs[MsgTraceJetStream](e)\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc (t *MsgTraceEvent) Egresses() []*MsgTraceEgress {\n\tvar eg []*MsgTraceEgress\n\tfor _, e := range t.Events {\n\t\tif e.typ() == MsgTraceEgressType {\n\t\t\teg = append(eg, getTraceAs[MsgTraceEgress](e))\n\t\t}\n\t}\n\treturn eg\n}\n\nconst (\n\terrMsgTraceOnlyNoSupport   = \"Not delivered because remote does not support message tracing\"\n\terrMsgTraceNoSupport       = \"Message delivered but remote does not support message tracing so no trace event generated from there\"\n\terrMsgTraceNoEcho          = \"Not delivered because of no echo\"\n\terrMsgTracePubViolation    = \"Not delivered because publish denied for this subject\"\n\terrMsgTraceSubDeny         = \"Not delivered because subscription denies this subject\"\n\terrMsgTraceSubClosed       = \"Not delivered because subscription is closed\"\n\terrMsgTraceClientClosed    = \"Not delivered because client is closed\"\n\terrMsgTraceAutoSubExceeded = \"Not delivered because auto-unsubscribe exceeded\"\n\terrMsgTraceFastProdNoStall = \"Not delivered because fast producer not stalled and consumer is slow\"\n)\n\ntype msgTrace struct {\n\tready int32\n\tsrv   *Server\n\tacc   *Account\n\t// Origin account name, set only if acc is nil when acc lookup failed.\n\toan   string\n\tdest  string\n\tevent *MsgTraceEvent\n\tjs    *MsgTraceJetStream\n\thop   string\n\tnhop  string\n\ttonly bool // Will only trace the message, not do delivery.\n\tct    compressionType\n}\n\n// This will be false outside of the tests, so when building the server binary,\n// any code where you see `if msgTraceRunInTests` statement will be compiled\n// out, so this will have no performance penalty.\nvar (\n\tmsgTraceRunInTests   bool\n\tmsgTraceCheckSupport bool\n)\n\n// Returns the message trace object, if message is being traced,\n// and `true` if we want to only trace, not actually deliver the message.\nfunc (c *client) isMsgTraceEnabled() (*msgTrace, bool) {\n\tt := c.pa.trace\n\tif t == nil {\n\t\treturn nil, false\n\t}\n\treturn t, t.tonly\n}\n\n// For LEAF/ROUTER/GATEWAY, return false if the remote does not support\n// message tracing (important if the tracing requests trace-only).\nfunc (c *client) msgTraceSupport() bool {\n\t// Exclude client connection from the protocol check.\n\treturn c.kind == CLIENT || c.opts.Protocol >= MsgTraceProto\n}\n\nfunc getConnName(c *client) string {\n\tswitch c.kind {\n\tcase ROUTER:\n\t\tif n := c.route.remoteName; n != _EMPTY_ {\n\t\t\treturn n\n\t\t}\n\tcase GATEWAY:\n\t\tif n := c.gw.remoteName; n != _EMPTY_ {\n\t\t\treturn n\n\t\t}\n\tcase LEAF:\n\t\tif n := c.leaf.remoteServer; n != _EMPTY_ {\n\t\t\treturn n\n\t\t}\n\t}\n\treturn c.opts.Name\n}\n\nfunc getCompressionType(cts string) compressionType {\n\tif cts == _EMPTY_ {\n\t\treturn noCompression\n\t}\n\tcts = strings.ToLower(cts)\n\tif strings.Contains(cts, \"snappy\") || strings.Contains(cts, \"s2\") {\n\t\treturn snappyCompression\n\t}\n\tif strings.Contains(cts, \"gzip\") {\n\t\treturn gzipCompression\n\t}\n\treturn unsupportedCompression\n}\n\nfunc (c *client) initMsgTrace() *msgTrace {\n\t// The code in the \"if\" statement is only running in test mode.\n\tif msgTraceRunInTests {\n\t\t// Check the type of client that tries to initialize a trace struct.\n\t\tif !(c.kind == CLIENT || c.kind == ROUTER || c.kind == GATEWAY || c.kind == LEAF) {\n\t\t\tpanic(fmt.Sprintf(\"Unexpected client type %q trying to initialize msgTrace\", c.kindString()))\n\t\t}\n\t\t// In some tests, we want to make a server behave like an old server\n\t\t// and so even if a trace header is received, we want the server to\n\t\t// simply ignore it.\n\t\tif msgTraceCheckSupport {\n\t\t\tif c.srv == nil || c.srv.getServerProto() < MsgTraceProto {\n\t\t\t\treturn nil\n\t\t\t}\n\t\t}\n\t}\n\tif c.pa.hdr <= 0 {\n\t\treturn nil\n\t}\n\thdr := c.msgBuf[:c.pa.hdr]\n\theaders, external := genHeaderMapIfTraceHeadersPresent(hdr)\n\tif len(headers) == 0 {\n\t\treturn nil\n\t}\n\t// Little helper to give us the first value of a given header, or _EMPTY_\n\t// if key is not present.\n\tgetHdrVal := func(key string) string {\n\t\tvv, ok := headers[key]\n\t\tif !ok {\n\t\t\treturn _EMPTY_\n\t\t}\n\t\treturn vv[0]\n\t}\n\tct := getCompressionType(getHdrVal(acceptEncodingHeader))\n\tvar (\n\t\tdest      string\n\t\ttraceOnly bool\n\t)\n\t// Check for traceOnly only if not external.\n\tif !external {\n\t\tif to := getHdrVal(MsgTraceOnly); to != _EMPTY_ {\n\t\t\ttos := strings.ToLower(to)\n\t\t\tswitch tos {\n\t\t\tcase \"1\", \"true\", \"on\":\n\t\t\t\ttraceOnly = true\n\t\t\t}\n\t\t}\n\t\tdest = getHdrVal(MsgTraceDest)\n\t\t// Check the destination to see if this is a valid public subject.\n\t\tif !IsValidPublishSubject(dest) {\n\t\t\t// We still have to return a msgTrace object (if traceOnly is set)\n\t\t\t// because if we don't, the message will end-up being delivered to\n\t\t\t// applications, which may break them. We report the error in any case.\n\t\t\tc.Errorf(\"Destination %q is not valid, won't be able to trace events\", dest)\n\t\t\tif !traceOnly {\n\t\t\t\t// We can bail, tracing will be disabled for this message.\n\t\t\t\treturn nil\n\t\t\t}\n\t\t}\n\t}\n\tvar (\n\t\t// Account to use when sending the trace event\n\t\tacc *Account\n\t\t// Ingress' account name\n\t\tian string\n\t\t// Origin account name\n\t\toan string\n\t\t// The hop \"id\", taken from headers only when not from CLIENT\n\t\thop string\n\t)\n\tif c.kind == ROUTER || c.kind == GATEWAY || c.kind == LEAF {\n\t\t// The ingress account name will always be c.pa.account, but `acc` may\n\t\t// be different if we have an origin account header.\n\t\tif c.kind == LEAF {\n\t\t\tian = c.acc.GetName()\n\t\t} else {\n\t\t\tian = string(c.pa.account)\n\t\t}\n\t\t// The remote will have set the origin account header only if the\n\t\t// message changed account (think of service imports).\n\t\toan = getHdrVal(MsgTraceOriginAccount)\n\t\tif oan == _EMPTY_ {\n\t\t\t// For LEAF or ROUTER with pinned-account, we can use the c.acc.\n\t\t\tif c.kind == LEAF || (c.kind == ROUTER && len(c.route.accName) > 0) {\n\t\t\t\tacc = c.acc\n\t\t\t} else {\n\t\t\t\t// We will lookup account with c.pa.account (or ian).\n\t\t\t\toan = ian\n\t\t\t}\n\t\t}\n\t\t// Unless we already got the account, we need to look it up.\n\t\tif acc == nil {\n\t\t\t// We don't want to do account resolving here.\n\t\t\tif acci, ok := c.srv.accounts.Load(oan); ok {\n\t\t\t\tacc = acci.(*Account)\n\t\t\t\t// Since we have looked-up the account, we don't need oan, so\n\t\t\t\t// clear it in case it was set.\n\t\t\t\toan = _EMPTY_\n\t\t\t} else {\n\t\t\t\t// We still have to return a msgTrace object (if traceOnly is set)\n\t\t\t\t// because if we don't, the message will end-up being delivered to\n\t\t\t\t// applications, which may break them. We report the error in any case.\n\t\t\t\tc.Errorf(\"Account %q was not found, won't be able to trace events\", oan)\n\t\t\t\tif !traceOnly {\n\t\t\t\t\t// We can bail, tracing will be disabled for this message.\n\t\t\t\t\treturn nil\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t// Check the hop header\n\t\thop = getHdrVal(MsgTraceHop)\n\t} else {\n\t\tacc = c.acc\n\t\tian = acc.GetName()\n\t}\n\t// If external, we need to have the account's trace destination set,\n\t// otherwise, we are not enabling tracing.\n\tif external {\n\t\tvar sampling int\n\t\tif acc != nil {\n\t\t\tdest, sampling = acc.getTraceDestAndSampling()\n\t\t}\n\t\tif dest == _EMPTY_ {\n\t\t\t// No account destination, no tracing for external trace headers.\n\t\t\treturn nil\n\t\t}\n\t\t// Check sampling, but only from origin server.\n\t\tif c.kind == CLIENT && !sample(sampling) {\n\t\t\t// Need to desactivate the traceParentHdr so that if the message\n\t\t\t// is routed, it does possibly trigger a trace there.\n\t\t\tdisableTraceHeaders(c, hdr)\n\t\t\treturn nil\n\t\t}\n\t}\n\tc.pa.trace = &msgTrace{\n\t\tsrv:  c.srv,\n\t\tacc:  acc,\n\t\toan:  oan,\n\t\tdest: dest,\n\t\tct:   ct,\n\t\thop:  hop,\n\t\tevent: &MsgTraceEvent{\n\t\t\tRequest: MsgTraceRequest{\n\t\t\t\tHeader:  headers,\n\t\t\t\tMsgSize: c.pa.size,\n\t\t\t},\n\t\t\tEvents: append(MsgTraceEvents(nil), &MsgTraceIngress{\n\t\t\t\tMsgTraceBase: MsgTraceBase{\n\t\t\t\t\tType:      MsgTraceIngressType,\n\t\t\t\t\tTimestamp: time.Now(),\n\t\t\t\t},\n\t\t\t\tKind:    c.kind,\n\t\t\t\tCID:     c.cid,\n\t\t\t\tName:    getConnName(c),\n\t\t\t\tAccount: ian,\n\t\t\t\tSubject: string(c.pa.subject),\n\t\t\t}),\n\t\t},\n\t\ttonly: traceOnly,\n\t}\n\treturn c.pa.trace\n}\n\nfunc sample(sampling int) bool {\n\t// Option parsing should ensure that sampling is [1..100], but consider\n\t// any value outside of this range to be 100%.\n\tif sampling <= 0 || sampling >= 100 {\n\t\treturn true\n\t}\n\treturn rand.Int31n(100) <= int32(sampling)\n}\n\n// This function will return the header as a map (instead of http.Header because\n// we want to preserve the header names' case) and a boolean that indicates if\n// the headers have been lifted due to the presence of the external trace header\n// only.\n// Note that because of the traceParentHdr, the search is done in a case\n// insensitive way, but if the header is found, it is rewritten in lower case\n// as suggested by the spec, but also to make it easier to disable the header\n// when needed.\nfunc genHeaderMapIfTraceHeadersPresent(hdr []byte) (map[string][]string, bool) {\n\n\tvar (\n\t\t_keys               = [64][]byte{}\n\t\t_vals               = [64][]byte{}\n\t\tm                   map[string][]string\n\t\ttraceDestHdrFound   bool\n\t\ttraceParentHdrFound bool\n\t)\n\t// Skip the hdrLine\n\tif !bytes.HasPrefix(hdr, stringToBytes(hdrLine)) {\n\t\treturn nil, false\n\t}\n\n\ttraceDestHdrAsBytes := stringToBytes(MsgTraceDest)\n\ttraceParentHdrAsBytes := stringToBytes(traceParentHdr)\n\tcrLFAsBytes := stringToBytes(CR_LF)\n\tdashAsBytes := stringToBytes(\"-\")\n\n\tkeys := _keys[:0]\n\tvals := _vals[:0]\n\n\tfor i := len(hdrLine); i < len(hdr); {\n\t\t// Search for key/val delimiter\n\t\tdel := bytes.IndexByte(hdr[i:], ':')\n\t\tif del < 0 {\n\t\t\tbreak\n\t\t}\n\t\tkeyStart := i\n\t\tkey := hdr[keyStart : keyStart+del]\n\t\ti += del + 1\n\t\tvalStart := i\n\t\tnl := bytes.Index(hdr[valStart:], crLFAsBytes)\n\t\tif nl < 0 {\n\t\t\tbreak\n\t\t}\n\t\tif len(key) > 0 {\n\t\t\tval := bytes.Trim(hdr[valStart:valStart+nl], \" \\t\")\n\t\t\tvals = append(vals, val)\n\n\t\t\t// Check for the external trace header.\n\t\t\tif bytes.EqualFold(key, traceParentHdrAsBytes) {\n\t\t\t\t// Rewrite the header using lower case if needed.\n\t\t\t\tif !bytes.Equal(key, traceParentHdrAsBytes) {\n\t\t\t\t\tcopy(hdr[keyStart:], traceParentHdrAsBytes)\n\t\t\t\t}\n\t\t\t\t// We will now check if the value has sampling or not.\n\t\t\t\t// TODO(ik): Not sure if this header can have multiple values\n\t\t\t\t// or not, and if so, what would be the rule to check for\n\t\t\t\t// sampling. What is done here is to check them all until we\n\t\t\t\t// found one with sampling.\n\t\t\t\tif !traceParentHdrFound {\n\t\t\t\t\ttk := bytes.Split(val, dashAsBytes)\n\t\t\t\t\tif len(tk) == 4 && len([]byte(tk[3])) == 2 {\n\t\t\t\t\t\tif hexVal, err := strconv.ParseInt(bytesToString(tk[3]), 16, 8); err == nil {\n\t\t\t\t\t\t\tif hexVal&0x1 == 0x1 {\n\t\t\t\t\t\t\t\ttraceParentHdrFound = true\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\t// Add to the keys with the external trace header in lower case.\n\t\t\t\tkeys = append(keys, traceParentHdrAsBytes)\n\t\t\t} else {\n\t\t\t\t// Is the key the Nats-Trace-Dest header?\n\t\t\t\tif bytes.EqualFold(key, traceDestHdrAsBytes) {\n\t\t\t\t\ttraceDestHdrFound = true\n\t\t\t\t}\n\t\t\t\t// Add to the keys and preserve the key's case\n\t\t\t\tkeys = append(keys, key)\n\t\t\t}\n\t\t}\n\t\ti += nl + 2\n\t}\n\tif !traceDestHdrFound && !traceParentHdrFound {\n\t\treturn nil, false\n\t}\n\tm = make(map[string][]string, len(keys))\n\tfor i, k := range keys {\n\t\thname := string(k)\n\t\tm[hname] = append(m[hname], string(vals[i]))\n\t}\n\treturn m, !traceDestHdrFound && traceParentHdrFound\n}\n\n// Special case where we create a trace event before parsing the message.\n// This is for cases where the connection will be closed when detecting\n// an error during early message processing (for instance max payload).\nfunc (c *client) initAndSendIngressErrEvent(hdr []byte, dest string, ingressError error) {\n\tif ingressError == nil {\n\t\treturn\n\t}\n\tct := getAcceptEncoding(hdr)\n\tt := &msgTrace{\n\t\tsrv:  c.srv,\n\t\tacc:  c.acc,\n\t\tdest: dest,\n\t\tct:   ct,\n\t\tevent: &MsgTraceEvent{\n\t\t\tRequest: MsgTraceRequest{MsgSize: c.pa.size},\n\t\t\tEvents: append(MsgTraceEvents(nil), &MsgTraceIngress{\n\t\t\t\tMsgTraceBase: MsgTraceBase{\n\t\t\t\t\tType:      MsgTraceIngressType,\n\t\t\t\t\tTimestamp: time.Now(),\n\t\t\t\t},\n\t\t\t\tKind:  c.kind,\n\t\t\t\tCID:   c.cid,\n\t\t\t\tName:  getConnName(c),\n\t\t\t\tError: ingressError.Error(),\n\t\t\t}),\n\t\t},\n\t}\n\tt.sendEvent()\n}\n\n// Returns `true` if message tracing is enabled and we are tracing only,\n// that is, we are not going to deliver the inbound message, returns\n// `false` otherwise (no tracing, or tracing and message delivery).\nfunc (t *msgTrace) traceOnly() bool {\n\treturn t != nil && t.tonly\n}\n\nfunc (t *msgTrace) setOriginAccountHeaderIfNeeded(c *client, acc *Account, msg []byte) []byte {\n\tvar oan string\n\t// If t.acc is set, only check that, not t.oan.\n\tif t.acc != nil {\n\t\tif t.acc != acc {\n\t\t\toan = t.acc.GetName()\n\t\t}\n\t} else if t.oan != acc.GetName() {\n\t\toan = t.oan\n\t}\n\tif oan != _EMPTY_ {\n\t\tmsg = c.setHeader(MsgTraceOriginAccount, oan, msg)\n\t}\n\treturn msg\n}\n\nfunc (t *msgTrace) setHopHeader(c *client, msg []byte) []byte {\n\te := t.event\n\te.Hops++\n\tif len(t.hop) > 0 {\n\t\tt.nhop = fmt.Sprintf(\"%s.%d\", t.hop, e.Hops)\n\t} else {\n\t\tt.nhop = fmt.Sprintf(\"%d\", e.Hops)\n\t}\n\treturn c.setHeader(MsgTraceHop, t.nhop, msg)\n}\n\n// Will look for the MsgTraceSendTo and traceParentHdr headers and change the first\n// character to an 'X' so that if this message is sent to a remote, the remote\n// will not initialize tracing since it won't find the actual trace headers.\n// The function returns the position of the headers so it can efficiently be\n// re-enabled by calling enableTraceHeaders.\n// Note that if `msg` can be either the header alone or the full message\n// (header and payload). This function will use c.pa.hdr to limit the\n// search to the header section alone.\nfunc disableTraceHeaders(c *client, msg []byte) []int {\n\t// Code largely copied from getHeader(), except that we don't need the value\n\tif c.pa.hdr <= 0 {\n\t\treturn []int{-1, -1}\n\t}\n\thdr := msg[:c.pa.hdr]\n\theaders := [2]string{MsgTraceDest, traceParentHdr}\n\tpositions := [2]int{-1, -1}\n\tfor i := 0; i < 2; i++ {\n\t\tkey := stringToBytes(headers[i])\n\t\tpos := bytes.Index(hdr, key)\n\t\tif pos < 0 {\n\t\t\tcontinue\n\t\t}\n\t\t// Make sure this key does not have additional prefix.\n\t\tif pos < 2 || hdr[pos-1] != '\\n' || hdr[pos-2] != '\\r' {\n\t\t\tcontinue\n\t\t}\n\t\tindex := pos + len(key)\n\t\tif index >= len(hdr) {\n\t\t\tcontinue\n\t\t}\n\t\tif hdr[index] != ':' {\n\t\t\tcontinue\n\t\t}\n\t\t// Disable the trace by altering the first character of the header\n\t\thdr[pos] = 'X'\n\t\tpositions[i] = pos\n\t}\n\t// Return the positions of those characters so we can re-enable the headers.\n\treturn positions[:2]\n}\n\n// Changes back the character at the given position `pos` in the `msg`\n// byte slice to the first character of the MsgTraceSendTo header.\nfunc enableTraceHeaders(msg []byte, positions []int) {\n\tfirstChar := [2]byte{MsgTraceDest[0], traceParentHdr[0]}\n\tfor i, pos := range positions {\n\t\tif pos == -1 {\n\t\t\tcontinue\n\t\t}\n\t\tmsg[pos] = firstChar[i]\n\t}\n}\n\nfunc (t *msgTrace) setIngressError(err string) {\n\tif i := t.event.Ingress(); i != nil {\n\t\ti.Error = err\n\t}\n}\n\nfunc (t *msgTrace) addSubjectMappingEvent(subj []byte) {\n\tif t == nil {\n\t\treturn\n\t}\n\tt.event.Events = append(t.event.Events, &MsgTraceSubjectMapping{\n\t\tMsgTraceBase: MsgTraceBase{\n\t\t\tType:      MsgTraceSubjectMappingType,\n\t\t\tTimestamp: time.Now(),\n\t\t},\n\t\tMappedTo: string(subj),\n\t})\n}\n\nfunc (t *msgTrace) addEgressEvent(dc *client, sub *subscription, err string) {\n\tif t == nil {\n\t\treturn\n\t}\n\te := &MsgTraceEgress{\n\t\tMsgTraceBase: MsgTraceBase{\n\t\t\tType:      MsgTraceEgressType,\n\t\t\tTimestamp: time.Now(),\n\t\t},\n\t\tKind:  dc.kind,\n\t\tCID:   dc.cid,\n\t\tName:  getConnName(dc),\n\t\tHop:   t.nhop,\n\t\tError: err,\n\t}\n\tt.nhop = _EMPTY_\n\t// Specific to CLIENT connections...\n\tif dc.kind == CLIENT {\n\t\t// Set the subscription's subject and possibly queue name.\n\t\te.Subscription = string(sub.subject)\n\t\tif len(sub.queue) > 0 {\n\t\t\te.Queue = string(sub.queue)\n\t\t}\n\t}\n\tif dc.kind == CLIENT || dc.kind == LEAF {\n\t\tif i := t.event.Ingress(); i != nil {\n\t\t\t// If the Ingress' account is different from the destination's\n\t\t\t// account, add the account name into the Egress trace event.\n\t\t\t// This would happen with service imports.\n\t\t\tif dcAccName := dc.acc.GetName(); dcAccName != i.Account {\n\t\t\t\te.Account = dcAccName\n\t\t\t}\n\t\t}\n\t}\n\tt.event.Events = append(t.event.Events, e)\n}\n\nfunc (t *msgTrace) addStreamExportEvent(dc *client, to []byte) {\n\tif t == nil {\n\t\treturn\n\t}\n\tdc.mu.Lock()\n\taccName := dc.acc.GetName()\n\tdc.mu.Unlock()\n\tt.event.Events = append(t.event.Events, &MsgTraceStreamExport{\n\t\tMsgTraceBase: MsgTraceBase{\n\t\t\tType:      MsgTraceStreamExportType,\n\t\t\tTimestamp: time.Now(),\n\t\t},\n\t\tAccount: accName,\n\t\tTo:      string(to),\n\t})\n}\n\nfunc (t *msgTrace) addServiceImportEvent(accName, from, to string) {\n\tif t == nil {\n\t\treturn\n\t}\n\tt.event.Events = append(t.event.Events, &MsgTraceServiceImport{\n\t\tMsgTraceBase: MsgTraceBase{\n\t\t\tType:      MsgTraceServiceImportType,\n\t\t\tTimestamp: time.Now(),\n\t\t},\n\t\tAccount: accName,\n\t\tFrom:    from,\n\t\tTo:      to,\n\t})\n}\n\nfunc (t *msgTrace) addJetStreamEvent(streamName string) {\n\tif t == nil {\n\t\treturn\n\t}\n\tt.js = &MsgTraceJetStream{\n\t\tMsgTraceBase: MsgTraceBase{\n\t\t\tType:      MsgTraceJetStreamType,\n\t\t\tTimestamp: time.Now(),\n\t\t},\n\t\tStream: streamName,\n\t}\n\tt.event.Events = append(t.event.Events, t.js)\n}\n\nfunc (t *msgTrace) updateJetStreamEvent(subject string, noInterest bool) {\n\tif t == nil {\n\t\treturn\n\t}\n\t// JetStream event should have been created in addJetStreamEvent\n\tif t.js == nil {\n\t\treturn\n\t}\n\tt.js.Subject = subject\n\tt.js.NoInterest = noInterest\n\t// Update the timestamp since this is more accurate than when it\n\t// was first added in addJetStreamEvent().\n\tt.js.Timestamp = time.Now()\n}\n\nfunc (t *msgTrace) sendEventFromJetStream(err error) {\n\tif t == nil {\n\t\treturn\n\t}\n\t// JetStream event should have been created in addJetStreamEvent\n\tif t.js == nil {\n\t\treturn\n\t}\n\tif err != nil {\n\t\tt.js.Error = err.Error()\n\t}\n\tt.sendEvent()\n}\n\nfunc (t *msgTrace) sendEvent() {\n\tif t == nil {\n\t\treturn\n\t}\n\tif t.js != nil {\n\t\tready := atomic.AddInt32(&t.ready, 1) == 2\n\t\tif !ready {\n\t\t\treturn\n\t\t}\n\t}\n\tt.srv.sendInternalAccountSysMsg(t.acc, t.dest, &t.event.Server, t.event, t.ct)\n}\n",
    "source_file": "server/msgtrace.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2021-2025 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage server\n\nimport (\n\t\"bytes\"\n\t\"crypto/sha256\"\n\t\"crypto/tls\"\n\t\"crypto/x509\"\n\t\"encoding/asn1\"\n\t\"encoding/base64\"\n\t\"encoding/pem\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"net/http\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"golang.org/x/crypto/ocsp\"\n\n\t\"github.com/nats-io/nats-server/v2/server/certidp\"\n\t\"github.com/nats-io/nats-server/v2/server/certstore\"\n)\n\nconst (\n\tdefaultOCSPStoreDir      = \"ocsp\"\n\tdefaultOCSPCheckInterval = 24 * time.Hour\n\tminOCSPCheckInterval     = 2 * time.Minute\n)\n\ntype OCSPMode uint8\n\nconst (\n\t// OCSPModeAuto staples a status, only if \"status_request\" is set in cert.\n\tOCSPModeAuto OCSPMode = iota\n\n\t// OCSPModeAlways enforces OCSP stapling for certs and shuts down the server in\n\t// case a server is revoked or cannot get OCSP staples.\n\tOCSPModeAlways\n\n\t// OCSPModeNever disables OCSP stapling even if cert has Must-Staple flag.\n\tOCSPModeNever\n\n\t// OCSPModeMust honors the Must-Staple flag from a certificate but also causing shutdown\n\t// in case the certificate has been revoked.\n\tOCSPModeMust\n)\n\n// OCSPMonitor monitors the state of a staple per certificate.\ntype OCSPMonitor struct {\n\tkind     string\n\tmu       sync.Mutex\n\traw      []byte\n\tsrv      *Server\n\tcertFile string\n\tresp     *ocsp.Response\n\thc       *http.Client\n\tstopCh   chan struct{}\n\tLeaf     *x509.Certificate\n\tIssuer   *x509.Certificate\n\n\tshutdownOnRevoke bool\n}\n\nfunc (oc *OCSPMonitor) getNextRun() time.Duration {\n\toc.mu.Lock()\n\tnextUpdate := oc.resp.NextUpdate\n\toc.mu.Unlock()\n\n\tnow := time.Now()\n\tif nextUpdate.IsZero() {\n\t\t// If response is missing NextUpdate, we check the day after.\n\t\t// Technically, if NextUpdate is missing, we can try whenever.\n\t\t// https://tools.ietf.org/html/rfc6960#section-4.2.2.1\n\t\treturn defaultOCSPCheckInterval\n\t}\n\tdur := nextUpdate.Sub(now) / 2\n\n\t// If negative, then wait a couple of minutes before getting another staple.\n\tif dur < 0 {\n\t\treturn minOCSPCheckInterval\n\t}\n\n\treturn dur\n}\n\nfunc (oc *OCSPMonitor) getStatus() ([]byte, *ocsp.Response, error) {\n\traw, resp := oc.getCacheStatus()\n\tif len(raw) > 0 && resp != nil {\n\t\t// Check if the OCSP is still valid.\n\t\tif err := validOCSPResponse(resp); err == nil {\n\t\t\treturn raw, resp, nil\n\t\t}\n\t}\n\tvar err error\n\traw, resp, err = oc.getLocalStatus()\n\tif err == nil {\n\t\treturn raw, resp, nil\n\t}\n\n\treturn oc.getRemoteStatus()\n}\n\nfunc (oc *OCSPMonitor) getCacheStatus() ([]byte, *ocsp.Response) {\n\toc.mu.Lock()\n\tdefer oc.mu.Unlock()\n\treturn oc.raw, oc.resp\n}\n\nfunc (oc *OCSPMonitor) getLocalStatus() ([]byte, *ocsp.Response, error) {\n\topts := oc.srv.getOpts()\n\tstoreDir := opts.StoreDir\n\tif storeDir == _EMPTY_ {\n\t\treturn nil, nil, fmt.Errorf(\"store_dir not set\")\n\t}\n\n\t// This key must be based upon the current full certificate, not the public key,\n\t// so MUST be on the full raw certificate and not an SPKI or other reduced form.\n\tkey := fmt.Sprintf(\"%x\", sha256.Sum256(oc.Leaf.Raw))\n\n\toc.mu.Lock()\n\traw, err := os.ReadFile(filepath.Join(storeDir, defaultOCSPStoreDir, key))\n\toc.mu.Unlock()\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\n\tresp, err := ocsp.ParseResponse(raw, oc.Issuer)\n\tif err != nil {\n\t\treturn nil, nil, fmt.Errorf(\"failed to get local status: %w\", err)\n\t}\n\tif err := validOCSPResponse(resp); err != nil {\n\t\treturn nil, nil, err\n\t}\n\n\t// Cache the response.\n\toc.mu.Lock()\n\toc.raw = raw\n\toc.resp = resp\n\toc.mu.Unlock()\n\n\treturn raw, resp, nil\n}\n\nfunc (oc *OCSPMonitor) getRemoteStatus() ([]byte, *ocsp.Response, error) {\n\topts := oc.srv.getOpts()\n\tvar overrideURLs []string\n\tif config := opts.OCSPConfig; config != nil {\n\t\toverrideURLs = config.OverrideURLs\n\t}\n\tgetRequestBytes := func(u string, reqDER []byte, hc *http.Client) ([]byte, error) {\n\t\treqEnc := base64.StdEncoding.EncodeToString(reqDER)\n\t\tu = fmt.Sprintf(\"%s/%s\", u, reqEnc)\n\t\tstart := time.Now()\n\t\tresp, err := hc.Get(u)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tdefer resp.Body.Close()\n\n\t\toc.srv.Debugf(\"Received OCSP response (method=GET, status=%v, url=%s, duration=%.3fs)\",\n\t\t\tresp.StatusCode, u, time.Since(start).Seconds())\n\t\tif resp.StatusCode > 299 {\n\t\t\treturn nil, fmt.Errorf(\"non-ok http status on GET request (reqlen=%d): %d\", len(reqEnc), resp.StatusCode)\n\t\t}\n\t\treturn io.ReadAll(resp.Body)\n\t}\n\tpostRequestBytes := func(u string, body []byte, hc *http.Client) ([]byte, error) {\n\t\threq, err := http.NewRequest(\"POST\", u, bytes.NewReader(body))\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\threq.Header.Add(\"Content-Type\", \"application/ocsp-request\")\n\t\threq.Header.Add(\"Accept\", \"application/ocsp-response\")\n\n\t\tstart := time.Now()\n\t\tresp, err := hc.Do(hreq)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tdefer resp.Body.Close()\n\n\t\toc.srv.Debugf(\"Received OCSP response (method=POST, status=%v, url=%s, duration=%.3fs)\",\n\t\t\tresp.StatusCode, u, time.Since(start).Seconds())\n\t\tif resp.StatusCode > 299 {\n\t\t\treturn nil, fmt.Errorf(\"non-ok http status on POST request (reqlen=%d): %d\", len(body), resp.StatusCode)\n\t\t}\n\t\treturn io.ReadAll(resp.Body)\n\t}\n\n\t// Request documentation:\n\t// https://tools.ietf.org/html/rfc6960#appendix-A.1\n\n\treqDER, err := ocsp.CreateRequest(oc.Leaf, oc.Issuer, nil)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\n\tresponders := oc.Leaf.OCSPServer\n\tif len(overrideURLs) > 0 {\n\t\tresponders = overrideURLs\n\t}\n\tif len(responders) == 0 {\n\t\treturn nil, nil, fmt.Errorf(\"no available ocsp servers\")\n\t}\n\n\toc.mu.Lock()\n\thc := oc.hc\n\toc.mu.Unlock()\n\n\tvar raw []byte\n\tfor _, u := range responders {\n\t\tvar postErr, getErr error\n\t\tu = strings.TrimSuffix(u, \"/\")\n\t\t// Prefer to make POST requests first.\n\t\traw, postErr = postRequestBytes(u, reqDER, hc)\n\t\tif postErr == nil {\n\t\t\terr = nil\n\t\t\tbreak\n\t\t} else {\n\t\t\t// Fallback to use a GET request.\n\t\t\traw, getErr = getRequestBytes(u, reqDER, hc)\n\t\t\tif getErr == nil {\n\t\t\t\terr = nil\n\t\t\t\tbreak\n\t\t\t} else {\n\t\t\t\terr = errors.Join(postErr, getErr)\n\t\t\t}\n\t\t}\n\t}\n\tif err != nil {\n\t\treturn nil, nil, fmt.Errorf(\"exhausted ocsp servers: %w\", err)\n\t}\n\tresp, err := ocsp.ParseResponse(raw, oc.Issuer)\n\tif err != nil {\n\t\treturn nil, nil, fmt.Errorf(\"failed to get remote status: %w\", err)\n\t}\n\tif err := validOCSPResponse(resp); err != nil {\n\t\treturn nil, nil, err\n\t}\n\n\tif storeDir := opts.StoreDir; storeDir != _EMPTY_ {\n\t\tkey := fmt.Sprintf(\"%x\", sha256.Sum256(oc.Leaf.Raw))\n\t\tif err := oc.writeOCSPStatus(storeDir, key, raw); err != nil {\n\t\t\treturn nil, nil, fmt.Errorf(\"failed to write ocsp status: %w\", err)\n\t\t}\n\t}\n\n\toc.mu.Lock()\n\toc.raw = raw\n\toc.resp = resp\n\toc.mu.Unlock()\n\n\treturn raw, resp, nil\n}\n\nfunc (oc *OCSPMonitor) run() {\n\ts := oc.srv\n\ts.mu.Lock()\n\tquitCh := s.quitCh\n\ts.mu.Unlock()\n\n\tvar doShutdown bool\n\tdefer func() {\n\t\t// Need to decrement before shuting down, otherwise shutdown\n\t\t// would be stuck waiting on grWG to go down to 0.\n\t\ts.grWG.Done()\n\t\tif doShutdown {\n\t\t\ts.Shutdown()\n\t\t}\n\t}()\n\n\toc.mu.Lock()\n\tshutdownOnRevoke := oc.shutdownOnRevoke\n\tcertFile := oc.certFile\n\tstopCh := oc.stopCh\n\tkind := oc.kind\n\toc.mu.Unlock()\n\n\tvar nextRun time.Duration\n\t_, resp, err := oc.getStatus()\n\tif err == nil && resp.Status == ocsp.Good {\n\t\tnextRun = oc.getNextRun()\n\t\tt := resp.NextUpdate.Format(time.RFC3339Nano)\n\t\ts.Noticef(\n\t\t\t\"Found OCSP status for %s certificate at '%s': good, next update %s, checking again in %s\",\n\t\t\tkind, certFile, t, nextRun,\n\t\t)\n\t} else if err == nil && shutdownOnRevoke {\n\t\t// If resp.Status is ocsp.Revoked, ocsp.Unknown, or any other value.\n\t\ts.Errorf(\"Found OCSP status for %s certificate at '%s': %s\", kind, certFile, ocspStatusString(resp.Status))\n\t\tdoShutdown = true\n\t\treturn\n\t}\n\n\tfor {\n\t\t// On reload, if the certificate changes then need to stop this monitor.\n\t\tselect {\n\t\tcase <-time.After(nextRun):\n\t\tcase <-stopCh:\n\t\t\t// In case of reload and have to restart the OCSP stapling monitoring.\n\t\t\treturn\n\t\tcase <-quitCh:\n\t\t\t// Server quit channel.\n\t\t\treturn\n\t\t}\n\t\t_, resp, err := oc.getRemoteStatus()\n\t\tif err != nil {\n\t\t\tnextRun = oc.getNextRun()\n\t\t\ts.Errorf(\"Bad OCSP status update for certificate '%s': %s, trying again in %v\", certFile, err, nextRun)\n\t\t\tcontinue\n\t\t}\n\n\t\tswitch n := resp.Status; n {\n\t\tcase ocsp.Good:\n\t\t\tnextRun = oc.getNextRun()\n\t\t\tt := resp.NextUpdate.Format(time.RFC3339Nano)\n\t\t\ts.Noticef(\n\t\t\t\t\"Received OCSP status for %s certificate '%s': good, next update %s, checking again in %s\",\n\t\t\t\tkind, certFile, t, nextRun,\n\t\t\t)\n\t\t\tcontinue\n\t\tdefault:\n\t\t\ts.Errorf(\"Received OCSP status for %s certificate '%s': %s\", kind, certFile, ocspStatusString(n))\n\t\t\tif shutdownOnRevoke {\n\t\t\t\tdoShutdown = true\n\t\t\t}\n\t\t\treturn\n\t\t}\n\t}\n}\n\nfunc (oc *OCSPMonitor) stop() {\n\toc.mu.Lock()\n\tstopCh := oc.stopCh\n\toc.mu.Unlock()\n\tstopCh <- struct{}{}\n}\n\n// NewOCSPMonitor takes a TLS configuration then wraps it with the callbacks set for OCSP verification\n// along with a monitor that will periodically fetch OCSP staples.\nfunc (srv *Server) NewOCSPMonitor(config *tlsConfigKind) (*tls.Config, *OCSPMonitor, error) {\n\tkind := config.kind\n\ttc := config.tlsConfig\n\ttcOpts := config.tlsOpts\n\topts := srv.getOpts()\n\toc := opts.OCSPConfig\n\n\t// We need to track the CA certificate in case the CA is not present\n\t// in the chain to be able to verify the signature of the OCSP staple.\n\tvar (\n\t\tcertFile string\n\t\tcaFile   string\n\t)\n\tif kind == kindStringMap[CLIENT] {\n\t\ttcOpts = opts.tlsConfigOpts\n\t\tif opts.TLSCert != _EMPTY_ {\n\t\t\tcertFile = opts.TLSCert\n\t\t}\n\t\tif opts.TLSCaCert != _EMPTY_ {\n\t\t\tcaFile = opts.TLSCaCert\n\t\t}\n\t}\n\tif tcOpts != nil {\n\t\tcertFile = tcOpts.CertFile\n\t\tcaFile = tcOpts.CaFile\n\t}\n\n\t// NOTE: Currently OCSP Stapling is enabled only for the first certificate found.\n\tvar mon *OCSPMonitor\n\tfor _, currentCert := range tc.Certificates {\n\t\t// Create local copy since this will be used in the GetCertificate callback.\n\t\tcert := currentCert\n\n\t\t// This is normally non-nil, but can still be nil here when in tests\n\t\t// or in some embedded scenarios.\n\t\tif cert.Leaf == nil {\n\t\t\tif len(cert.Certificate) <= 0 {\n\t\t\t\treturn nil, nil, fmt.Errorf(\"no certificate found\")\n\t\t\t}\n\t\t\tvar err error\n\t\t\tcert.Leaf, err = x509.ParseCertificate(cert.Certificate[0])\n\t\t\tif err != nil {\n\t\t\t\treturn nil, nil, fmt.Errorf(\"error parsing certificate: %v\", err)\n\t\t\t}\n\t\t}\n\t\tvar shutdownOnRevoke bool\n\t\tmustStaple := hasOCSPStatusRequest(cert.Leaf)\n\t\tif oc != nil {\n\t\t\tswitch {\n\t\t\tcase oc.Mode == OCSPModeNever:\n\t\t\t\tif mustStaple {\n\t\t\t\t\tsrv.Warnf(\"Certificate at '%s' has MustStaple but OCSP is disabled\", certFile)\n\t\t\t\t}\n\t\t\t\treturn tc, nil, nil\n\t\t\tcase oc.Mode == OCSPModeAlways:\n\t\t\t\t// Start the monitor for this cert even if it does not have\n\t\t\t\t// the MustStaple flag and shutdown the server in case the\n\t\t\t\t// staple ever gets revoked.\n\t\t\t\tmustStaple = true\n\t\t\t\tshutdownOnRevoke = true\n\t\t\tcase oc.Mode == OCSPModeMust && mustStaple:\n\t\t\t\tshutdownOnRevoke = true\n\t\t\tcase oc.Mode == OCSPModeAuto && !mustStaple:\n\t\t\t\t// \"status_request\" MustStaple flag not set in certificate. No need to do anything.\n\t\t\t\treturn tc, nil, nil\n\t\t\t}\n\t\t}\n\t\tif !mustStaple {\n\t\t\t// No explicit OCSP config and cert does not have MustStaple flag either.\n\t\t\treturn tc, nil, nil\n\t\t}\n\n\t\tif err := srv.setupOCSPStapleStoreDir(); err != nil {\n\t\t\treturn nil, nil, err\n\t\t}\n\n\t\t// TODO: Add OCSP 'responder_cert' option in case CA cert not available.\n\t\tissuer, err := getOCSPIssuer(caFile, cert.Certificate)\n\t\tif err != nil {\n\t\t\treturn nil, nil, err\n\t\t}\n\n\t\tmon = &OCSPMonitor{\n\t\t\tkind:             kind,\n\t\t\tsrv:              srv,\n\t\t\thc:               &http.Client{Timeout: 30 * time.Second},\n\t\t\tshutdownOnRevoke: shutdownOnRevoke,\n\t\t\tcertFile:         certFile,\n\t\t\tstopCh:           make(chan struct{}, 1),\n\t\t\tLeaf:             cert.Leaf,\n\t\t\tIssuer:           issuer,\n\t\t}\n\n\t\t// Get the certificate status from the memory, then remote OCSP responder.\n\t\tif _, resp, err := mon.getStatus(); err != nil {\n\t\t\treturn nil, nil, fmt.Errorf(\"bad OCSP status update for certificate at '%s': %s\", certFile, err)\n\t\t} else if resp != nil && resp.Status != ocsp.Good && shutdownOnRevoke {\n\t\t\treturn nil, nil, fmt.Errorf(\"found existing OCSP status for certificate at '%s': %s\", certFile, ocspStatusString(resp.Status))\n\t\t}\n\n\t\t// Callbacks below will be in charge of returning the certificate instead,\n\t\t// so this has to be nil.\n\t\ttc.Certificates = nil\n\n\t\t// GetCertificate returns a certificate that's presented to a client.\n\t\ttc.GetCertificate = func(info *tls.ClientHelloInfo) (*tls.Certificate, error) {\n\t\t\tccert := cert\n\t\t\traw, _, err := mon.getStatus()\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\treturn &tls.Certificate{\n\t\t\t\tOCSPStaple:                   raw,\n\t\t\t\tCertificate:                  ccert.Certificate,\n\t\t\t\tPrivateKey:                   ccert.PrivateKey,\n\t\t\t\tSupportedSignatureAlgorithms: ccert.SupportedSignatureAlgorithms,\n\t\t\t\tSignedCertificateTimestamps:  ccert.SignedCertificateTimestamps,\n\t\t\t\tLeaf:                         ccert.Leaf,\n\t\t\t}, nil\n\t\t}\n\n\t\t// Check whether need to verify staples from a peer router or gateway connection.\n\t\tswitch kind {\n\t\tcase kindStringMap[ROUTER], kindStringMap[GATEWAY]:\n\t\t\ttc.VerifyConnection = func(s tls.ConnectionState) error {\n\t\t\t\toresp := s.OCSPResponse\n\t\t\t\tif oresp == nil {\n\t\t\t\t\treturn fmt.Errorf(\"%s peer missing OCSP Staple\", kind)\n\t\t\t\t}\n\n\t\t\t\t// Peer connections will verify the response of the staple.\n\t\t\t\tif len(s.VerifiedChains) == 0 {\n\t\t\t\t\treturn fmt.Errorf(\"%s peer missing TLS verified chains\", kind)\n\t\t\t\t}\n\n\t\t\t\tchain := s.VerifiedChains[0]\n\t\t\t\tpeerLeaf := chain[0]\n\t\t\t\tpeerIssuer := certidp.GetLeafIssuerCert(chain, 0)\n\t\t\t\tif peerIssuer == nil {\n\t\t\t\t\treturn fmt.Errorf(\"failed to get issuer certificate for %s peer\", kind)\n\t\t\t\t}\n\n\t\t\t\t// Response signature of issuer or issuer delegate is checked in the library parse\n\t\t\t\tresp, err := ocsp.ParseResponseForCert(oresp, peerLeaf, peerIssuer)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn fmt.Errorf(\"failed to parse OCSP response from %s peer: %w\", kind, err)\n\t\t\t\t}\n\n\t\t\t\t// If signer was issuer delegate double-check issuer delegate authorization\n\t\t\t\tif resp.Certificate != nil {\n\t\t\t\t\tok := false\n\t\t\t\t\tfor _, eku := range resp.Certificate.ExtKeyUsage {\n\t\t\t\t\t\tif eku == x509.ExtKeyUsageOCSPSigning {\n\t\t\t\t\t\t\tok = true\n\t\t\t\t\t\t\tbreak\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tif !ok {\n\t\t\t\t\t\treturn fmt.Errorf(\"OCSP staple's signer missing authorization by CA to act as OCSP signer\")\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\t// Check that the OCSP response is effective, take defaults for clockskew and default validity\n\t\t\t\tpeerOpts := certidp.OCSPPeerConfig{ClockSkew: -1, TTLUnsetNextUpdate: -1}\n\t\t\t\tsLog := certidp.Log{Debugf: srv.Debugf}\n\t\t\t\tif !certidp.OCSPResponseCurrent(resp, &peerOpts, &sLog) {\n\t\t\t\t\treturn fmt.Errorf(\"OCSP staple from %s peer not current\", kind)\n\t\t\t\t}\n\n\t\t\t\tif resp.Status != ocsp.Good {\n\t\t\t\t\treturn fmt.Errorf(\"bad status for OCSP Staple from %s peer: %s\", kind, ocspStatusString(resp.Status))\n\t\t\t\t}\n\n\t\t\t\treturn nil\n\t\t\t}\n\n\t\t\t// When server makes a peer connection, need to also present an OCSP Staple.\n\t\t\ttc.GetClientCertificate = func(info *tls.CertificateRequestInfo) (*tls.Certificate, error) {\n\t\t\t\tccert := cert\n\t\t\t\traw, _, err := mon.getStatus()\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn nil, err\n\t\t\t\t}\n\t\t\t\t// NOTE: crypto/tls.sendClientCertificate internally also calls getClientCertificate\n\t\t\t\t// so if for some reason these callbacks are triggered concurrently during a reconnect\n\t\t\t\t// there can be a race. To avoid that, the OCSP monitor lock is used to serialize access\n\t\t\t\t// to the staple which could also change inflight during an update.\n\t\t\t\tmon.mu.Lock()\n\t\t\t\tccert.OCSPStaple = raw\n\t\t\t\tmon.mu.Unlock()\n\n\t\t\t\treturn &ccert, nil\n\t\t\t}\n\t\tdefault:\n\t\t\t// GetClientCertificate returns a certificate that's presented to a server.\n\t\t\ttc.GetClientCertificate = func(info *tls.CertificateRequestInfo) (*tls.Certificate, error) {\n\t\t\t\treturn &cert, nil\n\t\t\t}\n\t\t}\n\t}\n\treturn tc, mon, nil\n}\n\nfunc (s *Server) setupOCSPStapleStoreDir() error {\n\topts := s.getOpts()\n\tstoreDir := opts.StoreDir\n\tif storeDir == _EMPTY_ {\n\t\treturn nil\n\t}\n\tstoreDir = filepath.Join(storeDir, defaultOCSPStoreDir)\n\tif stat, err := os.Stat(storeDir); os.IsNotExist(err) {\n\t\tif err := os.MkdirAll(storeDir, defaultDirPerms); err != nil {\n\t\t\treturn fmt.Errorf(\"could not create OCSP storage directory - %v\", err)\n\t\t}\n\t} else if stat == nil || !stat.IsDir() {\n\t\treturn fmt.Errorf(\"OCSP storage directory is not a directory\")\n\t}\n\treturn nil\n}\n\ntype tlsConfigKind struct {\n\ttlsConfig   *tls.Config\n\ttlsOpts     *TLSConfigOpts\n\tkind        string\n\tisLeafSpoke bool\n\tapply       func(*tls.Config)\n}\n\nfunc (s *Server) configureOCSP() []*tlsConfigKind {\n\tsopts := s.getOpts()\n\n\tconfigs := make([]*tlsConfigKind, 0)\n\n\tif config := sopts.TLSConfig; config != nil {\n\t\topts := sopts.tlsConfigOpts\n\t\to := &tlsConfigKind{\n\t\t\tkind:      kindStringMap[CLIENT],\n\t\t\ttlsConfig: config,\n\t\t\ttlsOpts:   opts,\n\t\t\tapply:     func(tc *tls.Config) { sopts.TLSConfig = tc },\n\t\t}\n\t\tconfigs = append(configs, o)\n\t}\n\tif config := sopts.Websocket.TLSConfig; config != nil {\n\t\topts := sopts.Websocket.tlsConfigOpts\n\t\to := &tlsConfigKind{\n\t\t\tkind:      kindStringMap[CLIENT],\n\t\t\ttlsConfig: config,\n\t\t\ttlsOpts:   opts,\n\t\t\tapply:     func(tc *tls.Config) { sopts.Websocket.TLSConfig = tc },\n\t\t}\n\t\tconfigs = append(configs, o)\n\t}\n\tif config := sopts.MQTT.TLSConfig; config != nil {\n\t\topts := sopts.tlsConfigOpts\n\t\to := &tlsConfigKind{\n\t\t\tkind:      kindStringMap[CLIENT],\n\t\t\ttlsConfig: config,\n\t\t\ttlsOpts:   opts,\n\t\t\tapply:     func(tc *tls.Config) { sopts.MQTT.TLSConfig = tc },\n\t\t}\n\t\tconfigs = append(configs, o)\n\t}\n\tif config := sopts.Cluster.TLSConfig; config != nil {\n\t\topts := sopts.Cluster.tlsConfigOpts\n\t\to := &tlsConfigKind{\n\t\t\tkind:      kindStringMap[ROUTER],\n\t\t\ttlsConfig: config,\n\t\t\ttlsOpts:   opts,\n\t\t\tapply:     func(tc *tls.Config) { sopts.Cluster.TLSConfig = tc },\n\t\t}\n\t\tconfigs = append(configs, o)\n\t}\n\tif config := sopts.LeafNode.TLSConfig; config != nil {\n\t\topts := sopts.LeafNode.tlsConfigOpts\n\t\to := &tlsConfigKind{\n\t\t\tkind:      kindStringMap[LEAF],\n\t\t\ttlsConfig: config,\n\t\t\ttlsOpts:   opts,\n\t\t\tapply:     func(tc *tls.Config) { sopts.LeafNode.TLSConfig = tc },\n\t\t}\n\t\tconfigs = append(configs, o)\n\t}\n\tfor _, remote := range sopts.LeafNode.Remotes {\n\t\tif config := remote.TLSConfig; config != nil {\n\t\t\t// Use a copy of the remote here since will be used\n\t\t\t// in the apply func callback below.\n\t\t\tr, opts := remote, remote.tlsConfigOpts\n\t\t\to := &tlsConfigKind{\n\t\t\t\tkind:        kindStringMap[LEAF],\n\t\t\t\ttlsConfig:   config,\n\t\t\t\ttlsOpts:     opts,\n\t\t\t\tisLeafSpoke: true,\n\t\t\t\tapply:       func(tc *tls.Config) { r.TLSConfig = tc },\n\t\t\t}\n\t\t\tconfigs = append(configs, o)\n\t\t}\n\t}\n\tif config := sopts.Gateway.TLSConfig; config != nil {\n\t\topts := sopts.Gateway.tlsConfigOpts\n\t\to := &tlsConfigKind{\n\t\t\tkind:      kindStringMap[GATEWAY],\n\t\t\ttlsConfig: config,\n\t\t\ttlsOpts:   opts,\n\t\t\tapply:     func(tc *tls.Config) { sopts.Gateway.TLSConfig = tc },\n\t\t}\n\t\tconfigs = append(configs, o)\n\t}\n\tfor _, remote := range sopts.Gateway.Gateways {\n\t\tif config := remote.TLSConfig; config != nil {\n\t\t\tgw, opts := remote, remote.tlsConfigOpts\n\t\t\to := &tlsConfigKind{\n\t\t\t\tkind:      kindStringMap[GATEWAY],\n\t\t\t\ttlsConfig: config,\n\t\t\t\ttlsOpts:   opts,\n\t\t\t\tapply:     func(tc *tls.Config) { gw.TLSConfig = tc },\n\t\t\t}\n\t\t\tconfigs = append(configs, o)\n\t\t}\n\t}\n\treturn configs\n}\n\nfunc (s *Server) enableOCSP() error {\n\tconfigs := s.configureOCSP()\n\n\tfor _, config := range configs {\n\n\t\t// We do not staple Leaf Hub and Leaf Spokes, use ocsp_peer\n\t\tif config.kind != kindStringMap[LEAF] {\n\t\t\t// OCSP Stapling feature, will also enable tls server peer check for gateway and route peers\n\t\t\ttc, mon, err := s.NewOCSPMonitor(config)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\t// Check if an OCSP stapling monitor is required for this certificate.\n\t\t\tif mon != nil {\n\t\t\t\ts.ocsps = append(s.ocsps, mon)\n\n\t\t\t\t// Override the TLS config with one that follows OCSP stapling\n\t\t\t\tconfig.apply(tc)\n\t\t\t}\n\t\t}\n\n\t\t// OCSP peer check (client mTLS, leaf mTLS, leaf remote TLS)\n\t\tif config.kind == kindStringMap[CLIENT] || config.kind == kindStringMap[LEAF] {\n\t\t\ttc, plugged, err := s.plugTLSOCSPPeer(config)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tif plugged && tc != nil {\n\t\t\t\ts.ocspPeerVerify = true\n\t\t\t\tconfig.apply(tc)\n\t\t\t}\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc (s *Server) startOCSPMonitoring() {\n\ts.mu.Lock()\n\tocsps := s.ocsps\n\ts.mu.Unlock()\n\tif ocsps == nil {\n\t\treturn\n\t}\n\tfor _, mon := range ocsps {\n\t\tm := mon\n\t\tm.mu.Lock()\n\t\tkind := m.kind\n\t\tm.mu.Unlock()\n\t\ts.Noticef(\"OCSP Stapling enabled for %s connections\", kind)\n\t\ts.startGoRoutine(func() { m.run() })\n\t}\n}\n\nfunc (s *Server) reloadOCSP() error {\n\tif err := s.setupOCSPStapleStoreDir(); err != nil {\n\t\treturn err\n\t}\n\n\ts.mu.Lock()\n\tocsps := s.ocsps\n\ts.mu.Unlock()\n\n\t// Stop all OCSP Stapling monitors in case there were any running.\n\tfor _, oc := range ocsps {\n\t\toc.stop()\n\t}\n\n\tconfigs := s.configureOCSP()\n\n\t// Restart the monitors under the new configuration.\n\tocspm := make([]*OCSPMonitor, 0)\n\n\t// Reset server's ocspPeerVerify flag to re-detect at least one plugged OCSP peer\n\ts.mu.Lock()\n\ts.ocspPeerVerify = false\n\ts.mu.Unlock()\n\ts.stopOCSPResponseCache()\n\n\tfor _, config := range configs {\n\t\t// We do not staple Leaf Hub and Leaf Spokes, use ocsp_peer\n\t\tif config.kind != kindStringMap[LEAF] {\n\t\t\ttc, mon, err := s.NewOCSPMonitor(config)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\t// Check if an OCSP stapling monitor is required for this certificate.\n\t\t\tif mon != nil {\n\t\t\t\tocspm = append(ocspm, mon)\n\n\t\t\t\t// Apply latest TLS configuration after OCSP monitors have started.\n\t\t\t\tdefer config.apply(tc)\n\t\t\t}\n\t\t}\n\n\t\t// OCSP peer check (client mTLS, leaf mTLS, leaf remote TLS)\n\t\tif config.kind == kindStringMap[CLIENT] || config.kind == kindStringMap[LEAF] {\n\t\t\ttc, plugged, err := s.plugTLSOCSPPeer(config)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tif plugged && tc != nil {\n\t\t\t\ts.ocspPeerVerify = true\n\t\t\t\tdefer config.apply(tc)\n\t\t\t}\n\t\t}\n\t}\n\n\t// Replace stopped monitors with the new ones.\n\ts.mu.Lock()\n\ts.ocsps = ocspm\n\ts.mu.Unlock()\n\n\t// Dispatch all goroutines once again.\n\ts.startOCSPMonitoring()\n\n\t// Init and restart OCSP responder cache\n\ts.stopOCSPResponseCache()\n\ts.initOCSPResponseCache()\n\ts.startOCSPResponseCache()\n\n\treturn nil\n}\n\nfunc hasOCSPStatusRequest(cert *x509.Certificate) bool {\n\t// OID for id-pe-tlsfeature defined in RFC here:\n\t// https://datatracker.ietf.org/doc/html/rfc7633\n\ttlsFeatures := asn1.ObjectIdentifier{1, 3, 6, 1, 5, 5, 7, 1, 24}\n\tconst statusRequestExt = 5\n\n\t// Example values:\n\t// * [48 3 2 1 5] - seen when creating own certs locally\n\t// * [30 3 2 1 5] - seen in the wild\n\t// Documentation:\n\t// https://tools.ietf.org/html/rfc6066\n\n\tfor _, ext := range cert.Extensions {\n\t\tif !ext.Id.Equal(tlsFeatures) {\n\t\t\tcontinue\n\t\t}\n\n\t\tvar val []int\n\t\trest, err := asn1.Unmarshal(ext.Value, &val)\n\t\tif err != nil || len(rest) > 0 {\n\t\t\treturn false\n\t\t}\n\n\t\tfor _, n := range val {\n\t\t\tif n == statusRequestExt {\n\t\t\t\treturn true\n\t\t\t}\n\t\t}\n\t\tbreak\n\t}\n\n\treturn false\n}\n\n// writeOCSPStatus writes an OCSP status to a temporary file then moves it to a\n// new path, in an attempt to avoid corrupting existing data.\nfunc (oc *OCSPMonitor) writeOCSPStatus(storeDir, file string, data []byte) error {\n\tstoreDir = filepath.Join(storeDir, defaultOCSPStoreDir)\n\ttmp, err := os.CreateTemp(storeDir, \"tmp-cert-status\")\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif _, err := tmp.Write(data); err != nil {\n\t\ttmp.Close()\n\t\tos.Remove(tmp.Name())\n\t\treturn err\n\t}\n\tif err := tmp.Close(); err != nil {\n\t\treturn err\n\t}\n\n\toc.mu.Lock()\n\terr = os.Rename(tmp.Name(), filepath.Join(storeDir, file))\n\toc.mu.Unlock()\n\tif err != nil {\n\t\tos.Remove(tmp.Name())\n\t\treturn err\n\t}\n\n\treturn nil\n}\n\nfunc parseCertPEM(name string) ([]*x509.Certificate, error) {\n\tdata, err := os.ReadFile(name)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tvar pemBytes []byte\n\n\tvar block *pem.Block\n\tfor len(data) != 0 {\n\t\tblock, data = pem.Decode(data)\n\t\tif block == nil {\n\t\t\tbreak\n\t\t}\n\t\tif block.Type != \"CERTIFICATE\" {\n\t\t\treturn nil, fmt.Errorf(\"unexpected PEM certificate type: %s\", block.Type)\n\t\t}\n\n\t\tpemBytes = append(pemBytes, block.Bytes...)\n\t}\n\n\treturn x509.ParseCertificates(pemBytes)\n}\n\n// getOCSPIssuerLocally determines a leaf's issuer from locally configured certificates\nfunc getOCSPIssuerLocally(trustedCAs []*x509.Certificate, certBundle []*x509.Certificate) (*x509.Certificate, error) {\n\tvar vOpts x509.VerifyOptions\n\tvar leaf *x509.Certificate\n\ttrustedCAPool := x509.NewCertPool()\n\n\t// Require Leaf as first cert in bundle\n\tif len(certBundle) > 0 {\n\t\tleaf = certBundle[0]\n\t} else {\n\t\treturn nil, fmt.Errorf(\"invalid ocsp ca configuration\")\n\t}\n\n\t// Allow Issuer to be configured as second cert in bundle\n\tif len(certBundle) > 1 {\n\t\t// The operator may have misconfigured the cert bundle\n\t\tissuerCandidate := certBundle[1]\n\t\terr := issuerCandidate.CheckSignature(leaf.SignatureAlgorithm, leaf.RawTBSCertificate, leaf.Signature)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"invalid issuer configuration: %w\", err)\n\t\t} else {\n\t\t\treturn issuerCandidate, nil\n\t\t}\n\t}\n\n\t// Operator did not provide the Leaf Issuer in cert bundle second position\n\t// so we will attempt to create at least one ordered verified chain from the\n\t// trusted CA pool.\n\n\t// Specify CA trust store to validator; if unset, system trust store used\n\tif len(trustedCAs) > 0 {\n\t\tfor _, ca := range trustedCAs {\n\t\t\ttrustedCAPool.AddCert(ca)\n\t\t}\n\t\tvOpts.Roots = trustedCAPool\n\t}\n\n\treturn certstore.GetLeafIssuer(leaf, vOpts), nil\n}\n\n// getOCSPIssuer determines an issuer certificate from the cert (bundle) or the file-based CA trust store\nfunc getOCSPIssuer(caFile string, chain [][]byte) (*x509.Certificate, error) {\n\tvar issuer *x509.Certificate\n\tvar trustedCAs []*x509.Certificate\n\tvar certBundle []*x509.Certificate\n\tvar err error\n\n\t// FIXME(tgb): extend if pluggable CA store provider added to NATS (i.e. other than PEM file)\n\n\t// Non-system default CA trust store passed\n\tif caFile != _EMPTY_ {\n\t\ttrustedCAs, err = parseCertPEM(caFile)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"failed to parse ca_file: %v\", err)\n\t\t}\n\t}\n\n\t// Specify bundled intermediate CA store\n\tfor _, certBytes := range chain {\n\t\tcert, err := x509.ParseCertificate(certBytes)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"failed to parse cert: %v\", err)\n\t\t}\n\t\tcertBundle = append(certBundle, cert)\n\t}\n\n\tissuer, err = getOCSPIssuerLocally(trustedCAs, certBundle)\n\tif err != nil || issuer == nil {\n\t\treturn nil, fmt.Errorf(\"no issuers found\")\n\t}\n\n\tif !issuer.IsCA {\n\t\treturn nil, fmt.Errorf(\"%s invalid ca basic constraints: is not ca\", issuer.Subject)\n\t}\n\treturn issuer, nil\n}\n\nfunc ocspStatusString(n int) string {\n\tswitch n {\n\tcase ocsp.Good:\n\t\treturn \"good\"\n\tcase ocsp.Revoked:\n\t\treturn \"revoked\"\n\tdefault:\n\t\treturn \"unknown\"\n\t}\n}\n\nfunc validOCSPResponse(r *ocsp.Response) error {\n\t// Time validation not handled by ParseResponse.\n\t// https://tools.ietf.org/html/rfc6960#section-4.2.2.1\n\tif !r.NextUpdate.IsZero() && r.NextUpdate.Before(time.Now()) {\n\t\tt := r.NextUpdate.Format(time.RFC3339Nano)\n\t\treturn fmt.Errorf(\"invalid ocsp NextUpdate, is past time: %s\", t)\n\t}\n\tif r.ThisUpdate.After(time.Now()) {\n\t\tt := r.ThisUpdate.Format(time.RFC3339Nano)\n\t\treturn fmt.Errorf(\"invalid ocsp ThisUpdate, is future time: %s\", t)\n\t}\n\n\treturn nil\n}\n",
    "source_file": "server/ocsp.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2012-2025 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n//go:build !windows && !wasm\n\npackage server\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\t\"os\"\n\t\"os/exec\"\n\t\"os/signal\"\n\t\"strconv\"\n\t\"strings\"\n\t\"syscall\"\n)\n\nvar processName = \"nats-server\"\n\n// SetProcessName allows to change the expected name of the process.\nfunc SetProcessName(name string) {\n\tprocessName = name\n}\n\n// Signal Handling\nfunc (s *Server) handleSignals() {\n\tif s.getOpts().NoSigs {\n\t\treturn\n\t}\n\tc := make(chan os.Signal, 1)\n\n\tsignal.Notify(c, syscall.SIGINT, syscall.SIGTERM, syscall.SIGUSR1, syscall.SIGUSR2, syscall.SIGHUP)\n\n\tgo func() {\n\t\tfor {\n\t\t\tselect {\n\t\t\tcase sig := <-c:\n\t\t\t\ts.Noticef(\"Trapped %q signal\", sig)\n\t\t\t\tswitch sig {\n\t\t\t\tcase syscall.SIGINT:\n\t\t\t\t\ts.Shutdown()\n\t\t\t\t\ts.WaitForShutdown()\n\t\t\t\t\tos.Exit(0)\n\t\t\t\tcase syscall.SIGTERM:\n\t\t\t\t\t// Shutdown unless graceful shutdown already in progress.\n\t\t\t\t\ts.mu.Lock()\n\t\t\t\t\tldm := s.ldm\n\t\t\t\t\ts.mu.Unlock()\n\n\t\t\t\t\tif !ldm {\n\t\t\t\t\t\ts.Shutdown()\n\t\t\t\t\t\ts.WaitForShutdown()\n\t\t\t\t\t\tos.Exit(0)\n\t\t\t\t\t}\n\t\t\t\tcase syscall.SIGUSR1:\n\t\t\t\t\t// File log re-open for rotating file logs.\n\t\t\t\t\ts.ReOpenLogFile()\n\t\t\t\tcase syscall.SIGUSR2:\n\t\t\t\t\tgo s.lameDuckMode()\n\t\t\t\tcase syscall.SIGHUP:\n\t\t\t\t\t// Config reload.\n\t\t\t\t\tif err := s.Reload(); err != nil {\n\t\t\t\t\t\ts.Errorf(\"Failed to reload server configuration: %s\", err)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\tcase <-s.quitCh:\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}()\n}\n\n// ProcessSignal sends the given signal command to the given process. If pidStr\n// is empty, this will send the signal to the single running instance of\n// nats-server. If multiple instances are running, pidStr can be a globular\n// expression ending with '*'. This returns an error if the given process is\n// not running or the command is invalid.\nfunc ProcessSignal(command Command, pidExpr string) error {\n\tvar (\n\t\terr    error\n\t\terrStr string\n\t\tpids   = make([]int, 1)\n\t\tpidStr = strings.TrimSuffix(pidExpr, \"*\")\n\t\tisGlob = strings.HasSuffix(pidExpr, \"*\")\n\t)\n\n\t// Validate input if given\n\tif pidStr != \"\" {\n\t\tif pids[0], err = strconv.Atoi(pidStr); err != nil {\n\t\t\treturn fmt.Errorf(\"invalid pid: %s\", pidStr)\n\t\t}\n\t}\n\t// Gather all PIDs unless the input is specific\n\tif pidStr == \"\" || isGlob {\n\t\tif pids, err = resolvePids(); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\t// Multiple instances are running and the input is not an expression\n\tif len(pids) > 1 && !isGlob {\n\t\terrStr = fmt.Sprintf(\"multiple %s processes running:\", processName)\n\t\tfor _, p := range pids {\n\t\t\terrStr += fmt.Sprintf(\"\\n%d\", p)\n\t\t}\n\t\treturn errors.New(errStr)\n\t}\n\t// No instances are running\n\tif len(pids) == 0 {\n\t\treturn fmt.Errorf(\"no %s processes running\", processName)\n\t}\n\n\tvar signum syscall.Signal\n\tif signum, err = CommandToSignal(command); err != nil {\n\t\treturn err\n\t}\n\n\tfor _, pid := range pids {\n\t\tif _pidStr := strconv.Itoa(pid); _pidStr != pidStr && pidStr != \"\" {\n\t\t\tif !isGlob || !strings.HasPrefix(_pidStr, pidStr) {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t}\n\t\tif err = kill(pid, signum); err != nil {\n\t\t\terrStr += fmt.Sprintf(\"\\nsignal %q %d: %s\", command, pid, err)\n\t\t}\n\t}\n\tif errStr != \"\" {\n\t\treturn errors.New(errStr)\n\t}\n\treturn nil\n}\n\n// Translates a command to a signal number\nfunc CommandToSignal(command Command) (syscall.Signal, error) {\n\tswitch command {\n\tcase CommandStop:\n\t\treturn syscall.SIGKILL, nil\n\tcase CommandQuit:\n\t\treturn syscall.SIGINT, nil\n\tcase CommandReopen:\n\t\treturn syscall.SIGUSR1, nil\n\tcase CommandReload:\n\t\treturn syscall.SIGHUP, nil\n\tcase commandLDMode:\n\t\treturn syscall.SIGUSR2, nil\n\tcase commandTerm:\n\t\treturn syscall.SIGTERM, nil\n\tdefault:\n\t\treturn 0, fmt.Errorf(\"unknown signal %q\", command)\n\t}\n}\n\n// resolvePids returns the pids for all running nats-server processes.\nfunc resolvePids() ([]int, error) {\n\t// If pgrep isn't available, this will just bail out and the user will be\n\t// required to specify a pid.\n\toutput, err := pgrep()\n\tif err != nil {\n\t\tswitch err.(type) {\n\t\tcase *exec.ExitError:\n\t\t\t// ExitError indicates non-zero exit code, meaning no processes\n\t\t\t// found.\n\t\t\tbreak\n\t\tdefault:\n\t\t\treturn nil, errors.New(\"unable to resolve pid, try providing one\")\n\t\t}\n\t}\n\tvar (\n\t\tmyPid   = os.Getpid()\n\t\tpidStrs = strings.Split(string(output), \"\\n\")\n\t\tpids    = make([]int, 0, len(pidStrs))\n\t)\n\tfor _, pidStr := range pidStrs {\n\t\tif pidStr == \"\" {\n\t\t\tcontinue\n\t\t}\n\t\tpid, err := strconv.Atoi(pidStr)\n\t\tif err != nil {\n\t\t\treturn nil, errors.New(\"unable to resolve pid, try providing one\")\n\t\t}\n\t\t// Ignore the current process.\n\t\tif pid == myPid {\n\t\t\tcontinue\n\t\t}\n\t\tpids = append(pids, pid)\n\t}\n\treturn pids, nil\n}\n\nvar kill = func(pid int, signal syscall.Signal) error {\n\treturn syscall.Kill(pid, signal)\n}\n\nvar pgrep = func() ([]byte, error) {\n\treturn exec.Command(\"pgrep\", processName).Output()\n}\n",
    "source_file": "server/signal.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2013-2025 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage server\n\nimport (\n\t\"bytes\"\n\t\"cmp\"\n\t\"crypto/sha256\"\n\t\"crypto/tls\"\n\t\"crypto/x509\"\n\t\"encoding/hex\"\n\t\"encoding/json\"\n\t\"expvar\"\n\t\"fmt\"\n\t\"maps\"\n\t\"math\"\n\t\"net\"\n\t\"net/http\"\n\t\"net/url\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"runtime\"\n\t\"runtime/debug\"\n\t\"runtime/pprof\"\n\t\"slices\"\n\t\"sort\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t\"github.com/nats-io/jwt/v2\"\n\t\"github.com/nats-io/nats-server/v2/server/pse\"\n)\n\n// Connz represents detailed information on current client connections.\ntype Connz struct {\n\tID       string      `json:\"server_id\"`\n\tNow      time.Time   `json:\"now\"`\n\tNumConns int         `json:\"num_connections\"`\n\tTotal    int         `json:\"total\"`\n\tOffset   int         `json:\"offset\"`\n\tLimit    int         `json:\"limit\"`\n\tConns    []*ConnInfo `json:\"connections\"`\n}\n\n// ConnzOptions are the options passed to Connz()\ntype ConnzOptions struct {\n\t// Sort indicates how the results will be sorted. Check SortOpt for possible values.\n\t// Only the sort by connection ID (ByCid) is ascending, all others are descending.\n\tSort SortOpt `json:\"sort\"`\n\n\t// Username indicates if user names should be included in the results.\n\tUsername bool `json:\"auth\"`\n\n\t// Subscriptions indicates if subscriptions should be included in the results.\n\tSubscriptions bool `json:\"subscriptions\"`\n\n\t// SubscriptionsDetail indicates if subscription details should be included in the results\n\tSubscriptionsDetail bool `json:\"subscriptions_detail\"`\n\n\t// Offset is used for pagination. Connz() only returns connections starting at this\n\t// offset from the global results.\n\tOffset int `json:\"offset\"`\n\n\t// Limit is the maximum number of connections that should be returned by Connz().\n\tLimit int `json:\"limit\"`\n\n\t// Filter for this explicit client connection.\n\tCID uint64 `json:\"cid\"`\n\n\t// Filter for this explicit client connection based on the MQTT client ID\n\tMQTTClient string `json:\"mqtt_client\"`\n\n\t// Filter by connection state.\n\tState ConnState `json:\"state\"`\n\n\t// The below options only apply if auth is true.\n\n\t// Filter by username.\n\tUser string `json:\"user\"`\n\n\t// Filter by account.\n\tAccount string `json:\"acc\"`\n\n\t// Filter by subject interest\n\tFilterSubject string `json:\"filter_subject\"`\n}\n\n// ConnState is for filtering states of connections. We will only have two, open and closed.\ntype ConnState int\n\nconst (\n\t// ConnOpen filters on open clients.\n\tConnOpen = ConnState(iota)\n\t// ConnClosed filters on closed clients.\n\tConnClosed\n\t// ConnAll returns all clients.\n\tConnAll\n)\n\n// ConnInfo has detailed information on a per connection basis.\ntype ConnInfo struct {\n\tCid            uint64         `json:\"cid\"`\n\tKind           string         `json:\"kind,omitempty\"`\n\tType           string         `json:\"type,omitempty\"`\n\tIP             string         `json:\"ip\"`\n\tPort           int            `json:\"port\"`\n\tStart          time.Time      `json:\"start\"`\n\tLastActivity   time.Time      `json:\"last_activity\"`\n\tStop           *time.Time     `json:\"stop,omitempty\"`\n\tReason         string         `json:\"reason,omitempty\"`\n\tRTT            string         `json:\"rtt,omitempty\"`\n\tUptime         string         `json:\"uptime\"`\n\tIdle           string         `json:\"idle\"`\n\tPending        int            `json:\"pending_bytes\"`\n\tInMsgs         int64          `json:\"in_msgs\"`\n\tOutMsgs        int64          `json:\"out_msgs\"`\n\tInBytes        int64          `json:\"in_bytes\"`\n\tOutBytes       int64          `json:\"out_bytes\"`\n\tNumSubs        uint32         `json:\"subscriptions\"`\n\tName           string         `json:\"name,omitempty\"`\n\tLang           string         `json:\"lang,omitempty\"`\n\tVersion        string         `json:\"version,omitempty\"`\n\tTLSVersion     string         `json:\"tls_version,omitempty\"`\n\tTLSCipher      string         `json:\"tls_cipher_suite,omitempty\"`\n\tTLSPeerCerts   []*TLSPeerCert `json:\"tls_peer_certs,omitempty\"`\n\tTLSFirst       bool           `json:\"tls_first,omitempty\"`\n\tAuthorizedUser string         `json:\"authorized_user,omitempty\"`\n\tAccount        string         `json:\"account,omitempty\"`\n\tSubs           []string       `json:\"subscriptions_list,omitempty\"`\n\tSubsDetail     []SubDetail    `json:\"subscriptions_list_detail,omitempty\"`\n\tJWT            string         `json:\"jwt,omitempty\"`\n\tIssuerKey      string         `json:\"issuer_key,omitempty\"`\n\tNameTag        string         `json:\"name_tag,omitempty\"`\n\tTags           jwt.TagList    `json:\"tags,omitempty\"`\n\tMQTTClient     string         `json:\"mqtt_client,omitempty\"` // This is the MQTT client id\n\n\t// Internal\n\trtt int64 // For fast sorting\n}\n\n// TLSPeerCert contains basic information about a TLS peer certificate\ntype TLSPeerCert struct {\n\tSubject          string `json:\"subject,omitempty\"`\n\tSubjectPKISha256 string `json:\"spki_sha256,omitempty\"`\n\tCertSha256       string `json:\"cert_sha256,omitempty\"`\n}\n\n// DefaultConnListSize is the default size of the connection list.\nconst DefaultConnListSize = 1024\n\n// DefaultSubListSize is the default size of the subscriptions list.\nconst DefaultSubListSize = 1024\n\nconst defaultStackBufSize = 10000\n\nfunc newSubsDetailList(client *client) []SubDetail {\n\tsubsDetail := make([]SubDetail, 0, len(client.subs))\n\tfor _, sub := range client.subs {\n\t\tsubsDetail = append(subsDetail, newClientSubDetail(sub))\n\t}\n\treturn subsDetail\n}\n\nfunc newSubsList(client *client) []string {\n\tsubs := make([]string, 0, len(client.subs))\n\tfor _, sub := range client.subs {\n\t\tsubs = append(subs, string(sub.subject))\n\t}\n\treturn subs\n}\n\n// Connz returns a Connz struct containing information about connections.\nfunc (s *Server) Connz(opts *ConnzOptions) (*Connz, error) {\n\tvar (\n\t\tsortOpt = ByCid\n\t\tauth    bool\n\t\tsubs    bool\n\t\tsubsDet bool\n\t\toffset  int\n\t\tlimit   = DefaultConnListSize\n\t\tcid     = uint64(0)\n\t\tstate   = ConnOpen\n\t\tuser    string\n\t\tacc     string\n\t\ta       *Account\n\t\tfilter  string\n\t\tmqttCID string\n\t)\n\n\tif opts != nil {\n\t\t// If no sort option given or sort is by uptime, then sort by cid\n\t\tif opts.Sort != _EMPTY_ {\n\t\t\tsortOpt = opts.Sort\n\t\t\tif !sortOpt.IsValid() {\n\t\t\t\treturn nil, fmt.Errorf(\"invalid sorting option: %s\", sortOpt)\n\t\t\t}\n\t\t}\n\n\t\t// Auth specifics.\n\t\tauth = opts.Username\n\t\tuser = opts.User\n\t\tacc = opts.Account\n\t\tmqttCID = opts.MQTTClient\n\n\t\tsubs = opts.Subscriptions\n\t\tsubsDet = opts.SubscriptionsDetail\n\t\toffset = opts.Offset\n\t\tif offset < 0 {\n\t\t\toffset = 0\n\t\t}\n\t\tlimit = opts.Limit\n\t\tif limit <= 0 {\n\t\t\tlimit = DefaultConnListSize\n\t\t}\n\t\t// state\n\t\tstate = opts.State\n\n\t\t// ByStop only makes sense on closed connections\n\t\tif sortOpt == ByStop && state != ConnClosed {\n\t\t\treturn nil, fmt.Errorf(\"sort by stop only valid on closed connections\")\n\t\t}\n\t\t// ByReason is the same.\n\t\tif sortOpt == ByReason && state != ConnClosed {\n\t\t\treturn nil, fmt.Errorf(\"sort by reason only valid on closed connections\")\n\t\t}\n\t\t// If searching by CID\n\t\tif opts.CID > 0 {\n\t\t\tcid = opts.CID\n\t\t\tlimit = 1\n\t\t}\n\t\t// If filtering by subject.\n\t\tif opts.FilterSubject != _EMPTY_ && opts.FilterSubject != fwcs {\n\t\t\tif acc == _EMPTY_ {\n\t\t\t\treturn nil, fmt.Errorf(\"filter by subject only valid with account filtering\")\n\t\t\t}\n\t\t\tfilter = opts.FilterSubject\n\t\t}\n\t}\n\n\tc := &Connz{\n\t\tOffset: offset,\n\t\tLimit:  limit,\n\t\tNow:    time.Now().UTC(),\n\t}\n\n\t// Open clients\n\tvar openClients []*client\n\t// Hold for closed clients if requested.\n\tvar closedClients []*closedClient\n\n\tvar clist map[uint64]*client\n\n\tif acc != _EMPTY_ {\n\t\tvar err error\n\t\ta, err = s.lookupAccount(acc)\n\t\tif err != nil {\n\t\t\treturn c, nil\n\t\t}\n\t\ta.mu.RLock()\n\t\tclist = make(map[uint64]*client, a.numLocalConnections())\n\t\tfor c := range a.clients {\n\t\t\tif c.kind == CLIENT || c.kind == LEAF {\n\t\t\t\tclist[c.cid] = c\n\t\t\t}\n\t\t}\n\t\ta.mu.RUnlock()\n\t}\n\n\t// Walk the open client list with server lock held.\n\ts.mu.RLock()\n\t// Default to all client unless filled in above.\n\tif clist == nil {\n\t\tclist = make(map[uint64]*client, len(s.clients)+len(s.leafs))\n\t\tmaps.Copy(clist, s.clients)\n\t\tmaps.Copy(clist, s.leafs)\n\t}\n\n\t// copy the server id for monitoring\n\tc.ID = s.info.ID\n\n\t// Number of total clients. The resulting ConnInfo array\n\t// may be smaller if pagination is used.\n\tswitch state {\n\tcase ConnOpen:\n\t\tc.Total = len(clist)\n\tcase ConnClosed:\n\t\tclosedClients = s.closed.closedClients()\n\t\tc.Total = len(closedClients)\n\tcase ConnAll:\n\t\tc.Total = len(clist)\n\t\tclosedClients = s.closed.closedClients()\n\t\tc.Total += len(closedClients)\n\t}\n\n\t// We may need to filter these connections.\n\tif acc != _EMPTY_ && len(closedClients) > 0 {\n\t\tvar ccc []*closedClient\n\t\tfor _, cc := range closedClients {\n\t\t\tif cc.acc != acc {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tccc = append(ccc, cc)\n\t\t}\n\t\tc.Total -= (len(closedClients) - len(ccc))\n\t\tclosedClients = ccc\n\t}\n\n\ttotalClients := c.Total\n\tif cid > 0 { // Meaning we only want 1.\n\t\ttotalClients = 1\n\t}\n\tif state == ConnOpen || state == ConnAll {\n\t\topenClients = make([]*client, 0, totalClients)\n\t}\n\n\t// Data structures for results.\n\tvar conns []ConnInfo // Limits allocs for actual ConnInfos.\n\tvar pconns ConnInfos\n\n\tswitch state {\n\tcase ConnOpen:\n\t\tconns = make([]ConnInfo, totalClients)\n\t\tpconns = make(ConnInfos, totalClients)\n\tcase ConnClosed:\n\t\tpconns = make(ConnInfos, totalClients)\n\tcase ConnAll:\n\t\tconns = make([]ConnInfo, cap(openClients))\n\t\tpconns = make(ConnInfos, totalClients)\n\t}\n\n\t// Search by individual CID.\n\tif cid > 0 {\n\t\t// Let's first check if user also selects on ConnOpen or ConnAll\n\t\t// and look for opened connections.\n\t\tif state == ConnOpen || state == ConnAll {\n\t\t\tif client := s.clients[cid]; client != nil {\n\t\t\t\topenClients = append(openClients, client)\n\t\t\t\tclosedClients = nil\n\t\t\t}\n\t\t}\n\t\t// If we did not find, and the user selected for ConnClosed or ConnAll,\n\t\t// look for closed connections.\n\t\tif len(openClients) == 0 && (state == ConnClosed || state == ConnAll) {\n\t\t\tcopyClosed := closedClients\n\t\t\tclosedClients = nil\n\t\t\tfor _, cc := range copyClosed {\n\t\t\t\tif cc.Cid == cid {\n\t\t\t\t\tclosedClients = []*closedClient{cc}\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t} else {\n\t\t// Gather all open clients.\n\t\tif state == ConnOpen || state == ConnAll {\n\t\t\tfor _, client := range clist {\n\t\t\t\t// If we have an account specified we need to filter.\n\t\t\t\tif acc != _EMPTY_ && (client.acc == nil || client.acc.Name != acc) {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\t// Do user filtering second\n\t\t\t\tif user != _EMPTY_ && client.getRawAuthUserLock() != user {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\t// Do mqtt client ID filtering next\n\t\t\t\tif mqttCID != _EMPTY_ && client.getMQTTClientID() != mqttCID {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\topenClients = append(openClients, client)\n\t\t\t}\n\t\t}\n\t}\n\ts.mu.RUnlock()\n\n\t// Filter by subject now if needed. We do this outside of server lock.\n\tif filter != _EMPTY_ {\n\t\tvar oc []*client\n\t\tfor _, c := range openClients {\n\t\t\tc.mu.Lock()\n\t\t\tfor _, sub := range c.subs {\n\t\t\t\tif SubjectsCollide(filter, string(sub.subject)) {\n\t\t\t\t\toc = append(oc, c)\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t\tc.mu.Unlock()\n\t\t\topenClients = oc\n\t\t}\n\t}\n\n\t// Just return with empty array if nothing here.\n\tif len(openClients) == 0 && len(closedClients) == 0 {\n\t\tc.Conns = ConnInfos{}\n\t\treturn c, nil\n\t}\n\n\t// Now whip through and generate ConnInfo entries\n\t// Open Clients\n\ti := 0\n\tfor _, client := range openClients {\n\t\tclient.mu.Lock()\n\t\tci := &conns[i]\n\t\tci.fill(client, client.nc, c.Now, auth)\n\t\t// Fill in subscription data if requested.\n\t\tif len(client.subs) > 0 {\n\t\t\tif subsDet {\n\t\t\t\tci.SubsDetail = newSubsDetailList(client)\n\t\t\t} else if subs {\n\t\t\t\tci.Subs = newSubsList(client)\n\t\t\t}\n\t\t}\n\t\t// Fill in user if auth requested.\n\t\tif auth {\n\t\t\tci.AuthorizedUser = client.getRawAuthUser()\n\t\t\tif name := client.acc.GetName(); name != globalAccountName {\n\t\t\t\tci.Account = name\n\t\t\t}\n\t\t\tci.JWT = client.opts.JWT\n\t\t\tci.IssuerKey = issuerForClient(client)\n\t\t\tci.Tags = client.tags\n\t\t\tci.NameTag = client.acc.getNameTag()\n\t\t}\n\t\tclient.mu.Unlock()\n\t\tpconns[i] = ci\n\t\ti++\n\t}\n\t// Closed Clients\n\tvar needCopy bool\n\tif subs || auth {\n\t\tneedCopy = true\n\t}\n\tfor _, cc := range closedClients {\n\t\t// If we have an account specified we need to filter.\n\t\tif acc != _EMPTY_ && cc.acc != acc {\n\t\t\tcontinue\n\t\t}\n\t\t// Do user filtering second\n\t\tif user != _EMPTY_ && cc.user != user {\n\t\t\tcontinue\n\t\t}\n\t\t// Do mqtt client ID filtering next\n\t\tif mqttCID != _EMPTY_ && cc.MQTTClient != mqttCID {\n\t\t\tcontinue\n\t\t}\n\t\t// Copy if needed for any changes to the ConnInfo\n\t\tif needCopy {\n\t\t\tcx := *cc\n\t\t\tcc = &cx\n\t\t}\n\t\t// Fill in subscription data if requested.\n\t\tif len(cc.subs) > 0 {\n\t\t\tif subsDet {\n\t\t\t\tcc.SubsDetail = cc.subs\n\t\t\t} else if subs {\n\t\t\t\tcc.Subs = make([]string, 0, len(cc.subs))\n\t\t\t\tfor _, sub := range cc.subs {\n\t\t\t\t\tcc.Subs = append(cc.Subs, sub.Subject)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t// Fill in user if auth requested.\n\t\tif auth {\n\t\t\tcc.AuthorizedUser = cc.user\n\t\t\tif cc.acc != _EMPTY_ && (cc.acc != globalAccountName) {\n\t\t\t\tcc.Account = cc.acc\n\t\t\t\tif acc, err := s.LookupAccount(cc.acc); err == nil {\n\t\t\t\t\tcc.NameTag = acc.getNameTag()\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tpconns[i] = &cc.ConnInfo\n\t\ti++\n\t}\n\n\t// This will trip if we have filtered out client connections.\n\tif len(pconns) != i {\n\t\tpconns = pconns[:i]\n\t\ttotalClients = i\n\t}\n\n\tswitch sortOpt {\n\tcase ByCid, ByStart:\n\t\tsort.Sort(byCid{pconns})\n\tcase BySubs:\n\t\tsort.Sort(sort.Reverse(bySubs{pconns}))\n\tcase ByPending:\n\t\tsort.Sort(sort.Reverse(byPending{pconns}))\n\tcase ByOutMsgs:\n\t\tsort.Sort(sort.Reverse(byOutMsgs{pconns}))\n\tcase ByInMsgs:\n\t\tsort.Sort(sort.Reverse(byInMsgs{pconns}))\n\tcase ByOutBytes:\n\t\tsort.Sort(sort.Reverse(byOutBytes{pconns}))\n\tcase ByInBytes:\n\t\tsort.Sort(sort.Reverse(byInBytes{pconns}))\n\tcase ByLast:\n\t\tsort.Sort(sort.Reverse(byLast{pconns}))\n\tcase ByIdle:\n\t\tsort.Sort(sort.Reverse(byIdle{pconns, c.Now}))\n\tcase ByUptime:\n\t\tsort.Sort(byUptime{pconns, time.Now()})\n\tcase ByStop:\n\t\tsort.Sort(sort.Reverse(byStop{pconns}))\n\tcase ByReason:\n\t\tsort.Sort(byReason{pconns})\n\tcase ByRTT:\n\t\tsort.Sort(sort.Reverse(byRTT{pconns}))\n\t}\n\n\tminoff := c.Offset\n\tmaxoff := c.Offset + c.Limit\n\n\tmaxIndex := totalClients\n\n\t// Make sure these are sane.\n\tif minoff > maxIndex {\n\t\tminoff = maxIndex\n\t}\n\tif maxoff > maxIndex {\n\t\tmaxoff = maxIndex\n\t}\n\n\t// Now pare down to the requested size.\n\t// TODO(dlc) - for very large number of connections we\n\t// could save the whole list in a hash, send hash on first\n\t// request and allow users to use has for subsequent pages.\n\t// Low TTL, say < 1sec.\n\tc.Conns = pconns[minoff:maxoff]\n\tc.NumConns = len(c.Conns)\n\n\treturn c, nil\n}\n\n// Fills in the ConnInfo from the client.\n// client should be locked.\nfunc (ci *ConnInfo) fill(client *client, nc net.Conn, now time.Time, auth bool) {\n\t// For fast sort if required.\n\trtt := client.getRTT()\n\tci.rtt = int64(rtt)\n\n\tci.Cid = client.cid\n\tci.MQTTClient = client.getMQTTClientID()\n\tci.Kind = client.kindString()\n\tci.Type = client.clientTypeString()\n\tci.Start = client.start\n\tci.LastActivity = client.last\n\tci.Uptime = myUptime(now.Sub(client.start))\n\tci.Idle = myUptime(now.Sub(client.last))\n\tci.RTT = rtt.String()\n\tci.OutMsgs = client.outMsgs\n\tci.OutBytes = client.outBytes\n\tci.NumSubs = uint32(len(client.subs))\n\tci.Pending = int(client.out.pb)\n\tci.Name = client.opts.Name\n\tci.Lang = client.opts.Lang\n\tci.Version = client.opts.Version\n\t// inMsgs and inBytes are updated outside of the client's lock, so\n\t// we need to use atomic here.\n\tci.InMsgs = atomic.LoadInt64(&client.inMsgs)\n\tci.InBytes = atomic.LoadInt64(&client.inBytes)\n\n\t// If the connection is gone, too bad, we won't set TLSVersion and TLSCipher.\n\t// Exclude clients that are still doing handshake so we don't block in\n\t// ConnectionState().\n\tif client.flags.isSet(handshakeComplete) && nc != nil {\n\t\tif conn, ok := nc.(*tls.Conn); ok {\n\t\t\tcs := conn.ConnectionState()\n\t\t\tci.TLSVersion = tlsVersion(cs.Version)\n\t\t\tci.TLSCipher = tlsCipher(cs.CipherSuite)\n\t\t\tif auth && len(cs.PeerCertificates) > 0 {\n\t\t\t\tci.TLSPeerCerts = makePeerCerts(cs.PeerCertificates)\n\t\t\t}\n\t\t\tci.TLSFirst = client.flags.isSet(didTLSFirst)\n\t\t}\n\t}\n\n\tif client.port != 0 {\n\t\tci.Port = int(client.port)\n\t\tci.IP = client.host\n\t}\n}\n\nfunc makePeerCerts(pc []*x509.Certificate) []*TLSPeerCert {\n\tres := make([]*TLSPeerCert, len(pc))\n\tfor i, c := range pc {\n\t\ttmp := sha256.Sum256(c.RawSubjectPublicKeyInfo)\n\t\tssha := hex.EncodeToString(tmp[:])\n\t\ttmp = sha256.Sum256(c.Raw)\n\t\tcsha := hex.EncodeToString(tmp[:])\n\t\tres[i] = &TLSPeerCert{Subject: c.Subject.String(), SubjectPKISha256: ssha, CertSha256: csha}\n\t}\n\treturn res\n}\n\n// Assume lock is held\nfunc (c *client) getRTT() time.Duration {\n\tif c.rtt == 0 {\n\t\t// If a real client, go ahead and send ping now to get a value\n\t\t// for RTT. For tests and telnet, or if client is closing, etc skip.\n\t\tif c.opts.Lang != _EMPTY_ {\n\t\t\tc.sendRTTPingLocked()\n\t\t}\n\t\treturn 0\n\t}\n\tvar rtt time.Duration\n\tif c.rtt > time.Microsecond && c.rtt < time.Millisecond {\n\t\trtt = c.rtt.Truncate(time.Microsecond)\n\t} else {\n\t\trtt = c.rtt.Truncate(time.Nanosecond)\n\t}\n\treturn rtt\n}\n\nfunc decodeBool(w http.ResponseWriter, r *http.Request, param string) (bool, error) {\n\tstr := r.URL.Query().Get(param)\n\tif str == _EMPTY_ {\n\t\treturn false, nil\n\t}\n\tval, err := strconv.ParseBool(str)\n\tif err != nil {\n\t\tw.WriteHeader(http.StatusBadRequest)\n\t\tw.Write([]byte(fmt.Sprintf(\"Error decoding boolean for '%s': %v\", param, err)))\n\t\treturn false, err\n\t}\n\treturn val, nil\n}\n\nfunc decodeUint64(w http.ResponseWriter, r *http.Request, param string) (uint64, error) {\n\tstr := r.URL.Query().Get(param)\n\tif str == _EMPTY_ {\n\t\treturn 0, nil\n\t}\n\tval, err := strconv.ParseUint(str, 10, 64)\n\tif err != nil {\n\t\tw.WriteHeader(http.StatusBadRequest)\n\t\tw.Write([]byte(fmt.Sprintf(\"Error decoding uint64 for '%s': %v\", param, err)))\n\t\treturn 0, err\n\t}\n\treturn val, nil\n}\n\nfunc decodeInt(w http.ResponseWriter, r *http.Request, param string) (int, error) {\n\tstr := r.URL.Query().Get(param)\n\tif str == _EMPTY_ {\n\t\treturn 0, nil\n\t}\n\tval, err := strconv.Atoi(str)\n\tif err != nil {\n\t\tw.WriteHeader(http.StatusBadRequest)\n\t\tw.Write([]byte(fmt.Sprintf(\"Error decoding int for '%s': %v\", param, err)))\n\t\treturn 0, err\n\t}\n\treturn val, nil\n}\n\nfunc decodeState(w http.ResponseWriter, r *http.Request) (ConnState, error) {\n\tstr := r.URL.Query().Get(\"state\")\n\tif str == _EMPTY_ {\n\t\treturn ConnOpen, nil\n\t}\n\tswitch strings.ToLower(str) {\n\tcase \"open\":\n\t\treturn ConnOpen, nil\n\tcase \"closed\":\n\t\treturn ConnClosed, nil\n\tcase \"any\", \"all\":\n\t\treturn ConnAll, nil\n\t}\n\t// We do not understand intended state here.\n\tw.WriteHeader(http.StatusBadRequest)\n\terr := fmt.Errorf(\"Error decoding state for %s\", str)\n\tw.Write([]byte(err.Error()))\n\treturn 0, err\n}\n\nfunc decodeSubs(w http.ResponseWriter, r *http.Request) (subs bool, subsDet bool, err error) {\n\tsubsDet = strings.ToLower(r.URL.Query().Get(\"subs\")) == \"detail\"\n\tif !subsDet {\n\t\tsubs, err = decodeBool(w, r, \"subs\")\n\t}\n\treturn\n}\n\n// HandleConnz process HTTP requests for connection information.\nfunc (s *Server) HandleConnz(w http.ResponseWriter, r *http.Request) {\n\tsortOpt := SortOpt(r.URL.Query().Get(\"sort\"))\n\tauth, err := decodeBool(w, r, \"auth\")\n\tif err != nil {\n\t\treturn\n\t}\n\tsubs, subsDet, err := decodeSubs(w, r)\n\tif err != nil {\n\t\treturn\n\t}\n\toffset, err := decodeInt(w, r, \"offset\")\n\tif err != nil {\n\t\treturn\n\t}\n\tlimit, err := decodeInt(w, r, \"limit\")\n\tif err != nil {\n\t\treturn\n\t}\n\tcid, err := decodeUint64(w, r, \"cid\")\n\tif err != nil {\n\t\treturn\n\t}\n\tstate, err := decodeState(w, r)\n\tif err != nil {\n\t\treturn\n\t}\n\n\tuser := r.URL.Query().Get(\"user\")\n\tacc := r.URL.Query().Get(\"acc\")\n\tmqttCID := r.URL.Query().Get(\"mqtt_client\")\n\n\tconnzOpts := &ConnzOptions{\n\t\tSort:                sortOpt,\n\t\tUsername:            auth,\n\t\tSubscriptions:       subs,\n\t\tSubscriptionsDetail: subsDet,\n\t\tOffset:              offset,\n\t\tLimit:               limit,\n\t\tCID:                 cid,\n\t\tMQTTClient:          mqttCID,\n\t\tState:               state,\n\t\tUser:                user,\n\t\tAccount:             acc,\n\t}\n\n\ts.mu.Lock()\n\ts.httpReqStats[ConnzPath]++\n\ts.mu.Unlock()\n\n\tc, err := s.Connz(connzOpts)\n\tif err != nil {\n\t\tw.WriteHeader(http.StatusBadRequest)\n\t\tw.Write([]byte(err.Error()))\n\t\treturn\n\t}\n\tb, err := json.MarshalIndent(c, \"\", \"  \")\n\tif err != nil {\n\t\ts.Errorf(\"Error marshaling response to /connz request: %v\", err)\n\t}\n\n\t// Handle response\n\tResponseHandler(w, r, b)\n}\n\n// Routez represents detailed information on current client connections.\ntype Routez struct {\n\tID        string             `json:\"server_id\"`\n\tName      string             `json:\"server_name\"`\n\tNow       time.Time          `json:\"now\"`\n\tImport    *SubjectPermission `json:\"import,omitempty\"`\n\tExport    *SubjectPermission `json:\"export,omitempty\"`\n\tNumRoutes int                `json:\"num_routes\"`\n\tRoutes    []*RouteInfo       `json:\"routes\"`\n}\n\n// RoutezOptions are options passed to Routez\ntype RoutezOptions struct {\n\t// Subscriptions indicates that Routez will return a route's subscriptions\n\tSubscriptions bool `json:\"subscriptions\"`\n\t// SubscriptionsDetail indicates if subscription details should be included in the results\n\tSubscriptionsDetail bool `json:\"subscriptions_detail\"`\n}\n\n// RouteInfo has detailed information on a per connection basis.\ntype RouteInfo struct {\n\tRid          uint64             `json:\"rid\"`\n\tRemoteID     string             `json:\"remote_id\"`\n\tRemoteName   string             `json:\"remote_name\"`\n\tDidSolicit   bool               `json:\"did_solicit\"`\n\tIsConfigured bool               `json:\"is_configured\"`\n\tIP           string             `json:\"ip\"`\n\tPort         int                `json:\"port\"`\n\tStart        time.Time          `json:\"start\"`\n\tLastActivity time.Time          `json:\"last_activity\"`\n\tRTT          string             `json:\"rtt,omitempty\"`\n\tUptime       string             `json:\"uptime\"`\n\tIdle         string             `json:\"idle\"`\n\tImport       *SubjectPermission `json:\"import,omitempty\"`\n\tExport       *SubjectPermission `json:\"export,omitempty\"`\n\tPending      int                `json:\"pending_size\"`\n\tInMsgs       int64              `json:\"in_msgs\"`\n\tOutMsgs      int64              `json:\"out_msgs\"`\n\tInBytes      int64              `json:\"in_bytes\"`\n\tOutBytes     int64              `json:\"out_bytes\"`\n\tNumSubs      uint32             `json:\"subscriptions\"`\n\tSubs         []string           `json:\"subscriptions_list,omitempty\"`\n\tSubsDetail   []SubDetail        `json:\"subscriptions_list_detail,omitempty\"`\n\tAccount      string             `json:\"account,omitempty\"`\n\tCompression  string             `json:\"compression,omitempty\"`\n}\n\n// Routez returns a Routez struct containing information about routes.\nfunc (s *Server) Routez(routezOpts *RoutezOptions) (*Routez, error) {\n\trs := &Routez{Routes: []*RouteInfo{}}\n\trs.Now = time.Now().UTC()\n\n\tif routezOpts == nil {\n\t\troutezOpts = &RoutezOptions{}\n\t}\n\n\ts.mu.Lock()\n\trs.NumRoutes = s.numRoutes()\n\n\t// copy the server id for monitoring\n\trs.ID = s.info.ID\n\n\t// Check for defined permissions for all connected routes.\n\tif perms := s.getOpts().Cluster.Permissions; perms != nil {\n\t\trs.Import = perms.Import\n\t\trs.Export = perms.Export\n\t}\n\trs.Name = s.getOpts().ServerName\n\n\taddRoute := func(r *client) {\n\t\tr.mu.Lock()\n\t\tri := &RouteInfo{\n\t\t\tRid:          r.cid,\n\t\t\tRemoteID:     r.route.remoteID,\n\t\t\tRemoteName:   r.route.remoteName,\n\t\t\tDidSolicit:   r.route.didSolicit,\n\t\t\tIsConfigured: r.route.routeType == Explicit,\n\t\t\tInMsgs:       atomic.LoadInt64(&r.inMsgs),\n\t\t\tOutMsgs:      r.outMsgs,\n\t\t\tInBytes:      atomic.LoadInt64(&r.inBytes),\n\t\t\tOutBytes:     r.outBytes,\n\t\t\tNumSubs:      uint32(len(r.subs)),\n\t\t\tImport:       r.opts.Import,\n\t\t\tPending:      int(r.out.pb),\n\t\t\tExport:       r.opts.Export,\n\t\t\tRTT:          r.getRTT().String(),\n\t\t\tStart:        r.start,\n\t\t\tLastActivity: r.last,\n\t\t\tUptime:       myUptime(rs.Now.Sub(r.start)),\n\t\t\tIdle:         myUptime(rs.Now.Sub(r.last)),\n\t\t\tAccount:      string(r.route.accName),\n\t\t\tCompression:  r.route.compression,\n\t\t}\n\n\t\tif len(r.subs) > 0 {\n\t\t\tif routezOpts.SubscriptionsDetail {\n\t\t\t\tri.SubsDetail = newSubsDetailList(r)\n\t\t\t} else if routezOpts.Subscriptions {\n\t\t\t\tri.Subs = newSubsList(r)\n\t\t\t}\n\t\t}\n\n\t\tswitch conn := r.nc.(type) {\n\t\tcase *net.TCPConn, *tls.Conn:\n\t\t\taddr := conn.RemoteAddr().(*net.TCPAddr)\n\t\t\tri.Port = addr.Port\n\t\t\tri.IP = addr.IP.String()\n\t\t}\n\t\tr.mu.Unlock()\n\t\trs.Routes = append(rs.Routes, ri)\n\t}\n\n\t// Walk the list\n\ts.forEachRoute(func(r *client) {\n\t\taddRoute(r)\n\t})\n\ts.mu.Unlock()\n\treturn rs, nil\n}\n\n// HandleRoutez process HTTP requests for route information.\nfunc (s *Server) HandleRoutez(w http.ResponseWriter, r *http.Request) {\n\tsubs, subsDetail, err := decodeSubs(w, r)\n\tif err != nil {\n\t\treturn\n\t}\n\n\topts := RoutezOptions{Subscriptions: subs, SubscriptionsDetail: subsDetail}\n\n\ts.mu.Lock()\n\ts.httpReqStats[RoutezPath]++\n\ts.mu.Unlock()\n\n\t// As of now, no error is ever returned.\n\trs, _ := s.Routez(&opts)\n\tb, err := json.MarshalIndent(rs, \"\", \"  \")\n\tif err != nil {\n\t\ts.Errorf(\"Error marshaling response to /routez request: %v\", err)\n\t}\n\n\t// Handle response\n\tResponseHandler(w, r, b)\n}\n\n// Subsz represents detail information on current connections.\ntype Subsz struct {\n\tID  string    `json:\"server_id\"`\n\tNow time.Time `json:\"now\"`\n\t*SublistStats\n\tTotal  int         `json:\"total\"`\n\tOffset int         `json:\"offset\"`\n\tLimit  int         `json:\"limit\"`\n\tSubs   []SubDetail `json:\"subscriptions_list,omitempty\"`\n}\n\n// SubszOptions are the options passed to Subsz.\n// As of now, there are no options defined.\ntype SubszOptions struct {\n\t// Offset is used for pagination. Subsz() only returns connections starting at this\n\t// offset from the global results.\n\tOffset int `json:\"offset\"`\n\n\t// Limit is the maximum number of subscriptions that should be returned by Subsz().\n\tLimit int `json:\"limit\"`\n\n\t// Subscriptions indicates if subscription details should be included in the results.\n\tSubscriptions bool `json:\"subscriptions\"`\n\n\t// Filter based on this account name.\n\tAccount string `json:\"account,omitempty\"`\n\n\t// Test the list against this subject. Needs to be literal since it signifies a publish subject.\n\t// We will only return subscriptions that would match if a message was sent to this subject.\n\tTest string `json:\"test,omitempty\"`\n}\n\n// SubDetail is for verbose information for subscriptions.\ntype SubDetail struct {\n\tAccount    string `json:\"account,omitempty\"`\n\tAccountTag string `json:\"account_tag,omitempty\"`\n\tSubject    string `json:\"subject\"`\n\tQueue      string `json:\"qgroup,omitempty\"`\n\tSid        string `json:\"sid\"`\n\tMsgs       int64  `json:\"msgs\"`\n\tMax        int64  `json:\"max,omitempty\"`\n\tCid        uint64 `json:\"cid\"`\n}\n\n// Subscription client should be locked and guaranteed to be present.\nfunc newSubDetail(sub *subscription) SubDetail {\n\tsd := newClientSubDetail(sub)\n\tsd.Account = sub.client.acc.GetName()\n\tsd.AccountTag = sub.client.acc.getNameTag()\n\treturn sd\n}\n\n// For subs details under clients.\nfunc newClientSubDetail(sub *subscription) SubDetail {\n\treturn SubDetail{\n\t\tSubject: string(sub.subject),\n\t\tQueue:   string(sub.queue),\n\t\tSid:     string(sub.sid),\n\t\tMsgs:    sub.nm,\n\t\tMax:     sub.max,\n\t\tCid:     sub.client.cid,\n\t}\n}\n\n// Subsz returns a Subsz struct containing subjects statistics\nfunc (s *Server) Subsz(opts *SubszOptions) (*Subsz, error) {\n\tvar (\n\t\tsubdetail bool\n\t\ttest      bool\n\t\toffset    int\n\t\ttestSub   string\n\t\tfilterAcc string\n\t\tlimit     = DefaultSubListSize\n\t)\n\n\tif opts != nil {\n\t\tsubdetail = opts.Subscriptions\n\t\toffset = opts.Offset\n\t\tif offset < 0 {\n\t\t\toffset = 0\n\t\t}\n\t\tlimit = opts.Limit\n\t\tif limit <= 0 {\n\t\t\tlimit = DefaultSubListSize\n\t\t}\n\t\tif opts.Test != _EMPTY_ {\n\t\t\ttestSub = opts.Test\n\t\t\ttest = true\n\t\t\tif !IsValidLiteralSubject(testSub) {\n\t\t\t\treturn nil, fmt.Errorf(\"invalid test subject, must be valid publish subject: %s\", testSub)\n\t\t\t}\n\t\t}\n\t\tif opts.Account != _EMPTY_ {\n\t\t\tfilterAcc = opts.Account\n\t\t}\n\t}\n\n\tslStats := &SublistStats{}\n\n\t// FIXME(dlc) - Make account aware.\n\tsz := &Subsz{s.info.ID, time.Now().UTC(), slStats, 0, offset, limit, nil}\n\n\tif subdetail {\n\t\tvar raw [4096]*subscription\n\t\tsubs := raw[:0]\n\t\ts.accounts.Range(func(k, v any) bool {\n\t\t\tacc := v.(*Account)\n\t\t\tif filterAcc != _EMPTY_ && acc.GetName() != filterAcc {\n\t\t\t\treturn true\n\t\t\t}\n\t\t\tslStats.add(acc.sl.Stats())\n\t\t\tacc.sl.localSubs(&subs, false)\n\t\t\treturn true\n\t\t})\n\n\t\tdetails := make([]SubDetail, len(subs))\n\t\ti := 0\n\t\t// TODO(dlc) - may be inefficient and could just do normal match when total subs is large and filtering.\n\t\tfor _, sub := range subs {\n\t\t\t// Check for filter\n\t\t\tif test && !matchLiteral(testSub, string(sub.subject)) {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif sub.client == nil {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tsub.client.mu.Lock()\n\t\t\tdetails[i] = newSubDetail(sub)\n\t\t\tsub.client.mu.Unlock()\n\t\t\ti++\n\t\t}\n\t\tminoff := sz.Offset\n\t\tmaxoff := sz.Offset + sz.Limit\n\n\t\tmaxIndex := i\n\n\t\t// Make sure these are sane.\n\t\tif minoff > maxIndex {\n\t\t\tminoff = maxIndex\n\t\t}\n\t\tif maxoff > maxIndex {\n\t\t\tmaxoff = maxIndex\n\t\t}\n\t\tsz.Subs = details[minoff:maxoff]\n\t\tsz.Total = len(sz.Subs)\n\t} else {\n\t\ts.accounts.Range(func(k, v any) bool {\n\t\t\tacc := v.(*Account)\n\t\t\tif filterAcc != _EMPTY_ && acc.GetName() != filterAcc {\n\t\t\t\treturn true\n\t\t\t}\n\t\t\tslStats.add(acc.sl.Stats())\n\t\t\treturn true\n\t\t})\n\t}\n\n\treturn sz, nil\n}\n\n// HandleSubsz processes HTTP requests for subjects stats.\nfunc (s *Server) HandleSubsz(w http.ResponseWriter, r *http.Request) {\n\ts.mu.Lock()\n\ts.httpReqStats[SubszPath]++\n\ts.mu.Unlock()\n\n\tsubs, err := decodeBool(w, r, \"subs\")\n\tif err != nil {\n\t\treturn\n\t}\n\toffset, err := decodeInt(w, r, \"offset\")\n\tif err != nil {\n\t\treturn\n\t}\n\tlimit, err := decodeInt(w, r, \"limit\")\n\tif err != nil {\n\t\treturn\n\t}\n\ttestSub := r.URL.Query().Get(\"test\")\n\t// Filtered account.\n\tfilterAcc := r.URL.Query().Get(\"acc\")\n\n\tsubszOpts := &SubszOptions{\n\t\tSubscriptions: subs,\n\t\tOffset:        offset,\n\t\tLimit:         limit,\n\t\tAccount:       filterAcc,\n\t\tTest:          testSub,\n\t}\n\n\tst, err := s.Subsz(subszOpts)\n\tif err != nil {\n\t\tw.WriteHeader(http.StatusBadRequest)\n\t\tw.Write([]byte(err.Error()))\n\t\treturn\n\t}\n\n\tvar b []byte\n\n\tif len(st.Subs) == 0 {\n\t\tb, err = json.MarshalIndent(st.SublistStats, \"\", \"  \")\n\t} else {\n\t\tb, err = json.MarshalIndent(st, \"\", \"  \")\n\t}\n\tif err != nil {\n\t\ts.Errorf(\"Error marshaling response to /subscriptionsz request: %v\", err)\n\t}\n\n\t// Handle response\n\tResponseHandler(w, r, b)\n}\n\n// HandleStacksz processes HTTP requests for getting stacks\nfunc (s *Server) HandleStacksz(w http.ResponseWriter, r *http.Request) {\n\t// Do not get any lock here that would prevent getting the stacks\n\t// if we were to have a deadlock somewhere.\n\tvar defaultBuf [defaultStackBufSize]byte\n\tsize := defaultStackBufSize\n\tbuf := defaultBuf[:size]\n\tn := 0\n\tfor {\n\t\tn = runtime.Stack(buf, true)\n\t\tif n < size {\n\t\t\tbreak\n\t\t}\n\t\tsize *= 2\n\t\tbuf = make([]byte, size)\n\t}\n\t// Handle response\n\tResponseHandler(w, r, buf[:n])\n}\n\ntype IpqueueszStatusIPQ struct {\n\tPending    int `json:\"pending\"`\n\tInProgress int `json:\"in_progress,omitempty\"`\n}\n\ntype IpqueueszStatus map[string]IpqueueszStatusIPQ\n\nfunc (s *Server) Ipqueuesz(opts *IpqueueszOptions) *IpqueueszStatus {\n\tall, qfilter := opts.All, opts.Filter\n\tqueues := IpqueueszStatus{}\n\ts.ipQueues.Range(func(k, v any) bool {\n\t\tvar pending, inProgress int\n\t\tname := k.(string)\n\t\tqueue, ok := v.(interface {\n\t\t\tlen() int\n\t\t\tinProgress() int64\n\t\t})\n\t\tif ok {\n\t\t\tpending = queue.len()\n\t\t\tinProgress = int(queue.inProgress())\n\t\t}\n\t\tif !all && (pending == 0 && inProgress == 0) {\n\t\t\treturn true\n\t\t} else if qfilter != _EMPTY_ && !strings.Contains(name, qfilter) {\n\t\t\treturn true\n\t\t}\n\t\tqueues[name] = IpqueueszStatusIPQ{Pending: pending, InProgress: inProgress}\n\t\treturn true\n\t})\n\treturn &queues\n}\n\nfunc (s *Server) HandleIPQueuesz(w http.ResponseWriter, r *http.Request) {\n\tall, err := decodeBool(w, r, \"all\")\n\tif err != nil {\n\t\treturn\n\t}\n\tqfilter := r.URL.Query().Get(\"queues\")\n\n\tqueues := s.Ipqueuesz(&IpqueueszOptions{\n\t\tAll:    all,\n\t\tFilter: qfilter,\n\t})\n\n\tb, _ := json.MarshalIndent(queues, \"\", \"   \")\n\tResponseHandler(w, r, b)\n}\n\n// Varz will output server information on the monitoring port at /varz.\ntype Varz struct {\n\tID                    string                 `json:\"server_id\"`\n\tName                  string                 `json:\"server_name\"`\n\tVersion               string                 `json:\"version\"`\n\tProto                 int                    `json:\"proto\"`\n\tGitCommit             string                 `json:\"git_commit,omitempty\"`\n\tGoVersion             string                 `json:\"go\"`\n\tHost                  string                 `json:\"host\"`\n\tPort                  int                    `json:\"port\"`\n\tAuthRequired          bool                   `json:\"auth_required,omitempty\"`\n\tTLSRequired           bool                   `json:\"tls_required,omitempty\"`\n\tTLSVerify             bool                   `json:\"tls_verify,omitempty\"`\n\tTLSOCSPPeerVerify     bool                   `json:\"tls_ocsp_peer_verify,omitempty\"`\n\tIP                    string                 `json:\"ip,omitempty\"`\n\tClientConnectURLs     []string               `json:\"connect_urls,omitempty\"`\n\tWSConnectURLs         []string               `json:\"ws_connect_urls,omitempty\"`\n\tMaxConn               int                    `json:\"max_connections\"`\n\tMaxSubs               int                    `json:\"max_subscriptions,omitempty\"`\n\tPingInterval          time.Duration          `json:\"ping_interval\"`\n\tMaxPingsOut           int                    `json:\"ping_max\"`\n\tHTTPHost              string                 `json:\"http_host\"`\n\tHTTPPort              int                    `json:\"http_port\"`\n\tHTTPBasePath          string                 `json:\"http_base_path\"`\n\tHTTPSPort             int                    `json:\"https_port\"`\n\tAuthTimeout           float64                `json:\"auth_timeout\"`\n\tMaxControlLine        int32                  `json:\"max_control_line\"`\n\tMaxPayload            int                    `json:\"max_payload\"`\n\tMaxPending            int64                  `json:\"max_pending\"`\n\tCluster               ClusterOptsVarz        `json:\"cluster,omitempty\"`\n\tGateway               GatewayOptsVarz        `json:\"gateway,omitempty\"`\n\tLeafNode              LeafNodeOptsVarz       `json:\"leaf,omitempty\"`\n\tMQTT                  MQTTOptsVarz           `json:\"mqtt,omitempty\"`\n\tWebsocket             WebsocketOptsVarz      `json:\"websocket,omitempty\"`\n\tJetStream             JetStreamVarz          `json:\"jetstream,omitempty\"`\n\tTLSTimeout            float64                `json:\"tls_timeout\"`\n\tWriteDeadline         time.Duration          `json:\"write_deadline\"`\n\tStart                 time.Time              `json:\"start\"`\n\tNow                   time.Time              `json:\"now\"`\n\tUptime                string                 `json:\"uptime\"`\n\tMem                   int64                  `json:\"mem\"`\n\tCores                 int                    `json:\"cores\"`\n\tMaxProcs              int                    `json:\"gomaxprocs\"`\n\tMemLimit              int64                  `json:\"gomemlimit,omitempty\"`\n\tCPU                   float64                `json:\"cpu\"`\n\tConnections           int                    `json:\"connections\"`\n\tTotalConnections      uint64                 `json:\"total_connections\"`\n\tRoutes                int                    `json:\"routes\"`\n\tRemotes               int                    `json:\"remotes\"`\n\tLeafs                 int                    `json:\"leafnodes\"`\n\tInMsgs                int64                  `json:\"in_msgs\"`\n\tOutMsgs               int64                  `json:\"out_msgs\"`\n\tInBytes               int64                  `json:\"in_bytes\"`\n\tOutBytes              int64                  `json:\"out_bytes\"`\n\tSlowConsumers         int64                  `json:\"slow_consumers\"`\n\tSubscriptions         uint32                 `json:\"subscriptions\"`\n\tHTTPReqStats          map[string]uint64      `json:\"http_req_stats\"`\n\tConfigLoadTime        time.Time              `json:\"config_load_time\"`\n\tConfigDigest          string                 `json:\"config_digest\"`\n\tTags                  jwt.TagList            `json:\"tags,omitempty\"`\n\tTrustedOperatorsJwt   []string               `json:\"trusted_operators_jwt,omitempty\"`\n\tTrustedOperatorsClaim []*jwt.OperatorClaims  `json:\"trusted_operators_claim,omitempty\"`\n\tSystemAccount         string                 `json:\"system_account,omitempty\"`\n\tPinnedAccountFail     uint64                 `json:\"pinned_account_fails,omitempty\"`\n\tOCSPResponseCache     *OCSPResponseCacheVarz `json:\"ocsp_peer_cache,omitempty\"`\n\tSlowConsumersStats    *SlowConsumersStats    `json:\"slow_consumer_stats\"`\n}\n\n// JetStreamVarz contains basic runtime information about jetstream\ntype JetStreamVarz struct {\n\tConfig *JetStreamConfig `json:\"config,omitempty\"`\n\tStats  *JetStreamStats  `json:\"stats,omitempty\"`\n\tMeta   *MetaClusterInfo `json:\"meta,omitempty\"`\n\tLimits *JSLimitOpts     `json:\"limits,omitempty\"`\n}\n\n// ClusterOptsVarz contains monitoring cluster information\ntype ClusterOptsVarz struct {\n\tName        string   `json:\"name,omitempty\"`\n\tHost        string   `json:\"addr,omitempty\"`\n\tPort        int      `json:\"cluster_port,omitempty\"`\n\tAuthTimeout float64  `json:\"auth_timeout,omitempty\"`\n\tURLs        []string `json:\"urls,omitempty\"`\n\tTLSTimeout  float64  `json:\"tls_timeout,omitempty\"`\n\tTLSRequired bool     `json:\"tls_required,omitempty\"`\n\tTLSVerify   bool     `json:\"tls_verify,omitempty\"`\n\tPoolSize    int      `json:\"pool_size,omitempty\"`\n}\n\n// GatewayOptsVarz contains monitoring gateway information\ntype GatewayOptsVarz struct {\n\tName           string                  `json:\"name,omitempty\"`\n\tHost           string                  `json:\"host,omitempty\"`\n\tPort           int                     `json:\"port,omitempty\"`\n\tAuthTimeout    float64                 `json:\"auth_timeout,omitempty\"`\n\tTLSTimeout     float64                 `json:\"tls_timeout,omitempty\"`\n\tTLSRequired    bool                    `json:\"tls_required,omitempty\"`\n\tTLSVerify      bool                    `json:\"tls_verify,omitempty\"`\n\tAdvertise      string                  `json:\"advertise,omitempty\"`\n\tConnectRetries int                     `json:\"connect_retries,omitempty\"`\n\tGateways       []RemoteGatewayOptsVarz `json:\"gateways,omitempty\"`\n\tRejectUnknown  bool                    `json:\"reject_unknown,omitempty\"` // config got renamed to reject_unknown_cluster\n}\n\n// RemoteGatewayOptsVarz contains monitoring remote gateway information\ntype RemoteGatewayOptsVarz struct {\n\tName       string   `json:\"name\"`\n\tTLSTimeout float64  `json:\"tls_timeout,omitempty\"`\n\tURLs       []string `json:\"urls,omitempty\"`\n}\n\n// LeafNodeOptsVarz contains monitoring leaf node information\ntype LeafNodeOptsVarz struct {\n\tHost              string               `json:\"host,omitempty\"`\n\tPort              int                  `json:\"port,omitempty\"`\n\tAuthTimeout       float64              `json:\"auth_timeout,omitempty\"`\n\tTLSTimeout        float64              `json:\"tls_timeout,omitempty\"`\n\tTLSRequired       bool                 `json:\"tls_required,omitempty\"`\n\tTLSVerify         bool                 `json:\"tls_verify,omitempty\"`\n\tRemotes           []RemoteLeafOptsVarz `json:\"remotes,omitempty\"`\n\tTLSOCSPPeerVerify bool                 `json:\"tls_ocsp_peer_verify,omitempty\"`\n}\n\n// DenyRules Contains lists of subjects not allowed to be imported/exported\ntype DenyRules struct {\n\tExports []string `json:\"exports,omitempty\"`\n\tImports []string `json:\"imports,omitempty\"`\n}\n\n// RemoteLeafOptsVarz contains monitoring remote leaf node information\ntype RemoteLeafOptsVarz struct {\n\tLocalAccount      string     `json:\"local_account,omitempty\"`\n\tTLSTimeout        float64    `json:\"tls_timeout,omitempty\"`\n\tURLs              []string   `json:\"urls,omitempty\"`\n\tDeny              *DenyRules `json:\"deny,omitempty\"`\n\tTLSOCSPPeerVerify bool       `json:\"tls_ocsp_peer_verify,omitempty\"`\n}\n\n// MQTTOptsVarz contains monitoring MQTT information\ntype MQTTOptsVarz struct {\n\tHost              string        `json:\"host,omitempty\"`\n\tPort              int           `json:\"port,omitempty\"`\n\tNoAuthUser        string        `json:\"no_auth_user,omitempty\"`\n\tAuthTimeout       float64       `json:\"auth_timeout,omitempty\"`\n\tTLSMap            bool          `json:\"tls_map,omitempty\"`\n\tTLSTimeout        float64       `json:\"tls_timeout,omitempty\"`\n\tTLSPinnedCerts    []string      `json:\"tls_pinned_certs,omitempty\"`\n\tJsDomain          string        `json:\"js_domain,omitempty\"`\n\tAckWait           time.Duration `json:\"ack_wait,omitempty\"`\n\tMaxAckPending     uint16        `json:\"max_ack_pending,omitempty\"`\n\tTLSOCSPPeerVerify bool          `json:\"tls_ocsp_peer_verify,omitempty\"`\n}\n\n// WebsocketOptsVarz contains monitoring websocket information\ntype WebsocketOptsVarz struct {\n\tHost              string        `json:\"host,omitempty\"`\n\tPort              int           `json:\"port,omitempty\"`\n\tAdvertise         string        `json:\"advertise,omitempty\"`\n\tNoAuthUser        string        `json:\"no_auth_user,omitempty\"`\n\tJWTCookie         string        `json:\"jwt_cookie,omitempty\"`\n\tHandshakeTimeout  time.Duration `json:\"handshake_timeout,omitempty\"`\n\tAuthTimeout       float64       `json:\"auth_timeout,omitempty\"`\n\tNoTLS             bool          `json:\"no_tls,omitempty\"`\n\tTLSMap            bool          `json:\"tls_map,omitempty\"`\n\tTLSPinnedCerts    []string      `json:\"tls_pinned_certs,omitempty\"`\n\tSameOrigin        bool          `json:\"same_origin,omitempty\"`\n\tAllowedOrigins    []string      `json:\"allowed_origins,omitempty\"`\n\tCompression       bool          `json:\"compression,omitempty\"`\n\tTLSOCSPPeerVerify bool          `json:\"tls_ocsp_peer_verify,omitempty\"`\n}\n\n// OCSPResponseCacheVarz contains OCSP response cache information\ntype OCSPResponseCacheVarz struct {\n\tType      string `json:\"cache_type,omitempty\"`\n\tHits      int64  `json:\"cache_hits,omitempty\"`\n\tMisses    int64  `json:\"cache_misses,omitempty\"`\n\tResponses int64  `json:\"cached_responses,omitempty\"`\n\tRevokes   int64  `json:\"cached_revoked_responses,omitempty\"`\n\tGoods     int64  `json:\"cached_good_responses,omitempty\"`\n\tUnknowns  int64  `json:\"cached_unknown_responses,omitempty\"`\n}\n\n// VarzOptions are the options passed to Varz().\n// Currently, there are no options defined.\ntype VarzOptions struct{}\n\n// SlowConsumersStats contains information about the slow consumers from different type of connections.\ntype SlowConsumersStats struct {\n\tClients  uint64 `json:\"clients\"`\n\tRoutes   uint64 `json:\"routes\"`\n\tGateways uint64 `json:\"gateways\"`\n\tLeafs    uint64 `json:\"leafs\"`\n}\n\nfunc myUptime(d time.Duration) string {\n\t// Just use total seconds for uptime, and display days / years\n\ttsecs := d / time.Second\n\ttmins := tsecs / 60\n\tthrs := tmins / 60\n\ttdays := thrs / 24\n\ttyrs := tdays / 365\n\n\tif tyrs > 0 {\n\t\treturn fmt.Sprintf(\"%dy%dd%dh%dm%ds\", tyrs, tdays%365, thrs%24, tmins%60, tsecs%60)\n\t}\n\tif tdays > 0 {\n\t\treturn fmt.Sprintf(\"%dd%dh%dm%ds\", tdays, thrs%24, tmins%60, tsecs%60)\n\t}\n\tif thrs > 0 {\n\t\treturn fmt.Sprintf(\"%dh%dm%ds\", thrs, tmins%60, tsecs%60)\n\t}\n\tif tmins > 0 {\n\t\treturn fmt.Sprintf(\"%dm%ds\", tmins, tsecs%60)\n\t}\n\treturn fmt.Sprintf(\"%ds\", tsecs)\n}\n\n// HandleRoot will show basic info and links to others handlers.\nfunc (s *Server) HandleRoot(w http.ResponseWriter, r *http.Request) {\n\t// This feels dumb to me, but is required: https://code.google.com/p/go/issues/detail?id=4799\n\tif r.URL.Path != s.httpBasePath {\n\t\thttp.NotFound(w, r)\n\t\treturn\n\t}\n\ts.mu.Lock()\n\ts.httpReqStats[RootPath]++\n\ts.mu.Unlock()\n\n\t// Calculate source url. If git set go directly to that tag, otherwise just main.\n\tvar srcUrl string\n\tif gitCommit == _EMPTY_ {\n\t\tsrcUrl = \"https://github.com/nats-io/nats-server\"\n\t} else if serverVersion != _EMPTY_ {\n\t\tsrcUrl = fmt.Sprintf(\"https://github.com/nats-io/nats-server/tree/%s\", serverVersion)\n\t} else {\n\t\tsrcUrl = fmt.Sprintf(\"https://github.com/nats-io/nats-server/tree/%s\", gitCommit)\n\t}\n\n\tfmt.Fprintf(w, `<html lang=\"en\">\n\t<head>\n\t<link rel=\"shortcut icon\" href=\"https://nats.io/favicon.ico\">\n\t<style type=\"text/css\">\n\t\tbody { font-family: ui-sans-serif,system-ui,-apple-system,BlinkMacSystemFont,\"Segoe UI\",Roboto,\"Helvetica Neue\",Arial,\"Noto Sans\",sans-serif; font-size: 18; font-weight: light-bold; margin-left: 32px }\n\t\ta { display:block; margin-left: 7px; padding-bottom: 6px; color: rgb(72 72 92); text-decoration: none }\n\t\ta:hover { font-weight: 600; color: rgb(59 50 202) }\n\t\ta.help { display:inline; font-weight: 600; color: rgb(59 50 202); font-size: 20}\n\t\ta.last { padding-bottom: 16px }\n\t\ta.version { font-size: 14; font-weight: 400; width: 312px; text-align: right; margin-top: -2rem }\n\t\ta.version:hover { color: rgb(22 22 32) }\n\n\t</style>\n\t</head>\n\t<body>\n   <svg xmlns=\"http://www.w3.org/2000/svg\" role=\"img\" width=\"325\" height=\"110\" viewBox=\"-4.14 -3.89 436.28 119.03\"><style>.st1{fill:#fff}.st2{fill:#34a574}</style><path fill=\"#27aae1\" d=\"M4.3 84.6h42.2L70.7 107V84.6H103v-80H4.3v80zm15.9-61.3h18.5l35.6 33.2V23.3h11.8v42.9H68.2L32 32.4v33.8H20.2V23.3z\"/><path d=\"M32 32.4l36.2 33.8h17.9V23.3H74.3v33.2L38.7 23.3H20.2v42.9H32z\" class=\"st1\"/><path d=\"M159.8 30.7L147 49h25.6z\" class=\"st2\"/><path d=\"M111.3 84.6H210v-80h-98.7v80zm41-61.5H168l30.8 43.2h-14.1l-5.8-8.3h-38.1l-5.8 8.3h-13.5l30.8-43.2z\" class=\"st2\"/><path d=\"M140.8 57.9h38.1l5.8 8.3h14.1L168 23.1h-15.7l-30.8 43.2H135l5.8-8.4zm19-27.2L172.6 49H147l12.8-18.3z\" class=\"st1\"/><path fill=\"#375c93\" d=\"M218.3 84.6H317v-80h-98.7v80zm15.5-61.3h66.7V33h-27.2v33.2h-12.2V33h-27.3v-9.7z\"/><path d=\"M261.1 66.2h12.2V33h27.2v-9.7h-66.7V33h27.3z\" class=\"st1\"/><path fill=\"#8dc63f\" d=\"M325.3 4.6v80H424v-80h-98.7zm76.5 56.7c-3.2 3.2-10.2 5.7-26.8 5.7-12.3 0-24.1-1.9-30.7-4.7v-10c6.3 2.8 20.1 5.5 30.7 5.5 9.3 0 15.8-.3 17.5-2.1.6-.6.7-1.3.7-2 0-.8-.2-1.3-.7-1.8-1-1-2.6-1.7-17.4-2.1-15.7-.4-23.4-2-27-5.6-1.7-1.7-2.6-4.4-2.6-7.5 0-3.3.6-6.2 3.3-8.9 3.6-3.6 10.7-5.3 25.1-5.3 10.8 0 21.6 1.7 27.3 4v10.1c-6.5-2.8-17.8-4.8-27.2-4.8-10.4 0-14.8.6-16.2 2-.5.5-.8 1.1-.8 1.9 0 .9.2 1.5.7 2 1.3 1.3 6.1 1.7 17.3 1.9 16.4.4 23.5 1.8 27 5.2 1.8 1.8 2.8 4.7 2.8 7.7.1 3.2-.6 6.4-3 8.8z\"/><path d=\"M375.2 39.5c-11.2-.2-16-.6-17.3-1.9-.5-.5-.7-1.1-.7-2 0-.8.3-1.4.8-1.9 1.3-1.3 5.8-2 16.2-2 9.4 0 20.7 2 27.2 4.8v-10c-5.7-2.3-16.6-4-27.3-4-14.5 0-21.6 1.8-25.1 5.3-2.7 2.7-3.3 5.6-3.3 8.9 0 3.1 1 5.8 2.6 7.5 3.6 3.6 11.3 5.2 27 5.6 14.8.4 16.4 1.1 17.4 2.1.5.5.7 1 .7 1.8 0 .7-.1 1.3-.7 2-1.8 1.8-8.3 2.1-17.5 2.1-10.6 0-24.3-2.6-30.7-5.5v10.1c6.6 2.8 18.4 4.7 30.7 4.7 16.6 0 23.6-2.5 26.8-5.7 2.4-2.4 3.1-5.6 3.1-8.9 0-3.1-1-5.9-2.8-7.7-3.6-3.5-10.7-4.9-27.1-5.3z\" class=\"st1\"/></svg>\n\n\t<a href=%s class='version'>v%s</a>\n\n\t</div>\n\t<br/>\n\t<a href=.%s>General</a>\n\t<a href=.%s>JetStream</a>\n\t<a href=.%s>Connections</a>\n\t<a href=.%s>Accounts</a>\n\t<a href=.%s>Account Stats</a>\n\t<a href=.%s>Subscriptions</a>\n\t<a href=.%s>Routes</a>\n\t<a href=.%s>LeafNodes</a>\n\t<a href=.%s>Gateways</a>\n\t<a href=.%s>Raft Groups</a>\n\t<a href=.%s class=last>Health Probe</a>\n    <a href=https://docs.nats.io/running-a-nats-service/nats_admin/monitoring class=\"help\">Help</a>\n  </body>\n</html>`,\n\t\tsrcUrl,\n\t\tVERSION,\n\t\ts.basePath(VarzPath),\n\t\ts.basePath(JszPath),\n\t\ts.basePath(ConnzPath),\n\t\ts.basePath(AccountzPath),\n\t\ts.basePath(AccountStatzPath),\n\t\ts.basePath(SubszPath),\n\t\ts.basePath(RoutezPath),\n\t\ts.basePath(LeafzPath),\n\t\ts.basePath(GatewayzPath),\n\t\ts.basePath(RaftzPath),\n\t\ts.basePath(HealthzPath),\n\t)\n}\n\nfunc (s *Server) updateJszVarz(js *jetStream, v *JetStreamVarz, doConfig bool) {\n\tif doConfig {\n\t\tjs.mu.RLock()\n\t\t// We want to snapshot the config since it will then be available outside\n\t\t// of the js lock. So make a copy first, then point to this copy.\n\t\tcfg := js.config\n\t\tv.Config = &cfg\n\t\tjs.mu.RUnlock()\n\t}\n\tv.Stats = js.usageStats()\n\tv.Limits = &s.getOpts().JetStreamLimits\n\tif mg := js.getMetaGroup(); mg != nil {\n\t\tif ci := s.raftNodeToClusterInfo(mg); ci != nil {\n\t\t\tv.Meta = &MetaClusterInfo{Name: ci.Name, Leader: ci.Leader, Peer: getHash(ci.Leader), Size: mg.ClusterSize()}\n\t\t\tif ci.Leader == s.info.Name {\n\t\t\t\tv.Meta.Replicas = ci.Replicas\n\t\t\t}\n\t\t\tif ipq := s.jsAPIRoutedReqs; ipq != nil {\n\t\t\t\tv.Meta.Pending = ipq.len()\n\t\t\t}\n\t\t}\n\t}\n}\n\n// Varz returns a Varz struct containing the server information.\nfunc (s *Server) Varz(varzOpts *VarzOptions) (*Varz, error) {\n\tvar rss, vss int64\n\tvar pcpu float64\n\n\t// We want to do that outside of the lock.\n\tpse.ProcUsage(&pcpu, &rss, &vss)\n\n\ts.mu.RLock()\n\t// We need to create a new instance of Varz (with no reference\n\t// whatsoever to anything stored in the server) since the user\n\t// has access to the returned value.\n\tv := s.createVarz(pcpu, rss)\n\ts.mu.RUnlock()\n\n\tif js := s.getJetStream(); js != nil {\n\t\ts.updateJszVarz(js, &v.JetStream, true)\n\t}\n\n\treturn v, nil\n}\n\n// Returns a Varz instance.\n// Server lock is held on entry.\nfunc (s *Server) createVarz(pcpu float64, rss int64) *Varz {\n\tinfo := s.info\n\topts := s.getOpts()\n\tc := &opts.Cluster\n\tgw := &opts.Gateway\n\tln := &opts.LeafNode\n\tmqtt := &opts.MQTT\n\tws := &opts.Websocket\n\tclustTlsReq := c.TLSConfig != nil\n\tgatewayTlsReq := gw.TLSConfig != nil\n\tleafTlsReq := ln.TLSConfig != nil\n\tleafTlsVerify := leafTlsReq && ln.TLSConfig.ClientAuth == tls.RequireAndVerifyClientCert\n\tleafTlsOCSPPeerVerify := s.ocspPeerVerify && leafTlsReq && ln.tlsConfigOpts.OCSPPeerConfig != nil && ln.tlsConfigOpts.OCSPPeerConfig.Verify\n\tmqttTlsOCSPPeerVerify := s.ocspPeerVerify && mqtt.TLSConfig != nil && mqtt.tlsConfigOpts.OCSPPeerConfig != nil && mqtt.tlsConfigOpts.OCSPPeerConfig.Verify\n\twsTlsOCSPPeerVerify := s.ocspPeerVerify && ws.TLSConfig != nil && ws.tlsConfigOpts.OCSPPeerConfig != nil && ws.tlsConfigOpts.OCSPPeerConfig.Verify\n\tvarz := &Varz{\n\t\tID:           info.ID,\n\t\tVersion:      info.Version,\n\t\tProto:        info.Proto,\n\t\tGitCommit:    info.GitCommit,\n\t\tGoVersion:    info.GoVersion,\n\t\tName:         info.Name,\n\t\tHost:         info.Host,\n\t\tPort:         info.Port,\n\t\tIP:           info.IP,\n\t\tHTTPHost:     opts.HTTPHost,\n\t\tHTTPPort:     opts.HTTPPort,\n\t\tHTTPBasePath: opts.HTTPBasePath,\n\t\tHTTPSPort:    opts.HTTPSPort,\n\t\tCluster: ClusterOptsVarz{\n\t\t\tName:        info.Cluster,\n\t\t\tHost:        c.Host,\n\t\t\tPort:        c.Port,\n\t\t\tAuthTimeout: c.AuthTimeout,\n\t\t\tTLSTimeout:  c.TLSTimeout,\n\t\t\tTLSRequired: clustTlsReq,\n\t\t\tTLSVerify:   clustTlsReq,\n\t\t\tPoolSize:    opts.Cluster.PoolSize,\n\t\t},\n\t\tGateway: GatewayOptsVarz{\n\t\t\tName:           gw.Name,\n\t\t\tHost:           gw.Host,\n\t\t\tPort:           gw.Port,\n\t\t\tAuthTimeout:    gw.AuthTimeout,\n\t\t\tTLSTimeout:     gw.TLSTimeout,\n\t\t\tTLSRequired:    gatewayTlsReq,\n\t\t\tTLSVerify:      gatewayTlsReq,\n\t\t\tAdvertise:      gw.Advertise,\n\t\t\tConnectRetries: gw.ConnectRetries,\n\t\t\tGateways:       []RemoteGatewayOptsVarz{},\n\t\t\tRejectUnknown:  gw.RejectUnknown,\n\t\t},\n\t\tLeafNode: LeafNodeOptsVarz{\n\t\t\tHost:              ln.Host,\n\t\t\tPort:              ln.Port,\n\t\t\tAuthTimeout:       ln.AuthTimeout,\n\t\t\tTLSTimeout:        ln.TLSTimeout,\n\t\t\tTLSRequired:       leafTlsReq,\n\t\t\tTLSVerify:         leafTlsVerify,\n\t\t\tTLSOCSPPeerVerify: leafTlsOCSPPeerVerify,\n\t\t\tRemotes:           []RemoteLeafOptsVarz{},\n\t\t},\n\t\tMQTT: MQTTOptsVarz{\n\t\t\tHost:              mqtt.Host,\n\t\t\tPort:              mqtt.Port,\n\t\t\tNoAuthUser:        mqtt.NoAuthUser,\n\t\t\tAuthTimeout:       mqtt.AuthTimeout,\n\t\t\tTLSMap:            mqtt.TLSMap,\n\t\t\tTLSTimeout:        mqtt.TLSTimeout,\n\t\t\tJsDomain:          mqtt.JsDomain,\n\t\t\tAckWait:           mqtt.AckWait,\n\t\t\tMaxAckPending:     mqtt.MaxAckPending,\n\t\t\tTLSOCSPPeerVerify: mqttTlsOCSPPeerVerify,\n\t\t},\n\t\tWebsocket: WebsocketOptsVarz{\n\t\t\tHost:              ws.Host,\n\t\t\tPort:              ws.Port,\n\t\t\tAdvertise:         ws.Advertise,\n\t\t\tNoAuthUser:        ws.NoAuthUser,\n\t\t\tJWTCookie:         ws.JWTCookie,\n\t\t\tAuthTimeout:       ws.AuthTimeout,\n\t\t\tNoTLS:             ws.NoTLS,\n\t\t\tTLSMap:            ws.TLSMap,\n\t\t\tSameOrigin:        ws.SameOrigin,\n\t\t\tAllowedOrigins:    copyStrings(ws.AllowedOrigins),\n\t\t\tCompression:       ws.Compression,\n\t\t\tHandshakeTimeout:  ws.HandshakeTimeout,\n\t\t\tTLSOCSPPeerVerify: wsTlsOCSPPeerVerify,\n\t\t},\n\t\tStart:                 s.start.UTC(),\n\t\tMaxSubs:               opts.MaxSubs,\n\t\tCores:                 runtime.NumCPU(),\n\t\tMaxProcs:              runtime.GOMAXPROCS(0),\n\t\tTags:                  opts.Tags,\n\t\tTrustedOperatorsJwt:   opts.operatorJWT,\n\t\tTrustedOperatorsClaim: opts.TrustedOperators,\n\t}\n\tif mm := debug.SetMemoryLimit(-1); mm < math.MaxInt64 {\n\t\tvarz.MemLimit = mm\n\t}\n\t// If this is a leaf without cluster, reset the cluster name (that is otherwise\n\t// set to the server name).\n\tif s.leafNoCluster {\n\t\tvarz.Cluster.Name = _EMPTY_\n\t}\n\tif len(opts.Routes) > 0 {\n\t\tvarz.Cluster.URLs = urlsToStrings(opts.Routes)\n\t}\n\tif l := len(gw.Gateways); l > 0 {\n\t\trgwa := make([]RemoteGatewayOptsVarz, l)\n\t\tfor i, r := range gw.Gateways {\n\t\t\trgwa[i] = RemoteGatewayOptsVarz{\n\t\t\t\tName:       r.Name,\n\t\t\t\tTLSTimeout: r.TLSTimeout,\n\t\t\t}\n\t\t}\n\t\tvarz.Gateway.Gateways = rgwa\n\t}\n\tif l := len(ln.Remotes); l > 0 {\n\t\trlna := make([]RemoteLeafOptsVarz, l)\n\t\tfor i, r := range ln.Remotes {\n\t\t\tvar deny *DenyRules\n\t\t\tif len(r.DenyImports) > 0 || len(r.DenyExports) > 0 {\n\t\t\t\tdeny = &DenyRules{\n\t\t\t\t\tImports: r.DenyImports,\n\t\t\t\t\tExports: r.DenyExports,\n\t\t\t\t}\n\t\t\t}\n\t\t\tremoteTlsOCSPPeerVerify := s.ocspPeerVerify && r.tlsConfigOpts != nil && r.tlsConfigOpts.OCSPPeerConfig != nil && r.tlsConfigOpts.OCSPPeerConfig.Verify\n\n\t\t\trlna[i] = RemoteLeafOptsVarz{\n\t\t\t\tLocalAccount:      r.LocalAccount,\n\t\t\t\tURLs:              urlsToStrings(r.URLs),\n\t\t\t\tTLSTimeout:        r.TLSTimeout,\n\t\t\t\tDeny:              deny,\n\t\t\t\tTLSOCSPPeerVerify: remoteTlsOCSPPeerVerify,\n\t\t\t}\n\t\t}\n\t\tvarz.LeafNode.Remotes = rlna\n\t}\n\n\t// Finish setting it up with fields that can be updated during\n\t// configuration reload and runtime.\n\ts.updateVarzConfigReloadableFields(varz)\n\ts.updateVarzRuntimeFields(varz, true, pcpu, rss)\n\treturn varz\n}\n\nfunc urlsToStrings(urls []*url.URL) []string {\n\tsURLs := make([]string, len(urls))\n\tfor i, u := range urls {\n\t\tsURLs[i] = u.Host\n\t}\n\treturn sURLs\n}\n\n// Invoked during configuration reload once options have possibly be changed\n// and config load time has been set. If s.varz has not been initialized yet\n// (because no pooling of /varz has been made), this function does nothing.\n// Server lock is held on entry.\nfunc (s *Server) updateVarzConfigReloadableFields(v *Varz) {\n\tif v == nil {\n\t\treturn\n\t}\n\topts := s.getOpts()\n\tinfo := &s.info\n\tv.AuthRequired = info.AuthRequired\n\tv.TLSRequired = info.TLSRequired\n\tv.TLSVerify = info.TLSVerify\n\tv.MaxConn = opts.MaxConn\n\tv.PingInterval = opts.PingInterval\n\tv.MaxPingsOut = opts.MaxPingsOut\n\tv.AuthTimeout = opts.AuthTimeout\n\tv.MaxControlLine = opts.MaxControlLine\n\tv.MaxPayload = int(opts.MaxPayload)\n\tv.MaxPending = opts.MaxPending\n\tv.TLSTimeout = opts.TLSTimeout\n\tv.WriteDeadline = opts.WriteDeadline\n\tv.ConfigLoadTime = s.configTime.UTC()\n\tv.ConfigDigest = opts.configDigest\n\t// Update route URLs if applicable\n\tif s.varzUpdateRouteURLs {\n\t\tv.Cluster.URLs = urlsToStrings(opts.Routes)\n\t\ts.varzUpdateRouteURLs = false\n\t}\n\tif s.sys != nil && s.sys.account != nil {\n\t\tv.SystemAccount = s.sys.account.GetName()\n\t}\n\tv.MQTT.TLSPinnedCerts = getPinnedCertsAsSlice(opts.MQTT.TLSPinnedCerts)\n\tv.Websocket.TLSPinnedCerts = getPinnedCertsAsSlice(opts.Websocket.TLSPinnedCerts)\n\n\tv.TLSOCSPPeerVerify = s.ocspPeerVerify && v.TLSRequired && s.opts.tlsConfigOpts != nil && s.opts.tlsConfigOpts.OCSPPeerConfig != nil && s.opts.tlsConfigOpts.OCSPPeerConfig.Verify\n}\n\nfunc getPinnedCertsAsSlice(certs PinnedCertSet) []string {\n\tif len(certs) == 0 {\n\t\treturn nil\n\t}\n\tres := make([]string, 0, len(certs))\n\tfor cn := range certs {\n\t\tres = append(res, cn)\n\t}\n\treturn res\n}\n\n// Updates the runtime Varz fields, that is, fields that change during\n// runtime and that should be updated any time Varz() or polling of /varz\n// is done.\n// Server lock is held on entry.\nfunc (s *Server) updateVarzRuntimeFields(v *Varz, forceUpdate bool, pcpu float64, rss int64) {\n\tv.Now = time.Now().UTC()\n\tv.Uptime = myUptime(time.Since(s.start))\n\tv.Mem = rss\n\tv.CPU = pcpu\n\tif l := len(s.info.ClientConnectURLs); l > 0 {\n\t\tv.ClientConnectURLs = append([]string(nil), s.info.ClientConnectURLs...)\n\t}\n\tif l := len(s.info.WSConnectURLs); l > 0 {\n\t\tv.WSConnectURLs = append([]string(nil), s.info.WSConnectURLs...)\n\t}\n\tv.Connections = len(s.clients)\n\tv.TotalConnections = s.totalClients\n\tv.Routes = s.numRoutes()\n\tv.Remotes = s.numRemotes()\n\tv.Leafs = len(s.leafs)\n\tv.InMsgs = atomic.LoadInt64(&s.inMsgs)\n\tv.InBytes = atomic.LoadInt64(&s.inBytes)\n\tv.OutMsgs = atomic.LoadInt64(&s.outMsgs)\n\tv.OutBytes = atomic.LoadInt64(&s.outBytes)\n\tv.SlowConsumers = atomic.LoadInt64(&s.slowConsumers)\n\tv.SlowConsumersStats = &SlowConsumersStats{\n\t\tClients:  s.NumSlowConsumersClients(),\n\t\tRoutes:   s.NumSlowConsumersRoutes(),\n\t\tGateways: s.NumSlowConsumersGateways(),\n\t\tLeafs:    s.NumSlowConsumersLeafs(),\n\t}\n\tv.PinnedAccountFail = atomic.LoadUint64(&s.pinnedAccFail)\n\n\t// Make sure to reset in case we are re-using.\n\tv.Subscriptions = 0\n\ts.accounts.Range(func(k, val any) bool {\n\t\tacc := val.(*Account)\n\t\tv.Subscriptions += acc.sl.Count()\n\t\treturn true\n\t})\n\n\tv.HTTPReqStats = make(map[string]uint64, len(s.httpReqStats))\n\tfor key, val := range s.httpReqStats {\n\t\tv.HTTPReqStats[key] = val\n\t}\n\n\t// Update Gateway remote urls if applicable\n\tgw := s.gateway\n\tgw.RLock()\n\tif gw.enabled {\n\t\tfor i := 0; i < len(v.Gateway.Gateways); i++ {\n\t\t\tg := &v.Gateway.Gateways[i]\n\t\t\trgw := gw.remotes[g.Name]\n\t\t\tif rgw != nil {\n\t\t\t\trgw.RLock()\n\t\t\t\t// forceUpdate is needed if user calls Varz() programmatically,\n\t\t\t\t// since we need to create a new instance every time and the\n\t\t\t\t// gateway's varzUpdateURLs may have been set to false after\n\t\t\t\t// a web /varz inspection.\n\t\t\t\tif forceUpdate || rgw.varzUpdateURLs {\n\t\t\t\t\t// Make reuse of backend array\n\t\t\t\t\tg.URLs = g.URLs[:0]\n\t\t\t\t\t// rgw.urls is a map[string]*url.URL where the key is\n\t\t\t\t\t// already in the right format (host:port, without any\n\t\t\t\t\t// user info present).\n\t\t\t\t\tfor u := range rgw.urls {\n\t\t\t\t\t\tg.URLs = append(g.URLs, u)\n\t\t\t\t\t}\n\t\t\t\t\trgw.varzUpdateURLs = false\n\t\t\t\t}\n\t\t\t\trgw.RUnlock()\n\t\t\t} else if g.Name == gw.name && len(gw.ownCfgURLs) > 0 {\n\t\t\t\t// This is a remote that correspond to this very same server.\n\t\t\t\t// We report the URLs that were configured (if any).\n\t\t\t\t// Since we don't support changes to the gateway configuration\n\t\t\t\t// at this time, we could do this only if g.URLs has not been already\n\t\t\t\t// set, but let's do it regardless in case we add support for\n\t\t\t\t// gateway config reload.\n\t\t\t\tg.URLs = g.URLs[:0]\n\t\t\t\tg.URLs = append(g.URLs, gw.ownCfgURLs...)\n\t\t\t}\n\t\t}\n\t}\n\tgw.RUnlock()\n\n\tif s.ocsprc != nil && s.ocsprc.Type() != \"none\" {\n\t\tstats := s.ocsprc.Stats()\n\t\tif stats != nil {\n\t\t\tv.OCSPResponseCache = &OCSPResponseCacheVarz{\n\t\t\t\ts.ocsprc.Type(),\n\t\t\t\tstats.Hits,\n\t\t\t\tstats.Misses,\n\t\t\t\tstats.Responses,\n\t\t\t\tstats.Revokes,\n\t\t\t\tstats.Goods,\n\t\t\t\tstats.Unknowns,\n\t\t\t}\n\t\t}\n\t}\n}\n\n// HandleVarz will process HTTP requests for server information.\nfunc (s *Server) HandleVarz(w http.ResponseWriter, r *http.Request) {\n\tvar rss, vss int64\n\tvar pcpu float64\n\n\t// We want to do that outside of the lock.\n\tpse.ProcUsage(&pcpu, &rss, &vss)\n\n\t// In response to http requests, we want to minimize mem copies\n\t// so we use an object stored in the server. Creating/collecting\n\t// server metrics is done under server lock, but we don't want\n\t// to marshal under that lock. Still, we need to prevent concurrent\n\t// http requests to /varz to update s.varz while marshal is\n\t// happening, so we need a new lock that serialize those http\n\t// requests and include marshaling.\n\ts.varzMu.Lock()\n\n\t// Use server lock to create/update the server's varz object.\n\ts.mu.Lock()\n\tvar created bool\n\ts.httpReqStats[VarzPath]++\n\tif s.varz == nil {\n\t\ts.varz = s.createVarz(pcpu, rss)\n\t\tcreated = true\n\t} else {\n\t\ts.updateVarzRuntimeFields(s.varz, false, pcpu, rss)\n\t}\n\ts.mu.Unlock()\n\t// Since locking is jetStream -> Server, need to update jetstream\n\t// varz outside of server lock.\n\n\tif js := s.getJetStream(); js != nil {\n\t\tvar v JetStreamVarz\n\t\t// Work on stack variable\n\t\ts.updateJszVarz(js, &v, created)\n\t\t// Now update server's varz\n\t\ts.mu.RLock()\n\t\tsv := &s.varz.JetStream\n\t\tif created {\n\t\t\tsv.Config = v.Config\n\t\t}\n\t\tsv.Stats = v.Stats\n\t\tsv.Meta = v.Meta\n\t\tsv.Limits = v.Limits\n\t\ts.mu.RUnlock()\n\t}\n\n\t// Do the marshaling outside of server lock, but under varzMu lock.\n\tb, err := json.MarshalIndent(s.varz, \"\", \"  \")\n\ts.varzMu.Unlock()\n\n\tif err != nil {\n\t\ts.Errorf(\"Error marshaling response to /varz request: %v\", err)\n\t}\n\n\t// Handle response\n\tResponseHandler(w, r, b)\n}\n\n// GatewayzOptions are the options passed to Gatewayz()\ntype GatewayzOptions struct {\n\t// Name will output only remote gateways with this name\n\tName string `json:\"name\"`\n\n\t// Accounts indicates if accounts with its interest should be included in the results.\n\tAccounts bool `json:\"accounts\"`\n\n\t// AccountName will limit the list of accounts to that account name (makes Accounts implicit)\n\tAccountName string `json:\"account_name\"`\n\n\t// AccountSubscriptions indicates if subscriptions should be included in the results.\n\t// Note: This is used only if `Accounts` or `AccountName` are specified.\n\tAccountSubscriptions bool `json:\"subscriptions\"`\n\n\t// AccountSubscriptionsDetail indicates if subscription details should be included in the results.\n\t// Note: This is used only if `Accounts` or `AccountName` are specified.\n\tAccountSubscriptionsDetail bool `json:\"subscriptions_detail\"`\n}\n\n// Gatewayz represents detailed information on Gateways\ntype Gatewayz struct {\n\tID               string                       `json:\"server_id\"`\n\tNow              time.Time                    `json:\"now\"`\n\tName             string                       `json:\"name,omitempty\"`\n\tHost             string                       `json:\"host,omitempty\"`\n\tPort             int                          `json:\"port,omitempty\"`\n\tOutboundGateways map[string]*RemoteGatewayz   `json:\"outbound_gateways\"`\n\tInboundGateways  map[string][]*RemoteGatewayz `json:\"inbound_gateways\"`\n}\n\n// RemoteGatewayz represents information about an outbound connection to a gateway\ntype RemoteGatewayz struct {\n\tIsConfigured bool               `json:\"configured\"`\n\tConnection   *ConnInfo          `json:\"connection,omitempty\"`\n\tAccounts     []*AccountGatewayz `json:\"accounts,omitempty\"`\n}\n\n// AccountGatewayz represents interest mode for this account\ntype AccountGatewayz struct {\n\tName                  string      `json:\"name\"`\n\tInterestMode          string      `json:\"interest_mode\"`\n\tNoInterestCount       int         `json:\"no_interest_count,omitempty\"`\n\tInterestOnlyThreshold int         `json:\"interest_only_threshold,omitempty\"`\n\tTotalSubscriptions    int         `json:\"num_subs,omitempty\"`\n\tNumQueueSubscriptions int         `json:\"num_queue_subs,omitempty\"`\n\tSubs                  []string    `json:\"subscriptions_list,omitempty\"`\n\tSubsDetail            []SubDetail `json:\"subscriptions_list_detail,omitempty\"`\n}\n\n// Gatewayz returns a Gatewayz struct containing information about gateways.\nfunc (s *Server) Gatewayz(opts *GatewayzOptions) (*Gatewayz, error) {\n\tsrvID := s.ID()\n\tnow := time.Now().UTC()\n\tgw := s.gateway\n\tgw.RLock()\n\tif !gw.enabled || gw.info == nil {\n\t\tgw.RUnlock()\n\t\tgwz := &Gatewayz{\n\t\t\tID:               srvID,\n\t\t\tNow:              now,\n\t\t\tOutboundGateways: map[string]*RemoteGatewayz{},\n\t\t\tInboundGateways:  map[string][]*RemoteGatewayz{},\n\t\t}\n\t\treturn gwz, nil\n\t}\n\t// Here gateways are enabled, so fill up more.\n\tgwz := &Gatewayz{\n\t\tID:   srvID,\n\t\tNow:  now,\n\t\tName: gw.name,\n\t\tHost: gw.info.Host,\n\t\tPort: gw.info.Port,\n\t}\n\tgw.RUnlock()\n\n\tgwz.OutboundGateways = s.createOutboundsRemoteGatewayz(opts, now)\n\tgwz.InboundGateways = s.createInboundsRemoteGatewayz(opts, now)\n\n\treturn gwz, nil\n}\n\n// Based on give options struct, returns if there is a filtered\n// Gateway Name and if we should do report Accounts.\n// Note that if Accounts is false but AccountName is not empty,\n// then Accounts is implicitly set to true.\nfunc getMonitorGWOptions(opts *GatewayzOptions) (string, bool) {\n\tvar name string\n\tvar accs bool\n\tif opts != nil {\n\t\tif opts.Name != _EMPTY_ {\n\t\t\tname = opts.Name\n\t\t}\n\t\taccs = opts.Accounts\n\t\tif !accs && opts.AccountName != _EMPTY_ {\n\t\t\taccs = true\n\t\t}\n\t}\n\treturn name, accs\n}\n\n// Returns a map of gateways outbound connections.\n// Based on options, will include a single or all gateways,\n// with no/single/or all accounts interest information.\nfunc (s *Server) createOutboundsRemoteGatewayz(opts *GatewayzOptions, now time.Time) map[string]*RemoteGatewayz {\n\ttargetGWName, doAccs := getMonitorGWOptions(opts)\n\n\tif targetGWName != _EMPTY_ {\n\t\tc := s.getOutboundGatewayConnection(targetGWName)\n\t\tif c == nil {\n\t\t\treturn nil\n\t\t}\n\t\toutbounds := make(map[string]*RemoteGatewayz, 1)\n\t\t_, rgw := createOutboundRemoteGatewayz(c, opts, now, doAccs)\n\t\toutbounds[targetGWName] = rgw\n\t\treturn outbounds\n\t}\n\n\tvar connsa [16]*client\n\tvar conns = connsa[:0]\n\n\ts.getOutboundGatewayConnections(&conns)\n\n\toutbounds := make(map[string]*RemoteGatewayz, len(conns))\n\tfor _, c := range conns {\n\t\tname, rgw := createOutboundRemoteGatewayz(c, opts, now, doAccs)\n\t\tif rgw != nil {\n\t\t\toutbounds[name] = rgw\n\t\t}\n\t}\n\treturn outbounds\n}\n\n// Returns a RemoteGatewayz for a given outbound gw connection\nfunc createOutboundRemoteGatewayz(c *client, opts *GatewayzOptions, now time.Time, doAccs bool) (string, *RemoteGatewayz) {\n\tvar name string\n\tvar rgw *RemoteGatewayz\n\n\tc.mu.Lock()\n\tif c.gw != nil {\n\t\trgw = &RemoteGatewayz{}\n\t\tif doAccs {\n\t\t\trgw.Accounts = createOutboundAccountsGatewayz(opts, c.gw)\n\t\t}\n\t\tif c.gw.cfg != nil {\n\t\t\trgw.IsConfigured = !c.gw.cfg.isImplicit()\n\t\t}\n\t\trgw.Connection = &ConnInfo{}\n\t\trgw.Connection.fill(c, c.nc, now, false)\n\t\tname = c.gw.name\n\t}\n\tc.mu.Unlock()\n\n\treturn name, rgw\n}\n\n// Returns the list of accounts for this outbound gateway connection.\n// Based on the options, it will be a single or all accounts for\n// this outbound.\nfunc createOutboundAccountsGatewayz(opts *GatewayzOptions, gw *gateway) []*AccountGatewayz {\n\tif gw.outsim == nil {\n\t\treturn nil\n\t}\n\n\tvar accName string\n\tif opts != nil {\n\t\taccName = opts.AccountName\n\t}\n\tif accName != _EMPTY_ {\n\t\tei, ok := gw.outsim.Load(accName)\n\t\tif !ok {\n\t\t\treturn nil\n\t\t}\n\t\ta := createAccountOutboundGatewayz(opts, accName, ei)\n\t\treturn []*AccountGatewayz{a}\n\t}\n\n\taccs := make([]*AccountGatewayz, 0, 4)\n\tgw.outsim.Range(func(k, v any) bool {\n\t\tname := k.(string)\n\t\ta := createAccountOutboundGatewayz(opts, name, v)\n\t\taccs = append(accs, a)\n\t\treturn true\n\t})\n\treturn accs\n}\n\n// Returns an AccountGatewayz for this gateway outbound connection\nfunc createAccountOutboundGatewayz(opts *GatewayzOptions, name string, ei any) *AccountGatewayz {\n\ta := &AccountGatewayz{\n\t\tName:                  name,\n\t\tInterestOnlyThreshold: gatewayMaxRUnsubBeforeSwitch,\n\t}\n\tif ei != nil {\n\t\te := ei.(*outsie)\n\t\te.RLock()\n\t\ta.InterestMode = e.mode.String()\n\t\ta.NoInterestCount = len(e.ni)\n\t\ta.NumQueueSubscriptions = e.qsubs\n\t\ta.TotalSubscriptions = int(e.sl.Count())\n\t\tif opts.AccountSubscriptions || opts.AccountSubscriptionsDetail {\n\t\t\tvar subsa [4096]*subscription\n\t\t\tsubs := subsa[:0]\n\t\t\te.sl.All(&subs)\n\t\t\tif opts.AccountSubscriptions {\n\t\t\t\ta.Subs = make([]string, 0, len(subs))\n\t\t\t} else {\n\t\t\t\ta.SubsDetail = make([]SubDetail, 0, len(subs))\n\t\t\t}\n\t\t\tfor _, sub := range subs {\n\t\t\t\tif opts.AccountSubscriptions {\n\t\t\t\t\ta.Subs = append(a.Subs, string(sub.subject))\n\t\t\t\t} else {\n\t\t\t\t\ta.SubsDetail = append(a.SubsDetail, newClientSubDetail(sub))\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\te.RUnlock()\n\t} else {\n\t\ta.InterestMode = Optimistic.String()\n\t}\n\treturn a\n}\n\n// Returns a map of gateways inbound connections.\n// Each entry is an array of RemoteGatewayz since a given server\n// may have more than one inbound from the same remote gateway.\n// Based on options, will include a single or all gateways,\n// with no/single/or all accounts interest information.\nfunc (s *Server) createInboundsRemoteGatewayz(opts *GatewayzOptions, now time.Time) map[string][]*RemoteGatewayz {\n\ttargetGWName, doAccs := getMonitorGWOptions(opts)\n\n\tvar connsa [16]*client\n\tvar conns = connsa[:0]\n\ts.getInboundGatewayConnections(&conns)\n\n\tm := make(map[string][]*RemoteGatewayz)\n\tfor _, c := range conns {\n\t\tc.mu.Lock()\n\t\tif c.gw != nil && (targetGWName == _EMPTY_ || targetGWName == c.gw.name) {\n\t\t\tigws := m[c.gw.name]\n\t\t\tif igws == nil {\n\t\t\t\tigws = make([]*RemoteGatewayz, 0, 2)\n\t\t\t}\n\t\t\trgw := &RemoteGatewayz{}\n\t\t\tif doAccs {\n\t\t\t\trgw.Accounts = createInboundAccountsGatewayz(opts, c.gw)\n\t\t\t}\n\t\t\trgw.Connection = &ConnInfo{}\n\t\t\trgw.Connection.fill(c, c.nc, now, false)\n\t\t\tigws = append(igws, rgw)\n\t\t\tm[c.gw.name] = igws\n\t\t}\n\t\tc.mu.Unlock()\n\t}\n\treturn m\n}\n\n// Returns the list of accounts for this inbound gateway connection.\n// Based on the options, it will be a single or all accounts for\n// this inbound.\nfunc createInboundAccountsGatewayz(opts *GatewayzOptions, gw *gateway) []*AccountGatewayz {\n\tif gw.insim == nil {\n\t\treturn nil\n\t}\n\n\tvar accName string\n\tif opts != nil {\n\t\taccName = opts.AccountName\n\t}\n\tif accName != _EMPTY_ {\n\t\te, ok := gw.insim[accName]\n\t\tif !ok {\n\t\t\treturn nil\n\t\t}\n\t\ta := createInboundAccountGatewayz(accName, e)\n\t\treturn []*AccountGatewayz{a}\n\t}\n\n\taccs := make([]*AccountGatewayz, 0, 4)\n\tfor name, e := range gw.insim {\n\t\ta := createInboundAccountGatewayz(name, e)\n\t\taccs = append(accs, a)\n\t}\n\treturn accs\n}\n\n// Returns an AccountGatewayz for this gateway inbound connection\nfunc createInboundAccountGatewayz(name string, e *insie) *AccountGatewayz {\n\ta := &AccountGatewayz{\n\t\tName:                  name,\n\t\tInterestOnlyThreshold: gatewayMaxRUnsubBeforeSwitch,\n\t}\n\tif e != nil {\n\t\ta.InterestMode = e.mode.String()\n\t\ta.NoInterestCount = len(e.ni)\n\t} else {\n\t\ta.InterestMode = Optimistic.String()\n\t}\n\treturn a\n}\n\n// HandleGatewayz process HTTP requests for route information.\nfunc (s *Server) HandleGatewayz(w http.ResponseWriter, r *http.Request) {\n\ts.mu.Lock()\n\ts.httpReqStats[GatewayzPath]++\n\ts.mu.Unlock()\n\n\tsubs, subsDet, err := decodeSubs(w, r)\n\tif err != nil {\n\t\treturn\n\t}\n\taccs, err := decodeBool(w, r, \"accs\")\n\tif err != nil {\n\t\treturn\n\t}\n\tgwName := r.URL.Query().Get(\"gw_name\")\n\taccName := r.URL.Query().Get(\"acc_name\")\n\tif accName != _EMPTY_ {\n\t\taccs = true\n\t}\n\n\topts := &GatewayzOptions{\n\t\tName:                       gwName,\n\t\tAccounts:                   accs,\n\t\tAccountName:                accName,\n\t\tAccountSubscriptions:       subs,\n\t\tAccountSubscriptionsDetail: subsDet,\n\t}\n\tgw, err := s.Gatewayz(opts)\n\tif err != nil {\n\t\tw.WriteHeader(http.StatusBadRequest)\n\t\tw.Write([]byte(err.Error()))\n\t\treturn\n\t}\n\tb, err := json.MarshalIndent(gw, \"\", \"  \")\n\tif err != nil {\n\t\ts.Errorf(\"Error marshaling response to /gatewayz request: %v\", err)\n\t}\n\n\t// Handle response\n\tResponseHandler(w, r, b)\n}\n\n// Leafz represents detailed information on Leafnodes.\ntype Leafz struct {\n\tID       string      `json:\"server_id\"`\n\tNow      time.Time   `json:\"now\"`\n\tNumLeafs int         `json:\"leafnodes\"`\n\tLeafs    []*LeafInfo `json:\"leafs\"`\n}\n\n// LeafzOptions are options passed to Leafz\ntype LeafzOptions struct {\n\t// Subscriptions indicates that Leafz will return a leafnode's subscriptions\n\tSubscriptions bool   `json:\"subscriptions\"`\n\tAccount       string `json:\"account\"`\n}\n\n// LeafInfo has detailed information on each remote leafnode connection.\ntype LeafInfo struct {\n\tName        string   `json:\"name\"`\n\tIsSpoke     bool     `json:\"is_spoke\"`\n\tAccount     string   `json:\"account\"`\n\tIP          string   `json:\"ip\"`\n\tPort        int      `json:\"port\"`\n\tRTT         string   `json:\"rtt,omitempty\"`\n\tInMsgs      int64    `json:\"in_msgs\"`\n\tOutMsgs     int64    `json:\"out_msgs\"`\n\tInBytes     int64    `json:\"in_bytes\"`\n\tOutBytes    int64    `json:\"out_bytes\"`\n\tNumSubs     uint32   `json:\"subscriptions\"`\n\tSubs        []string `json:\"subscriptions_list,omitempty\"`\n\tCompression string   `json:\"compression,omitempty\"`\n}\n\n// Leafz returns a Leafz structure containing information about leafnodes.\nfunc (s *Server) Leafz(opts *LeafzOptions) (*Leafz, error) {\n\t// Grab leafnodes\n\tvar lconns []*client\n\ts.mu.Lock()\n\tif len(s.leafs) > 0 {\n\t\tlconns = make([]*client, 0, len(s.leafs))\n\t\tfor _, ln := range s.leafs {\n\t\t\tif opts != nil && opts.Account != _EMPTY_ {\n\t\t\t\tln.mu.Lock()\n\t\t\t\tok := ln.acc.Name == opts.Account\n\t\t\t\tln.mu.Unlock()\n\t\t\t\tif !ok {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t}\n\t\t\tlconns = append(lconns, ln)\n\t\t}\n\t}\n\ts.mu.Unlock()\n\n\tleafnodes := make([]*LeafInfo, 0, len(lconns))\n\n\tif len(lconns) > 0 {\n\t\tfor _, ln := range lconns {\n\t\t\tln.mu.Lock()\n\t\t\tlni := &LeafInfo{\n\t\t\t\tName:        ln.leaf.remoteServer,\n\t\t\t\tIsSpoke:     ln.isSpokeLeafNode(),\n\t\t\t\tAccount:     ln.acc.Name,\n\t\t\t\tIP:          ln.host,\n\t\t\t\tPort:        int(ln.port),\n\t\t\t\tRTT:         ln.getRTT().String(),\n\t\t\t\tInMsgs:      atomic.LoadInt64(&ln.inMsgs),\n\t\t\t\tOutMsgs:     ln.outMsgs,\n\t\t\t\tInBytes:     atomic.LoadInt64(&ln.inBytes),\n\t\t\t\tOutBytes:    ln.outBytes,\n\t\t\t\tNumSubs:     uint32(len(ln.subs)),\n\t\t\t\tCompression: ln.leaf.compression,\n\t\t\t}\n\t\t\tif opts != nil && opts.Subscriptions {\n\t\t\t\tlni.Subs = make([]string, 0, len(ln.subs))\n\t\t\t\tfor _, sub := range ln.subs {\n\t\t\t\t\tlni.Subs = append(lni.Subs, string(sub.subject))\n\t\t\t\t}\n\t\t\t}\n\t\t\tln.mu.Unlock()\n\t\t\tleafnodes = append(leafnodes, lni)\n\t\t}\n\t}\n\n\treturn &Leafz{\n\t\tID:       s.ID(),\n\t\tNow:      time.Now().UTC(),\n\t\tNumLeafs: len(leafnodes),\n\t\tLeafs:    leafnodes,\n\t}, nil\n}\n\n// HandleLeafz process HTTP requests for leafnode information.\nfunc (s *Server) HandleLeafz(w http.ResponseWriter, r *http.Request) {\n\ts.mu.Lock()\n\ts.httpReqStats[LeafzPath]++\n\ts.mu.Unlock()\n\n\tsubs, err := decodeBool(w, r, \"subs\")\n\tif err != nil {\n\t\treturn\n\t}\n\tl, err := s.Leafz(&LeafzOptions{subs, r.URL.Query().Get(\"acc\")})\n\tif err != nil {\n\t\tw.WriteHeader(http.StatusBadRequest)\n\t\tw.Write([]byte(err.Error()))\n\t\treturn\n\t}\n\tb, err := json.MarshalIndent(l, \"\", \"  \")\n\tif err != nil {\n\t\ts.Errorf(\"Error marshaling response to /leafz request: %v\", err)\n\t}\n\n\t// Handle response\n\tResponseHandler(w, r, b)\n}\n\n// Leafz represents detailed information on Leafnodes.\ntype AccountStatz struct {\n\tID       string         `json:\"server_id\"`\n\tNow      time.Time      `json:\"now\"`\n\tAccounts []*AccountStat `json:\"account_statz\"`\n}\n\n// AccountStatzOptions are options passed to account stats requests.\ntype AccountStatzOptions struct {\n\tAccounts      []string `json:\"accounts\"`\n\tIncludeUnused bool     `json:\"include_unused\"`\n}\n\n// Leafz returns a AccountStatz structure containing summary information about accounts.\nfunc (s *Server) AccountStatz(opts *AccountStatzOptions) (*AccountStatz, error) {\n\tstz := &AccountStatz{\n\t\tID:       s.ID(),\n\t\tNow:      time.Now().UTC(),\n\t\tAccounts: []*AccountStat{},\n\t}\n\tif opts == nil || len(opts.Accounts) == 0 {\n\t\ts.accounts.Range(func(key, a any) bool {\n\t\t\tacc := a.(*Account)\n\t\t\tacc.mu.RLock()\n\t\t\tif opts.IncludeUnused || acc.numLocalConnections() != 0 {\n\t\t\t\tstz.Accounts = append(stz.Accounts, acc.statz())\n\t\t\t}\n\t\t\tacc.mu.RUnlock()\n\t\t\treturn true\n\t\t})\n\t} else {\n\t\tfor _, a := range opts.Accounts {\n\t\t\tif acc, ok := s.accounts.Load(a); ok {\n\t\t\t\tacc := acc.(*Account)\n\t\t\t\tacc.mu.RLock()\n\t\t\t\tif opts.IncludeUnused || acc.numLocalConnections() != 0 {\n\t\t\t\t\tstz.Accounts = append(stz.Accounts, acc.statz())\n\t\t\t\t}\n\t\t\t\tacc.mu.RUnlock()\n\t\t\t}\n\t\t}\n\t}\n\treturn stz, nil\n}\n\n// HandleAccountStatz process HTTP requests for statz information of all accounts.\nfunc (s *Server) HandleAccountStatz(w http.ResponseWriter, r *http.Request) {\n\ts.mu.Lock()\n\ts.httpReqStats[AccountStatzPath]++\n\ts.mu.Unlock()\n\n\tunused, err := decodeBool(w, r, \"unused\")\n\tif err != nil {\n\t\treturn\n\t}\n\n\tl, err := s.AccountStatz(&AccountStatzOptions{IncludeUnused: unused})\n\tif err != nil {\n\t\tw.WriteHeader(http.StatusBadRequest)\n\t\tw.Write([]byte(err.Error()))\n\t\treturn\n\t}\n\tb, err := json.MarshalIndent(l, \"\", \"  \")\n\tif err != nil {\n\t\ts.Errorf(\"Error marshaling response to %s request: %v\", AccountStatzPath, err)\n\t\treturn\n\t}\n\n\t// Handle response\n\tResponseHandler(w, r, b)\n}\n\n// ResponseHandler handles responses for monitoring routes.\nfunc ResponseHandler(w http.ResponseWriter, r *http.Request, data []byte) {\n\thandleResponse(http.StatusOK, w, r, data)\n}\n\n// handleResponse handles responses for monitoring routes with a specific HTTP status code.\nfunc handleResponse(code int, w http.ResponseWriter, r *http.Request, data []byte) {\n\t// Get callback from request\n\tcallback := r.URL.Query().Get(\"callback\")\n\tif callback != _EMPTY_ {\n\t\t// Response for JSONP\n\t\tw.Header().Set(\"Content-Type\", \"application/javascript\")\n\t\tw.WriteHeader(code)\n\t\tfmt.Fprintf(w, \"%s(%s)\", callback, data)\n\t} else {\n\t\t// Otherwise JSON\n\t\tw.Header().Set(\"Content-Type\", \"application/json\")\n\t\tw.Header().Set(\"Access-Control-Allow-Origin\", \"*\")\n\t\tw.WriteHeader(code)\n\t\tw.Write(data)\n\t}\n}\n\nfunc (reason ClosedState) String() string {\n\tswitch reason {\n\tcase ClientClosed:\n\t\treturn \"Client Closed\"\n\tcase AuthenticationTimeout:\n\t\treturn \"Authentication Timeout\"\n\tcase AuthenticationViolation:\n\t\treturn \"Authentication Failure\"\n\tcase TLSHandshakeError:\n\t\treturn \"TLS Handshake Failure\"\n\tcase SlowConsumerPendingBytes:\n\t\treturn \"Slow Consumer (Pending Bytes)\"\n\tcase SlowConsumerWriteDeadline:\n\t\treturn \"Slow Consumer (Write Deadline)\"\n\tcase WriteError:\n\t\treturn \"Write Error\"\n\tcase ReadError:\n\t\treturn \"Read Error\"\n\tcase ParseError:\n\t\treturn \"Parse Error\"\n\tcase StaleConnection:\n\t\treturn \"Stale Connection\"\n\tcase ProtocolViolation:\n\t\treturn \"Protocol Violation\"\n\tcase BadClientProtocolVersion:\n\t\treturn \"Bad Client Protocol Version\"\n\tcase WrongPort:\n\t\treturn \"Incorrect Port\"\n\tcase MaxConnectionsExceeded:\n\t\treturn \"Maximum Connections Exceeded\"\n\tcase MaxAccountConnectionsExceeded:\n\t\treturn \"Maximum Account Connections Exceeded\"\n\tcase MaxPayloadExceeded:\n\t\treturn \"Maximum Message Payload Exceeded\"\n\tcase MaxControlLineExceeded:\n\t\treturn \"Maximum Control Line Exceeded\"\n\tcase MaxSubscriptionsExceeded:\n\t\treturn \"Maximum Subscriptions Exceeded\"\n\tcase DuplicateRoute:\n\t\treturn \"Duplicate Route\"\n\tcase RouteRemoved:\n\t\treturn \"Route Removed\"\n\tcase ServerShutdown:\n\t\treturn \"Server Shutdown\"\n\tcase AuthenticationExpired:\n\t\treturn \"Authentication Expired\"\n\tcase WrongGateway:\n\t\treturn \"Wrong Gateway\"\n\tcase MissingAccount:\n\t\treturn \"Missing Account\"\n\tcase Revocation:\n\t\treturn \"Credentials Revoked\"\n\tcase InternalClient:\n\t\treturn \"Internal Client\"\n\tcase MsgHeaderViolation:\n\t\treturn \"Message Header Violation\"\n\tcase NoRespondersRequiresHeaders:\n\t\treturn \"No Responders Requires Headers\"\n\tcase ClusterNameConflict:\n\t\treturn \"Cluster Name Conflict\"\n\tcase DuplicateRemoteLeafnodeConnection:\n\t\treturn \"Duplicate Remote LeafNode Connection\"\n\tcase DuplicateClientID:\n\t\treturn \"Duplicate Client ID\"\n\tcase DuplicateServerName:\n\t\treturn \"Duplicate Server Name\"\n\tcase MinimumVersionRequired:\n\t\treturn \"Minimum Version Required\"\n\tcase ClusterNamesIdentical:\n\t\treturn \"Cluster Names Identical\"\n\tcase Kicked:\n\t\treturn \"Kicked\"\n\t}\n\n\treturn \"Unknown State\"\n}\n\n// AccountzOptions are options passed to Accountz\ntype AccountzOptions struct {\n\t// Account indicates that Accountz will return details for the account\n\tAccount string `json:\"account\"`\n}\n\nfunc newExtServiceLatency(l *serviceLatency) *jwt.ServiceLatency {\n\tif l == nil {\n\t\treturn nil\n\t}\n\treturn &jwt.ServiceLatency{\n\t\tSampling: jwt.SamplingRate(l.sampling),\n\t\tResults:  jwt.Subject(l.subject),\n\t}\n}\n\ntype ExtImport struct {\n\tjwt.Import\n\tInvalid     bool                `json:\"invalid\"`\n\tShare       bool                `json:\"share\"`\n\tTracking    bool                `json:\"tracking\"`\n\tTrackingHdr http.Header         `json:\"tracking_header,omitempty\"`\n\tLatency     *jwt.ServiceLatency `json:\"latency,omitempty\"`\n\tM1          *ServiceLatency     `json:\"m1,omitempty\"`\n}\n\ntype ExtExport struct {\n\tjwt.Export\n\tApprovedAccounts []string             `json:\"approved_accounts,omitempty\"`\n\tRevokedAct       map[string]time.Time `json:\"revoked_activations,omitempty\"`\n}\n\ntype ExtVrIssues struct {\n\tDescription string `json:\"description\"`\n\tBlocking    bool   `json:\"blocking\"`\n\tTime        bool   `json:\"time_check\"`\n}\n\ntype ExtMap map[string][]*MapDest\n\ntype AccountInfo struct {\n\tAccountName string               `json:\"account_name\"`\n\tLastUpdate  time.Time            `json:\"update_time,omitempty\"`\n\tIsSystem    bool                 `json:\"is_system,omitempty\"`\n\tExpired     bool                 `json:\"expired\"`\n\tComplete    bool                 `json:\"complete\"`\n\tJetStream   bool                 `json:\"jetstream_enabled\"`\n\tLeafCnt     int                  `json:\"leafnode_connections\"`\n\tClientCnt   int                  `json:\"client_connections\"`\n\tSubCnt      uint32               `json:\"subscriptions\"`\n\tMappings    ExtMap               `json:\"mappings,omitempty\"`\n\tExports     []ExtExport          `json:\"exports,omitempty\"`\n\tImports     []ExtImport          `json:\"imports,omitempty\"`\n\tJwt         string               `json:\"jwt,omitempty\"`\n\tIssuerKey   string               `json:\"issuer_key,omitempty\"`\n\tNameTag     string               `json:\"name_tag,omitempty\"`\n\tTags        jwt.TagList          `json:\"tags,omitempty\"`\n\tClaim       *jwt.AccountClaims   `json:\"decoded_jwt,omitempty\"`\n\tVr          []ExtVrIssues        `json:\"validation_result_jwt,omitempty\"`\n\tRevokedUser map[string]time.Time `json:\"revoked_user,omitempty\"`\n\tSublist     *SublistStats        `json:\"sublist_stats,omitempty\"`\n\tResponses   map[string]ExtImport `json:\"responses,omitempty\"`\n}\n\ntype Accountz struct {\n\tID            string       `json:\"server_id\"`\n\tNow           time.Time    `json:\"now\"`\n\tSystemAccount string       `json:\"system_account,omitempty\"`\n\tAccounts      []string     `json:\"accounts,omitempty\"`\n\tAccount       *AccountInfo `json:\"account_detail,omitempty\"`\n}\n\n// HandleAccountz process HTTP requests for account information.\nfunc (s *Server) HandleAccountz(w http.ResponseWriter, r *http.Request) {\n\ts.mu.Lock()\n\ts.httpReqStats[AccountzPath]++\n\ts.mu.Unlock()\n\tif l, err := s.Accountz(&AccountzOptions{r.URL.Query().Get(\"acc\")}); err != nil {\n\t\tw.WriteHeader(http.StatusBadRequest)\n\t\tw.Write([]byte(err.Error()))\n\t} else if b, err := json.MarshalIndent(l, \"\", \"  \"); err != nil {\n\t\ts.Errorf(\"Error marshaling response to %s request: %v\", AccountzPath, err)\n\t\tw.WriteHeader(http.StatusBadRequest)\n\t\tw.Write([]byte(err.Error()))\n\t} else {\n\t\tResponseHandler(w, r, b) // Handle response\n\t}\n}\n\nfunc (s *Server) Accountz(optz *AccountzOptions) (*Accountz, error) {\n\ta := &Accountz{\n\t\tID:  s.ID(),\n\t\tNow: time.Now().UTC(),\n\t}\n\tif sacc := s.SystemAccount(); sacc != nil {\n\t\ta.SystemAccount = sacc.GetName()\n\t}\n\tif optz == nil || optz.Account == _EMPTY_ {\n\t\ta.Accounts = []string{}\n\t\ts.accounts.Range(func(key, value any) bool {\n\t\t\ta.Accounts = append(a.Accounts, key.(string))\n\t\t\treturn true\n\t\t})\n\t\treturn a, nil\n\t}\n\taInfo, err := s.accountInfo(optz.Account)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\ta.Account = aInfo\n\treturn a, nil\n}\n\nfunc newExtImport(v *serviceImport) ExtImport {\n\timp := ExtImport{\n\t\tInvalid: true,\n\t\tImport:  jwt.Import{Type: jwt.Service},\n\t}\n\tif v != nil {\n\t\timp.Share = v.share\n\t\timp.Tracking = v.tracking\n\t\timp.Invalid = v.invalid\n\t\timp.Import = jwt.Import{\n\t\t\tSubject: jwt.Subject(v.to),\n\t\t\tAccount: v.acc.Name,\n\t\t\tType:    jwt.Service,\n\t\t\t// Deprecated so we duplicate. Use LocalSubject.\n\t\t\tTo:           jwt.Subject(v.from),\n\t\t\tLocalSubject: jwt.RenamingSubject(v.from),\n\t\t}\n\t\timp.TrackingHdr = v.trackingHdr\n\t\timp.Latency = newExtServiceLatency(v.latency)\n\t\tif v.m1 != nil {\n\t\t\tm1 := *v.m1\n\t\t\timp.M1 = &m1\n\t\t}\n\t}\n\treturn imp\n}\n\nfunc (s *Server) accountInfo(accName string) (*AccountInfo, error) {\n\tvar a *Account\n\tif v, ok := s.accounts.Load(accName); !ok {\n\t\treturn nil, fmt.Errorf(\"Account %s does not exist\", accName)\n\t} else {\n\t\ta = v.(*Account)\n\t}\n\tisSys := a == s.SystemAccount()\n\ta.mu.RLock()\n\tdefer a.mu.RUnlock()\n\tvar vrIssues []ExtVrIssues\n\tclaim, _ := jwt.DecodeAccountClaims(a.claimJWT) // ignore error\n\tif claim != nil {\n\t\tvr := jwt.ValidationResults{}\n\t\tclaim.Validate(&vr)\n\t\tvrIssues = make([]ExtVrIssues, len(vr.Issues))\n\t\tfor i, v := range vr.Issues {\n\t\t\tvrIssues[i] = ExtVrIssues{v.Description, v.Blocking, v.TimeCheck}\n\t\t}\n\t}\n\tcollectRevocations := func(revocations map[string]int64) map[string]time.Time {\n\t\tl := len(revocations)\n\t\tif l == 0 {\n\t\t\treturn nil\n\t\t}\n\t\trev := make(map[string]time.Time, l)\n\t\tfor k, v := range revocations {\n\t\t\trev[k] = time.Unix(v, 0)\n\t\t}\n\t\treturn rev\n\t}\n\texports := []ExtExport{}\n\tfor k, v := range a.exports.services {\n\t\te := ExtExport{\n\t\t\tExport: jwt.Export{\n\t\t\t\tSubject: jwt.Subject(k),\n\t\t\t\tType:    jwt.Service,\n\t\t\t},\n\t\t\tApprovedAccounts: []string{},\n\t\t}\n\t\tif v != nil {\n\t\t\te.Latency = newExtServiceLatency(v.latency)\n\t\t\te.TokenReq = v.tokenReq\n\t\t\te.ResponseType = jwt.ResponseType(v.respType.String())\n\t\t\tfor name := range v.approved {\n\t\t\t\te.ApprovedAccounts = append(e.ApprovedAccounts, name)\n\t\t\t}\n\t\t\te.RevokedAct = collectRevocations(v.actsRevoked)\n\t\t}\n\t\texports = append(exports, e)\n\t}\n\tfor k, v := range a.exports.streams {\n\t\te := ExtExport{\n\t\t\tExport: jwt.Export{\n\t\t\t\tSubject: jwt.Subject(k),\n\t\t\t\tType:    jwt.Stream,\n\t\t\t},\n\t\t\tApprovedAccounts: []string{},\n\t\t}\n\t\tif v != nil {\n\t\t\te.TokenReq = v.tokenReq\n\t\t\tfor name := range v.approved {\n\t\t\t\te.ApprovedAccounts = append(e.ApprovedAccounts, name)\n\t\t\t}\n\t\t\te.RevokedAct = collectRevocations(v.actsRevoked)\n\t\t}\n\t\texports = append(exports, e)\n\t}\n\timports := []ExtImport{}\n\tfor _, v := range a.imports.streams {\n\t\timp := ExtImport{\n\t\t\tInvalid: true,\n\t\t\tImport:  jwt.Import{Type: jwt.Stream},\n\t\t}\n\t\tif v != nil {\n\t\t\timp.Invalid = v.invalid\n\t\t\timp.Import = jwt.Import{\n\t\t\t\tSubject:      jwt.Subject(v.from),\n\t\t\t\tAccount:      v.acc.Name,\n\t\t\t\tType:         jwt.Stream,\n\t\t\t\tLocalSubject: jwt.RenamingSubject(v.to),\n\t\t\t}\n\t\t}\n\t\timports = append(imports, imp)\n\t}\n\tfor _, sis := range a.imports.services {\n\t\tfor _, v := range sis {\n\t\t\timports = append(imports, newExtImport(v))\n\t\t}\n\t}\n\tresponses := map[string]ExtImport{}\n\tfor k, v := range a.exports.responses {\n\t\tresponses[k] = newExtImport(v)\n\t}\n\tmappings := ExtMap{}\n\tfor _, m := range a.mappings {\n\t\tvar dests []*MapDest\n\t\tvar src string\n\t\tif m == nil {\n\t\t\tsrc = \"nil\"\n\t\t\tif _, ok := mappings[src]; ok { // only set if not present (keep orig in case nil is used)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tdests = append(dests, &MapDest{})\n\t\t} else {\n\t\t\tsrc = m.src\n\t\t\tfor _, d := range m.dests {\n\t\t\t\tdests = append(dests, &MapDest{d.tr.dest, d.weight, _EMPTY_})\n\t\t\t}\n\t\t\tfor c, cd := range m.cdests {\n\t\t\t\tfor _, d := range cd {\n\t\t\t\t\tdests = append(dests, &MapDest{d.tr.dest, d.weight, c})\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tmappings[src] = dests\n\t}\n\treturn &AccountInfo{\n\t\tAccountName: accName,\n\t\tLastUpdate:  a.updated.UTC(),\n\t\tIsSystem:    isSys,\n\t\tExpired:     a.expired.Load(),\n\t\tComplete:    !a.incomplete,\n\t\tJetStream:   a.js != nil,\n\t\tLeafCnt:     a.numLocalLeafNodes(),\n\t\tClientCnt:   a.numLocalConnections(),\n\t\tSubCnt:      a.sl.Count(),\n\t\tMappings:    mappings,\n\t\tExports:     exports,\n\t\tImports:     imports,\n\t\tJwt:         a.claimJWT,\n\t\tIssuerKey:   a.Issuer,\n\t\tNameTag:     a.getNameTagLocked(),\n\t\tTags:        a.tags,\n\t\tClaim:       claim,\n\t\tVr:          vrIssues,\n\t\tRevokedUser: collectRevocations(a.usersRevoked),\n\t\tSublist:     a.sl.Stats(),\n\t\tResponses:   responses,\n\t}, nil\n}\n\n// JSzOptions are options passed to Jsz\ntype JSzOptions struct {\n\tAccount          string `json:\"account,omitempty\"`\n\tAccounts         bool   `json:\"accounts,omitempty\"`\n\tStreams          bool   `json:\"streams,omitempty\"`\n\tConsumer         bool   `json:\"consumer,omitempty\"`\n\tConfig           bool   `json:\"config,omitempty\"`\n\tLeaderOnly       bool   `json:\"leader_only,omitempty\"`\n\tOffset           int    `json:\"offset,omitempty\"`\n\tLimit            int    `json:\"limit,omitempty\"`\n\tRaftGroups       bool   `json:\"raft,omitempty\"`\n\tStreamLeaderOnly bool   `json:\"stream_leader_only,omitempty\"`\n}\n\n// HealthzOptions are options passed to Healthz\ntype HealthzOptions struct {\n\t// Deprecated: Use JSEnabledOnly instead\n\tJSEnabled     bool   `json:\"js-enabled,omitempty\"`\n\tJSEnabledOnly bool   `json:\"js-enabled-only,omitempty\"`\n\tJSServerOnly  bool   `json:\"js-server-only,omitempty\"`\n\tJSMetaOnly    bool   `json:\"js-meta-only,omitempty\"`\n\tAccount       string `json:\"account,omitempty\"`\n\tStream        string `json:\"stream,omitempty\"`\n\tConsumer      string `json:\"consumer,omitempty\"`\n\tDetails       bool   `json:\"details,omitempty\"`\n}\n\n// ProfilezOptions are options passed to Profilez\ntype ProfilezOptions struct {\n\tName     string        `json:\"name\"`\n\tDebug    int           `json:\"debug\"`\n\tDuration time.Duration `json:\"duration,omitempty\"`\n}\n\n// IpqueueszOptions are options passed to Ipqueuesz\ntype IpqueueszOptions struct {\n\tAll    bool   `json:\"all\"`\n\tFilter string `json:\"filter\"`\n}\n\n// RaftzOptions are options passed to Raftz\ntype RaftzOptions struct {\n\tAccountFilter string `json:\"account\"`\n\tGroupFilter   string `json:\"group\"`\n}\n\n// StreamDetail shows information about the stream state and its consumers.\ntype StreamDetail struct {\n\tName               string              `json:\"name\"`\n\tCreated            time.Time           `json:\"created\"`\n\tCluster            *ClusterInfo        `json:\"cluster,omitempty\"`\n\tConfig             *StreamConfig       `json:\"config,omitempty\"`\n\tState              StreamState         `json:\"state,omitempty\"`\n\tConsumer           []*ConsumerInfo     `json:\"consumer_detail,omitempty\"`\n\tMirror             *StreamSourceInfo   `json:\"mirror,omitempty\"`\n\tSources            []*StreamSourceInfo `json:\"sources,omitempty\"`\n\tRaftGroup          string              `json:\"stream_raft_group,omitempty\"`\n\tConsumerRaftGroups []*RaftGroupDetail  `json:\"consumer_raft_groups,omitempty\"`\n}\n\n// RaftGroupDetail shows information details about the Raft group.\ntype RaftGroupDetail struct {\n\tName      string `json:\"name\"`\n\tRaftGroup string `json:\"raft_group,omitempty\"`\n}\n\ntype AccountDetail struct {\n\tName string `json:\"name\"`\n\tId   string `json:\"id\"`\n\tJetStreamStats\n\tStreams []StreamDetail `json:\"stream_detail,omitempty\"`\n}\n\n// MetaClusterInfo shows information about the meta group.\ntype MetaClusterInfo struct {\n\tName     string      `json:\"name,omitempty\"`\n\tLeader   string      `json:\"leader,omitempty\"`\n\tPeer     string      `json:\"peer,omitempty\"`\n\tReplicas []*PeerInfo `json:\"replicas,omitempty\"`\n\tSize     int         `json:\"cluster_size\"`\n\tPending  int         `json:\"pending\"`\n}\n\n// JSInfo has detailed information on JetStream.\ntype JSInfo struct {\n\tJetStreamStats\n\tID             string           `json:\"server_id\"`\n\tNow            time.Time        `json:\"now\"`\n\tDisabled       bool             `json:\"disabled,omitempty\"`\n\tConfig         JetStreamConfig  `json:\"config,omitempty\"`\n\tLimits         *JSLimitOpts     `json:\"limits,omitempty\"`\n\tStreams        int              `json:\"streams\"`\n\tConsumers      int              `json:\"consumers\"`\n\tMessages       uint64           `json:\"messages\"`\n\tBytes          uint64           `json:\"bytes\"`\n\tMeta           *MetaClusterInfo `json:\"meta_cluster,omitempty\"`\n\tAccountDetails []*AccountDetail `json:\"account_details,omitempty\"`\n\tTotal          int              `json:\"total\"`\n}\n\nfunc (s *Server) accountDetail(jsa *jsAccount, optStreams, optConsumers, optCfg, optRaft, optStreamLeader bool) *AccountDetail {\n\tjsa.mu.RLock()\n\tacc := jsa.account\n\tname := acc.GetName()\n\tid := name\n\tif acc.nameTag != _EMPTY_ {\n\t\tname = acc.nameTag\n\t}\n\tjsa.usageMu.RLock()\n\ttotalMem, totalStore := jsa.storageTotals()\n\tdetail := AccountDetail{\n\t\tName: name,\n\t\tId:   id,\n\t\tJetStreamStats: JetStreamStats{\n\t\t\tMemory: totalMem,\n\t\t\tStore:  totalStore,\n\t\t\tAPI: JetStreamAPIStats{\n\t\t\t\tTotal:  jsa.apiTotal,\n\t\t\t\tErrors: jsa.apiErrors,\n\t\t\t},\n\t\t},\n\t\tStreams: make([]StreamDetail, 0, len(jsa.streams)),\n\t}\n\tif reserved, ok := jsa.limits[_EMPTY_]; ok {\n\t\tdetail.JetStreamStats.ReservedMemory = uint64(reserved.MaxMemory)\n\t\tdetail.JetStreamStats.ReservedStore = uint64(reserved.MaxStore)\n\t}\n\tjsa.usageMu.RUnlock()\n\n\tvar streams []*stream\n\tif optStreams {\n\t\tfor _, stream := range jsa.streams {\n\t\t\tstreams = append(streams, stream)\n\t\t}\n\t}\n\tjsa.mu.RUnlock()\n\n\tif js := s.getJetStream(); js != nil && optStreams {\n\t\tfor _, stream := range streams {\n\t\t\trgroup := stream.raftGroup()\n\t\t\tci := js.clusterInfo(rgroup)\n\t\t\tvar cfg *StreamConfig\n\t\t\tif optCfg {\n\t\t\t\tc := stream.config()\n\t\t\t\tcfg = &c\n\t\t\t}\n\t\t\t// Skip if we are only looking for stream leaders.\n\t\t\tif optStreamLeader && ci != nil && ci.Leader != s.Name() {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tsdet := StreamDetail{\n\t\t\t\tName:    stream.name(),\n\t\t\t\tCreated: stream.createdTime(),\n\t\t\t\tState:   stream.state(),\n\t\t\t\tCluster: ci,\n\t\t\t\tConfig:  cfg,\n\t\t\t\tMirror:  stream.mirrorInfo(),\n\t\t\t\tSources: stream.sourcesInfo(),\n\t\t\t}\n\t\t\tif optRaft && rgroup != nil {\n\t\t\t\tsdet.RaftGroup = rgroup.Name\n\t\t\t\tsdet.ConsumerRaftGroups = make([]*RaftGroupDetail, 0)\n\t\t\t}\n\t\t\tif optConsumers {\n\t\t\t\tfor _, consumer := range stream.getPublicConsumers() {\n\t\t\t\t\tcInfo := consumer.info()\n\t\t\t\t\tif cInfo == nil {\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\t\t\t\t\tif !optCfg {\n\t\t\t\t\t\tcInfo.Config = nil\n\t\t\t\t\t}\n\t\t\t\t\tsdet.Consumer = append(sdet.Consumer, cInfo)\n\t\t\t\t\tif optRaft {\n\t\t\t\t\t\tcrgroup := consumer.raftGroup()\n\t\t\t\t\t\tif crgroup != nil {\n\t\t\t\t\t\t\tsdet.ConsumerRaftGroups = append(sdet.ConsumerRaftGroups,\n\t\t\t\t\t\t\t\t&RaftGroupDetail{cInfo.Name, crgroup.Name},\n\t\t\t\t\t\t\t)\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tdetail.Streams = append(detail.Streams, sdet)\n\t\t}\n\t}\n\treturn &detail\n}\n\nfunc (s *Server) JszAccount(opts *JSzOptions) (*AccountDetail, error) {\n\tjs := s.getJetStream()\n\tif js == nil {\n\t\treturn nil, fmt.Errorf(\"jetstream not enabled\")\n\t}\n\tacc := opts.Account\n\taccount, ok := s.accounts.Load(acc)\n\tif !ok {\n\t\treturn nil, fmt.Errorf(\"account %q not found\", acc)\n\t}\n\tjs.mu.RLock()\n\tjsa, ok := js.accounts[account.(*Account).Name]\n\tjs.mu.RUnlock()\n\tif !ok {\n\t\treturn nil, fmt.Errorf(\"account %q not jetstream enabled\", acc)\n\t}\n\treturn s.accountDetail(jsa, opts.Streams, opts.Consumer, opts.Config, opts.RaftGroups, opts.StreamLeaderOnly), nil\n}\n\n// helper to get cluster info from node via dummy group\nfunc (s *Server) raftNodeToClusterInfo(node RaftNode) *ClusterInfo {\n\tif node == nil {\n\t\treturn nil\n\t}\n\tpeers := node.Peers()\n\tpeerList := make([]string, len(peers))\n\tfor i, p := range peers {\n\t\tpeerList[i] = p.ID\n\t}\n\tgroup := &raftGroup{\n\t\tName:  _EMPTY_,\n\t\tPeers: peerList,\n\t\tnode:  node,\n\t}\n\treturn s.getJetStream().clusterInfo(group)\n}\n\n// Jsz returns a Jsz structure containing information about JetStream.\nfunc (s *Server) Jsz(opts *JSzOptions) (*JSInfo, error) {\n\t// set option defaults\n\tif opts == nil {\n\t\topts = &JSzOptions{}\n\t}\n\tif opts.Offset < 0 {\n\t\topts.Offset = 0\n\t}\n\tif opts.Limit == 0 {\n\t\topts.Limit = 1024\n\t}\n\tif opts.Consumer {\n\t\topts.Streams = true\n\t}\n\tif opts.Streams && opts.Account == _EMPTY_ {\n\t\topts.Accounts = true\n\t}\n\n\tjsi := &JSInfo{\n\t\tID:  s.ID(),\n\t\tNow: time.Now().UTC(),\n\t}\n\n\tjs := s.getJetStream()\n\tif js == nil || !js.isEnabled() {\n\t\tif opts.LeaderOnly {\n\t\t\treturn nil, fmt.Errorf(\"%w: not leader\", errSkipZreq)\n\t\t}\n\n\t\tjsi.Disabled = true\n\t\treturn jsi, nil\n\t}\n\n\tjsi.Limits = &s.getOpts().JetStreamLimits\n\n\tjs.mu.RLock()\n\tisLeader := js.cluster == nil || js.cluster.isLeader()\n\tjs.mu.RUnlock()\n\n\tif opts.LeaderOnly && !isLeader {\n\t\treturn nil, fmt.Errorf(\"%w: not leader\", errSkipZreq)\n\t}\n\n\tvar accounts []*jsAccount\n\n\tjs.mu.RLock()\n\tjsi.Config = js.config\n\tfor _, info := range js.accounts {\n\t\taccounts = append(accounts, info)\n\t}\n\tjs.mu.RUnlock()\n\n\tjsi.Total = len(accounts)\n\n\tif mg := js.getMetaGroup(); mg != nil {\n\t\tif ci := s.raftNodeToClusterInfo(mg); ci != nil {\n\t\t\tjsi.Meta = &MetaClusterInfo{Name: ci.Name, Leader: ci.Leader, Peer: getHash(ci.Leader), Size: mg.ClusterSize()}\n\t\t\tif isLeader {\n\t\t\t\tjsi.Meta.Replicas = ci.Replicas\n\t\t\t}\n\t\t\tif ipq := s.jsAPIRoutedReqs; ipq != nil {\n\t\t\t\tjsi.Meta.Pending = ipq.len()\n\t\t\t}\n\t\t}\n\t}\n\n\tjsi.JetStreamStats = *js.usageStats()\n\n\t// If a specific account is requested, track the index.\n\tfilterIdx := -1\n\n\t// Calculate the stats of all accounts and streams regardless of the filtering.\n\tfor i, jsa := range accounts {\n\t\tif jsa.acc().GetName() == opts.Account {\n\t\t\tfilterIdx = i\n\t\t}\n\n\t\tjsa.mu.RLock()\n\t\tstreams := make([]*stream, 0, len(jsa.streams))\n\t\tfor _, stream := range jsa.streams {\n\t\t\tstreams = append(streams, stream)\n\t\t}\n\t\tjsa.mu.RUnlock()\n\n\t\tjsi.Streams += len(streams)\n\t\tfor _, stream := range streams {\n\t\t\tstreamState := stream.state()\n\t\t\tjsi.Messages += streamState.Msgs\n\t\t\tjsi.Bytes += streamState.Bytes\n\t\t\tjsi.Consumers += streamState.Consumers\n\t\t}\n\t}\n\n\t// Targeted account takes precedence.\n\tif filterIdx >= 0 {\n\t\taccounts = accounts[filterIdx : filterIdx+1]\n\t} else if opts.Accounts {\n\n\t\tif opts.Limit > 0 {\n\t\t\t// Sort by name for a consistent read (barring any concurrent changes)\n\t\t\tslices.SortFunc(accounts, func(i, j *jsAccount) int { return cmp.Compare(i.acc().Name, j.acc().Name) })\n\n\t\t\t// Offset larger than the number of accounts.\n\t\t\toffset := min(opts.Offset, len(accounts))\n\t\t\taccounts = accounts[offset:]\n\n\t\t\tlimit := min(opts.Limit, len(accounts))\n\t\t\taccounts = accounts[:limit]\n\t\t}\n\t} else {\n\t\taccounts = nil\n\t}\n\n\tif len(accounts) > 0 {\n\t\tjsi.AccountDetails = make([]*AccountDetail, 0, len(accounts))\n\n\t\tfor _, jsa := range accounts {\n\t\t\tdetail := s.accountDetail(jsa, opts.Streams, opts.Consumer, opts.Config, opts.RaftGroups, opts.StreamLeaderOnly)\n\t\t\tjsi.AccountDetails = append(jsi.AccountDetails, detail)\n\t\t}\n\t}\n\n\treturn jsi, nil\n}\n\n// HandleJsz process HTTP requests for jetstream information.\nfunc (s *Server) HandleJsz(w http.ResponseWriter, r *http.Request) {\n\ts.mu.Lock()\n\ts.httpReqStats[JszPath]++\n\ts.mu.Unlock()\n\taccounts, err := decodeBool(w, r, \"accounts\")\n\tif err != nil {\n\t\treturn\n\t}\n\tstreams, err := decodeBool(w, r, \"streams\")\n\tif err != nil {\n\t\treturn\n\t}\n\tconsumers, err := decodeBool(w, r, \"consumers\")\n\tif err != nil {\n\t\treturn\n\t}\n\tconfig, err := decodeBool(w, r, \"config\")\n\tif err != nil {\n\t\treturn\n\t}\n\toffset, err := decodeInt(w, r, \"offset\")\n\tif err != nil {\n\t\treturn\n\t}\n\tlimit, err := decodeInt(w, r, \"limit\")\n\tif err != nil {\n\t\treturn\n\t}\n\tleader, err := decodeBool(w, r, \"leader-only\")\n\tif err != nil {\n\t\treturn\n\t}\n\trgroups, err := decodeBool(w, r, \"raft\")\n\tif err != nil {\n\t\treturn\n\t}\n\n\tsleader, err := decodeBool(w, r, \"stream-leader-only\")\n\tif err != nil {\n\t\treturn\n\t}\n\n\tl, err := s.Jsz(&JSzOptions{\n\t\tAccount:          r.URL.Query().Get(\"acc\"),\n\t\tAccounts:         accounts,\n\t\tStreams:          streams,\n\t\tConsumer:         consumers,\n\t\tConfig:           config,\n\t\tLeaderOnly:       leader,\n\t\tOffset:           offset,\n\t\tLimit:            limit,\n\t\tRaftGroups:       rgroups,\n\t\tStreamLeaderOnly: sleader,\n\t})\n\tif err != nil {\n\t\tw.WriteHeader(http.StatusBadRequest)\n\t\tw.Write([]byte(err.Error()))\n\t\treturn\n\t}\n\tb, err := json.MarshalIndent(l, \"\", \"  \")\n\tif err != nil {\n\t\ts.Errorf(\"Error marshaling response to /jsz request: %v\", err)\n\t}\n\n\t// Handle response\n\tResponseHandler(w, r, b)\n}\n\ntype HealthStatus struct {\n\tStatus     string         `json:\"status\"`\n\tStatusCode int            `json:\"status_code,omitempty\"`\n\tError      string         `json:\"error,omitempty\"`\n\tErrors     []HealthzError `json:\"errors,omitempty\"`\n}\n\ntype HealthzError struct {\n\tType     HealthZErrorType `json:\"type\"`\n\tAccount  string           `json:\"account,omitempty\"`\n\tStream   string           `json:\"stream,omitempty\"`\n\tConsumer string           `json:\"consumer,omitempty\"`\n\tError    string           `json:\"error,omitempty\"`\n}\n\ntype HealthZErrorType int\n\nconst (\n\tHealthzErrorConn HealthZErrorType = iota\n\tHealthzErrorBadRequest\n\tHealthzErrorJetStream\n\tHealthzErrorAccount\n\tHealthzErrorStream\n\tHealthzErrorConsumer\n)\n\nfunc (t HealthZErrorType) String() string {\n\tswitch t {\n\tcase HealthzErrorConn:\n\t\treturn \"CONNECTION\"\n\tcase HealthzErrorBadRequest:\n\t\treturn \"BAD_REQUEST\"\n\tcase HealthzErrorJetStream:\n\t\treturn \"JETSTREAM\"\n\tcase HealthzErrorAccount:\n\t\treturn \"ACCOUNT\"\n\tcase HealthzErrorStream:\n\t\treturn \"STREAM\"\n\tcase HealthzErrorConsumer:\n\t\treturn \"CONSUMER\"\n\tdefault:\n\t\treturn \"unknown\"\n\t}\n}\n\nfunc (t HealthZErrorType) MarshalJSON() ([]byte, error) {\n\treturn json.Marshal(t.String())\n}\n\nfunc (t *HealthZErrorType) UnmarshalJSON(data []byte) error {\n\tswitch string(data) {\n\tcase `\"CONNECTION\"`:\n\t\t*t = HealthzErrorConn\n\tcase `\"BAD_REQUEST\"`:\n\t\t*t = HealthzErrorBadRequest\n\tcase `\"JETSTREAM\"`:\n\t\t*t = HealthzErrorJetStream\n\tcase `\"ACCOUNT\"`:\n\t\t*t = HealthzErrorAccount\n\tcase `\"STREAM\"`:\n\t\t*t = HealthzErrorStream\n\tcase `\"CONSUMER\"`:\n\t\t*t = HealthzErrorConsumer\n\tdefault:\n\t\treturn fmt.Errorf(\"unknown healthz error type %q\", data)\n\t}\n\treturn nil\n}\n\n// https://datatracker.ietf.org/doc/html/draft-inadarei-api-health-check\nfunc (s *Server) HandleHealthz(w http.ResponseWriter, r *http.Request) {\n\ts.mu.Lock()\n\ts.httpReqStats[HealthzPath]++\n\ts.mu.Unlock()\n\n\tjsEnabled, err := decodeBool(w, r, \"js-enabled\")\n\tif err != nil {\n\t\treturn\n\t}\n\tif jsEnabled {\n\t\ts.Warnf(\"Healthcheck: js-enabled deprecated, use js-enabled-only instead\")\n\t}\n\tjsEnabledOnly, err := decodeBool(w, r, \"js-enabled-only\")\n\tif err != nil {\n\t\treturn\n\t}\n\tjsServerOnly, err := decodeBool(w, r, \"js-server-only\")\n\tif err != nil {\n\t\treturn\n\t}\n\tjsMetaOnly, err := decodeBool(w, r, \"js-meta-only\")\n\tif err != nil {\n\t\treturn\n\t}\n\n\tincludeDetails, err := decodeBool(w, r, \"details\")\n\tif err != nil {\n\t\treturn\n\t}\n\n\ths := s.healthz(&HealthzOptions{\n\t\tJSEnabled:     jsEnabled,\n\t\tJSEnabledOnly: jsEnabledOnly,\n\t\tJSServerOnly:  jsServerOnly,\n\t\tJSMetaOnly:    jsMetaOnly,\n\t\tAccount:       r.URL.Query().Get(\"account\"),\n\t\tStream:        r.URL.Query().Get(\"stream\"),\n\t\tConsumer:      r.URL.Query().Get(\"consumer\"),\n\t\tDetails:       includeDetails,\n\t})\n\n\tcode := hs.StatusCode\n\tif hs.Error != _EMPTY_ {\n\t\ts.Warnf(\"Healthcheck failed: %q\", hs.Error)\n\t} else if len(hs.Errors) != 0 {\n\t\ts.Warnf(\"Healthcheck failed: %d errors\", len(hs.Errors))\n\t}\n\t// Remove StatusCode from JSON representation when responding via HTTP\n\t// since this is already in the response.\n\ths.StatusCode = 0\n\tb, err := json.Marshal(hs)\n\tif err != nil {\n\t\ts.Errorf(\"Error marshaling response to /healthz request: %v\", err)\n\t}\n\n\thandleResponse(code, w, r, b)\n}\n\n// Generate health status.\nfunc (s *Server) healthz(opts *HealthzOptions) *HealthStatus {\n\tvar health = &HealthStatus{Status: \"ok\"}\n\n\t// set option defaults\n\tif opts == nil {\n\t\topts = &HealthzOptions{}\n\t}\n\tdetails := opts.Details\n\tdefer func() {\n\t\t// for response with details enabled, set status to either \"error\" or \"ok\"\n\t\tif details {\n\t\t\tif len(health.Errors) != 0 {\n\t\t\t\thealth.Status = \"error\"\n\t\t\t} else {\n\t\t\t\thealth.Status = \"ok\"\n\t\t\t}\n\t\t}\n\t\t// if no specific status code was set, set it based on the presence of errors\n\t\tif health.StatusCode == 0 {\n\t\t\tif health.Error != _EMPTY_ || len(health.Errors) != 0 {\n\t\t\t\thealth.StatusCode = http.StatusServiceUnavailable\n\t\t\t} else {\n\t\t\t\thealth.StatusCode = http.StatusOK\n\t\t\t}\n\t\t}\n\t}()\n\n\tif opts.Account == _EMPTY_ && opts.Stream != _EMPTY_ {\n\t\thealth.StatusCode = http.StatusBadRequest\n\t\tif !details {\n\t\t\thealth.Status = \"error\"\n\t\t\thealth.Error = fmt.Sprintf(\"%q must not be empty when checking stream health\", \"account\")\n\t\t} else {\n\t\t\thealth.Errors = append(health.Errors, HealthzError{\n\t\t\t\tType:  HealthzErrorBadRequest,\n\t\t\t\tError: fmt.Sprintf(\"%q must not be empty when checking stream health\", \"account\"),\n\t\t\t})\n\t\t}\n\t\treturn health\n\t}\n\n\tif opts.Stream == _EMPTY_ && opts.Consumer != _EMPTY_ {\n\t\thealth.StatusCode = http.StatusBadRequest\n\t\tif !details {\n\t\t\thealth.Status = \"error\"\n\t\t\thealth.Error = fmt.Sprintf(\"%q must not be empty when checking consumer health\", \"stream\")\n\t\t} else {\n\t\t\thealth.Errors = append(health.Errors, HealthzError{\n\t\t\t\tType:  HealthzErrorBadRequest,\n\t\t\t\tError: fmt.Sprintf(\"%q must not be empty when checking consumer health\", \"stream\"),\n\t\t\t})\n\t\t}\n\t\treturn health\n\t}\n\n\tif err := s.readyForConnections(time.Millisecond); err != nil {\n\t\thealth.StatusCode = http.StatusInternalServerError\n\t\thealth.Status = \"error\"\n\t\tif !details {\n\t\t\thealth.Error = err.Error()\n\t\t} else {\n\t\t\thealth.Errors = append(health.Errors, HealthzError{\n\t\t\t\tType:  HealthzErrorConn,\n\t\t\t\tError: err.Error(),\n\t\t\t})\n\t\t}\n\t\treturn health\n\t}\n\n\t// If JSServerOnly is true, then do not check further accounts, streams and consumers.\n\tif opts.JSServerOnly {\n\t\treturn health\n\t}\n\n\tsopts := s.getOpts()\n\n\t// If JS is not enabled in the config, we stop.\n\tif !sopts.JetStream {\n\t\treturn health\n\t}\n\n\t// Access the Jetstream state to perform additional checks.\n\tjs := s.getJetStream()\n\tconst na = \"unavailable\"\n\tif !js.isEnabled() {\n\t\thealth.StatusCode = http.StatusServiceUnavailable\n\t\thealth.Status = na\n\t\tif !details {\n\t\t\thealth.Error = NewJSNotEnabledError().Error()\n\t\t} else {\n\t\t\thealth.Errors = append(health.Errors, HealthzError{\n\t\t\t\tType:  HealthzErrorJetStream,\n\t\t\t\tError: NewJSNotEnabledError().Error(),\n\t\t\t})\n\t\t}\n\t\treturn health\n\t}\n\t// Only check if JS is enabled, skip meta and asset check.\n\tif opts.JSEnabledOnly || opts.JSEnabled {\n\t\treturn health\n\t}\n\n\t// Clustered JetStream\n\tjs.mu.RLock()\n\tcc := js.cluster\n\tjs.mu.RUnlock()\n\n\t// Currently single server we make sure the streams were recovered.\n\tif cc == nil {\n\t\tsdir := js.config.StoreDir\n\t\t// Whip through account folders and pull each stream name.\n\t\tfis, _ := os.ReadDir(sdir)\n\t\tvar accFound, streamFound, consumerFound bool\n\t\tfor _, fi := range fis {\n\t\t\tif fi.Name() == snapStagingDir {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif opts.Account != _EMPTY_ {\n\t\t\t\tif fi.Name() != opts.Account {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\taccFound = true\n\t\t\t}\n\t\t\tacc, err := s.LookupAccount(fi.Name())\n\t\t\tif err != nil {\n\t\t\t\tif !details {\n\t\t\t\t\thealth.Status = na\n\t\t\t\t\thealth.Error = fmt.Sprintf(\"JetStream account '%s' could not be resolved\", fi.Name())\n\t\t\t\t\treturn health\n\t\t\t\t}\n\t\t\t\thealth.Errors = append(health.Errors, HealthzError{\n\t\t\t\t\tType:    HealthzErrorAccount,\n\t\t\t\t\tAccount: fi.Name(),\n\t\t\t\t\tError:   fmt.Sprintf(\"JetStream account '%s' could not be resolved\", fi.Name()),\n\t\t\t\t})\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tsfis, _ := os.ReadDir(filepath.Join(sdir, fi.Name(), \"streams\"))\n\t\t\tfor _, sfi := range sfis {\n\t\t\t\tif opts.Stream != _EMPTY_ {\n\t\t\t\t\tif sfi.Name() != opts.Stream {\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\t\t\t\t\tstreamFound = true\n\t\t\t\t}\n\t\t\t\tstream := sfi.Name()\n\t\t\t\ts, err := acc.lookupStream(stream)\n\t\t\t\tif err != nil {\n\t\t\t\t\tif !details {\n\t\t\t\t\t\thealth.Status = na\n\t\t\t\t\t\thealth.Error = fmt.Sprintf(\"JetStream stream '%s > %s' could not be recovered\", acc, stream)\n\t\t\t\t\t\treturn health\n\t\t\t\t\t}\n\t\t\t\t\thealth.Errors = append(health.Errors, HealthzError{\n\t\t\t\t\t\tType:    HealthzErrorStream,\n\t\t\t\t\t\tAccount: acc.Name,\n\t\t\t\t\t\tStream:  stream,\n\t\t\t\t\t\tError:   fmt.Sprintf(\"JetStream stream '%s > %s' could not be recovered\", acc, stream),\n\t\t\t\t\t})\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tif streamFound {\n\t\t\t\t\t// if consumer option is passed, verify that the consumer exists on stream\n\t\t\t\t\tif opts.Consumer != _EMPTY_ {\n\t\t\t\t\t\tfor _, cons := range s.consumers {\n\t\t\t\t\t\t\tif cons.name == opts.Consumer {\n\t\t\t\t\t\t\t\tconsumerFound = true\n\t\t\t\t\t\t\t\tbreak\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t\tif accFound {\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\tif opts.Account != _EMPTY_ && !accFound {\n\t\t\thealth.StatusCode = http.StatusNotFound\n\t\t\tif !details {\n\t\t\t\thealth.Status = na\n\t\t\t\thealth.Error = fmt.Sprintf(\"JetStream account %q not found\", opts.Account)\n\t\t\t} else {\n\t\t\t\thealth.Errors = []HealthzError{\n\t\t\t\t\t{\n\t\t\t\t\t\tType:    HealthzErrorAccount,\n\t\t\t\t\t\tAccount: opts.Account,\n\t\t\t\t\t\tError:   fmt.Sprintf(\"JetStream account %q not found\", opts.Account),\n\t\t\t\t\t},\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn health\n\t\t}\n\t\tif opts.Stream != _EMPTY_ && !streamFound {\n\t\t\thealth.StatusCode = http.StatusNotFound\n\t\t\tif !details {\n\t\t\t\thealth.Status = na\n\t\t\t\thealth.Error = fmt.Sprintf(\"JetStream stream %q not found on account %q\", opts.Stream, opts.Account)\n\t\t\t} else {\n\t\t\t\thealth.Errors = []HealthzError{\n\t\t\t\t\t{\n\t\t\t\t\t\tType:    HealthzErrorStream,\n\t\t\t\t\t\tAccount: opts.Account,\n\t\t\t\t\t\tStream:  opts.Stream,\n\t\t\t\t\t\tError:   fmt.Sprintf(\"JetStream stream %q not found on account %q\", opts.Stream, opts.Account),\n\t\t\t\t\t},\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn health\n\t\t}\n\t\tif opts.Consumer != _EMPTY_ && !consumerFound {\n\t\t\thealth.StatusCode = http.StatusNotFound\n\t\t\tif !details {\n\t\t\t\thealth.Status = na\n\t\t\t\thealth.Error = fmt.Sprintf(\"JetStream consumer %q not found for stream %q on account %q\", opts.Consumer, opts.Stream, opts.Account)\n\t\t\t} else {\n\t\t\t\thealth.Errors = []HealthzError{\n\t\t\t\t\t{\n\t\t\t\t\t\tType:     HealthzErrorConsumer,\n\t\t\t\t\t\tAccount:  opts.Account,\n\t\t\t\t\t\tStream:   opts.Stream,\n\t\t\t\t\t\tConsumer: opts.Consumer,\n\t\t\t\t\t\tError:    fmt.Sprintf(\"JetStream consumer %q not found for stream %q on account %q\", opts.Consumer, opts.Stream, opts.Account),\n\t\t\t\t\t},\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\treturn health\n\t}\n\n\t// If we are here we want to check for any assets assigned to us.\n\tvar meta RaftNode\n\tjs.mu.RLock()\n\tmeta = cc.meta\n\tjs.mu.RUnlock()\n\n\t// If no meta leader.\n\tif meta == nil || meta.GroupLeader() == _EMPTY_ {\n\t\tif !details {\n\t\t\thealth.Status = na\n\t\t\thealth.Error = \"JetStream has not established contact with a meta leader\"\n\t\t} else {\n\t\t\thealth.Errors = []HealthzError{\n\t\t\t\t{\n\t\t\t\t\tType:  HealthzErrorJetStream,\n\t\t\t\t\tError: \"JetStream has not established contact with a meta leader\",\n\t\t\t\t},\n\t\t\t}\n\t\t}\n\t\treturn health\n\t}\n\n\t// If we are not current with the meta leader.\n\tif !meta.Healthy() {\n\t\tif !details {\n\t\t\thealth.Status = na\n\t\t\thealth.Error = \"JetStream is not current with the meta leader\"\n\t\t} else {\n\t\t\thealth.Errors = []HealthzError{\n\t\t\t\t{\n\t\t\t\t\tType:  HealthzErrorJetStream,\n\t\t\t\t\tError: \"JetStream is not current with the meta leader\",\n\t\t\t\t},\n\t\t\t}\n\t\t}\n\t\treturn health\n\t}\n\n\t// Are we still recovering meta layer?\n\tif js.isMetaRecovering() {\n\t\tif !details {\n\t\t\thealth.Status = na\n\t\t\thealth.Error = \"JetStream is still recovering meta layer\"\n\n\t\t} else {\n\t\t\thealth.Errors = []HealthzError{\n\t\t\t\t{\n\t\t\t\t\tType:  HealthzErrorJetStream,\n\t\t\t\t\tError: \"JetStream is still recovering meta layer\",\n\t\t\t\t},\n\t\t\t}\n\t\t}\n\t\treturn health\n\t}\n\n\t// Skips doing full healthz and only checks the meta leader.\n\tif opts.JSMetaOnly {\n\t\treturn health\n\t}\n\n\t// Range across all accounts, the streams assigned to them, and the consumers.\n\t// If they are assigned to this server check their status.\n\tourID := meta.ID()\n\n\t// Copy the meta layer so we do not need to hold the js read lock for an extended period of time.\n\tvar streams map[string]map[string]*streamAssignment\n\tjs.mu.RLock()\n\tif opts.Account == _EMPTY_ {\n\t\t// Collect all relevant streams and consumers.\n\t\tstreams = make(map[string]map[string]*streamAssignment, len(cc.streams))\n\t\tfor acc, asa := range cc.streams {\n\t\t\tnasa := make(map[string]*streamAssignment)\n\t\t\tfor stream, sa := range asa {\n\t\t\t\t// If we are a member and we are not being restored, select for check.\n\t\t\t\tif sa.Group.isMember(ourID) && sa.Restore == nil {\n\t\t\t\t\tcsa := sa.copyGroup()\n\t\t\t\t\tcsa.consumers = make(map[string]*consumerAssignment)\n\t\t\t\t\tfor consumer, ca := range sa.consumers {\n\t\t\t\t\t\tif ca.Group.isMember(ourID) {\n\t\t\t\t\t\t\t// Use original here. Not a copy.\n\t\t\t\t\t\t\tcsa.consumers[consumer] = ca\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tnasa[stream] = csa\n\t\t\t\t}\n\t\t\t}\n\t\t\tstreams[acc] = nasa\n\t\t}\n\t} else {\n\t\tstreams = make(map[string]map[string]*streamAssignment, 1)\n\t\tasa, ok := cc.streams[opts.Account]\n\t\tif !ok {\n\t\t\thealth.StatusCode = http.StatusNotFound\n\t\t\tif !details {\n\t\t\t\thealth.Status = na\n\t\t\t\thealth.Error = fmt.Sprintf(\"JetStream account %q not found\", opts.Account)\n\t\t\t} else {\n\t\t\t\thealth.Errors = []HealthzError{\n\t\t\t\t\t{\n\t\t\t\t\t\tType:    HealthzErrorAccount,\n\t\t\t\t\t\tAccount: opts.Account,\n\t\t\t\t\t\tError:   fmt.Sprintf(\"JetStream account %q not found\", opts.Account),\n\t\t\t\t\t},\n\t\t\t\t}\n\t\t\t}\n\t\t\tjs.mu.RUnlock()\n\t\t\treturn health\n\t\t}\n\t\tnasa := make(map[string]*streamAssignment)\n\t\tif opts.Stream != _EMPTY_ {\n\t\t\tsa, ok := asa[opts.Stream]\n\t\t\tif !ok || !sa.Group.isMember(ourID) {\n\t\t\t\thealth.StatusCode = http.StatusNotFound\n\t\t\t\tif !details {\n\t\t\t\t\thealth.Status = na\n\t\t\t\t\thealth.Error = fmt.Sprintf(\"JetStream stream %q not found on account %q\", opts.Stream, opts.Account)\n\t\t\t\t} else {\n\t\t\t\t\thealth.Errors = []HealthzError{\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tType:    HealthzErrorStream,\n\t\t\t\t\t\t\tAccount: opts.Account,\n\t\t\t\t\t\t\tStream:  opts.Stream,\n\t\t\t\t\t\t\tError:   fmt.Sprintf(\"JetStream stream %q not found on account %q\", opts.Stream, opts.Account),\n\t\t\t\t\t\t},\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tjs.mu.RUnlock()\n\t\t\t\treturn health\n\t\t\t}\n\t\t\tcsa := sa.copyGroup()\n\t\t\tcsa.consumers = make(map[string]*consumerAssignment)\n\t\t\tvar consumerFound bool\n\t\t\tfor consumer, ca := range sa.consumers {\n\t\t\t\tif opts.Consumer != _EMPTY_ {\n\t\t\t\t\tif consumer != opts.Consumer || !ca.Group.isMember(ourID) {\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\t\t\t\t\tconsumerFound = true\n\t\t\t\t}\n\t\t\t\t// If we are a member and we are not being restored, select for check.\n\t\t\t\tif sa.Group.isMember(ourID) && sa.Restore == nil {\n\t\t\t\t\tcsa.consumers[consumer] = ca\n\t\t\t\t}\n\t\t\t\tif consumerFound {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t\tif opts.Consumer != _EMPTY_ && !consumerFound {\n\t\t\t\thealth.StatusCode = http.StatusNotFound\n\t\t\t\tif !details {\n\t\t\t\t\thealth.Status = na\n\t\t\t\t\thealth.Error = fmt.Sprintf(\"JetStream consumer %q not found for stream %q on account %q\", opts.Consumer, opts.Stream, opts.Account)\n\t\t\t\t} else {\n\t\t\t\t\thealth.Errors = []HealthzError{\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tType:     HealthzErrorConsumer,\n\t\t\t\t\t\t\tAccount:  opts.Account,\n\t\t\t\t\t\t\tStream:   opts.Stream,\n\t\t\t\t\t\t\tConsumer: opts.Consumer,\n\t\t\t\t\t\t\tError:    fmt.Sprintf(\"JetStream consumer %q not found for stream %q on account %q\", opts.Consumer, opts.Stream, opts.Account),\n\t\t\t\t\t\t},\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tjs.mu.RUnlock()\n\t\t\t\treturn health\n\t\t\t}\n\t\t\tnasa[opts.Stream] = csa\n\t\t} else {\n\t\t\tfor stream, sa := range asa {\n\t\t\t\t// If we are a member and we are not being restored, select for check.\n\t\t\t\tif sa.Group.isMember(ourID) && sa.Restore == nil {\n\t\t\t\t\tcsa := sa.copyGroup()\n\t\t\t\t\tcsa.consumers = make(map[string]*consumerAssignment)\n\t\t\t\t\tfor consumer, ca := range sa.consumers {\n\t\t\t\t\t\tif ca.Group.isMember(ourID) {\n\t\t\t\t\t\t\tcsa.consumers[consumer] = ca\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tnasa[stream] = csa\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tstreams[opts.Account] = nasa\n\t}\n\tjs.mu.RUnlock()\n\n\t// Use our copy to traverse so we do not need to hold the js lock.\n\tfor accName, asa := range streams {\n\t\tacc, err := s.LookupAccount(accName)\n\t\tif err != nil && len(asa) > 0 {\n\t\t\tif !details {\n\t\t\t\thealth.Status = na\n\t\t\t\thealth.Error = fmt.Sprintf(\"JetStream can not lookup account %q: %v\", accName, err)\n\t\t\t\treturn health\n\t\t\t}\n\t\t\thealth.Errors = append(health.Errors, HealthzError{\n\t\t\t\tType:    HealthzErrorAccount,\n\t\t\t\tAccount: accName,\n\t\t\t\tError:   fmt.Sprintf(\"JetStream can not lookup account %q: %v\", accName, err),\n\t\t\t})\n\t\t\tcontinue\n\t\t}\n\n\t\tfor stream, sa := range asa {\n\t\t\t// Make sure we can look up\n\t\t\tif err := js.isStreamHealthy(acc, sa); err != nil {\n\t\t\t\tif !details {\n\t\t\t\t\thealth.Status = na\n\t\t\t\t\thealth.Error = fmt.Sprintf(\"JetStream stream '%s > %s' is not current: %s\", accName, stream, err)\n\t\t\t\t\treturn health\n\t\t\t\t}\n\t\t\t\thealth.Errors = append(health.Errors, HealthzError{\n\t\t\t\t\tType:    HealthzErrorStream,\n\t\t\t\t\tAccount: accName,\n\t\t\t\t\tStream:  stream,\n\t\t\t\t\tError:   fmt.Sprintf(\"JetStream stream '%s > %s' is not current: %s\", accName, stream, err),\n\t\t\t\t})\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tmset, _ := acc.lookupStream(stream)\n\t\t\t// Now check consumers.\n\t\t\tfor consumer, ca := range sa.consumers {\n\t\t\t\tif err := js.isConsumerHealthy(mset, consumer, ca); err != nil {\n\t\t\t\t\tif !details {\n\t\t\t\t\t\thealth.Status = na\n\t\t\t\t\t\thealth.Error = fmt.Sprintf(\"JetStream consumer '%s > %s > %s' is not current: %s\", acc, stream, consumer, err)\n\t\t\t\t\t\treturn health\n\t\t\t\t\t}\n\t\t\t\t\thealth.Errors = append(health.Errors, HealthzError{\n\t\t\t\t\t\tType:     HealthzErrorConsumer,\n\t\t\t\t\t\tAccount:  accName,\n\t\t\t\t\t\tStream:   stream,\n\t\t\t\t\t\tConsumer: consumer,\n\t\t\t\t\t\tError:    fmt.Sprintf(\"JetStream consumer '%s > %s > %s' is not current: %s\", acc, stream, consumer, err),\n\t\t\t\t\t})\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\t// Success.\n\treturn health\n}\n\ntype ExpvarzStatus struct {\n\tMemstats json.RawMessage `json:\"memstats\"`\n\tCmdline  json.RawMessage `json:\"cmdline\"`\n}\n\nfunc (s *Server) expvarz(_ *ExpvarzEventOptions) *ExpvarzStatus {\n\tvar stat ExpvarzStatus\n\n\tconst memStatsKey = \"memstats\"\n\tconst cmdLineKey = \"cmdline\"\n\n\texpvar.Do(func(v expvar.KeyValue) {\n\t\tswitch v.Key {\n\t\tcase memStatsKey:\n\t\t\tstat.Memstats = json.RawMessage(v.Value.String())\n\n\t\tcase cmdLineKey:\n\t\t\tstat.Cmdline = json.RawMessage(v.Value.String())\n\t\t}\n\t})\n\n\treturn &stat\n}\n\ntype ProfilezStatus struct {\n\tProfile []byte `json:\"profile\"`\n\tError   string `json:\"error\"`\n}\n\nfunc (s *Server) profilez(opts *ProfilezOptions) *ProfilezStatus {\n\tvar buffer bytes.Buffer\n\tswitch opts.Name {\n\tcase _EMPTY_:\n\t\treturn &ProfilezStatus{\n\t\t\tError: \"Profile name not specified\",\n\t\t}\n\tcase \"cpu\":\n\t\tif opts.Duration <= 0 || opts.Duration > 15*time.Second {\n\t\t\treturn &ProfilezStatus{\n\t\t\t\tError: fmt.Sprintf(\"Duration %s should be between 0s and 15s\", opts.Duration),\n\t\t\t}\n\t\t}\n\t\tif err := pprof.StartCPUProfile(&buffer); err != nil {\n\t\t\treturn &ProfilezStatus{\n\t\t\t\tError: fmt.Sprintf(\"Failed to start CPU profile: %s\", err),\n\t\t\t}\n\t\t}\n\t\ttime.Sleep(opts.Duration)\n\t\tpprof.StopCPUProfile()\n\tdefault:\n\t\tprofile := pprof.Lookup(opts.Name)\n\t\tif profile == nil {\n\t\t\treturn &ProfilezStatus{\n\t\t\t\tError: fmt.Sprintf(\"Profile %q not found\", opts.Name),\n\t\t\t}\n\t\t}\n\t\tif err := profile.WriteTo(&buffer, opts.Debug); err != nil {\n\t\t\treturn &ProfilezStatus{\n\t\t\t\tError: fmt.Sprintf(\"Profile %q error: %s\", opts.Name, err),\n\t\t\t}\n\t\t}\n\t}\n\treturn &ProfilezStatus{\n\t\tProfile: buffer.Bytes(),\n\t}\n}\n\ntype RaftzGroup struct {\n\tID            string                    `json:\"id\"`\n\tState         string                    `json:\"state\"`\n\tSize          int                       `json:\"size\"`\n\tQuorumNeeded  int                       `json:\"quorum_needed\"`\n\tObserver      bool                      `json:\"observer,omitempty\"`\n\tPaused        bool                      `json:\"paused,omitempty\"`\n\tCommitted     uint64                    `json:\"committed\"`\n\tApplied       uint64                    `json:\"applied\"`\n\tCatchingUp    bool                      `json:\"catching_up,omitempty\"`\n\tLeader        string                    `json:\"leader,omitempty\"`\n\tEverHadLeader bool                      `json:\"ever_had_leader\"`\n\tTerm          uint64                    `json:\"term\"`\n\tVote          string                    `json:\"voted_for,omitempty\"`\n\tPTerm         uint64                    `json:\"pterm\"`\n\tPIndex        uint64                    `json:\"pindex\"`\n\tIPQPropLen    int                       `json:\"ipq_proposal_len\"`\n\tIPQEntryLen   int                       `json:\"ipq_entry_len\"`\n\tIPQRespLen    int                       `json:\"ipq_resp_len\"`\n\tIPQApplyLen   int                       `json:\"ipq_apply_len\"`\n\tWAL           StreamState               `json:\"wal\"`\n\tWALError      error                     `json:\"wal_error,omitempty\"`\n\tPeers         map[string]RaftzGroupPeer `json:\"peers\"`\n}\n\ntype RaftzGroupPeer struct {\n\tName                string `json:\"name\"`\n\tKnown               bool   `json:\"known\"`\n\tLastReplicatedIndex uint64 `json:\"last_replicated_index,omitempty\"`\n\tLastSeen            string `json:\"last_seen,omitempty\"`\n}\n\ntype RaftzStatus map[string]map[string]RaftzGroup\n\nfunc (s *Server) HandleRaftz(w http.ResponseWriter, r *http.Request) {\n\tif s.raftNodes == nil {\n\t\tw.WriteHeader(404)\n\t\tw.Write([]byte(\"No Raft nodes registered\"))\n\t\treturn\n\t}\n\n\tgroups := s.Raftz(&RaftzOptions{\n\t\tAccountFilter: r.URL.Query().Get(\"acc\"),\n\t\tGroupFilter:   r.URL.Query().Get(\"group\"),\n\t})\n\n\tif groups == nil {\n\t\tw.WriteHeader(404)\n\t\tw.Write([]byte(\"No Raft nodes returned, check supplied filters\"))\n\t\treturn\n\t}\n\n\tb, _ := json.MarshalIndent(groups, \"\", \"   \")\n\tResponseHandler(w, r, b)\n}\n\nfunc (s *Server) Raftz(opts *RaftzOptions) *RaftzStatus {\n\tafilter, gfilter := opts.AccountFilter, opts.GroupFilter\n\n\tif afilter == _EMPTY_ {\n\t\tif sys := s.SystemAccount(); sys != nil {\n\t\t\tafilter = sys.Name\n\t\t} else {\n\t\t\treturn nil\n\t\t}\n\t}\n\n\tgroups := map[string]RaftNode{}\n\tinfos := RaftzStatus{} // account -> group ID\n\n\ts.rnMu.RLock()\n\tif gfilter != _EMPTY_ {\n\t\tif rg, ok := s.raftNodes[gfilter]; ok && rg != nil {\n\t\t\tif n, ok := rg.(*raft); ok {\n\t\t\t\tif n.accName == afilter {\n\t\t\t\t\tgroups[gfilter] = rg\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t} else {\n\t\tfor name, rg := range s.raftNodes {\n\t\t\tif rg == nil {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif n, ok := rg.(*raft); ok {\n\t\t\t\tif n.accName != afilter {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tgroups[name] = rg\n\t\t\t}\n\t\t}\n\t}\n\ts.rnMu.RUnlock()\n\n\tfor name, rg := range groups {\n\t\tn, ok := rg.(*raft)\n\t\tif n == nil || !ok {\n\t\t\tcontinue\n\t\t}\n\t\tif _, ok := infos[n.accName]; !ok {\n\t\t\tinfos[n.accName] = map[string]RaftzGroup{}\n\t\t}\n\t\t// Only take the lock once, using the public RaftNode functions would\n\t\t// cause us to take and release the locks over and over again.\n\t\tn.RLock()\n\t\tinfo := RaftzGroup{\n\t\t\tID:            n.id,\n\t\t\tState:         RaftState(n.state.Load()).String(),\n\t\t\tSize:          n.csz,\n\t\t\tQuorumNeeded:  n.qn,\n\t\t\tObserver:      n.observer,\n\t\t\tPaused:        n.paused,\n\t\t\tCommitted:     n.commit,\n\t\t\tApplied:       n.applied,\n\t\t\tCatchingUp:    n.catchup != nil,\n\t\t\tLeader:        n.leader,\n\t\t\tEverHadLeader: n.pleader.Load(),\n\t\t\tTerm:          n.term,\n\t\t\tVote:          n.vote,\n\t\t\tPTerm:         n.pterm,\n\t\t\tPIndex:        n.pindex,\n\t\t\tIPQPropLen:    n.prop.len(),\n\t\t\tIPQEntryLen:   n.entry.len(),\n\t\t\tIPQRespLen:    n.resp.len(),\n\t\t\tIPQApplyLen:   n.apply.len(),\n\t\t\tWALError:      n.werr,\n\t\t\tPeers:         map[string]RaftzGroupPeer{},\n\t\t}\n\t\tn.wal.FastState(&info.WAL)\n\t\tfor id, p := range n.peers {\n\t\t\tif id == n.id {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tpeer := RaftzGroupPeer{\n\t\t\t\tName:                s.serverNameForNode(id),\n\t\t\t\tKnown:               p.kp,\n\t\t\t\tLastReplicatedIndex: p.li,\n\t\t\t}\n\t\t\tif p.ts > 0 {\n\t\t\t\tpeer.LastSeen = time.Since(time.Unix(0, p.ts)).String()\n\t\t\t}\n\t\t\tinfo.Peers[id] = peer\n\t\t}\n\t\tn.RUnlock()\n\t\tinfos[n.accName][name] = info\n\t}\n\n\treturn &infos\n}\n",
    "source_file": "server/monitor.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2019-2025 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage server\n\nimport (\n\t\"crypto/hmac\"\n\t\"crypto/sha256\"\n\t\"encoding/binary\"\n\t\"encoding/hex\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"math\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"runtime/debug\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t\"github.com/minio/highwayhash\"\n\t\"github.com/nats-io/nats-server/v2/server/sysmem\"\n\t\"github.com/nats-io/nats-server/v2/server/tpm\"\n\t\"github.com/nats-io/nkeys\"\n\t\"github.com/nats-io/nuid\"\n)\n\n// JetStreamConfig determines this server's configuration.\n// MaxMemory and MaxStore are in bytes.\ntype JetStreamConfig struct {\n\tMaxMemory    int64         `json:\"max_memory\"`\n\tMaxStore     int64         `json:\"max_storage\"`\n\tStoreDir     string        `json:\"store_dir,omitempty\"`\n\tSyncInterval time.Duration `json:\"sync_interval,omitempty\"`\n\tSyncAlways   bool          `json:\"sync_always,omitempty\"`\n\tDomain       string        `json:\"domain,omitempty\"`\n\tCompressOK   bool          `json:\"compress_ok,omitempty\"`\n\tUniqueTag    string        `json:\"unique_tag,omitempty\"`\n\tStrict       bool          `json:\"strict,omitempty\"`\n}\n\n// Statistics about JetStream for this server.\ntype JetStreamStats struct {\n\tMemory         uint64            `json:\"memory\"`\n\tStore          uint64            `json:\"storage\"`\n\tReservedMemory uint64            `json:\"reserved_memory\"`\n\tReservedStore  uint64            `json:\"reserved_storage\"`\n\tAccounts       int               `json:\"accounts\"`\n\tHAAssets       int               `json:\"ha_assets\"`\n\tAPI            JetStreamAPIStats `json:\"api\"`\n}\n\ntype JetStreamAccountLimits struct {\n\tMaxMemory            int64 `json:\"max_memory\"`\n\tMaxStore             int64 `json:\"max_storage\"`\n\tMaxStreams           int   `json:\"max_streams\"`\n\tMaxConsumers         int   `json:\"max_consumers\"`\n\tMaxAckPending        int   `json:\"max_ack_pending\"`\n\tMemoryMaxStreamBytes int64 `json:\"memory_max_stream_bytes\"`\n\tStoreMaxStreamBytes  int64 `json:\"storage_max_stream_bytes\"`\n\tMaxBytesRequired     bool  `json:\"max_bytes_required\"`\n}\n\ntype JetStreamTier struct {\n\tMemory         uint64                 `json:\"memory\"`\n\tStore          uint64                 `json:\"storage\"`\n\tReservedMemory uint64                 `json:\"reserved_memory\"`\n\tReservedStore  uint64                 `json:\"reserved_storage\"`\n\tStreams        int                    `json:\"streams\"`\n\tConsumers      int                    `json:\"consumers\"`\n\tLimits         JetStreamAccountLimits `json:\"limits\"`\n}\n\n// JetStreamAccountStats returns current statistics about the account's JetStream usage.\ntype JetStreamAccountStats struct {\n\tJetStreamTier                          // in case tiers are used, reflects totals with limits not set\n\tDomain        string                   `json:\"domain,omitempty\"`\n\tAPI           JetStreamAPIStats        `json:\"api\"`\n\tTiers         map[string]JetStreamTier `json:\"tiers,omitempty\"` // indexed by tier name\n}\n\ntype JetStreamAPIStats struct {\n\tLevel    int    `json:\"level\"`\n\tTotal    uint64 `json:\"total\"`\n\tErrors   uint64 `json:\"errors\"`\n\tInflight uint64 `json:\"inflight,omitempty\"`\n}\n\n// This is for internal accounting for JetStream for this server.\ntype jetStream struct {\n\t// These are here first because of atomics on 32bit systems.\n\tapiInflight   int64\n\tapiTotal      int64\n\tapiErrors     int64\n\tmemReserved   int64\n\tstoreReserved int64\n\tmemUsed       int64\n\tstoreUsed     int64\n\tqueueLimit    int64\n\tclustered     int32\n\tmu            sync.RWMutex\n\tsrv           *Server\n\tconfig        JetStreamConfig\n\tcluster       *jetStreamCluster\n\taccounts      map[string]*jsAccount\n\tapiSubs       *Sublist\n\tstarted       time.Time\n\n\t// System level request to purge a stream move\n\taccountPurge *subscription\n\n\t// Some bools regarding general state.\n\tmetaRecovering bool\n\tstandAlone     bool\n\toos            bool\n\tshuttingDown   bool\n\n\t// Atomic versions\n\tdisabled atomic.Bool\n}\n\ntype remoteUsage struct {\n\ttiers map[string]*jsaUsage // indexed by tier name\n\tapi   uint64\n\terr   uint64\n}\n\ntype jsaStorage struct {\n\ttotal jsaUsage\n\tlocal jsaUsage\n}\n\n// This represents a jetstream enabled account.\n// Worth noting that we include the jetstream pointer, this is because\n// in general we want to be very efficient when receiving messages on\n// an internal sub for a stream, so we will direct link to the stream\n// and walk backwards as needed vs multiple hash lookups and locks, etc.\ntype jsAccount struct {\n\tmu        sync.RWMutex\n\tjs        *jetStream\n\taccount   *Account\n\tstoreDir  string\n\tinflight  sync.Map\n\tstreams   map[string]*stream\n\ttemplates map[string]*streamTemplate\n\tstore     TemplateStore\n\n\t// From server\n\tsendq *ipQueue[*pubMsg]\n\n\t// For limiting only running one checkAndSync at a time.\n\tsync atomic.Bool\n\n\t// Usage/limits related fields that will be protected by usageMu\n\tusageMu    sync.RWMutex\n\tlimits     map[string]JetStreamAccountLimits // indexed by tierName\n\tusage      map[string]*jsaStorage            // indexed by tierName\n\trusage     map[string]*remoteUsage           // indexed by node id\n\tapiTotal   uint64\n\tapiErrors  uint64\n\tusageApi   uint64\n\tusageErr   uint64\n\tupdatesPub string\n\tupdatesSub *subscription\n\tlupdate    time.Time\n\tutimer     *time.Timer\n}\n\n// Track general usage for this account.\ntype jsaUsage struct {\n\tmem   int64\n\tstore int64\n}\n\n// EnableJetStream will enable JetStream support on this server with the given configuration.\n// A nil configuration will dynamically choose the limits and temporary file storage directory.\nfunc (s *Server) EnableJetStream(config *JetStreamConfig) error {\n\tif s.JetStreamEnabled() {\n\t\treturn fmt.Errorf(\"jetstream already enabled\")\n\t}\n\n\ts.Noticef(\"Starting JetStream\")\n\tif config == nil || config.MaxMemory <= 0 || config.MaxStore <= 0 {\n\t\tvar storeDir, domain, uniqueTag string\n\t\tvar maxStore, maxMem int64\n\t\tif config != nil {\n\t\t\tstoreDir, domain, uniqueTag = config.StoreDir, config.Domain, config.UniqueTag\n\t\t\tmaxStore, maxMem = config.MaxStore, config.MaxMemory\n\t\t}\n\t\tconfig = s.dynJetStreamConfig(storeDir, maxStore, maxMem)\n\t\tif maxMem > 0 {\n\t\t\tconfig.MaxMemory = maxMem\n\t\t}\n\t\tif domain != _EMPTY_ {\n\t\t\tconfig.Domain = domain\n\t\t}\n\t\tif uniqueTag != _EMPTY_ {\n\t\t\tconfig.UniqueTag = uniqueTag\n\t\t}\n\t\ts.Debugf(\"JetStream creating dynamic configuration - %s memory, %s disk\", friendlyBytes(config.MaxMemory), friendlyBytes(config.MaxStore))\n\t} else if config.StoreDir != _EMPTY_ {\n\t\tconfig.StoreDir = filepath.Join(config.StoreDir, JetStreamStoreDir)\n\t}\n\n\tcfg := *config\n\tif cfg.StoreDir == _EMPTY_ {\n\t\tcfg.StoreDir = filepath.Join(os.TempDir(), JetStreamStoreDir)\n\t\ts.Warnf(\"Temporary storage directory used, data could be lost on system reboot\")\n\t}\n\n\t// We will consistently place the 'jetstream' directory under the storedir that was handed to us. Prior to 2.2.3 though\n\t// we could have a directory on disk without the 'jetstream' directory. This will check and fix if needed.\n\tif err := s.checkStoreDir(&cfg); err != nil {\n\t\treturn err\n\t}\n\n\treturn s.enableJetStream(cfg)\n}\n\n// Function signature to generate a key encryption key.\ntype keyGen func(context []byte) ([]byte, error)\n\n// Return a key generation function or nil if encryption not enabled.\nfunc (s *Server) jsKeyGen(jsKey, info string) keyGen {\n\tif ek := jsKey; ek != _EMPTY_ {\n\t\treturn func(context []byte) ([]byte, error) {\n\t\t\th := hmac.New(sha256.New, []byte(ek))\n\t\t\tif _, err := h.Write([]byte(info)); err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tif _, err := h.Write(context); err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\treturn h.Sum(nil), nil\n\t\t}\n\t}\n\treturn nil\n}\n\n// Decode the encrypted metafile.\nfunc (s *Server) decryptMeta(sc StoreCipher, ekey, buf []byte, acc, context string) ([]byte, bool, error) {\n\tif len(ekey) < minMetaKeySize {\n\t\treturn nil, false, errBadKeySize\n\t}\n\tvar osc StoreCipher\n\tswitch sc {\n\tcase AES:\n\t\tosc = ChaCha\n\tcase ChaCha:\n\t\tosc = AES\n\t}\n\ttype prfWithCipher struct {\n\t\tkeyGen\n\t\tStoreCipher\n\t}\n\tvar prfs []prfWithCipher\n\tif prf := s.jsKeyGen(s.getOpts().JetStreamKey, acc); prf == nil {\n\t\treturn nil, false, errNoEncryption\n\t} else {\n\t\t// First of all, try our current encryption keys with both\n\t\t// store cipher algorithms.\n\t\tprfs = append(prfs, prfWithCipher{prf, sc})\n\t\tprfs = append(prfs, prfWithCipher{prf, osc})\n\t}\n\tif prf := s.jsKeyGen(s.getOpts().JetStreamOldKey, acc); prf != nil {\n\t\t// Then, if we have an old encryption key, try with also with\n\t\t// both store cipher algorithms.\n\t\tprfs = append(prfs, prfWithCipher{prf, sc})\n\t\tprfs = append(prfs, prfWithCipher{prf, osc})\n\t}\n\n\tfor i, prf := range prfs {\n\t\trb, err := prf.keyGen([]byte(context))\n\t\tif err != nil {\n\t\t\tcontinue\n\t\t}\n\t\tkek, err := genEncryptionKey(prf.StoreCipher, rb)\n\t\tif err != nil {\n\t\t\tcontinue\n\t\t}\n\t\tns := kek.NonceSize()\n\t\tseed, err := kek.Open(nil, ekey[:ns], ekey[ns:], nil)\n\t\tif err != nil {\n\t\t\tcontinue\n\t\t}\n\t\taek, err := genEncryptionKey(prf.StoreCipher, seed)\n\t\tif err != nil {\n\t\t\tcontinue\n\t\t}\n\t\tif aek.NonceSize() != kek.NonceSize() {\n\t\t\tcontinue\n\t\t}\n\t\tplain, err := aek.Open(nil, buf[:ns], buf[ns:], nil)\n\t\tif err != nil {\n\t\t\tcontinue\n\t\t}\n\t\treturn plain, i > 0, nil\n\t}\n\treturn nil, false, fmt.Errorf(\"unable to recover keys\")\n}\n\n// Check to make sure directory has the jetstream directory.\n// We will have it properly configured here now regardless, so need to look inside.\nfunc (s *Server) checkStoreDir(cfg *JetStreamConfig) error {\n\tfis, _ := os.ReadDir(cfg.StoreDir)\n\t// If we have nothing underneath us, could be just starting new, but if we see this we can check.\n\tif len(fis) != 0 {\n\t\treturn nil\n\t}\n\t// Let's check the directory above. If it has us 'jetstream' but also other stuff that we can\n\t// identify as accounts then we can fix.\n\tfis, _ = os.ReadDir(filepath.Dir(cfg.StoreDir))\n\t// If just one that is us 'jetstream' and all is ok.\n\tif len(fis) == 1 {\n\t\treturn nil\n\t}\n\n\thaveJetstreamDir := false\n\tfor _, fi := range fis {\n\t\tif fi.Name() == JetStreamStoreDir {\n\t\t\thaveJetstreamDir = true\n\t\t\tbreak\n\t\t}\n\t}\n\n\tfor _, fi := range fis {\n\t\t// Skip the 'jetstream' directory.\n\t\tif fi.Name() == JetStreamStoreDir {\n\t\t\tcontinue\n\t\t}\n\t\t// Let's see if this is an account.\n\t\tif accName := fi.Name(); accName != _EMPTY_ {\n\t\t\t_, ok := s.accounts.Load(accName)\n\t\t\tif !ok && s.AccountResolver() != nil && nkeys.IsValidPublicAccountKey(accName) {\n\t\t\t\t// Account is not local but matches the NKEY account public key,\n\t\t\t\t// this is enough indication to move this directory, no need to\n\t\t\t\t// fetch the account.\n\t\t\t\tok = true\n\t\t\t}\n\t\t\t// If this seems to be an account go ahead and move the directory. This will include all assets\n\t\t\t// like streams and consumers.\n\t\t\tif ok {\n\t\t\t\tif !haveJetstreamDir {\n\t\t\t\t\terr := os.Mkdir(filepath.Join(filepath.Dir(cfg.StoreDir), JetStreamStoreDir), defaultDirPerms)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn err\n\t\t\t\t\t}\n\t\t\t\t\thaveJetstreamDir = true\n\t\t\t\t}\n\t\t\t\told := filepath.Join(filepath.Dir(cfg.StoreDir), fi.Name())\n\t\t\t\tnew := filepath.Join(cfg.StoreDir, fi.Name())\n\t\t\t\ts.Noticef(\"JetStream relocated account %q to %q\", old, new)\n\t\t\t\tif err := os.Rename(old, new); err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// This function sets/updates the jetstream encryption key and cipher based\n// on options. If the TPM options have been specified, a key is generated\n// and sealed by the TPM.\nfunc (s *Server) initJetStreamEncryption() (err error) {\n\topts := s.getOpts()\n\n\t// The TPM settings and other encryption settings are mutually exclusive.\n\tif opts.JetStreamKey != _EMPTY_ && opts.JetStreamTpm.KeysFile != _EMPTY_ {\n\t\treturn fmt.Errorf(\"JetStream encryption key may not be used with TPM options\")\n\t}\n\t// if we are using the standard method to set the encryption key just return and carry on.\n\tif opts.JetStreamKey != _EMPTY_ {\n\t\treturn nil\n\t}\n\t// if the tpm options are not used then no encryption has been configured and return.\n\tif opts.JetStreamTpm.KeysFile == _EMPTY_ {\n\t\treturn nil\n\t}\n\n\tif opts.JetStreamTpm.Pcr == 0 {\n\t\t// Default PCR to use in the TPM. Values can be 0-23, and most platforms\n\t\t// reserve values 0-12 for the OS, boot locker, disc encryption, etc.\n\t\t// 16 used for debugging. In sticking to NATS tradition, we'll use 22\n\t\t// as the default with the option being configurable.\n\t\topts.JetStreamTpm.Pcr = 22\n\t}\n\n\t// Using the TPM to generate or get the encryption key and update the encryption options.\n\topts.JetStreamKey, err = tpm.LoadJetStreamEncryptionKeyFromTPM(opts.JetStreamTpm.SrkPassword,\n\t\topts.JetStreamTpm.KeysFile, opts.JetStreamTpm.KeyPassword, opts.JetStreamTpm.Pcr)\n\n\treturn err\n}\n\n// enableJetStream will start up the JetStream subsystem.\nfunc (s *Server) enableJetStream(cfg JetStreamConfig) error {\n\tjs := &jetStream{srv: s, config: cfg, accounts: make(map[string]*jsAccount), apiSubs: NewSublistNoCache()}\n\ts.gcbMu.Lock()\n\tif s.gcbOutMax = s.getOpts().JetStreamMaxCatchup; s.gcbOutMax == 0 {\n\t\ts.gcbOutMax = defaultMaxTotalCatchupOutBytes\n\t}\n\ts.gcbMu.Unlock()\n\n\t// TODO: Not currently reloadable.\n\tatomic.StoreInt64(&js.queueLimit, s.getOpts().JetStreamRequestQueueLimit)\n\n\ts.js.Store(js)\n\n\t// FIXME(dlc) - Allow memory only operation?\n\tif stat, err := os.Stat(cfg.StoreDir); os.IsNotExist(err) {\n\t\tif err := os.MkdirAll(cfg.StoreDir, defaultDirPerms); err != nil {\n\t\t\treturn fmt.Errorf(\"could not create storage directory - %v\", err)\n\t\t}\n\t} else {\n\t\t// Make sure its a directory and that we can write to it.\n\t\tif stat == nil || !stat.IsDir() {\n\t\t\treturn fmt.Errorf(\"storage directory is not a directory\")\n\t\t}\n\t\ttmpfile, err := os.CreateTemp(cfg.StoreDir, \"_test_\")\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"storage directory is not writable\")\n\t\t}\n\t\ttmpfile.Close()\n\t\tos.Remove(tmpfile.Name())\n\t}\n\n\tif err := s.initJetStreamEncryption(); err != nil {\n\t\treturn err\n\t}\n\n\t// JetStream is an internal service so we need to make sure we have a system account.\n\t// This system account will export the JetStream service endpoints.\n\tif s.SystemAccount() == nil {\n\t\ts.SetDefaultSystemAccount()\n\t}\n\n\topts := s.getOpts()\n\tif !opts.DisableJetStreamBanner {\n\t\ts.Noticef(\"    _ ___ _____ ___ _____ ___ ___   _   __  __\")\n\t\ts.Noticef(\" _ | | __|_   _/ __|_   _| _ \\\\ __| /_\\\\ |  \\\\/  |\")\n\t\ts.Noticef(\"| || | _|  | | \\\\__ \\\\ | | |   / _| / _ \\\\| |\\\\/| |\")\n\t\ts.Noticef(\" \\\\__/|___| |_| |___/ |_| |_|_\\\\___/_/ \\\\_\\\\_|  |_|\")\n\t\ts.Noticef(\"\")\n\t\ts.Noticef(\"         https://docs.nats.io/jetstream\")\n\t\ts.Noticef(\"\")\n\t}\n\ts.Noticef(\"---------------- JETSTREAM ----------------\")\n\n\tif cfg.Strict {\n\t\ts.Noticef(\"  Strict:          %t\", cfg.Strict)\n\t}\n\n\ts.Noticef(\"  Max Memory:      %s\", friendlyBytes(cfg.MaxMemory))\n\ts.Noticef(\"  Max Storage:     %s\", friendlyBytes(cfg.MaxStore))\n\ts.Noticef(\"  Store Directory: \\\"%s\\\"\", cfg.StoreDir)\n\tif cfg.Domain != _EMPTY_ {\n\t\ts.Noticef(\"  Domain:          %s\", cfg.Domain)\n\t}\n\n\tif ek := opts.JetStreamKey; ek != _EMPTY_ {\n\t\ts.Noticef(\"  Encryption:      %s\", opts.JetStreamCipher)\n\t}\n\tif opts.JetStreamTpm.KeysFile != _EMPTY_ {\n\t\ts.Noticef(\"  TPM File:        %q, Pcr: %d\", opts.JetStreamTpm.KeysFile,\n\t\t\topts.JetStreamTpm.Pcr)\n\t}\n\ts.Noticef(\"  API Level:       %d\", JSApiLevel)\n\ts.Noticef(\"-------------------------------------------\")\n\n\t// Setup our internal subscriptions.\n\tif err := s.setJetStreamExportSubs(); err != nil {\n\t\treturn fmt.Errorf(\"setting up internal jetstream subscriptions failed: %v\", err)\n\t}\n\n\t// Setup our internal system exports.\n\ts.Debugf(\"  Exports:\")\n\ts.Debugf(\"     %s\", jsAllAPI)\n\ts.setupJetStreamExports()\n\n\tstandAlone, canExtend := s.standAloneMode(), s.canExtendOtherDomain()\n\tif standAlone && canExtend && s.getOpts().JetStreamExtHint != jsWillExtend {\n\t\tcanExtend = false\n\t\ts.Noticef(\"Standalone server started in clustered mode do not support extending domains\")\n\t\ts.Noticef(`Manually disable standalone mode by setting the JetStream Option \"extension_hint: %s\"`, jsWillExtend)\n\t}\n\n\t// Indicate if we will be standalone for checking resource reservations, etc.\n\tjs.setJetStreamStandAlone(standAlone && !canExtend)\n\n\t// Enable accounts and restore state before starting clustering.\n\tif err := s.enableJetStreamAccounts(); err != nil {\n\t\treturn err\n\t}\n\n\t// If we are in clustered mode go ahead and start the meta controller.\n\tif !standAlone || canExtend {\n\t\tif err := s.enableJetStreamClustering(); err != nil {\n\t\t\treturn err\n\t\t}\n\t\t// Set our atomic bool to clustered.\n\t\ts.jsClustered.Store(true)\n\t}\n\n\t// Mark when we are up and running.\n\tjs.setStarted()\n\n\treturn nil\n}\n\nconst jsNoExtend = \"no_extend\"\nconst jsWillExtend = \"will_extend\"\n\n// This will check if we have a solicited leafnode that shares the system account\n// and extension is not manually disabled\nfunc (s *Server) canExtendOtherDomain() bool {\n\topts := s.getOpts()\n\tsysAcc := s.SystemAccount().GetName()\n\tfor _, r := range opts.LeafNode.Remotes {\n\t\tif r.LocalAccount == sysAcc {\n\t\t\tfor _, denySub := range r.DenyImports {\n\t\t\t\tif subjectIsSubsetMatch(denySub, raftAllSubj) {\n\t\t\t\t\treturn false\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\nfunc (s *Server) updateJetStreamInfoStatus(enabled bool) {\n\ts.mu.Lock()\n\ts.info.JetStream = enabled\n\ts.mu.Unlock()\n}\n\n// restartJetStream will try to re-enable JetStream during a reload if it had been disabled during runtime.\nfunc (s *Server) restartJetStream() error {\n\topts := s.getOpts()\n\tcfg := JetStreamConfig{\n\t\tStoreDir:     opts.StoreDir,\n\t\tSyncInterval: opts.SyncInterval,\n\t\tSyncAlways:   opts.SyncAlways,\n\t\tMaxMemory:    opts.JetStreamMaxMemory,\n\t\tMaxStore:     opts.JetStreamMaxStore,\n\t\tDomain:       opts.JetStreamDomain,\n\t\tStrict:       opts.JetStreamStrict,\n\t}\n\ts.Noticef(\"Restarting JetStream\")\n\terr := s.EnableJetStream(&cfg)\n\tif err != nil {\n\t\ts.Warnf(\"Can't start JetStream: %v\", err)\n\t\treturn s.DisableJetStream()\n\t}\n\ts.updateJetStreamInfoStatus(true)\n\treturn nil\n}\n\n// checkJetStreamExports will check if we have the JS exports setup\n// on the system account, and if not go ahead and set them up.\nfunc (s *Server) checkJetStreamExports() {\n\tif sacc := s.SystemAccount(); sacc != nil {\n\t\tsacc.mu.RLock()\n\t\tse := sacc.getServiceExport(jsAllAPI)\n\t\tsacc.mu.RUnlock()\n\t\tif se == nil {\n\t\t\ts.setupJetStreamExports()\n\t\t}\n\t}\n}\n\nfunc (s *Server) setupJetStreamExports() {\n\t// Setup our internal system export.\n\tif err := s.SystemAccount().AddServiceExport(jsAllAPI, nil); err != nil {\n\t\ts.Warnf(\"Error setting up jetstream service exports: %v\", err)\n\t}\n}\n\nfunc (s *Server) jetStreamOOSPending() (wasPending bool) {\n\tif js := s.getJetStream(); js != nil {\n\t\tjs.mu.Lock()\n\t\twasPending = js.oos\n\t\tjs.oos = true\n\t\tjs.mu.Unlock()\n\t}\n\treturn wasPending\n}\n\nfunc (s *Server) setJetStreamDisabled() {\n\tif js := s.getJetStream(); js != nil {\n\t\tjs.disabled.Store(true)\n\t}\n}\n\nfunc (s *Server) handleOutOfSpace(mset *stream) {\n\tif s.JetStreamEnabled() && !s.jetStreamOOSPending() {\n\t\tvar stream string\n\t\tif mset != nil {\n\t\t\tstream = mset.name()\n\t\t\ts.Errorf(\"JetStream out of %s resources, will be DISABLED\", mset.Store().Type())\n\t\t} else {\n\t\t\ts.Errorf(\"JetStream out of resources, will be DISABLED\")\n\t\t}\n\n\t\tgo s.DisableJetStream()\n\n\t\tadv := &JSServerOutOfSpaceAdvisory{\n\t\t\tTypedEvent: TypedEvent{\n\t\t\t\tType: JSServerOutOfStorageAdvisoryType,\n\t\t\t\tID:   nuid.Next(),\n\t\t\t\tTime: time.Now().UTC(),\n\t\t\t},\n\t\t\tServer:   s.Name(),\n\t\t\tServerID: s.ID(),\n\t\t\tStream:   stream,\n\t\t\tCluster:  s.cachedClusterName(),\n\t\t\tDomain:   s.getOpts().JetStreamDomain,\n\t\t}\n\t\ts.publishAdvisory(nil, JSAdvisoryServerOutOfStorage, adv)\n\t}\n}\n\n// DisableJetStream will turn off JetStream and signals in clustered mode\n// to have the metacontroller remove us from the peer list.\nfunc (s *Server) DisableJetStream() error {\n\tif !s.JetStreamEnabled() {\n\t\treturn nil\n\t}\n\n\ts.setJetStreamDisabled()\n\n\tif s.JetStreamIsClustered() {\n\t\tisLeader := s.JetStreamIsLeader()\n\t\tjs, cc := s.getJetStreamCluster()\n\t\tif js == nil {\n\t\t\ts.shutdownJetStream()\n\t\t\treturn nil\n\t\t}\n\t\tjs.mu.RLock()\n\t\tmeta := cc.meta\n\t\tjs.mu.RUnlock()\n\n\t\tif meta != nil {\n\t\t\tif isLeader {\n\t\t\t\ts.Warnf(\"JetStream initiating meta leader transfer\")\n\t\t\t\tmeta.StepDown()\n\t\t\t\tselect {\n\t\t\t\tcase <-s.quitCh:\n\t\t\t\t\treturn nil\n\t\t\t\tcase <-time.After(2 * time.Second):\n\t\t\t\t}\n\t\t\t\tif !s.JetStreamIsCurrent() {\n\t\t\t\t\ts.Warnf(\"JetStream timeout waiting for meta leader transfer\")\n\t\t\t\t}\n\t\t\t}\n\t\t\tmeta.Delete()\n\t\t}\n\t}\n\n\t// Update our info status.\n\ts.updateJetStreamInfoStatus(false)\n\n\t// Normal shutdown.\n\ts.shutdownJetStream()\n\n\t// Shut down the RAFT groups.\n\ts.shutdownRaftNodes()\n\n\treturn nil\n}\n\nfunc (s *Server) enableJetStreamAccounts() error {\n\t// If we have no configured accounts setup then setup imports on global account.\n\tif s.globalAccountOnly() {\n\t\tgacc := s.GlobalAccount()\n\t\tgacc.mu.Lock()\n\t\tif len(gacc.jsLimits) == 0 {\n\t\t\tgacc.jsLimits = defaultJSAccountTiers\n\t\t}\n\t\tgacc.mu.Unlock()\n\t\tif err := s.configJetStream(gacc); err != nil {\n\t\t\treturn err\n\t\t}\n\t} else if err := s.configAllJetStreamAccounts(); err != nil {\n\t\treturn fmt.Errorf(\"Error enabling jetstream on configured accounts: %v\", err)\n\t}\n\treturn nil\n}\n\n// enableAllJetStreamServiceImportsAndMappings turns on all service imports and mappings for jetstream for this account.\nfunc (a *Account) enableAllJetStreamServiceImportsAndMappings() error {\n\ta.mu.RLock()\n\ts := a.srv\n\ta.mu.RUnlock()\n\n\tif s == nil {\n\t\treturn fmt.Errorf(\"jetstream account not registered\")\n\t}\n\n\tvar dstAccName string\n\tif sacc := s.SystemAccount(); sacc != nil {\n\t\tdstAccName = sacc.Name\n\t}\n\n\tif !a.serviceImportExists(dstAccName, jsAllAPI) {\n\t\t// Capture si so we can turn on implicit sharing with JetStream layer.\n\t\t// Make sure to set \"to\" otherwise will incur performance slow down.\n\t\tsi, err := a.addServiceImport(s.SystemAccount(), jsAllAPI, jsAllAPI, nil)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"Error setting up jetstream service imports for account: %v\", err)\n\t\t}\n\t\ta.mu.Lock()\n\t\tsi.share = true\n\t\ta.mu.Unlock()\n\t}\n\n\t// Check if we have a Domain specified.\n\t// If so add in a subject mapping that will allow local connected clients to reach us here as well.\n\tif opts := s.getOpts(); opts.JetStreamDomain != _EMPTY_ {\n\t\tmappings := generateJSMappingTable(opts.JetStreamDomain)\n\t\ta.mu.RLock()\n\t\tfor _, m := range a.mappings {\n\t\t\tdelete(mappings, m.src)\n\t\t}\n\t\ta.mu.RUnlock()\n\t\tfor src, dest := range mappings {\n\t\t\tif err := a.AddMapping(src, dest); err != nil {\n\t\t\t\ts.Errorf(\"Error adding JetStream domain mapping: %v\", err)\n\t\t\t}\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// enableJetStreamInfoServiceImportOnly will enable the single service import responder.\n// Should we do them all regardless?\nfunc (a *Account) enableJetStreamInfoServiceImportOnly() error {\n\t// Check if this import would be overshadowed. This can happen when accounts\n\t// are importing from another account for JS access.\n\tif a.serviceImportShadowed(JSApiAccountInfo) {\n\t\treturn nil\n\t}\n\n\treturn a.enableAllJetStreamServiceImportsAndMappings()\n}\n\nfunc (s *Server) configJetStream(acc *Account) error {\n\tif acc == nil {\n\t\treturn nil\n\t}\n\tacc.mu.RLock()\n\tjsLimits := acc.jsLimits\n\tacc.mu.RUnlock()\n\tif jsLimits != nil {\n\t\t// Check if already enabled. This can be during a reload.\n\t\tif acc.JetStreamEnabled() {\n\t\t\tif err := acc.enableAllJetStreamServiceImportsAndMappings(); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tif err := acc.UpdateJetStreamLimits(jsLimits); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t} else {\n\t\t\tif err := acc.EnableJetStream(jsLimits); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tif s.gateway.enabled {\n\t\t\t\ts.switchAccountToInterestMode(acc.GetName())\n\t\t\t}\n\t\t}\n\t} else if acc != s.SystemAccount() {\n\t\tif acc.JetStreamEnabled() {\n\t\t\tacc.DisableJetStream()\n\t\t}\n\t\t// We will setup basic service imports to respond to\n\t\t// requests if JS is enabled for this account.\n\t\tif err := acc.enableJetStreamInfoServiceImportOnly(); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn nil\n}\n\n// configAllJetStreamAccounts walk all configured accounts and turn on jetstream if requested.\nfunc (s *Server) configAllJetStreamAccounts() error {\n\t// Check to see if system account has been enabled. We could arrive here via reload and\n\t// a non-default system account.\n\ts.checkJetStreamExports()\n\n\t// Bail if server not enabled. If it was enabled and a reload turns it off\n\t// that will be handled elsewhere.\n\tjs := s.getJetStream()\n\tif js == nil {\n\t\treturn nil\n\t}\n\n\t// Snapshot into our own list. Might not be needed.\n\ts.mu.RLock()\n\tif s.sys != nil {\n\t\t// clustered stream removal will perform this cleanup as well\n\t\t// this is mainly for initial cleanup\n\t\tsaccName := s.sys.account.Name\n\t\taccStoreDirs, _ := os.ReadDir(js.config.StoreDir)\n\t\tfor _, acc := range accStoreDirs {\n\t\t\tif accName := acc.Name(); accName != saccName {\n\t\t\t\t// no op if not empty\n\t\t\t\taccDir := filepath.Join(js.config.StoreDir, accName)\n\t\t\t\tos.Remove(filepath.Join(accDir, streamsDir))\n\t\t\t\tos.Remove(accDir)\n\t\t\t}\n\t\t}\n\t}\n\n\tvar jsAccounts []*Account\n\ts.accounts.Range(func(k, v any) bool {\n\t\tjsAccounts = append(jsAccounts, v.(*Account))\n\t\treturn true\n\t})\n\taccounts := &s.accounts\n\ts.mu.RUnlock()\n\n\t// Process any jetstream enabled accounts here. These will be accounts we are\n\t// already aware of at startup etc.\n\tfor _, acc := range jsAccounts {\n\t\tif err := s.configJetStream(acc); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\t// Now walk all the storage we have and resolve any accounts that we did not process already.\n\t// This is important in resolver/operator models.\n\tfis, _ := os.ReadDir(js.config.StoreDir)\n\tfor _, fi := range fis {\n\t\tif accName := fi.Name(); accName != _EMPTY_ {\n\t\t\t// Only load up ones not already loaded since they are processed above.\n\t\t\tif _, ok := accounts.Load(accName); !ok {\n\t\t\t\tif acc, err := s.lookupAccount(accName); err != nil && acc != nil {\n\t\t\t\t\tif err := s.configJetStream(acc); err != nil {\n\t\t\t\t\t\treturn err\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// Mark our started time.\nfunc (js *jetStream) setStarted() {\n\tjs.mu.Lock()\n\tdefer js.mu.Unlock()\n\tjs.started = time.Now()\n}\n\nfunc (js *jetStream) isEnabled() bool {\n\tif js == nil {\n\t\treturn false\n\t}\n\treturn !js.disabled.Load()\n}\n\n// Mark that we will be in standlone mode.\nfunc (js *jetStream) setJetStreamStandAlone(isStandAlone bool) {\n\tif js == nil {\n\t\treturn\n\t}\n\tjs.mu.Lock()\n\tdefer js.mu.Unlock()\n\tif js.standAlone = isStandAlone; js.standAlone {\n\t\t// Update our server atomic.\n\t\tjs.srv.isMetaLeader.Store(true)\n\t\tjs.accountPurge, _ = js.srv.systemSubscribe(JSApiAccountPurge, _EMPTY_, false, nil, js.srv.jsLeaderAccountPurgeRequest)\n\t} else if js.accountPurge != nil {\n\t\tjs.srv.sysUnsubscribe(js.accountPurge)\n\t}\n}\n\n// JetStreamEnabled reports if jetstream is enabled for this server.\nfunc (s *Server) JetStreamEnabled() bool {\n\treturn s.getJetStream().isEnabled()\n}\n\n// JetStreamEnabledForDomain will report if any servers have JetStream enabled within this domain.\nfunc (s *Server) JetStreamEnabledForDomain() bool {\n\tif s.JetStreamEnabled() {\n\t\treturn true\n\t}\n\n\tvar jsFound bool\n\t// If we are here we do not have JetStream enabled for ourselves, but we need to check all connected servers.\n\t// TODO(dlc) - Could optimize and memoize this.\n\ts.nodeToInfo.Range(func(k, v any) bool {\n\t\t// This should not be dependent on online status, so only check js.\n\t\tif v.(nodeInfo).js {\n\t\t\tjsFound = true\n\t\t\treturn false\n\t\t}\n\t\treturn true\n\t})\n\n\treturn jsFound\n}\n\n// Will signal that all pull requests for consumers on this server are now invalid.\nfunc (s *Server) signalPullConsumers() {\n\tjs := s.getJetStream()\n\tif js == nil {\n\t\treturn\n\t}\n\n\tjs.mu.RLock()\n\tdefer js.mu.RUnlock()\n\n\t// In case we have stale pending requests.\n\tconst hdr = \"NATS/1.0 409 Server Shutdown\\r\\n\" + JSPullRequestPendingMsgs + \": %d\\r\\n\" + JSPullRequestPendingBytes + \": %d\\r\\n\\r\\n\"\n\tvar didSend bool\n\n\tfor _, jsa := range js.accounts {\n\t\tjsa.mu.RLock()\n\t\tfor _, stream := range jsa.streams {\n\t\t\tstream.mu.RLock()\n\t\t\tfor _, o := range stream.consumers {\n\t\t\t\to.mu.RLock()\n\t\t\t\t// Only signal on R1.\n\t\t\t\tif o.cfg.Replicas <= 1 {\n\t\t\t\t\tfor reply, wr := range o.pendingRequests() {\n\t\t\t\t\t\tshdr := fmt.Sprintf(hdr, wr.n, wr.b)\n\t\t\t\t\t\to.outq.send(newJSPubMsg(reply, _EMPTY_, _EMPTY_, []byte(shdr), nil, nil, 0))\n\t\t\t\t\t\tdidSend = true\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\to.mu.RUnlock()\n\t\t\t}\n\t\t\tstream.mu.RUnlock()\n\t\t}\n\t\tjsa.mu.RUnlock()\n\t}\n\t// Give time for migration information to make it out of our server.\n\tif didSend {\n\t\ttime.Sleep(50 * time.Millisecond)\n\t}\n}\n\n// Helper for determining if we are shutting down.\nfunc (js *jetStream) isShuttingDown() bool {\n\tjs.mu.RLock()\n\tdefer js.mu.RUnlock()\n\treturn js.shuttingDown\n}\n\n// Shutdown jetstream for this server.\nfunc (s *Server) shutdownJetStream() {\n\tjs := s.getJetStream()\n\tif js == nil {\n\t\treturn\n\t}\n\n\ts.Noticef(\"Initiating JetStream Shutdown...\")\n\tdefer s.Noticef(\"JetStream Shutdown\")\n\n\t// If we have folks blocked on sync requests, unblock.\n\t// Send 1 is enough, but use select in case they were all present.\n\tselect {\n\tcase s.syncOutSem <- struct{}{}:\n\tdefault:\n\t}\n\n\tvar _a [512]*Account\n\taccounts := _a[:0]\n\n\tjs.mu.Lock()\n\t// Collect accounts.\n\tfor _, jsa := range js.accounts {\n\t\tif a := jsa.acc(); a != nil {\n\t\t\taccounts = append(accounts, a)\n\t\t}\n\t}\n\taccPurgeSub := js.accountPurge\n\tjs.accountPurge = nil\n\t// Signal we are shutting down.\n\tjs.shuttingDown = true\n\tjs.mu.Unlock()\n\n\tif accPurgeSub != nil {\n\t\ts.sysUnsubscribe(accPurgeSub)\n\t}\n\n\tfor _, a := range accounts {\n\t\ta.removeJetStream()\n\t}\n\n\ts.js.Store(nil)\n\n\tjs.mu.Lock()\n\tjs.accounts = nil\n\n\tvar qch chan struct{}\n\n\tif cc := js.cluster; cc != nil {\n\t\tif cc.qch != nil {\n\t\t\tqch = cc.qch\n\t\t\tcc.qch = nil\n\t\t}\n\t\tjs.stopUpdatesSub()\n\t\tif cc.c != nil {\n\t\t\tcc.c.closeConnection(ClientClosed)\n\t\t\tcc.c = nil\n\t\t}\n\t\tcc.meta = nil\n\t\t// Set our atomic bool to false.\n\t\ts.jsClustered.Store(false)\n\t}\n\tjs.mu.Unlock()\n\n\t// If we were clustered signal the monitor cluster go routine.\n\t// We will wait for a bit for it to close.\n\t// Do this without the lock.\n\tif qch != nil {\n\t\tselect {\n\t\tcase qch <- struct{}{}:\n\t\t\tselect {\n\t\t\tcase <-qch:\n\t\t\tcase <-time.After(2 * time.Second):\n\t\t\t\ts.Warnf(\"Did not receive signal for successful shutdown of cluster routine\")\n\t\t\t}\n\t\tdefault:\n\t\t}\n\t}\n}\n\n// JetStreamConfig will return the current config. Useful if the system\n// created a dynamic configuration. A copy is returned.\nfunc (s *Server) JetStreamConfig() *JetStreamConfig {\n\tvar c *JetStreamConfig\n\tif js := s.getJetStream(); js != nil {\n\t\tcopy := js.config\n\t\tc = &(copy)\n\t}\n\treturn c\n}\n\n// StoreDir returns the current JetStream directory.\nfunc (s *Server) StoreDir() string {\n\tjs := s.getJetStream()\n\tif js == nil {\n\t\treturn _EMPTY_\n\t}\n\treturn js.config.StoreDir\n}\n\n// JetStreamNumAccounts returns the number of enabled accounts this server is tracking.\nfunc (s *Server) JetStreamNumAccounts() int {\n\tjs := s.getJetStream()\n\tif js == nil {\n\t\treturn 0\n\t}\n\tjs.mu.Lock()\n\tdefer js.mu.Unlock()\n\treturn len(js.accounts)\n}\n\n// JetStreamReservedResources returns the reserved resources if JetStream is enabled.\nfunc (s *Server) JetStreamReservedResources() (int64, int64, error) {\n\tjs := s.getJetStream()\n\tif js == nil {\n\t\treturn -1, -1, NewJSNotEnabledForAccountError()\n\t}\n\tjs.mu.RLock()\n\tdefer js.mu.RUnlock()\n\treturn js.memReserved, js.storeReserved, nil\n}\n\nfunc (s *Server) getJetStream() *jetStream {\n\treturn s.js.Load()\n}\n\nfunc (a *Account) assignJetStreamLimits(limits map[string]JetStreamAccountLimits) {\n\ta.mu.Lock()\n\ta.jsLimits = limits\n\ta.mu.Unlock()\n}\n\n// EnableJetStream will enable JetStream on this account with the defined limits.\n// This is a helper for JetStreamEnableAccount.\nfunc (a *Account) EnableJetStream(limits map[string]JetStreamAccountLimits) error {\n\ta.mu.RLock()\n\ts := a.srv\n\ta.mu.RUnlock()\n\n\tif s == nil {\n\t\treturn fmt.Errorf(\"jetstream account not registered\")\n\t}\n\n\tif s.SystemAccount() == a {\n\t\treturn fmt.Errorf(\"jetstream can not be enabled on the system account\")\n\t}\n\n\ts.mu.RLock()\n\tif s.sys == nil {\n\t\ts.mu.RUnlock()\n\t\treturn ErrServerNotRunning\n\t}\n\tsendq := s.sys.sendq\n\ts.mu.RUnlock()\n\n\t// No limits means we dynamically set up limits.\n\t// We also place limits here so we know that the account is configured for JetStream.\n\tif len(limits) == 0 {\n\t\tlimits = defaultJSAccountTiers\n\t}\n\n\ta.assignJetStreamLimits(limits)\n\n\tjs := s.getJetStream()\n\tif js == nil {\n\t\treturn NewJSNotEnabledError()\n\t}\n\n\tjs.mu.Lock()\n\tif jsa, ok := js.accounts[a.Name]; ok {\n\t\ta.mu.Lock()\n\t\ta.js = jsa\n\t\ta.mu.Unlock()\n\t\tjs.mu.Unlock()\n\t\treturn a.enableAllJetStreamServiceImportsAndMappings()\n\t}\n\n\t// Check the limits against existing reservations.\n\tif err := js.sufficientResources(limits); err != nil {\n\t\tjs.mu.Unlock()\n\t\treturn err\n\t}\n\n\tsysNode := s.Node()\n\n\tjsa := &jsAccount{js: js, account: a, limits: limits, streams: make(map[string]*stream), sendq: sendq, usage: make(map[string]*jsaStorage)}\n\tjsa.storeDir = filepath.Join(js.config.StoreDir, a.Name)\n\n\t// A single server does not need to do the account updates at this point.\n\tif js.cluster != nil || !s.standAloneMode() {\n\t\tjsa.usageMu.Lock()\n\t\tjsa.utimer = time.AfterFunc(usageTick, jsa.sendClusterUsageUpdateTimer)\n\t\t// Cluster mode updates to resource usage. System internal prevents echos.\n\t\tjsa.updatesPub = fmt.Sprintf(jsaUpdatesPubT, a.Name, sysNode)\n\t\tjsa.updatesSub, _ = s.sysSubscribe(fmt.Sprintf(jsaUpdatesSubT, a.Name), jsa.remoteUpdateUsage)\n\t\tjsa.usageMu.Unlock()\n\t}\n\n\tjs.accounts[a.Name] = jsa\n\t// Stamp inside account as well. Needs to be done under js's lock.\n\ta.mu.Lock()\n\ta.js = jsa\n\ta.mu.Unlock()\n\tjs.mu.Unlock()\n\n\t// Create the proper imports here.\n\tif err := a.enableAllJetStreamServiceImportsAndMappings(); err != nil {\n\t\treturn err\n\t}\n\n\ts.Debugf(\"Enabled JetStream for account %q\", a.Name)\n\tif l, ok := limits[_EMPTY_]; ok {\n\t\ts.Debugf(\"  Max Memory:      %s\", friendlyBytes(l.MaxMemory))\n\t\ts.Debugf(\"  Max Storage:     %s\", friendlyBytes(l.MaxStore))\n\t} else {\n\t\tfor t, l := range limits {\n\t\t\ts.Debugf(\"  Tier: %s\", t)\n\t\t\ts.Debugf(\"    Max Memory:      %s\", friendlyBytes(l.MaxMemory))\n\t\t\ts.Debugf(\"    Max Storage:     %s\", friendlyBytes(l.MaxStore))\n\t\t}\n\t}\n\n\t// Clean up any old snapshots that were orphaned while staging.\n\tos.RemoveAll(filepath.Join(js.config.StoreDir, snapStagingDir))\n\n\tsdir := filepath.Join(jsa.storeDir, streamsDir)\n\tif _, err := os.Stat(sdir); os.IsNotExist(err) {\n\t\tif err := os.MkdirAll(sdir, defaultDirPerms); err != nil {\n\t\t\treturn fmt.Errorf(\"could not create storage streams directory - %v\", err)\n\t\t}\n\t\t// Just need to make sure we can write to the directory.\n\t\t// Remove the directory will create later if needed.\n\t\tos.RemoveAll(sdir)\n\t\t// when empty remove parent directory, which may have been created as well\n\t\tos.Remove(jsa.storeDir)\n\t} else {\n\t\t// Restore any state here.\n\t\ts.Debugf(\"Recovering JetStream state for account %q\", a.Name)\n\t}\n\n\t// Check templates first since messsage sets will need proper ownership.\n\t// FIXME(dlc) - Make this consistent.\n\ttdir := filepath.Join(jsa.storeDir, tmplsDir)\n\tif stat, err := os.Stat(tdir); err == nil && stat.IsDir() {\n\t\tkey := sha256.Sum256([]byte(\"templates\"))\n\t\thh, err := highwayhash.New64(key[:])\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tfis, _ := os.ReadDir(tdir)\n\t\tfor _, fi := range fis {\n\t\t\tmetafile := filepath.Join(tdir, fi.Name(), JetStreamMetaFile)\n\t\t\tmetasum := filepath.Join(tdir, fi.Name(), JetStreamMetaFileSum)\n\t\t\tbuf, err := os.ReadFile(metafile)\n\t\t\tif err != nil {\n\t\t\t\ts.Warnf(\"  Error reading StreamTemplate metafile %q: %v\", metasum, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif _, err := os.Stat(metasum); os.IsNotExist(err) {\n\t\t\t\ts.Warnf(\"  Missing StreamTemplate checksum for %q\", metasum)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tsum, err := os.ReadFile(metasum)\n\t\t\tif err != nil {\n\t\t\t\ts.Warnf(\"  Error reading StreamTemplate checksum %q: %v\", metasum, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\thh.Reset()\n\t\t\thh.Write(buf)\n\t\t\tchecksum := hex.EncodeToString(hh.Sum(nil))\n\t\t\tif checksum != string(sum) {\n\t\t\t\ts.Warnf(\"  StreamTemplate checksums do not match %q vs %q\", sum, checksum)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tvar cfg StreamTemplateConfig\n\t\t\tif err := json.Unmarshal(buf, &cfg); err != nil {\n\t\t\t\ts.Warnf(\"  Error unmarshalling StreamTemplate metafile: %v\", err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tcfg.Config.Name = _EMPTY_\n\t\t\tif _, err := a.addStreamTemplate(&cfg); err != nil {\n\t\t\t\ts.Warnf(\"  Error recreating StreamTemplate %q: %v\", cfg.Name, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t}\n\t}\n\n\t// Collect consumers, do after all streams.\n\ttype ce struct {\n\t\tmset *stream\n\t\todir string\n\t}\n\tvar consumers []*ce\n\n\t// Collect any interest policy streams to check for\n\t// https://github.com/nats-io/nats-server/issues/3612\n\tvar ipstreams []*stream\n\n\t// Remember if we should be encrypted and what cipher we think we should use.\n\tencrypted := s.getOpts().JetStreamKey != _EMPTY_\n\tplaintext := true\n\tsc := s.getOpts().JetStreamCipher\n\n\t// Now recover the streams.\n\tfis, _ := os.ReadDir(sdir)\n\tfor _, fi := range fis {\n\t\tmdir := filepath.Join(sdir, fi.Name())\n\t\t// Check for partially deleted streams. They are marked with \".\" prefix.\n\t\tif strings.HasPrefix(fi.Name(), tsep) {\n\t\t\tgo os.RemoveAll(mdir)\n\t\t\tcontinue\n\t\t}\n\t\tkey := sha256.Sum256([]byte(fi.Name()))\n\t\thh, err := highwayhash.New64(key[:])\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tmetafile := filepath.Join(mdir, JetStreamMetaFile)\n\t\tmetasum := filepath.Join(mdir, JetStreamMetaFileSum)\n\t\tif _, err := os.Stat(metafile); os.IsNotExist(err) {\n\t\t\ts.Warnf(\"  Missing stream metafile for %q\", metafile)\n\t\t\tcontinue\n\t\t}\n\t\tbuf, err := os.ReadFile(metafile)\n\t\tif err != nil {\n\t\t\ts.Warnf(\"  Error reading metafile %q: %v\", metafile, err)\n\t\t\tcontinue\n\t\t}\n\t\tif _, err := os.Stat(metasum); os.IsNotExist(err) {\n\t\t\ts.Warnf(\"  Missing stream checksum file %q\", metasum)\n\t\t\tcontinue\n\t\t}\n\t\tsum, err := os.ReadFile(metasum)\n\t\tif err != nil {\n\t\t\ts.Warnf(\"  Error reading Stream metafile checksum %q: %v\", metasum, err)\n\t\t\tcontinue\n\t\t}\n\t\thh.Write(buf)\n\t\tchecksum := hex.EncodeToString(hh.Sum(nil))\n\t\tif checksum != string(sum) {\n\t\t\ts.Warnf(\"  Stream metafile %q: checksums do not match %q vs %q\", metafile, sum, checksum)\n\t\t\tcontinue\n\t\t}\n\n\t\t// Track if we are converting ciphers.\n\t\tvar convertingCiphers bool\n\n\t\t// Check if we are encrypted.\n\t\tkeyFile := filepath.Join(mdir, JetStreamMetaFileKey)\n\t\tkeyBuf, err := os.ReadFile(keyFile)\n\t\tif err == nil {\n\t\t\ts.Debugf(\"  Stream metafile is encrypted, reading encrypted keyfile\")\n\t\t\tif len(keyBuf) < minMetaKeySize {\n\t\t\t\ts.Warnf(\"  Bad stream encryption key length of %d\", len(keyBuf))\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\t// Decode the buffer before proceeding.\n\t\t\tvar nbuf []byte\n\t\t\tnbuf, convertingCiphers, err = s.decryptMeta(sc, keyBuf, buf, a.Name, fi.Name())\n\t\t\tif err != nil {\n\t\t\t\ts.Warnf(\"  Error decrypting our stream metafile: %v\", err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tbuf = nbuf\n\t\t\tplaintext = false\n\t\t}\n\n\t\tvar cfg FileStreamInfo\n\t\tif err := json.Unmarshal(buf, &cfg); err != nil {\n\t\t\ts.Warnf(\"  Error unmarshalling stream metafile %q: %v\", metafile, err)\n\t\t\tcontinue\n\t\t}\n\n\t\tif cfg.Template != _EMPTY_ {\n\t\t\tif err := jsa.addStreamNameToTemplate(cfg.Template, cfg.Name); err != nil {\n\t\t\t\ts.Warnf(\"  Error adding stream %q to template %q: %v\", cfg.Name, cfg.Template, err)\n\t\t\t}\n\t\t}\n\n\t\t// We had a bug that set a default de dupe window on mirror, despite that being not a valid config\n\t\tfixCfgMirrorWithDedupWindow(&cfg.StreamConfig)\n\n\t\t// We had a bug that could allow subjects in that had prefix or suffix spaces. We check for that here\n\t\t// and will patch them on the fly for now. We will warn about them.\n\t\tvar hadSubjErr bool\n\t\tfor i, subj := range cfg.StreamConfig.Subjects {\n\t\t\tif !IsValidSubject(subj) {\n\t\t\t\ts.Warnf(\"  Detected bad subject %q while adding stream %q, will attempt to repair\", subj, cfg.Name)\n\t\t\t\tif nsubj := strings.TrimSpace(subj); IsValidSubject(nsubj) {\n\t\t\t\t\ts.Warnf(\"  Bad subject %q repaired to %q\", subj, nsubj)\n\t\t\t\t\tcfg.StreamConfig.Subjects[i] = nsubj\n\t\t\t\t} else {\n\t\t\t\t\ts.Warnf(\"  Error recreating stream %q: %v\", cfg.Name, \"invalid subject\")\n\t\t\t\t\thadSubjErr = true\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tif hadSubjErr {\n\t\t\tcontinue\n\t\t}\n\n\t\t// The other possible bug is assigning subjects to mirrors, so check for that and patch as well.\n\t\tif cfg.StreamConfig.Mirror != nil && len(cfg.StreamConfig.Subjects) > 0 {\n\t\t\ts.Warnf(\"  Detected subjects on a mirrored stream %q, will remove\", cfg.Name)\n\t\t\tcfg.StreamConfig.Subjects = nil\n\t\t}\n\n\t\ts.Noticef(\"  Starting restore for stream '%s > %s'\", a.Name, cfg.StreamConfig.Name)\n\t\trt := time.Now()\n\n\t\t// Log if we are converting from plaintext to encrypted.\n\t\tif encrypted {\n\t\t\tif plaintext {\n\t\t\t\ts.Noticef(\"  Encrypting stream '%s > %s'\", a.Name, cfg.StreamConfig.Name)\n\t\t\t} else if convertingCiphers {\n\t\t\t\ts.Noticef(\"  Converting to %s for stream '%s > %s'\", sc, a.Name, cfg.StreamConfig.Name)\n\t\t\t\t// Remove the key file to have system regenerate with the new cipher.\n\t\t\t\tos.Remove(keyFile)\n\t\t\t}\n\t\t}\n\n\t\t// Add in the stream.\n\t\tmset, err := a.addStream(&cfg.StreamConfig)\n\t\tif err != nil {\n\t\t\ts.Warnf(\"  Error recreating stream %q: %v\", cfg.Name, err)\n\t\t\t// If we removed a keyfile from above make sure to put it back.\n\t\t\tif convertingCiphers {\n\t\t\t\terr := os.WriteFile(keyFile, keyBuf, defaultFilePerms)\n\t\t\t\tif err != nil {\n\t\t\t\t\ts.Warnf(\"  Error replacing meta keyfile for stream %q: %v\", cfg.Name, err)\n\t\t\t\t}\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\t\tif !cfg.Created.IsZero() {\n\t\t\tmset.setCreatedTime(cfg.Created)\n\t\t}\n\n\t\tstate := mset.state()\n\t\ts.Noticef(\"  Restored %s messages for stream '%s > %s' in %v\",\n\t\t\tcomma(int64(state.Msgs)), mset.accName(), mset.name(), time.Since(rt).Round(time.Millisecond))\n\n\t\t// Collect to check for dangling messages.\n\t\t// TODO(dlc) - Can be removed eventually.\n\t\tif cfg.StreamConfig.Retention == InterestPolicy {\n\t\t\tipstreams = append(ipstreams, mset)\n\t\t}\n\n\t\t// Now do the consumers.\n\t\todir := filepath.Join(sdir, fi.Name(), consumerDir)\n\t\tconsumers = append(consumers, &ce{mset, odir})\n\t}\n\n\tfor _, e := range consumers {\n\t\tofis, _ := os.ReadDir(e.odir)\n\t\tif len(ofis) > 0 {\n\t\t\ts.Noticef(\"  Recovering %d consumers for stream - '%s > %s'\", len(ofis), e.mset.accName(), e.mset.name())\n\t\t}\n\t\tfor _, ofi := range ofis {\n\t\t\tmetafile := filepath.Join(e.odir, ofi.Name(), JetStreamMetaFile)\n\t\t\tmetasum := filepath.Join(e.odir, ofi.Name(), JetStreamMetaFileSum)\n\t\t\tif _, err := os.Stat(metafile); os.IsNotExist(err) {\n\t\t\t\ts.Warnf(\"    Missing consumer metafile %q\", metafile)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tbuf, err := os.ReadFile(metafile)\n\t\t\tif err != nil {\n\t\t\t\ts.Warnf(\"    Error reading consumer metafile %q: %v\", metafile, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif _, err := os.Stat(metasum); os.IsNotExist(err) {\n\t\t\t\ts.Warnf(\"    Missing consumer checksum for %q\", metasum)\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// Check if we are encrypted.\n\t\t\tif key, err := os.ReadFile(filepath.Join(e.odir, ofi.Name(), JetStreamMetaFileKey)); err == nil {\n\t\t\t\ts.Debugf(\"  Consumer metafile is encrypted, reading encrypted keyfile\")\n\t\t\t\t// Decode the buffer before proceeding.\n\t\t\t\tctxName := e.mset.name() + tsep + ofi.Name()\n\t\t\t\tnbuf, _, err := s.decryptMeta(sc, key, buf, a.Name, ctxName)\n\t\t\t\tif err != nil {\n\t\t\t\t\ts.Warnf(\"  Error decrypting our consumer metafile: %v\", err)\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tbuf = nbuf\n\t\t\t}\n\n\t\t\tvar cfg FileConsumerInfo\n\t\t\tif err := json.Unmarshal(buf, &cfg); err != nil {\n\t\t\t\ts.Warnf(\"    Error unmarshalling consumer metafile %q: %v\", metafile, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tisEphemeral := !isDurableConsumer(&cfg.ConsumerConfig)\n\t\t\tif isEphemeral {\n\t\t\t\t// This is an ephermal consumer and this could fail on restart until\n\t\t\t\t// the consumer can reconnect. We will create it as a durable and switch it.\n\t\t\t\tcfg.ConsumerConfig.Durable = ofi.Name()\n\t\t\t}\n\t\t\tobs, err := e.mset.addConsumerWithAssignment(&cfg.ConsumerConfig, _EMPTY_, nil, true, ActionCreateOrUpdate, false)\n\t\t\tif err != nil {\n\t\t\t\ts.Warnf(\"    Error adding consumer %q: %v\", cfg.Name, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif isEphemeral {\n\t\t\t\tobs.switchToEphemeral()\n\t\t\t}\n\t\t\tif !cfg.Created.IsZero() {\n\t\t\t\tobs.setCreatedTime(cfg.Created)\n\t\t\t}\n\t\t\tif err != nil {\n\t\t\t\ts.Warnf(\"    Error restoring consumer %q state: %v\", cfg.Name, err)\n\t\t\t}\n\t\t}\n\t}\n\n\t// Make sure to cleanup any old remaining snapshots.\n\tos.RemoveAll(filepath.Join(jsa.storeDir, snapsDir))\n\n\t// Check interest policy streams for auto cleanup.\n\tfor _, mset := range ipstreams {\n\t\tmset.checkForOrphanMsgs()\n\t\tmset.checkConsumerReplication()\n\t}\n\n\ts.Debugf(\"JetStream state for account %q recovered\", a.Name)\n\n\treturn nil\n}\n\n// Return whether we require MaxBytes to be set and if > 0 an upper limit for stream size exists\n// Both limits are independent of each other.\nfunc (a *Account) maxBytesLimits(cfg *StreamConfig) (bool, int64) {\n\ta.mu.RLock()\n\tjsa := a.js\n\ta.mu.RUnlock()\n\tif jsa == nil {\n\t\treturn false, 0\n\t}\n\tjsa.usageMu.RLock()\n\tvar replicas int\n\tif cfg != nil {\n\t\treplicas = cfg.Replicas\n\t}\n\tselectedLimits, _, ok := jsa.selectLimits(replicas)\n\tjsa.usageMu.RUnlock()\n\tif !ok {\n\t\treturn false, 0\n\t}\n\tmaxStreamBytes := int64(0)\n\tif cfg.Storage == MemoryStorage {\n\t\tmaxStreamBytes = selectedLimits.MemoryMaxStreamBytes\n\t} else {\n\t\tmaxStreamBytes = selectedLimits.StoreMaxStreamBytes\n\t}\n\treturn selectedLimits.MaxBytesRequired, maxStreamBytes\n}\n\n// NumStreams will return how many streams we have.\nfunc (a *Account) numStreams() int {\n\ta.mu.RLock()\n\tjsa := a.js\n\ta.mu.RUnlock()\n\tif jsa == nil {\n\t\treturn 0\n\t}\n\tjsa.mu.Lock()\n\tn := len(jsa.streams)\n\tjsa.mu.Unlock()\n\treturn n\n}\n\n// Streams will return all known streams.\nfunc (a *Account) streams() []*stream {\n\treturn a.filteredStreams(_EMPTY_)\n}\n\nfunc (a *Account) filteredStreams(filter string) []*stream {\n\ta.mu.RLock()\n\tjsa := a.js\n\ta.mu.RUnlock()\n\n\tif jsa == nil {\n\t\treturn nil\n\t}\n\n\tjsa.mu.RLock()\n\tdefer jsa.mu.RUnlock()\n\n\tvar msets []*stream\n\tfor _, mset := range jsa.streams {\n\t\tif filter != _EMPTY_ {\n\t\t\tmset.cfgMu.RLock()\n\t\t\tfor _, subj := range mset.cfg.Subjects {\n\t\t\t\tif SubjectsCollide(filter, subj) {\n\t\t\t\t\tmsets = append(msets, mset)\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t\tmset.cfgMu.RUnlock()\n\t\t} else {\n\t\t\tmsets = append(msets, mset)\n\t\t}\n\t}\n\n\treturn msets\n}\n\n// lookupStream will lookup a stream by name.\nfunc (a *Account) lookupStream(name string) (*stream, error) {\n\ta.mu.RLock()\n\tjsa := a.js\n\ta.mu.RUnlock()\n\n\tif jsa == nil {\n\t\treturn nil, NewJSNotEnabledForAccountError()\n\t}\n\tjsa.mu.RLock()\n\tdefer jsa.mu.RUnlock()\n\n\tmset, ok := jsa.streams[name]\n\tif !ok {\n\t\treturn nil, NewJSStreamNotFoundError()\n\t}\n\treturn mset, nil\n}\n\n// UpdateJetStreamLimits will update the account limits for a JetStream enabled account.\nfunc (a *Account) UpdateJetStreamLimits(limits map[string]JetStreamAccountLimits) error {\n\ta.mu.RLock()\n\ts, jsa := a.srv, a.js\n\ta.mu.RUnlock()\n\n\tif s == nil {\n\t\treturn fmt.Errorf(\"jetstream account not registered\")\n\t}\n\tjs := s.getJetStream()\n\tif js == nil {\n\t\treturn NewJSNotEnabledError()\n\t}\n\tif jsa == nil {\n\t\treturn NewJSNotEnabledForAccountError()\n\t}\n\n\tif len(limits) == 0 {\n\t\tlimits = defaultJSAccountTiers\n\t}\n\n\t// Calculate the delta between what we have and what we want.\n\tjsa.usageMu.RLock()\n\tdl := diffCheckedLimits(jsa.limits, limits)\n\tjsa.usageMu.RUnlock()\n\n\tjs.mu.Lock()\n\t// Check the limits against existing reservations.\n\tif err := js.sufficientResources(dl); err != nil {\n\t\tjs.mu.Unlock()\n\t\treturn err\n\t}\n\tjs.mu.Unlock()\n\n\t// Update\n\tjsa.usageMu.Lock()\n\tjsa.limits = limits\n\tjsa.usageMu.Unlock()\n\n\treturn nil\n}\n\nfunc diffCheckedLimits(a, b map[string]JetStreamAccountLimits) map[string]JetStreamAccountLimits {\n\tdiff := map[string]JetStreamAccountLimits{}\n\tfor t, la := range a {\n\t\t// in a, not in b will return 0\n\t\tlb := b[t]\n\t\tdiff[t] = JetStreamAccountLimits{\n\t\t\tMaxMemory: lb.MaxMemory - la.MaxMemory,\n\t\t\tMaxStore:  lb.MaxStore - la.MaxStore,\n\t\t}\n\t}\n\tfor t, lb := range b {\n\t\tif la, ok := a[t]; !ok {\n\t\t\t// only in b not in a. (in a and b already covered)\n\t\t\tdiff[t] = JetStreamAccountLimits{\n\t\t\t\tMaxMemory: lb.MaxMemory - la.MaxMemory,\n\t\t\t\tMaxStore:  lb.MaxStore - la.MaxStore,\n\t\t\t}\n\t\t}\n\t}\n\treturn diff\n}\n\n// Return reserved bytes for memory and store for this account on this server.\n// Lock should be held.\nfunc (jsa *jsAccount) reservedStorage(tier string) (mem, store uint64) {\n\tfor _, mset := range jsa.streams {\n\t\tcfg := &mset.cfg\n\t\tif tier == _EMPTY_ || tier == tierName(cfg.Replicas) && cfg.MaxBytes > 0 {\n\t\t\tswitch cfg.Storage {\n\t\t\tcase FileStorage:\n\t\t\t\tstore += uint64(cfg.MaxBytes)\n\t\t\tcase MemoryStorage:\n\t\t\t\tmem += uint64(cfg.MaxBytes)\n\t\t\t}\n\t\t}\n\t}\n\treturn mem, store\n}\n\n// Return reserved bytes for memory and store for this account in clustered mode.\n// js lock should be held.\nfunc reservedStorage(sas map[string]*streamAssignment, tier string) (mem, store uint64) {\n\tfor _, sa := range sas {\n\t\tcfg := sa.Config\n\t\tif tier == _EMPTY_ || tier == tierName(cfg.Replicas) && cfg.MaxBytes > 0 {\n\t\t\tswitch cfg.Storage {\n\t\t\tcase FileStorage:\n\t\t\t\tstore += uint64(cfg.MaxBytes)\n\t\t\tcase MemoryStorage:\n\t\t\t\tmem += uint64(cfg.MaxBytes)\n\t\t\t}\n\t\t}\n\t}\n\treturn mem, store\n}\n\n// JetStreamUsage reports on JetStream usage and limits for an account.\nfunc (a *Account) JetStreamUsage() JetStreamAccountStats {\n\ta.mu.RLock()\n\tjsa, aname := a.js, a.Name\n\taccJsLimits := a.jsLimits\n\ta.mu.RUnlock()\n\n\tvar stats JetStreamAccountStats\n\tif jsa != nil {\n\t\tjs := jsa.js\n\t\tjs.mu.RLock()\n\t\tcc := js.cluster\n\t\tsingleServer := cc == nil\n\t\tjsa.mu.RLock()\n\t\tjsa.usageMu.RLock()\n\t\tstats.Memory, stats.Store = jsa.storageTotals()\n\t\tstats.Domain = js.config.Domain\n\t\tstats.API = JetStreamAPIStats{\n\t\t\tLevel:  JSApiLevel,\n\t\t\tTotal:  jsa.apiTotal,\n\t\t\tErrors: jsa.apiErrors,\n\t\t}\n\t\tif singleServer {\n\t\t\tstats.ReservedMemory, stats.ReservedStore = jsa.reservedStorage(_EMPTY_)\n\t\t} else {\n\t\t\tstats.ReservedMemory, stats.ReservedStore = reservedStorage(cc.streams[aname], _EMPTY_)\n\t\t}\n\t\tl, defaultTier := jsa.limits[_EMPTY_]\n\t\tif defaultTier {\n\t\t\tstats.Limits = l\n\t\t} else {\n\t\t\tskipped := 0\n\t\t\tstats.Tiers = make(map[string]JetStreamTier)\n\t\t\tfor t, total := range jsa.usage {\n\t\t\t\tif _, ok := jsa.limits[t]; !ok && (*total) == (jsaStorage{}) {\n\t\t\t\t\t// skip tiers not present that don't contain a count\n\t\t\t\t\t// In case this shows an empty stream, that tier will be added when iterating over streams\n\t\t\t\t\tskipped++\n\t\t\t\t} else {\n\t\t\t\t\ttier := JetStreamTier{\n\t\t\t\t\t\tMemory: uint64(total.total.mem),\n\t\t\t\t\t\tStore:  uint64(total.total.store),\n\t\t\t\t\t\tLimits: jsa.limits[t],\n\t\t\t\t\t}\n\t\t\t\t\tif singleServer {\n\t\t\t\t\t\ttier.ReservedMemory, tier.ReservedStore = jsa.reservedStorage(t)\n\t\t\t\t\t} else {\n\t\t\t\t\t\ttier.ReservedMemory, tier.ReservedStore = reservedStorage(cc.streams[aname], t)\n\t\t\t\t\t}\n\t\t\t\t\tstats.Tiers[t] = tier\n\t\t\t\t}\n\t\t\t}\n\t\t\tif len(accJsLimits) != len(jsa.usage)-skipped {\n\t\t\t\t// insert unused limits\n\t\t\t\tfor t, lim := range accJsLimits {\n\t\t\t\t\tif _, ok := stats.Tiers[t]; !ok {\n\t\t\t\t\t\ttier := JetStreamTier{Limits: lim}\n\t\t\t\t\t\tif singleServer {\n\t\t\t\t\t\t\ttier.ReservedMemory, tier.ReservedStore = jsa.reservedStorage(t)\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\ttier.ReservedMemory, tier.ReservedStore = reservedStorage(cc.streams[aname], t)\n\t\t\t\t\t\t}\n\t\t\t\t\t\tstats.Tiers[t] = tier\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tjsa.usageMu.RUnlock()\n\n\t\t// Clustered\n\t\tif cc := js.cluster; cc != nil {\n\t\t\tsas := cc.streams[aname]\n\t\t\tif defaultTier {\n\t\t\t\tstats.Streams = len(sas)\n\t\t\t\tstats.ReservedMemory, stats.ReservedStore = reservedStorage(sas, _EMPTY_)\n\t\t\t}\n\t\t\tfor _, sa := range sas {\n\t\t\t\tif defaultTier {\n\t\t\t\t\tstats.Consumers += len(sa.consumers)\n\t\t\t\t} else {\n\t\t\t\t\tstats.Streams++\n\t\t\t\t\tstreamTier := tierName(sa.Config.Replicas)\n\t\t\t\t\tsu, ok := stats.Tiers[streamTier]\n\t\t\t\t\tif !ok {\n\t\t\t\t\t\tsu = JetStreamTier{}\n\t\t\t\t\t}\n\t\t\t\t\tsu.Streams++\n\t\t\t\t\tstats.Tiers[streamTier] = su\n\n\t\t\t\t\t// Now consumers, check each since could be different tiers.\n\t\t\t\t\tfor _, ca := range sa.consumers {\n\t\t\t\t\t\tstats.Consumers++\n\t\t\t\t\t\tconsumerTier := tierName(ca.Config.replicas(sa.Config))\n\t\t\t\t\t\tcu, ok := stats.Tiers[consumerTier]\n\t\t\t\t\t\tif !ok {\n\t\t\t\t\t\t\tcu = JetStreamTier{}\n\t\t\t\t\t\t}\n\t\t\t\t\t\tcu.Consumers++\n\t\t\t\t\t\tstats.Tiers[consumerTier] = cu\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t} else {\n\t\t\tif defaultTier {\n\t\t\t\tstats.Streams = len(jsa.streams)\n\t\t\t}\n\t\t\tfor _, mset := range jsa.streams {\n\t\t\t\tconsCount := mset.numConsumers()\n\t\t\t\tstats.Consumers += consCount\n\t\t\t\tif !defaultTier {\n\t\t\t\t\tu, ok := stats.Tiers[mset.tier]\n\t\t\t\t\tif !ok {\n\t\t\t\t\t\tu = JetStreamTier{}\n\t\t\t\t\t}\n\t\t\t\t\tu.Streams++\n\t\t\t\t\tstats.Streams++\n\t\t\t\t\tu.Consumers += consCount\n\t\t\t\t\tstats.Tiers[mset.tier] = u\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tjsa.mu.RUnlock()\n\t\tjs.mu.RUnlock()\n\t}\n\treturn stats\n}\n\n// DisableJetStream will disable JetStream for this account.\nfunc (a *Account) DisableJetStream() error {\n\treturn a.removeJetStream()\n}\n\n// removeJetStream is called when JetStream has been disabled for this account.\nfunc (a *Account) removeJetStream() error {\n\ta.mu.Lock()\n\ts := a.srv\n\ta.js = nil\n\ta.mu.Unlock()\n\n\tif s == nil {\n\t\treturn fmt.Errorf(\"jetstream account not registered\")\n\t}\n\n\tjs := s.getJetStream()\n\tif js == nil {\n\t\treturn NewJSNotEnabledForAccountError()\n\t}\n\n\treturn js.disableJetStream(js.lookupAccount(a))\n}\n\n// Disable JetStream for the account.\nfunc (js *jetStream) disableJetStream(jsa *jsAccount) error {\n\tif jsa == nil || jsa.account == nil {\n\t\treturn NewJSNotEnabledForAccountError()\n\t}\n\n\tjs.mu.Lock()\n\tdelete(js.accounts, jsa.account.Name)\n\tjs.mu.Unlock()\n\n\tjsa.delete()\n\n\treturn nil\n}\n\n// jetStreamConfigured reports whether the account has JetStream configured, regardless of this\n// servers JetStream status.\nfunc (a *Account) jetStreamConfigured() bool {\n\tif a == nil {\n\t\treturn false\n\t}\n\ta.mu.RLock()\n\tdefer a.mu.RUnlock()\n\treturn len(a.jsLimits) > 0\n}\n\n// JetStreamEnabled is a helper to determine if jetstream is enabled for an account.\nfunc (a *Account) JetStreamEnabled() bool {\n\tif a == nil {\n\t\treturn false\n\t}\n\ta.mu.RLock()\n\tenabled := a.js != nil\n\ta.mu.RUnlock()\n\treturn enabled\n}\n\nfunc (jsa *jsAccount) remoteUpdateUsage(sub *subscription, c *client, _ *Account, subject, _ string, msg []byte) {\n\t// jsa.js.srv is immutable and guaranteed to no be nil, so no lock needed.\n\ts := jsa.js.srv\n\n\tjsa.usageMu.Lock()\n\tdefer jsa.usageMu.Unlock()\n\n\tif len(msg) < minUsageUpdateLen {\n\t\ts.Warnf(\"Ignoring remote usage update with size too short\")\n\t\treturn\n\t}\n\tvar rnode string\n\tif li := strings.LastIndexByte(subject, btsep); li > 0 && li < len(subject) {\n\t\trnode = subject[li+1:]\n\t}\n\tif rnode == _EMPTY_ {\n\t\ts.Warnf(\"Received remote usage update with no remote node\")\n\t\treturn\n\t}\n\trUsage, ok := jsa.rusage[rnode]\n\tif !ok {\n\t\tif jsa.rusage == nil {\n\t\t\tjsa.rusage = make(map[string]*remoteUsage)\n\t\t}\n\t\trUsage = &remoteUsage{tiers: make(map[string]*jsaUsage)}\n\t\tjsa.rusage[rnode] = rUsage\n\t}\n\tupdateTotal := func(tierName string, memUsed, storeUsed int64) {\n\t\ttotal, ok := jsa.usage[tierName]\n\t\tif !ok {\n\t\t\ttotal = &jsaStorage{}\n\t\t\tjsa.usage[tierName] = total\n\t\t}\n\t\t// Update the usage for this remote.\n\t\tif usage := rUsage.tiers[tierName]; usage != nil {\n\t\t\t// Decrement our old values.\n\t\t\ttotal.total.mem -= usage.mem\n\t\t\ttotal.total.store -= usage.store\n\t\t\tusage.mem, usage.store = memUsed, storeUsed\n\t\t} else {\n\t\t\trUsage.tiers[tierName] = &jsaUsage{memUsed, storeUsed}\n\t\t}\n\t\ttotal.total.mem += memUsed\n\t\ttotal.total.store += storeUsed\n\t}\n\n\tvar le = binary.LittleEndian\n\tapiTotal, apiErrors := le.Uint64(msg[16:]), le.Uint64(msg[24:])\n\tmemUsed, storeUsed := int64(le.Uint64(msg[0:])), int64(le.Uint64(msg[8:]))\n\n\t// We later extended the data structure to support multiple tiers\n\tvar excessRecordCnt uint32\n\tvar tierName string\n\n\tif len(msg) >= usageMultiTiersLen {\n\t\texcessRecordCnt = le.Uint32(msg[minUsageUpdateLen:])\n\t\tlength := le.Uint64(msg[minUsageUpdateLen+4:])\n\t\t// Need to protect past this point in case this is wrong.\n\t\tif uint64(len(msg)) < usageMultiTiersLen+length {\n\t\t\ts.Warnf(\"Received corrupt remote usage update\")\n\t\t\treturn\n\t\t}\n\t\ttierName = string(msg[usageMultiTiersLen : usageMultiTiersLen+length])\n\t\tmsg = msg[usageMultiTiersLen+length:]\n\t}\n\tupdateTotal(tierName, memUsed, storeUsed)\n\tfor ; excessRecordCnt > 0 && len(msg) >= usageRecordLen; excessRecordCnt-- {\n\t\tmemUsed, storeUsed := int64(le.Uint64(msg[0:])), int64(le.Uint64(msg[8:]))\n\t\tlength := le.Uint64(msg[16:])\n\t\tif uint64(len(msg)) < usageRecordLen+length {\n\t\t\ts.Warnf(\"Received corrupt remote usage update on excess record\")\n\t\t\treturn\n\t\t}\n\t\ttierName = string(msg[usageRecordLen : usageRecordLen+length])\n\t\tmsg = msg[usageRecordLen+length:]\n\t\tupdateTotal(tierName, memUsed, storeUsed)\n\t}\n\tjsa.apiTotal -= rUsage.api\n\tjsa.apiErrors -= rUsage.err\n\trUsage.api = apiTotal\n\trUsage.err = apiErrors\n\tjsa.apiTotal += apiTotal\n\tjsa.apiErrors += apiErrors\n}\n\n// When we detect a skew of some sort this will verify the usage reporting is correct.\n// No locks should be held.\nfunc (jsa *jsAccount) checkAndSyncUsage(tierName string, storeType StorageType) {\n\t// This will run in a separate go routine, so check that we are only running once.\n\tif !jsa.sync.CompareAndSwap(false, true) {\n\t\treturn\n\t}\n\tdefer jsa.sync.Store(false)\n\n\t// Hold the account read lock and the usage lock while we calculate.\n\t// We scope by tier and storage type, but if R3 File has 200 streams etc. could\n\t// show a pause. I did test with > 100 non-active streams and was 80-200ns or so.\n\t// Should be rare this gets called as well.\n\tjsa.mu.RLock()\n\tdefer jsa.mu.RUnlock()\n\tjs := jsa.js\n\tif js == nil {\n\t\treturn\n\t}\n\ts := js.srv\n\n\t// We need to collect the stream stores before we acquire the usage lock since in storeUpdates the\n\t// stream lock could be held if deletion are inline with storing a new message, e.g. via limits.\n\tvar stores []StreamStore\n\tfor _, mset := range jsa.streams {\n\t\tmset.mu.RLock()\n\t\tif mset.tier == tierName && mset.stype == storeType && mset.store != nil {\n\t\t\tstores = append(stores, mset.store)\n\t\t}\n\t\tmset.mu.RUnlock()\n\t}\n\n\t// Now range and qualify, hold usage lock to prevent updates.\n\tjsa.usageMu.Lock()\n\tdefer jsa.usageMu.Unlock()\n\n\tusage, ok := jsa.usage[tierName]\n\tif !ok {\n\t\treturn\n\t}\n\n\t// Collect current total for all stream stores that matched.\n\tvar total int64\n\tvar state StreamState\n\tfor _, store := range stores {\n\t\tstore.FastState(&state)\n\t\ttotal += int64(state.Bytes)\n\t}\n\n\tvar needClusterUpdate bool\n\t// If we do not match on our calculations compute delta and adjust.\n\tif storeType == MemoryStorage {\n\t\tif total != usage.local.mem {\n\t\t\ts.Warnf(\"MemStore usage drift of %v vs %v detected for account %q\",\n\t\t\t\tfriendlyBytes(total), friendlyBytes(usage.local.mem), jsa.account.GetName())\n\t\t\tdelta := total - usage.local.mem\n\t\t\tusage.local.mem += delta\n\t\t\tusage.total.mem += delta\n\t\t\tatomic.AddInt64(&js.memUsed, delta)\n\t\t\tneedClusterUpdate = true\n\t\t}\n\t} else {\n\t\tif total != usage.local.store {\n\t\t\ts.Warnf(\"FileStore usage drift of %v vs %v detected for account %q\",\n\t\t\t\tfriendlyBytes(total), friendlyBytes(usage.local.store), jsa.account.GetName())\n\t\t\tdelta := total - usage.local.store\n\t\t\tusage.local.store += delta\n\t\t\tusage.total.store += delta\n\t\t\tatomic.AddInt64(&js.storeUsed, delta)\n\t\t\tneedClusterUpdate = true\n\t\t}\n\t}\n\n\t// Publish our local updates if in clustered mode.\n\tif needClusterUpdate && js.isClusteredNoLock() {\n\t\tjsa.sendClusterUsageUpdate()\n\t}\n}\n\n// Updates accounting on in use memory and storage. This is called from locally\n// by the lower storage layers.\nfunc (jsa *jsAccount) updateUsage(tierName string, storeType StorageType, delta int64) {\n\t// jsa.js is immutable and cannot be nil, so ok w/o lock.\n\tjs := jsa.js\n\t// updateUsage() may be invoked under the mset's lock, so we can't get\n\t// the js' lock to check if clustered. So use this function that make\n\t// use of an atomic to do the check without having data race reports.\n\tisClustered := js.isClusteredNoLock()\n\n\tvar needsCheck bool\n\tjsa.usageMu.Lock()\n\ts, ok := jsa.usage[tierName]\n\tif !ok {\n\t\ts = &jsaStorage{}\n\t\tjsa.usage[tierName] = s\n\t}\n\tif storeType == MemoryStorage {\n\t\ts.local.mem += delta\n\t\ts.total.mem += delta\n\t\tatomic.AddInt64(&js.memUsed, delta)\n\t\tneedsCheck = s.local.mem < 0\n\t} else {\n\t\ts.local.store += delta\n\t\ts.total.store += delta\n\t\tatomic.AddInt64(&js.storeUsed, delta)\n\t\tneedsCheck = s.local.store < 0\n\t}\n\t// Publish our local updates if in clustered mode.\n\tif isClustered {\n\t\tjsa.sendClusterUsageUpdate()\n\t}\n\tjsa.usageMu.Unlock()\n\n\tif needsCheck {\n\t\t// We could be holding the stream lock from up in the stack, and this\n\t\t// will want the jsa lock, which would violate locking order.\n\t\t// So do this in a Go routine. The function will check if it is already running.\n\t\tgo jsa.checkAndSyncUsage(tierName, storeType)\n\t}\n}\n\nvar usageTick = 1500 * time.Millisecond\n\nfunc (jsa *jsAccount) sendClusterUsageUpdateTimer() {\n\tjsa.usageMu.Lock()\n\tdefer jsa.usageMu.Unlock()\n\tjsa.sendClusterUsageUpdate()\n\tif jsa.utimer != nil {\n\t\tjsa.utimer.Reset(usageTick)\n\t}\n}\n\n// For usage fields.\nconst (\n\tminUsageUpdateLen    = 32\n\tstackUsageUpdate     = 72\n\tusageRecordLen       = 24\n\tusageMultiTiersLen   = 44\n\tapiStatsAndNumTiers  = 20\n\tminUsageUpdateWindow = 250 * time.Millisecond\n)\n\n// Send updates to our account usage for this server.\n// jsa.usageMu lock should be held.\nfunc (jsa *jsAccount) sendClusterUsageUpdate() {\n\t// These values are absolute so we can limit send rates.\n\tnow := time.Now()\n\tif now.Sub(jsa.lupdate) < minUsageUpdateWindow {\n\t\treturn\n\t}\n\tjsa.lupdate = now\n\n\tlenUsage := len(jsa.usage)\n\tif lenUsage == 0 {\n\t\treturn\n\t}\n\t// every base record contains mem/store/len(tier) as well as the tier name\n\tl := usageRecordLen * lenUsage\n\tfor tier := range jsa.usage {\n\t\tl += len(tier)\n\t}\n\t// first record contains api/usage errors as well as count for extra base records\n\tl += apiStatsAndNumTiers\n\n\tvar raw [stackUsageUpdate]byte\n\tvar b []byte\n\tif l > stackUsageUpdate {\n\t\tb = make([]byte, l)\n\t} else {\n\t\tb = raw[:l]\n\t}\n\n\tvar i int\n\tvar le = binary.LittleEndian\n\tfor tier, usage := range jsa.usage {\n\t\tle.PutUint64(b[i+0:], uint64(usage.local.mem))\n\t\tle.PutUint64(b[i+8:], uint64(usage.local.store))\n\t\tif i == 0 {\n\t\t\tle.PutUint64(b[16:], jsa.usageApi)\n\t\t\tle.PutUint64(b[24:], jsa.usageErr)\n\t\t\tle.PutUint32(b[32:], uint32(len(jsa.usage)-1))\n\t\t\tle.PutUint64(b[36:], uint64(len(tier)))\n\t\t\tcopy(b[usageMultiTiersLen:], tier)\n\t\t\ti = usageMultiTiersLen + len(tier)\n\t\t} else {\n\t\t\tle.PutUint64(b[i+16:], uint64(len(tier)))\n\t\t\tcopy(b[i+usageRecordLen:], tier)\n\t\t\ti += usageRecordLen + len(tier)\n\t\t}\n\t}\n\tjsa.sendq.push(newPubMsg(nil, jsa.updatesPub, _EMPTY_, nil, nil, b, noCompression, false, false))\n}\n\nfunc (js *jetStream) wouldExceedLimits(storeType StorageType, sz int) bool {\n\tvar (\n\t\ttotal *int64\n\t\tmax   int64\n\t)\n\tif storeType == MemoryStorage {\n\t\ttotal, max = &js.memUsed, js.config.MaxMemory\n\t} else {\n\t\ttotal, max = &js.storeUsed, js.config.MaxStore\n\t}\n\treturn (atomic.LoadInt64(total) + int64(sz)) > max\n}\n\nfunc (js *jetStream) limitsExceeded(storeType StorageType) bool {\n\treturn js.wouldExceedLimits(storeType, 0)\n}\n\nfunc tierName(replicas int) string {\n\t// TODO (mh) this is where we could select based off a placement tag as well \"qos:tier\"\n\tif replicas == 0 {\n\t\treplicas = 1\n\t}\n\treturn fmt.Sprintf(\"R%d\", replicas)\n}\n\nfunc isSameTier(cfgA, cfgB *StreamConfig) bool {\n\t// TODO (mh) this is where we could select based off a placement tag as well \"qos:tier\"\n\treturn cfgA.Replicas == cfgB.Replicas\n}\n\nfunc (jsa *jsAccount) jetStreamAndClustered() (*jetStream, bool) {\n\tjsa.mu.RLock()\n\tjs := jsa.js\n\tjsa.mu.RUnlock()\n\treturn js, js.isClustered()\n}\n\n// jsa.usageMu read lock should be held.\nfunc (jsa *jsAccount) selectLimits(replicas int) (JetStreamAccountLimits, string, bool) {\n\tif selectedLimits, ok := jsa.limits[_EMPTY_]; ok {\n\t\treturn selectedLimits, _EMPTY_, true\n\t}\n\ttier := tierName(replicas)\n\tif selectedLimits, ok := jsa.limits[tier]; ok {\n\t\treturn selectedLimits, tier, true\n\t}\n\treturn JetStreamAccountLimits{}, _EMPTY_, false\n}\n\n// Lock should be held.\nfunc (jsa *jsAccount) countStreams(tier string, cfg *StreamConfig) (streams int) {\n\tfor _, sa := range jsa.streams {\n\t\t// Don't count the stream toward the limit if it already exists.\n\t\tif (tier == _EMPTY_ || isSameTier(&sa.cfg, cfg)) && sa.cfg.Name != cfg.Name {\n\t\t\tstreams++\n\t\t}\n\t}\n\treturn streams\n}\n\n// jsa.usageMu read lock (at least) should be held.\nfunc (jsa *jsAccount) storageTotals() (uint64, uint64) {\n\tmem := uint64(0)\n\tstore := uint64(0)\n\tfor _, sa := range jsa.usage {\n\t\tmem += uint64(sa.total.mem)\n\t\tstore += uint64(sa.total.store)\n\t}\n\treturn mem, store\n}\n\nfunc (jsa *jsAccount) limitsExceeded(storeType StorageType, tierName string, replicas int) (bool, *ApiError) {\n\treturn jsa.wouldExceedLimits(storeType, tierName, replicas, _EMPTY_, nil, nil)\n}\n\nfunc (jsa *jsAccount) wouldExceedLimits(storeType StorageType, tierName string, replicas int, subj string, hdr, msg []byte) (bool, *ApiError) {\n\tjsa.usageMu.RLock()\n\tdefer jsa.usageMu.RUnlock()\n\n\tselectedLimits, ok := jsa.limits[tierName]\n\tif !ok {\n\t\treturn true, NewJSNoLimitsError()\n\t}\n\tinUse := jsa.usage[tierName]\n\tif inUse == nil {\n\t\t// Imply totals of 0\n\t\treturn false, nil\n\t}\n\tr := int64(replicas)\n\t// Make sure replicas is correct.\n\tif r < 1 {\n\t\tr = 1\n\t}\n\t// This is for limits. If we have no tier, consider all to be flat, vs tiers like R3 where we want to scale limit by replication.\n\tlr := r\n\tif tierName == _EMPTY_ {\n\t\tlr = 1\n\t}\n\n\t// Since tiers are flat we need to scale limit up by replicas when checking.\n\tif storeType == MemoryStorage {\n\t\ttotalMem := inUse.total.mem + (int64(memStoreMsgSize(subj, hdr, msg)) * r)\n\t\tif selectedLimits.MemoryMaxStreamBytes > 0 && totalMem > selectedLimits.MemoryMaxStreamBytes*lr {\n\t\t\treturn true, nil\n\t\t}\n\t\tif selectedLimits.MaxMemory >= 0 && totalMem > selectedLimits.MaxMemory*lr {\n\t\t\treturn true, nil\n\t\t}\n\t} else {\n\t\ttotalStore := inUse.total.store + (int64(fileStoreMsgSize(subj, hdr, msg)) * r)\n\t\tif selectedLimits.StoreMaxStreamBytes > 0 && totalStore > selectedLimits.StoreMaxStreamBytes*lr {\n\t\t\treturn true, nil\n\t\t}\n\t\tif selectedLimits.MaxStore >= 0 && totalStore > selectedLimits.MaxStore*lr {\n\t\t\treturn true, nil\n\t\t}\n\t}\n\n\treturn false, nil\n}\n\n// Check account limits.\n// Read Lock should be held\nfunc (js *jetStream) checkAccountLimits(selected *JetStreamAccountLimits, config *StreamConfig, currentRes int64) error {\n\treturn js.checkLimits(selected, config, false, currentRes, 0)\n}\n\n// Check account and server limits.\n// Read Lock should be held\nfunc (js *jetStream) checkAllLimits(selected *JetStreamAccountLimits, config *StreamConfig, currentRes, maxBytesOffset int64) error {\n\treturn js.checkLimits(selected, config, true, currentRes, maxBytesOffset)\n}\n\n// Check if a new proposed msg set while exceed our account limits.\n// Lock should be held.\nfunc (js *jetStream) checkLimits(selected *JetStreamAccountLimits, config *StreamConfig, checkServer bool, currentRes, maxBytesOffset int64) error {\n\t// Check MaxConsumers\n\tif config.MaxConsumers > 0 && selected.MaxConsumers > 0 && config.MaxConsumers > selected.MaxConsumers {\n\t\treturn NewJSMaximumConsumersLimitError()\n\t}\n\t// stream limit is checked separately on stream create only!\n\t// Check storage, memory or disk.\n\treturn js.checkBytesLimits(selected, config.MaxBytes, config.Storage, checkServer, currentRes, maxBytesOffset)\n}\n\n// Check if additional bytes will exceed our account limits and optionally the server itself.\n// Read Lock should be held.\nfunc (js *jetStream) checkBytesLimits(selectedLimits *JetStreamAccountLimits, addBytes int64, storage StorageType, checkServer bool, currentRes, maxBytesOffset int64) error {\n\tif addBytes < 0 {\n\t\taddBytes = 1\n\t}\n\ttotalBytes := addBytes + maxBytesOffset\n\n\tswitch storage {\n\tcase MemoryStorage:\n\t\t// Account limits defined.\n\t\tif selectedLimits.MaxMemory >= 0 && currentRes+totalBytes > selectedLimits.MaxMemory {\n\t\t\treturn NewJSMemoryResourcesExceededError()\n\t\t}\n\t\t// Check if this server can handle request.\n\t\tif checkServer && js.memReserved+totalBytes > js.config.MaxMemory {\n\t\t\treturn NewJSMemoryResourcesExceededError()\n\t\t}\n\tcase FileStorage:\n\t\t// Account limits defined.\n\t\tif selectedLimits.MaxStore >= 0 && currentRes+totalBytes > selectedLimits.MaxStore {\n\t\t\treturn NewJSStorageResourcesExceededError()\n\t\t}\n\t\t// Check if this server can handle request.\n\t\tif checkServer && js.storeReserved+totalBytes > js.config.MaxStore {\n\t\t\treturn NewJSStorageResourcesExceededError()\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc (jsa *jsAccount) acc() *Account {\n\treturn jsa.account\n}\n\n// Delete the JetStream resources.\nfunc (jsa *jsAccount) delete() {\n\tvar streams []*stream\n\tvar ts []string\n\n\tjsa.mu.Lock()\n\t// The update timer and subs need to be protected by usageMu lock\n\tjsa.usageMu.Lock()\n\tif jsa.utimer != nil {\n\t\tjsa.utimer.Stop()\n\t\tjsa.utimer = nil\n\t}\n\tif jsa.updatesSub != nil && jsa.js.srv != nil {\n\t\ts := jsa.js.srv\n\t\ts.sysUnsubscribe(jsa.updatesSub)\n\t\tjsa.updatesSub = nil\n\t}\n\tjsa.usageMu.Unlock()\n\n\tfor _, ms := range jsa.streams {\n\t\tstreams = append(streams, ms)\n\t}\n\tacc := jsa.account\n\tfor _, t := range jsa.templates {\n\t\tts = append(ts, t.Name)\n\t}\n\tjsa.templates = nil\n\tjsa.mu.Unlock()\n\n\tfor _, mset := range streams {\n\t\tmset.stop(false, false)\n\t}\n\n\tfor _, t := range ts {\n\t\tacc.deleteStreamTemplate(t)\n\t}\n}\n\n// Lookup the jetstream account for a given account.\nfunc (js *jetStream) lookupAccount(a *Account) *jsAccount {\n\tif a == nil {\n\t\treturn nil\n\t}\n\tjs.mu.RLock()\n\tjsa := js.accounts[a.Name]\n\tjs.mu.RUnlock()\n\treturn jsa\n}\n\n// Report on JetStream stats and usage for this server.\nfunc (js *jetStream) usageStats() *JetStreamStats {\n\tvar stats JetStreamStats\n\tjs.mu.RLock()\n\tstats.Accounts = len(js.accounts)\n\tstats.ReservedMemory = uint64(js.memReserved)\n\tstats.ReservedStore = uint64(js.storeReserved)\n\ts := js.srv\n\tjs.mu.RUnlock()\n\tstats.API.Level = JSApiLevel\n\tstats.API.Total = uint64(atomic.LoadInt64(&js.apiTotal))\n\tstats.API.Errors = uint64(atomic.LoadInt64(&js.apiErrors))\n\tstats.API.Inflight = uint64(atomic.LoadInt64(&js.apiInflight))\n\t// Make sure we do not report negative.\n\tused := atomic.LoadInt64(&js.memUsed)\n\tif used < 0 {\n\t\tused = 0\n\t}\n\tstats.Memory = uint64(used)\n\tused = atomic.LoadInt64(&js.storeUsed)\n\tif used < 0 {\n\t\tused = 0\n\t}\n\tstats.Store = uint64(used)\n\tstats.HAAssets = s.numRaftNodes()\n\treturn &stats\n}\n\n// Check to see if we have enough system resources for this account.\n// Lock should be held.\nfunc (js *jetStream) sufficientResources(limits map[string]JetStreamAccountLimits) error {\n\t// If we are clustered we do not really know how many resources will be ultimately available.\n\t// This needs to be handled out of band.\n\t// If we are a single server, we can make decisions here.\n\tif limits == nil || !js.standAlone {\n\t\treturn nil\n\t}\n\n\ttotalMaxBytes := func(limits map[string]JetStreamAccountLimits) (int64, int64) {\n\t\ttotalMaxMemory := int64(0)\n\t\ttotalMaxStore := int64(0)\n\t\tfor _, l := range limits {\n\t\t\tif l.MaxMemory > 0 {\n\t\t\t\ttotalMaxMemory += l.MaxMemory\n\t\t\t}\n\t\t\tif l.MaxStore > 0 {\n\t\t\t\ttotalMaxStore += l.MaxStore\n\t\t\t}\n\t\t}\n\t\treturn totalMaxMemory, totalMaxStore\n\t}\n\n\ttotalMaxMemory, totalMaxStore := totalMaxBytes(limits)\n\n\t// Reserved is now specific to the MaxBytes for streams.\n\tif js.memReserved+totalMaxMemory > js.config.MaxMemory {\n\t\treturn NewJSMemoryResourcesExceededError()\n\t}\n\tif js.storeReserved+totalMaxStore > js.config.MaxStore {\n\t\treturn NewJSStorageResourcesExceededError()\n\t}\n\n\t// Since we know if we are here we are single server mode, check the account reservations.\n\tvar storeReserved, memReserved int64\n\tfor _, jsa := range js.accounts {\n\t\tif jsa.account.IsExpired() {\n\t\t\tcontinue\n\t\t}\n\t\tjsa.usageMu.RLock()\n\t\tmaxMemory, maxStore := totalMaxBytes(jsa.limits)\n\t\tjsa.usageMu.RUnlock()\n\t\tmemReserved += maxMemory\n\t\tstoreReserved += maxStore\n\t}\n\n\tif memReserved+totalMaxMemory > js.config.MaxMemory {\n\t\treturn NewJSMemoryResourcesExceededError()\n\t}\n\tif storeReserved+totalMaxStore > js.config.MaxStore {\n\t\treturn NewJSStorageResourcesExceededError()\n\t}\n\n\treturn nil\n}\n\n// This will reserve the stream resources requested.\n// This will spin off off of MaxBytes.\nfunc (js *jetStream) reserveStreamResources(cfg *StreamConfig) {\n\tif cfg == nil || cfg.MaxBytes <= 0 {\n\t\treturn\n\t}\n\n\tjs.mu.Lock()\n\tswitch cfg.Storage {\n\tcase MemoryStorage:\n\t\tjs.memReserved += cfg.MaxBytes\n\tcase FileStorage:\n\t\tjs.storeReserved += cfg.MaxBytes\n\t}\n\ts, clustered := js.srv, !js.standAlone\n\tjs.mu.Unlock()\n\t// If clustered send an update to the system immediately.\n\tif clustered {\n\t\ts.sendStatszUpdate()\n\t}\n}\n\n// Release reserved resources held by a stream.\nfunc (js *jetStream) releaseStreamResources(cfg *StreamConfig) {\n\tif cfg == nil || cfg.MaxBytes <= 0 {\n\t\treturn\n\t}\n\n\tjs.mu.Lock()\n\tswitch cfg.Storage {\n\tcase MemoryStorage:\n\t\tjs.memReserved -= cfg.MaxBytes\n\tcase FileStorage:\n\t\tjs.storeReserved -= cfg.MaxBytes\n\t}\n\ts, clustered := js.srv, !js.standAlone\n\tjs.mu.Unlock()\n\t// If clustered send an update to the system immediately.\n\tif clustered {\n\t\ts.sendStatszUpdate()\n\t}\n}\n\nconst (\n\t// JetStreamStoreDir is the prefix we use.\n\tJetStreamStoreDir = \"jetstream\"\n\t// JetStreamMaxStoreDefault is the default disk storage limit. 1TB\n\tJetStreamMaxStoreDefault = 1024 * 1024 * 1024 * 1024\n\t// JetStreamMaxMemDefault is only used when we can't determine system memory. 256MB\n\tJetStreamMaxMemDefault = 1024 * 1024 * 256\n\t// snapshot staging for restores.\n\tsnapStagingDir = \".snap-staging\"\n)\n\n// Dynamically create a config with a tmp based directory (repeatable) and 75% of system memory.\nfunc (s *Server) dynJetStreamConfig(storeDir string, maxStore, maxMem int64) *JetStreamConfig {\n\tjsc := &JetStreamConfig{}\n\tif storeDir != _EMPTY_ {\n\t\tjsc.StoreDir = filepath.Join(storeDir, JetStreamStoreDir)\n\t} else {\n\t\t// Create one in tmp directory, but make it consistent for restarts.\n\t\tjsc.StoreDir = filepath.Join(os.TempDir(), \"nats\", JetStreamStoreDir)\n\t\ts.Warnf(\"Temporary storage directory used, data could be lost on system reboot\")\n\t}\n\n\topts := s.getOpts()\n\n\t// Strict mode.\n\tjsc.Strict = opts.JetStreamStrict\n\n\t// Sync options.\n\tjsc.SyncInterval = opts.SyncInterval\n\tjsc.SyncAlways = opts.SyncAlways\n\n\tif opts.maxStoreSet && maxStore >= 0 {\n\t\tjsc.MaxStore = maxStore\n\t} else {\n\t\tjsc.MaxStore = diskAvailable(jsc.StoreDir)\n\t}\n\n\tif opts.maxMemSet && maxMem >= 0 {\n\t\tjsc.MaxMemory = maxMem\n\t} else {\n\t\t// Estimate to 75% of total memory if we can determine system memory.\n\t\tif sysMem := sysmem.Memory(); sysMem > 0 {\n\t\t\t// Check if we have been limited with GOMEMLIMIT and if lower use that value.\n\t\t\tif gml := debug.SetMemoryLimit(-1); gml != math.MaxInt64 && gml < sysMem {\n\t\t\t\ts.Debugf(\"JetStream detected GOMEMLIMIT of %v\", friendlyBytes(gml))\n\t\t\t\tsysMem = gml\n\t\t\t}\n\t\t\tjsc.MaxMemory = sysMem / 4 * 3\n\t\t} else {\n\t\t\tjsc.MaxMemory = JetStreamMaxMemDefault\n\t\t}\n\t}\n\n\treturn jsc\n}\n\n// Helper function.\nfunc (a *Account) checkForJetStream() (*Server, *jsAccount, error) {\n\ta.mu.RLock()\n\ts := a.srv\n\tjsa := a.js\n\ta.mu.RUnlock()\n\n\tif s == nil || jsa == nil {\n\t\treturn nil, nil, NewJSNotEnabledForAccountError()\n\t}\n\n\treturn s, jsa, nil\n}\n\n// StreamTemplateConfig allows a configuration to auto-create streams based on this template when a message\n// is received that matches. Each new stream will use the config as the template config to create them.\ntype StreamTemplateConfig struct {\n\tName       string        `json:\"name\"`\n\tConfig     *StreamConfig `json:\"config\"`\n\tMaxStreams uint32        `json:\"max_streams\"`\n}\n\n// StreamTemplateInfo\ntype StreamTemplateInfo struct {\n\tConfig  *StreamTemplateConfig `json:\"config\"`\n\tStreams []string              `json:\"streams\"`\n}\n\n// streamTemplate\ntype streamTemplate struct {\n\tmu  sync.Mutex\n\ttc  *client\n\tjsa *jsAccount\n\t*StreamTemplateConfig\n\tstreams []string\n}\n\nfunc (t *StreamTemplateConfig) deepCopy() *StreamTemplateConfig {\n\tcopy := *t\n\tcfg := *t.Config\n\tcopy.Config = &cfg\n\treturn &copy\n}\n\n// addStreamTemplate will add a stream template to this account that allows auto-creation of streams.\nfunc (a *Account) addStreamTemplate(tc *StreamTemplateConfig) (*streamTemplate, error) {\n\ts, jsa, err := a.checkForJetStream()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif tc.Config.Name != \"\" {\n\t\treturn nil, fmt.Errorf(\"template config name should be empty\")\n\t}\n\tif len(tc.Name) > JSMaxNameLen {\n\t\treturn nil, fmt.Errorf(\"template name is too long, maximum allowed is %d\", JSMaxNameLen)\n\t}\n\n\t// FIXME(dlc) - Hacky\n\ttcopy := tc.deepCopy()\n\ttcopy.Config.Name = \"_\"\n\tcfg, apiErr := s.checkStreamCfg(tcopy.Config, a, false)\n\tif apiErr != nil {\n\t\treturn nil, apiErr\n\t}\n\ttcopy.Config = &cfg\n\tt := &streamTemplate{\n\t\tStreamTemplateConfig: tcopy,\n\t\ttc:                   s.createInternalJetStreamClient(),\n\t\tjsa:                  jsa,\n\t}\n\tt.tc.registerWithAccount(a)\n\n\tjsa.mu.Lock()\n\tif jsa.templates == nil {\n\t\tjsa.templates = make(map[string]*streamTemplate)\n\t\t// Create the appropriate store\n\t\tif cfg.Storage == FileStorage {\n\t\t\tjsa.store = newTemplateFileStore(jsa.storeDir)\n\t\t} else {\n\t\t\tjsa.store = newTemplateMemStore()\n\t\t}\n\t} else if _, ok := jsa.templates[tcopy.Name]; ok {\n\t\tjsa.mu.Unlock()\n\t\treturn nil, fmt.Errorf(\"template with name %q already exists\", tcopy.Name)\n\t}\n\tjsa.templates[tcopy.Name] = t\n\tjsa.mu.Unlock()\n\n\t// FIXME(dlc) - we can not overlap subjects between templates. Need to have test.\n\n\t// Setup the internal subscriptions to trap the messages.\n\tif err := t.createTemplateSubscriptions(); err != nil {\n\t\treturn nil, err\n\t}\n\tif err := jsa.store.Store(t); err != nil {\n\t\tt.delete()\n\t\treturn nil, err\n\t}\n\treturn t, nil\n}\n\nfunc (t *streamTemplate) createTemplateSubscriptions() error {\n\tif t == nil {\n\t\treturn fmt.Errorf(\"no template\")\n\t}\n\tif t.tc == nil {\n\t\treturn fmt.Errorf(\"template not enabled\")\n\t}\n\tc := t.tc\n\tif !c.srv.EventsEnabled() {\n\t\treturn ErrNoSysAccount\n\t}\n\tsid := 1\n\tfor _, subject := range t.Config.Subjects {\n\t\t// Now create the subscription\n\t\tif _, err := c.processSub([]byte(subject), nil, []byte(strconv.Itoa(sid)), t.processInboundTemplateMsg, false); err != nil {\n\t\t\tc.acc.deleteStreamTemplate(t.Name)\n\t\t\treturn err\n\t\t}\n\t\tsid++\n\t}\n\treturn nil\n}\n\nfunc (t *streamTemplate) processInboundTemplateMsg(_ *subscription, pc *client, acc *Account, subject, reply string, msg []byte) {\n\tif t == nil || t.jsa == nil {\n\t\treturn\n\t}\n\tjsa := t.jsa\n\tcn := canonicalName(subject)\n\n\tjsa.mu.Lock()\n\t// If we already are registered then we can just return here.\n\tif _, ok := jsa.streams[cn]; ok {\n\t\tjsa.mu.Unlock()\n\t\treturn\n\t}\n\tjsa.mu.Unlock()\n\n\t// Check if we are at the maximum and grab some variables.\n\tt.mu.Lock()\n\tc := t.tc\n\tcfg := *t.Config\n\tcfg.Template = t.Name\n\tatLimit := len(t.streams) >= int(t.MaxStreams)\n\tif !atLimit {\n\t\tt.streams = append(t.streams, cn)\n\t}\n\tt.mu.Unlock()\n\n\tif atLimit {\n\t\tc.RateLimitWarnf(\"JetStream could not create stream for account %q on subject %q, at limit\", acc.Name, subject)\n\t\treturn\n\t}\n\n\t// We need to create the stream here.\n\t// Change the config from the template and only use literal subject.\n\tcfg.Name = cn\n\tcfg.Subjects = []string{subject}\n\tmset, err := acc.addStream(&cfg)\n\tif err != nil {\n\t\tacc.validateStreams(t)\n\t\tc.RateLimitWarnf(\"JetStream could not create stream for account %q on subject %q: %v\", acc.Name, subject, err)\n\t\treturn\n\t}\n\n\t// Process this message directly by invoking mset.\n\tmset.processInboundJetStreamMsg(nil, pc, acc, subject, reply, msg)\n}\n\n// lookupStreamTemplate looks up the names stream template.\nfunc (a *Account) lookupStreamTemplate(name string) (*streamTemplate, error) {\n\t_, jsa, err := a.checkForJetStream()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tjsa.mu.Lock()\n\tdefer jsa.mu.Unlock()\n\tif jsa.templates == nil {\n\t\treturn nil, fmt.Errorf(\"template not found\")\n\t}\n\tt, ok := jsa.templates[name]\n\tif !ok {\n\t\treturn nil, fmt.Errorf(\"template not found\")\n\t}\n\treturn t, nil\n}\n\n// This function will check all named streams and make sure they are valid.\nfunc (a *Account) validateStreams(t *streamTemplate) {\n\tt.mu.Lock()\n\tvar vstreams []string\n\tfor _, sname := range t.streams {\n\t\tif _, err := a.lookupStream(sname); err == nil {\n\t\t\tvstreams = append(vstreams, sname)\n\t\t}\n\t}\n\tt.streams = vstreams\n\tt.mu.Unlock()\n}\n\nfunc (t *streamTemplate) delete() error {\n\tif t == nil {\n\t\treturn fmt.Errorf(\"nil stream template\")\n\t}\n\n\tt.mu.Lock()\n\tjsa := t.jsa\n\tc := t.tc\n\tt.tc = nil\n\tdefer func() {\n\t\tif c != nil {\n\t\t\tc.closeConnection(ClientClosed)\n\t\t}\n\t}()\n\tt.mu.Unlock()\n\n\tif jsa == nil {\n\t\treturn NewJSNotEnabledForAccountError()\n\t}\n\n\tjsa.mu.Lock()\n\tif jsa.templates == nil {\n\t\tjsa.mu.Unlock()\n\t\treturn fmt.Errorf(\"template not found\")\n\t}\n\tif _, ok := jsa.templates[t.Name]; !ok {\n\t\tjsa.mu.Unlock()\n\t\treturn fmt.Errorf(\"template not found\")\n\t}\n\tdelete(jsa.templates, t.Name)\n\tacc := jsa.account\n\tjsa.mu.Unlock()\n\n\t// Remove streams associated with this template.\n\tvar streams []*stream\n\tt.mu.Lock()\n\tfor _, name := range t.streams {\n\t\tif mset, err := acc.lookupStream(name); err == nil {\n\t\t\tstreams = append(streams, mset)\n\t\t}\n\t}\n\tt.mu.Unlock()\n\n\tif jsa.store != nil {\n\t\tif err := jsa.store.Delete(t); err != nil {\n\t\t\treturn fmt.Errorf(\"error deleting template from store: %v\", err)\n\t\t}\n\t}\n\n\tvar lastErr error\n\tfor _, mset := range streams {\n\t\tif err := mset.delete(); err != nil {\n\t\t\tlastErr = err\n\t\t}\n\t}\n\treturn lastErr\n}\n\nfunc (a *Account) deleteStreamTemplate(name string) error {\n\tt, err := a.lookupStreamTemplate(name)\n\tif err != nil {\n\t\treturn NewJSStreamTemplateNotFoundError()\n\t}\n\treturn t.delete()\n}\n\nfunc (a *Account) templates() []*streamTemplate {\n\tvar ts []*streamTemplate\n\t_, jsa, err := a.checkForJetStream()\n\tif err != nil {\n\t\treturn nil\n\t}\n\n\tjsa.mu.Lock()\n\tfor _, t := range jsa.templates {\n\t\t// FIXME(dlc) - Copy?\n\t\tts = append(ts, t)\n\t}\n\tjsa.mu.Unlock()\n\n\treturn ts\n}\n\n// Will add a stream to a template, this is for recovery.\nfunc (jsa *jsAccount) addStreamNameToTemplate(tname, mname string) error {\n\tif jsa.templates == nil {\n\t\treturn fmt.Errorf(\"template not found\")\n\t}\n\tt, ok := jsa.templates[tname]\n\tif !ok {\n\t\treturn fmt.Errorf(\"template not found\")\n\t}\n\t// We found template.\n\tt.mu.Lock()\n\tt.streams = append(t.streams, mname)\n\tt.mu.Unlock()\n\treturn nil\n}\n\n// This will check if a template owns this stream.\n// jsAccount lock should be held\nfunc (jsa *jsAccount) checkTemplateOwnership(tname, sname string) bool {\n\tif jsa.templates == nil {\n\t\treturn false\n\t}\n\tt, ok := jsa.templates[tname]\n\tif !ok {\n\t\treturn false\n\t}\n\t// We found template, make sure we are in streams.\n\tfor _, streamName := range t.streams {\n\t\tif sname == streamName {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\ntype Number interface {\n\tint | int8 | int16 | int32 | int64 | uint | uint8 | uint16 | uint32 | uint64 | float32 | float64\n}\n\n// friendlyBytes returns a string with the given bytes int64\n// represented as a size, such as 1KB, 10MB, etc...\nfunc friendlyBytes[T Number](bytes T) string {\n\tfbytes := float64(bytes)\n\tbase := 1024\n\tpre := []string{\"K\", \"M\", \"G\", \"T\", \"P\", \"E\"}\n\tif fbytes < float64(base) {\n\t\treturn fmt.Sprintf(\"%v B\", fbytes)\n\t}\n\texp := int(math.Log(fbytes) / math.Log(float64(base)))\n\tindex := exp - 1\n\treturn fmt.Sprintf(\"%.2f %sB\", fbytes/math.Pow(float64(base), float64(exp)), pre[index])\n}\n\nfunc isValidName(name string) bool {\n\tif name == _EMPTY_ {\n\t\treturn false\n\t}\n\treturn !strings.ContainsAny(name, \" \\t\\r\\n\\f.*>\")\n}\n\n// CanonicalName will replace all token separators '.' with '_'.\n// This can be used when naming streams or consumers with multi-token subjects.\nfunc canonicalName(name string) string {\n\treturn strings.ReplaceAll(name, \".\", \"_\")\n}\n\n// To throttle the out of resources errors.\nfunc (s *Server) resourcesExceededError() {\n\tvar didAlert bool\n\n\ts.rerrMu.Lock()\n\tif now := time.Now(); now.Sub(s.rerrLast) > 10*time.Second {\n\t\ts.Errorf(\"JetStream resource limits exceeded for server\")\n\t\ts.rerrLast = now\n\t\tdidAlert = true\n\t}\n\ts.rerrMu.Unlock()\n\n\t// If we are meta leader we should relinquish that here.\n\tif didAlert {\n\t\tif js := s.getJetStream(); js != nil {\n\t\t\tjs.mu.RLock()\n\t\t\tif cc := js.cluster; cc != nil && cc.meta != nil {\n\t\t\t\tcc.meta.StepDown()\n\t\t\t}\n\t\t\tjs.mu.RUnlock()\n\t\t}\n\t}\n}\n\n// For validating options.\nfunc validateJetStreamOptions(o *Options) error {\n\t// in non operator mode, the account names need to be configured\n\tif len(o.JsAccDefaultDomain) > 0 {\n\t\tif len(o.TrustedOperators) == 0 {\n\t\t\tfor a, domain := range o.JsAccDefaultDomain {\n\t\t\t\tfound := false\n\t\t\t\tif isReservedAccount(a) {\n\t\t\t\t\tfound = true\n\t\t\t\t} else {\n\t\t\t\t\tfor _, acc := range o.Accounts {\n\t\t\t\t\t\tif a == acc.GetName() {\n\t\t\t\t\t\t\tif len(acc.jsLimits) > 0 && domain != _EMPTY_ {\n\t\t\t\t\t\t\t\treturn fmt.Errorf(\"default_js_domain contains account name %q with enabled JetStream\", a)\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tfound = true\n\t\t\t\t\t\t\tbreak\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif !found {\n\t\t\t\t\treturn fmt.Errorf(\"in non operator mode, `default_js_domain` references non existing account %q\", a)\n\t\t\t\t}\n\t\t\t}\n\t\t} else {\n\t\t\tfor a := range o.JsAccDefaultDomain {\n\t\t\t\tif !nkeys.IsValidPublicAccountKey(a) {\n\t\t\t\t\treturn fmt.Errorf(\"default_js_domain contains account name %q, which is not a valid public account nkey\", a)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tfor a, d := range o.JsAccDefaultDomain {\n\t\t\tsacc := DEFAULT_SYSTEM_ACCOUNT\n\t\t\tif o.SystemAccount != _EMPTY_ {\n\t\t\t\tsacc = o.SystemAccount\n\t\t\t}\n\t\t\tif a == sacc {\n\t\t\t\treturn fmt.Errorf(\"system account %q can not be in default_js_domain\", a)\n\t\t\t}\n\t\t\tif d == _EMPTY_ {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif sub := fmt.Sprintf(jsDomainAPI, d); !IsValidSubject(sub) {\n\t\t\t\treturn fmt.Errorf(\"default_js_domain contains account %q with invalid domain name %q\", a, d)\n\t\t\t}\n\t\t}\n\t}\n\tif o.JetStreamDomain != _EMPTY_ {\n\t\tif subj := fmt.Sprintf(jsDomainAPI, o.JetStreamDomain); !IsValidSubject(subj) {\n\t\t\treturn fmt.Errorf(\"invalid domain name: derived %q is not a valid subject\", subj)\n\t\t}\n\n\t\tif !isValidName(o.JetStreamDomain) {\n\t\t\treturn fmt.Errorf(\"invalid domain name: may not contain ., * or >\")\n\t\t}\n\t}\n\t// If not clustered no checks needed past here.\n\tif !o.JetStream || o.Cluster.Port == 0 {\n\t\treturn nil\n\t}\n\tif o.ServerName == _EMPTY_ {\n\t\treturn fmt.Errorf(\"jetstream cluster requires `server_name` to be set\")\n\t}\n\tif o.Cluster.Name == _EMPTY_ {\n\t\treturn fmt.Errorf(\"jetstream cluster requires `cluster.name` to be set\")\n\t}\n\n\th := strings.ToLower(o.JetStreamExtHint)\n\tswitch h {\n\tcase jsWillExtend, jsNoExtend, _EMPTY_:\n\t\to.JetStreamExtHint = h\n\tdefault:\n\t\treturn fmt.Errorf(\"expected 'no_extend' for string value, got '%s'\", h)\n\t}\n\n\tif o.JetStreamMaxCatchup < 0 {\n\t\treturn fmt.Errorf(\"jetstream max catchup cannot be negative\")\n\t}\n\treturn nil\n}\n\n// We had a bug that set a default de dupe window on mirror, despite that being not a valid config\nfunc fixCfgMirrorWithDedupWindow(cfg *StreamConfig) {\n\tif cfg == nil || cfg.Mirror == nil {\n\t\treturn\n\t}\n\tif cfg.Duplicates != 0 {\n\t\tcfg.Duplicates = 0\n\t}\n}\n\nfunc (s *Server) handleWritePermissionError() {\n\t//TODO Check if we should add s.jetStreamOOSPending in condition\n\tif s.JetStreamEnabled() {\n\t\ts.Errorf(\"File system permission denied while writing, disabling JetStream\")\n\n\t\tgo s.DisableJetStream()\n\n\t\t//TODO Send respective advisory if needed, same as in handleOutOfSpace\n\t}\n}\n",
    "source_file": "server/jetstream.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2018-2025 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage server\n\nimport (\n\t\"bytes\"\n\t\"cmp\"\n\t\"crypto/sha256\"\n\t\"crypto/tls\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"fmt\"\n\t\"math/rand\"\n\t\"net\"\n\t\"net/url\"\n\t\"slices\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"time\"\n)\n\nconst (\n\tdefaultSolicitGatewaysDelay         = time.Second\n\tdefaultGatewayConnectDelay          = time.Second\n\tdefaultGatewayReconnectDelay        = time.Second\n\tdefaultGatewayRecentSubExpiration   = 2 * time.Second\n\tdefaultGatewayMaxRUnsubBeforeSwitch = 1000\n\n\toldGWReplyPrefix    = \"$GR.\"\n\toldGWReplyPrefixLen = len(oldGWReplyPrefix)\n\toldGWReplyStart     = oldGWReplyPrefixLen + 5 // len of prefix above + len of hash (4) + \".\"\n\n\t// The new prefix is \"_GR_.<cluster>.<server>.\" where <cluster> is 6 characters\n\t// hash of origin cluster name and <server> is 6 characters hash of origin server pub key.\n\tgwReplyPrefix    = \"_GR_.\"\n\tgwReplyPrefixLen = len(gwReplyPrefix)\n\tgwHashLen        = 6\n\tgwClusterOffset  = gwReplyPrefixLen\n\tgwServerOffset   = gwClusterOffset + gwHashLen + 1\n\tgwSubjectOffset  = gwServerOffset + gwHashLen + 1\n\n\t// Gateway connections send PINGs regardless of traffic. The interval is\n\t// either Options.PingInterval or this value, whichever is the smallest.\n\tgwMaxPingInterval = 15 * time.Second\n)\n\nvar (\n\tgatewayConnectDelay          = defaultGatewayConnectDelay\n\tgatewayReconnectDelay        = defaultGatewayReconnectDelay\n\tgatewayMaxRUnsubBeforeSwitch = defaultGatewayMaxRUnsubBeforeSwitch\n\tgatewaySolicitDelay          = int64(defaultSolicitGatewaysDelay)\n\tgatewayMaxPingInterval       = gwMaxPingInterval\n)\n\n// Warning when user configures gateway TLS insecure\nconst gatewayTLSInsecureWarning = \"TLS certificate chain and hostname of solicited gateways will not be verified. DO NOT USE IN PRODUCTION!\"\n\n// SetGatewaysSolicitDelay sets the initial delay before gateways\n// connections are initiated.\n// Used by tests.\nfunc SetGatewaysSolicitDelay(delay time.Duration) {\n\tatomic.StoreInt64(&gatewaySolicitDelay, int64(delay))\n}\n\n// ResetGatewaysSolicitDelay resets the initial delay before gateways\n// connections are initiated to its default values.\n// Used by tests.\nfunc ResetGatewaysSolicitDelay() {\n\tatomic.StoreInt64(&gatewaySolicitDelay, int64(defaultSolicitGatewaysDelay))\n}\n\nconst (\n\tgatewayCmdGossip          byte = 1\n\tgatewayCmdAllSubsStart    byte = 2\n\tgatewayCmdAllSubsComplete byte = 3\n)\n\n// GatewayInterestMode represents an account interest mode for a gateway connection\ntype GatewayInterestMode byte\n\n// GatewayInterestMode values\nconst (\n\t// optimistic is the default mode where a cluster will send\n\t// to a gateway unless it is been told that there is no interest\n\t// (this is for plain subscribers only).\n\tOptimistic GatewayInterestMode = iota\n\t// transitioning is when a gateway has to send too many\n\t// no interest on subjects to the remote and decides that it is\n\t// now time to move to modeInterestOnly (this is on a per account\n\t// basis).\n\tTransitioning\n\t// interestOnly means that a cluster sends all it subscriptions\n\t// interest to the gateway, which in return does not send a message\n\t// unless it knows that there is explicit interest.\n\tInterestOnly\n)\n\nfunc (im GatewayInterestMode) String() string {\n\tswitch im {\n\tcase Optimistic:\n\t\treturn \"Optimistic\"\n\tcase InterestOnly:\n\t\treturn \"Interest-Only\"\n\tcase Transitioning:\n\t\treturn \"Transitioning\"\n\tdefault:\n\t\treturn \"Unknown\"\n\t}\n}\n\nvar gwDoNotForceInterestOnlyMode bool\n\n// GatewayDoNotForceInterestOnlyMode is used ONLY in tests.\n// DO NOT USE in normal code or if you embed the NATS Server.\nfunc GatewayDoNotForceInterestOnlyMode(doNotForce bool) {\n\tgwDoNotForceInterestOnlyMode = doNotForce\n}\n\ntype srvGateway struct {\n\ttotalQSubs int64 //total number of queue subs in all remote gateways (used with atomic operations)\n\tsync.RWMutex\n\tenabled  bool                   // Immutable, true if both a name and port are configured\n\tname     string                 // Name of the Gateway on this server\n\tout      map[string]*client     // outbound gateways\n\touto     []*client              // outbound gateways maintained in an order suitable for sending msgs (currently based on RTT)\n\tin       map[uint64]*client     // inbound gateways\n\tremotes  map[string]*gatewayCfg // Config of remote gateways\n\tURLs     refCountedUrlSet       // Set of all Gateway URLs in the cluster\n\tURL      string                 // This server gateway URL (after possible random port is resolved)\n\tinfo     *Info                  // Gateway Info protocol\n\tinfoJSON []byte                 // Marshal'ed Info protocol\n\trunknown bool                   // Rejects unknown (not configured) gateway connections\n\treplyPfx []byte                 // Will be \"$GNR.<1:reserved>.<8:cluster hash>.<8:server hash>.\"\n\n\t// For backward compatibility\n\toldReplyPfx []byte\n\toldHash     []byte\n\n\t// We maintain the interest of subjects and queues per account.\n\t// For a given account, entries in the map could be something like this:\n\t// foo.bar {n: 3} \t\t\t// 3 subs on foo.bar\n\t// foo.>   {n: 6}\t\t\t// 6 subs on foo.>\n\t// foo bar {n: 1, q: true}  // 1 qsub on foo, queue bar\n\t// foo baz {n: 3, q: true}  // 3 qsubs on foo, queue baz\n\tpasi struct {\n\t\t// Protect map since accessed from different go-routine and avoid\n\t\t// possible race resulting in RS+ being sent before RS- resulting\n\t\t// in incorrect interest suppression.\n\t\t// Will use while sending QSubs (on GW connection accept) and when\n\t\t// switching to the send-all-subs mode.\n\t\tsync.Mutex\n\t\tm map[string]map[string]*sitally\n\t}\n\n\t// This is to track recent subscriptions for a given account\n\trsubs sync.Map\n\n\tresolver  netResolver   // Used to resolve host name before calling net.Dial()\n\tsqbsz     int           // Max buffer size to send queue subs protocol. Used for testing.\n\trecSubExp time.Duration // For how long do we check if there is a subscription match for a message with reply\n\n\t// These are used for routing of mapped replies.\n\tsIDHash        []byte   // Server ID hash (6 bytes)\n\troutesIDByHash sync.Map // Route's server ID is hashed (6 bytes) and stored in this map.\n\n\t// If a server has its own configuration in the \"Gateways\" remotes configuration\n\t// we will keep track of the URLs that are defined in the config so they can\n\t// be reported in monitoring.\n\townCfgURLs []string\n}\n\n// Subject interest tally. Also indicates if the key in the map is a\n// queue or not.\ntype sitally struct {\n\tn int32 // number of subscriptions directly matching\n\tq bool  // indicate that this is a queue\n}\n\ntype gatewayCfg struct {\n\tsync.RWMutex\n\t*RemoteGatewayOpts\n\thash           []byte\n\toldHash        []byte\n\turls           map[string]*url.URL\n\tconnAttempts   int\n\ttlsName        string\n\timplicit       bool\n\tvarzUpdateURLs bool // Tells monitoring code to update URLs when varz is inspected.\n}\n\n// Struct for client's gateway related fields\ntype gateway struct {\n\tname       string\n\tcfg        *gatewayCfg\n\tconnectURL *url.URL          // Needed when sending CONNECT after receiving INFO from remote\n\toutsim     *sync.Map         // Per-account subject interest (or no-interest) (outbound conn)\n\tinsim      map[string]*insie // Per-account subject no-interest sent or modeInterestOnly mode (inbound conn)\n\n\t// This is an outbound GW connection\n\toutbound bool\n\t// Set/check in readLoop without lock. This is to know that an inbound has sent the CONNECT protocol first\n\tconnected bool\n\t// Set to true if outbound is to a server that only knows about $GR, not $GNR\n\tuseOldPrefix bool\n\t// If true, it indicates that the inbound side will switch any account to\n\t// interest-only mode \"immediately\", so the outbound should disregard\n\t// the optimistic mode when checking for interest.\n\tinterestOnlyMode bool\n\t// Name of the remote server\n\tremoteName string\n}\n\n// Outbound subject interest entry.\ntype outsie struct {\n\tsync.RWMutex\n\t// Indicate that all subs should be stored. This is\n\t// set to true when receiving the command from the\n\t// remote that we are about to receive all its subs.\n\tmode GatewayInterestMode\n\t// If not nil, used for no-interest for plain subs.\n\t// If a subject is present in this map, it means that\n\t// the remote is not interested in that subject.\n\t// When we have received the command that says that\n\t// the remote has sent all its subs, this is set to nil.\n\tni map[string]struct{}\n\t// Contains queue subscriptions when in optimistic mode,\n\t// and all subs when pk is > 0.\n\tsl *Sublist\n\t// Number of queue subs\n\tqsubs int\n}\n\n// Inbound subject interest entry.\n// If `ni` is not nil, it stores the subjects for which an\n// RS- was sent to the remote gateway. When a subscription\n// is created, this is used to know if we need to send\n// an RS+ to clear the no-interest in the remote.\n// When an account is switched to modeInterestOnly (we send\n// all subs of an account to the remote), then `ni` is nil and\n// when all subs have been sent, mode is set to modeInterestOnly\ntype insie struct {\n\tni   map[string]struct{} // Record if RS- was sent for given subject\n\tmode GatewayInterestMode\n}\n\ntype gwReplyMap struct {\n\tms  string\n\texp int64\n}\n\ntype gwReplyMapping struct {\n\t// Indicate if we should check the map or not. Since checking the map is done\n\t// when processing inbound messages and requires the lock we want to\n\t// check only when needed. This is set/get using atomic, so needs to\n\t// be memory aligned.\n\tcheck int32\n\t// To keep track of gateway replies mapping\n\tmapping map[string]*gwReplyMap\n}\n\n// Returns the corresponding gw routed subject, and `true` to indicate that a\n// mapping was found. If no entry is found, the passed subject is returned\n// as-is and `false` is returned to indicate that no mapping was found.\n// Caller is responsible to ensure the locking.\nfunc (g *gwReplyMapping) get(subject []byte) ([]byte, bool) {\n\trm, ok := g.mapping[string(subject)]\n\tif !ok {\n\t\treturn subject, false\n\t}\n\tsubj := []byte(rm.ms)\n\treturn subj, true\n}\n\n// clone returns a deep copy of the RemoteGatewayOpts object\nfunc (r *RemoteGatewayOpts) clone() *RemoteGatewayOpts {\n\tif r == nil {\n\t\treturn nil\n\t}\n\tclone := &RemoteGatewayOpts{\n\t\tName: r.Name,\n\t\tURLs: deepCopyURLs(r.URLs),\n\t}\n\tif r.TLSConfig != nil {\n\t\tclone.TLSConfig = r.TLSConfig.Clone()\n\t\tclone.TLSTimeout = r.TLSTimeout\n\t}\n\treturn clone\n}\n\n// Ensure that gateway is properly configured.\nfunc validateGatewayOptions(o *Options) error {\n\tif o.Gateway.Name == _EMPTY_ && o.Gateway.Port == 0 {\n\t\treturn nil\n\t}\n\tif o.Gateway.Name == _EMPTY_ {\n\t\treturn errors.New(\"gateway has no name\")\n\t}\n\tif strings.Contains(o.Gateway.Name, \" \") {\n\t\treturn ErrGatewayNameHasSpaces\n\t}\n\tif o.Gateway.Port == 0 {\n\t\treturn fmt.Errorf(\"gateway %q has no port specified (select -1 for random port)\", o.Gateway.Name)\n\t}\n\tfor i, g := range o.Gateway.Gateways {\n\t\tif g.Name == _EMPTY_ {\n\t\t\treturn fmt.Errorf(\"gateway in the list %d has no name\", i)\n\t\t}\n\t\tif len(g.URLs) == 0 {\n\t\t\treturn fmt.Errorf(\"gateway %q has no URL\", g.Name)\n\t\t}\n\t}\n\tif err := validatePinnedCerts(o.Gateway.TLSPinnedCerts); err != nil {\n\t\treturn fmt.Errorf(\"gateway %q: %v\", o.Gateway.Name, err)\n\t}\n\treturn nil\n}\n\n// Computes a hash of 6 characters for the name.\n// This will be used for routing of replies.\nfunc getGWHash(name string) []byte {\n\treturn []byte(getHashSize(name, gwHashLen))\n}\n\nfunc getOldHash(name string) []byte {\n\tsha := sha256.New()\n\tsha.Write([]byte(name))\n\tfullHash := []byte(fmt.Sprintf(\"%x\", sha.Sum(nil)))\n\treturn fullHash[:4]\n}\n\n// Initialize the s.gateway structure. We do this even if the server\n// does not have a gateway configured. In some part of the code, the\n// server will check the number of outbound gateways, etc.. and so\n// we don't have to check if s.gateway is nil or not.\nfunc (s *Server) newGateway(opts *Options) error {\n\tgateway := &srvGateway{\n\t\tname:     opts.Gateway.Name,\n\t\tout:      make(map[string]*client),\n\t\touto:     make([]*client, 0, 4),\n\t\tin:       make(map[uint64]*client),\n\t\tremotes:  make(map[string]*gatewayCfg),\n\t\tURLs:     make(refCountedUrlSet),\n\t\tresolver: opts.Gateway.resolver,\n\t\trunknown: opts.Gateway.RejectUnknown,\n\t\toldHash:  getOldHash(opts.Gateway.Name),\n\t}\n\tgateway.Lock()\n\tdefer gateway.Unlock()\n\n\tgateway.sIDHash = getGWHash(s.info.ID)\n\tclusterHash := getGWHash(opts.Gateway.Name)\n\tprefix := make([]byte, 0, gwSubjectOffset)\n\tprefix = append(prefix, gwReplyPrefix...)\n\tprefix = append(prefix, clusterHash...)\n\tprefix = append(prefix, '.')\n\tprefix = append(prefix, gateway.sIDHash...)\n\tprefix = append(prefix, '.')\n\tgateway.replyPfx = prefix\n\n\tprefix = make([]byte, 0, oldGWReplyStart)\n\tprefix = append(prefix, oldGWReplyPrefix...)\n\tprefix = append(prefix, gateway.oldHash...)\n\tprefix = append(prefix, '.')\n\tgateway.oldReplyPfx = prefix\n\n\tgateway.pasi.m = make(map[string]map[string]*sitally)\n\n\tif gateway.resolver == nil {\n\t\tgateway.resolver = netResolver(net.DefaultResolver)\n\t}\n\n\t// Create remote gateways\n\tfor _, rgo := range opts.Gateway.Gateways {\n\t\t// Ignore if there is a remote gateway with our name.\n\t\tif rgo.Name == gateway.name {\n\t\t\tgateway.ownCfgURLs = getURLsAsString(rgo.URLs)\n\t\t\tcontinue\n\t\t}\n\t\tcfg := &gatewayCfg{\n\t\t\tRemoteGatewayOpts: rgo.clone(),\n\t\t\thash:              getGWHash(rgo.Name),\n\t\t\toldHash:           getOldHash(rgo.Name),\n\t\t\turls:              make(map[string]*url.URL, len(rgo.URLs)),\n\t\t}\n\t\tif opts.Gateway.TLSConfig != nil && cfg.TLSConfig == nil {\n\t\t\tcfg.TLSConfig = opts.Gateway.TLSConfig.Clone()\n\t\t}\n\t\tif cfg.TLSTimeout == 0 {\n\t\t\tcfg.TLSTimeout = opts.Gateway.TLSTimeout\n\t\t}\n\t\tfor _, u := range rgo.URLs {\n\t\t\t// For TLS, look for a hostname that we can use for TLSConfig.ServerName\n\t\t\tcfg.saveTLSHostname(u)\n\t\t\tcfg.urls[u.Host] = u\n\t\t}\n\t\tgateway.remotes[cfg.Name] = cfg\n\t}\n\n\tgateway.sqbsz = opts.Gateway.sendQSubsBufSize\n\tif gateway.sqbsz == 0 {\n\t\tgateway.sqbsz = maxBufSize\n\t}\n\tgateway.recSubExp = defaultGatewayRecentSubExpiration\n\n\tgateway.enabled = opts.Gateway.Name != \"\" && opts.Gateway.Port != 0\n\ts.gateway = gateway\n\treturn nil\n}\n\n// Update remote gateways TLS configurations after a config reload.\nfunc (g *srvGateway) updateRemotesTLSConfig(opts *Options) {\n\tg.Lock()\n\tdefer g.Unlock()\n\t// Instead of going over opts.Gateway.Gateways, which would include only\n\t// explicit remotes, we are going to go through g.remotes.\n\tfor name, cfg := range g.remotes {\n\t\tif name == g.name {\n\t\t\tcontinue\n\t\t}\n\t\tvar ro *RemoteGatewayOpts\n\t\t// We now need to go back and find the RemoteGatewayOpts but only if\n\t\t// this remote is explicit (otherwise it won't be found).\n\t\tif !cfg.isImplicit() {\n\t\t\tfor _, r := range opts.Gateway.Gateways {\n\t\t\t\tif r.Name == name {\n\t\t\t\t\tro = r\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tcfg.Lock()\n\t\t// If we have an `ro` (that means an explicitly defined remote gateway)\n\t\t// and it has an explicit TLS config, use that one, otherwise (no explicit\n\t\t// TLS config in the remote, or implicit remote), use the TLS config from\n\t\t// the main block.\n\t\tif ro != nil && ro.TLSConfig != nil {\n\t\t\tcfg.TLSConfig = ro.TLSConfig.Clone()\n\t\t} else if opts.Gateway.TLSConfig != nil {\n\t\t\tcfg.TLSConfig = opts.Gateway.TLSConfig.Clone()\n\t\t}\n\t\t// Ensure that OCSP callbacks are always setup after a reload if needed.\n\t\tmustStaple := opts.OCSPConfig != nil && opts.OCSPConfig.Mode == OCSPModeAlways\n\t\tif mustStaple && opts.Gateway.TLSConfig != nil {\n\t\t\tclientCB := opts.Gateway.TLSConfig.GetClientCertificate\n\t\t\tverifyCB := opts.Gateway.TLSConfig.VerifyConnection\n\t\t\tif mustStaple && cfg.TLSConfig != nil {\n\t\t\t\tif clientCB != nil && cfg.TLSConfig.GetClientCertificate == nil {\n\t\t\t\t\tcfg.TLSConfig.GetClientCertificate = clientCB\n\t\t\t\t}\n\t\t\t\tif verifyCB != nil && cfg.TLSConfig.VerifyConnection == nil {\n\t\t\t\t\tcfg.TLSConfig.VerifyConnection = verifyCB\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tcfg.Unlock()\n\t}\n}\n\n// Returns if this server rejects connections from gateways that are not\n// explicitly configured.\nfunc (g *srvGateway) rejectUnknown() bool {\n\tg.RLock()\n\treject := g.runknown\n\tg.RUnlock()\n\treturn reject\n}\n\n// Starts the gateways accept loop and solicit explicit gateways\n// after an initial delay. This delay is meant to give a chance to\n// the cluster to form and this server gathers gateway URLs for this\n// cluster in order to send that as part of the connect/info process.\nfunc (s *Server) startGateways() {\n\ts.startGatewayAcceptLoop()\n\n\t// Delay start of creation of gateways to give a chance\n\t// to the local cluster to form.\n\ts.startGoRoutine(func() {\n\t\tdefer s.grWG.Done()\n\n\t\tdur := s.getOpts().gatewaysSolicitDelay\n\t\tif dur == 0 {\n\t\t\tdur = time.Duration(atomic.LoadInt64(&gatewaySolicitDelay))\n\t\t}\n\n\t\tselect {\n\t\tcase <-time.After(dur):\n\t\t\ts.solicitGateways()\n\t\tcase <-s.quitCh:\n\t\t\treturn\n\t\t}\n\t})\n}\n\n// This starts the gateway accept loop in a go routine, unless it\n// is detected that the server has already been shutdown.\nfunc (s *Server) startGatewayAcceptLoop() {\n\tif s.isShuttingDown() {\n\t\treturn\n\t}\n\n\t// Snapshot server options.\n\topts := s.getOpts()\n\n\tport := opts.Gateway.Port\n\tif port == -1 {\n\t\tport = 0\n\t}\n\n\ts.mu.Lock()\n\thp := net.JoinHostPort(opts.Gateway.Host, strconv.Itoa(port))\n\tl, e := natsListen(\"tcp\", hp)\n\ts.gatewayListenerErr = e\n\tif e != nil {\n\t\ts.mu.Unlock()\n\t\ts.Fatalf(\"Error listening on gateway port: %d - %v\", opts.Gateway.Port, e)\n\t\treturn\n\t}\n\ts.Noticef(\"Gateway name is %s\", s.getGatewayName())\n\ts.Noticef(\"Listening for gateways connections on %s\",\n\t\tnet.JoinHostPort(opts.Gateway.Host, strconv.Itoa(l.Addr().(*net.TCPAddr).Port)))\n\n\ttlsReq := opts.Gateway.TLSConfig != nil\n\tauthRequired := opts.Gateway.Username != \"\"\n\tinfo := &Info{\n\t\tID:           s.info.ID,\n\t\tName:         opts.ServerName,\n\t\tVersion:      s.info.Version,\n\t\tAuthRequired: authRequired,\n\t\tTLSRequired:  tlsReq,\n\t\tTLSVerify:    tlsReq,\n\t\tMaxPayload:   s.info.MaxPayload,\n\t\tGateway:      opts.Gateway.Name,\n\t\tGatewayNRP:   true,\n\t\tHeaders:      s.supportsHeaders(),\n\t\tProto:        s.getServerProto(),\n\t}\n\t// Unless in some tests we want to keep the old behavior, we are now\n\t// (since v2.9.0) indicate that this server will switch all accounts\n\t// to InterestOnly mode when accepting an inbound or when a new\n\t// account is fetched.\n\tif !gwDoNotForceInterestOnlyMode {\n\t\tinfo.GatewayIOM = true\n\t}\n\n\t// If we have selected a random port...\n\tif port == 0 {\n\t\t// Write resolved port back to options.\n\t\topts.Gateway.Port = l.Addr().(*net.TCPAddr).Port\n\t}\n\t// Possibly override Host/Port based on Gateway.Advertise\n\tif err := s.setGatewayInfoHostPort(info, opts); err != nil {\n\t\ts.Fatalf(\"Error setting gateway INFO with Gateway.Advertise value of %s, err=%v\", opts.Gateway.Advertise, err)\n\t\tl.Close()\n\t\ts.mu.Unlock()\n\t\treturn\n\t}\n\t// Setup state that can enable shutdown\n\ts.gatewayListener = l\n\n\t// Warn if insecure is configured in the main Gateway configuration\n\t// or any of the RemoteGateway's. This means that we need to check\n\t// remotes even if TLS would not be configured for the accept.\n\twarn := tlsReq && opts.Gateway.TLSConfig.InsecureSkipVerify\n\tif !warn {\n\t\tfor _, g := range opts.Gateway.Gateways {\n\t\t\tif g.TLSConfig != nil && g.TLSConfig.InsecureSkipVerify {\n\t\t\t\twarn = true\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t}\n\tif warn {\n\t\ts.Warnf(gatewayTLSInsecureWarning)\n\t}\n\tgo s.acceptConnections(l, \"Gateway\", func(conn net.Conn) { s.createGateway(nil, nil, conn) }, nil)\n\ts.mu.Unlock()\n}\n\n// Similar to setInfoHostPortAndGenerateJSON, but for gatewayInfo.\nfunc (s *Server) setGatewayInfoHostPort(info *Info, o *Options) error {\n\tgw := s.gateway\n\tgw.Lock()\n\tdefer gw.Unlock()\n\tgw.URLs.removeUrl(gw.URL)\n\tif o.Gateway.Advertise != \"\" {\n\t\tadvHost, advPort, err := parseHostPort(o.Gateway.Advertise, o.Gateway.Port)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tinfo.Host = advHost\n\t\tinfo.Port = advPort\n\t} else {\n\t\tinfo.Host = o.Gateway.Host\n\t\tinfo.Port = o.Gateway.Port\n\t\t// If the host is \"0.0.0.0\" or \"::\" we need to resolve to a public IP.\n\t\t// This will return at most 1 IP.\n\t\thostIsIPAny, ips, err := s.getNonLocalIPsIfHostIsIPAny(info.Host, false)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif hostIsIPAny {\n\t\t\tif len(ips) == 0 {\n\t\t\t\t// TODO(ik): Should we fail here (prevent starting)? If not, we\n\t\t\t\t// are going to \"advertise\" the 0.0.0.0:<port> url, which means\n\t\t\t\t// that remote are going to try to connect to 0.0.0.0:<port>,\n\t\t\t\t// which means a connect to loopback address, which is going\n\t\t\t\t// to fail with either TLS error, conn refused if the remote\n\t\t\t\t// is using different gateway port than this one, or error\n\t\t\t\t// saying that it tried to connect to itself.\n\t\t\t\ts.Errorf(\"Could not find any non-local IP for gateway %q with listen specification %q\",\n\t\t\t\t\tgw.name, info.Host)\n\t\t\t} else {\n\t\t\t\t// Take the first from the list...\n\t\t\t\tinfo.Host = ips[0]\n\t\t\t}\n\t\t}\n\t}\n\tgw.URL = net.JoinHostPort(info.Host, strconv.Itoa(info.Port))\n\tif o.Gateway.Advertise != \"\" {\n\t\ts.Noticef(\"Advertise address for gateway %q is set to %s\", gw.name, gw.URL)\n\t} else {\n\t\ts.Noticef(\"Address for gateway %q is %s\", gw.name, gw.URL)\n\t}\n\tgw.URLs[gw.URL]++\n\tgw.info = info\n\tinfo.GatewayURL = gw.URL\n\t// (re)generate the gatewayInfoJSON byte array\n\tgw.generateInfoJSON()\n\treturn nil\n}\n\n// Generates the Gateway INFO protocol.\n// The gateway lock is held on entry\nfunc (g *srvGateway) generateInfoJSON() {\n\t// We could be here when processing a route INFO that has a gateway URL,\n\t// but this server is not configured for gateways, so simply ignore here.\n\t// The configuration mismatch is reported somewhere else.\n\tif !g.enabled || g.info == nil {\n\t\treturn\n\t}\n\tg.info.GatewayURLs = g.URLs.getAsStringSlice()\n\tb, err := json.Marshal(g.info)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\tg.infoJSON = []byte(fmt.Sprintf(InfoProto, b))\n}\n\n// Goes through the list of registered gateways and try to connect to those.\n// The list (remotes) is initially containing the explicit remote gateways,\n// but the list is augmented with any implicit (discovered) gateway. Therefore,\n// this function only solicit explicit ones.\nfunc (s *Server) solicitGateways() {\n\tgw := s.gateway\n\tgw.RLock()\n\tdefer gw.RUnlock()\n\tfor _, cfg := range gw.remotes {\n\t\t// Since we delay the creation of gateways, it is\n\t\t// possible that server starts to receive inbound from\n\t\t// other clusters and in turn create outbounds. So here\n\t\t// we create only the ones that are configured.\n\t\tif !cfg.isImplicit() {\n\t\t\tcfg := cfg // Create new instance for the goroutine.\n\t\t\ts.startGoRoutine(func() {\n\t\t\t\ts.solicitGateway(cfg, true)\n\t\t\t\ts.grWG.Done()\n\t\t\t})\n\t\t}\n\t}\n}\n\n// Reconnect to the gateway after a little wait period. For explicit\n// gateways, we also wait for the default reconnect time.\nfunc (s *Server) reconnectGateway(cfg *gatewayCfg) {\n\tdefer s.grWG.Done()\n\n\tdelay := time.Duration(rand.Intn(100)) * time.Millisecond\n\tif !cfg.isImplicit() {\n\t\tdelay += gatewayReconnectDelay\n\t}\n\tselect {\n\tcase <-time.After(delay):\n\tcase <-s.quitCh:\n\t\treturn\n\t}\n\ts.solicitGateway(cfg, false)\n}\n\n// This function will loop trying to connect to any URL attached\n// to the given Gateway. It will return once a connection has been created.\nfunc (s *Server) solicitGateway(cfg *gatewayCfg, firstConnect bool) {\n\tvar (\n\t\topts       = s.getOpts()\n\t\tisImplicit = cfg.isImplicit()\n\t\tattempts   int\n\t\ttypeStr    string\n\t)\n\tif isImplicit {\n\t\ttypeStr = \"implicit\"\n\t} else {\n\t\ttypeStr = \"explicit\"\n\t}\n\n\tconst connFmt = \"Connecting to %s gateway %q (%s) at %s (attempt %v)\"\n\tconst connErrFmt = \"Error connecting to %s gateway %q (%s) at %s (attempt %v): %v\"\n\n\tfor s.isRunning() {\n\t\turls := cfg.getURLs()\n\t\tif len(urls) == 0 {\n\t\t\tbreak\n\t\t}\n\t\tattempts++\n\t\treport := s.shouldReportConnectErr(firstConnect, attempts)\n\t\t// Iteration is random\n\t\tfor _, u := range urls {\n\t\t\taddress, err := s.getRandomIP(s.gateway.resolver, u.Host, nil)\n\t\t\tif err != nil {\n\t\t\t\ts.Errorf(\"Error getting IP for %s gateway %q (%s): %v\", typeStr, cfg.Name, u.Host, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif report {\n\t\t\t\ts.Noticef(connFmt, typeStr, cfg.Name, u.Host, address, attempts)\n\t\t\t} else {\n\t\t\t\ts.Debugf(connFmt, typeStr, cfg.Name, u.Host, address, attempts)\n\t\t\t}\n\t\t\tconn, err := natsDialTimeout(\"tcp\", address, DEFAULT_ROUTE_DIAL)\n\t\t\tif err == nil {\n\t\t\t\t// We could connect, create the gateway connection and return.\n\t\t\t\ts.createGateway(cfg, u, conn)\n\t\t\t\treturn\n\t\t\t}\n\t\t\tif report {\n\t\t\t\ts.Errorf(connErrFmt, typeStr, cfg.Name, u.Host, address, attempts, err)\n\t\t\t} else {\n\t\t\t\ts.Debugf(connErrFmt, typeStr, cfg.Name, u.Host, address, attempts, err)\n\t\t\t}\n\t\t\t// Break this loop if server is being shutdown...\n\t\t\tif !s.isRunning() {\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\tif isImplicit {\n\t\t\tif opts.Gateway.ConnectRetries == 0 || attempts > opts.Gateway.ConnectRetries {\n\t\t\t\ts.gateway.Lock()\n\t\t\t\t// We could have just accepted an inbound for this remote gateway.\n\t\t\t\t// So if there is an inbound, let's try again to connect.\n\t\t\t\tif s.gateway.hasInbound(cfg.Name) {\n\t\t\t\t\ts.gateway.Unlock()\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tdelete(s.gateway.remotes, cfg.Name)\n\t\t\t\ts.gateway.Unlock()\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t\tselect {\n\t\tcase <-s.quitCh:\n\t\t\treturn\n\t\tcase <-time.After(gatewayConnectDelay):\n\t\t\tcontinue\n\t\t}\n\t}\n}\n\n// Returns true if there is an inbound for the given `name`.\n// Lock held on entry.\nfunc (g *srvGateway) hasInbound(name string) bool {\n\tfor _, ig := range g.in {\n\t\tig.mu.Lock()\n\t\tigname := ig.gw.name\n\t\tig.mu.Unlock()\n\t\tif igname == name {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\n// Called when a gateway connection is either accepted or solicited.\n// If accepted, the gateway is marked as inbound.\n// If solicited, the gateway is marked as outbound.\nfunc (s *Server) createGateway(cfg *gatewayCfg, url *url.URL, conn net.Conn) {\n\t// Snapshot server options.\n\topts := s.getOpts()\n\n\tnow := time.Now()\n\tc := &client{srv: s, nc: conn, start: now, last: now, kind: GATEWAY}\n\n\t// Are we creating the gateway based on the configuration\n\tsolicit := cfg != nil\n\tvar tlsRequired bool\n\n\ts.gateway.RLock()\n\tinfoJSON := s.gateway.infoJSON\n\ts.gateway.RUnlock()\n\n\t// Perform some initialization under the client lock\n\tc.mu.Lock()\n\tc.initClient()\n\tc.gw = &gateway{}\n\tif solicit {\n\t\t// This is an outbound gateway connection\n\t\tcfg.RLock()\n\t\ttlsRequired = cfg.TLSConfig != nil\n\t\tcfgName := cfg.Name\n\t\tcfg.RUnlock()\n\t\tc.gw.outbound = true\n\t\tc.gw.name = cfgName\n\t\tc.gw.cfg = cfg\n\t\tcfg.bumpConnAttempts()\n\t\t// Since we are delaying the connect until after receiving\n\t\t// the remote's INFO protocol, save the URL we need to connect to.\n\t\tc.gw.connectURL = url\n\n\t\tc.Noticef(\"Creating outbound gateway connection to %q\", cfgName)\n\t} else {\n\t\tc.flags.set(expectConnect)\n\t\t// Inbound gateway connection\n\t\tc.Noticef(\"Processing inbound gateway connection\")\n\t\t// Check if TLS is required for inbound GW connections.\n\t\ttlsRequired = opts.Gateway.TLSConfig != nil\n\t\t// We expect a CONNECT from the accepted connection.\n\t\tc.setAuthTimer(secondsToDuration(opts.Gateway.AuthTimeout))\n\t}\n\n\t// Check for TLS\n\tif tlsRequired {\n\t\tvar tlsConfig *tls.Config\n\t\tvar tlsName string\n\t\tvar timeout float64\n\n\t\tif solicit {\n\t\t\tvar (\n\t\t\t\tmustStaple = opts.OCSPConfig != nil && opts.OCSPConfig.Mode == OCSPModeAlways\n\t\t\t\tclientCB   func(*tls.CertificateRequestInfo) (*tls.Certificate, error)\n\t\t\t\tverifyCB   func(tls.ConnectionState) error\n\t\t\t)\n\t\t\t// Snapshot callbacks for OCSP outside an ongoing reload which might be happening.\n\t\t\tif mustStaple {\n\t\t\t\ts.reloadMu.RLock()\n\t\t\t\ts.optsMu.RLock()\n\t\t\t\tclientCB = s.opts.Gateway.TLSConfig.GetClientCertificate\n\t\t\t\tverifyCB = s.opts.Gateway.TLSConfig.VerifyConnection\n\t\t\t\ts.optsMu.RUnlock()\n\t\t\t\ts.reloadMu.RUnlock()\n\t\t\t}\n\n\t\t\tcfg.RLock()\n\t\t\ttlsName = cfg.tlsName\n\t\t\ttlsConfig = cfg.TLSConfig.Clone()\n\t\t\ttimeout = cfg.TLSTimeout\n\n\t\t\t// Ensure that OCSP callbacks are always setup on gateway reconnect when OCSP policy is set to always.\n\t\t\tif mustStaple {\n\t\t\t\tif clientCB != nil && tlsConfig.GetClientCertificate == nil {\n\t\t\t\t\ttlsConfig.GetClientCertificate = clientCB\n\t\t\t\t}\n\t\t\t\tif verifyCB != nil && tlsConfig.VerifyConnection == nil {\n\t\t\t\t\ttlsConfig.VerifyConnection = verifyCB\n\t\t\t\t}\n\t\t\t}\n\t\t\tcfg.RUnlock()\n\t\t} else {\n\t\t\ttlsConfig = opts.Gateway.TLSConfig\n\t\t\ttimeout = opts.Gateway.TLSTimeout\n\t\t}\n\n\t\t// Perform (either server or client side) TLS handshake.\n\t\tif resetTLSName, err := c.doTLSHandshake(\"gateway\", solicit, url, tlsConfig, tlsName, timeout, opts.Gateway.TLSPinnedCerts); err != nil {\n\t\t\tif resetTLSName {\n\t\t\t\tcfg.Lock()\n\t\t\t\tcfg.tlsName = _EMPTY_\n\t\t\t\tcfg.Unlock()\n\t\t\t}\n\t\t\tc.mu.Unlock()\n\t\t\treturn\n\t\t}\n\t}\n\n\t// Do final client initialization\n\tc.in.pacache = make(map[string]*perAccountCache)\n\tif solicit {\n\t\t// This is an outbound gateway connection\n\t\tc.gw.outsim = &sync.Map{}\n\t} else {\n\t\t// Inbound gateway connection\n\t\tc.gw.insim = make(map[string]*insie)\n\t}\n\n\t// Register in temp map for now until gateway properly registered\n\t// in out or in gateways.\n\tif !s.addToTempClients(c.cid, c) {\n\t\tc.mu.Unlock()\n\t\tc.closeConnection(ServerShutdown)\n\t\treturn\n\t}\n\n\t// Only send if we accept a connection. Will send CONNECT+INFO as an\n\t// outbound only after processing peer's INFO protocol.\n\tif !solicit {\n\t\tc.enqueueProto(infoJSON)\n\t}\n\n\t// Spin up the read loop.\n\ts.startGoRoutine(func() { c.readLoop(nil) })\n\n\t// Spin up the write loop.\n\ts.startGoRoutine(func() { c.writeLoop() })\n\n\tif tlsRequired {\n\t\tc.Debugf(\"TLS handshake complete\")\n\t\tcs := c.nc.(*tls.Conn).ConnectionState()\n\t\tc.Debugf(\"TLS version %s, cipher suite %s\", tlsVersion(cs.Version), tlsCipher(cs.CipherSuite))\n\t}\n\n\t// For outbound, we can't set the normal ping timer yet since the other\n\t// side would fail with a parse error should it receive anything but the\n\t// CONNECT protocol as the first protocol. We still want to make sure\n\t// that the connection is not stale until the first INFO from the remote\n\t// is received.\n\tif solicit {\n\t\tc.watchForStaleConnection(adjustPingInterval(GATEWAY, opts.PingInterval), opts.MaxPingsOut)\n\t}\n\n\tc.mu.Unlock()\n\n\t// Announce ourselves again to new connections.\n\tif solicit && s.EventsEnabled() {\n\t\ts.sendStatszUpdate()\n\t}\n}\n\n// Builds and sends the CONNECT protocol for a gateway.\n// Client lock held on entry.\nfunc (c *client) sendGatewayConnect(opts *Options) {\n\t// FIXME: This can race with updateRemotesTLSConfig\n\ttlsRequired := c.gw.cfg.TLSConfig != nil\n\turl := c.gw.connectURL\n\tc.gw.connectURL = nil\n\tvar user, pass string\n\tif userInfo := url.User; userInfo != nil {\n\t\tuser = userInfo.Username()\n\t\tpass, _ = userInfo.Password()\n\t} else if opts != nil {\n\t\tuser = opts.Gateway.Username\n\t\tpass = opts.Gateway.Password\n\t}\n\tcinfo := connectInfo{\n\t\tVerbose:  false,\n\t\tPedantic: false,\n\t\tUser:     user,\n\t\tPass:     pass,\n\t\tTLS:      tlsRequired,\n\t\tName:     c.srv.info.ID,\n\t\tGateway:  c.srv.gateway.name,\n\t}\n\tb, err := json.Marshal(cinfo)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\tc.enqueueProto([]byte(fmt.Sprintf(ConProto, b)))\n}\n\n// Process the CONNECT protocol from a gateway connection.\n// Returns an error to the connection if the CONNECT is not from a gateway\n// (for instance a client or route connecting to the gateway port), or\n// if the destination does not match the gateway name of this server.\n//\n// <Invoked from inbound connection's readLoop>\nfunc (c *client) processGatewayConnect(arg []byte) error {\n\tconnect := &connectInfo{}\n\tif err := json.Unmarshal(arg, connect); err != nil {\n\t\treturn err\n\t}\n\n\t// Coming from a client or a route, reject\n\tif connect.Gateway == \"\" {\n\t\tc.sendErrAndErr(ErrClientOrRouteConnectedToGatewayPort.Error())\n\t\tc.closeConnection(WrongPort)\n\t\treturn ErrClientOrRouteConnectedToGatewayPort\n\t}\n\n\tc.mu.Lock()\n\ts := c.srv\n\tc.mu.Unlock()\n\n\t// If we reject unknown gateways, make sure we have it configured,\n\t// otherwise return an error.\n\tif s.gateway.rejectUnknown() && s.getRemoteGateway(connect.Gateway) == nil {\n\t\tc.Errorf(\"Rejecting connection from gateway %q\", connect.Gateway)\n\t\tc.sendErr(fmt.Sprintf(\"Connection to gateway %q rejected\", s.getGatewayName()))\n\t\tc.closeConnection(WrongGateway)\n\t\treturn ErrWrongGateway\n\t}\n\n\tc.mu.Lock()\n\tc.gw.connected = true\n\t// Set the Ping timer after sending connect and info.\n\tc.setFirstPingTimer()\n\tc.mu.Unlock()\n\n\treturn nil\n}\n\n// Process the INFO protocol from a gateway connection.\n//\n// If the gateway connection is an outbound (this server initiated the connection),\n// this function checks that the incoming INFO contains the Gateway field. If empty,\n// it means that this is a response from an older server or that this server connected\n// to the wrong port.\n// The outbound gateway may also receive a gossip INFO protocol from the remote gateway,\n// indicating other gateways that the remote knows about. This server will try to connect\n// to those gateways (if not explicitly configured or already implicitly connected).\n// In both cases (explicit or implicit), the local cluster is notified about the existence\n// of this new gateway. This allows servers in the cluster to ensure that they have an\n// outbound connection to this gateway.\n//\n// For an inbound gateway, the gateway is simply registered and the info protocol\n// is saved to be used after processing the CONNECT.\n//\n// <Invoked from both inbound/outbound readLoop's connection>\nfunc (c *client) processGatewayInfo(info *Info) {\n\tvar (\n\t\tgwName string\n\t\tcfg    *gatewayCfg\n\t)\n\tc.mu.Lock()\n\ts := c.srv\n\tcid := c.cid\n\n\t// Check if this is the first INFO. (this call sets the flag if not already set).\n\tisFirstINFO := c.flags.setIfNotSet(infoReceived)\n\n\tisOutbound := c.gw.outbound\n\tif isOutbound {\n\t\tgwName = c.gw.name\n\t\tcfg = c.gw.cfg\n\t} else if isFirstINFO {\n\t\tc.gw.name = info.Gateway\n\t}\n\tif isFirstINFO {\n\t\tc.opts.Name = info.ID\n\t\t// Get the protocol version from the INFO protocol. This will be checked\n\t\t// to see if this connection supports message tracing for instance.\n\t\tc.opts.Protocol = info.Proto\n\t\tc.gw.remoteName = info.Name\n\t}\n\tc.mu.Unlock()\n\n\t// For an outbound connection...\n\tif isOutbound {\n\t\t// Check content of INFO for fields indicating that it comes from a gateway.\n\t\t// If we incorrectly connect to the wrong port (client or route), we won't\n\t\t// have the Gateway field set.\n\t\tif info.Gateway == \"\" {\n\t\t\tc.sendErrAndErr(fmt.Sprintf(\"Attempt to connect to gateway %q using wrong port\", gwName))\n\t\t\tc.closeConnection(WrongPort)\n\t\t\treturn\n\t\t}\n\t\t// Check that the gateway name we got is what we expect\n\t\tif info.Gateway != gwName {\n\t\t\t// Unless this is the very first INFO, it may be ok if this is\n\t\t\t// a gossip request to connect to other gateways.\n\t\t\tif !isFirstINFO && info.GatewayCmd == gatewayCmdGossip {\n\t\t\t\t// If we are configured to reject unknown, do not attempt to\n\t\t\t\t// connect to one that we don't have configured.\n\t\t\t\tif s.gateway.rejectUnknown() && s.getRemoteGateway(info.Gateway) == nil {\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t\ts.processImplicitGateway(info)\n\t\t\t\treturn\n\t\t\t}\n\t\t\t// Otherwise, this is a failure...\n\t\t\t// We are reporting this error in the log...\n\t\t\tc.Errorf(\"Failing connection to gateway %q, remote gateway name is %q\",\n\t\t\t\tgwName, info.Gateway)\n\t\t\t// ...and sending this back to the remote so that the error\n\t\t\t// makes more sense in the remote server's log.\n\t\t\tc.sendErr(fmt.Sprintf(\"Connection from %q rejected, wanted to connect to %q, this is %q\",\n\t\t\t\ts.getGatewayName(), gwName, info.Gateway))\n\t\t\tc.closeConnection(WrongGateway)\n\t\t\treturn\n\t\t}\n\n\t\t// Check for duplicate server name with servers in our cluster\n\t\tif s.isDuplicateServerName(info.Name) {\n\t\t\tc.Errorf(\"Remote server has a duplicate name: %q\", info.Name)\n\t\t\tc.closeConnection(DuplicateServerName)\n\t\t\treturn\n\t\t}\n\n\t\t// Possibly add URLs that we get from the INFO protocol.\n\t\tif len(info.GatewayURLs) > 0 {\n\t\t\tcfg.updateURLs(info.GatewayURLs)\n\t\t}\n\n\t\t// If this is the first INFO, send our connect\n\t\tif isFirstINFO {\n\t\t\ts.gateway.RLock()\n\t\t\tinfoJSON := s.gateway.infoJSON\n\t\t\ts.gateway.RUnlock()\n\n\t\t\tsupportsHeaders := s.supportsHeaders()\n\t\t\topts := s.getOpts()\n\n\t\t\t// Note, if we want to support NKeys, then we would get the nonce\n\t\t\t// from this INFO protocol and can sign it in the CONNECT we are\n\t\t\t// going to send now.\n\t\t\tc.mu.Lock()\n\t\t\tc.gw.interestOnlyMode = info.GatewayIOM\n\t\t\tc.sendGatewayConnect(opts)\n\t\t\tc.Debugf(\"Gateway connect protocol sent to %q\", gwName)\n\t\t\t// Send INFO too\n\t\t\tc.enqueueProto(infoJSON)\n\t\t\tc.gw.useOldPrefix = !info.GatewayNRP\n\t\t\tc.headers = supportsHeaders && info.Headers\n\t\t\tc.mu.Unlock()\n\n\t\t\t// Register as an outbound gateway.. if we had a protocol to ack our connect,\n\t\t\t// then we should do that when process that ack.\n\t\t\tif s.registerOutboundGatewayConnection(gwName, c) {\n\t\t\t\tc.Noticef(\"Outbound gateway connection to %q (%s) registered\", gwName, info.ID)\n\t\t\t\t// Now that the outbound gateway is registered, we can remove from temp map.\n\t\t\t\ts.removeFromTempClients(cid)\n\t\t\t\t// Set the Ping timer after sending connect and info.\n\t\t\t\tc.mu.Lock()\n\t\t\t\tc.setFirstPingTimer()\n\t\t\t\tc.mu.Unlock()\n\t\t\t} else {\n\t\t\t\t// There was a bug that would cause a connection to possibly\n\t\t\t\t// be called twice resulting in reconnection of twice the\n\t\t\t\t// same outbound connection. The issue is fixed, but adding\n\t\t\t\t// defensive code above that if we did not register this connection\n\t\t\t\t// because we already have an outbound for this name, then\n\t\t\t\t// close this connection (and make sure it does not try to reconnect)\n\t\t\t\tc.mu.Lock()\n\t\t\t\tc.flags.set(noReconnect)\n\t\t\t\tc.mu.Unlock()\n\t\t\t\tc.closeConnection(WrongGateway)\n\t\t\t\treturn\n\t\t\t}\n\t\t} else if info.GatewayCmd > 0 {\n\t\t\tswitch info.GatewayCmd {\n\t\t\tcase gatewayCmdAllSubsStart:\n\t\t\t\tc.gatewayAllSubsReceiveStart(info)\n\t\t\t\treturn\n\t\t\tcase gatewayCmdAllSubsComplete:\n\t\t\t\tc.gatewayAllSubsReceiveComplete(info)\n\t\t\t\treturn\n\t\t\tdefault:\n\t\t\t\ts.Warnf(\"Received unknown command %v from gateway %q\", info.GatewayCmd, gwName)\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\n\t\t// Flood local cluster with information about this gateway.\n\t\t// Servers in this cluster will ensure that they have (or otherwise create)\n\t\t// an outbound connection to this gateway.\n\t\ts.forwardNewGatewayToLocalCluster(info)\n\n\t} else if isFirstINFO {\n\t\t// This is the first INFO of an inbound connection...\n\n\t\t// Check for duplicate server name with servers in our cluster\n\t\tif s.isDuplicateServerName(info.Name) {\n\t\t\tc.Errorf(\"Remote server has a duplicate name: %q\", info.Name)\n\t\t\tc.closeConnection(DuplicateServerName)\n\t\t\treturn\n\t\t}\n\n\t\ts.registerInboundGatewayConnection(cid, c)\n\t\tc.Noticef(\"Inbound gateway connection from %q (%s) registered\", info.Gateway, info.ID)\n\n\t\t// Now that it is registered, we can remove from temp map.\n\t\ts.removeFromTempClients(cid)\n\n\t\t// Send our QSubs.\n\t\ts.sendQueueSubsToGateway(c)\n\n\t\t// Initiate outbound connection. This function will behave correctly if\n\t\t// we have already one.\n\t\ts.processImplicitGateway(info)\n\n\t\t// Send back to the server that initiated this gateway connection the\n\t\t// list of all remote gateways known on this server.\n\t\ts.gossipGatewaysToInboundGateway(info.Gateway, c)\n\n\t\t// Now make sure if we have any knowledge of connected leafnodes that we resend the\n\t\t// connect events to switch those accounts into interest only mode.\n\t\ts.mu.Lock()\n\t\ts.ensureGWsInterestOnlyForLeafNodes()\n\t\ts.mu.Unlock()\n\t\tjs := s.js.Load()\n\n\t\t// If running in some tests, maintain the original behavior.\n\t\tif gwDoNotForceInterestOnlyMode && js != nil {\n\t\t\t// Switch JetStream accounts to interest-only mode.\n\t\t\tvar accounts []string\n\t\t\tjs.mu.Lock()\n\t\t\tif len(js.accounts) > 0 {\n\t\t\t\taccounts = make([]string, 0, len(js.accounts))\n\t\t\t\tfor accName := range js.accounts {\n\t\t\t\t\taccounts = append(accounts, accName)\n\t\t\t\t}\n\t\t\t}\n\t\t\tjs.mu.Unlock()\n\t\t\tfor _, accName := range accounts {\n\t\t\t\tif acc, err := s.LookupAccount(accName); err == nil && acc != nil {\n\t\t\t\t\tif acc.JetStreamEnabled() {\n\t\t\t\t\t\ts.switchAccountToInterestMode(acc.GetName())\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t} else if !gwDoNotForceInterestOnlyMode {\n\t\t\t// Starting 2.9.0, we are phasing out the optimistic mode, so change\n\t\t\t// all accounts to interest-only mode, unless instructed not to do so\n\t\t\t// in some tests.\n\t\t\ts.accounts.Range(func(_, v any) bool {\n\t\t\t\tacc := v.(*Account)\n\t\t\t\ts.switchAccountToInterestMode(acc.GetName())\n\t\t\t\treturn true\n\t\t\t})\n\t\t}\n\t}\n}\n\n// Sends to the given inbound gateway connection a gossip INFO protocol\n// for each gateway known by this server. This allows for a \"full mesh\"\n// of gateways.\nfunc (s *Server) gossipGatewaysToInboundGateway(gwName string, c *client) {\n\tgw := s.gateway\n\tgw.RLock()\n\tdefer gw.RUnlock()\n\tfor gwCfgName, cfg := range gw.remotes {\n\t\t// Skip the gateway that we just created\n\t\tif gwCfgName == gwName {\n\t\t\tcontinue\n\t\t}\n\t\tinfo := Info{\n\t\t\tID:         s.info.ID,\n\t\t\tGatewayCmd: gatewayCmdGossip,\n\t\t}\n\t\turls := cfg.getURLsAsStrings()\n\t\tif len(urls) > 0 {\n\t\t\tinfo.Gateway = gwCfgName\n\t\t\tinfo.GatewayURLs = urls\n\t\t\tb, _ := json.Marshal(&info)\n\t\t\tc.mu.Lock()\n\t\t\tc.enqueueProto([]byte(fmt.Sprintf(InfoProto, b)))\n\t\t\tc.mu.Unlock()\n\t\t}\n\t}\n}\n\n// Sends the INFO protocol of a gateway to all routes known by this server.\nfunc (s *Server) forwardNewGatewayToLocalCluster(oinfo *Info) {\n\t// Need to protect s.routes here, so use server's lock\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\n\t// We don't really need the ID to be set, but, we need to make sure\n\t// that it is not set to the server ID so that if we were to connect\n\t// to an older server that does not expect a \"gateway\" INFO, it\n\t// would think that it needs to create an implicit route (since info.ID\n\t// would not match the route's remoteID), but will fail to do so because\n\t// the sent protocol will not have host/port defined.\n\tinfo := &Info{\n\t\tID:          \"GW\" + s.info.ID,\n\t\tName:        s.getOpts().ServerName,\n\t\tGateway:     oinfo.Gateway,\n\t\tGatewayURLs: oinfo.GatewayURLs,\n\t\tGatewayCmd:  gatewayCmdGossip,\n\t}\n\tb, _ := json.Marshal(info)\n\tinfoJSON := []byte(fmt.Sprintf(InfoProto, b))\n\n\ts.forEachRemote(func(r *client) {\n\t\tr.mu.Lock()\n\t\tr.enqueueProto(infoJSON)\n\t\tr.mu.Unlock()\n\t})\n}\n\n// Sends queue subscriptions interest to remote gateway.\n// This is sent from the inbound side, that is, the side that receives\n// messages from the remote's outbound connection. This side is\n// the one sending the subscription interest.\nfunc (s *Server) sendQueueSubsToGateway(c *client) {\n\ts.sendSubsToGateway(c, _EMPTY_)\n}\n\n// Sends all subscriptions for the given account to the remove gateway\n// This is sent from the inbound side, that is, the side that receives\n// messages from the remote's outbound connection. This side is\n// the one sending the subscription interest.\nfunc (s *Server) sendAccountSubsToGateway(c *client, accName string) {\n\ts.sendSubsToGateway(c, accName)\n}\n\nfunc gwBuildSubProto(buf *bytes.Buffer, accName string, acc map[string]*sitally, doQueues bool) {\n\tfor saq, si := range acc {\n\t\tif doQueues && si.q || !doQueues && !si.q {\n\t\t\tbuf.Write(rSubBytes)\n\t\t\tbuf.WriteString(accName)\n\t\t\tbuf.WriteByte(' ')\n\t\t\t// For queue subs (si.q is true), saq will be\n\t\t\t// subject + ' ' + queue, for plain subs, this is\n\t\t\t// just the subject.\n\t\t\tbuf.WriteString(saq)\n\t\t\tif doQueues {\n\t\t\t\tbuf.WriteString(\" 1\")\n\t\t\t}\n\t\t\tbuf.WriteString(CR_LF)\n\t\t}\n\t}\n}\n\n// Sends subscriptions to remote gateway.\nfunc (s *Server) sendSubsToGateway(c *client, accountName string) {\n\tvar (\n\t\tbufa = [32 * 1024]byte{}\n\t\tbbuf = bytes.NewBuffer(bufa[:0])\n\t)\n\n\tgw := s.gateway\n\n\t// This needs to run under this lock for the whole duration\n\tgw.pasi.Lock()\n\tdefer gw.pasi.Unlock()\n\n\t// If account is specified...\n\tif accountName != _EMPTY_ {\n\t\t// Simply send all plain subs (no queues) for this specific account\n\t\tgwBuildSubProto(bbuf, accountName, gw.pasi.m[accountName], false)\n\t\t// Instruct to send all subs (RS+/-) for this account from now on.\n\t\tc.mu.Lock()\n\t\te := c.gw.insim[accountName]\n\t\tif e == nil {\n\t\t\te = &insie{}\n\t\t\tc.gw.insim[accountName] = e\n\t\t}\n\t\te.mode = InterestOnly\n\t\tc.mu.Unlock()\n\t} else {\n\t\t// Send queues for all accounts\n\t\tfor accName, acc := range gw.pasi.m {\n\t\t\tgwBuildSubProto(bbuf, accName, acc, true)\n\t\t}\n\t}\n\n\tbuf := bbuf.Bytes()\n\n\t// Nothing to send.\n\tif len(buf) == 0 {\n\t\treturn\n\t}\n\tif len(buf) > cap(bufa) {\n\t\ts.Debugf(\"Sending subscriptions to %q, buffer size: %v\", c.gw.name, len(buf))\n\t}\n\t// Send\n\tc.mu.Lock()\n\tc.enqueueProto(buf)\n\tc.Debugf(\"Sent queue subscriptions to gateway\")\n\tc.mu.Unlock()\n}\n\n// This is invoked when getting an INFO protocol for gateway on the ROUTER port.\n// This function will then execute appropriate function based on the command\n// contained in the protocol.\n// <Invoked from a route connection's readLoop>\nfunc (s *Server) processGatewayInfoFromRoute(info *Info, routeSrvID string) {\n\tswitch info.GatewayCmd {\n\tcase gatewayCmdGossip:\n\t\ts.processImplicitGateway(info)\n\tdefault:\n\t\ts.Errorf(\"Unknown command %d from server %v\", info.GatewayCmd, routeSrvID)\n\t}\n}\n\n// Sends INFO protocols to the given route connection for each known Gateway.\n// These will be processed by the route and delegated to the gateway code to\n// invoke processImplicitGateway.\nfunc (s *Server) sendGatewayConfigsToRoute(route *client) {\n\tgw := s.gateway\n\tgw.RLock()\n\t// Send only to gateways for which we have actual outbound connection to.\n\tif len(gw.out) == 0 {\n\t\tgw.RUnlock()\n\t\treturn\n\t}\n\t// Collect gateway configs for which we have an outbound connection.\n\tgwCfgsa := [16]*gatewayCfg{}\n\tgwCfgs := gwCfgsa[:0]\n\tfor _, c := range gw.out {\n\t\tc.mu.Lock()\n\t\tif c.gw.cfg != nil {\n\t\t\tgwCfgs = append(gwCfgs, c.gw.cfg)\n\t\t}\n\t\tc.mu.Unlock()\n\t}\n\tgw.RUnlock()\n\tif len(gwCfgs) == 0 {\n\t\treturn\n\t}\n\n\t// Check forwardNewGatewayToLocalCluster() as to why we set ID this way.\n\tinfo := Info{\n\t\tID:         \"GW\" + s.info.ID,\n\t\tGatewayCmd: gatewayCmdGossip,\n\t}\n\tfor _, cfg := range gwCfgs {\n\t\turls := cfg.getURLsAsStrings()\n\t\tif len(urls) > 0 {\n\t\t\tinfo.Gateway = cfg.Name\n\t\t\tinfo.GatewayURLs = urls\n\t\t\tb, _ := json.Marshal(&info)\n\t\t\troute.mu.Lock()\n\t\t\troute.enqueueProto([]byte(fmt.Sprintf(InfoProto, b)))\n\t\t\troute.mu.Unlock()\n\t\t}\n\t}\n}\n\n// Initiates a gateway connection using the info contained in the INFO protocol.\n// If a gateway with the same name is already registered (either because explicitly\n// configured, or already implicitly connected), this function will augmment the\n// remote URLs with URLs present in the info protocol and return.\n// Otherwise, this function will register this remote (to prevent multiple connections\n// to the same remote) and call solicitGateway (which will run in a different go-routine).\nfunc (s *Server) processImplicitGateway(info *Info) {\n\ts.gateway.Lock()\n\tdefer s.gateway.Unlock()\n\t// Name of the gateway to connect to is the Info.Gateway field.\n\tgwName := info.Gateway\n\t// If this is our name, bail.\n\tif gwName == s.gateway.name {\n\t\treturn\n\t}\n\t// Check if we already have this config, and if so, we are done\n\tcfg := s.gateway.remotes[gwName]\n\tif cfg != nil {\n\t\t// However, possibly augment the list of URLs with the given\n\t\t// info.GatewayURLs content.\n\t\tcfg.Lock()\n\t\tcfg.addURLs(info.GatewayURLs)\n\t\tcfg.Unlock()\n\t\treturn\n\t}\n\topts := s.getOpts()\n\tcfg = &gatewayCfg{\n\t\tRemoteGatewayOpts: &RemoteGatewayOpts{Name: gwName},\n\t\thash:              getGWHash(gwName),\n\t\toldHash:           getOldHash(gwName),\n\t\turls:              make(map[string]*url.URL, len(info.GatewayURLs)),\n\t\timplicit:          true,\n\t}\n\tif opts.Gateway.TLSConfig != nil {\n\t\tcfg.TLSConfig = opts.Gateway.TLSConfig.Clone()\n\t\tcfg.TLSTimeout = opts.Gateway.TLSTimeout\n\t}\n\n\t// Since we know we don't have URLs (no config, so just based on what we\n\t// get from INFO), directly call addURLs(). We don't need locking since\n\t// we just created that structure and no one else has access to it yet.\n\tcfg.addURLs(info.GatewayURLs)\n\t// If there is no URL, we can't proceed.\n\tif len(cfg.urls) == 0 {\n\t\treturn\n\t}\n\ts.gateway.remotes[gwName] = cfg\n\ts.startGoRoutine(func() {\n\t\ts.solicitGateway(cfg, true)\n\t\ts.grWG.Done()\n\t})\n}\n\n// NumOutboundGateways is public here mostly for testing.\nfunc (s *Server) NumOutboundGateways() int {\n\treturn s.numOutboundGateways()\n}\n\n// Returns the number of outbound gateway connections\nfunc (s *Server) numOutboundGateways() int {\n\ts.gateway.RLock()\n\tn := len(s.gateway.out)\n\ts.gateway.RUnlock()\n\treturn n\n}\n\n// Returns the number of inbound gateway connections\nfunc (s *Server) numInboundGateways() int {\n\ts.gateway.RLock()\n\tn := len(s.gateway.in)\n\ts.gateway.RUnlock()\n\treturn n\n}\n\n// Returns the remoteGateway (if any) that has the given `name`\nfunc (s *Server) getRemoteGateway(name string) *gatewayCfg {\n\ts.gateway.RLock()\n\tcfg := s.gateway.remotes[name]\n\ts.gateway.RUnlock()\n\treturn cfg\n}\n\n// Used in tests\nfunc (g *gatewayCfg) bumpConnAttempts() {\n\tg.Lock()\n\tg.connAttempts++\n\tg.Unlock()\n}\n\n// Used in tests\nfunc (g *gatewayCfg) getConnAttempts() int {\n\tg.Lock()\n\tca := g.connAttempts\n\tg.Unlock()\n\treturn ca\n}\n\n// Used in tests\nfunc (g *gatewayCfg) resetConnAttempts() {\n\tg.Lock()\n\tg.connAttempts = 0\n\tg.Unlock()\n}\n\n// Returns if this remote gateway is implicit or not.\nfunc (g *gatewayCfg) isImplicit() bool {\n\tg.RLock()\n\tii := g.implicit\n\tg.RUnlock()\n\treturn ii\n}\n\n// getURLs returns an array of URLs in random order suitable for\n// an iteration to try to connect.\nfunc (g *gatewayCfg) getURLs() []*url.URL {\n\tg.RLock()\n\ta := make([]*url.URL, 0, len(g.urls))\n\tfor _, u := range g.urls {\n\t\ta = append(a, u)\n\t}\n\tg.RUnlock()\n\t// Map iteration is random, but not that good with small maps.\n\trand.Shuffle(len(a), func(i, j int) {\n\t\ta[i], a[j] = a[j], a[i]\n\t})\n\treturn a\n}\n\n// Similar to getURLs but returns the urls as an array of strings.\nfunc (g *gatewayCfg) getURLsAsStrings() []string {\n\tg.RLock()\n\ta := make([]string, 0, len(g.urls))\n\tfor _, u := range g.urls {\n\t\ta = append(a, u.Host)\n\t}\n\tg.RUnlock()\n\treturn a\n}\n\n// updateURLs creates the urls map with the content of the config's URLs array\n// and the given array that we get from the INFO protocol.\nfunc (g *gatewayCfg) updateURLs(infoURLs []string) {\n\tg.Lock()\n\t// Clear the map...\n\tg.urls = make(map[string]*url.URL, len(g.URLs)+len(infoURLs))\n\t// Add the urls from the config URLs array.\n\tfor _, u := range g.URLs {\n\t\tg.urls[u.Host] = u\n\t}\n\t// Then add the ones from the infoURLs array we got.\n\tg.addURLs(infoURLs)\n\t// The call above will set varzUpdateURLs only when finding ULRs in infoURLs\n\t// that are not present in the config. That does not cover the case where\n\t// previously \"discovered\" URLs are now gone. We could check \"before\" size\n\t// of g.urls and if bigger than current size, set the boolean to true.\n\t// Not worth it... simply set this to true to allow a refresh of gateway\n\t// URLs in varz.\n\tg.varzUpdateURLs = true\n\tg.Unlock()\n}\n\n// Saves the hostname of the given URL (if not already done).\n// This may be used as the ServerName of the TLSConfig when initiating a\n// TLS connection.\n// Write lock held on entry.\nfunc (g *gatewayCfg) saveTLSHostname(u *url.URL) {\n\tif g.TLSConfig != nil && g.tlsName == \"\" && net.ParseIP(u.Hostname()) == nil {\n\t\tg.tlsName = u.Hostname()\n\t}\n}\n\n// add URLs from the given array to the urls map only if not already present.\n// remoteGateway write lock is assumed to be held on entry.\n// Write lock is held on entry.\nfunc (g *gatewayCfg) addURLs(infoURLs []string) {\n\tvar scheme string\n\tif g.TLSConfig != nil {\n\t\tscheme = \"tls\"\n\t} else {\n\t\tscheme = \"nats\"\n\t}\n\tfor _, iu := range infoURLs {\n\t\tif _, present := g.urls[iu]; !present {\n\t\t\t// Urls in Info.GatewayURLs come without scheme. Add it to parse\n\t\t\t// the url (otherwise it fails).\n\t\t\tif u, err := url.Parse(fmt.Sprintf(\"%s://%s\", scheme, iu)); err == nil {\n\t\t\t\t// Also, if a tlsName has not been set yet and we are dealing\n\t\t\t\t// with a hostname and not a bare IP, save the hostname.\n\t\t\t\tg.saveTLSHostname(u)\n\t\t\t\t// Use u.Host for the key.\n\t\t\t\tg.urls[u.Host] = u\n\t\t\t\t// Signal that we have updated the list. Used by monitoring code.\n\t\t\t\tg.varzUpdateURLs = true\n\t\t\t}\n\t\t}\n\t}\n}\n\n// Adds this URL to the set of Gateway URLs.\n// Returns true if the URL has been added, false otherwise.\n// Server lock held on entry\nfunc (s *Server) addGatewayURL(urlStr string) bool {\n\ts.gateway.Lock()\n\tadded := s.gateway.URLs.addUrl(urlStr)\n\tif added {\n\t\ts.gateway.generateInfoJSON()\n\t}\n\ts.gateway.Unlock()\n\treturn added\n}\n\n// Removes this URL from the set of gateway URLs.\n// Returns true if the URL has been removed, false otherwise.\n// Server lock held on entry\nfunc (s *Server) removeGatewayURL(urlStr string) bool {\n\tif s.isShuttingDown() {\n\t\treturn false\n\t}\n\ts.gateway.Lock()\n\tremoved := s.gateway.URLs.removeUrl(urlStr)\n\tif removed {\n\t\ts.gateway.generateInfoJSON()\n\t}\n\ts.gateway.Unlock()\n\treturn removed\n}\n\n// Sends a Gateway's INFO to all inbound GW connections.\n// Server lock is held on entry\nfunc (s *Server) sendAsyncGatewayInfo() {\n\ts.gateway.RLock()\n\tfor _, ig := range s.gateway.in {\n\t\tig.mu.Lock()\n\t\tig.enqueueProto(s.gateway.infoJSON)\n\t\tig.mu.Unlock()\n\t}\n\ts.gateway.RUnlock()\n}\n\n// This returns the URL of the Gateway listen spec, or empty string\n// if the server has no gateway configured.\nfunc (s *Server) getGatewayURL() string {\n\ts.gateway.RLock()\n\turl := s.gateway.URL\n\ts.gateway.RUnlock()\n\treturn url\n}\n\n// Returns this server gateway name.\n// Same than calling s.gateway.getName()\nfunc (s *Server) getGatewayName() string {\n\t// This is immutable\n\treturn s.gateway.name\n}\n\n// All gateway connections (outbound and inbound) are put in the given map.\nfunc (s *Server) getAllGatewayConnections(conns map[uint64]*client) {\n\tgw := s.gateway\n\tgw.RLock()\n\tfor _, c := range gw.out {\n\t\tc.mu.Lock()\n\t\tcid := c.cid\n\t\tc.mu.Unlock()\n\t\tconns[cid] = c\n\t}\n\tfor cid, c := range gw.in {\n\t\tconns[cid] = c\n\t}\n\tgw.RUnlock()\n}\n\n// Register the given gateway connection (*client) in the inbound gateways\n// map. The key is the connection ID (like for clients and routes).\nfunc (s *Server) registerInboundGatewayConnection(cid uint64, gwc *client) {\n\ts.gateway.Lock()\n\ts.gateway.in[cid] = gwc\n\ts.gateway.Unlock()\n}\n\n// Register the given gateway connection (*client) in the outbound gateways\n// map with the given name as the key.\nfunc (s *Server) registerOutboundGatewayConnection(name string, gwc *client) bool {\n\ts.gateway.Lock()\n\tif _, exist := s.gateway.out[name]; exist {\n\t\ts.gateway.Unlock()\n\t\treturn false\n\t}\n\ts.gateway.out[name] = gwc\n\ts.gateway.outo = append(s.gateway.outo, gwc)\n\ts.gateway.orderOutboundConnectionsLocked()\n\ts.gateway.Unlock()\n\treturn true\n}\n\n// Returns the outbound gateway connection (*client) with the given name,\n// or nil if not found\nfunc (s *Server) getOutboundGatewayConnection(name string) *client {\n\ts.gateway.RLock()\n\tgwc := s.gateway.out[name]\n\ts.gateway.RUnlock()\n\treturn gwc\n}\n\n// Returns all outbound gateway connections in the provided array.\n// The order of the gateways is suited for the sending of a message.\n// Current ordering is based on individual gateway's RTT value.\nfunc (s *Server) getOutboundGatewayConnections(a *[]*client) {\n\ts.gateway.RLock()\n\tfor i := 0; i < len(s.gateway.outo); i++ {\n\t\t*a = append(*a, s.gateway.outo[i])\n\t}\n\ts.gateway.RUnlock()\n}\n\n// Orders the array of outbound connections.\n// Current ordering is by lowest RTT.\n// Gateway write lock is held on entry\nfunc (g *srvGateway) orderOutboundConnectionsLocked() {\n\t// Order the gateways by lowest RTT\n\tslices.SortFunc(g.outo, func(i, j *client) int { return cmp.Compare(i.getRTTValue(), j.getRTTValue()) })\n}\n\n// Orders the array of outbound connections.\n// Current ordering is by lowest RTT.\nfunc (g *srvGateway) orderOutboundConnections() {\n\tg.Lock()\n\tg.orderOutboundConnectionsLocked()\n\tg.Unlock()\n}\n\n// Returns all inbound gateway connections in the provided array\nfunc (s *Server) getInboundGatewayConnections(a *[]*client) {\n\ts.gateway.RLock()\n\tfor _, gwc := range s.gateway.in {\n\t\t*a = append(*a, gwc)\n\t}\n\ts.gateway.RUnlock()\n}\n\n// This is invoked when a gateway connection is closed and the server\n// is removing this connection from its state.\nfunc (s *Server) removeRemoteGatewayConnection(c *client) {\n\tc.mu.Lock()\n\tcid := c.cid\n\tisOutbound := c.gw.outbound\n\tgwName := c.gw.name\n\tif isOutbound && c.gw.outsim != nil {\n\t\t// We do this to allow the GC to release this connection.\n\t\t// Since the map is used by the rest of the code without client lock,\n\t\t// we can't simply set it to nil, instead, just make sure we empty it.\n\t\tc.gw.outsim.Range(func(k, _ any) bool {\n\t\t\tc.gw.outsim.Delete(k)\n\t\t\treturn true\n\t\t})\n\t}\n\tc.mu.Unlock()\n\n\tgw := s.gateway\n\tgw.Lock()\n\tif isOutbound {\n\t\tdelete(gw.out, gwName)\n\t\tlouto := len(gw.outo)\n\t\treorder := false\n\t\tfor i := 0; i < len(gw.outo); i++ {\n\t\t\tif gw.outo[i] == c {\n\t\t\t\t// If last, simply remove and no need to reorder\n\t\t\t\tif i != louto-1 {\n\t\t\t\t\tgw.outo[i] = gw.outo[louto-1]\n\t\t\t\t\treorder = true\n\t\t\t\t}\n\t\t\t\tgw.outo = gw.outo[:louto-1]\n\t\t\t}\n\t\t}\n\t\tif reorder {\n\t\t\tgw.orderOutboundConnectionsLocked()\n\t\t}\n\t} else {\n\t\tdelete(gw.in, cid)\n\t}\n\tgw.Unlock()\n\ts.removeFromTempClients(cid)\n\n\tif isOutbound {\n\t\t// Update number of totalQSubs for this gateway\n\t\tqSubsRemoved := int64(0)\n\t\tc.mu.Lock()\n\t\tfor _, sub := range c.subs {\n\t\t\tif sub.queue != nil {\n\t\t\t\tqSubsRemoved++\n\t\t\t}\n\t\t}\n\t\tc.subs = nil\n\t\tc.mu.Unlock()\n\t\t// Update total count of qsubs in remote gateways.\n\t\tatomic.AddInt64(&c.srv.gateway.totalQSubs, -qSubsRemoved)\n\n\t} else {\n\t\tvar subsa [1024]*subscription\n\t\tvar subs = subsa[:0]\n\n\t\t// For inbound GW connection, if we have subs, those are\n\t\t// local subs on \"_R_.\" subjects.\n\t\tc.mu.Lock()\n\t\tfor _, sub := range c.subs {\n\t\t\tsubs = append(subs, sub)\n\t\t}\n\t\tc.subs = nil\n\t\tc.mu.Unlock()\n\t\tfor _, sub := range subs {\n\t\t\tc.removeReplySub(sub)\n\t\t}\n\t}\n}\n\n// GatewayAddr returns the net.Addr object for the gateway listener.\nfunc (s *Server) GatewayAddr() *net.TCPAddr {\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\tif s.gatewayListener == nil {\n\t\treturn nil\n\t}\n\treturn s.gatewayListener.Addr().(*net.TCPAddr)\n}\n\n// A- protocol received from the remote after sending messages\n// on an account that it has no interest in. Mark this account\n// with a \"no interest\" marker to prevent further messages send.\n// <Invoked from outbound connection's readLoop>\nfunc (c *client) processGatewayAccountUnsub(accName string) {\n\t// Just to indicate activity around \"subscriptions\" events.\n\tc.in.subs++\n\t// This account may have an entry because of queue subs.\n\t// If that's the case, we can reset the no-interest map,\n\t// but not set the entry to nil.\n\tsetToNil := true\n\tif ei, ok := c.gw.outsim.Load(accName); ei != nil {\n\t\te := ei.(*outsie)\n\t\te.Lock()\n\t\t// Reset the no-interest map if we have queue subs\n\t\t// and don't set the entry to nil.\n\t\tif e.qsubs > 0 {\n\t\t\te.ni = make(map[string]struct{})\n\t\t\tsetToNil = false\n\t\t}\n\t\te.Unlock()\n\t} else if ok {\n\t\t// Already set to nil, so skip\n\t\tsetToNil = false\n\t}\n\tif setToNil {\n\t\tc.gw.outsim.Store(accName, nil)\n\t}\n}\n\n// A+ protocol received from remote gateway if it had previously\n// sent an A-. Clear the \"no interest\" marker for this account.\n// <Invoked from outbound connection's readLoop>\nfunc (c *client) processGatewayAccountSub(accName string) error {\n\t// Just to indicate activity around \"subscriptions\" events.\n\tc.in.subs++\n\t// If this account has an entry because of queue subs, we\n\t// can't delete the entry.\n\tremove := true\n\tif ei, ok := c.gw.outsim.Load(accName); ei != nil {\n\t\te := ei.(*outsie)\n\t\te.Lock()\n\t\tif e.qsubs > 0 {\n\t\t\tremove = false\n\t\t}\n\t\te.Unlock()\n\t} else if !ok {\n\t\t// There is no entry, so skip\n\t\tremove = false\n\t}\n\tif remove {\n\t\tc.gw.outsim.Delete(accName)\n\t}\n\treturn nil\n}\n\n// RS- protocol received from the remote after sending messages\n// on a subject that it has no interest in (but knows about the\n// account). Mark this subject with a \"no interest\" marker to\n// prevent further messages being sent.\n// If in modeInterestOnly or for a queue sub, remove from\n// the sublist if present.\n// <Invoked from outbound connection's readLoop>\nfunc (c *client) processGatewayRUnsub(arg []byte) error {\n\t_, accName, subject, queue, err := c.parseUnsubProto(arg, true, false)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"processGatewaySubjectUnsub %s\", err.Error())\n\t}\n\n\tvar (\n\t\te          *outsie\n\t\tuseSl      bool\n\t\tnewe       bool\n\t\tcallUpdate bool\n\t\tsrv        *Server\n\t\tsub        *subscription\n\t)\n\n\t// Possibly execute this on exit after all locks have been released.\n\t// If callUpdate is true, srv and sub will be not nil.\n\tdefer func() {\n\t\tif callUpdate {\n\t\t\tsrv.updateInterestForAccountOnGateway(accName, sub, -1)\n\t\t}\n\t}()\n\n\tc.mu.Lock()\n\tif c.gw.outsim == nil {\n\t\tc.Errorf(\"Received RS- from gateway on inbound connection\")\n\t\tc.mu.Unlock()\n\t\tc.closeConnection(ProtocolViolation)\n\t\treturn nil\n\t}\n\tdefer c.mu.Unlock()\n\t// If closed, c.subs map will be nil, so bail out.\n\tif c.isClosed() {\n\t\treturn nil\n\t}\n\n\tei, _ := c.gw.outsim.Load(accName)\n\tif ei != nil {\n\t\te = ei.(*outsie)\n\t\te.Lock()\n\t\tdefer e.Unlock()\n\t\t// If there is an entry, for plain sub we need\n\t\t// to know if we should store the sub\n\t\tuseSl = queue != nil || e.mode != Optimistic\n\t} else if queue != nil {\n\t\t// should not even happen...\n\t\tc.Debugf(\"Received RS- without prior RS+ for subject %q, queue %q\", subject, queue)\n\t\treturn nil\n\t} else {\n\t\t// Plain sub, assume optimistic sends, create entry.\n\t\te = &outsie{ni: make(map[string]struct{}), sl: NewSublistWithCache()}\n\t\tnewe = true\n\t}\n\t// This is when a sub or queue sub is supposed to be in\n\t// the sublist. Look for it and remove.\n\tif useSl {\n\t\tvar ok bool\n\t\tkey := arg\n\t\t// m[string()] does not cause mem allocation\n\t\tsub, ok = c.subs[string(key)]\n\t\t// if RS- for a sub that we don't have, just ignore.\n\t\tif !ok {\n\t\t\treturn nil\n\t\t}\n\t\tif e.sl.Remove(sub) == nil {\n\t\t\tdelete(c.subs, bytesToString(key))\n\t\t\tif queue != nil {\n\t\t\t\te.qsubs--\n\t\t\t\tatomic.AddInt64(&c.srv.gateway.totalQSubs, -1)\n\t\t\t}\n\t\t\t// If last, we can remove the whole entry only\n\t\t\t// when in optimistic mode and there is no element\n\t\t\t// in the `ni` map.\n\t\t\tif e.sl.Count() == 0 && e.mode == Optimistic && len(e.ni) == 0 {\n\t\t\t\tc.gw.outsim.Delete(accName)\n\t\t\t}\n\t\t}\n\t\t// We are going to call updateInterestForAccountOnGateway on exit.\n\t\tsrv = c.srv\n\t\tcallUpdate = true\n\t} else {\n\t\te.ni[string(subject)] = struct{}{}\n\t\tif newe {\n\t\t\tc.gw.outsim.Store(accName, e)\n\t\t}\n\t}\n\treturn nil\n}\n\n// For plain subs, RS+ protocol received from remote gateway if it\n// had previously sent a RS-. Clear the \"no interest\" marker for\n// this subject (under this account).\n// For queue subs, or if in modeInterestOnly, register interest\n// from remote gateway.\n// <Invoked from outbound connection's readLoop>\nfunc (c *client) processGatewayRSub(arg []byte) error {\n\t// Indicate activity.\n\tc.in.subs++\n\n\tvar (\n\t\tqueue []byte\n\t\tqw    int32\n\t)\n\n\targs := splitArg(arg)\n\tswitch len(args) {\n\tcase 2:\n\tcase 4:\n\t\tqueue = args[2]\n\t\tqw = int32(parseSize(args[3]))\n\tdefault:\n\t\treturn fmt.Errorf(\"processGatewaySubjectSub Parse Error: '%s'\", arg)\n\t}\n\taccName := args[0]\n\tsubject := args[1]\n\n\tvar (\n\t\te          *outsie\n\t\tuseSl      bool\n\t\tnewe       bool\n\t\tcallUpdate bool\n\t\tsrv        *Server\n\t\tsub        *subscription\n\t)\n\n\t// Possibly execute this on exit after all locks have been released.\n\t// If callUpdate is true, srv and sub will be not nil.\n\tdefer func() {\n\t\tif callUpdate {\n\t\t\tsrv.updateInterestForAccountOnGateway(string(accName), sub, 1)\n\t\t}\n\t}()\n\n\tc.mu.Lock()\n\tif c.gw.outsim == nil {\n\t\tc.Errorf(\"Received RS+ from gateway on inbound connection\")\n\t\tc.mu.Unlock()\n\t\tc.closeConnection(ProtocolViolation)\n\t\treturn nil\n\t}\n\tdefer c.mu.Unlock()\n\t// If closed, c.subs map will be nil, so bail out.\n\tif c.isClosed() {\n\t\treturn nil\n\t}\n\n\tei, _ := c.gw.outsim.Load(bytesToString(accName))\n\t// We should always have an existing entry for plain subs because\n\t// in optimistic mode we would have received RS- first, and\n\t// in full knowledge, we are receiving RS+ for an account after\n\t// getting many RS- from the remote..\n\tif ei != nil {\n\t\te = ei.(*outsie)\n\t\te.Lock()\n\t\tdefer e.Unlock()\n\t\tuseSl = queue != nil || e.mode != Optimistic\n\t} else if queue == nil {\n\t\treturn nil\n\t} else {\n\t\te = &outsie{ni: make(map[string]struct{}), sl: NewSublistWithCache()}\n\t\tnewe = true\n\t\tuseSl = true\n\t}\n\tif useSl {\n\t\tvar key []byte\n\t\t// We store remote subs by account/subject[/queue].\n\t\t// For queue, remove the trailing weight\n\t\tif queue != nil {\n\t\t\tkey = arg[:len(arg)-len(args[3])-1]\n\t\t} else {\n\t\t\tkey = arg\n\t\t}\n\t\t// If RS+ for a sub that we already have, ignore.\n\t\t// (m[string()] does not allocate memory)\n\t\tif _, ok := c.subs[string(key)]; ok {\n\t\t\treturn nil\n\t\t}\n\t\t// new subscription. copy subject (and queue) to\n\t\t// not reference the underlying possibly big buffer.\n\t\tvar csubject []byte\n\t\tvar cqueue []byte\n\t\tif queue != nil {\n\t\t\t// make single allocation and use different slices\n\t\t\t// to point to subject and queue name.\n\t\t\tcbuf := make([]byte, len(subject)+1+len(queue))\n\t\t\tcopy(cbuf, key[len(accName)+1:])\n\t\t\tcsubject = cbuf[:len(subject)]\n\t\t\tcqueue = cbuf[len(subject)+1:]\n\t\t} else {\n\t\t\tcsubject = make([]byte, len(subject))\n\t\t\tcopy(csubject, subject)\n\t\t}\n\t\tsub = &subscription{client: c, subject: csubject, queue: cqueue, qw: qw}\n\t\t// If no error inserting in sublist...\n\t\tif e.sl.Insert(sub) == nil {\n\t\t\tc.subs[string(key)] = sub\n\t\t\tif queue != nil {\n\t\t\t\te.qsubs++\n\t\t\t\tatomic.AddInt64(&c.srv.gateway.totalQSubs, 1)\n\t\t\t}\n\t\t\tif newe {\n\t\t\t\tc.gw.outsim.Store(string(accName), e)\n\t\t\t}\n\t\t}\n\t\t// We are going to call updateInterestForAccountOnGateway on exit.\n\t\tsrv = c.srv\n\t\tcallUpdate = true\n\t} else {\n\t\tsubj := bytesToString(subject)\n\t\t// If this is an RS+ for a wc subject, then\n\t\t// remove from the no interest map all subjects\n\t\t// that are a subset of this wc subject.\n\t\tif subjectHasWildcard(subj) {\n\t\t\tfor k := range e.ni {\n\t\t\t\tif subjectIsSubsetMatch(k, subj) {\n\t\t\t\t\tdelete(e.ni, k)\n\t\t\t\t}\n\t\t\t}\n\t\t} else {\n\t\t\tdelete(e.ni, subj)\n\t\t}\n\t}\n\treturn nil\n}\n\n// Returns true if this gateway has possible interest in the\n// given account/subject (which means, it does not have a registered\n// no-interest on the account and/or subject) and the sublist result\n// for queue subscriptions.\n// <Outbound connection: invoked when client message is published,\n// so from any client connection's readLoop>\nfunc (c *client) gatewayInterest(acc string, subj []byte) (bool, *SublistResult) {\n\tei, accountInMap := c.gw.outsim.Load(acc)\n\t// If there is an entry for this account and ei is nil,\n\t// it means that the remote is not interested at all in\n\t// this account and we could not possibly have queue subs.\n\tif accountInMap && ei == nil {\n\t\treturn false, nil\n\t}\n\t// Assume interest if account not in map, unless we support\n\t// only interest-only mode.\n\tpsi := !accountInMap && !c.gw.interestOnlyMode\n\tvar r *SublistResult\n\tif accountInMap {\n\t\t// If in map, check for subs interest with sublist.\n\t\te := ei.(*outsie)\n\t\te.RLock()\n\t\t// Unless each side has agreed on interest-only mode,\n\t\t// we may be in transition to modeInterestOnly\n\t\t// but until e.ni is nil, use it to know if we\n\t\t// should suppress interest or not.\n\t\tif !c.gw.interestOnlyMode && e.ni != nil {\n\t\t\tif _, inMap := e.ni[string(subj)]; !inMap {\n\t\t\t\tpsi = true\n\t\t\t}\n\t\t}\n\t\t// If we are in modeInterestOnly (e.ni will be nil)\n\t\t// or if we have queue subs, we also need to check sl.Match.\n\t\tif e.ni == nil || e.qsubs > 0 {\n\t\t\tr = e.sl.MatchBytes(subj)\n\t\t\tif len(r.psubs) > 0 {\n\t\t\t\tpsi = true\n\t\t\t}\n\t\t}\n\t\te.RUnlock()\n\t\t// Since callers may just check if the sublist result is nil or not,\n\t\t// make sure that if what is returned by sl.Match() is the emptyResult, then\n\t\t// we return nil to the caller.\n\t\tif r == emptyResult {\n\t\t\tr = nil\n\t\t}\n\t}\n\treturn psi, r\n}\n\n// switchAccountToInterestMode will switch an account over to interestMode.\n// Lock should NOT be held.\nfunc (s *Server) switchAccountToInterestMode(accName string) {\n\tgwsa := [16]*client{}\n\tgws := gwsa[:0]\n\ts.getInboundGatewayConnections(&gws)\n\n\tfor _, gin := range gws {\n\t\tvar e *insie\n\t\tvar ok bool\n\n\t\tgin.mu.Lock()\n\t\tif e, ok = gin.gw.insim[accName]; !ok || e == nil {\n\t\t\te = &insie{}\n\t\t\tgin.gw.insim[accName] = e\n\t\t}\n\t\t// Do it only if we are in Optimistic mode\n\t\tif e.mode == Optimistic {\n\t\t\tgin.gatewaySwitchAccountToSendAllSubs(e, accName)\n\t\t}\n\t\tgin.mu.Unlock()\n\t}\n}\n\n// This is invoked when registering (or unregistering) the first\n// (or last) subscription on a given account/subject. For each\n// GWs inbound connections, we will check if we need to send an RS+ or A+\n// protocol.\nfunc (s *Server) maybeSendSubOrUnsubToGateways(accName string, sub *subscription, added bool) {\n\tif sub.queue != nil {\n\t\treturn\n\t}\n\tgwsa := [16]*client{}\n\tgws := gwsa[:0]\n\ts.getInboundGatewayConnections(&gws)\n\tif len(gws) == 0 {\n\t\treturn\n\t}\n\tvar (\n\t\trsProtoa  [512]byte\n\t\trsProto   []byte\n\t\taccProtoa [256]byte\n\t\taccProto  []byte\n\t\tproto     []byte\n\t\tsubject   = bytesToString(sub.subject)\n\t\thasWC     = subjectHasWildcard(subject)\n\t)\n\tfor _, c := range gws {\n\t\tproto = nil\n\t\tc.mu.Lock()\n\t\te, inMap := c.gw.insim[accName]\n\t\t// If there is a inbound subject interest entry...\n\t\tif e != nil {\n\t\t\tsendProto := false\n\t\t\t// In optimistic mode, we care only about possibly sending RS+ (or A+)\n\t\t\t// so if e.ni is not nil we do things only when adding a new subscription.\n\t\t\tif e.ni != nil && added {\n\t\t\t\t// For wildcard subjects, we will remove from our no-interest\n\t\t\t\t// map, all subjects that are a subset of this wc subject, but we\n\t\t\t\t// still send the wc subject and let the remote do its own cleanup.\n\t\t\t\tif hasWC {\n\t\t\t\t\tfor enis := range e.ni {\n\t\t\t\t\t\tif subjectIsSubsetMatch(enis, subject) {\n\t\t\t\t\t\t\tdelete(e.ni, enis)\n\t\t\t\t\t\t\tsendProto = true\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t} else if _, noInterest := e.ni[subject]; noInterest {\n\t\t\t\t\tdelete(e.ni, subject)\n\t\t\t\t\tsendProto = true\n\t\t\t\t}\n\t\t\t} else if e.mode == InterestOnly {\n\t\t\t\t// We are in the mode where we always send RS+/- protocols.\n\t\t\t\tsendProto = true\n\t\t\t}\n\t\t\tif sendProto {\n\t\t\t\tif rsProto == nil {\n\t\t\t\t\t// Construct the RS+/- only once\n\t\t\t\t\tproto = rsProtoa[:0]\n\t\t\t\t\tif added {\n\t\t\t\t\t\tproto = append(proto, rSubBytes...)\n\t\t\t\t\t} else {\n\t\t\t\t\t\tproto = append(proto, rUnsubBytes...)\n\t\t\t\t\t}\n\t\t\t\t\tproto = append(proto, accName...)\n\t\t\t\t\tproto = append(proto, ' ')\n\t\t\t\t\tproto = append(proto, sub.subject...)\n\t\t\t\t\tproto = append(proto, CR_LF...)\n\t\t\t\t\trsProto = proto\n\t\t\t\t} else {\n\t\t\t\t\t// Point to the already constructed RS+/-\n\t\t\t\t\tproto = rsProto\n\t\t\t\t}\n\t\t\t}\n\t\t} else if added && inMap {\n\t\t\t// Here, we have a `nil` entry for this account in\n\t\t\t// the map, which means that we have previously sent\n\t\t\t// an A-. We have a new subscription, so we need to\n\t\t\t// send an A+ and delete the entry from the map so\n\t\t\t// that we do this only once.\n\t\t\tdelete(c.gw.insim, accName)\n\t\t\tif accProto == nil {\n\t\t\t\t// Construct the A+ only once\n\t\t\t\tproto = accProtoa[:0]\n\t\t\t\tproto = append(proto, aSubBytes...)\n\t\t\t\tproto = append(proto, accName...)\n\t\t\t\tproto = append(proto, CR_LF...)\n\t\t\t\taccProto = proto\n\t\t\t} else {\n\t\t\t\t// Point to the already constructed A+\n\t\t\t\tproto = accProto\n\t\t\t}\n\t\t}\n\t\tif proto != nil {\n\t\t\tc.enqueueProto(proto)\n\t\t\tif c.trace {\n\t\t\t\tc.traceOutOp(\"\", proto[:len(proto)-LEN_CR_LF])\n\t\t\t}\n\t\t}\n\t\tc.mu.Unlock()\n\t}\n}\n\n// This is invoked when the first (or last) queue subscription on a\n// given subject/group is registered (or unregistered). Sent to all\n// inbound gateways.\nfunc (s *Server) sendQueueSubOrUnsubToGateways(accName string, qsub *subscription, added bool) {\n\tif qsub.queue == nil {\n\t\treturn\n\t}\n\n\tgwsa := [16]*client{}\n\tgws := gwsa[:0]\n\ts.getInboundGatewayConnections(&gws)\n\tif len(gws) == 0 {\n\t\treturn\n\t}\n\n\tvar protoa [512]byte\n\tvar proto []byte\n\tfor _, c := range gws {\n\t\tif proto == nil {\n\t\t\tproto = protoa[:0]\n\t\t\tif added {\n\t\t\t\tproto = append(proto, rSubBytes...)\n\t\t\t} else {\n\t\t\t\tproto = append(proto, rUnsubBytes...)\n\t\t\t}\n\t\t\tproto = append(proto, accName...)\n\t\t\tproto = append(proto, ' ')\n\t\t\tproto = append(proto, qsub.subject...)\n\t\t\tproto = append(proto, ' ')\n\t\t\tproto = append(proto, qsub.queue...)\n\t\t\tif added {\n\t\t\t\t// For now, just use 1 for the weight\n\t\t\t\tproto = append(proto, ' ', '1')\n\t\t\t}\n\t\t\tproto = append(proto, CR_LF...)\n\t\t}\n\t\tc.mu.Lock()\n\t\t// If we add a queue sub, and we had previously sent an A-,\n\t\t// we don't need to send an A+ here, but we need to clear\n\t\t// the fact that we did sent the A- so that we don't send\n\t\t// an A+ when we will get the first non-queue sub registered.\n\t\tif added {\n\t\t\tif ei, ok := c.gw.insim[accName]; ok && ei == nil {\n\t\t\t\tdelete(c.gw.insim, accName)\n\t\t\t}\n\t\t}\n\t\tc.enqueueProto(proto)\n\t\tif c.trace {\n\t\t\tc.traceOutOp(\"\", proto[:len(proto)-LEN_CR_LF])\n\t\t}\n\t\tc.mu.Unlock()\n\t}\n}\n\n// This is invoked when a subscription (plain or queue) is\n// added/removed locally or in our cluster. We use ref counting\n// to know when to update the inbound gateways.\n// <Invoked from client or route connection's readLoop or when such\n// connection is closed>\nfunc (s *Server) gatewayUpdateSubInterest(accName string, sub *subscription, change int32) {\n\tif sub.si {\n\t\treturn\n\t}\n\n\tvar (\n\t\tkeya  [1024]byte\n\t\tkey   = keya[:0]\n\t\tentry *sitally\n\t\tisNew bool\n\t)\n\n\ts.gateway.pasi.Lock()\n\tdefer s.gateway.pasi.Unlock()\n\n\taccMap := s.gateway.pasi.m\n\n\t// First see if we have the account\n\tst := accMap[accName]\n\tif st == nil {\n\t\t// Ignore remove of something we don't have\n\t\tif change < 0 {\n\t\t\treturn\n\t\t}\n\t\tst = make(map[string]*sitally)\n\t\taccMap[accName] = st\n\t\tisNew = true\n\t}\n\t// Lookup: build the key as subject[+' '+queue]\n\tkey = append(key, sub.subject...)\n\tif sub.queue != nil {\n\t\tkey = append(key, ' ')\n\t\tkey = append(key, sub.queue...)\n\t}\n\tif !isNew {\n\t\tentry = st[string(key)]\n\t}\n\tfirst := false\n\tlast := false\n\tif entry == nil {\n\t\t// Ignore remove of something we don't have\n\t\tif change < 0 {\n\t\t\treturn\n\t\t}\n\t\tentry = &sitally{n: change, q: sub.queue != nil}\n\t\tst[string(key)] = entry\n\t\tfirst = true\n\t} else {\n\t\tentry.n += change\n\t\tif entry.n <= 0 {\n\t\t\tdelete(st, bytesToString(key))\n\t\t\tlast = true\n\t\t\tif len(st) == 0 {\n\t\t\t\tdelete(accMap, accName)\n\t\t\t}\n\t\t}\n\t}\n\tif sub.client != nil {\n\t\trsubs := &s.gateway.rsubs\n\t\tacc := sub.client.acc\n\t\tsli, _ := rsubs.Load(acc)\n\t\tif change > 0 {\n\t\t\tvar sl *Sublist\n\t\t\tif sli == nil {\n\t\t\t\tsl = NewSublistNoCache()\n\t\t\t\trsubs.Store(acc, sl)\n\t\t\t} else {\n\t\t\t\tsl = sli.(*Sublist)\n\t\t\t}\n\t\t\tsl.Insert(sub)\n\t\t\ttime.AfterFunc(s.gateway.recSubExp, func() {\n\t\t\t\tsl.Remove(sub)\n\t\t\t})\n\t\t} else if sli != nil {\n\t\t\tsl := sli.(*Sublist)\n\t\t\tsl.Remove(sub)\n\t\t\tif sl.Count() == 0 {\n\t\t\t\trsubs.Delete(acc)\n\t\t\t}\n\t\t}\n\t}\n\tif first || last {\n\t\tif entry.q {\n\t\t\ts.sendQueueSubOrUnsubToGateways(accName, sub, first)\n\t\t} else {\n\t\t\ts.maybeSendSubOrUnsubToGateways(accName, sub, first)\n\t\t}\n\t}\n}\n\n// Returns true if the given subject is a GW routed reply subject,\n// that is, starts with $GNR and is long enough to contain cluster/server hash\n// and subject.\nfunc isGWRoutedReply(subj []byte) bool {\n\treturn len(subj) > gwSubjectOffset && bytesToString(subj[:gwReplyPrefixLen]) == gwReplyPrefix\n}\n\n// Same than isGWRoutedReply but accepts the old prefix $GR and returns\n// a boolean indicating if this is the old prefix\nfunc isGWRoutedSubjectAndIsOldPrefix(subj []byte) (bool, bool) {\n\tif isGWRoutedReply(subj) {\n\t\treturn true, false\n\t}\n\tif len(subj) > oldGWReplyStart && bytesToString(subj[:oldGWReplyPrefixLen]) == oldGWReplyPrefix {\n\t\treturn true, true\n\t}\n\treturn false, false\n}\n\n// Returns true if subject starts with \"$GNR.\". This is to check that\n// clients can't publish on this subject.\nfunc hasGWRoutedReplyPrefix(subj []byte) bool {\n\treturn len(subj) > gwReplyPrefixLen && bytesToString(subj[:gwReplyPrefixLen]) == gwReplyPrefix\n}\n\n// Evaluates if the given reply should be mapped or not.\nfunc (g *srvGateway) shouldMapReplyForGatewaySend(acc *Account, reply []byte) bool {\n\t// If for this account there is a recent matching subscription interest\n\t// then we will map.\n\tsli, _ := g.rsubs.Load(acc)\n\tif sli == nil {\n\t\treturn false\n\t}\n\tsl := sli.(*Sublist)\n\tif sl.Count() > 0 {\n\t\tif sl.HasInterest(string(reply)) {\n\t\t\treturn true\n\t\t}\n\t}\n\n\treturn false\n}\n\nvar subPool = &sync.Pool{\n\tNew: func() any {\n\t\treturn &subscription{}\n\t},\n}\n\n// May send a message to all outbound gateways. It is possible\n// that the message is not sent to a given gateway if for instance\n// it is known that this gateway has no interest in the account or\n// subject, etc..\n// When invoked from a LEAF connection, `checkLeafQF` should be passed as `true`\n// so that we skip any queue subscription interest that is not part of the\n// `c.pa.queues` filter (similar to what we do in `processMsgResults`). However,\n// when processing service imports, then this boolean should be passes as `false`,\n// regardless if it is a LEAF connection or not.\n// <Invoked from any client connection's readLoop>\nfunc (c *client) sendMsgToGateways(acc *Account, msg, subject, reply []byte, qgroups [][]byte, checkLeafQF bool) bool {\n\t// We had some times when we were sending across a GW with no subject, and the other side would break\n\t// due to parser error. These need to be fixed upstream but also double check here.\n\tif len(subject) == 0 {\n\t\treturn false\n\t}\n\tgwsa := [16]*client{}\n\tgws := gwsa[:0]\n\t// This is in fast path, so avoid calling functions when possible.\n\t// Get the outbound connections in place instead of calling\n\t// getOutboundGatewayConnections().\n\tsrv := c.srv\n\tgw := srv.gateway\n\tgw.RLock()\n\tfor i := 0; i < len(gw.outo); i++ {\n\t\tgws = append(gws, gw.outo[i])\n\t}\n\tthisClusterReplyPrefix := gw.replyPfx\n\tthisClusterOldReplyPrefix := gw.oldReplyPfx\n\tgw.RUnlock()\n\tif len(gws) == 0 {\n\t\treturn false\n\t}\n\n\tmt, _ := c.isMsgTraceEnabled()\n\tif mt != nil {\n\t\tpa := c.pa\n\t\tmsg = mt.setOriginAccountHeaderIfNeeded(c, acc, msg)\n\t\tdefer func() { c.pa = pa }()\n\t}\n\n\tvar (\n\t\tqueuesa    = [512]byte{}\n\t\tqueues     = queuesa[:0]\n\t\taccName    = acc.Name\n\t\tmreplya    [256]byte\n\t\tmreply     []byte\n\t\tdstHash    []byte\n\t\tcheckReply = len(reply) > 0\n\t\tdidDeliver bool\n\t\tprodIsMQTT = c.isMqtt()\n\t\tdlvMsgs    int64\n\t)\n\n\t// Get a subscription from the pool\n\tsub := subPool.Get().(*subscription)\n\n\t// Check if the subject is on the reply prefix, if so, we\n\t// need to send that message directly to the origin cluster.\n\tdirectSend, old := isGWRoutedSubjectAndIsOldPrefix(subject)\n\tif directSend {\n\t\tif old {\n\t\t\tdstHash = subject[oldGWReplyPrefixLen : oldGWReplyStart-1]\n\t\t} else {\n\t\t\tdstHash = subject[gwClusterOffset : gwClusterOffset+gwHashLen]\n\t\t}\n\t}\n\tfor i := 0; i < len(gws); i++ {\n\t\tgwc := gws[i]\n\t\tif directSend {\n\t\t\tgwc.mu.Lock()\n\t\t\tvar ok bool\n\t\t\tif gwc.gw.cfg != nil {\n\t\t\t\tif old {\n\t\t\t\t\tok = bytes.Equal(dstHash, gwc.gw.cfg.oldHash)\n\t\t\t\t} else {\n\t\t\t\t\tok = bytes.Equal(dstHash, gwc.gw.cfg.hash)\n\t\t\t\t}\n\t\t\t}\n\t\t\tgwc.mu.Unlock()\n\t\t\tif !ok {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t} else {\n\t\t\t// Plain sub interest and queue sub results for this account/subject\n\t\t\tpsi, qr := gwc.gatewayInterest(accName, subject)\n\t\t\tif !psi && qr == nil {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tqueues = queuesa[:0]\n\t\t\tif qr != nil {\n\t\t\t\tfor i := 0; i < len(qr.qsubs); i++ {\n\t\t\t\t\tqsubs := qr.qsubs[i]\n\t\t\t\t\tif len(qsubs) > 0 {\n\t\t\t\t\t\tqueue := qsubs[0].queue\n\t\t\t\t\t\tif checkLeafQF {\n\t\t\t\t\t\t\t// Skip any queue that is not in the leaf's queue filter.\n\t\t\t\t\t\t\tskip := true\n\t\t\t\t\t\t\tfor _, qn := range c.pa.queues {\n\t\t\t\t\t\t\t\tif bytes.Equal(queue, qn) {\n\t\t\t\t\t\t\t\t\tskip = false\n\t\t\t\t\t\t\t\t\tbreak\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tif skip {\n\t\t\t\t\t\t\t\tcontinue\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t// Now we still need to check that it was not delivered\n\t\t\t\t\t\t\t// locally by checking the given `qgroups`.\n\t\t\t\t\t\t}\n\t\t\t\t\t\tadd := true\n\t\t\t\t\t\tfor _, qn := range qgroups {\n\t\t\t\t\t\t\tif bytes.Equal(queue, qn) {\n\t\t\t\t\t\t\t\tadd = false\n\t\t\t\t\t\t\t\tbreak\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif add {\n\t\t\t\t\t\t\tqgroups = append(qgroups, queue)\n\t\t\t\t\t\t\tqueues = append(queues, queue...)\n\t\t\t\t\t\t\tqueues = append(queues, ' ')\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tif !psi && len(queues) == 0 {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t}\n\t\tif checkReply {\n\t\t\t// Check/map only once\n\t\t\tcheckReply = false\n\t\t\t// Assume we will use original\n\t\t\tmreply = reply\n\t\t\t// Decide if we should map.\n\t\t\tif gw.shouldMapReplyForGatewaySend(acc, reply) {\n\t\t\t\tmreply = mreplya[:0]\n\t\t\t\tgwc.mu.Lock()\n\t\t\t\tuseOldPrefix := gwc.gw.useOldPrefix\n\t\t\t\tgwc.mu.Unlock()\n\t\t\t\tif useOldPrefix {\n\t\t\t\t\tmreply = append(mreply, thisClusterOldReplyPrefix...)\n\t\t\t\t} else {\n\t\t\t\t\tmreply = append(mreply, thisClusterReplyPrefix...)\n\t\t\t\t}\n\t\t\t\tmreply = append(mreply, reply...)\n\t\t\t}\n\t\t}\n\n\t\tif mt != nil {\n\t\t\tmsg = mt.setHopHeader(c, msg)\n\t\t}\n\n\t\t// Setup the message header.\n\t\t// Make sure we are an 'R' proto by default\n\t\tc.msgb[0] = 'R'\n\t\tmh := c.msgb[:msgHeadProtoLen]\n\t\tmh = append(mh, accName...)\n\t\tmh = append(mh, ' ')\n\t\tmh = append(mh, subject...)\n\t\tmh = append(mh, ' ')\n\t\tif len(queues) > 0 {\n\t\t\tif len(reply) > 0 {\n\t\t\t\tmh = append(mh, \"+ \"...) // Signal that there is a reply.\n\t\t\t\tmh = append(mh, mreply...)\n\t\t\t\tmh = append(mh, ' ')\n\t\t\t} else {\n\t\t\t\tmh = append(mh, \"| \"...) // Only queues\n\t\t\t}\n\t\t\tmh = append(mh, queues...)\n\t\t} else if len(reply) > 0 {\n\t\t\tmh = append(mh, mreply...)\n\t\t\tmh = append(mh, ' ')\n\t\t}\n\t\t// Headers\n\t\thasHeader := c.pa.hdr > 0\n\t\tcanReceiveHeader := gwc.headers\n\n\t\tif hasHeader {\n\t\t\tif canReceiveHeader {\n\t\t\t\tmh[0] = 'H'\n\t\t\t\tmh = append(mh, c.pa.hdb...)\n\t\t\t\tmh = append(mh, ' ')\n\t\t\t\tmh = append(mh, c.pa.szb...)\n\t\t\t} else {\n\t\t\t\t// If we are here we need to truncate the payload size\n\t\t\t\tnsz := strconv.Itoa(c.pa.size - c.pa.hdr)\n\t\t\t\tmh = append(mh, nsz...)\n\t\t\t}\n\t\t} else {\n\t\t\tmh = append(mh, c.pa.szb...)\n\t\t}\n\n\t\tmh = append(mh, CR_LF...)\n\n\t\t// We reuse the subscription object that we pass to deliverMsg.\n\t\t// So set/reset important fields.\n\t\tsub.nm, sub.max = 0, 0\n\t\tsub.client = gwc\n\t\tsub.subject = subject\n\t\tif c.deliverMsg(prodIsMQTT, sub, acc, subject, mreply, mh, msg, false) {\n\t\t\t// We don't count internal deliveries so count only if sub.icb is nil\n\t\t\tif sub.icb == nil {\n\t\t\t\tdlvMsgs++\n\t\t\t}\n\t\t\tdidDeliver = true\n\t\t}\n\t}\n\tif dlvMsgs > 0 {\n\t\ttotalBytes := dlvMsgs * int64(len(msg))\n\t\t// For non MQTT producers, remove the CR_LF * number of messages\n\t\tif !prodIsMQTT {\n\t\t\ttotalBytes -= dlvMsgs * int64(LEN_CR_LF)\n\t\t}\n\t\tif acc != nil {\n\t\t\tacc.stats.Lock()\n\t\t\tacc.stats.outMsgs += dlvMsgs\n\t\t\tacc.stats.outBytes += totalBytes\n\t\t\tacc.stats.gw.outMsgs += dlvMsgs\n\t\t\tacc.stats.gw.outBytes += totalBytes\n\t\t\tacc.stats.Unlock()\n\t\t}\n\t\tatomic.AddInt64(&srv.outMsgs, dlvMsgs)\n\t\tatomic.AddInt64(&srv.outBytes, totalBytes)\n\t}\n\t// Done with subscription, put back to pool. We don't need\n\t// to reset content since we explicitly set when using it.\n\t// However, make sure to not hold a reference to a connection.\n\tsub.client = nil\n\tsubPool.Put(sub)\n\treturn didDeliver\n}\n\n// Possibly sends an A- to the remote gateway `c`.\n// Invoked when processing an inbound message and the account is not found.\n// A check under a lock that protects processing of SUBs and UNSUBs is\n// done to make sure that we don't send the A- if a subscription has just\n// been created at the same time, which would otherwise results in the\n// remote never sending messages on this account until a new subscription\n// is created.\nfunc (s *Server) gatewayHandleAccountNoInterest(c *client, accName []byte) {\n\t// Check and possibly send the A- under this lock.\n\ts.gateway.pasi.Lock()\n\tdefer s.gateway.pasi.Unlock()\n\n\tsi, inMap := s.gateway.pasi.m[string(accName)]\n\tif inMap && si != nil && len(si) > 0 {\n\t\treturn\n\t}\n\tc.sendAccountUnsubToGateway(accName)\n}\n\n// Helper that sends an A- to this remote gateway if not already done.\n// This function should not be invoked directly but instead be invoked\n// by functions holding the gateway.pasi's Lock.\nfunc (c *client) sendAccountUnsubToGateway(accName []byte) {\n\t// Check if we have sent the A- or not.\n\tc.mu.Lock()\n\te, sent := c.gw.insim[string(accName)]\n\tif e != nil || !sent {\n\t\t// Add a nil value to indicate that we have sent an A-\n\t\t// so that we know to send A+ when needed.\n\t\tc.gw.insim[string(accName)] = nil\n\t\tvar protoa [256]byte\n\t\tproto := protoa[:0]\n\t\tproto = append(proto, aUnsubBytes...)\n\t\tproto = append(proto, accName...)\n\t\tproto = append(proto, CR_LF...)\n\t\tc.enqueueProto(proto)\n\t\tif c.trace {\n\t\t\tc.traceOutOp(\"\", proto[:len(proto)-LEN_CR_LF])\n\t\t}\n\t}\n\tc.mu.Unlock()\n}\n\n// Possibly sends an A- for this account or RS- for this subject.\n// Invoked when processing an inbound message and the account is found\n// but there is no interest on this subject.\n// A test is done under a lock that protects processing of SUBs and UNSUBs\n// and if there is no subscription at this time, we send an A-. If there\n// is at least a subscription, but no interest on this subject, we send\n// an RS- for this subject (if not already done).\nfunc (s *Server) gatewayHandleSubjectNoInterest(c *client, acc *Account, accName, subject []byte) {\n\ts.gateway.pasi.Lock()\n\tdefer s.gateway.pasi.Unlock()\n\n\t// If there is no subscription for this account, we would normally\n\t// send an A-, however, if this account has the internal subscription\n\t// for service reply, send a specific RS- for the subject instead.\n\t// Need to grab the lock here since sublist can change during reload.\n\tacc.mu.RLock()\n\thasSubs := acc.sl.Count() > 0 || acc.siReply != nil\n\tacc.mu.RUnlock()\n\n\t// If there is at least a subscription, possibly send RS-\n\tif hasSubs {\n\t\tsendProto := false\n\t\tc.mu.Lock()\n\t\t// Send an RS- protocol if not already done and only if\n\t\t// not in the modeInterestOnly.\n\t\te := c.gw.insim[string(accName)]\n\t\tif e == nil {\n\t\t\te = &insie{ni: make(map[string]struct{})}\n\t\t\te.ni[string(subject)] = struct{}{}\n\t\t\tc.gw.insim[string(accName)] = e\n\t\t\tsendProto = true\n\t\t} else if e.ni != nil {\n\t\t\t// If we are not in modeInterestOnly, check if we\n\t\t\t// have already sent an RS-\n\t\t\tif _, alreadySent := e.ni[string(subject)]; !alreadySent {\n\t\t\t\t// TODO(ik): pick some threshold as to when\n\t\t\t\t// we need to switch mode\n\t\t\t\tif len(e.ni) >= gatewayMaxRUnsubBeforeSwitch {\n\t\t\t\t\t// If too many RS-, switch to all-subs-mode.\n\t\t\t\t\tc.gatewaySwitchAccountToSendAllSubs(e, string(accName))\n\t\t\t\t} else {\n\t\t\t\t\te.ni[string(subject)] = struct{}{}\n\t\t\t\t\tsendProto = true\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tif sendProto {\n\t\t\tvar (\n\t\t\t\tprotoa = [512]byte{}\n\t\t\t\tproto  = protoa[:0]\n\t\t\t)\n\t\t\tproto = append(proto, rUnsubBytes...)\n\t\t\tproto = append(proto, accName...)\n\t\t\tproto = append(proto, ' ')\n\t\t\tproto = append(proto, subject...)\n\t\t\tproto = append(proto, CR_LF...)\n\t\t\tc.enqueueProto(proto)\n\t\t\tif c.trace {\n\t\t\t\tc.traceOutOp(\"\", proto[:len(proto)-LEN_CR_LF])\n\t\t\t}\n\t\t}\n\t\tc.mu.Unlock()\n\t} else {\n\t\t// There is not a single subscription, send an A- (if not already done).\n\t\tc.sendAccountUnsubToGateway([]byte(acc.Name))\n\t}\n}\n\n// Returns the cluster hash from the gateway reply prefix\nfunc (g *srvGateway) getClusterHash() []byte {\n\tg.RLock()\n\tclusterHash := g.replyPfx[gwClusterOffset : gwClusterOffset+gwHashLen]\n\tg.RUnlock()\n\treturn clusterHash\n}\n\n// Store this route in map with the key being the remote server's name hash\n// and the remote server's ID hash used by gateway replies mapping routing.\nfunc (s *Server) storeRouteByHash(srvIDHash string, c *client) {\n\tif !s.gateway.enabled {\n\t\treturn\n\t}\n\ts.gateway.routesIDByHash.Store(srvIDHash, c)\n}\n\n// Remove the route with the given keys from the map.\nfunc (s *Server) removeRouteByHash(srvIDHash string) {\n\tif !s.gateway.enabled {\n\t\treturn\n\t}\n\ts.gateway.routesIDByHash.Delete(srvIDHash)\n}\n\n// Returns the route with given hash or nil if not found.\n// This is for gateways only.\nfunc (s *Server) getRouteByHash(hash, accName []byte) (*client, bool) {\n\tid := bytesToString(hash)\n\tvar perAccount bool\n\tif v, ok := s.accRouteByHash.Load(bytesToString(accName)); ok {\n\t\tif v == nil {\n\t\t\tid += bytesToString(accName)\n\t\t\tperAccount = true\n\t\t} else {\n\t\t\tid += strconv.Itoa(v.(int))\n\t\t}\n\t}\n\tif v, ok := s.gateway.routesIDByHash.Load(id); ok {\n\t\treturn v.(*client), perAccount\n\t} else if !perAccount {\n\t\t// Check if we have a \"no pool\" connection at index 0.\n\t\tif v, ok := s.gateway.routesIDByHash.Load(bytesToString(hash) + \"0\"); ok {\n\t\t\tif r := v.(*client); r != nil {\n\t\t\t\tr.mu.Lock()\n\t\t\t\tnoPool := r.route.noPool\n\t\t\t\tr.mu.Unlock()\n\t\t\t\tif noPool {\n\t\t\t\t\treturn r, false\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn nil, perAccount\n}\n\n// Returns the subject from the routed reply\nfunc getSubjectFromGWRoutedReply(reply []byte, isOldPrefix bool) []byte {\n\tif isOldPrefix {\n\t\treturn reply[oldGWReplyStart:]\n\t}\n\treturn reply[gwSubjectOffset:]\n}\n\n// This should be invoked only from processInboundGatewayMsg() or\n// processInboundRoutedMsg() and is checking if the subject\n// (c.pa.subject) has the _GR_ prefix. If so, this is processed\n// as a GW reply and `true` is returned to indicate to the caller\n// that it should stop processing.\n// If gateway is not enabled on this server or if the subject\n// does not start with _GR_, `false` is returned and caller should\n// process message as usual.\nfunc (c *client) handleGatewayReply(msg []byte) (processed bool) {\n\t// Do not handle GW prefixed messages if this server does not have\n\t// gateway enabled or if the subject does not start with the previx.\n\tif !c.srv.gateway.enabled {\n\t\treturn false\n\t}\n\tisGWPrefix, oldPrefix := isGWRoutedSubjectAndIsOldPrefix(c.pa.subject)\n\tif !isGWPrefix {\n\t\treturn false\n\t}\n\t// Save original subject (in case we have to forward)\n\torgSubject := c.pa.subject\n\n\tvar clusterHash []byte\n\tvar srvHash []byte\n\tvar subject []byte\n\n\tif oldPrefix {\n\t\tclusterHash = c.pa.subject[oldGWReplyPrefixLen : oldGWReplyStart-1]\n\t\t// Check if this reply is intended for our cluster.\n\t\tif !bytes.Equal(clusterHash, c.srv.gateway.oldHash) {\n\t\t\t// We could report, for now, just drop.\n\t\t\treturn true\n\t\t}\n\t\tsubject = c.pa.subject[oldGWReplyStart:]\n\t} else {\n\t\tclusterHash = c.pa.subject[gwClusterOffset : gwClusterOffset+gwHashLen]\n\t\t// Check if this reply is intended for our cluster.\n\t\tif !bytes.Equal(clusterHash, c.srv.gateway.getClusterHash()) {\n\t\t\t// We could report, for now, just drop.\n\t\t\treturn true\n\t\t}\n\t\tsrvHash = c.pa.subject[gwServerOffset : gwServerOffset+gwHashLen]\n\t\tsubject = c.pa.subject[gwSubjectOffset:]\n\t}\n\n\tvar route *client\n\tvar perAccount bool\n\n\t// If the origin is not this server, get the route this should be sent to.\n\tif c.kind == GATEWAY && srvHash != nil && !bytes.Equal(srvHash, c.srv.gateway.sIDHash) {\n\t\troute, perAccount = c.srv.getRouteByHash(srvHash, c.pa.account)\n\t\t// This will be possibly nil, and in this case we will try to process\n\t\t// the interest from this server.\n\t}\n\n\t// Adjust the subject\n\tc.pa.subject = subject\n\n\t// Use a stack buffer to rewrite c.pa.cache since we only need it for\n\t// getAccAndResultFromCache()\n\tvar _pacache [256]byte\n\tpacache := _pacache[:0]\n\t// For routes that are dedicated to an account, do not put the account\n\t// name in the pacache.\n\tif c.kind == GATEWAY || (c.kind == ROUTER && c.route != nil && len(c.route.accName) == 0) {\n\t\tpacache = append(pacache, c.pa.account...)\n\t\tpacache = append(pacache, ' ')\n\t}\n\tpacache = append(pacache, c.pa.subject...)\n\tc.pa.pacache = pacache\n\n\tacc, r := c.getAccAndResultFromCache()\n\tif acc == nil {\n\t\ttypeConn := \"routed\"\n\t\tif c.kind == GATEWAY {\n\t\t\ttypeConn = \"gateway\"\n\t\t}\n\t\tc.Debugf(\"Unknown account %q for %s message on subject: %q\", c.pa.account, typeConn, c.pa.subject)\n\t\tif c.kind == GATEWAY {\n\t\t\tc.srv.gatewayHandleAccountNoInterest(c, c.pa.account)\n\t\t}\n\t\treturn true\n\t}\n\n\t// If route is nil, we will process the incoming message locally.\n\tif route == nil {\n\t\t// Check if this is a service reply subject (_R_)\n\t\tisServiceReply := isServiceReply(c.pa.subject)\n\n\t\tvar queues [][]byte\n\t\tif len(r.psubs)+len(r.qsubs) > 0 {\n\t\t\tflags := pmrCollectQueueNames | pmrIgnoreEmptyQueueFilter\n\t\t\t// If this message came from a ROUTE, allow to pick queue subs\n\t\t\t// only if the message was directly sent by the \"gateway\" server\n\t\t\t// in our cluster that received it.\n\t\t\tif c.kind == ROUTER {\n\t\t\t\tflags |= pmrAllowSendFromRouteToRoute\n\t\t\t}\n\t\t\t_, queues = c.processMsgResults(acc, r, msg, nil, c.pa.subject, c.pa.reply, flags)\n\t\t}\n\t\t// Since this was a reply that made it to the origin cluster,\n\t\t// we now need to send the message with the real subject to\n\t\t// gateways in case they have interest on that reply subject.\n\t\tif !isServiceReply {\n\t\t\tc.sendMsgToGateways(acc, msg, c.pa.subject, c.pa.reply, queues, false)\n\t\t}\n\t} else if c.kind == GATEWAY {\n\t\t// Only if we are a gateway connection should we try to route\n\t\t// to the server where the request originated.\n\t\tvar bufa [256]byte\n\t\tvar buf = bufa[:0]\n\t\tbuf = append(buf, msgHeadProto...)\n\t\tif !perAccount {\n\t\t\tbuf = append(buf, acc.Name...)\n\t\t\tbuf = append(buf, ' ')\n\t\t}\n\t\tbuf = append(buf, orgSubject...)\n\t\tbuf = append(buf, ' ')\n\t\tif len(c.pa.reply) > 0 {\n\t\t\tbuf = append(buf, c.pa.reply...)\n\t\t\tbuf = append(buf, ' ')\n\t\t}\n\t\tszb := c.pa.szb\n\t\tif c.pa.hdr >= 0 {\n\t\t\tif route.headers {\n\t\t\t\tbuf[0] = 'H'\n\t\t\t\tbuf = append(buf, c.pa.hdb...)\n\t\t\t\tbuf = append(buf, ' ')\n\t\t\t} else {\n\t\t\t\tszb = []byte(strconv.Itoa(c.pa.size - c.pa.hdr))\n\t\t\t\tmsg = msg[c.pa.hdr:]\n\t\t\t}\n\t\t}\n\t\tbuf = append(buf, szb...)\n\t\tmhEnd := len(buf)\n\t\tbuf = append(buf, _CRLF_...)\n\t\tbuf = append(buf, msg...)\n\n\t\troute.mu.Lock()\n\t\troute.enqueueProto(buf)\n\t\tif route.trace {\n\t\t\troute.traceOutOp(\"\", buf[:mhEnd])\n\t\t}\n\t\troute.mu.Unlock()\n\t}\n\treturn true\n}\n\n// Process a message coming from a remote gateway. Send to any sub/qsub\n// in our cluster that is matching. When receiving a message for an\n// account or subject for which there is no interest in this cluster\n// an A-/RS- protocol may be send back.\n// <Invoked from inbound connection's readLoop>\nfunc (c *client) processInboundGatewayMsg(msg []byte) {\n\t// Update statistics\n\tc.in.msgs++\n\t// The msg includes the CR_LF, so pull back out for accounting.\n\tsize := len(msg) - LEN_CR_LF\n\tc.in.bytes += int32(size)\n\n\tif c.opts.Verbose {\n\t\tc.sendOK()\n\t}\n\n\t// Mostly under testing scenarios.\n\tif c.srv == nil {\n\t\treturn\n\t}\n\n\t// If the subject (c.pa.subject) has the gateway prefix, this function will\n\t// handle it.\n\tif c.handleGatewayReply(msg) {\n\t\t// We are done here.\n\t\treturn\n\t}\n\n\tacc, r := c.getAccAndResultFromCache()\n\tif acc == nil {\n\t\tc.Debugf(\"Unknown account %q for gateway message on subject: %q\", c.pa.account, c.pa.subject)\n\t\tc.srv.gatewayHandleAccountNoInterest(c, c.pa.account)\n\t\treturn\n\t}\n\n\tacc.stats.Lock()\n\tacc.stats.inMsgs++\n\tacc.stats.inBytes += int64(size)\n\tacc.stats.gw.inMsgs++\n\tacc.stats.gw.inBytes += int64(size)\n\tacc.stats.Unlock()\n\n\t// Check if this is a service reply subject (_R_)\n\tnoInterest := len(r.psubs) == 0\n\tcheckNoInterest := true\n\tif acc.NumServiceImports() > 0 {\n\t\tif isServiceReply(c.pa.subject) {\n\t\t\tcheckNoInterest = false\n\t\t} else {\n\t\t\t// We need to eliminate the subject interest from the service imports here to\n\t\t\t// make sure we send the proper no interest if the service import is the only interest.\n\t\t\tnoInterest = true\n\t\t\tfor _, sub := range r.psubs {\n\t\t\t\t// sub.si indicates that this is a subscription for service import, and is immutable.\n\t\t\t\t// So sub.si is false, then this is a subscription for something else, so there is\n\t\t\t\t// actually proper interest.\n\t\t\t\tif !sub.si {\n\t\t\t\t\tnoInterest = false\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tif checkNoInterest && noInterest {\n\t\t// If there is no interest on plain subs, possibly send an RS-,\n\t\t// even if there is qsubs interest.\n\t\tc.srv.gatewayHandleSubjectNoInterest(c, acc, c.pa.account, c.pa.subject)\n\n\t\t// If there is also no queue filter, then no point in continuing\n\t\t// (even if r.qsubs i > 0).\n\t\tif len(c.pa.queues) == 0 {\n\t\t\treturn\n\t\t}\n\t}\n\tc.processMsgResults(acc, r, msg, nil, c.pa.subject, c.pa.reply, pmrNoFlag)\n}\n\n// Indicates that the remote which we are sending messages to\n// has decided to send us all its subs interest so that we\n// stop doing optimistic sends.\n// <Invoked from outbound connection's readLoop>\nfunc (c *client) gatewayAllSubsReceiveStart(info *Info) {\n\taccount := getAccountFromGatewayCommand(c, info, \"start\")\n\tif account == \"\" {\n\t\treturn\n\t}\n\n\tc.Debugf(\"Gateway %q: switching account %q to %s mode\",\n\t\tinfo.Gateway, account, InterestOnly)\n\n\t// Since the remote would send us this start command\n\t// only after sending us too many RS- for this account,\n\t// we should always have an entry here.\n\t// TODO(ik): Should we close connection with protocol violation\n\t// error if that happens?\n\tei, _ := c.gw.outsim.Load(account)\n\tif ei != nil {\n\t\te := ei.(*outsie)\n\t\te.Lock()\n\t\te.mode = Transitioning\n\t\te.Unlock()\n\t} else {\n\t\te := &outsie{sl: NewSublistWithCache()}\n\t\te.mode = Transitioning\n\t\tc.mu.Lock()\n\t\tc.gw.outsim.Store(account, e)\n\t\tc.mu.Unlock()\n\t}\n}\n\n// Indicates that the remote has finished sending all its\n// subscriptions and we should now not send unless we know\n// there is explicit interest.\n// <Invoked from outbound connection's readLoop>\nfunc (c *client) gatewayAllSubsReceiveComplete(info *Info) {\n\taccount := getAccountFromGatewayCommand(c, info, \"complete\")\n\tif account == _EMPTY_ {\n\t\treturn\n\t}\n\t// Done receiving all subs from remote. Set the `ni`\n\t// map to nil so that gatewayInterest() no longer\n\t// uses it.\n\tei, _ := c.gw.outsim.Load(account)\n\tif ei != nil {\n\t\te := ei.(*outsie)\n\t\t// Needs locking here since `ni` is checked by\n\t\t// many go-routines calling gatewayInterest()\n\t\te.Lock()\n\t\te.ni = nil\n\t\te.mode = InterestOnly\n\t\te.Unlock()\n\n\t\tc.Debugf(\"Gateway %q: switching account %q to %s mode complete\",\n\t\t\tinfo.Gateway, account, InterestOnly)\n\t}\n}\n\n// small helper to get the account name from the INFO command.\nfunc getAccountFromGatewayCommand(c *client, info *Info, cmd string) string {\n\tif info.GatewayCmdPayload == nil {\n\t\tc.sendErrAndErr(fmt.Sprintf(\"Account absent from receive-all-subscriptions-%s command\", cmd))\n\t\tc.closeConnection(ProtocolViolation)\n\t\treturn _EMPTY_\n\t}\n\treturn string(info.GatewayCmdPayload)\n}\n\n// Switch to send-all-subs mode for the given gateway and account.\n// This is invoked when processing an inbound message and we\n// reach a point where we had to send a lot of RS- for this\n// account. We will send an INFO protocol to indicate that we\n// start sending all our subs (for this account), followed by\n// all subs (RS+) and finally an INFO to indicate the end of it.\n// The remote will then send messages only if it finds explicit\n// interest in the sublist created based on all RS+ that we just\n// sent.\n// The client's lock is held on entry.\n// <Invoked from inbound connection's readLoop>\nfunc (c *client) gatewaySwitchAccountToSendAllSubs(e *insie, accName string) {\n\t// Set this map to nil so that the no-interest is no longer checked.\n\te.ni = nil\n\t// Switch mode to transitioning to prevent switchAccountToInterestMode\n\t// to possibly call this function multiple times.\n\te.mode = Transitioning\n\ts := c.srv\n\n\tremoteGWName := c.gw.name\n\tc.Debugf(\"Gateway %q: switching account %q to %s mode\",\n\t\tremoteGWName, accName, InterestOnly)\n\n\t// Function that will create an INFO protocol\n\t// and set proper command.\n\tsendCmd := func(cmd byte, useLock bool) {\n\t\t// Use bare server info and simply set the\n\t\t// gateway name and command\n\t\tinfo := Info{\n\t\t\tGateway:           s.gateway.name,\n\t\t\tGatewayCmd:        cmd,\n\t\t\tGatewayCmdPayload: stringToBytes(accName),\n\t\t}\n\n\t\tb, _ := json.Marshal(&info)\n\t\tinfoJSON := []byte(fmt.Sprintf(InfoProto, b))\n\t\tif useLock {\n\t\t\tc.mu.Lock()\n\t\t}\n\t\tc.enqueueProto(infoJSON)\n\t\tif useLock {\n\t\t\tc.mu.Unlock()\n\t\t}\n\t}\n\t// Send the start command. When remote receives this,\n\t// it may continue to send optimistic messages, but\n\t// it will start to register RS+/RS- in sublist instead\n\t// of noInterest map.\n\tsendCmd(gatewayCmdAllSubsStart, false)\n\n\t// Execute this in separate go-routine as to not block\n\t// the readLoop (which may cause the otherside to close\n\t// the connection due to slow consumer)\n\ts.startGoRoutine(func() {\n\t\tdefer s.grWG.Done()\n\n\t\ts.sendAccountSubsToGateway(c, accName)\n\t\t// Send the complete command. When the remote receives\n\t\t// this, it will not send a message unless it has a\n\t\t// matching sub from us.\n\t\tsendCmd(gatewayCmdAllSubsComplete, true)\n\n\t\tc.Debugf(\"Gateway %q: switching account %q to %s mode complete\",\n\t\t\tremoteGWName, accName, InterestOnly)\n\t})\n}\n\n// Keeps track of the routed reply to be used when/if application sends back a\n// message on the reply without the prefix.\n// If `client` is not nil, it will be stored in the client gwReplyMapping structure,\n// and client lock is held on entry.\n// If `client` is nil, the mapping is stored in the client's account's gwReplyMapping\n// structure. Account lock will be explicitly acquired.\n// This is a server receiver because we use a timer interval that is avail in\n// Server.gateway object.\nfunc (s *Server) trackGWReply(c *client, acc *Account, reply, routedReply []byte) {\n\tvar l sync.Locker\n\tvar g *gwReplyMapping\n\tif acc != nil {\n\t\tacc.mu.Lock()\n\t\tdefer acc.mu.Unlock()\n\t\tg = &acc.gwReplyMapping\n\t\tl = &acc.mu\n\t} else {\n\t\tg = &c.gwReplyMapping\n\t\tl = &c.mu\n\t}\n\tttl := s.gateway.recSubExp\n\twasEmpty := len(g.mapping) == 0\n\tif g.mapping == nil {\n\t\tg.mapping = make(map[string]*gwReplyMap)\n\t}\n\t// The reason we pass both `reply` and `routedReply`, is that in some cases,\n\t// `routedReply` may have a deliver subject appended, something look like:\n\t// \"_GR_.xxx.yyy.$JS.ACK.$MQTT_msgs.someid.1.1.1.1620086713306484000.0@$MQTT.msgs.foo\"\n\t// but `reply` has already been cleaned up (delivery subject removed from tail):\n\t// \"$JS.ACK.$MQTT_msgs.someid.1.1.1.1620086713306484000.0\"\n\t// So we will use that knowledge so we don't have to make any cleaning here.\n\troutedReply = routedReply[:gwSubjectOffset+len(reply)]\n\t// We need to make a copy so that we don't reference the underlying\n\t// read buffer.\n\tms := string(routedReply)\n\tgrm := &gwReplyMap{ms: ms, exp: time.Now().Add(ttl).UnixNano()}\n\t// If we are here with the same key but different mapped replies\n\t// (say $GNR._.A.srv1.bar and then $GNR._.B.srv2.bar), we need to\n\t// store it otherwise we would take the risk of the reply not\n\t// making it back.\n\tg.mapping[ms[gwSubjectOffset:]] = grm\n\tif wasEmpty {\n\t\tatomic.StoreInt32(&g.check, 1)\n\t\ts.gwrm.m.Store(g, l)\n\t\tif atomic.CompareAndSwapInt32(&s.gwrm.w, 0, 1) {\n\t\t\tselect {\n\t\t\tcase s.gwrm.ch <- ttl:\n\t\t\tdefault:\n\t\t\t}\n\t\t}\n\t}\n}\n\n// Starts a long lived go routine that is responsible to\n// remove GW reply mapping that have expired.\nfunc (s *Server) startGWReplyMapExpiration() {\n\ts.mu.Lock()\n\ts.gwrm.ch = make(chan time.Duration, 1)\n\ts.mu.Unlock()\n\ts.startGoRoutine(func() {\n\t\tdefer s.grWG.Done()\n\n\t\tt := time.NewTimer(time.Hour)\n\t\tvar ttl time.Duration\n\t\tfor {\n\t\t\tselect {\n\t\t\tcase <-t.C:\n\t\t\t\tif ttl == 0 {\n\t\t\t\t\tt.Reset(time.Hour)\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tnow := time.Now().UnixNano()\n\t\t\t\tmapEmpty := true\n\t\t\t\ts.gwrm.m.Range(func(k, v any) bool {\n\t\t\t\t\tg := k.(*gwReplyMapping)\n\t\t\t\t\tl := v.(sync.Locker)\n\t\t\t\t\tl.Lock()\n\t\t\t\t\tfor k, grm := range g.mapping {\n\t\t\t\t\t\tif grm.exp <= now {\n\t\t\t\t\t\t\tdelete(g.mapping, k)\n\t\t\t\t\t\t\tif len(g.mapping) == 0 {\n\t\t\t\t\t\t\t\tatomic.StoreInt32(&g.check, 0)\n\t\t\t\t\t\t\t\ts.gwrm.m.Delete(g)\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tl.Unlock()\n\t\t\t\t\tmapEmpty = false\n\t\t\t\t\treturn true\n\t\t\t\t})\n\t\t\t\tif mapEmpty && atomic.CompareAndSwapInt32(&s.gwrm.w, 1, 0) {\n\t\t\t\t\tttl = 0\n\t\t\t\t\tt.Reset(time.Hour)\n\t\t\t\t} else {\n\t\t\t\t\tt.Reset(ttl)\n\t\t\t\t}\n\t\t\tcase cttl := <-s.gwrm.ch:\n\t\t\t\tttl = cttl\n\t\t\t\tif !t.Stop() {\n\t\t\t\t\tselect {\n\t\t\t\t\tcase <-t.C:\n\t\t\t\t\tdefault:\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tt.Reset(ttl)\n\t\t\tcase <-s.quitCh:\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t})\n}\n",
    "source_file": "server/gateway.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2012-2025 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage server\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"crypto/tls\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"flag\"\n\t\"fmt\"\n\t\"io\"\n\t\"log\"\n\t\"math/rand\"\n\t\"net\"\n\t\"net/http\"\n\t\"net/url\"\n\t\"os\"\n\t\"path\"\n\t\"path/filepath\"\n\t\"regexp\"\n\t\"runtime\"\n\t\"runtime/pprof\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t// Allow dynamic profiling.\n\t_ \"net/http/pprof\"\n\n\t\"github.com/klauspost/compress/s2\"\n\t\"github.com/nats-io/jwt/v2\"\n\t\"github.com/nats-io/nats-server/v2/logger\"\n\t\"github.com/nats-io/nkeys\"\n\t\"github.com/nats-io/nuid\"\n)\n\nconst (\n\t// Interval for the first PING for non client connections.\n\tfirstPingInterval = time.Second\n\n\t// This is for the first ping for client connections.\n\tfirstClientPingInterval = 2 * time.Second\n)\n\n// These are protocol versions sent between server connections: ROUTER, LEAF and\n// GATEWAY. We may have protocol versions that have a meaning only for a certain\n// type of connections, but we don't have to have separate enums for that.\n// However, it is CRITICAL to not change the order of those constants since they\n// are exchanged between servers. When adding a new protocol version, add to the\n// end of the list, don't try to group them by connection types.\nconst (\n\t// RouteProtoZero is the original Route protocol from 2009.\n\t// http://nats.io/documentation/internals/nats-protocol/\n\tRouteProtoZero = iota\n\t// RouteProtoInfo signals a route can receive more then the original INFO block.\n\t// This can be used to update remote cluster permissions, etc...\n\tRouteProtoInfo\n\t// RouteProtoV2 is the new route/cluster protocol that provides account support.\n\tRouteProtoV2\n\t// MsgTraceProto indicates that this server understands distributed message tracing.\n\tMsgTraceProto\n)\n\n// Will return the latest server-to-server protocol versions, unless the\n// option to override it is set.\nfunc (s *Server) getServerProto() int {\n\topts := s.getOpts()\n\t// Initialize with the latest protocol version.\n\tproto := MsgTraceProto\n\t// For tests, we want to be able to make this server behave\n\t// as an older server so check this option to see if we should override.\n\tif opts.overrideProto < 0 {\n\t\t// The option overrideProto is set to 0 by default (when creating an\n\t\t// Options structure). Since this is the same value than the original\n\t\t// proto RouteProtoZero, tests call setServerProtoForTest() with the\n\t\t// desired protocol level, which sets it as negative value equal to:\n\t\t// (wantedProto + 1) * -1. Here we compute back the real value.\n\t\tproto = (opts.overrideProto * -1) - 1\n\t}\n\treturn proto\n}\n\n// Used by tests.\nfunc setServerProtoForTest(wantedProto int) int {\n\treturn (wantedProto + 1) * -1\n}\n\n// Info is the information sent to clients, routes, gateways, and leaf nodes,\n// to help them understand information about this server.\ntype Info struct {\n\tID                string   `json:\"server_id\"`\n\tName              string   `json:\"server_name\"`\n\tVersion           string   `json:\"version\"`\n\tProto             int      `json:\"proto\"`\n\tGitCommit         string   `json:\"git_commit,omitempty\"`\n\tGoVersion         string   `json:\"go\"`\n\tHost              string   `json:\"host\"`\n\tPort              int      `json:\"port\"`\n\tHeaders           bool     `json:\"headers\"`\n\tAuthRequired      bool     `json:\"auth_required,omitempty\"`\n\tTLSRequired       bool     `json:\"tls_required,omitempty\"`\n\tTLSVerify         bool     `json:\"tls_verify,omitempty\"`\n\tTLSAvailable      bool     `json:\"tls_available,omitempty\"`\n\tMaxPayload        int32    `json:\"max_payload\"`\n\tJetStream         bool     `json:\"jetstream,omitempty\"`\n\tIP                string   `json:\"ip,omitempty\"`\n\tCID               uint64   `json:\"client_id,omitempty\"`\n\tClientIP          string   `json:\"client_ip,omitempty\"`\n\tNonce             string   `json:\"nonce,omitempty\"`\n\tCluster           string   `json:\"cluster,omitempty\"`\n\tDynamic           bool     `json:\"cluster_dynamic,omitempty\"`\n\tDomain            string   `json:\"domain,omitempty\"`\n\tClientConnectURLs []string `json:\"connect_urls,omitempty\"`    // Contains URLs a client can connect to.\n\tWSConnectURLs     []string `json:\"ws_connect_urls,omitempty\"` // Contains URLs a ws client can connect to.\n\tLameDuckMode      bool     `json:\"ldm,omitempty\"`\n\tCompression       string   `json:\"compression,omitempty\"`\n\tConnectInfo       bool     `json:\"connect_info,omitempty\"`   // When true this is the server INFO response to CONNECT\n\tRemoteAccount     string   `json:\"remote_account,omitempty\"` // Lets the client or leafnode side know the remote account that they bind to.\n\tIsSystemAccount   bool     `json:\"acc_is_sys,omitempty\"`     // Indicates if the account is a system account.\n\n\t// Route Specific\n\tImport        *SubjectPermission `json:\"import,omitempty\"`\n\tExport        *SubjectPermission `json:\"export,omitempty\"`\n\tLNOC          bool               `json:\"lnoc,omitempty\"`\n\tLNOCU         bool               `json:\"lnocu,omitempty\"`\n\tInfoOnConnect bool               `json:\"info_on_connect,omitempty\"` // When true the server will respond to CONNECT with an INFO\n\tRoutePoolSize int                `json:\"route_pool_size,omitempty\"`\n\tRoutePoolIdx  int                `json:\"route_pool_idx,omitempty\"`\n\tRouteAccount  string             `json:\"route_account,omitempty\"`\n\tRouteAccReqID string             `json:\"route_acc_add_reqid,omitempty\"`\n\tGossipMode    byte               `json:\"gossip_mode,omitempty\"`\n\n\t// Gateways Specific\n\tGateway           string   `json:\"gateway,omitempty\"`             // Name of the origin Gateway (sent by gateway's INFO)\n\tGatewayURLs       []string `json:\"gateway_urls,omitempty\"`        // Gateway URLs in the originating cluster (sent by gateway's INFO)\n\tGatewayURL        string   `json:\"gateway_url,omitempty\"`         // Gateway URL on that server (sent by route's INFO)\n\tGatewayCmd        byte     `json:\"gateway_cmd,omitempty\"`         // Command code for the receiving server to know what to do\n\tGatewayCmdPayload []byte   `json:\"gateway_cmd_payload,omitempty\"` // Command payload when needed\n\tGatewayNRP        bool     `json:\"gateway_nrp,omitempty\"`         // Uses new $GNR. prefix for mapped replies\n\tGatewayIOM        bool     `json:\"gateway_iom,omitempty\"`         // Indicate that all accounts will be switched to InterestOnly mode \"right away\"\n\n\t// LeafNode Specific\n\tLeafNodeURLs []string `json:\"leafnode_urls,omitempty\"` // LeafNode URLs that the server can reconnect to.\n\n\tXKey string `json:\"xkey,omitempty\"` // Public server's x25519 key.\n}\n\n// Server is our main struct.\ntype Server struct {\n\t// Fields accessed with atomic operations need to be 64-bit aligned\n\tgcid uint64\n\t// How often user logon fails due to the issuer account not being pinned.\n\tpinnedAccFail uint64\n\tstats\n\tscStats\n\tmu                  sync.RWMutex\n\treloadMu            sync.RWMutex // Write-locked when a config reload is taking place ONLY\n\tkp                  nkeys.KeyPair\n\txkp                 nkeys.KeyPair\n\txpub                string\n\tinfo                Info\n\tconfigFile          string\n\toptsMu              sync.RWMutex\n\topts                *Options\n\trunning             atomic.Bool\n\tshutdown            atomic.Bool\n\tlistener            net.Listener\n\tlistenerErr         error\n\tgacc                *Account\n\tsys                 *internal\n\tsysAcc              atomic.Pointer[Account]\n\tjs                  atomic.Pointer[jetStream]\n\tisMetaLeader        atomic.Bool\n\tjsClustered         atomic.Bool\n\taccounts            sync.Map\n\ttmpAccounts         sync.Map // Temporarily stores accounts that are being built\n\tactiveAccounts      int32\n\taccResolver         AccountResolver\n\tclients             map[uint64]*client\n\troutes              map[string][]*client\n\tremoteRoutePoolSize map[string]int                // Map for remote's configure route pool size\n\troutesPoolSize      int                           // Configured pool size\n\troutesReject        bool                          // During reload, we may want to reject adding routes until some conditions are met\n\troutesNoPool        int                           // Number of routes that don't use pooling (connecting to older server for instance)\n\taccRoutes           map[string]map[string]*client // Key is account name, value is key=remoteID/value=route connection\n\taccRouteByHash      sync.Map                      // Key is account name, value is nil or a pool index\n\taccAddedCh          chan struct{}\n\taccAddedReqID       string\n\tleafs               map[uint64]*client\n\tusers               map[string]*User\n\tnkeys               map[string]*NkeyUser\n\ttotalClients        uint64\n\tclosed              *closedRingBuffer\n\tdone                chan bool\n\tstart               time.Time\n\thttp                net.Listener\n\thttpHandler         http.Handler\n\thttpBasePath        string\n\tprofiler            net.Listener\n\thttpReqStats        map[string]uint64\n\trouteListener       net.Listener\n\trouteListenerErr    error\n\trouteInfo           Info\n\trouteResolver       netResolver\n\troutesToSelf        map[string]struct{}\n\trouteTLSName        string\n\tleafNodeListener    net.Listener\n\tleafNodeListenerErr error\n\tleafNodeInfo        Info\n\tleafNodeInfoJSON    []byte\n\tleafURLsMap         refCountedUrlSet\n\tleafNodeOpts        struct {\n\t\tresolver    netResolver\n\t\tdialTimeout time.Duration\n\t}\n\tleafRemoteCfgs     []*leafNodeCfg\n\tleafRemoteAccounts sync.Map\n\tleafNodeEnabled    bool\n\tleafDisableConnect bool // Used in test only\n\tleafNoCluster      bool // Indicate that this server has only remotes and no cluster defined\n\n\tquitCh           chan struct{}\n\tstartupComplete  chan struct{}\n\tshutdownComplete chan struct{}\n\n\t// Tracking Go routines\n\tgrMu         sync.Mutex\n\tgrTmpClients map[uint64]*client\n\tgrRunning    bool\n\tgrWG         sync.WaitGroup // to wait on various go routines\n\n\tcproto     int64     // number of clients supporting async INFO\n\tconfigTime time.Time // last time config was loaded\n\n\tlogging struct {\n\t\tsync.RWMutex\n\t\tlogger      Logger\n\t\ttrace       int32\n\t\tdebug       int32\n\t\ttraceSysAcc int32\n\t}\n\n\tclientConnectURLs []string\n\n\t// Used internally for quick look-ups.\n\tclientConnectURLsMap refCountedUrlSet\n\n\t// For Gateways\n\tgatewayListener    net.Listener // Accept listener\n\tgatewayListenerErr error\n\tgateway            *srvGateway\n\n\t// Used by tests to check that http.Servers do\n\t// not set any timeout.\n\tmonitoringServer *http.Server\n\tprofilingServer  *http.Server\n\n\t// LameDuck mode\n\tldm   bool\n\tldmCh chan bool\n\n\t// Trusted public operator keys.\n\ttrustedKeys []string\n\t// map of trusted keys to operator setting StrictSigningKeyUsage\n\tstrictSigningKeyUsage map[string]struct{}\n\n\t// We use this to minimize mem copies for requests to monitoring\n\t// endpoint /varz (when it comes from http).\n\tvarzMu sync.Mutex\n\tvarz   *Varz\n\t// This is set during a config reload if we detect that we have\n\t// added/removed routes. The monitoring code then check that\n\t// to know if it should update the cluster's URLs array.\n\tvarzUpdateRouteURLs bool\n\n\t// Keeps a sublist of of subscriptions attached to leafnode connections\n\t// for the $GNR.*.*.*.> subject so that a server can send back a mapped\n\t// gateway reply.\n\tgwLeafSubs *Sublist\n\n\t// Used for expiration of mapped GW replies\n\tgwrm struct {\n\t\tw  int32\n\t\tch chan time.Duration\n\t\tm  sync.Map\n\t}\n\n\t// For eventIDs\n\teventIds *nuid.NUID\n\n\t// Websocket structure\n\twebsocket srvWebsocket\n\n\t// MQTT structure\n\tmqtt srvMQTT\n\n\t// OCSP monitoring\n\tocsps []*OCSPMonitor\n\n\t// OCSP peer verification (at least one TLS block)\n\tocspPeerVerify bool\n\n\t// OCSP response cache\n\tocsprc OCSPResponseCache\n\n\t// exporting account name the importer experienced issues with\n\tincompleteAccExporterMap sync.Map\n\n\t// Holds cluster name under different lock for mapping\n\tcnMu sync.RWMutex\n\tcn   string\n\n\t// For registering raft nodes with the server.\n\trnMu      sync.RWMutex\n\traftNodes map[string]RaftNode\n\n\t// For mapping from a raft node name back to a server name and cluster. Node has to be in the same domain.\n\tnodeToInfo sync.Map\n\n\t// For out of resources to not log errors too fast.\n\trerrMu   sync.Mutex\n\trerrLast time.Time\n\n\tconnRateCounter *rateCounter\n\n\t// If there is a system account configured, to still support the $G account,\n\t// the server will create a fake user and add it to the list of users.\n\t// Keep track of what that user name is for config reload purposes.\n\tsysAccOnlyNoAuthUser string\n\n\t// IPQueues map\n\tipQueues sync.Map\n\n\t// To limit logging frequency\n\trateLimitLogging   sync.Map\n\trateLimitLoggingCh chan time.Duration\n\n\t// Total outstanding catchup bytes in flight.\n\tgcbMu     sync.RWMutex\n\tgcbOut    int64\n\tgcbOutMax int64 // Taken from JetStreamMaxCatchup or defaultMaxTotalCatchupOutBytes\n\t// A global chanel to kick out stalled catchup sequences.\n\tgcbKick chan struct{}\n\n\t// Total outbound syncRequests\n\tsyncOutSem chan struct{}\n\n\t// Queue to process JS API requests that come from routes (or gateways)\n\tjsAPIRoutedReqs *ipQueue[*jsAPIRoutedReq]\n\n\t// Delayed API responses.\n\tdelayedAPIResponses *ipQueue[*delayedAPIResponse]\n\n\t// Whether moving NRG traffic into accounts is permitted on this server.\n\t// Controls whether or not the account NRG capability is set in statsz.\n\t// Currently used by unit tests to simulate nodes not supporting account NRG.\n\taccountNRGAllowed atomic.Bool\n}\n\n// For tracking JS nodes.\ntype nodeInfo struct {\n\tname            string\n\tversion         string\n\tcluster         string\n\tdomain          string\n\tid              string\n\ttags            jwt.TagList\n\tcfg             *JetStreamConfig\n\tstats           *JetStreamStats\n\toffline         bool\n\tjs              bool\n\tbinarySnapshots bool\n\taccountNRG      bool\n}\n\ntype stats struct {\n\tinMsgs        int64\n\toutMsgs       int64\n\tinBytes       int64\n\toutBytes      int64\n\tslowConsumers int64\n}\n\n// scStats includes the total and per connection counters of Slow Consumers.\ntype scStats struct {\n\tclients  atomic.Uint64\n\troutes   atomic.Uint64\n\tleafs    atomic.Uint64\n\tgateways atomic.Uint64\n}\n\n// This is used by tests so we can run all server tests with a default route\n// or leafnode compression mode. For instance:\n// go test -race -v ./server -cluster_compression=fast\nvar (\n\ttestDefaultClusterCompression  string\n\ttestDefaultLeafNodeCompression string\n)\n\n// Compression modes.\nconst (\n\tCompressionNotSupported   = \"not supported\"\n\tCompressionOff            = \"off\"\n\tCompressionAccept         = \"accept\"\n\tCompressionS2Auto         = \"s2_auto\"\n\tCompressionS2Uncompressed = \"s2_uncompressed\"\n\tCompressionS2Fast         = \"s2_fast\"\n\tCompressionS2Better       = \"s2_better\"\n\tCompressionS2Best         = \"s2_best\"\n)\n\n// defaultCompressionS2AutoRTTThresholds is the default of RTT thresholds for\n// the CompressionS2Auto mode.\nvar defaultCompressionS2AutoRTTThresholds = []time.Duration{\n\t// [0..10ms] -> CompressionS2Uncompressed\n\t10 * time.Millisecond,\n\t// ]10ms..50ms] -> CompressionS2Fast\n\t50 * time.Millisecond,\n\t// ]50ms..100ms] -> CompressionS2Better\n\t100 * time.Millisecond,\n\t// ]100ms..] -> CompressionS2Best\n}\n\n// For a given user provided string, matches to one of the compression mode\n// constant and updates the provided string to that constant. Returns an\n// error if the provided compression mode is not known.\n// The parameter `chosenModeForOn` indicates which compression mode to use\n// when the user selects \"on\" (or enabled, true, etc..). This is because\n// we may have different defaults depending on where the compression is used.\nfunc validateAndNormalizeCompressionOption(c *CompressionOpts, chosenModeForOn string) error {\n\tif c == nil {\n\t\treturn nil\n\t}\n\tcmtl := strings.ToLower(c.Mode)\n\t// First, check for the \"on\" case so that we set to the default compression\n\t// mode for that. The other switch/case will finish setup if needed (for\n\t// instance if the default mode is s2Auto).\n\tswitch cmtl {\n\tcase \"on\", \"enabled\", \"true\":\n\t\tcmtl = chosenModeForOn\n\tdefault:\n\t}\n\t// Check (again) with the proper mode.\n\tswitch cmtl {\n\tcase \"not supported\", \"not_supported\":\n\t\tc.Mode = CompressionNotSupported\n\tcase \"disabled\", \"off\", \"false\":\n\t\tc.Mode = CompressionOff\n\tcase \"accept\":\n\t\tc.Mode = CompressionAccept\n\tcase \"auto\", \"s2_auto\":\n\t\tvar rtts []time.Duration\n\t\tif len(c.RTTThresholds) == 0 {\n\t\t\trtts = defaultCompressionS2AutoRTTThresholds\n\t\t} else {\n\t\t\tfor _, n := range c.RTTThresholds {\n\t\t\t\t// Do not error on negative, but simply set to 0\n\t\t\t\tif n < 0 {\n\t\t\t\t\tn = 0\n\t\t\t\t}\n\t\t\t\t// Make sure they are properly ordered. However, it is possible\n\t\t\t\t// to have a \"0\" anywhere in the list to indicate that this\n\t\t\t\t// compression level should not be used.\n\t\t\t\tif l := len(rtts); l > 0 && n != 0 {\n\t\t\t\t\tfor _, v := range rtts {\n\t\t\t\t\t\tif n < v {\n\t\t\t\t\t\t\treturn fmt.Errorf(\"RTT threshold values %v should be in ascending order\", c.RTTThresholds)\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\trtts = append(rtts, n)\n\t\t\t}\n\t\t\tif len(rtts) > 0 {\n\t\t\t\t// Trim 0 that are at the end.\n\t\t\t\tstop := -1\n\t\t\t\tfor i := len(rtts) - 1; i >= 0; i-- {\n\t\t\t\t\tif rtts[i] != 0 {\n\t\t\t\t\t\tstop = i\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\trtts = rtts[:stop+1]\n\t\t\t}\n\t\t\tif len(rtts) > 4 {\n\t\t\t\t// There should be at most values for \"uncompressed\", \"fast\",\n\t\t\t\t// \"better\" and \"best\" (when some 0 are present).\n\t\t\t\treturn fmt.Errorf(\"compression mode %q should have no more than 4 RTT thresholds: %v\", c.Mode, c.RTTThresholds)\n\t\t\t} else if len(rtts) == 0 {\n\t\t\t\t// But there should be at least 1 if the user provided the slice.\n\t\t\t\t// We would be here only if it was provided by say with values\n\t\t\t\t// being a single or all zeros.\n\t\t\t\treturn fmt.Errorf(\"compression mode %q requires at least one RTT threshold\", c.Mode)\n\t\t\t}\n\t\t}\n\t\tc.Mode = CompressionS2Auto\n\t\tc.RTTThresholds = rtts\n\tcase \"fast\", \"s2_fast\":\n\t\tc.Mode = CompressionS2Fast\n\tcase \"better\", \"s2_better\":\n\t\tc.Mode = CompressionS2Better\n\tcase \"best\", \"s2_best\":\n\t\tc.Mode = CompressionS2Best\n\tdefault:\n\t\treturn fmt.Errorf(\"unsupported compression mode %q\", c.Mode)\n\t}\n\treturn nil\n}\n\n// Returns `true` if the compression mode `m` indicates that the server\n// will negotiate compression with the remote server, `false` otherwise.\n// Note that the provided compression mode is assumed to have been\n// normalized and validated.\nfunc needsCompression(m string) bool {\n\treturn m != _EMPTY_ && m != CompressionOff && m != CompressionNotSupported\n}\n\n// Compression is asymmetric, meaning that one side can have a different\n// compression level than the other. However, we need to check for cases\n// when this server `scm` or the remote `rcm` do not support compression\n// (say older server, or test to make it behave as it is not), or have\n// the compression off.\n// Note that `scm` is assumed to not be \"off\" or \"not supported\".\nfunc selectCompressionMode(scm, rcm string) (mode string, err error) {\n\tif rcm == CompressionNotSupported || rcm == _EMPTY_ {\n\t\treturn CompressionNotSupported, nil\n\t}\n\tswitch rcm {\n\tcase CompressionOff:\n\t\t// If the remote explicitly disables compression, then we won't\n\t\t// use compression.\n\t\treturn CompressionOff, nil\n\tcase CompressionAccept:\n\t\t// If the remote is ok with compression (but is not initiating it),\n\t\t// and if we too are in this mode, then it means no compression.\n\t\tif scm == CompressionAccept {\n\t\t\treturn CompressionOff, nil\n\t\t}\n\t\t// Otherwise use our compression mode.\n\t\treturn scm, nil\n\tcase CompressionS2Auto, CompressionS2Uncompressed, CompressionS2Fast, CompressionS2Better, CompressionS2Best:\n\t\t// This case is here to make sure that if we don't recognize a\n\t\t// compression setting, we error out.\n\t\tif scm == CompressionAccept {\n\t\t\t// If our compression mode is \"accept\", then we will use the remote\n\t\t\t// compression mode, except if it is \"auto\", in which case we will\n\t\t\t// default to \"fast\". This is not a configuration (auto in one\n\t\t\t// side and accept in the other) that would be recommended.\n\t\t\tif rcm == CompressionS2Auto {\n\t\t\t\treturn CompressionS2Fast, nil\n\t\t\t}\n\t\t\t// Use their compression mode.\n\t\t\treturn rcm, nil\n\t\t}\n\t\t// Otherwise use our compression mode.\n\t\treturn scm, nil\n\tdefault:\n\t\treturn _EMPTY_, fmt.Errorf(\"unsupported route compression mode %q\", rcm)\n\t}\n}\n\n// If the configured compression mode is \"auto\" then will return that,\n// otherwise will return the given `cm` compression mode.\nfunc compressionModeForInfoProtocol(co *CompressionOpts, cm string) string {\n\tif co.Mode == CompressionS2Auto {\n\t\treturn CompressionS2Auto\n\t}\n\treturn cm\n}\n\n// Given a connection RTT and a list of thresholds durations, this\n// function will return an S2 compression level such as \"uncompressed\",\n// \"fast\", \"better\" or \"best\". For instance, with the following slice:\n// [5ms, 10ms, 15ms, 20ms], a RTT of up to 5ms will result\n// in the compression level \"uncompressed\", ]5ms..10ms] will result in\n// \"fast\" compression, etc..\n// However, the 0 value allows for disabling of some compression levels.\n// For instance, the following slice: [0, 0, 20, 30] means that a RTT of\n// [0..20ms] would result in the \"better\" compression - effectively disabling\n// the use of \"uncompressed\" and \"fast\", then anything above 20ms would\n// result in the use of \"best\" level (the 30 in the list has no effect\n// and the list could have been simplified to [0, 0, 20]).\nfunc selectS2AutoModeBasedOnRTT(rtt time.Duration, rttThresholds []time.Duration) string {\n\tvar idx int\n\tvar found bool\n\tfor i, d := range rttThresholds {\n\t\tif rtt <= d {\n\t\t\tidx = i\n\t\t\tfound = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif !found {\n\t\t// If we did not find but we have all levels, then use \"best\",\n\t\t// otherwise use the last one in array.\n\t\tif l := len(rttThresholds); l >= 3 {\n\t\t\tidx = 3\n\t\t} else {\n\t\t\tidx = l - 1\n\t\t}\n\t}\n\tswitch idx {\n\tcase 0:\n\t\treturn CompressionS2Uncompressed\n\tcase 1:\n\t\treturn CompressionS2Fast\n\tcase 2:\n\t\treturn CompressionS2Better\n\t}\n\treturn CompressionS2Best\n}\n\n// Returns an array of s2 WriterOption based on the route compression mode.\n// So far we return a single option, but this way we can call s2.NewWriter()\n// with a nil []s2.WriterOption, but not with a nil s2.WriterOption, so\n// this is more versatile.\nfunc s2WriterOptions(cm string) []s2.WriterOption {\n\t_opts := [2]s2.WriterOption{}\n\topts := append(\n\t\t_opts[:0],\n\t\ts2.WriterConcurrency(1), // Stop asynchronous flushing in separate goroutines\n\t)\n\tswitch cm {\n\tcase CompressionS2Uncompressed:\n\t\treturn append(opts, s2.WriterUncompressed())\n\tcase CompressionS2Best:\n\t\treturn append(opts, s2.WriterBestCompression())\n\tcase CompressionS2Better:\n\t\treturn append(opts, s2.WriterBetterCompression())\n\tdefault:\n\t\treturn nil\n\t}\n}\n\n// New will setup a new server struct after parsing the options.\n// DEPRECATED: Use NewServer(opts)\nfunc New(opts *Options) *Server {\n\ts, _ := NewServer(opts)\n\treturn s\n}\n\n// NewServer will setup a new server struct after parsing the options.\n// Could return an error if options can not be validated.\n// The provided Options type should not be re-used afterwards.\n// Either use Options.Clone() to pass a copy, or make a new one.\nfunc NewServer(opts *Options) (*Server, error) {\n\tsetBaselineOptions(opts)\n\n\t// Process TLS options, including whether we require client certificates.\n\ttlsReq := opts.TLSConfig != nil\n\tverify := (tlsReq && opts.TLSConfig.ClientAuth == tls.RequireAndVerifyClientCert)\n\n\t// Create our server's nkey identity.\n\tkp, _ := nkeys.CreateServer()\n\tpub, _ := kp.PublicKey()\n\n\t// Create an xkey for encrypting messages from this server.\n\txkp, _ := nkeys.CreateCurveKeys()\n\txpub, _ := xkp.PublicKey()\n\n\tserverName := pub\n\tif opts.ServerName != _EMPTY_ {\n\t\tserverName = opts.ServerName\n\t}\n\n\thttpBasePath := normalizeBasePath(opts.HTTPBasePath)\n\n\t// Validate some options. This is here because we cannot assume that\n\t// server will always be started with configuration parsing (that could\n\t// report issues). Its options can be (incorrectly) set by hand when\n\t// server is embedded. If there is an error, return nil.\n\tif err := validateOptions(opts); err != nil {\n\t\treturn nil, err\n\t}\n\n\tinfo := Info{\n\t\tID:           pub,\n\t\tXKey:         xpub,\n\t\tVersion:      VERSION,\n\t\tProto:        PROTO,\n\t\tGitCommit:    gitCommit,\n\t\tGoVersion:    runtime.Version(),\n\t\tName:         serverName,\n\t\tHost:         opts.Host,\n\t\tPort:         opts.Port,\n\t\tAuthRequired: false,\n\t\tTLSRequired:  tlsReq && !opts.AllowNonTLS,\n\t\tTLSVerify:    verify,\n\t\tMaxPayload:   opts.MaxPayload,\n\t\tJetStream:    opts.JetStream,\n\t\tHeaders:      !opts.NoHeaderSupport,\n\t\tCluster:      opts.Cluster.Name,\n\t\tDomain:       opts.JetStreamDomain,\n\t}\n\n\tif tlsReq && !info.TLSRequired {\n\t\tinfo.TLSAvailable = true\n\t}\n\n\tnow := time.Now()\n\n\ts := &Server{\n\t\tkp:                 kp,\n\t\txkp:                xkp,\n\t\txpub:               xpub,\n\t\tconfigFile:         opts.ConfigFile,\n\t\tinfo:               info,\n\t\topts:               opts,\n\t\tdone:               make(chan bool, 1),\n\t\tstart:              now,\n\t\tconfigTime:         now,\n\t\tgwLeafSubs:         NewSublistWithCache(),\n\t\thttpBasePath:       httpBasePath,\n\t\teventIds:           nuid.New(),\n\t\troutesToSelf:       make(map[string]struct{}),\n\t\thttpReqStats:       make(map[string]uint64), // Used to track HTTP requests\n\t\trateLimitLoggingCh: make(chan time.Duration, 1),\n\t\tleafNodeEnabled:    opts.LeafNode.Port != 0 || len(opts.LeafNode.Remotes) > 0,\n\t\tsyncOutSem:         make(chan struct{}, maxConcurrentSyncRequests),\n\t}\n\n\t// Delayed API response queue. Create regardless if JetStream is configured\n\t// or not (since it can be enabled/disabled with config reload, we want this\n\t// queue to exist at all times).\n\ts.delayedAPIResponses = newIPQueue[*delayedAPIResponse](s, \"delayed API responses\")\n\n\t// By default we'll allow account NRG.\n\ts.accountNRGAllowed.Store(true)\n\n\t// Fill up the maximum in flight syncRequests for this server.\n\t// Used in JetStream catchup semantics.\n\tfor i := 0; i < maxConcurrentSyncRequests; i++ {\n\t\ts.syncOutSem <- struct{}{}\n\t}\n\n\tif opts.TLSRateLimit > 0 {\n\t\ts.connRateCounter = newRateCounter(opts.tlsConfigOpts.RateLimit)\n\t}\n\n\t// Trusted root operator keys.\n\tif !s.processTrustedKeys() {\n\t\treturn nil, fmt.Errorf(\"Error processing trusted operator keys\")\n\t}\n\n\t// If we have solicited leafnodes but no clustering and no clustername.\n\t// However we may need a stable clustername so use the server name.\n\tif len(opts.LeafNode.Remotes) > 0 && opts.Cluster.Port == 0 && opts.Cluster.Name == _EMPTY_ {\n\t\ts.leafNoCluster = true\n\t\topts.Cluster.Name = opts.ServerName\n\t}\n\n\tif opts.Cluster.Name != _EMPTY_ {\n\t\t// Also place into mapping cn with cnMu lock.\n\t\ts.cnMu.Lock()\n\t\ts.cn = opts.Cluster.Name\n\t\ts.cnMu.Unlock()\n\t}\n\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\n\t// Place ourselves in the JetStream nodeInfo if needed.\n\tif opts.JetStream {\n\t\tourNode := getHash(serverName)\n\t\ts.nodeToInfo.Store(ourNode, nodeInfo{\n\t\t\tserverName,\n\t\t\tVERSION,\n\t\t\topts.Cluster.Name,\n\t\t\topts.JetStreamDomain,\n\t\t\tinfo.ID,\n\t\t\topts.Tags,\n\t\t\t&JetStreamConfig{MaxMemory: opts.JetStreamMaxMemory, MaxStore: opts.JetStreamMaxStore, CompressOK: true},\n\t\t\tnil,\n\t\t\tfalse, true, true, true,\n\t\t})\n\t}\n\n\ts.routeResolver = opts.Cluster.resolver\n\tif s.routeResolver == nil {\n\t\ts.routeResolver = net.DefaultResolver\n\t}\n\n\t// Used internally for quick look-ups.\n\ts.clientConnectURLsMap = make(refCountedUrlSet)\n\ts.websocket.connectURLsMap = make(refCountedUrlSet)\n\ts.leafURLsMap = make(refCountedUrlSet)\n\n\t// Ensure that non-exported options (used in tests) are properly set.\n\ts.setLeafNodeNonExportedOptions()\n\n\t// Setup OCSP Stapling and OCSP Peer. This will abort server from starting if there\n\t// are no valid staples and OCSP Stapling policy is set to Always or MustStaple.\n\tif err := s.enableOCSP(); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Call this even if there is no gateway defined. It will\n\t// initialize the structure so we don't have to check for\n\t// it to be nil or not in various places in the code.\n\tif err := s.newGateway(opts); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// If we have a cluster definition but do not have a cluster name, create one.\n\tif opts.Cluster.Port != 0 && opts.Cluster.Name == _EMPTY_ {\n\t\ts.info.Cluster = nuid.Next()\n\t} else if opts.Cluster.Name != _EMPTY_ {\n\t\t// Likewise here if we have a cluster name set.\n\t\ts.info.Cluster = opts.Cluster.Name\n\t}\n\n\t// This is normally done in the AcceptLoop, once the\n\t// listener has been created (possibly with random port),\n\t// but since some tests may expect the INFO to be properly\n\t// set after New(), let's do it now.\n\ts.setInfoHostPort()\n\n\t// For tracking clients\n\ts.clients = make(map[uint64]*client)\n\n\t// For tracking closed clients.\n\ts.closed = newClosedRingBuffer(opts.MaxClosedClients)\n\n\t// For tracking connections that are not yet registered\n\t// in s.routes, but for which readLoop has started.\n\ts.grTmpClients = make(map[uint64]*client)\n\n\t// For tracking routes and their remote ids\n\ts.initRouteStructures(opts)\n\n\t// For tracking leaf nodes.\n\ts.leafs = make(map[uint64]*client)\n\n\t// Used to kick out all go routines possibly waiting on server\n\t// to shutdown.\n\ts.quitCh = make(chan struct{})\n\n\t// Closed when startup is complete. ReadyForConnections() will block on\n\t// this before checking the presence of listening sockets.\n\ts.startupComplete = make(chan struct{})\n\n\t// Closed when Shutdown() is complete. Allows WaitForShutdown() to block\n\t// waiting for complete shutdown.\n\ts.shutdownComplete = make(chan struct{})\n\n\t// Check for configured account resolvers.\n\tif err := s.configureResolver(); err != nil {\n\t\treturn nil, err\n\t}\n\t// If there is an URL account resolver, do basic test to see if anyone is home.\n\tif ar := opts.AccountResolver; ar != nil {\n\t\tif ur, ok := ar.(*URLAccResolver); ok {\n\t\t\tif _, err := ur.Fetch(_EMPTY_); err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t}\n\t}\n\t// For other resolver:\n\t// In operator mode, when the account resolver depends on an external system and\n\t// the system account can't be fetched, inject a temporary one.\n\tif ar := s.accResolver; len(opts.TrustedOperators) == 1 && ar != nil &&\n\t\topts.SystemAccount != _EMPTY_ && opts.SystemAccount != DEFAULT_SYSTEM_ACCOUNT {\n\t\tif _, ok := ar.(*MemAccResolver); !ok {\n\t\t\ts.mu.Unlock()\n\t\t\tvar a *Account\n\t\t\t// perform direct lookup to avoid warning trace\n\t\t\tif _, err := fetchAccount(ar, opts.SystemAccount); err == nil {\n\t\t\t\ta, _ = s.lookupAccount(opts.SystemAccount)\n\t\t\t}\n\t\t\ts.mu.Lock()\n\t\t\tif a == nil {\n\t\t\t\tsac := NewAccount(opts.SystemAccount)\n\t\t\t\tsac.Issuer = opts.TrustedOperators[0].Issuer\n\t\t\t\tsac.signingKeys = map[string]jwt.Scope{}\n\t\t\t\tsac.signingKeys[opts.SystemAccount] = nil\n\t\t\t\ts.registerAccountNoLock(sac)\n\t\t\t}\n\t\t}\n\t}\n\n\t// For tracking accounts\n\tif _, err := s.configureAccounts(false); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Used to setup Authorization.\n\ts.configureAuthorization()\n\n\t// Start signal handler\n\ts.handleSignals()\n\n\treturn s, nil\n}\n\n// Initializes route structures based on pooling and/or per-account routes.\n//\n// Server lock is held on entry\nfunc (s *Server) initRouteStructures(opts *Options) {\n\ts.routes = make(map[string][]*client)\n\tif ps := opts.Cluster.PoolSize; ps > 0 {\n\t\ts.routesPoolSize = ps\n\t} else {\n\t\ts.routesPoolSize = 1\n\t}\n\t// If we have per-account routes, we create accRoutes and initialize it\n\t// with nil values. The presence of an account as the key will allow us\n\t// to know if a given account is supposed to have dedicated routes.\n\tif l := len(opts.Cluster.PinnedAccounts); l > 0 {\n\t\ts.accRoutes = make(map[string]map[string]*client, l)\n\t\tfor _, acc := range opts.Cluster.PinnedAccounts {\n\t\t\ts.accRoutes[acc] = make(map[string]*client)\n\t\t}\n\t}\n}\n\nfunc (s *Server) logRejectedTLSConns() {\n\tdefer s.grWG.Done()\n\tt := time.NewTicker(time.Second)\n\tdefer t.Stop()\n\tfor {\n\t\tselect {\n\t\tcase <-s.quitCh:\n\t\t\treturn\n\t\tcase <-t.C:\n\t\t\tblocked := s.connRateCounter.countBlocked()\n\t\t\tif blocked > 0 {\n\t\t\t\ts.Warnf(\"Rejected %d connections due to TLS rate limiting\", blocked)\n\t\t\t}\n\t\t}\n\t}\n}\n\n// clusterName returns our cluster name which could be dynamic.\nfunc (s *Server) ClusterName() string {\n\ts.mu.RLock()\n\tcn := s.info.Cluster\n\ts.mu.RUnlock()\n\treturn cn\n}\n\n// Grabs cluster name with cluster name specific lock.\nfunc (s *Server) cachedClusterName() string {\n\ts.cnMu.RLock()\n\tcn := s.cn\n\ts.cnMu.RUnlock()\n\treturn cn\n}\n\n// setClusterName will update the cluster name for this server.\nfunc (s *Server) setClusterName(name string) {\n\ts.mu.Lock()\n\tvar resetCh chan struct{}\n\tif s.sys != nil && s.info.Cluster != name {\n\t\t// can't hold the lock as go routine reading it may be waiting for lock as well\n\t\tresetCh = s.sys.resetCh\n\t}\n\ts.info.Cluster = name\n\ts.routeInfo.Cluster = name\n\n\t// Need to close solicited leaf nodes. The close has to be done outside of the server lock.\n\tvar leafs []*client\n\tfor _, c := range s.leafs {\n\t\tc.mu.Lock()\n\t\tif c.leaf != nil && c.leaf.remote != nil {\n\t\t\tleafs = append(leafs, c)\n\t\t}\n\t\tc.mu.Unlock()\n\t}\n\ts.mu.Unlock()\n\n\t// Also place into mapping cn with cnMu lock.\n\ts.cnMu.Lock()\n\ts.cn = name\n\ts.cnMu.Unlock()\n\n\tfor _, l := range leafs {\n\t\tl.closeConnection(ClusterNameConflict)\n\t}\n\tif resetCh != nil {\n\t\tresetCh <- struct{}{}\n\t}\n\ts.Noticef(\"Cluster name updated to %s\", name)\n}\n\n// Return whether the cluster name is dynamic.\nfunc (s *Server) isClusterNameDynamic() bool {\n\t// We need to lock the whole \"Cluster.Name\" check and not use s.getOpts()\n\t// because otherwise this could cause a data race with setting the name in\n\t// route.go's processRouteConnect().\n\ts.optsMu.RLock()\n\tdynamic := s.opts.Cluster.Name == _EMPTY_\n\ts.optsMu.RUnlock()\n\treturn dynamic\n}\n\n// Returns our configured serverName.\nfunc (s *Server) serverName() string {\n\treturn s.getOpts().ServerName\n}\n\n// ClientURL returns the URL used to connect clients. Helpful in testing\n// when we designate a random client port (-1).\nfunc (s *Server) ClientURL() string {\n\t// FIXME(dlc) - should we add in user and pass if defined single?\n\topts := s.getOpts()\n\tvar u url.URL\n\tu.Scheme = \"nats\"\n\tif opts.TLSConfig != nil {\n\t\tu.Scheme = \"tls\"\n\t}\n\tu.Host = net.JoinHostPort(opts.Host, fmt.Sprintf(\"%d\", opts.Port))\n\treturn u.String()\n}\n\nfunc validateCluster(o *Options) error {\n\tif o.Cluster.Name != _EMPTY_ && strings.Contains(o.Cluster.Name, \" \") {\n\t\treturn ErrClusterNameHasSpaces\n\t}\n\tif o.Cluster.Compression.Mode != _EMPTY_ {\n\t\tif err := validateAndNormalizeCompressionOption(&o.Cluster.Compression, CompressionS2Fast); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\tif err := validatePinnedCerts(o.Cluster.TLSPinnedCerts); err != nil {\n\t\treturn fmt.Errorf(\"cluster: %v\", err)\n\t}\n\t// Check that cluster name if defined matches any gateway name.\n\t// Note that we have already verified that the gateway name does not have spaces.\n\tif o.Gateway.Name != _EMPTY_ && o.Gateway.Name != o.Cluster.Name {\n\t\tif o.Cluster.Name != _EMPTY_ {\n\t\t\treturn ErrClusterNameConfigConflict\n\t\t}\n\t\t// Set this here so we do not consider it dynamic.\n\t\to.Cluster.Name = o.Gateway.Name\n\t}\n\tif l := len(o.Cluster.PinnedAccounts); l > 0 {\n\t\tif o.Cluster.PoolSize < 0 {\n\t\t\treturn fmt.Errorf(\"pool_size cannot be negative if pinned accounts are specified\")\n\t\t}\n\t\tm := make(map[string]struct{}, l)\n\t\tfor _, a := range o.Cluster.PinnedAccounts {\n\t\t\tif _, exists := m[a]; exists {\n\t\t\t\treturn fmt.Errorf(\"found duplicate account name %q in pinned accounts list %q\", a, o.Cluster.PinnedAccounts)\n\t\t\t}\n\t\t\tm[a] = struct{}{}\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc validatePinnedCerts(pinned PinnedCertSet) error {\n\tre := regexp.MustCompile(\"^[a-f0-9]{64}$\")\n\tfor certId := range pinned {\n\t\tentry := strings.ToLower(certId)\n\t\tif !re.MatchString(entry) {\n\t\t\treturn fmt.Errorf(\"error parsing 'pinned_certs' key %s does not look like lower case hex-encoded sha256 of DER encoded SubjectPublicKeyInfo\", entry)\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc validateOptions(o *Options) error {\n\tif o.LameDuckDuration > 0 && o.LameDuckGracePeriod >= o.LameDuckDuration {\n\t\treturn fmt.Errorf(\"lame duck grace period (%v) should be strictly lower than lame duck duration (%v)\",\n\t\t\to.LameDuckGracePeriod, o.LameDuckDuration)\n\t}\n\tif int64(o.MaxPayload) > o.MaxPending {\n\t\treturn fmt.Errorf(\"max_payload (%v) cannot be higher than max_pending (%v)\",\n\t\t\to.MaxPayload, o.MaxPending)\n\t}\n\tif o.ServerName != _EMPTY_ && strings.Contains(o.ServerName, \" \") {\n\t\treturn errors.New(\"server name cannot contain spaces\")\n\t}\n\t// Check that the trust configuration is correct.\n\tif err := validateTrustedOperators(o); err != nil {\n\t\treturn err\n\t}\n\t// Check on leaf nodes which will require a system\n\t// account when gateways are also configured.\n\tif err := validateLeafNode(o); err != nil {\n\t\treturn err\n\t}\n\t// Check that authentication is properly configured.\n\tif err := validateAuth(o); err != nil {\n\t\treturn err\n\t}\n\t// Check that gateway is properly configured. Returns no error\n\t// if there is no gateway defined.\n\tif err := validateGatewayOptions(o); err != nil {\n\t\treturn err\n\t}\n\t// Check that cluster name if defined matches any gateway name.\n\tif err := validateCluster(o); err != nil {\n\t\treturn err\n\t}\n\tif err := validateMQTTOptions(o); err != nil {\n\t\treturn err\n\t}\n\tif err := validateJetStreamOptions(o); err != nil {\n\t\treturn err\n\t}\n\t// Finally check websocket options.\n\treturn validateWebsocketOptions(o)\n}\n\nfunc (s *Server) getOpts() *Options {\n\ts.optsMu.RLock()\n\topts := s.opts\n\ts.optsMu.RUnlock()\n\treturn opts\n}\n\nfunc (s *Server) setOpts(opts *Options) {\n\ts.optsMu.Lock()\n\ts.opts = opts\n\ts.optsMu.Unlock()\n}\n\nfunc (s *Server) globalAccount() *Account {\n\ts.mu.RLock()\n\tgacc := s.gacc\n\ts.mu.RUnlock()\n\treturn gacc\n}\n\n// Used to setup or update Accounts.\n// Returns a map that indicates which accounts have had their stream imports\n// changed (in case of an update in configuration reload).\n// Lock is held upon entry, but will be released/reacquired in this function.\nfunc (s *Server) configureAccounts(reloading bool) (map[string]struct{}, error) {\n\tawcsti := make(map[string]struct{})\n\n\t// Create the global account.\n\tif s.gacc == nil {\n\t\ts.gacc = NewAccount(globalAccountName)\n\t\ts.registerAccountNoLock(s.gacc)\n\t}\n\n\topts := s.getOpts()\n\n\t// We need to track service imports since we can not swap them out (unsub and re-sub)\n\t// until the proper server struct accounts have been swapped in properly. Doing it in\n\t// place could lead to data loss or server panic since account under new si has no real\n\t// account and hence no sublist, so will panic on inbound message.\n\tsiMap := make(map[*Account][][]byte)\n\n\t// Check opts and walk through them. We need to copy them here\n\t// so that we do not keep a real one sitting in the options.\n\tfor _, acc := range opts.Accounts {\n\t\tvar a *Account\n\t\tcreate := true\n\t\t// For the global account, we want to skip the reload process\n\t\t// and fall back into the \"create\" case which will in that\n\t\t// case really be just an update (shallowCopy will make sure\n\t\t// that mappings are copied over).\n\t\tif reloading && acc.Name != globalAccountName {\n\t\t\tif ai, ok := s.accounts.Load(acc.Name); ok {\n\t\t\t\ta = ai.(*Account)\n\t\t\t\t// Before updating the account, check if stream imports have changed.\n\t\t\t\tif !a.checkStreamImportsEqual(acc) {\n\t\t\t\t\tawcsti[acc.Name] = struct{}{}\n\t\t\t\t}\n\t\t\t\ta.mu.Lock()\n\t\t\t\t// Collect the sids for the service imports since we are going to\n\t\t\t\t// replace with new ones.\n\t\t\t\tvar sids [][]byte\n\t\t\t\tfor _, sis := range a.imports.services {\n\t\t\t\t\tfor _, si := range sis {\n\t\t\t\t\t\tif si.sid != nil {\n\t\t\t\t\t\t\tsids = append(sids, si.sid)\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\t// Setup to process later if needed.\n\t\t\t\tif len(sids) > 0 || len(acc.imports.services) > 0 {\n\t\t\t\t\tsiMap[a] = sids\n\t\t\t\t}\n\n\t\t\t\t// Now reset all export/imports fields since they are going to be\n\t\t\t\t// filled in shallowCopy()\n\t\t\t\ta.imports.streams, a.imports.services = nil, nil\n\t\t\t\ta.exports.streams, a.exports.services = nil, nil\n\t\t\t\t// We call shallowCopy from the account `acc` (the one in Options)\n\t\t\t\t// and pass `a` (our existing account) to get it updated.\n\t\t\t\tacc.shallowCopy(a)\n\t\t\t\ta.mu.Unlock()\n\t\t\t\tcreate = false\n\t\t\t}\n\t\t}\n\t\t// Track old mappings if global account.\n\t\tvar oldGMappings []*mapping\n\t\tif create {\n\t\t\tif acc.Name == globalAccountName {\n\t\t\t\ta = s.gacc\n\t\t\t\ta.mu.Lock()\n\t\t\t\toldGMappings = append(oldGMappings, a.mappings...)\n\t\t\t\ta.mu.Unlock()\n\t\t\t} else {\n\t\t\t\ta = NewAccount(acc.Name)\n\t\t\t}\n\t\t\t// Locking matters in the case of an update of the global account\n\t\t\ta.mu.Lock()\n\t\t\tacc.shallowCopy(a)\n\t\t\ta.mu.Unlock()\n\t\t\t// Will be a no-op in case of the global account since it is already registered.\n\t\t\ts.registerAccountNoLock(a)\n\t\t}\n\n\t\t// The `acc` account is stored in options, not in the server, and these can be cleared.\n\t\tacc.sl, acc.clients, acc.mappings = nil, nil, nil\n\n\t\t// Check here if we have been reloaded and we have a global account with mappings that may have changed.\n\t\t// If we have leafnodes they need to be updated.\n\t\tif reloading && a == s.gacc {\n\t\t\ta.mu.Lock()\n\t\t\tmappings := make(map[string]*mapping)\n\t\t\tif len(a.mappings) > 0 && a.nleafs > 0 {\n\t\t\t\tfor _, em := range a.mappings {\n\t\t\t\t\tmappings[em.src] = em\n\t\t\t\t}\n\t\t\t}\n\t\t\ta.mu.Unlock()\n\t\t\tif len(mappings) > 0 || len(oldGMappings) > 0 {\n\t\t\t\ta.lmu.RLock()\n\t\t\t\tfor _, lc := range a.lleafs {\n\t\t\t\t\tfor _, em := range mappings {\n\t\t\t\t\t\tlc.forceAddToSmap(em.src)\n\t\t\t\t\t}\n\t\t\t\t\t// Remove any old ones if needed.\n\t\t\t\t\tfor _, em := range oldGMappings {\n\t\t\t\t\t\t// Only remove if not in the new ones.\n\t\t\t\t\t\tif _, ok := mappings[em.src]; !ok {\n\t\t\t\t\t\t\tlc.forceRemoveFromSmap(em.src)\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\ta.lmu.RUnlock()\n\t\t\t}\n\t\t}\n\n\t\t// If we see an account defined using $SYS we will make sure that is set as system account.\n\t\tif acc.Name == DEFAULT_SYSTEM_ACCOUNT && opts.SystemAccount == _EMPTY_ {\n\t\t\topts.SystemAccount = DEFAULT_SYSTEM_ACCOUNT\n\t\t}\n\t}\n\n\t// Now that we have this we need to remap any referenced accounts in\n\t// import or export maps to the new ones.\n\tswapApproved := func(ea *exportAuth) {\n\t\tfor sub, a := range ea.approved {\n\t\t\tvar acc *Account\n\t\t\tif v, ok := s.accounts.Load(a.Name); ok {\n\t\t\t\tacc = v.(*Account)\n\t\t\t}\n\t\t\tea.approved[sub] = acc\n\t\t}\n\t}\n\tvar numAccounts int\n\ts.accounts.Range(func(k, v any) bool {\n\t\tnumAccounts++\n\t\tacc := v.(*Account)\n\t\tacc.mu.Lock()\n\t\t// Exports\n\t\tfor _, se := range acc.exports.streams {\n\t\t\tif se != nil {\n\t\t\t\tswapApproved(&se.exportAuth)\n\t\t\t}\n\t\t}\n\t\tfor _, se := range acc.exports.services {\n\t\t\tif se != nil {\n\t\t\t\t// Swap over the bound account for service exports.\n\t\t\t\tif se.acc != nil {\n\t\t\t\t\tif v, ok := s.accounts.Load(se.acc.Name); ok {\n\t\t\t\t\t\tse.acc = v.(*Account)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tswapApproved(&se.exportAuth)\n\t\t\t}\n\t\t}\n\t\t// Imports\n\t\tfor _, si := range acc.imports.streams {\n\t\t\tif v, ok := s.accounts.Load(si.acc.Name); ok {\n\t\t\t\tsi.acc = v.(*Account)\n\t\t\t}\n\t\t}\n\t\tfor _, sis := range acc.imports.services {\n\t\t\tfor _, si := range sis {\n\t\t\t\tif v, ok := s.accounts.Load(si.acc.Name); ok {\n\t\t\t\t\tsi.acc = v.(*Account)\n\n\t\t\t\t\t// It is possible to allow for latency tracking inside your\n\t\t\t\t\t// own account, so lock only when not the same account.\n\t\t\t\t\tif si.acc == acc {\n\t\t\t\t\t\tsi.se = si.acc.getServiceExport(si.to)\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\t\t\t\t\tsi.acc.mu.RLock()\n\t\t\t\t\tsi.se = si.acc.getServiceExport(si.to)\n\t\t\t\t\tsi.acc.mu.RUnlock()\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t// Make sure the subs are running, but only if not reloading.\n\t\tif len(acc.imports.services) > 0 && acc.ic == nil && !reloading {\n\t\t\tacc.ic = s.createInternalAccountClient()\n\t\t\tacc.ic.acc = acc\n\t\t\t// Need to release locks to invoke this function.\n\t\t\tacc.mu.Unlock()\n\t\t\ts.mu.Unlock()\n\t\t\tacc.addAllServiceImportSubs()\n\t\t\ts.mu.Lock()\n\t\t\tacc.mu.Lock()\n\t\t}\n\t\tacc.updated = time.Now()\n\t\tacc.mu.Unlock()\n\t\treturn true\n\t})\n\n\t// Check if we need to process service imports pending from above.\n\t// This processing needs to be after we swap in the real accounts above.\n\tfor acc, sids := range siMap {\n\t\tc := acc.ic\n\t\tfor _, sid := range sids {\n\t\t\tc.processUnsub(sid)\n\t\t}\n\t\tacc.addAllServiceImportSubs()\n\t\ts.mu.Unlock()\n\t\ts.registerSystemImports(acc)\n\t\ts.mu.Lock()\n\t}\n\n\t// Set the system account if it was configured.\n\t// Otherwise create a default one.\n\tif opts.SystemAccount != _EMPTY_ {\n\t\t// Lock may be acquired in lookupAccount, so release to call lookupAccount.\n\t\ts.mu.Unlock()\n\t\tacc, err := s.lookupAccount(opts.SystemAccount)\n\t\ts.mu.Lock()\n\t\tif err == nil && s.sys != nil && acc != s.sys.account {\n\t\t\t// sys.account.clients (including internal client)/respmap/etc... are transferred separately\n\t\t\ts.sys.account = acc\n\t\t\ts.sysAcc.Store(acc)\n\t\t}\n\t\tif err != nil {\n\t\t\treturn awcsti, fmt.Errorf(\"error resolving system account: %v\", err)\n\t\t}\n\n\t\t// If we have defined a system account here check to see if its just us and the $G account.\n\t\t// We would do this to add user/pass to the system account. If this is the case add in\n\t\t// no-auth-user for $G.\n\t\t// Only do this if non-operator mode and we did not have an authorization block defined.\n\t\tif len(opts.TrustedOperators) == 0 && numAccounts == 2 && opts.NoAuthUser == _EMPTY_ && !opts.authBlockDefined {\n\t\t\t// If we come here from config reload, let's not recreate the fake user name otherwise\n\t\t\t// it will cause currently clients to be disconnected.\n\t\t\tuname := s.sysAccOnlyNoAuthUser\n\t\t\tif uname == _EMPTY_ {\n\t\t\t\t// Create a unique name so we do not collide.\n\t\t\t\tvar b [8]byte\n\t\t\t\trn := rand.Int63()\n\t\t\t\tfor i, l := 0, rn; i < len(b); i++ {\n\t\t\t\t\tb[i] = digits[l%base]\n\t\t\t\t\tl /= base\n\t\t\t\t}\n\t\t\t\tuname = fmt.Sprintf(\"nats-%s\", b[:])\n\t\t\t\ts.sysAccOnlyNoAuthUser = uname\n\t\t\t}\n\t\t\topts.Users = append(opts.Users, &User{Username: uname, Password: uname[6:], Account: s.gacc})\n\t\t\topts.NoAuthUser = uname\n\t\t}\n\t}\n\n\t// Add any required exports from system account.\n\tif s.sys != nil {\n\t\tsysAcc := s.sys.account\n\t\ts.mu.Unlock()\n\t\ts.addSystemAccountExports(sysAcc)\n\t\ts.mu.Lock()\n\t}\n\n\treturn awcsti, nil\n}\n\n// Setup the account resolver. For memory resolver, make sure the JWTs are\n// properly formed but do not enforce expiration etc.\n// Lock is held on entry, but may be released/reacquired during this call.\nfunc (s *Server) configureResolver() error {\n\topts := s.getOpts()\n\ts.accResolver = opts.AccountResolver\n\tif opts.AccountResolver != nil {\n\t\t// For URL resolver, set the TLSConfig if specified.\n\t\tif opts.AccountResolverTLSConfig != nil {\n\t\t\tif ar, ok := opts.AccountResolver.(*URLAccResolver); ok {\n\t\t\t\tif t, ok := ar.c.Transport.(*http.Transport); ok {\n\t\t\t\t\tt.CloseIdleConnections()\n\t\t\t\t\tt.TLSClientConfig = opts.AccountResolverTLSConfig.Clone()\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tif len(opts.resolverPreloads) > 0 {\n\t\t\t// Lock ordering is account resolver -> server, so we need to release\n\t\t\t// the lock and reacquire it when done with account resolver's calls.\n\t\t\tar := s.accResolver\n\t\t\ts.mu.Unlock()\n\t\t\tdefer s.mu.Lock()\n\t\t\tif ar.IsReadOnly() {\n\t\t\t\treturn fmt.Errorf(\"resolver preloads only available for writeable resolver types MEM/DIR/CACHE_DIR\")\n\t\t\t}\n\t\t\tfor k, v := range opts.resolverPreloads {\n\t\t\t\t_, err := jwt.DecodeAccountClaims(v)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn fmt.Errorf(\"preload account error for %q: %v\", k, err)\n\t\t\t\t}\n\t\t\t\tar.Store(k, v)\n\t\t\t}\n\t\t}\n\t}\n\treturn nil\n}\n\n// This will check preloads for validation issues.\nfunc (s *Server) checkResolvePreloads() {\n\topts := s.getOpts()\n\t// We can just check the read-only opts versions here, that way we do not need\n\t// to grab server lock or access s.accResolver.\n\tfor k, v := range opts.resolverPreloads {\n\t\tclaims, err := jwt.DecodeAccountClaims(v)\n\t\tif err != nil {\n\t\t\ts.Errorf(\"Preloaded account [%s] not valid\", k)\n\t\t\tcontinue\n\t\t}\n\t\t// Check if it is expired.\n\t\tvr := jwt.CreateValidationResults()\n\t\tclaims.Validate(vr)\n\t\tif vr.IsBlocking(true) {\n\t\t\ts.Warnf(\"Account [%s] has validation issues:\", k)\n\t\t\tfor _, v := range vr.Issues {\n\t\t\t\ts.Warnf(\"  - %s\", v.Description)\n\t\t\t}\n\t\t}\n\t}\n}\n\n// Determines if we are in pre NATS 2.0 setup with no accounts.\nfunc (s *Server) globalAccountOnly() bool {\n\tvar hasOthers bool\n\n\tif s.trustedKeys != nil {\n\t\treturn false\n\t}\n\n\ts.mu.RLock()\n\ts.accounts.Range(func(k, v any) bool {\n\t\tacc := v.(*Account)\n\t\t// Ignore global and system\n\t\tif acc == s.gacc || (s.sys != nil && acc == s.sys.account) {\n\t\t\treturn true\n\t\t}\n\t\thasOthers = true\n\t\treturn false\n\t})\n\ts.mu.RUnlock()\n\n\treturn !hasOthers\n}\n\n// Determines if this server is in standalone mode, meaning no routes or gateways.\nfunc (s *Server) standAloneMode() bool {\n\topts := s.getOpts()\n\treturn opts.Cluster.Port == 0 && opts.Gateway.Port == 0\n}\n\nfunc (s *Server) configuredRoutes() int {\n\treturn len(s.getOpts().Routes)\n}\n\n// activePeers is used in bootstrapping raft groups like the JetStream meta controller.\nfunc (s *Server) ActivePeers() (peers []string) {\n\ts.nodeToInfo.Range(func(k, v any) bool {\n\t\tsi := v.(nodeInfo)\n\t\tif !si.offline {\n\t\t\tpeers = append(peers, k.(string))\n\t\t}\n\t\treturn true\n\t})\n\treturn peers\n}\n\n// isTrustedIssuer will check that the issuer is a trusted public key.\n// This is used to make sure an account was signed by a trusted operator.\nfunc (s *Server) isTrustedIssuer(issuer string) bool {\n\ts.mu.RLock()\n\tdefer s.mu.RUnlock()\n\t// If we are not running in trusted mode and there is no issuer, that is ok.\n\tif s.trustedKeys == nil && issuer == _EMPTY_ {\n\t\treturn true\n\t}\n\tfor _, tk := range s.trustedKeys {\n\t\tif tk == issuer {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\n// processTrustedKeys will process binary stamped and\n// options-based trusted nkeys. Returns success.\nfunc (s *Server) processTrustedKeys() bool {\n\ts.strictSigningKeyUsage = map[string]struct{}{}\n\topts := s.getOpts()\n\tif trustedKeys != _EMPTY_ && !s.initStampedTrustedKeys() {\n\t\treturn false\n\t} else if opts.TrustedKeys != nil {\n\t\tfor _, key := range opts.TrustedKeys {\n\t\t\tif !nkeys.IsValidPublicOperatorKey(key) {\n\t\t\t\treturn false\n\t\t\t}\n\t\t}\n\t\ts.trustedKeys = append([]string(nil), opts.TrustedKeys...)\n\t\tfor _, claim := range opts.TrustedOperators {\n\t\t\tif !claim.StrictSigningKeyUsage {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tfor _, key := range claim.SigningKeys {\n\t\t\t\ts.strictSigningKeyUsage[key] = struct{}{}\n\t\t\t}\n\t\t}\n\t}\n\treturn true\n}\n\n// checkTrustedKeyString will check that the string is a valid array\n// of public operator nkeys.\nfunc checkTrustedKeyString(keys string) []string {\n\ttks := strings.Fields(keys)\n\tif len(tks) == 0 {\n\t\treturn nil\n\t}\n\t// Walk all the keys and make sure they are valid.\n\tfor _, key := range tks {\n\t\tif !nkeys.IsValidPublicOperatorKey(key) {\n\t\t\treturn nil\n\t\t}\n\t}\n\treturn tks\n}\n\n// initStampedTrustedKeys will check the stamped trusted keys\n// and will set the server field 'trustedKeys'. Returns whether\n// it succeeded or not.\nfunc (s *Server) initStampedTrustedKeys() bool {\n\t// Check to see if we have an override in options, which will cause us to fail.\n\tif len(s.getOpts().TrustedKeys) > 0 {\n\t\treturn false\n\t}\n\ttks := checkTrustedKeyString(trustedKeys)\n\tif len(tks) == 0 {\n\t\treturn false\n\t}\n\ts.trustedKeys = tks\n\treturn true\n}\n\n// PrintAndDie is exported for access in other packages.\nfunc PrintAndDie(msg string) {\n\tfmt.Fprintln(os.Stderr, msg)\n\tos.Exit(1)\n}\n\n// PrintServerAndExit will print our version and exit.\nfunc PrintServerAndExit() {\n\tfmt.Printf(\"nats-server: v%s\\n\", VERSION)\n\tos.Exit(0)\n}\n\n// ProcessCommandLineArgs takes the command line arguments\n// validating and setting flags for handling in case any\n// sub command was present.\nfunc ProcessCommandLineArgs(cmd *flag.FlagSet) (showVersion bool, showHelp bool, err error) {\n\tif len(cmd.Args()) > 0 {\n\t\targ := cmd.Args()[0]\n\t\tswitch strings.ToLower(arg) {\n\t\tcase \"version\":\n\t\t\treturn true, false, nil\n\t\tcase \"help\":\n\t\t\treturn false, true, nil\n\t\tdefault:\n\t\t\treturn false, false, fmt.Errorf(\"unrecognized command: %q\", arg)\n\t\t}\n\t}\n\n\treturn false, false, nil\n}\n\n// Public version.\nfunc (s *Server) Running() bool {\n\treturn s.isRunning()\n}\n\n// Protected check on running state\nfunc (s *Server) isRunning() bool {\n\treturn s.running.Load()\n}\n\nfunc (s *Server) logPid() error {\n\tpidStr := strconv.Itoa(os.Getpid())\n\treturn os.WriteFile(s.getOpts().PidFile, []byte(pidStr), defaultFilePerms)\n}\n\n// numReservedAccounts will return the number of reserved accounts configured in the server.\n// Currently this is 1, one for the global default account.\nfunc (s *Server) numReservedAccounts() int {\n\treturn 1\n}\n\n// NumActiveAccounts reports number of active accounts on this server.\nfunc (s *Server) NumActiveAccounts() int32 {\n\treturn atomic.LoadInt32(&s.activeAccounts)\n}\n\n// incActiveAccounts() just adds one under lock.\nfunc (s *Server) incActiveAccounts() {\n\tatomic.AddInt32(&s.activeAccounts, 1)\n}\n\n// decActiveAccounts() just subtracts one under lock.\nfunc (s *Server) decActiveAccounts() {\n\tatomic.AddInt32(&s.activeAccounts, -1)\n}\n\n// This should be used for testing only. Will be slow since we have to\n// range over all accounts in the sync.Map to count.\nfunc (s *Server) numAccounts() int {\n\tcount := 0\n\ts.mu.RLock()\n\ts.accounts.Range(func(k, v any) bool {\n\t\tcount++\n\t\treturn true\n\t})\n\ts.mu.RUnlock()\n\treturn count\n}\n\n// NumLoadedAccounts returns the number of loaded accounts.\nfunc (s *Server) NumLoadedAccounts() int {\n\treturn s.numAccounts()\n}\n\n// LookupOrRegisterAccount will return the given account if known or create a new entry.\nfunc (s *Server) LookupOrRegisterAccount(name string) (account *Account, isNew bool) {\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\tif v, ok := s.accounts.Load(name); ok {\n\t\treturn v.(*Account), false\n\t}\n\tacc := NewAccount(name)\n\ts.registerAccountNoLock(acc)\n\treturn acc, true\n}\n\n// RegisterAccount will register an account. The account must be new\n// or this call will fail.\nfunc (s *Server) RegisterAccount(name string) (*Account, error) {\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\tif _, ok := s.accounts.Load(name); ok {\n\t\treturn nil, ErrAccountExists\n\t}\n\tacc := NewAccount(name)\n\ts.registerAccountNoLock(acc)\n\treturn acc, nil\n}\n\n// SetSystemAccount will set the internal system account.\n// If root operators are present it will also check validity.\nfunc (s *Server) SetSystemAccount(accName string) error {\n\t// Lookup from sync.Map first.\n\tif v, ok := s.accounts.Load(accName); ok {\n\t\treturn s.setSystemAccount(v.(*Account))\n\t}\n\n\t// If we are here we do not have local knowledge of this account.\n\t// Do this one by hand to return more useful error.\n\tac, jwt, err := s.fetchAccountClaims(accName)\n\tif err != nil {\n\t\treturn err\n\t}\n\tacc := s.buildInternalAccount(ac)\n\tacc.claimJWT = jwt\n\t// Due to race, we need to make sure that we are not\n\t// registering twice.\n\tif racc := s.registerAccount(acc); racc != nil {\n\t\treturn nil\n\t}\n\treturn s.setSystemAccount(acc)\n}\n\n// SystemAccount returns the system account if set.\nfunc (s *Server) SystemAccount() *Account {\n\treturn s.sysAcc.Load()\n}\n\n// GlobalAccount returns the global account.\n// Default clients will use the global account.\nfunc (s *Server) GlobalAccount() *Account {\n\ts.mu.RLock()\n\tdefer s.mu.RUnlock()\n\treturn s.gacc\n}\n\n// SetDefaultSystemAccount will create a default system account if one is not present.\nfunc (s *Server) SetDefaultSystemAccount() error {\n\tif _, isNew := s.LookupOrRegisterAccount(DEFAULT_SYSTEM_ACCOUNT); !isNew {\n\t\treturn nil\n\t}\n\ts.Debugf(\"Created system account: %q\", DEFAULT_SYSTEM_ACCOUNT)\n\treturn s.SetSystemAccount(DEFAULT_SYSTEM_ACCOUNT)\n}\n\n// Assign a system account. Should only be called once.\n// This sets up a server to send and receive messages from\n// inside the server itself.\nfunc (s *Server) setSystemAccount(acc *Account) error {\n\tif acc == nil {\n\t\treturn ErrMissingAccount\n\t}\n\t// Don't try to fix this here.\n\tif acc.IsExpired() {\n\t\treturn ErrAccountExpired\n\t}\n\t// If we are running with trusted keys for an operator\n\t// make sure we check the account is legit.\n\tif !s.isTrustedIssuer(acc.Issuer) {\n\t\treturn ErrAccountValidation\n\t}\n\n\ts.mu.Lock()\n\n\tif s.sys != nil {\n\t\ts.mu.Unlock()\n\t\treturn ErrAccountExists\n\t}\n\n\t// This is here in an attempt to quiet the race detector and not have to place\n\t// locks on fast path for inbound messages and checking service imports.\n\tacc.mu.Lock()\n\tif acc.imports.services == nil {\n\t\tacc.imports.services = make(map[string][]*serviceImport)\n\t}\n\tacc.mu.Unlock()\n\n\ts.sys = &internal{\n\t\taccount: acc,\n\t\tclient:  s.createInternalSystemClient(),\n\t\tseq:     1,\n\t\tsid:     1,\n\t\tservers: make(map[string]*serverUpdate),\n\t\treplies: make(map[string]msgHandler),\n\t\tsendq:   newIPQueue[*pubMsg](s, \"System sendQ\"),\n\t\trecvq:   newIPQueue[*inSysMsg](s, \"System recvQ\"),\n\t\trecvqp:  newIPQueue[*inSysMsg](s, \"System recvQ Pings\"),\n\t\tresetCh: make(chan struct{}),\n\t\tsq:      s.newSendQ(acc),\n\t\tstatsz:  statsHBInterval,\n\t\torphMax: 5 * eventsHBInterval,\n\t\tchkOrph: 3 * eventsHBInterval,\n\t}\n\trecvq, recvqp := s.sys.recvq, s.sys.recvqp\n\ts.sys.wg.Add(1)\n\ts.mu.Unlock()\n\n\t// Store in atomic for fast lookup.\n\ts.sysAcc.Store(acc)\n\n\t// Register with the account.\n\ts.sys.client.registerWithAccount(acc)\n\n\ts.addSystemAccountExports(acc)\n\n\t// Start our internal loop to serialize outbound messages.\n\t// We do our own wg here since we will stop first during shutdown.\n\tgo s.internalSendLoop(&s.sys.wg)\n\n\t// Start the internal loop for inbound messages.\n\tgo s.internalReceiveLoop(recvq)\n\t// Start the internal loop for inbound STATSZ/Ping messages.\n\tgo s.internalReceiveLoop(recvqp)\n\n\t// Start up our general subscriptions\n\ts.initEventTracking()\n\n\t// Track for dead remote servers.\n\ts.wrapChk(s.startRemoteServerSweepTimer)()\n\n\t// Send out statsz updates periodically.\n\ts.wrapChk(s.startStatszTimer)()\n\n\t// If we have existing accounts make sure we enable account tracking.\n\ts.mu.Lock()\n\ts.accounts.Range(func(k, v any) bool {\n\t\tacc := v.(*Account)\n\t\ts.enableAccountTracking(acc)\n\t\treturn true\n\t})\n\ts.mu.Unlock()\n\n\treturn nil\n}\n\n// Creates an internal system client.\nfunc (s *Server) createInternalSystemClient() *client {\n\treturn s.createInternalClient(SYSTEM)\n}\n\n// Creates an internal jetstream client.\nfunc (s *Server) createInternalJetStreamClient() *client {\n\treturn s.createInternalClient(JETSTREAM)\n}\n\n// Creates an internal client for Account.\nfunc (s *Server) createInternalAccountClient() *client {\n\treturn s.createInternalClient(ACCOUNT)\n}\n\n// Internal clients. kind should be SYSTEM, JETSTREAM or ACCOUNT\nfunc (s *Server) createInternalClient(kind int) *client {\n\tif !isInternalClient(kind) {\n\t\treturn nil\n\t}\n\tnow := time.Now()\n\tc := &client{srv: s, kind: kind, opts: internalOpts, msubs: -1, mpay: -1, start: now, last: now}\n\tc.initClient()\n\tc.echo = false\n\tc.headers = true\n\tc.flags.set(noReconnect)\n\treturn c\n}\n\n// Determine if accounts should track subscriptions for\n// efficient propagation.\n// Lock should be held on entry.\nfunc (s *Server) shouldTrackSubscriptions() bool {\n\topts := s.getOpts()\n\treturn (opts.Cluster.Port != 0 || opts.Gateway.Port != 0)\n}\n\n// Invokes registerAccountNoLock under the protection of the server lock.\n// That is, server lock is acquired/released in this function.\n// See registerAccountNoLock for comment on returned value.\nfunc (s *Server) registerAccount(acc *Account) *Account {\n\ts.mu.Lock()\n\tracc := s.registerAccountNoLock(acc)\n\ts.mu.Unlock()\n\treturn racc\n}\n\n// Helper to set the sublist based on preferences.\nfunc (s *Server) setAccountSublist(acc *Account) {\n\tif acc != nil && acc.sl == nil {\n\t\topts := s.getOpts()\n\t\tif opts != nil && opts.NoSublistCache {\n\t\t\tacc.sl = NewSublistNoCache()\n\t\t} else {\n\t\t\tacc.sl = NewSublistWithCache()\n\t\t}\n\t}\n}\n\n// Registers an account in the server.\n// Due to some locking considerations, we may end-up trying\n// to register the same account twice. This function will\n// then return the already registered account.\n// Lock should be held on entry.\nfunc (s *Server) registerAccountNoLock(acc *Account) *Account {\n\t// We are under the server lock. Lookup from map, if present\n\t// return existing account.\n\tif a, _ := s.accounts.Load(acc.Name); a != nil {\n\t\ts.tmpAccounts.Delete(acc.Name)\n\t\treturn a.(*Account)\n\t}\n\t// Finish account setup and store.\n\ts.setAccountSublist(acc)\n\n\tacc.mu.Lock()\n\ts.setRouteInfo(acc)\n\tif acc.clients == nil {\n\t\tacc.clients = make(map[*client]struct{})\n\t}\n\n\t// If we are capable of routing we will track subscription\n\t// information for efficient interest propagation.\n\t// During config reload, it is possible that account was\n\t// already created (global account), so use locking and\n\t// make sure we create only if needed.\n\t// TODO(dlc)- Double check that we need this for GWs.\n\tif acc.rm == nil && s.opts != nil && s.shouldTrackSubscriptions() {\n\t\tacc.rm = make(map[string]int32)\n\t\tacc.lqws = make(map[string]int32)\n\t}\n\tacc.srv = s\n\tacc.updated = time.Now()\n\taccName := acc.Name\n\tjsEnabled := len(acc.jsLimits) > 0\n\tacc.mu.Unlock()\n\n\tif opts := s.getOpts(); opts != nil && len(opts.JsAccDefaultDomain) > 0 {\n\t\tif defDomain, ok := opts.JsAccDefaultDomain[accName]; ok {\n\t\t\tif jsEnabled {\n\t\t\t\ts.Warnf(\"Skipping Default Domain %q, set for JetStream enabled account %q\", defDomain, accName)\n\t\t\t} else if defDomain != _EMPTY_ {\n\t\t\t\tfor src, dest := range generateJSMappingTable(defDomain) {\n\t\t\t\t\t// flip src and dest around so the domain is inserted\n\t\t\t\t\ts.Noticef(\"Adding default domain mapping %q -> %q to account %q %p\", dest, src, accName, acc)\n\t\t\t\t\tif err := acc.AddMapping(dest, src); err != nil {\n\t\t\t\t\t\ts.Errorf(\"Error adding JetStream default domain mapping: %v\", err)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\ts.accounts.Store(acc.Name, acc)\n\ts.tmpAccounts.Delete(acc.Name)\n\ts.enableAccountTracking(acc)\n\n\t// Can not have server lock here.\n\ts.mu.Unlock()\n\ts.registerSystemImports(acc)\n\t// Starting 2.9.0, we are phasing out the optimistic mode, so change\n\t// the account to interest-only mode (except if instructed not to do\n\t// it in some tests).\n\tif s.gateway.enabled && !gwDoNotForceInterestOnlyMode {\n\t\ts.switchAccountToInterestMode(acc.GetName())\n\t}\n\ts.mu.Lock()\n\n\treturn nil\n}\n\n// Sets the account's routePoolIdx depending on presence or not of\n// pooling or per-account routes. Also updates a map used by\n// gateway code to retrieve a route based on some route hash.\n//\n// Both Server and Account lock held on entry.\nfunc (s *Server) setRouteInfo(acc *Account) {\n\t// If there is a dedicated route configured for this account\n\tif _, ok := s.accRoutes[acc.Name]; ok {\n\t\t// We want the account name to be in the map, but we don't\n\t\t// need a value (we could store empty string)\n\t\ts.accRouteByHash.Store(acc.Name, nil)\n\t\t// Set the route pool index to -1 so that it is easy when\n\t\t// ranging over accounts to exclude those accounts when\n\t\t// trying to get accounts for a given pool index.\n\t\tacc.routePoolIdx = accDedicatedRoute\n\t} else {\n\t\t// If pool size more than 1, we will compute a hash code and\n\t\t// use modulo to assign to an index of the pool slice. For 1\n\t\t// and below, all accounts will be bound to the single connection\n\t\t// at index 0.\n\t\tacc.routePoolIdx = computeRoutePoolIdx(s.routesPoolSize, acc.Name)\n\t\tif s.routesPoolSize > 1 {\n\t\t\ts.accRouteByHash.Store(acc.Name, acc.routePoolIdx)\n\t\t}\n\t}\n}\n\n// lookupAccount is a function to return the account structure\n// associated with an account name.\n// Lock MUST NOT be held upon entry.\nfunc (s *Server) lookupAccount(name string) (*Account, error) {\n\tvar acc *Account\n\tif v, ok := s.accounts.Load(name); ok {\n\t\tacc = v.(*Account)\n\t}\n\tif acc != nil {\n\t\t// If we are expired and we have a resolver, then\n\t\t// return the latest information from the resolver.\n\t\tif acc.IsExpired() {\n\t\t\ts.Debugf(\"Requested account [%s] has expired\", name)\n\t\t\tif s.AccountResolver() != nil {\n\t\t\t\tif err := s.updateAccount(acc); err != nil {\n\t\t\t\t\t// This error could mask expired, so just return expired here.\n\t\t\t\t\treturn nil, ErrAccountExpired\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\treturn nil, ErrAccountExpired\n\t\t\t}\n\t\t}\n\t\treturn acc, nil\n\t}\n\t// If we have a resolver see if it can fetch the account.\n\tif s.AccountResolver() == nil {\n\t\treturn nil, ErrMissingAccount\n\t}\n\treturn s.fetchAccount(name)\n}\n\n// LookupAccount is a public function to return the account structure\n// associated with name.\nfunc (s *Server) LookupAccount(name string) (*Account, error) {\n\treturn s.lookupAccount(name)\n}\n\n// This will fetch new claims and if found update the account with new claims.\n// Lock MUST NOT be held upon entry.\nfunc (s *Server) updateAccount(acc *Account) error {\n\tacc.mu.RLock()\n\t// TODO(dlc) - Make configurable\n\tif !acc.incomplete && time.Since(acc.updated) < time.Second {\n\t\tacc.mu.RUnlock()\n\t\ts.Debugf(\"Requested account update for [%s] ignored, too soon\", acc.Name)\n\t\treturn ErrAccountResolverUpdateTooSoon\n\t}\n\tacc.mu.RUnlock()\n\tclaimJWT, err := s.fetchRawAccountClaims(acc.Name)\n\tif err != nil {\n\t\treturn err\n\t}\n\treturn s.updateAccountWithClaimJWT(acc, claimJWT)\n}\n\n// updateAccountWithClaimJWT will check and apply the claim update.\n// Lock MUST NOT be held upon entry.\nfunc (s *Server) updateAccountWithClaimJWT(acc *Account, claimJWT string) error {\n\tif acc == nil {\n\t\treturn ErrMissingAccount\n\t}\n\tacc.mu.RLock()\n\tsameClaim := acc.claimJWT != _EMPTY_ && acc.claimJWT == claimJWT && !acc.incomplete\n\tacc.mu.RUnlock()\n\tif sameClaim {\n\t\ts.Debugf(\"Requested account update for [%s], same claims detected\", acc.Name)\n\t\treturn nil\n\t}\n\taccClaims, _, err := s.verifyAccountClaims(claimJWT)\n\tif err == nil && accClaims != nil {\n\t\tacc.mu.Lock()\n\t\t// if an account is updated with a different operator signing key, we want to\n\t\t// show a consistent issuer.\n\t\tacc.Issuer = accClaims.Issuer\n\t\tif acc.Name != accClaims.Subject {\n\t\t\tacc.mu.Unlock()\n\t\t\treturn ErrAccountValidation\n\t\t}\n\t\tacc.mu.Unlock()\n\t\ts.UpdateAccountClaims(acc, accClaims)\n\t\tacc.mu.Lock()\n\t\t// needs to be set after update completed.\n\t\t// This causes concurrent calls to return with sameClaim=true if the change is effective.\n\t\tacc.claimJWT = claimJWT\n\t\tacc.mu.Unlock()\n\t\treturn nil\n\t}\n\treturn err\n}\n\n// fetchRawAccountClaims will grab raw account claims iff we have a resolver.\n// Lock is NOT held upon entry.\nfunc (s *Server) fetchRawAccountClaims(name string) (string, error) {\n\taccResolver := s.AccountResolver()\n\tif accResolver == nil {\n\t\treturn _EMPTY_, ErrNoAccountResolver\n\t}\n\t// Need to do actual Fetch\n\tstart := time.Now()\n\tclaimJWT, err := fetchAccount(accResolver, name)\n\tfetchTime := time.Since(start)\n\tif fetchTime > time.Second {\n\t\ts.Warnf(\"Account [%s] fetch took %v\", name, fetchTime)\n\t} else {\n\t\ts.Debugf(\"Account [%s] fetch took %v\", name, fetchTime)\n\t}\n\tif err != nil {\n\t\ts.Warnf(\"Account fetch failed: %v\", err)\n\t\treturn \"\", err\n\t}\n\treturn claimJWT, nil\n}\n\n// fetchAccountClaims will attempt to fetch new claims if a resolver is present.\n// Lock is NOT held upon entry.\nfunc (s *Server) fetchAccountClaims(name string) (*jwt.AccountClaims, string, error) {\n\tclaimJWT, err := s.fetchRawAccountClaims(name)\n\tif err != nil {\n\t\treturn nil, _EMPTY_, err\n\t}\n\tvar claim *jwt.AccountClaims\n\tclaim, claimJWT, err = s.verifyAccountClaims(claimJWT)\n\tif claim != nil && claim.Subject != name {\n\t\treturn nil, _EMPTY_, ErrAccountValidation\n\t}\n\treturn claim, claimJWT, err\n}\n\n// verifyAccountClaims will decode and validate any account claims.\nfunc (s *Server) verifyAccountClaims(claimJWT string) (*jwt.AccountClaims, string, error) {\n\taccClaims, err := jwt.DecodeAccountClaims(claimJWT)\n\tif err != nil {\n\t\treturn nil, _EMPTY_, err\n\t}\n\tif !s.isTrustedIssuer(accClaims.Issuer) {\n\t\treturn nil, _EMPTY_, ErrAccountValidation\n\t}\n\tvr := jwt.CreateValidationResults()\n\taccClaims.Validate(vr)\n\tif vr.IsBlocking(true) {\n\t\treturn nil, _EMPTY_, ErrAccountValidation\n\t}\n\treturn accClaims, claimJWT, nil\n}\n\n// This will fetch an account from a resolver if defined.\n// Lock is NOT held upon entry.\nfunc (s *Server) fetchAccount(name string) (*Account, error) {\n\taccClaims, claimJWT, err := s.fetchAccountClaims(name)\n\tif accClaims == nil {\n\t\treturn nil, err\n\t}\n\tacc := s.buildInternalAccount(accClaims)\n\t// Due to possible race, if registerAccount() returns a non\n\t// nil account, it means the same account was already\n\t// registered and we should use this one.\n\tif racc := s.registerAccount(acc); racc != nil {\n\t\t// Update with the new claims in case they are new.\n\t\tif err = s.updateAccountWithClaimJWT(racc, claimJWT); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn racc, nil\n\t}\n\t// The sub imports may have been setup but will not have had their\n\t// subscriptions properly setup. Do that here.\n\tvar needImportSubs bool\n\n\tacc.mu.Lock()\n\tacc.claimJWT = claimJWT\n\tif len(acc.imports.services) > 0 {\n\t\tif acc.ic == nil {\n\t\t\tacc.ic = s.createInternalAccountClient()\n\t\t\tacc.ic.acc = acc\n\t\t}\n\t\tneedImportSubs = true\n\t}\n\tacc.mu.Unlock()\n\n\t// Do these outside the lock.\n\tif needImportSubs {\n\t\tacc.addAllServiceImportSubs()\n\t}\n\n\treturn acc, nil\n}\n\n// Start up the server, this will not block.\n//\n// WaitForShutdown can be used to block and wait for the server to shutdown properly if needed\n// after calling s.Shutdown()\nfunc (s *Server) Start() {\n\ts.Noticef(\"Starting nats-server\")\n\n\tgc := gitCommit\n\tif gc == _EMPTY_ {\n\t\tgc = \"not set\"\n\t}\n\n\t// Snapshot server options.\n\topts := s.getOpts()\n\n\t// Capture if this server is a leaf that has no cluster, so we don't\n\t// display the cluster name if that is the case.\n\ts.mu.RLock()\n\tleafNoCluster := s.leafNoCluster\n\ts.mu.RUnlock()\n\n\tvar clusterName string\n\tif !leafNoCluster {\n\t\tclusterName = s.ClusterName()\n\t}\n\n\ts.Noticef(\"  Version:  %s\", VERSION)\n\ts.Noticef(\"  Git:      [%s]\", gc)\n\ts.Debugf(\"  Go build: %s\", s.info.GoVersion)\n\tif clusterName != _EMPTY_ {\n\t\ts.Noticef(\"  Cluster:  %s\", clusterName)\n\t}\n\ts.Noticef(\"  Name:     %s\", s.info.Name)\n\tif opts.JetStream {\n\t\ts.Noticef(\"  Node:     %s\", getHash(s.info.Name))\n\t}\n\ts.Noticef(\"  ID:       %s\", s.info.ID)\n\n\tdefer s.Noticef(\"Server is ready\")\n\n\t// Check for insecure configurations.\n\ts.checkAuthforWarnings()\n\n\t// Avoid RACE between Start() and Shutdown()\n\ts.running.Store(true)\n\ts.mu.Lock()\n\t// Update leafNodeEnabled in case options have changed post NewServer()\n\t// and before Start() (we should not be able to allow that, but server has\n\t// direct reference to user-provided options - at least before a Reload() is\n\t// performed.\n\ts.leafNodeEnabled = opts.LeafNode.Port != 0 || len(opts.LeafNode.Remotes) > 0\n\ts.mu.Unlock()\n\n\ts.grMu.Lock()\n\ts.grRunning = true\n\ts.grMu.Unlock()\n\n\ts.startRateLimitLogExpiration()\n\n\t// Pprof http endpoint for the profiler.\n\tif opts.ProfPort != 0 {\n\t\ts.StartProfiler()\n\t} else {\n\t\t// It's still possible to access this profile via a SYS endpoint, so set\n\t\t// this anyway. (Otherwise StartProfiler would have called it.)\n\t\ts.setBlockProfileRate(opts.ProfBlockRate)\n\t}\n\n\tif opts.ConfigFile != _EMPTY_ {\n\t\tvar cd string\n\t\tif opts.configDigest != \"\" {\n\t\t\tcd = fmt.Sprintf(\"(%s)\", opts.configDigest)\n\t\t}\n\t\ts.Noticef(\"Using configuration file: %s %s\", opts.ConfigFile, cd)\n\t}\n\n\thasOperators := len(opts.TrustedOperators) > 0\n\tif hasOperators {\n\t\ts.Noticef(\"Trusted Operators\")\n\t}\n\tfor _, opc := range opts.TrustedOperators {\n\t\ts.Noticef(\"  System  : %q\", opc.Audience)\n\t\ts.Noticef(\"  Operator: %q\", opc.Name)\n\t\ts.Noticef(\"  Issued  : %v\", time.Unix(opc.IssuedAt, 0))\n\t\tswitch opc.Expires {\n\t\tcase 0:\n\t\t\ts.Noticef(\"  Expires : Never\")\n\t\tdefault:\n\t\t\ts.Noticef(\"  Expires : %v\", time.Unix(opc.Expires, 0))\n\t\t}\n\t}\n\tif hasOperators && opts.SystemAccount == _EMPTY_ {\n\t\ts.Warnf(\"Trusted Operators should utilize a System Account\")\n\t}\n\tif opts.MaxPayload > MAX_PAYLOAD_MAX_SIZE {\n\t\ts.Warnf(\"Maximum payloads over %v are generally discouraged and could lead to poor performance\",\n\t\t\tfriendlyBytes(int64(MAX_PAYLOAD_MAX_SIZE)))\n\t}\n\n\tif len(opts.JsAccDefaultDomain) > 0 {\n\t\ts.Warnf(\"The option `default_js_domain` is a temporary backwards compatibility measure and will be removed\")\n\t}\n\n\t// If we have a memory resolver, check the accounts here for validation exceptions.\n\t// This allows them to be logged right away vs when they are accessed via a client.\n\tif hasOperators && len(opts.resolverPreloads) > 0 {\n\t\ts.checkResolvePreloads()\n\t}\n\n\t// Log the pid to a file.\n\tif opts.PidFile != _EMPTY_ {\n\t\tif err := s.logPid(); err != nil {\n\t\t\ts.Fatalf(\"Could not write pidfile: %v\", err)\n\t\t\treturn\n\t\t}\n\t}\n\n\t// Setup system account which will start the eventing stack.\n\tif sa := opts.SystemAccount; sa != _EMPTY_ {\n\t\tif err := s.SetSystemAccount(sa); err != nil {\n\t\t\ts.Fatalf(\"Can't set system account: %v\", err)\n\t\t\treturn\n\t\t}\n\t} else if !opts.NoSystemAccount {\n\t\t// We will create a default system account here.\n\t\ts.SetDefaultSystemAccount()\n\t}\n\n\t// Start monitoring before enabling other subsystems of the\n\t// server to be able to monitor during startup.\n\tif err := s.StartMonitoring(); err != nil {\n\t\ts.Fatalf(\"Can't start monitoring: %v\", err)\n\t\treturn\n\t}\n\n\t// Start up resolver machinery.\n\tif ar := s.AccountResolver(); ar != nil {\n\t\tif err := ar.Start(s); err != nil {\n\t\t\ts.Fatalf(\"Could not start resolver: %v\", err)\n\t\t\treturn\n\t\t}\n\t\t// In operator mode, when the account resolver depends on an external system and\n\t\t// the system account is the bootstrapping account, start fetching it.\n\t\tif len(opts.TrustedOperators) == 1 && opts.SystemAccount != _EMPTY_ && opts.SystemAccount != DEFAULT_SYSTEM_ACCOUNT {\n\t\t\topts := s.getOpts()\n\t\t\t_, isMemResolver := ar.(*MemAccResolver)\n\t\t\tif v, ok := s.accounts.Load(opts.SystemAccount); !isMemResolver && ok && v.(*Account).claimJWT == _EMPTY_ {\n\t\t\t\ts.Noticef(\"Using bootstrapping system account\")\n\t\t\t\ts.startGoRoutine(func() {\n\t\t\t\t\tdefer s.grWG.Done()\n\t\t\t\t\tt := time.NewTicker(time.Second)\n\t\t\t\t\tdefer t.Stop()\n\t\t\t\t\tfor {\n\t\t\t\t\t\tselect {\n\t\t\t\t\t\tcase <-s.quitCh:\n\t\t\t\t\t\t\treturn\n\t\t\t\t\t\tcase <-t.C:\n\t\t\t\t\t\t\tsacc := s.SystemAccount()\n\t\t\t\t\t\t\tif claimJWT, err := fetchAccount(ar, opts.SystemAccount); err != nil {\n\t\t\t\t\t\t\t\tcontinue\n\t\t\t\t\t\t\t} else if err = s.updateAccountWithClaimJWT(sacc, claimJWT); err != nil {\n\t\t\t\t\t\t\t\tcontinue\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\ts.Noticef(\"System account fetched and updated\")\n\t\t\t\t\t\t\treturn\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t})\n\t\t\t}\n\t\t}\n\t}\n\n\t// Start expiration of mapped GW replies, regardless if\n\t// this server is configured with gateway or not.\n\ts.startGWReplyMapExpiration()\n\n\t// Check if JetStream has been enabled. This needs to be after\n\t// the system account setup above. JetStream will create its\n\t// own system account if one is not present.\n\tif opts.JetStream {\n\t\t// Make sure someone is not trying to enable on the system account.\n\t\tif sa := s.SystemAccount(); sa != nil && len(sa.jsLimits) > 0 {\n\t\t\ts.Fatalf(\"Not allowed to enable JetStream on the system account\")\n\t\t}\n\t\tcfg := &JetStreamConfig{\n\t\t\tStoreDir:     opts.StoreDir,\n\t\t\tSyncInterval: opts.SyncInterval,\n\t\t\tSyncAlways:   opts.SyncAlways,\n\t\t\tStrict:       opts.JetStreamStrict,\n\t\t\tMaxMemory:    opts.JetStreamMaxMemory,\n\t\t\tMaxStore:     opts.JetStreamMaxStore,\n\t\t\tDomain:       opts.JetStreamDomain,\n\t\t\tCompressOK:   true,\n\t\t\tUniqueTag:    opts.JetStreamUniqueTag,\n\t\t}\n\t\tif err := s.EnableJetStream(cfg); err != nil {\n\t\t\ts.Fatalf(\"Can't start JetStream: %v\", err)\n\t\t\treturn\n\t\t}\n\t} else {\n\t\t// Check to see if any configured accounts have JetStream enabled.\n\t\tsa, ga := s.SystemAccount(), s.GlobalAccount()\n\t\tvar hasSys, hasGlobal bool\n\t\tvar total int\n\n\t\ts.accounts.Range(func(k, v any) bool {\n\t\t\ttotal++\n\t\t\tacc := v.(*Account)\n\t\t\tif acc == sa {\n\t\t\t\thasSys = true\n\t\t\t} else if acc == ga {\n\t\t\t\thasGlobal = true\n\t\t\t}\n\t\t\tacc.mu.RLock()\n\t\t\thasJs := len(acc.jsLimits) > 0\n\t\t\tacc.mu.RUnlock()\n\t\t\tif hasJs {\n\t\t\t\ts.checkJetStreamExports()\n\t\t\t\tacc.enableAllJetStreamServiceImportsAndMappings()\n\t\t\t}\n\t\t\treturn true\n\t\t})\n\t\t// If we only have the system account and the global account and we are not standalone,\n\t\t// go ahead and enable JS on $G in case we are in simple mixed mode setup.\n\t\tif total == 2 && hasSys && hasGlobal && !s.standAloneMode() {\n\t\t\tga.mu.Lock()\n\t\t\tga.jsLimits = map[string]JetStreamAccountLimits{\n\t\t\t\t_EMPTY_: dynamicJSAccountLimits,\n\t\t\t}\n\t\t\tga.mu.Unlock()\n\t\t\ts.checkJetStreamExports()\n\t\t\tga.enableAllJetStreamServiceImportsAndMappings()\n\t\t}\n\t}\n\n\t// Delayed API response handling. Start regardless of JetStream being\n\t// currently configured or not (since it can be enabled/disabled with\n\t// configuration reload).\n\ts.startGoRoutine(s.delayedAPIResponder)\n\n\t// Start OCSP Stapling monitoring for TLS certificates if enabled. Hook TLS handshake for\n\t// OCSP check on peers (LEAF and CLIENT kind) if enabled.\n\ts.startOCSPMonitoring()\n\n\t// Configure OCSP Response Cache for peer OCSP checks if enabled.\n\ts.initOCSPResponseCache()\n\n\t// Start up gateway if needed. Do this before starting the routes, because\n\t// we want to resolve the gateway host:port so that this information can\n\t// be sent to other routes.\n\tif opts.Gateway.Port != 0 {\n\t\ts.startGateways()\n\t}\n\n\t// Start websocket server if needed. Do this before starting the routes, and\n\t// leaf node because we want to resolve the gateway host:port so that this\n\t// information can be sent to other routes.\n\tif opts.Websocket.Port != 0 {\n\t\ts.startWebsocketServer()\n\t}\n\n\t// Start up listen if we want to accept leaf node connections.\n\tif opts.LeafNode.Port != 0 {\n\t\t// Will resolve or assign the advertise address for the leafnode listener.\n\t\t// We need that in StartRouting().\n\t\ts.startLeafNodeAcceptLoop()\n\t}\n\n\t// Solicit remote servers for leaf node connections.\n\tif len(opts.LeafNode.Remotes) > 0 {\n\t\ts.solicitLeafNodeRemotes(opts.LeafNode.Remotes)\n\t}\n\n\t// TODO (ik): I wanted to refactor this by starting the client\n\t// accept loop first, that is, it would resolve listen spec\n\t// in place, but start the accept-for-loop in a different go\n\t// routine. This would get rid of the synchronization between\n\t// this function and StartRouting, which I also would have wanted\n\t// to refactor, but both AcceptLoop() and StartRouting() have\n\t// been exported and not sure if that would break users using them.\n\t// We could mark them as deprecated and remove in a release or two...\n\n\t// The Routing routine needs to wait for the client listen\n\t// port to be opened and potential ephemeral port selected.\n\tclientListenReady := make(chan struct{})\n\n\t// MQTT\n\tif opts.MQTT.Port != 0 {\n\t\ts.startMQTT()\n\t}\n\n\t// Start up routing as well if needed.\n\tif opts.Cluster.Port != 0 {\n\t\ts.startGoRoutine(func() {\n\t\t\ts.StartRouting(clientListenReady)\n\t\t})\n\t}\n\n\tif opts.PortsFileDir != _EMPTY_ {\n\t\ts.logPorts()\n\t}\n\n\tif opts.TLSRateLimit > 0 {\n\t\ts.startGoRoutine(s.logRejectedTLSConns)\n\t}\n\n\t// We've finished starting up.\n\tclose(s.startupComplete)\n\n\t// Wait for clients.\n\tif !opts.DontListen {\n\t\ts.AcceptLoop(clientListenReady)\n\t}\n\n\t// Bring OSCP Response cache online after accept loop started in anticipation of NATS-enabled cache types\n\ts.startOCSPResponseCache()\n}\n\nfunc (s *Server) isShuttingDown() bool {\n\treturn s.shutdown.Load()\n}\n\n// Shutdown will shutdown the server instance by kicking out the AcceptLoop\n// and closing all associated clients.\nfunc (s *Server) Shutdown() {\n\tif s == nil {\n\t\treturn\n\t}\n\t// This is for JetStream R1 Pull Consumers to allow signaling\n\t// that pending pull requests are invalid.\n\ts.signalPullConsumers()\n\n\t// Transfer off any raft nodes that we are a leader by stepping them down.\n\ts.stepdownRaftNodes()\n\n\t// Shutdown the eventing system as needed.\n\t// This is done first to send out any messages for\n\t// account status. We will also clean up any\n\t// eventing items associated with accounts.\n\ts.shutdownEventing()\n\n\t// Prevent issues with multiple calls.\n\tif s.isShuttingDown() {\n\t\treturn\n\t}\n\n\ts.mu.Lock()\n\ts.Noticef(\"Initiating Shutdown...\")\n\n\taccRes := s.accResolver\n\n\topts := s.getOpts()\n\n\ts.shutdown.Store(true)\n\ts.running.Store(false)\n\ts.grMu.Lock()\n\ts.grRunning = false\n\ts.grMu.Unlock()\n\ts.mu.Unlock()\n\n\tif accRes != nil {\n\t\taccRes.Close()\n\t}\n\n\t// Now check and shutdown jetstream.\n\ts.shutdownJetStream()\n\n\t// Now shutdown the nodes\n\ts.shutdownRaftNodes()\n\n\ts.mu.Lock()\n\tconns := make(map[uint64]*client)\n\n\t// Copy off the clients\n\tfor i, c := range s.clients {\n\t\tconns[i] = c\n\t}\n\t// Copy off the connections that are not yet registered\n\t// in s.routes, but for which the readLoop has started\n\ts.grMu.Lock()\n\tfor i, c := range s.grTmpClients {\n\t\tconns[i] = c\n\t}\n\ts.grMu.Unlock()\n\t// Copy off the routes\n\ts.forEachRoute(func(r *client) {\n\t\tr.mu.Lock()\n\t\tconns[r.cid] = r\n\t\tr.mu.Unlock()\n\t})\n\t// Copy off the gateways\n\ts.getAllGatewayConnections(conns)\n\n\t// Copy off the leaf nodes\n\tfor i, c := range s.leafs {\n\t\tconns[i] = c\n\t}\n\n\t// Number of done channel responses we expect.\n\tdoneExpected := 0\n\n\t// Kick client AcceptLoop()\n\tif s.listener != nil {\n\t\tdoneExpected++\n\t\ts.listener.Close()\n\t\ts.listener = nil\n\t}\n\n\t// Kick websocket server\n\tdoneExpected += s.closeWebsocketServer()\n\n\t// Kick MQTT accept loop\n\tif s.mqtt.listener != nil {\n\t\tdoneExpected++\n\t\ts.mqtt.listener.Close()\n\t\ts.mqtt.listener = nil\n\t}\n\n\t// Kick leafnodes AcceptLoop()\n\tif s.leafNodeListener != nil {\n\t\tdoneExpected++\n\t\ts.leafNodeListener.Close()\n\t\ts.leafNodeListener = nil\n\t}\n\n\t// Kick route AcceptLoop()\n\tif s.routeListener != nil {\n\t\tdoneExpected++\n\t\ts.routeListener.Close()\n\t\ts.routeListener = nil\n\t}\n\n\t// Kick Gateway AcceptLoop()\n\tif s.gatewayListener != nil {\n\t\tdoneExpected++\n\t\ts.gatewayListener.Close()\n\t\ts.gatewayListener = nil\n\t}\n\n\t// Kick HTTP monitoring if its running\n\tif s.http != nil {\n\t\tdoneExpected++\n\t\ts.http.Close()\n\t\ts.http = nil\n\t}\n\n\t// Kick Profiling if its running\n\tif s.profiler != nil {\n\t\tdoneExpected++\n\t\ts.profiler.Close()\n\t}\n\n\ts.mu.Unlock()\n\n\t// Release go routines that wait on that channel\n\tclose(s.quitCh)\n\n\t// Close client and route connections\n\tfor _, c := range conns {\n\t\tc.setNoReconnect()\n\t\tc.closeConnection(ServerShutdown)\n\t}\n\n\t// Block until the accept loops exit\n\tfor doneExpected > 0 {\n\t\t<-s.done\n\t\tdoneExpected--\n\t}\n\n\t// Wait for go routines to be done.\n\ts.grWG.Wait()\n\n\tif opts.PortsFileDir != _EMPTY_ {\n\t\ts.deletePortsFile(opts.PortsFileDir)\n\t}\n\n\ts.Noticef(\"Server Exiting..\")\n\n\t// Stop OCSP Response Cache\n\tif s.ocsprc != nil {\n\t\ts.ocsprc.Stop(s)\n\t}\n\n\t// Close logger if applicable. It allows tests on Windows\n\t// to be able to do proper cleanup (delete log file).\n\ts.logging.RLock()\n\tlog := s.logging.logger\n\ts.logging.RUnlock()\n\tif log != nil {\n\t\tif l, ok := log.(*logger.Logger); ok {\n\t\t\tl.Close()\n\t\t}\n\t}\n\t// Notify that the shutdown is complete\n\tclose(s.shutdownComplete)\n}\n\n// Close the websocket server if running. If so, returns 1, else 0.\n// Server lock held on entry.\nfunc (s *Server) closeWebsocketServer() int {\n\tws := &s.websocket\n\tws.mu.Lock()\n\ths := ws.server\n\tif hs != nil {\n\t\tws.server = nil\n\t\tws.listener = nil\n\t}\n\tws.mu.Unlock()\n\tif hs != nil {\n\t\ths.Close()\n\t\treturn 1\n\t}\n\treturn 0\n}\n\n// WaitForShutdown will block until the server has been fully shutdown.\nfunc (s *Server) WaitForShutdown() {\n\t<-s.shutdownComplete\n}\n\n// AcceptLoop is exported for easier testing.\nfunc (s *Server) AcceptLoop(clr chan struct{}) {\n\t// If we were to exit before the listener is setup properly,\n\t// make sure we close the channel.\n\tdefer func() {\n\t\tif clr != nil {\n\t\t\tclose(clr)\n\t\t}\n\t}()\n\n\tif s.isShuttingDown() {\n\t\treturn\n\t}\n\n\t// Snapshot server options.\n\topts := s.getOpts()\n\n\t// Setup state that can enable shutdown\n\ts.mu.Lock()\n\thp := net.JoinHostPort(opts.Host, strconv.Itoa(opts.Port))\n\tl, e := natsListen(\"tcp\", hp)\n\ts.listenerErr = e\n\tif e != nil {\n\t\ts.mu.Unlock()\n\t\ts.Fatalf(\"Error listening on port: %s, %q\", hp, e)\n\t\treturn\n\t}\n\ts.Noticef(\"Listening for client connections on %s\",\n\t\tnet.JoinHostPort(opts.Host, strconv.Itoa(l.Addr().(*net.TCPAddr).Port)))\n\n\t// Alert of TLS enabled.\n\tif opts.TLSConfig != nil {\n\t\ts.Noticef(\"TLS required for client connections\")\n\t\tif opts.TLSHandshakeFirst && opts.TLSHandshakeFirstFallback == 0 {\n\t\t\ts.Warnf(\"Clients that are not using \\\"TLS Handshake First\\\" option will fail to connect\")\n\t\t}\n\t}\n\n\t// If server was started with RANDOM_PORT (-1), opts.Port would be equal\n\t// to 0 at the beginning this function. So we need to get the actual port\n\tif opts.Port == 0 {\n\t\t// Write resolved port back to options.\n\t\topts.Port = l.Addr().(*net.TCPAddr).Port\n\t}\n\n\t// Now that port has been set (if it was set to RANDOM), set the\n\t// server's info Host/Port with either values from Options or\n\t// ClientAdvertise.\n\tif err := s.setInfoHostPort(); err != nil {\n\t\ts.Fatalf(\"Error setting server INFO with ClientAdvertise value of %s, err=%v\", opts.ClientAdvertise, err)\n\t\tl.Close()\n\t\ts.mu.Unlock()\n\t\treturn\n\t}\n\t// Keep track of client connect URLs. We may need them later.\n\ts.clientConnectURLs = s.getClientConnectURLs()\n\ts.listener = l\n\n\tgo s.acceptConnections(l, \"Client\", func(conn net.Conn) { s.createClient(conn) },\n\t\tfunc(_ error) bool {\n\t\t\tif s.isLameDuckMode() {\n\t\t\t\t// Signal that we are not accepting new clients\n\t\t\t\ts.ldmCh <- true\n\t\t\t\t// Now wait for the Shutdown...\n\t\t\t\t<-s.quitCh\n\t\t\t\treturn true\n\t\t\t}\n\t\t\treturn false\n\t\t})\n\ts.mu.Unlock()\n\n\t// Let the caller know that we are ready\n\tclose(clr)\n\tclr = nil\n}\n\n// InProcessConn returns an in-process connection to the server,\n// avoiding the need to use a TCP listener for local connectivity\n// within the same process. This can be used regardless of the\n// state of the DontListen option.\nfunc (s *Server) InProcessConn() (net.Conn, error) {\n\tpl, pr := net.Pipe()\n\tif !s.startGoRoutine(func() {\n\t\ts.createClientInProcess(pl)\n\t\ts.grWG.Done()\n\t}) {\n\t\tpl.Close()\n\t\tpr.Close()\n\t\treturn nil, fmt.Errorf(\"failed to create connection\")\n\t}\n\treturn pr, nil\n}\n\nfunc (s *Server) acceptConnections(l net.Listener, acceptName string, createFunc func(conn net.Conn), errFunc func(err error) bool) {\n\ttmpDelay := ACCEPT_MIN_SLEEP\n\n\tfor {\n\t\tconn, err := l.Accept()\n\t\tif err != nil {\n\t\t\tif errFunc != nil && errFunc(err) {\n\t\t\t\treturn\n\t\t\t}\n\t\t\tif tmpDelay = s.acceptError(acceptName, err, tmpDelay); tmpDelay < 0 {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\t\ttmpDelay = ACCEPT_MIN_SLEEP\n\t\tif !s.startGoRoutine(func() {\n\t\t\ts.reloadMu.RLock()\n\t\t\tcreateFunc(conn)\n\t\t\ts.reloadMu.RUnlock()\n\t\t\ts.grWG.Done()\n\t\t}) {\n\t\t\tconn.Close()\n\t\t}\n\t}\n\ts.Debugf(acceptName + \" accept loop exiting..\")\n\ts.done <- true\n}\n\n// This function sets the server's info Host/Port based on server Options.\n// Note that this function may be called during config reload, this is why\n// Host/Port may be reset to original Options if the ClientAdvertise option\n// is not set (since it may have previously been).\nfunc (s *Server) setInfoHostPort() error {\n\t// When this function is called, opts.Port is set to the actual listen\n\t// port (if option was originally set to RANDOM), even during a config\n\t// reload. So use of s.opts.Port is safe.\n\topts := s.getOpts()\n\tif opts.ClientAdvertise != _EMPTY_ {\n\t\th, p, err := parseHostPort(opts.ClientAdvertise, opts.Port)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\ts.info.Host = h\n\t\ts.info.Port = p\n\t} else {\n\t\ts.info.Host = opts.Host\n\t\ts.info.Port = opts.Port\n\t}\n\treturn nil\n}\n\n// StartProfiler is called to enable dynamic profiling.\nfunc (s *Server) StartProfiler() {\n\tif s.isShuttingDown() {\n\t\treturn\n\t}\n\n\t// Snapshot server options.\n\topts := s.getOpts()\n\n\tport := opts.ProfPort\n\n\t// Check for Random Port\n\tif port == -1 {\n\t\tport = 0\n\t}\n\n\ts.mu.Lock()\n\thp := net.JoinHostPort(opts.Host, strconv.Itoa(port))\n\tl, err := net.Listen(\"tcp\", hp)\n\n\tif err != nil {\n\t\ts.mu.Unlock()\n\t\ts.Fatalf(\"error starting profiler: %s\", err)\n\t\treturn\n\t}\n\ts.Noticef(\"profiling port: %d\", l.Addr().(*net.TCPAddr).Port)\n\n\tsrv := &http.Server{\n\t\tAddr:           hp,\n\t\tHandler:        http.DefaultServeMux,\n\t\tMaxHeaderBytes: 1 << 20,\n\t\tReadTimeout:    time.Second * 5,\n\t}\n\ts.profiler = l\n\ts.profilingServer = srv\n\n\ts.setBlockProfileRate(opts.ProfBlockRate)\n\n\tgo func() {\n\t\t// if this errors out, it's probably because the server is being shutdown\n\t\terr := srv.Serve(l)\n\t\tif err != nil {\n\t\t\tif !s.isShuttingDown() {\n\t\t\t\ts.Fatalf(\"error starting profiler: %s\", err)\n\t\t\t}\n\t\t}\n\t\tsrv.Close()\n\t\ts.done <- true\n\t}()\n\ts.mu.Unlock()\n}\n\nfunc (s *Server) setBlockProfileRate(rate int) {\n\t// Passing i ProfBlockRate <= 0 here will disable or > 0 will enable.\n\truntime.SetBlockProfileRate(rate)\n\n\tif rate > 0 {\n\t\ts.Warnf(\"Block profiling is enabled (rate %d), this may have a performance impact\", rate)\n\t}\n}\n\n// StartHTTPMonitoring will enable the HTTP monitoring port.\n// DEPRECATED: Should use StartMonitoring.\nfunc (s *Server) StartHTTPMonitoring() {\n\ts.startMonitoring(false)\n}\n\n// StartHTTPSMonitoring will enable the HTTPS monitoring port.\n// DEPRECATED: Should use StartMonitoring.\nfunc (s *Server) StartHTTPSMonitoring() {\n\ts.startMonitoring(true)\n}\n\n// StartMonitoring starts the HTTP or HTTPs server if needed.\nfunc (s *Server) StartMonitoring() error {\n\t// Snapshot server options.\n\topts := s.getOpts()\n\n\t// Specifying both HTTP and HTTPS ports is a misconfiguration\n\tif opts.HTTPPort != 0 && opts.HTTPSPort != 0 {\n\t\treturn fmt.Errorf(\"can't specify both HTTP (%v) and HTTPs (%v) ports\", opts.HTTPPort, opts.HTTPSPort)\n\t}\n\tvar err error\n\tif opts.HTTPPort != 0 {\n\t\terr = s.startMonitoring(false)\n\t} else if opts.HTTPSPort != 0 {\n\t\tif opts.TLSConfig == nil {\n\t\t\treturn fmt.Errorf(\"TLS cert and key required for HTTPS\")\n\t\t}\n\t\terr = s.startMonitoring(true)\n\t}\n\treturn err\n}\n\n// HTTP endpoints\nconst (\n\tRootPath         = \"/\"\n\tVarzPath         = \"/varz\"\n\tConnzPath        = \"/connz\"\n\tRoutezPath       = \"/routez\"\n\tGatewayzPath     = \"/gatewayz\"\n\tLeafzPath        = \"/leafz\"\n\tSubszPath        = \"/subsz\"\n\tStackszPath      = \"/stacksz\"\n\tAccountzPath     = \"/accountz\"\n\tAccountStatzPath = \"/accstatz\"\n\tJszPath          = \"/jsz\"\n\tHealthzPath      = \"/healthz\"\n\tIPQueuesPath     = \"/ipqueuesz\"\n\tRaftzPath        = \"/raftz\"\n)\n\nfunc (s *Server) basePath(p string) string {\n\treturn path.Join(s.httpBasePath, p)\n}\n\ntype captureHTTPServerLog struct {\n\ts      *Server\n\tprefix string\n}\n\nfunc (cl *captureHTTPServerLog) Write(p []byte) (int, error) {\n\tvar buf [128]byte\n\tvar b = buf[:0]\n\n\tb = append(b, []byte(cl.prefix)...)\n\toffset := 0\n\tif bytes.HasPrefix(p, []byte(\"http:\")) {\n\t\toffset = 6\n\t}\n\tb = append(b, p[offset:]...)\n\tcl.s.Errorf(string(b))\n\treturn len(p), nil\n}\n\n// The TLS configuration is passed to the listener when the monitoring\n// \"server\" is setup. That prevents TLS configuration updates on reload\n// from being used. By setting this function in tls.Config.GetConfigForClient\n// we instruct the TLS handshake to ask for the tls configuration to be\n// used for a specific client. We don't care which client, we always use\n// the same TLS configuration.\nfunc (s *Server) getMonitoringTLSConfig(_ *tls.ClientHelloInfo) (*tls.Config, error) {\n\topts := s.getOpts()\n\ttc := opts.TLSConfig.Clone()\n\ttc.ClientAuth = tls.NoClientCert\n\treturn tc, nil\n}\n\n// Start the monitoring server\nfunc (s *Server) startMonitoring(secure bool) error {\n\tif s.isShuttingDown() {\n\t\treturn nil\n\t}\n\n\t// Snapshot server options.\n\topts := s.getOpts()\n\n\tvar (\n\t\thp           string\n\t\terr          error\n\t\thttpListener net.Listener\n\t\tport         int\n\t)\n\n\tmonitorProtocol := \"http\"\n\n\tif secure {\n\t\tmonitorProtocol += \"s\"\n\t\tport = opts.HTTPSPort\n\t\tif port == -1 {\n\t\t\tport = 0\n\t\t}\n\t\thp = net.JoinHostPort(opts.HTTPHost, strconv.Itoa(port))\n\t\tconfig := opts.TLSConfig.Clone()\n\t\tif !s.ocspPeerVerify {\n\t\t\tconfig.GetConfigForClient = s.getMonitoringTLSConfig\n\t\t\tconfig.ClientAuth = tls.NoClientCert\n\t\t}\n\t\thttpListener, err = tls.Listen(\"tcp\", hp, config)\n\n\t} else {\n\t\tport = opts.HTTPPort\n\t\tif port == -1 {\n\t\t\tport = 0\n\t\t}\n\t\thp = net.JoinHostPort(opts.HTTPHost, strconv.Itoa(port))\n\t\thttpListener, err = net.Listen(\"tcp\", hp)\n\t}\n\n\tif err != nil {\n\t\treturn fmt.Errorf(\"can't listen to the monitor port: %v\", err)\n\t}\n\n\trport := httpListener.Addr().(*net.TCPAddr).Port\n\ts.Noticef(\"Starting %s monitor on %s\", monitorProtocol, net.JoinHostPort(opts.HTTPHost, strconv.Itoa(rport)))\n\n\tmux := http.NewServeMux()\n\n\t// Root\n\tmux.HandleFunc(s.basePath(RootPath), s.HandleRoot)\n\t// Varz\n\tmux.HandleFunc(s.basePath(VarzPath), s.HandleVarz)\n\t// Connz\n\tmux.HandleFunc(s.basePath(ConnzPath), s.HandleConnz)\n\t// Routez\n\tmux.HandleFunc(s.basePath(RoutezPath), s.HandleRoutez)\n\t// Gatewayz\n\tmux.HandleFunc(s.basePath(GatewayzPath), s.HandleGatewayz)\n\t// Leafz\n\tmux.HandleFunc(s.basePath(LeafzPath), s.HandleLeafz)\n\t// Subz\n\tmux.HandleFunc(s.basePath(SubszPath), s.HandleSubsz)\n\t// Subz alias for backwards compatibility\n\tmux.HandleFunc(s.basePath(\"/subscriptionsz\"), s.HandleSubsz)\n\t// Stacksz\n\tmux.HandleFunc(s.basePath(StackszPath), s.HandleStacksz)\n\t// Accountz\n\tmux.HandleFunc(s.basePath(AccountzPath), s.HandleAccountz)\n\t// Accstatz\n\tmux.HandleFunc(s.basePath(AccountStatzPath), s.HandleAccountStatz)\n\t// Jsz\n\tmux.HandleFunc(s.basePath(JszPath), s.HandleJsz)\n\t// Healthz\n\tmux.HandleFunc(s.basePath(HealthzPath), s.HandleHealthz)\n\t// IPQueuesz\n\tmux.HandleFunc(s.basePath(IPQueuesPath), s.HandleIPQueuesz)\n\t// Raftz\n\tmux.HandleFunc(s.basePath(RaftzPath), s.HandleRaftz)\n\n\t// Do not set a WriteTimeout because it could cause cURL/browser\n\t// to return empty response or unable to display page if the\n\t// server needs more time to build the response.\n\tsrv := &http.Server{\n\t\tAddr:              hp,\n\t\tHandler:           mux,\n\t\tMaxHeaderBytes:    1 << 20,\n\t\tErrorLog:          log.New(&captureHTTPServerLog{s, \"monitoring: \"}, _EMPTY_, 0),\n\t\tReadHeaderTimeout: time.Second * 5,\n\t}\n\ts.mu.Lock()\n\ts.http = httpListener\n\ts.httpHandler = mux\n\ts.monitoringServer = srv\n\ts.mu.Unlock()\n\n\tgo func() {\n\t\tif err := srv.Serve(httpListener); err != nil {\n\t\t\tif !s.isShuttingDown() {\n\t\t\t\ts.Fatalf(\"Error starting monitor on %q: %v\", hp, err)\n\t\t\t}\n\t\t}\n\t\tsrv.Close()\n\t\ts.mu.Lock()\n\t\ts.httpHandler = nil\n\t\ts.mu.Unlock()\n\t\ts.done <- true\n\t}()\n\n\treturn nil\n}\n\n// HTTPHandler returns the http.Handler object used to handle monitoring\n// endpoints. It will return nil if the server is not configured for\n// monitoring, or if the server has not been started yet (Server.Start()).\nfunc (s *Server) HTTPHandler() http.Handler {\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\treturn s.httpHandler\n}\n\n// Perform a conditional deep copy due to reference nature of [Client|WS]ConnectURLs.\n// If updates are made to Info, this function should be consulted and updated.\n// Assume lock is held.\nfunc (s *Server) copyInfo() Info {\n\tinfo := s.info\n\tif len(info.ClientConnectURLs) > 0 {\n\t\tinfo.ClientConnectURLs = append([]string(nil), s.info.ClientConnectURLs...)\n\t}\n\tif len(info.WSConnectURLs) > 0 {\n\t\tinfo.WSConnectURLs = append([]string(nil), s.info.WSConnectURLs...)\n\t}\n\treturn info\n}\n\n// tlsMixConn is used when we can receive both TLS and non-TLS connections on same port.\ntype tlsMixConn struct {\n\tnet.Conn\n\tpre *bytes.Buffer\n}\n\n// Read for our mixed multi-reader.\nfunc (c *tlsMixConn) Read(b []byte) (int, error) {\n\tif c.pre != nil {\n\t\tn, err := c.pre.Read(b)\n\t\tif c.pre.Len() == 0 {\n\t\t\tc.pre = nil\n\t\t}\n\t\treturn n, err\n\t}\n\treturn c.Conn.Read(b)\n}\n\nfunc (s *Server) createClient(conn net.Conn) *client {\n\treturn s.createClientEx(conn, false)\n}\n\nfunc (s *Server) createClientInProcess(conn net.Conn) *client {\n\treturn s.createClientEx(conn, true)\n}\n\nfunc (s *Server) createClientEx(conn net.Conn, inProcess bool) *client {\n\t// Snapshot server options.\n\topts := s.getOpts()\n\n\tmaxPay := int32(opts.MaxPayload)\n\tmaxSubs := int32(opts.MaxSubs)\n\t// For system, maxSubs of 0 means unlimited, so re-adjust here.\n\tif maxSubs == 0 {\n\t\tmaxSubs = -1\n\t}\n\tnow := time.Now()\n\n\tc := &client{\n\t\tsrv:   s,\n\t\tnc:    conn,\n\t\topts:  defaultOpts,\n\t\tmpay:  maxPay,\n\t\tmsubs: maxSubs,\n\t\tstart: now,\n\t\tlast:  now,\n\t\tiproc: inProcess,\n\t}\n\n\tc.registerWithAccount(s.globalAccount())\n\n\tvar info Info\n\tvar authRequired bool\n\n\ts.mu.Lock()\n\t// Grab JSON info string\n\tinfo = s.copyInfo()\n\tif s.nonceRequired() {\n\t\t// Nonce handling\n\t\tvar raw [nonceLen]byte\n\t\tnonce := raw[:]\n\t\ts.generateNonce(nonce)\n\t\tinfo.Nonce = string(nonce)\n\t}\n\tc.nonce = []byte(info.Nonce)\n\tauthRequired = info.AuthRequired\n\n\t// Check to see if we have auth_required set but we also have a no_auth_user.\n\t// If so set back to false.\n\tif info.AuthRequired && opts.NoAuthUser != _EMPTY_ && opts.NoAuthUser != s.sysAccOnlyNoAuthUser {\n\t\tinfo.AuthRequired = false\n\t}\n\n\t// Check to see if this is an in-process connection with tls_required.\n\t// If so, set as not required, but available.\n\tif inProcess && info.TLSRequired {\n\t\tinfo.TLSRequired = false\n\t\tinfo.TLSAvailable = true\n\t}\n\n\ts.totalClients++\n\ts.mu.Unlock()\n\n\t// Grab lock\n\tc.mu.Lock()\n\tif authRequired {\n\t\tc.flags.set(expectConnect)\n\t}\n\n\t// Initialize\n\tc.initClient()\n\n\tc.Debugf(\"Client connection created\")\n\n\t// Save info.TLSRequired value since we may neeed to change it back and forth.\n\torgInfoTLSReq := info.TLSRequired\n\n\tvar tlsFirstFallback time.Duration\n\t// Check if we should do TLS first.\n\ttlsFirst := opts.TLSConfig != nil && opts.TLSHandshakeFirst\n\tif tlsFirst {\n\t\t// Make sure info.TLSRequired is set to true (it could be false\n\t\t// if AllowNonTLS is enabled).\n\t\tinfo.TLSRequired = true\n\t\t// Get the fallback delay value if applicable.\n\t\tif f := opts.TLSHandshakeFirstFallback; f > 0 {\n\t\t\ttlsFirstFallback = f\n\t\t} else if inProcess {\n\t\t\t// For in-process connection, we will always have a fallback\n\t\t\t// delay. It allows support for non-TLS, TLS and \"TLS First\"\n\t\t\t// in-process clients to successfully connect.\n\t\t\ttlsFirstFallback = DEFAULT_TLS_HANDSHAKE_FIRST_FALLBACK_DELAY\n\t\t}\n\t}\n\n\t// Decide if we are going to require TLS or not and generate INFO json.\n\ttlsRequired := info.TLSRequired\n\tinfoBytes := c.generateClientInfoJSON(info)\n\n\t// Send our information, except if TLS and TLSHandshakeFirst is requested.\n\tif !tlsFirst {\n\t\t// Need to be sent in place since writeLoop cannot be started until\n\t\t// TLS handshake is done (if applicable).\n\t\tc.sendProtoNow(infoBytes)\n\t}\n\n\t// Unlock to register\n\tc.mu.Unlock()\n\n\t// Register with the server.\n\ts.mu.Lock()\n\t// If server is not running, Shutdown() may have already gathered the\n\t// list of connections to close. It won't contain this one, so we need\n\t// to bail out now otherwise the readLoop started down there would not\n\t// be interrupted. Skip also if in lame duck mode.\n\tif !s.isRunning() || s.ldm {\n\t\t// There are some tests that create a server but don't start it,\n\t\t// and use \"async\" clients and perform the parsing manually. Such\n\t\t// clients would branch here (since server is not running). However,\n\t\t// when a server was really running and has been shutdown, we must\n\t\t// close this connection.\n\t\tif s.isShuttingDown() {\n\t\t\tconn.Close()\n\t\t}\n\t\ts.mu.Unlock()\n\t\treturn c\n\t}\n\n\t// If there is a max connections specified, check that adding\n\t// this new client would not push us over the max\n\tif opts.MaxConn > 0 && len(s.clients) >= opts.MaxConn {\n\t\ts.mu.Unlock()\n\t\tc.maxConnExceeded()\n\t\treturn nil\n\t}\n\ts.clients[c.cid] = c\n\n\ts.mu.Unlock()\n\n\t// Re-Grab lock\n\tc.mu.Lock()\n\n\tisClosed := c.isClosed()\n\tvar pre []byte\n\t// We need first to check for \"TLS First\" fallback delay.\n\tif !isClosed && tlsFirstFallback > 0 {\n\t\t// We wait and see if we are getting any data. Since we did not send\n\t\t// the INFO protocol yet, only clients that use TLS first should be\n\t\t// sending data (the TLS handshake). We don't really check the content:\n\t\t// if it is a rogue agent and not an actual client performing the\n\t\t// TLS handshake, the error will be detected when performing the\n\t\t// handshake on our side.\n\t\tpre = make([]byte, 4)\n\t\tc.nc.SetReadDeadline(time.Now().Add(tlsFirstFallback))\n\t\tn, _ := io.ReadFull(c.nc, pre[:])\n\t\tc.nc.SetReadDeadline(time.Time{})\n\t\t// If we get any data (regardless of possible timeout), we will proceed\n\t\t// with the TLS handshake.\n\t\tif n > 0 {\n\t\t\tpre = pre[:n]\n\t\t} else {\n\t\t\t// We did not get anything so we will send the INFO protocol.\n\t\t\tpre = nil\n\n\t\t\t// Restore the original info.TLSRequired value if it is\n\t\t\t// different that the current value and regenerate infoBytes.\n\t\t\tif orgInfoTLSReq != info.TLSRequired {\n\t\t\t\tinfo.TLSRequired = orgInfoTLSReq\n\t\t\t\tinfoBytes = c.generateClientInfoJSON(info)\n\t\t\t}\n\t\t\tc.sendProtoNow(infoBytes)\n\t\t\t// Set the boolean to false for the rest of the function.\n\t\t\ttlsFirst = false\n\t\t\t// Check closed status again\n\t\t\tisClosed = c.isClosed()\n\t\t}\n\t}\n\t// If we have both TLS and non-TLS allowed we need to see which\n\t// one the client wants. We'll always allow this for in-process\n\t// connections.\n\tif !isClosed && !tlsFirst && opts.TLSConfig != nil && (inProcess || opts.AllowNonTLS) {\n\t\tpre = make([]byte, 4)\n\t\tc.nc.SetReadDeadline(time.Now().Add(secondsToDuration(opts.TLSTimeout)))\n\t\tn, _ := io.ReadFull(c.nc, pre[:])\n\t\tc.nc.SetReadDeadline(time.Time{})\n\t\tpre = pre[:n]\n\t\tif n > 0 && pre[0] == 0x16 {\n\t\t\ttlsRequired = true\n\t\t} else {\n\t\t\ttlsRequired = false\n\t\t}\n\t}\n\n\t// Check for TLS\n\tif !isClosed && tlsRequired {\n\t\tif s.connRateCounter != nil && !s.connRateCounter.allow() {\n\t\t\tc.mu.Unlock()\n\t\t\tc.sendErr(\"Connection throttling is active. Please try again later.\")\n\t\t\tc.closeConnection(MaxConnectionsExceeded)\n\t\t\treturn nil\n\t\t}\n\n\t\t// If we have a prebuffer create a multi-reader.\n\t\tif len(pre) > 0 {\n\t\t\tc.nc = &tlsMixConn{c.nc, bytes.NewBuffer(pre)}\n\t\t\t// Clear pre so it is not parsed.\n\t\t\tpre = nil\n\t\t}\n\t\t// Performs server-side TLS handshake.\n\t\tif err := c.doTLSServerHandshake(_EMPTY_, opts.TLSConfig, opts.TLSTimeout, opts.TLSPinnedCerts); err != nil {\n\t\t\tc.mu.Unlock()\n\t\t\treturn nil\n\t\t}\n\t}\n\n\t// Now, send the INFO if it was delayed\n\tif !isClosed && tlsFirst {\n\t\tc.flags.set(didTLSFirst)\n\t\tc.sendProtoNow(infoBytes)\n\t\t// Check closed status\n\t\tisClosed = c.isClosed()\n\t}\n\n\t// Connection could have been closed while sending the INFO proto.\n\tif isClosed {\n\t\tc.mu.Unlock()\n\t\t// We need to call closeConnection() to make sure that proper cleanup is done.\n\t\tc.closeConnection(WriteError)\n\t\treturn nil\n\t}\n\n\t// Check for Auth. We schedule this timer after the TLS handshake to avoid\n\t// the race where the timer fires during the handshake and causes the\n\t// server to write bad data to the socket. See issue #432.\n\tif authRequired {\n\t\tc.setAuthTimer(secondsToDuration(opts.AuthTimeout))\n\t}\n\n\t// Do final client initialization\n\n\t// Set the Ping timer. Will be reset once connect was received.\n\tc.setPingTimer()\n\n\t// Spin up the read loop.\n\ts.startGoRoutine(func() { c.readLoop(pre) })\n\n\t// Spin up the write loop.\n\ts.startGoRoutine(func() { c.writeLoop() })\n\n\tif tlsRequired {\n\t\tc.Debugf(\"TLS handshake complete\")\n\t\tcs := c.nc.(*tls.Conn).ConnectionState()\n\t\tc.Debugf(\"TLS version %s, cipher suite %s\", tlsVersion(cs.Version), tlsCipher(cs.CipherSuite))\n\t}\n\n\tc.mu.Unlock()\n\n\treturn c\n}\n\n// This will save off a closed client in a ring buffer such that\n// /connz can inspect. Useful for debugging, etc.\nfunc (s *Server) saveClosedClient(c *client, nc net.Conn, subs map[string]*subscription, reason ClosedState) {\n\tnow := time.Now()\n\n\ts.accountDisconnectEvent(c, now, reason.String())\n\n\tc.mu.Lock()\n\n\tcc := &closedClient{}\n\tcc.fill(c, nc, now, false)\n\t// Note that cc.fill is using len(c.subs), which may have been set to nil by now,\n\t// so replace cc.NumSubs with len(subs).\n\tcc.NumSubs = uint32(len(subs))\n\tcc.Stop = &now\n\tcc.Reason = reason.String()\n\n\t// Do subs, do not place by default in main ConnInfo\n\tif len(subs) > 0 {\n\t\tcc.subs = make([]SubDetail, 0, len(subs))\n\t\tfor _, sub := range subs {\n\t\t\tcc.subs = append(cc.subs, newSubDetail(sub))\n\t\t}\n\t}\n\t// Hold user as well.\n\tcc.user = c.getRawAuthUser()\n\t// Hold account name if not the global account.\n\tif c.acc != nil && c.acc.Name != globalAccountName {\n\t\tcc.acc = c.acc.Name\n\t}\n\tcc.JWT = c.opts.JWT\n\tcc.IssuerKey = issuerForClient(c)\n\tcc.Tags = c.tags\n\tcc.NameTag = c.nameTag\n\tc.mu.Unlock()\n\n\t// Place in the ring buffer\n\ts.mu.Lock()\n\tif s.closed != nil {\n\t\ts.closed.append(cc)\n\t}\n\ts.mu.Unlock()\n}\n\n// Adds to the list of client and websocket clients connect URLs.\n// If there was a change, an INFO protocol is sent to registered clients\n// that support async INFO protocols.\n// Server lock held on entry.\nfunc (s *Server) addConnectURLsAndSendINFOToClients(curls, wsurls []string) {\n\ts.updateServerINFOAndSendINFOToClients(curls, wsurls, true)\n}\n\n// Removes from the list of client and websocket clients connect URLs.\n// If there was a change, an INFO protocol is sent to registered clients\n// that support async INFO protocols.\n// Server lock held on entry.\nfunc (s *Server) removeConnectURLsAndSendINFOToClients(curls, wsurls []string) {\n\ts.updateServerINFOAndSendINFOToClients(curls, wsurls, false)\n}\n\n// Updates the list of client and websocket clients connect URLs and if any change\n// sends an async INFO update to clients that support it.\n// Server lock held on entry.\nfunc (s *Server) updateServerINFOAndSendINFOToClients(curls, wsurls []string, add bool) {\n\tremove := !add\n\t// Will return true if we need alter the server's Info object.\n\tupdateMap := func(urls []string, m refCountedUrlSet) bool {\n\t\twasUpdated := false\n\t\tfor _, url := range urls {\n\t\t\tif add && m.addUrl(url) {\n\t\t\t\twasUpdated = true\n\t\t\t} else if remove && m.removeUrl(url) {\n\t\t\t\twasUpdated = true\n\t\t\t}\n\t\t}\n\t\treturn wasUpdated\n\t}\n\tcliUpdated := updateMap(curls, s.clientConnectURLsMap)\n\twsUpdated := updateMap(wsurls, s.websocket.connectURLsMap)\n\n\tupdateInfo := func(infoURLs *[]string, urls []string, m refCountedUrlSet) {\n\t\t// Recreate the info's slice from the map\n\t\t*infoURLs = (*infoURLs)[:0]\n\t\t// Add this server client connect ULRs first...\n\t\t*infoURLs = append(*infoURLs, urls...)\n\t\t// Then the ones from the map\n\t\tfor url := range m {\n\t\t\t*infoURLs = append(*infoURLs, url)\n\t\t}\n\t}\n\tif cliUpdated {\n\t\tupdateInfo(&s.info.ClientConnectURLs, s.clientConnectURLs, s.clientConnectURLsMap)\n\t}\n\tif wsUpdated {\n\t\tupdateInfo(&s.info.WSConnectURLs, s.websocket.connectURLs, s.websocket.connectURLsMap)\n\t}\n\tif cliUpdated || wsUpdated {\n\t\t// Send to all registered clients that support async INFO protocols.\n\t\ts.sendAsyncInfoToClients(cliUpdated, wsUpdated)\n\t}\n}\n\n// Handle closing down a connection when the handshake has timedout.\nfunc tlsTimeout(c *client, conn *tls.Conn) {\n\tc.mu.Lock()\n\tclosed := c.isClosed()\n\tc.mu.Unlock()\n\t// Check if already closed\n\tif closed {\n\t\treturn\n\t}\n\tcs := conn.ConnectionState()\n\tif !cs.HandshakeComplete {\n\t\tc.Errorf(\"TLS handshake timeout\")\n\t\tc.sendErr(\"Secure Connection - TLS Required\")\n\t\tc.closeConnection(TLSHandshakeError)\n\t}\n}\n\n// Seems silly we have to write these\nfunc tlsVersion(ver uint16) string {\n\tswitch ver {\n\tcase tls.VersionTLS10:\n\t\treturn \"1.0\"\n\tcase tls.VersionTLS11:\n\t\treturn \"1.1\"\n\tcase tls.VersionTLS12:\n\t\treturn \"1.2\"\n\tcase tls.VersionTLS13:\n\t\treturn \"1.3\"\n\t}\n\treturn fmt.Sprintf(\"Unknown [0x%x]\", ver)\n}\n\nfunc tlsVersionFromString(ver string) (uint16, error) {\n\tswitch ver {\n\tcase \"1.0\":\n\t\treturn tls.VersionTLS10, nil\n\tcase \"1.1\":\n\t\treturn tls.VersionTLS11, nil\n\tcase \"1.2\":\n\t\treturn tls.VersionTLS12, nil\n\tcase \"1.3\":\n\t\treturn tls.VersionTLS13, nil\n\t}\n\treturn 0, fmt.Errorf(\"unknown version: %v\", ver)\n}\n\n// We use hex here so we don't need multiple versions\nfunc tlsCipher(cs uint16) string {\n\tname, present := cipherMapByID[cs]\n\tif present {\n\t\treturn name\n\t}\n\treturn fmt.Sprintf(\"Unknown [0x%x]\", cs)\n}\n\n// Remove a client or route from our internal accounting.\nfunc (s *Server) removeClient(c *client) {\n\t// kind is immutable, so can check without lock\n\tswitch c.kind {\n\tcase CLIENT:\n\t\tc.mu.Lock()\n\t\tcid := c.cid\n\t\tupdateProtoInfoCount := false\n\t\tif c.kind == CLIENT && c.opts.Protocol >= ClientProtoInfo {\n\t\t\tupdateProtoInfoCount = true\n\t\t}\n\t\tc.mu.Unlock()\n\n\t\ts.mu.Lock()\n\t\tdelete(s.clients, cid)\n\t\tif updateProtoInfoCount {\n\t\t\ts.cproto--\n\t\t}\n\t\ts.mu.Unlock()\n\tcase ROUTER:\n\t\ts.removeRoute(c)\n\tcase GATEWAY:\n\t\ts.removeRemoteGatewayConnection(c)\n\tcase LEAF:\n\t\ts.removeLeafNodeConnection(c)\n\t}\n}\n\nfunc (s *Server) removeFromTempClients(cid uint64) {\n\ts.grMu.Lock()\n\tdelete(s.grTmpClients, cid)\n\ts.grMu.Unlock()\n}\n\nfunc (s *Server) addToTempClients(cid uint64, c *client) bool {\n\tadded := false\n\ts.grMu.Lock()\n\tif s.grRunning {\n\t\ts.grTmpClients[cid] = c\n\t\tadded = true\n\t}\n\ts.grMu.Unlock()\n\treturn added\n}\n\n/////////////////////////////////////////////////////////////////\n// These are some helpers for accounting in functional tests.\n/////////////////////////////////////////////////////////////////\n\n// NumRoutes will report the number of registered routes.\nfunc (s *Server) NumRoutes() int {\n\ts.mu.RLock()\n\tdefer s.mu.RUnlock()\n\treturn s.numRoutes()\n}\n\n// numRoutes will report the number of registered routes.\n// Server lock held on entry\nfunc (s *Server) numRoutes() int {\n\tvar nr int\n\ts.forEachRoute(func(c *client) {\n\t\tnr++\n\t})\n\treturn nr\n}\n\n// NumRemotes will report number of registered remotes.\nfunc (s *Server) NumRemotes() int {\n\ts.mu.RLock()\n\tdefer s.mu.RUnlock()\n\treturn s.numRemotes()\n}\n\n// numRemotes will report number of registered remotes.\n// Server lock held on entry\nfunc (s *Server) numRemotes() int {\n\treturn len(s.routes)\n}\n\n// NumLeafNodes will report number of leaf node connections.\nfunc (s *Server) NumLeafNodes() int {\n\ts.mu.RLock()\n\tdefer s.mu.RUnlock()\n\treturn len(s.leafs)\n}\n\n// NumClients will report the number of registered clients.\nfunc (s *Server) NumClients() int {\n\ts.mu.RLock()\n\tdefer s.mu.RUnlock()\n\treturn len(s.clients)\n}\n\n// GetClient will return the client associated with cid.\nfunc (s *Server) GetClient(cid uint64) *client {\n\treturn s.getClient(cid)\n}\n\n// getClient will return the client associated with cid.\nfunc (s *Server) getClient(cid uint64) *client {\n\ts.mu.RLock()\n\tdefer s.mu.RUnlock()\n\treturn s.clients[cid]\n}\n\n// GetLeafNode returns the leafnode associated with the cid.\nfunc (s *Server) GetLeafNode(cid uint64) *client {\n\ts.mu.RLock()\n\tdefer s.mu.RUnlock()\n\treturn s.leafs[cid]\n}\n\n// NumSubscriptions will report how many subscriptions are active.\nfunc (s *Server) NumSubscriptions() uint32 {\n\ts.mu.RLock()\n\tdefer s.mu.RUnlock()\n\treturn s.numSubscriptions()\n}\n\n// numSubscriptions will report how many subscriptions are active.\n// Lock should be held.\nfunc (s *Server) numSubscriptions() uint32 {\n\tvar subs int\n\ts.accounts.Range(func(k, v any) bool {\n\t\tacc := v.(*Account)\n\t\tsubs += acc.TotalSubs()\n\t\treturn true\n\t})\n\treturn uint32(subs)\n}\n\n// NumSlowConsumers will report the number of slow consumers.\nfunc (s *Server) NumSlowConsumers() int64 {\n\treturn atomic.LoadInt64(&s.slowConsumers)\n}\n\n// NumSlowConsumersClients will report the number of slow consumers clients.\nfunc (s *Server) NumSlowConsumersClients() uint64 {\n\treturn s.scStats.clients.Load()\n}\n\n// NumSlowConsumersRoutes will report the number of slow consumers routes.\nfunc (s *Server) NumSlowConsumersRoutes() uint64 {\n\treturn s.scStats.routes.Load()\n}\n\n// NumSlowConsumersGateways will report the number of slow consumers leafs.\nfunc (s *Server) NumSlowConsumersGateways() uint64 {\n\treturn s.scStats.gateways.Load()\n}\n\n// NumSlowConsumersLeafs will report the number of slow consumers leafs.\nfunc (s *Server) NumSlowConsumersLeafs() uint64 {\n\treturn s.scStats.leafs.Load()\n}\n\n// ConfigTime will report the last time the server configuration was loaded.\nfunc (s *Server) ConfigTime() time.Time {\n\ts.mu.RLock()\n\tdefer s.mu.RUnlock()\n\treturn s.configTime\n}\n\n// Addr will return the net.Addr object for the current listener.\nfunc (s *Server) Addr() net.Addr {\n\ts.mu.RLock()\n\tdefer s.mu.RUnlock()\n\tif s.listener == nil {\n\t\treturn nil\n\t}\n\treturn s.listener.Addr()\n}\n\n// MonitorAddr will return the net.Addr object for the monitoring listener.\nfunc (s *Server) MonitorAddr() *net.TCPAddr {\n\ts.mu.RLock()\n\tdefer s.mu.RUnlock()\n\tif s.http == nil {\n\t\treturn nil\n\t}\n\treturn s.http.Addr().(*net.TCPAddr)\n}\n\n// ClusterAddr returns the net.Addr object for the route listener.\nfunc (s *Server) ClusterAddr() *net.TCPAddr {\n\ts.mu.RLock()\n\tdefer s.mu.RUnlock()\n\tif s.routeListener == nil {\n\t\treturn nil\n\t}\n\treturn s.routeListener.Addr().(*net.TCPAddr)\n}\n\n// ProfilerAddr returns the net.Addr object for the profiler listener.\nfunc (s *Server) ProfilerAddr() *net.TCPAddr {\n\ts.mu.RLock()\n\tdefer s.mu.RUnlock()\n\tif s.profiler == nil {\n\t\treturn nil\n\t}\n\treturn s.profiler.Addr().(*net.TCPAddr)\n}\n\nfunc (s *Server) readyForConnections(d time.Duration) error {\n\t// Snapshot server options.\n\topts := s.getOpts()\n\n\ttype info struct {\n\t\tok  bool\n\t\terr error\n\t}\n\tchk := make(map[string]info)\n\n\tend := time.Now().Add(d)\n\tfor time.Now().Before(end) {\n\t\ts.mu.RLock()\n\t\tchk[\"server\"] = info{ok: s.listener != nil || opts.DontListen, err: s.listenerErr}\n\t\tchk[\"route\"] = info{ok: (opts.Cluster.Port == 0 || s.routeListener != nil), err: s.routeListenerErr}\n\t\tchk[\"gateway\"] = info{ok: (opts.Gateway.Name == _EMPTY_ || s.gatewayListener != nil), err: s.gatewayListenerErr}\n\t\tchk[\"leafnode\"] = info{ok: (opts.LeafNode.Port == 0 || s.leafNodeListener != nil), err: s.leafNodeListenerErr}\n\t\tchk[\"websocket\"] = info{ok: (opts.Websocket.Port == 0 || s.websocket.listener != nil), err: s.websocket.listenerErr}\n\t\tchk[\"mqtt\"] = info{ok: (opts.MQTT.Port == 0 || s.mqtt.listener != nil), err: s.mqtt.listenerErr}\n\t\ts.mu.RUnlock()\n\n\t\tvar numOK int\n\t\tfor _, inf := range chk {\n\t\t\tif inf.ok {\n\t\t\t\tnumOK++\n\t\t\t}\n\t\t}\n\t\tif numOK == len(chk) {\n\t\t\t// In the case of DontListen option (no accept loop), we still want\n\t\t\t// to make sure that Start() has done all the work, so we wait on\n\t\t\t// that.\n\t\t\tif opts.DontListen {\n\t\t\t\tselect {\n\t\t\t\tcase <-s.startupComplete:\n\t\t\t\tcase <-time.After(d):\n\t\t\t\t\treturn fmt.Errorf(\"failed to be ready for connections after %s: startup did not complete\", d)\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn nil\n\t\t}\n\t\tif d > 25*time.Millisecond {\n\t\t\ttime.Sleep(25 * time.Millisecond)\n\t\t}\n\t}\n\n\tfailed := make([]string, 0, len(chk))\n\tfor name, inf := range chk {\n\t\tif inf.ok && inf.err != nil {\n\t\t\tfailed = append(failed, fmt.Sprintf(\"%s(ok, but %s)\", name, inf.err))\n\t\t}\n\t\tif !inf.ok && inf.err == nil {\n\t\t\tfailed = append(failed, name)\n\t\t}\n\t\tif !inf.ok && inf.err != nil {\n\t\t\tfailed = append(failed, fmt.Sprintf(\"%s(%s)\", name, inf.err))\n\t\t}\n\t}\n\n\treturn fmt.Errorf(\n\t\t\"failed to be ready for connections after %s: %s\",\n\t\td, strings.Join(failed, \", \"),\n\t)\n}\n\n// ReadyForConnections returns `true` if the server is ready to accept clients\n// and, if routing is enabled, route connections. If after the duration\n// `dur` the server is still not ready, returns `false`.\nfunc (s *Server) ReadyForConnections(dur time.Duration) bool {\n\treturn s.readyForConnections(dur) == nil\n}\n\n// Quick utility to function to tell if the server supports headers.\nfunc (s *Server) supportsHeaders() bool {\n\tif s == nil {\n\t\treturn false\n\t}\n\treturn !(s.getOpts().NoHeaderSupport)\n}\n\n// ID returns the server's ID\nfunc (s *Server) ID() string {\n\treturn s.info.ID\n}\n\n// NodeName returns the node name for this server.\nfunc (s *Server) NodeName() string {\n\treturn getHash(s.info.Name)\n}\n\n// Name returns the server's name. This will be the same as the ID if it was not set.\nfunc (s *Server) Name() string {\n\treturn s.info.Name\n}\n\nfunc (s *Server) String() string {\n\treturn s.info.Name\n}\n\ntype pprofLabels map[string]string\n\nfunc setGoRoutineLabels(tags ...pprofLabels) {\n\tvar labels []string\n\tfor _, m := range tags {\n\t\tfor k, v := range m {\n\t\t\tlabels = append(labels, k, v)\n\t\t}\n\t}\n\tif len(labels) > 0 {\n\t\tpprof.SetGoroutineLabels(\n\t\t\tpprof.WithLabels(context.Background(), pprof.Labels(labels...)),\n\t\t)\n\t}\n}\n\nfunc (s *Server) startGoRoutine(f func(), tags ...pprofLabels) bool {\n\tvar started bool\n\ts.grMu.Lock()\n\tdefer s.grMu.Unlock()\n\tif s.grRunning {\n\t\ts.grWG.Add(1)\n\t\tgo func() {\n\t\t\tsetGoRoutineLabels(tags...)\n\t\t\tf()\n\t\t}()\n\t\tstarted = true\n\t}\n\treturn started\n}\n\nfunc (s *Server) numClosedConns() int {\n\ts.mu.RLock()\n\tdefer s.mu.RUnlock()\n\treturn s.closed.len()\n}\n\nfunc (s *Server) totalClosedConns() uint64 {\n\ts.mu.RLock()\n\tdefer s.mu.RUnlock()\n\treturn s.closed.totalConns()\n}\n\nfunc (s *Server) closedClients() []*closedClient {\n\ts.mu.RLock()\n\tdefer s.mu.RUnlock()\n\treturn s.closed.closedClients()\n}\n\n// getClientConnectURLs returns suitable URLs for clients to connect to the listen\n// port based on the server options' Host and Port. If the Host corresponds to\n// \"any\" interfaces, this call returns the list of resolved IP addresses.\n// If ClientAdvertise is set, returns the client advertise host and port.\n// The server lock is assumed held on entry.\nfunc (s *Server) getClientConnectURLs() []string {\n\t// Snapshot server options.\n\topts := s.getOpts()\n\t// Ignore error here since we know that if there is client advertise, the\n\t// parseHostPort is correct because we did it right before calling this\n\t// function in Server.New().\n\turls, _ := s.getConnectURLs(opts.ClientAdvertise, opts.Host, opts.Port)\n\treturn urls\n}\n\n// Generic version that will return an array of URLs based on the given\n// advertise, host and port values.\nfunc (s *Server) getConnectURLs(advertise, host string, port int) ([]string, error) {\n\turls := make([]string, 0, 1)\n\n\t// short circuit if advertise is set\n\tif advertise != \"\" {\n\t\th, p, err := parseHostPort(advertise, port)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\turls = append(urls, net.JoinHostPort(h, strconv.Itoa(p)))\n\t} else {\n\t\tsPort := strconv.Itoa(port)\n\t\t_, ips, err := s.getNonLocalIPsIfHostIsIPAny(host, true)\n\t\tfor _, ip := range ips {\n\t\t\turls = append(urls, net.JoinHostPort(ip, sPort))\n\t\t}\n\t\tif err != nil || len(urls) == 0 {\n\t\t\t// We are here if s.opts.Host is not \"0.0.0.0\" nor \"::\", or if for some\n\t\t\t// reason we could not add any URL in the loop above.\n\t\t\t// We had a case where a Windows VM was hosed and would have err == nil\n\t\t\t// and not add any address in the array in the loop above, and we\n\t\t\t// ended-up returning 0.0.0.0, which is problematic for Windows clients.\n\t\t\t// Check for 0.0.0.0 or :: specifically, and ignore if that's the case.\n\t\t\tif host == \"0.0.0.0\" || host == \"::\" {\n\t\t\t\ts.Errorf(\"Address %q can not be resolved properly\", host)\n\t\t\t} else {\n\t\t\t\turls = append(urls, net.JoinHostPort(host, sPort))\n\t\t\t}\n\t\t}\n\t}\n\treturn urls, nil\n}\n\n// Returns an array of non local IPs if the provided host is\n// 0.0.0.0 or ::. It returns the first resolved if `all` is\n// false.\n// The boolean indicate if the provided host was 0.0.0.0 (or ::)\n// so that if the returned array is empty caller can decide\n// what to do next.\nfunc (s *Server) getNonLocalIPsIfHostIsIPAny(host string, all bool) (bool, []string, error) {\n\tip := net.ParseIP(host)\n\t// If this is not an IP, we are done\n\tif ip == nil {\n\t\treturn false, nil, nil\n\t}\n\t// If this is not 0.0.0.0 or :: we have nothing to do.\n\tif !ip.IsUnspecified() {\n\t\treturn false, nil, nil\n\t}\n\ts.Debugf(\"Get non local IPs for %q\", host)\n\tvar ips []string\n\tifaces, _ := net.Interfaces()\n\tfor _, i := range ifaces {\n\t\taddrs, _ := i.Addrs()\n\t\tfor _, addr := range addrs {\n\t\t\tswitch v := addr.(type) {\n\t\t\tcase *net.IPNet:\n\t\t\t\tip = v.IP\n\t\t\tcase *net.IPAddr:\n\t\t\t\tip = v.IP\n\t\t\t}\n\t\t\tipStr := ip.String()\n\t\t\t// Skip non global unicast addresses\n\t\t\tif !ip.IsGlobalUnicast() || ip.IsUnspecified() {\n\t\t\t\tip = nil\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\ts.Debugf(\"  ip=%s\", ipStr)\n\t\t\tips = append(ips, ipStr)\n\t\t\tif !all {\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t}\n\treturn true, ips, nil\n}\n\n// if the ip is not specified, attempt to resolve it\nfunc resolveHostPorts(addr net.Listener) []string {\n\thostPorts := make([]string, 0)\n\thp := addr.Addr().(*net.TCPAddr)\n\tport := strconv.Itoa(hp.Port)\n\tif hp.IP.IsUnspecified() {\n\t\tvar ip net.IP\n\t\tifaces, _ := net.Interfaces()\n\t\tfor _, i := range ifaces {\n\t\t\taddrs, _ := i.Addrs()\n\t\t\tfor _, addr := range addrs {\n\t\t\t\tswitch v := addr.(type) {\n\t\t\t\tcase *net.IPNet:\n\t\t\t\t\tip = v.IP\n\t\t\t\t\thostPorts = append(hostPorts, net.JoinHostPort(ip.String(), port))\n\t\t\t\tcase *net.IPAddr:\n\t\t\t\t\tip = v.IP\n\t\t\t\t\thostPorts = append(hostPorts, net.JoinHostPort(ip.String(), port))\n\t\t\t\tdefault:\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t} else {\n\t\thostPorts = append(hostPorts, net.JoinHostPort(hp.IP.String(), port))\n\t}\n\treturn hostPorts\n}\n\n// format the address of a net.Listener with a protocol\nfunc formatURL(protocol string, addr net.Listener) []string {\n\thostports := resolveHostPorts(addr)\n\tfor i, hp := range hostports {\n\t\thostports[i] = fmt.Sprintf(\"%s://%s\", protocol, hp)\n\t}\n\treturn hostports\n}\n\n// Ports describes URLs that the server can be contacted in\ntype Ports struct {\n\tNats       []string `json:\"nats,omitempty\"`\n\tMonitoring []string `json:\"monitoring,omitempty\"`\n\tCluster    []string `json:\"cluster,omitempty\"`\n\tProfile    []string `json:\"profile,omitempty\"`\n\tWebSocket  []string `json:\"websocket,omitempty\"`\n}\n\n// PortsInfo attempts to resolve all the ports. If after maxWait the ports are not\n// resolved, it returns nil. Otherwise it returns a Ports struct\n// describing ports where the server can be contacted\nfunc (s *Server) PortsInfo(maxWait time.Duration) *Ports {\n\tif s.readyForListeners(maxWait) {\n\t\topts := s.getOpts()\n\n\t\ts.mu.RLock()\n\t\ttls := s.info.TLSRequired\n\t\tlistener := s.listener\n\t\thttpListener := s.http\n\t\tclusterListener := s.routeListener\n\t\tprofileListener := s.profiler\n\t\twsListener := s.websocket.listener\n\t\twss := s.websocket.tls\n\t\ts.mu.RUnlock()\n\n\t\tports := Ports{}\n\n\t\tif listener != nil {\n\t\t\tnatsProto := \"nats\"\n\t\t\tif tls {\n\t\t\t\tnatsProto = \"tls\"\n\t\t\t}\n\t\t\tports.Nats = formatURL(natsProto, listener)\n\t\t}\n\n\t\tif httpListener != nil {\n\t\t\tmonProto := \"http\"\n\t\t\tif opts.HTTPSPort != 0 {\n\t\t\t\tmonProto = \"https\"\n\t\t\t}\n\t\t\tports.Monitoring = formatURL(monProto, httpListener)\n\t\t}\n\n\t\tif clusterListener != nil {\n\t\t\tclusterProto := \"nats\"\n\t\t\tif opts.Cluster.TLSConfig != nil {\n\t\t\t\tclusterProto = \"tls\"\n\t\t\t}\n\t\t\tports.Cluster = formatURL(clusterProto, clusterListener)\n\t\t}\n\n\t\tif profileListener != nil {\n\t\t\tports.Profile = formatURL(\"http\", profileListener)\n\t\t}\n\n\t\tif wsListener != nil {\n\t\t\tprotocol := wsSchemePrefix\n\t\t\tif wss {\n\t\t\t\tprotocol = wsSchemePrefixTLS\n\t\t\t}\n\t\t\tports.WebSocket = formatURL(protocol, wsListener)\n\t\t}\n\n\t\treturn &ports\n\t}\n\n\treturn nil\n}\n\n// Returns the portsFile. If a non-empty dirHint is provided, the dirHint\n// path is used instead of the server option value\nfunc (s *Server) portFile(dirHint string) string {\n\tdirname := s.getOpts().PortsFileDir\n\tif dirHint != \"\" {\n\t\tdirname = dirHint\n\t}\n\tif dirname == _EMPTY_ {\n\t\treturn _EMPTY_\n\t}\n\treturn filepath.Join(dirname, fmt.Sprintf(\"%s_%d.ports\", filepath.Base(os.Args[0]), os.Getpid()))\n}\n\n// Delete the ports file. If a non-empty dirHint is provided, the dirHint\n// path is used instead of the server option value\nfunc (s *Server) deletePortsFile(hintDir string) {\n\tportsFile := s.portFile(hintDir)\n\tif portsFile != \"\" {\n\t\tif err := os.Remove(portsFile); err != nil {\n\t\t\ts.Errorf(\"Error cleaning up ports file %s: %v\", portsFile, err)\n\t\t}\n\t}\n}\n\n// Writes a file with a serialized Ports to the specified ports_file_dir.\n// The name of the file is `exename_pid.ports`, typically nats-server_pid.ports.\n// if ports file is not set, this function has no effect\nfunc (s *Server) logPorts() {\n\topts := s.getOpts()\n\tportsFile := s.portFile(opts.PortsFileDir)\n\tif portsFile != _EMPTY_ {\n\t\tgo func() {\n\t\t\tinfo := s.PortsInfo(5 * time.Second)\n\t\t\tif info == nil {\n\t\t\t\ts.Errorf(\"Unable to resolve the ports in the specified time\")\n\t\t\t\treturn\n\t\t\t}\n\t\t\tdata, err := json.Marshal(info)\n\t\t\tif err != nil {\n\t\t\t\ts.Errorf(\"Error marshaling ports file: %v\", err)\n\t\t\t\treturn\n\t\t\t}\n\t\t\tif err := os.WriteFile(portsFile, data, 0666); err != nil {\n\t\t\t\ts.Errorf(\"Error writing ports file (%s): %v\", portsFile, err)\n\t\t\t\treturn\n\t\t\t}\n\n\t\t}()\n\t}\n}\n\n// waits until a calculated list of listeners is resolved or a timeout\nfunc (s *Server) readyForListeners(dur time.Duration) bool {\n\tend := time.Now().Add(dur)\n\tfor time.Now().Before(end) {\n\t\ts.mu.RLock()\n\t\tlisteners := s.serviceListeners()\n\t\ts.mu.RUnlock()\n\t\tif len(listeners) == 0 {\n\t\t\treturn false\n\t\t}\n\n\t\tok := true\n\t\tfor _, l := range listeners {\n\t\t\tif l == nil {\n\t\t\t\tok = false\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\tif ok {\n\t\t\treturn true\n\t\t}\n\t\tselect {\n\t\tcase <-s.quitCh:\n\t\t\treturn false\n\t\tcase <-time.After(25 * time.Millisecond):\n\t\t\t// continue - unable to select from quit - we are still running\n\t\t}\n\t}\n\treturn false\n}\n\n// returns a list of listeners that are intended for the process\n// if the entry is nil, the interface is yet to be resolved\nfunc (s *Server) serviceListeners() []net.Listener {\n\tlisteners := make([]net.Listener, 0)\n\topts := s.getOpts()\n\tlisteners = append(listeners, s.listener)\n\tif opts.Cluster.Port != 0 {\n\t\tlisteners = append(listeners, s.routeListener)\n\t}\n\tif opts.HTTPPort != 0 || opts.HTTPSPort != 0 {\n\t\tlisteners = append(listeners, s.http)\n\t}\n\tif opts.ProfPort != 0 {\n\t\tlisteners = append(listeners, s.profiler)\n\t}\n\tif opts.Websocket.Port != 0 {\n\t\tlisteners = append(listeners, s.websocket.listener)\n\t}\n\treturn listeners\n}\n\n// Returns true if in lame duck mode.\nfunc (s *Server) isLameDuckMode() bool {\n\ts.mu.RLock()\n\tdefer s.mu.RUnlock()\n\treturn s.ldm\n}\n\n// LameDuckShutdown will perform a lame duck shutdown of NATS, whereby\n// the client listener is closed, existing client connections are\n// kicked, Raft leaderships are transferred, JetStream is shutdown\n// and then finally shutdown the the NATS Server itself.\n// This function blocks and will not return until the NATS Server\n// has completed the entire shutdown operation.\nfunc (s *Server) LameDuckShutdown() {\n\ts.lameDuckMode()\n}\n\n// This function will close the client listener then close the clients\n// at some interval to avoid a reconnect storm.\n// We will also transfer any raft leaders and shutdown JetStream.\nfunc (s *Server) lameDuckMode() {\n\ts.mu.Lock()\n\t// Check if there is actually anything to do\n\tif s.isShuttingDown() || s.ldm || s.listener == nil {\n\t\ts.mu.Unlock()\n\t\treturn\n\t}\n\ts.Noticef(\"Entering lame duck mode, stop accepting new clients\")\n\ts.ldm = true\n\ts.sendLDMShutdownEventLocked()\n\texpected := 1\n\ts.listener.Close()\n\ts.listener = nil\n\texpected += s.closeWebsocketServer()\n\ts.ldmCh = make(chan bool, expected)\n\topts := s.getOpts()\n\tgp := opts.LameDuckGracePeriod\n\t// For tests, we want the grace period to be in some cases bigger\n\t// than the ldm duration, so to by-pass the validateOptions() check,\n\t// we use negative number and flip it here.\n\tif gp < 0 {\n\t\tgp *= -1\n\t}\n\ts.mu.Unlock()\n\n\t// If we are running any raftNodes transfer leaders.\n\tif hadTransfers := s.transferRaftLeaders(); hadTransfers {\n\t\t// They will transfer leadership quickly, but wait here for a second.\n\t\tselect {\n\t\tcase <-time.After(time.Second):\n\t\tcase <-s.quitCh:\n\t\t\treturn\n\t\t}\n\t}\n\n\t// Now check and shutdown jetstream.\n\ts.shutdownJetStream()\n\n\t// Now shutdown the nodes\n\ts.shutdownRaftNodes()\n\n\t// Wait for accept loops to be done to make sure that no new\n\t// client can connect\n\tfor i := 0; i < expected; i++ {\n\t\t<-s.ldmCh\n\t}\n\n\ts.mu.Lock()\n\t// Need to recheck few things\n\tif s.isShuttingDown() || len(s.clients) == 0 {\n\t\ts.mu.Unlock()\n\t\t// If there is no client, we need to call Shutdown() to complete\n\t\t// the LDMode. If server has been shutdown while lock was released,\n\t\t// calling Shutdown() should be no-op.\n\t\ts.Shutdown()\n\t\treturn\n\t}\n\tdur := int64(opts.LameDuckDuration)\n\tdur -= int64(gp)\n\tif dur <= 0 {\n\t\tdur = int64(time.Second)\n\t}\n\tnumClients := int64(len(s.clients))\n\tbatch := 1\n\t// Sleep interval between each client connection close.\n\tvar si int64\n\tif numClients != 0 {\n\t\tsi = dur / numClients\n\t}\n\tif si < 1 {\n\t\t// Should not happen (except in test with very small LD duration), but\n\t\t// if there are too many clients, batch the number of close and\n\t\t// use a tiny sleep interval that will result in yield likely.\n\t\tsi = 1\n\t\tbatch = int(numClients / dur)\n\t} else if si > int64(time.Second) {\n\t\t// Conversely, there is no need to sleep too long between clients\n\t\t// and spread say 10 clients for the 2min duration. Sleeping no\n\t\t// more than 1sec.\n\t\tsi = int64(time.Second)\n\t}\n\n\t// Now capture all clients\n\tclients := make([]*client, 0, len(s.clients))\n\tfor _, client := range s.clients {\n\t\tclients = append(clients, client)\n\t}\n\t// Now that we know that no new client can be accepted,\n\t// send INFO to routes and clients to notify this state.\n\ts.sendLDMToRoutes()\n\ts.sendLDMToClients()\n\ts.mu.Unlock()\n\n\tt := time.NewTimer(gp)\n\t// Delay start of closing of client connections in case\n\t// we have several servers that we want to signal to enter LD mode\n\t// and not have their client reconnect to each other.\n\tselect {\n\tcase <-t.C:\n\t\ts.Noticef(\"Closing existing clients\")\n\tcase <-s.quitCh:\n\t\tt.Stop()\n\t\treturn\n\t}\n\tfor i, client := range clients {\n\t\tclient.closeConnection(ServerShutdown)\n\t\tif i == len(clients)-1 {\n\t\t\tbreak\n\t\t}\n\t\tif batch == 1 || i%batch == 0 {\n\t\t\t// We pick a random interval which will be at least si/2\n\t\t\tv := rand.Int63n(si)\n\t\t\tif v < si/2 {\n\t\t\t\tv = si / 2\n\t\t\t}\n\t\t\tt.Reset(time.Duration(v))\n\t\t\t// Sleep for given interval or bail out if kicked by Shutdown().\n\t\t\tselect {\n\t\t\tcase <-t.C:\n\t\t\tcase <-s.quitCh:\n\t\t\t\tt.Stop()\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}\n\ts.Shutdown()\n\ts.WaitForShutdown()\n}\n\n// Send an INFO update to routes with the indication that this server is in LDM mode.\n// Server lock is held on entry.\nfunc (s *Server) sendLDMToRoutes() {\n\ts.routeInfo.LameDuckMode = true\n\tinfoJSON := generateInfoJSON(&s.routeInfo)\n\ts.forEachRemote(func(r *client) {\n\t\tr.mu.Lock()\n\t\tr.enqueueProto(infoJSON)\n\t\tr.mu.Unlock()\n\t})\n\t// Clear now so that we notify only once, should we have to send other INFOs.\n\ts.routeInfo.LameDuckMode = false\n}\n\n// Send an INFO update to clients with the indication that this server is in\n// LDM mode and with only URLs of other nodes.\n// Server lock is held on entry.\nfunc (s *Server) sendLDMToClients() {\n\ts.info.LameDuckMode = true\n\t// Clear this so that if there are further updates, we don't send our URLs.\n\ts.clientConnectURLs = s.clientConnectURLs[:0]\n\tif s.websocket.connectURLs != nil {\n\t\ts.websocket.connectURLs = s.websocket.connectURLs[:0]\n\t}\n\t// Reset content first.\n\ts.info.ClientConnectURLs = s.info.ClientConnectURLs[:0]\n\ts.info.WSConnectURLs = s.info.WSConnectURLs[:0]\n\t// Only add the other nodes if we are allowed to.\n\tif !s.getOpts().Cluster.NoAdvertise {\n\t\tfor url := range s.clientConnectURLsMap {\n\t\t\ts.info.ClientConnectURLs = append(s.info.ClientConnectURLs, url)\n\t\t}\n\t\tfor url := range s.websocket.connectURLsMap {\n\t\t\ts.info.WSConnectURLs = append(s.info.WSConnectURLs, url)\n\t\t}\n\t}\n\t// Send to all registered clients that support async INFO protocols.\n\ts.sendAsyncInfoToClients(true, true)\n\t// We now clear the info.LameDuckMode flag so that if there are\n\t// cluster updates and we send the INFO, we don't have the boolean\n\t// set which would cause multiple LDM notifications to clients.\n\ts.info.LameDuckMode = false\n}\n\n// If given error is a net.Error and is temporary, sleeps for the given\n// delay and double it, but cap it to ACCEPT_MAX_SLEEP. The sleep is\n// interrupted if the server is shutdown.\n// An error message is displayed depending on the type of error.\n// Returns the new (or unchanged) delay, or a negative value if the\n// server has been or is being shutdown.\nfunc (s *Server) acceptError(acceptName string, err error, tmpDelay time.Duration) time.Duration {\n\tif !s.isRunning() {\n\t\treturn -1\n\t}\n\t//lint:ignore SA1019 We want to retry on a bunch of errors here.\n\tif ne, ok := err.(net.Error); ok && ne.Temporary() { // nolint:staticcheck\n\t\ts.Errorf(\"Temporary %s Accept Error(%v), sleeping %dms\", acceptName, ne, tmpDelay/time.Millisecond)\n\t\tselect {\n\t\tcase <-time.After(tmpDelay):\n\t\tcase <-s.quitCh:\n\t\t\treturn -1\n\t\t}\n\t\ttmpDelay *= 2\n\t\tif tmpDelay > ACCEPT_MAX_SLEEP {\n\t\t\ttmpDelay = ACCEPT_MAX_SLEEP\n\t\t}\n\t} else {\n\t\ts.Errorf(\"%s Accept error: %v\", acceptName, err)\n\t}\n\treturn tmpDelay\n}\n\nvar errNoIPAvail = errors.New(\"no IP available\")\n\nfunc (s *Server) getRandomIP(resolver netResolver, url string, excludedAddresses map[string]struct{}) (string, error) {\n\thost, port, err := net.SplitHostPort(url)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\t// If already an IP, skip.\n\tif net.ParseIP(host) != nil {\n\t\treturn url, nil\n\t}\n\tips, err := resolver.LookupHost(context.Background(), host)\n\tif err != nil {\n\t\treturn \"\", fmt.Errorf(\"lookup for host %q: %v\", host, err)\n\t}\n\tif len(excludedAddresses) > 0 {\n\t\tfor i := 0; i < len(ips); i++ {\n\t\t\tip := ips[i]\n\t\t\taddr := net.JoinHostPort(ip, port)\n\t\t\tif _, excluded := excludedAddresses[addr]; excluded {\n\t\t\t\tif len(ips) == 1 {\n\t\t\t\t\tips = nil\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t\tips[i] = ips[len(ips)-1]\n\t\t\t\tips = ips[:len(ips)-1]\n\t\t\t\ti--\n\t\t\t}\n\t\t}\n\t\tif len(ips) == 0 {\n\t\t\treturn \"\", errNoIPAvail\n\t\t}\n\t}\n\tvar address string\n\tif len(ips) == 0 {\n\t\ts.Warnf(\"Unable to get IP for %s, will try with %s: %v\", host, url, err)\n\t\taddress = url\n\t} else {\n\t\tvar ip string\n\t\tif len(ips) == 1 {\n\t\t\tip = ips[0]\n\t\t} else {\n\t\t\tip = ips[rand.Int31n(int32(len(ips)))]\n\t\t}\n\t\t// add the port\n\t\taddress = net.JoinHostPort(ip, port)\n\t}\n\treturn address, nil\n}\n\n// Returns true for the first attempt and depending on the nature\n// of the attempt (first connect or a reconnect), when the number\n// of attempts is equal to the configured report attempts.\nfunc (s *Server) shouldReportConnectErr(firstConnect bool, attempts int) bool {\n\topts := s.getOpts()\n\tif firstConnect {\n\t\tif attempts == 1 || attempts%opts.ConnectErrorReports == 0 {\n\t\t\treturn true\n\t\t}\n\t\treturn false\n\t}\n\tif attempts == 1 || attempts%opts.ReconnectErrorReports == 0 {\n\t\treturn true\n\t}\n\treturn false\n}\n\nfunc (s *Server) updateRemoteSubscription(acc *Account, sub *subscription, delta int32) {\n\ts.updateRouteSubscriptionMap(acc, sub, delta)\n\tif s.gateway.enabled {\n\t\ts.gatewayUpdateSubInterest(acc.Name, sub, delta)\n\t}\n\n\tacc.updateLeafNodes(sub, delta)\n}\n\nfunc (s *Server) startRateLimitLogExpiration() {\n\tinterval := time.Second\n\ts.startGoRoutine(func() {\n\t\tdefer s.grWG.Done()\n\n\t\tticker := time.NewTicker(time.Second)\n\t\tdefer ticker.Stop()\n\t\tfor {\n\t\t\tselect {\n\t\t\tcase <-s.quitCh:\n\t\t\t\treturn\n\t\t\tcase interval = <-s.rateLimitLoggingCh:\n\t\t\t\tticker.Reset(interval)\n\t\t\tcase <-ticker.C:\n\t\t\t\ts.rateLimitLogging.Range(func(k, v any) bool {\n\t\t\t\t\tstart := v.(time.Time)\n\t\t\t\t\tif time.Since(start) >= interval {\n\t\t\t\t\t\ts.rateLimitLogging.Delete(k)\n\t\t\t\t\t}\n\t\t\t\t\treturn true\n\t\t\t\t})\n\t\t\t}\n\t\t}\n\t})\n}\n\nfunc (s *Server) changeRateLimitLogInterval(d time.Duration) {\n\tif d <= 0 {\n\t\treturn\n\t}\n\tselect {\n\tcase s.rateLimitLoggingCh <- d:\n\tdefault:\n\t}\n}\n\n// DisconnectClientByID disconnects a client by connection ID\nfunc (s *Server) DisconnectClientByID(id uint64) error {\n\tif s == nil {\n\t\treturn ErrServerNotRunning\n\t}\n\tif client := s.getClient(id); client != nil {\n\t\tclient.closeConnection(Kicked)\n\t\treturn nil\n\t} else if client = s.GetLeafNode(id); client != nil {\n\t\tclient.closeConnection(Kicked)\n\t\treturn nil\n\t}\n\treturn errors.New(\"no such client or leafnode id\")\n}\n\n// LDMClientByID sends a Lame Duck Mode info message to a client by connection ID\nfunc (s *Server) LDMClientByID(id uint64) error {\n\tif s == nil {\n\t\treturn ErrServerNotRunning\n\t}\n\ts.mu.RLock()\n\tc := s.clients[id]\n\tif c == nil {\n\t\ts.mu.RUnlock()\n\t\treturn errors.New(\"no such client id\")\n\t}\n\tinfo := s.copyInfo()\n\tinfo.LameDuckMode = true\n\ts.mu.RUnlock()\n\tc.mu.Lock()\n\tdefer c.mu.Unlock()\n\tif c.opts.Protocol >= ClientProtoInfo && c.flags.isSet(firstPongSent) {\n\t\t// sendInfo takes care of checking if the connection is still\n\t\t// valid or not, so don't duplicate tests here.\n\t\tc.Debugf(\"Sending Lame Duck Mode info to client\")\n\t\tc.enqueueProto(c.generateClientInfoJSON(info))\n\t\treturn nil\n\t} else {\n\t\treturn errors.New(\"client does not support Lame Duck Mode or is not ready to receive the notification\")\n\t}\n}\n",
    "source_file": "server/server.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2024-2025 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage server\n\nimport \"strconv\"\n\nconst (\n\t// JSApiLevel is the maximum supported JetStream API level for this server.\n\tJSApiLevel int = 2\n\n\tJSRequiredLevelMetadataKey = \"_nats.req.level\"\n\tJSServerVersionMetadataKey = \"_nats.ver\"\n\tJSServerLevelMetadataKey   = \"_nats.level\"\n)\n\n// setStaticStreamMetadata sets JetStream stream metadata, like the server version and API level.\n// Any dynamic metadata is removed, it must not be stored and only be added for responses.\nfunc setStaticStreamMetadata(cfg *StreamConfig) {\n\tif cfg.Metadata == nil {\n\t\tcfg.Metadata = make(map[string]string)\n\t} else {\n\t\tdeleteDynamicMetadata(cfg.Metadata)\n\t}\n\n\tvar requiredApiLevel int\n\trequires := func(level int) {\n\t\tif level > requiredApiLevel {\n\t\t\trequiredApiLevel = level\n\t\t}\n\t}\n\n\t// TTLs were added in v2.11 and require API level 1.\n\tif cfg.AllowMsgTTL || cfg.SubjectDeleteMarkerTTL > 0 {\n\t\trequires(1)\n\t}\n\n\tcfg.Metadata[JSRequiredLevelMetadataKey] = strconv.Itoa(requiredApiLevel)\n}\n\n// setDynamicStreamMetadata adds dynamic fields into the (copied) metadata.\nfunc setDynamicStreamMetadata(cfg *StreamConfig) *StreamConfig {\n\tnewCfg := *cfg\n\tnewCfg.Metadata = make(map[string]string)\n\tfor key, value := range cfg.Metadata {\n\t\tnewCfg.Metadata[key] = value\n\t}\n\tnewCfg.Metadata[JSServerVersionMetadataKey] = VERSION\n\tnewCfg.Metadata[JSServerLevelMetadataKey] = strconv.Itoa(JSApiLevel)\n\treturn &newCfg\n}\n\n// copyConsumerMetadata copies versioning fields from metadata of prevCfg into cfg.\n// Removes versioning fields if no previous metadata, updates if set, and removes fields if it doesn't exist in prevCfg.\n// Any dynamic metadata is removed, it must not be stored and only be added for responses.\n//\n// Note: useful when doing equality checks on cfg and prevCfg, but ignoring any versioning metadata differences.\nfunc copyStreamMetadata(cfg *StreamConfig, prevCfg *StreamConfig) {\n\tif cfg.Metadata != nil {\n\t\tdeleteDynamicMetadata(cfg.Metadata)\n\t}\n\tsetOrDeleteInStreamMetadata(cfg, prevCfg, JSRequiredLevelMetadataKey)\n}\n\n// setOrDeleteInConsumerMetadata sets field with key/value in metadata of cfg if set, deletes otherwise.\nfunc setOrDeleteInStreamMetadata(cfg *StreamConfig, prevCfg *StreamConfig, key string) {\n\tif prevCfg != nil && prevCfg.Metadata != nil {\n\t\tif value, ok := prevCfg.Metadata[key]; ok {\n\t\t\tif cfg.Metadata == nil {\n\t\t\t\tcfg.Metadata = make(map[string]string)\n\t\t\t}\n\t\t\tcfg.Metadata[key] = value\n\t\t\treturn\n\t\t}\n\t}\n\tdelete(cfg.Metadata, key)\n\tif len(cfg.Metadata) == 0 {\n\t\tcfg.Metadata = nil\n\t}\n}\n\n// setStaticConsumerMetadata sets JetStream consumer metadata, like the server version and API level.\n// Any dynamic metadata is removed, it must not be stored and only be added for responses.\nfunc setStaticConsumerMetadata(cfg *ConsumerConfig) {\n\tif cfg.Metadata == nil {\n\t\tcfg.Metadata = make(map[string]string)\n\t} else {\n\t\tdeleteDynamicMetadata(cfg.Metadata)\n\t}\n\n\tvar requiredApiLevel int\n\trequires := func(level int) {\n\t\tif level > requiredApiLevel {\n\t\t\trequiredApiLevel = level\n\t\t}\n\t}\n\n\t// Added in 2.11, absent | zero is the feature is not used.\n\t// one could be stricter and say even if its set but the time\n\t// has already passed it is also not needed to restore the consumer\n\tif cfg.PauseUntil != nil && !cfg.PauseUntil.IsZero() {\n\t\trequires(1)\n\t}\n\n\tif cfg.PriorityPolicy != PriorityNone || cfg.PinnedTTL != 0 || len(cfg.PriorityGroups) > 0 {\n\t\trequires(1)\n\t}\n\n\tcfg.Metadata[JSRequiredLevelMetadataKey] = strconv.Itoa(requiredApiLevel)\n}\n\n// setDynamicConsumerMetadata adds dynamic fields into the (copied) metadata.\nfunc setDynamicConsumerMetadata(cfg *ConsumerConfig) *ConsumerConfig {\n\tnewCfg := *cfg\n\tnewCfg.Metadata = make(map[string]string)\n\tfor key, value := range cfg.Metadata {\n\t\tnewCfg.Metadata[key] = value\n\t}\n\tnewCfg.Metadata[JSServerVersionMetadataKey] = VERSION\n\tnewCfg.Metadata[JSServerLevelMetadataKey] = strconv.Itoa(JSApiLevel)\n\treturn &newCfg\n}\n\n// setDynamicConsumerInfoMetadata adds dynamic fields into the (copied) metadata.\nfunc setDynamicConsumerInfoMetadata(info *ConsumerInfo) *ConsumerInfo {\n\tif info == nil {\n\t\treturn nil\n\t}\n\n\tnewInfo := *info\n\tcfg := setDynamicConsumerMetadata(info.Config)\n\tnewInfo.Config = cfg\n\treturn &newInfo\n}\n\n// copyConsumerMetadata copies versioning fields from metadata of prevCfg into cfg.\n// Removes versioning fields if no previous metadata, updates if set, and removes fields if it doesn't exist in prevCfg.\n// Any dynamic metadata is removed, it must not be stored and only be added for responses.\n//\n// Note: useful when doing equality checks on cfg and prevCfg, but ignoring any versioning metadata differences.\nfunc copyConsumerMetadata(cfg *ConsumerConfig, prevCfg *ConsumerConfig) {\n\tif cfg.Metadata != nil {\n\t\tdeleteDynamicMetadata(cfg.Metadata)\n\t}\n\tsetOrDeleteInConsumerMetadata(cfg, prevCfg, JSRequiredLevelMetadataKey)\n}\n\n// setOrDeleteInConsumerMetadata sets field with key/value in metadata of cfg if set, deletes otherwise.\nfunc setOrDeleteInConsumerMetadata(cfg *ConsumerConfig, prevCfg *ConsumerConfig, key string) {\n\tif prevCfg != nil && prevCfg.Metadata != nil {\n\t\tif value, ok := prevCfg.Metadata[key]; ok {\n\t\t\tif cfg.Metadata == nil {\n\t\t\t\tcfg.Metadata = make(map[string]string)\n\t\t\t}\n\t\t\tcfg.Metadata[key] = value\n\t\t\treturn\n\t\t}\n\t}\n\tdelete(cfg.Metadata, key)\n\tif len(cfg.Metadata) == 0 {\n\t\tcfg.Metadata = nil\n\t}\n}\n\n// deleteDynamicMetadata deletes dynamic fields from the metadata.\nfunc deleteDynamicMetadata(metadata map[string]string) {\n\tdelete(metadata, JSServerVersionMetadataKey)\n\tdelete(metadata, JSServerLevelMetadataKey)\n}\n",
    "source_file": "server/jetstream_versioning.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2012-2025 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n//go:build !windows\n\npackage server\n\n// Run starts the NATS server. This wrapper function allows Windows to add a\n// hook for running NATS as a service.\nfunc Run(server *Server) error {\n\tserver.Start()\n\treturn nil\n}\n\n// isWindowsService indicates if NATS is running as a Windows service.\nfunc isWindowsService() bool {\n\treturn false\n}\n",
    "source_file": "server/service.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2012-2025 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage server\n\nimport (\n\t\"crypto/sha256\"\n\t\"crypto/subtle\"\n\t\"crypto/tls\"\n\t\"crypto/x509/pkix\"\n\t\"encoding/asn1\"\n\t\"encoding/base64\"\n\t\"encoding/hex\"\n\t\"fmt\"\n\t\"net\"\n\t\"net/url\"\n\t\"regexp\"\n\t\"strings\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t\"github.com/nats-io/jwt/v2\"\n\t\"github.com/nats-io/nats-server/v2/internal/ldap\"\n\t\"github.com/nats-io/nkeys\"\n\t\"golang.org/x/crypto/bcrypt\"\n)\n\n// Authentication is an interface for implementing authentication\ntype Authentication interface {\n\t// Check if a client is authorized to connect\n\tCheck(c ClientAuthentication) bool\n}\n\n// ClientAuthentication is an interface for client authentication\ntype ClientAuthentication interface {\n\t// GetOpts gets options associated with a client\n\tGetOpts() *ClientOpts\n\t// GetTLSConnectionState if TLS is enabled, TLS ConnectionState, nil otherwise\n\tGetTLSConnectionState() *tls.ConnectionState\n\t// RegisterUser optionally map a user after auth.\n\tRegisterUser(*User)\n\t// RemoteAddress expose the connection information of the client\n\tRemoteAddress() net.Addr\n\t// GetNonce is the nonce presented to the user in the INFO line\n\tGetNonce() []byte\n\t// Kind indicates what type of connection this is matching defined constants like CLIENT, ROUTER, GATEWAY, LEAF etc\n\tKind() int\n}\n\n// NkeyUser is for multiple nkey based users\ntype NkeyUser struct {\n\tNkey                   string              `json:\"user\"`\n\tIssued                 int64               `json:\"issued,omitempty\"` // this is a copy of the issued at (iat) field in the jwt\n\tPermissions            *Permissions        `json:\"permissions,omitempty\"`\n\tAccount                *Account            `json:\"account,omitempty\"`\n\tSigningKey             string              `json:\"signing_key,omitempty\"`\n\tAllowedConnectionTypes map[string]struct{} `json:\"connection_types,omitempty\"`\n}\n\n// User is for multiple accounts/users.\ntype User struct {\n\tUsername               string              `json:\"user\"`\n\tPassword               string              `json:\"password\"`\n\tPermissions            *Permissions        `json:\"permissions,omitempty\"`\n\tAccount                *Account            `json:\"account,omitempty\"`\n\tConnectionDeadline     time.Time           `json:\"connection_deadline,omitempty\"`\n\tAllowedConnectionTypes map[string]struct{} `json:\"connection_types,omitempty\"`\n}\n\n// clone performs a deep copy of the User struct, returning a new clone with\n// all values copied.\nfunc (u *User) clone() *User {\n\tif u == nil {\n\t\treturn nil\n\t}\n\tclone := &User{}\n\t*clone = *u\n\t// Account is not cloned because it is always by reference to an existing struct.\n\tclone.Permissions = u.Permissions.clone()\n\n\tif u.AllowedConnectionTypes != nil {\n\t\tclone.AllowedConnectionTypes = make(map[string]struct{})\n\t\tfor k, v := range u.AllowedConnectionTypes {\n\t\t\tclone.AllowedConnectionTypes[k] = v\n\t\t}\n\t}\n\n\treturn clone\n}\n\n// clone performs a deep copy of the NkeyUser struct, returning a new clone with\n// all values copied.\nfunc (n *NkeyUser) clone() *NkeyUser {\n\tif n == nil {\n\t\treturn nil\n\t}\n\tclone := &NkeyUser{}\n\t*clone = *n\n\t// Account is not cloned because it is always by reference to an existing struct.\n\tclone.Permissions = n.Permissions.clone()\n\n\tif n.AllowedConnectionTypes != nil {\n\t\tclone.AllowedConnectionTypes = make(map[string]struct{})\n\t\tfor k, v := range n.AllowedConnectionTypes {\n\t\t\tclone.AllowedConnectionTypes[k] = v\n\t\t}\n\t}\n\n\treturn clone\n}\n\n// SubjectPermission is an individual allow and deny struct for publish\n// and subscribe authorizations.\ntype SubjectPermission struct {\n\tAllow []string `json:\"allow,omitempty\"`\n\tDeny  []string `json:\"deny,omitempty\"`\n}\n\n// ResponsePermission can be used to allow responses to any reply subject\n// that is received on a valid subscription.\ntype ResponsePermission struct {\n\tMaxMsgs int           `json:\"max\"`\n\tExpires time.Duration `json:\"ttl\"`\n}\n\n// Permissions are the allowed subjects on a per\n// publish or subscribe basis.\ntype Permissions struct {\n\tPublish   *SubjectPermission  `json:\"publish\"`\n\tSubscribe *SubjectPermission  `json:\"subscribe\"`\n\tResponse  *ResponsePermission `json:\"responses,omitempty\"`\n}\n\n// RoutePermissions are similar to user permissions\n// but describe what a server can import/export from and to\n// another server.\ntype RoutePermissions struct {\n\tImport *SubjectPermission `json:\"import\"`\n\tExport *SubjectPermission `json:\"export\"`\n}\n\n// clone will clone an individual subject permission.\nfunc (p *SubjectPermission) clone() *SubjectPermission {\n\tif p == nil {\n\t\treturn nil\n\t}\n\tclone := &SubjectPermission{}\n\tif p.Allow != nil {\n\t\tclone.Allow = make([]string, len(p.Allow))\n\t\tcopy(clone.Allow, p.Allow)\n\t}\n\tif p.Deny != nil {\n\t\tclone.Deny = make([]string, len(p.Deny))\n\t\tcopy(clone.Deny, p.Deny)\n\t}\n\treturn clone\n}\n\n// clone performs a deep copy of the Permissions struct, returning a new clone\n// with all values copied.\nfunc (p *Permissions) clone() *Permissions {\n\tif p == nil {\n\t\treturn nil\n\t}\n\tclone := &Permissions{}\n\tif p.Publish != nil {\n\t\tclone.Publish = p.Publish.clone()\n\t}\n\tif p.Subscribe != nil {\n\t\tclone.Subscribe = p.Subscribe.clone()\n\t}\n\tif p.Response != nil {\n\t\tclone.Response = &ResponsePermission{\n\t\t\tMaxMsgs: p.Response.MaxMsgs,\n\t\t\tExpires: p.Response.Expires,\n\t\t}\n\t}\n\treturn clone\n}\n\n// checkAuthforWarnings will look for insecure settings and log concerns.\n// Lock is assumed held.\nfunc (s *Server) checkAuthforWarnings() {\n\twarn := false\n\topts := s.getOpts()\n\tif opts.Password != _EMPTY_ && !isBcrypt(opts.Password) {\n\t\twarn = true\n\t}\n\tfor _, u := range s.users {\n\t\t// Skip warn if using TLS certs based auth\n\t\t// unless a password has been left in the config.\n\t\tif u.Password == _EMPTY_ && opts.TLSMap {\n\t\t\tcontinue\n\t\t}\n\t\t// Check if this is our internal sys client created on the fly.\n\t\tif s.sysAccOnlyNoAuthUser != _EMPTY_ && u.Username == s.sysAccOnlyNoAuthUser {\n\t\t\tcontinue\n\t\t}\n\t\tif !isBcrypt(u.Password) {\n\t\t\twarn = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif warn {\n\t\t// Warning about using plaintext passwords.\n\t\ts.Warnf(\"Plaintext passwords detected, use nkeys or bcrypt\")\n\t}\n}\n\n// If Users or Nkeys options have definitions without an account defined,\n// assign them to the default global account.\n// Lock should be held.\nfunc (s *Server) assignGlobalAccountToOrphanUsers(nkeys map[string]*NkeyUser, users map[string]*User) {\n\tfor _, u := range users {\n\t\tif u.Account == nil {\n\t\t\tu.Account = s.gacc\n\t\t}\n\t}\n\tfor _, u := range nkeys {\n\t\tif u.Account == nil {\n\t\t\tu.Account = s.gacc\n\t\t}\n\t}\n}\n\n// If the given permissions has a ResponsePermission\n// set, ensure that defaults are set (if values are 0)\n// and that a Publish permission is set, and Allow\n// is disabled if not explicitly set.\nfunc validateResponsePermissions(p *Permissions) {\n\tif p == nil || p.Response == nil {\n\t\treturn\n\t}\n\tif p.Publish == nil {\n\t\tp.Publish = &SubjectPermission{}\n\t}\n\tif p.Publish.Allow == nil {\n\t\t// We turn off the blanket allow statement.\n\t\tp.Publish.Allow = []string{}\n\t}\n\t// If there is a response permission, ensure\n\t// that if value is 0, we set the default value.\n\tif p.Response.MaxMsgs == 0 {\n\t\tp.Response.MaxMsgs = DEFAULT_ALLOW_RESPONSE_MAX_MSGS\n\t}\n\tif p.Response.Expires == 0 {\n\t\tp.Response.Expires = DEFAULT_ALLOW_RESPONSE_EXPIRATION\n\t}\n}\n\n// configureAuthorization will do any setup needed for authorization.\n// Lock is assumed held.\nfunc (s *Server) configureAuthorization() {\n\topts := s.getOpts()\n\tif opts == nil {\n\t\treturn\n\t}\n\n\t// Check for multiple users first\n\t// This just checks and sets up the user map if we have multiple users.\n\tif opts.CustomClientAuthentication != nil {\n\t\ts.info.AuthRequired = true\n\t} else if s.trustedKeys != nil {\n\t\ts.info.AuthRequired = true\n\t} else if opts.Nkeys != nil || opts.Users != nil {\n\t\ts.nkeys, s.users = s.buildNkeysAndUsersFromOptions(opts.Nkeys, opts.Users)\n\t\ts.info.AuthRequired = true\n\t} else if opts.Username != _EMPTY_ || opts.Authorization != _EMPTY_ {\n\t\ts.info.AuthRequired = true\n\t} else {\n\t\ts.users = nil\n\t\ts.nkeys = nil\n\t\ts.info.AuthRequired = false\n\t}\n\n\t// Do similar for websocket config\n\ts.wsConfigAuth(&opts.Websocket)\n\t// And for mqtt config\n\ts.mqttConfigAuth(&opts.MQTT)\n\n\t// Check for server configured auth callouts.\n\tif opts.AuthCallout != nil {\n\t\ts.mu.Unlock()\n\t\t// Give operator log entries if not valid account and auth_users.\n\t\t_, err := s.lookupAccount(opts.AuthCallout.Account)\n\t\ts.mu.Lock()\n\t\tif err != nil {\n\t\t\ts.Errorf(\"Authorization callout account %q not valid\", opts.AuthCallout.Account)\n\t\t}\n\t\tfor _, u := range opts.AuthCallout.AuthUsers {\n\t\t\t// Check for user in users and nkeys since this is server config.\n\t\t\tvar found bool\n\t\t\tif len(s.users) > 0 {\n\t\t\t\t_, found = s.users[u]\n\t\t\t}\n\t\t\tif !found && len(s.nkeys) > 0 {\n\t\t\t\t_, found = s.nkeys[u]\n\t\t\t}\n\t\t\tif !found {\n\t\t\t\ts.Errorf(\"Authorization callout user %q not valid: %v\", u, err)\n\t\t\t}\n\t\t}\n\t}\n}\n\n// Takes the given slices of NkeyUser and User options and build\n// corresponding maps used by the server. The users are cloned\n// so that server does not reference options.\n// The global account is assigned to users that don't have an\n// existing account.\n// Server lock is held on entry.\nfunc (s *Server) buildNkeysAndUsersFromOptions(nko []*NkeyUser, uo []*User) (map[string]*NkeyUser, map[string]*User) {\n\tvar nkeys map[string]*NkeyUser\n\tvar users map[string]*User\n\n\tif nko != nil {\n\t\tnkeys = make(map[string]*NkeyUser, len(nko))\n\t\tfor _, u := range nko {\n\t\t\tcopy := u.clone()\n\t\t\tif u.Account != nil {\n\t\t\t\tif v, ok := s.accounts.Load(u.Account.Name); ok {\n\t\t\t\t\tcopy.Account = v.(*Account)\n\t\t\t\t}\n\t\t\t}\n\t\t\tif copy.Permissions != nil {\n\t\t\t\tvalidateResponsePermissions(copy.Permissions)\n\t\t\t}\n\t\t\tnkeys[u.Nkey] = copy\n\t\t}\n\t}\n\tif uo != nil {\n\t\tusers = make(map[string]*User, len(uo))\n\t\tfor _, u := range uo {\n\t\t\tcopy := u.clone()\n\t\t\tif u.Account != nil {\n\t\t\t\tif v, ok := s.accounts.Load(u.Account.Name); ok {\n\t\t\t\t\tcopy.Account = v.(*Account)\n\t\t\t\t}\n\t\t\t}\n\t\t\tif copy.Permissions != nil {\n\t\t\t\tvalidateResponsePermissions(copy.Permissions)\n\t\t\t}\n\t\t\tusers[u.Username] = copy\n\t\t}\n\t}\n\ts.assignGlobalAccountToOrphanUsers(nkeys, users)\n\treturn nkeys, users\n}\n\n// checkAuthentication will check based on client type and\n// return boolean indicating if client is authorized.\nfunc (s *Server) checkAuthentication(c *client) bool {\n\tswitch c.kind {\n\tcase CLIENT:\n\t\treturn s.isClientAuthorized(c)\n\tcase ROUTER:\n\t\treturn s.isRouterAuthorized(c)\n\tcase GATEWAY:\n\t\treturn s.isGatewayAuthorized(c)\n\tcase LEAF:\n\t\treturn s.isLeafNodeAuthorized(c)\n\tdefault:\n\t\treturn false\n\t}\n}\n\n// isClientAuthorized will check the client against the proper authorization method and data.\n// This could be nkey, token, or username/password based.\nfunc (s *Server) isClientAuthorized(c *client) bool {\n\topts := s.getOpts()\n\n\t// Check custom auth first, then jwts, then nkeys, then\n\t// multiple users with TLS map if enabled, then token,\n\t// then single user/pass.\n\tif opts.CustomClientAuthentication != nil && !opts.CustomClientAuthentication.Check(c) {\n\t\treturn false\n\t}\n\n\tif opts.CustomClientAuthentication == nil && !s.processClientOrLeafAuthentication(c, opts) {\n\t\treturn false\n\t}\n\n\tif c.kind == CLIENT || c.kind == LEAF {\n\t\t// Generate an event if we have a system account.\n\t\ts.accountConnectEvent(c)\n\t}\n\n\treturn true\n}\n\n// returns false if the client needs to be disconnected\nfunc (c *client) matchesPinnedCert(tlsPinnedCerts PinnedCertSet) bool {\n\tif tlsPinnedCerts == nil {\n\t\treturn true\n\t}\n\ttlsState := c.GetTLSConnectionState()\n\tif tlsState == nil || len(tlsState.PeerCertificates) == 0 || tlsState.PeerCertificates[0] == nil {\n\t\tc.Debugf(\"Failed pinned cert test as client did not provide a certificate\")\n\t\treturn false\n\t}\n\tsha := sha256.Sum256(tlsState.PeerCertificates[0].RawSubjectPublicKeyInfo)\n\tkeyId := hex.EncodeToString(sha[:])\n\tif _, ok := tlsPinnedCerts[keyId]; !ok {\n\t\tc.Debugf(\"Failed pinned cert test for key id: %s\", keyId)\n\t\treturn false\n\t}\n\treturn true\n}\n\nvar (\n\tmustacheRE = regexp.MustCompile(`{{2}([^}]+)}{2}`)\n)\n\nfunc processUserPermissionsTemplate(lim jwt.UserPermissionLimits, ujwt *jwt.UserClaims, acc *Account) (jwt.UserPermissionLimits, error) {\n\tnArrayCartesianProduct := func(a ...[]string) [][]string {\n\t\tc := 1\n\t\tfor _, a := range a {\n\t\t\tc *= len(a)\n\t\t}\n\t\tif c == 0 {\n\t\t\treturn nil\n\t\t}\n\t\tp := make([][]string, c)\n\t\tb := make([]string, c*len(a))\n\t\tn := make([]int, len(a))\n\t\ts := 0\n\t\tfor i := range p {\n\t\t\te := s + len(a)\n\t\t\tpi := b[s:e]\n\t\t\tp[i] = pi\n\t\t\ts = e\n\t\t\tfor j, n := range n {\n\t\t\t\tpi[j] = a[j][n]\n\t\t\t}\n\t\t\tfor j := len(n) - 1; j >= 0; j-- {\n\t\t\t\tn[j]++\n\t\t\t\tif n[j] < len(a[j]) {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t\tn[j] = 0\n\t\t\t}\n\t\t}\n\t\treturn p\n\t}\n\tisTag := func(op string) []string {\n\t\tif strings.EqualFold(\"tag(\", op[:4]) && strings.HasSuffix(op, \")\") {\n\t\t\tv := strings.TrimPrefix(op, \"tag(\")\n\t\t\tv = strings.TrimSuffix(v, \")\")\n\t\t\treturn []string{\"tag\", v}\n\t\t} else if strings.EqualFold(\"account-tag(\", op[:12]) && strings.HasSuffix(op, \")\") {\n\t\t\tv := strings.TrimPrefix(op, \"account-tag(\")\n\t\t\tv = strings.TrimSuffix(v, \")\")\n\t\t\treturn []string{\"account-tag\", v}\n\t\t}\n\t\treturn nil\n\t}\n\tapplyTemplate := func(list jwt.StringList, failOnBadSubject bool) (jwt.StringList, error) {\n\t\tfound := false\n\tFOR_FIND:\n\t\tfor i := 0; i < len(list); i++ {\n\t\t\t// check if templates are present\n\t\t\tif mustacheRE.MatchString(list[i]) {\n\t\t\t\tfound = true\n\t\t\t\tbreak FOR_FIND\n\t\t\t}\n\t\t}\n\t\tif !found {\n\t\t\treturn list, nil\n\t\t}\n\t\t// process the templates\n\t\temittedList := make([]string, 0, len(list))\n\t\tfor i := 0; i < len(list); i++ {\n\t\t\t// find all the templates {{}} in this acl\n\t\t\ttokens := mustacheRE.FindAllString(list[i], -1)\n\t\t\tsrcs := make([]string, len(tokens))\n\t\t\tvalues := make([][]string, len(tokens))\n\t\t\thasTags := false\n\t\t\tfor tokenNum, tk := range tokens {\n\t\t\t\tsrcs[tokenNum] = tk\n\t\t\t\top := strings.TrimSpace(strings.TrimSuffix(strings.TrimPrefix(tk, \"{{\"), \"}}\"))\n\t\t\t\tif strings.EqualFold(\"name()\", op) {\n\t\t\t\t\tvalues[tokenNum] = []string{ujwt.Name}\n\t\t\t\t} else if strings.EqualFold(\"subject()\", op) {\n\t\t\t\t\tvalues[tokenNum] = []string{ujwt.Subject}\n\t\t\t\t} else if strings.EqualFold(\"account-name()\", op) {\n\t\t\t\t\tacc.mu.RLock()\n\t\t\t\t\tvalues[tokenNum] = []string{acc.nameTag}\n\t\t\t\t\tacc.mu.RUnlock()\n\t\t\t\t} else if strings.EqualFold(\"account-subject()\", op) {\n\t\t\t\t\t// this always has an issuer account since this is a scoped signer\n\t\t\t\t\tvalues[tokenNum] = []string{ujwt.IssuerAccount}\n\t\t\t\t} else if isTag(op) != nil {\n\t\t\t\t\thasTags = true\n\t\t\t\t\tmatch := isTag(op)\n\t\t\t\t\tvar tags jwt.TagList\n\t\t\t\t\tif match[0] == \"account-tag\" {\n\t\t\t\t\t\tacc.mu.RLock()\n\t\t\t\t\t\ttags = acc.tags\n\t\t\t\t\t\tacc.mu.RUnlock()\n\t\t\t\t\t} else {\n\t\t\t\t\t\ttags = ujwt.Tags\n\t\t\t\t\t}\n\t\t\t\t\ttagPrefix := fmt.Sprintf(\"%s:\", strings.ToLower(match[1]))\n\t\t\t\t\tvar valueList []string\n\t\t\t\t\tfor _, tag := range tags {\n\t\t\t\t\t\tif strings.HasPrefix(tag, tagPrefix) {\n\t\t\t\t\t\t\ttagValue := strings.TrimPrefix(tag, tagPrefix)\n\t\t\t\t\t\t\tvalueList = append(valueList, tagValue)\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tif len(valueList) != 0 {\n\t\t\t\t\t\tvalues[tokenNum] = valueList\n\t\t\t\t\t} else if failOnBadSubject {\n\t\t\t\t\t\treturn nil, fmt.Errorf(\"generated invalid subject %q: %q is not defined\", list[i], match[1])\n\t\t\t\t\t} else {\n\t\t\t\t\t\t// generate an invalid subject?\n\t\t\t\t\t\tvalues[tokenNum] = []string{\" \"}\n\t\t\t\t\t}\n\t\t\t\t} else if failOnBadSubject {\n\t\t\t\t\treturn nil, fmt.Errorf(\"template operation in %q: %q is not defined\", list[i], op)\n\t\t\t\t}\n\t\t\t}\n\t\t\tif !hasTags {\n\t\t\t\tsubj := list[i]\n\t\t\t\tfor idx, m := range srcs {\n\t\t\t\t\tsubj = strings.Replace(subj, m, values[idx][0], -1)\n\t\t\t\t}\n\t\t\t\tif IsValidSubject(subj) {\n\t\t\t\t\temittedList = append(emittedList, subj)\n\t\t\t\t} else if failOnBadSubject {\n\t\t\t\t\treturn nil, fmt.Errorf(\"generated invalid subject\")\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\ta := nArrayCartesianProduct(values...)\n\t\t\t\tfor _, aa := range a {\n\t\t\t\t\tsubj := list[i]\n\t\t\t\t\tfor j := 0; j < len(srcs); j++ {\n\t\t\t\t\t\tsubj = strings.Replace(subj, srcs[j], aa[j], -1)\n\t\t\t\t\t}\n\t\t\t\t\tif IsValidSubject(subj) {\n\t\t\t\t\t\temittedList = append(emittedList, subj)\n\t\t\t\t\t} else if failOnBadSubject {\n\t\t\t\t\t\treturn nil, fmt.Errorf(\"generated invalid subject\")\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\treturn emittedList, nil\n\t}\n\n\tsubAllowWasNotEmpty := len(lim.Permissions.Sub.Allow) > 0\n\tpubAllowWasNotEmpty := len(lim.Permissions.Pub.Allow) > 0\n\n\tvar err error\n\tif lim.Permissions.Sub.Allow, err = applyTemplate(lim.Permissions.Sub.Allow, false); err != nil {\n\t\treturn jwt.UserPermissionLimits{}, err\n\t} else if lim.Permissions.Sub.Deny, err = applyTemplate(lim.Permissions.Sub.Deny, true); err != nil {\n\t\treturn jwt.UserPermissionLimits{}, err\n\t} else if lim.Permissions.Pub.Allow, err = applyTemplate(lim.Permissions.Pub.Allow, false); err != nil {\n\t\treturn jwt.UserPermissionLimits{}, err\n\t} else if lim.Permissions.Pub.Deny, err = applyTemplate(lim.Permissions.Pub.Deny, true); err != nil {\n\t\treturn jwt.UserPermissionLimits{}, err\n\t}\n\n\t// if pub/sub allow were not empty, but are empty post template processing, add in a \"deny >\" to compensate\n\tif subAllowWasNotEmpty && len(lim.Permissions.Sub.Allow) == 0 {\n\t\tlim.Permissions.Sub.Deny.Add(\">\")\n\t}\n\tif pubAllowWasNotEmpty && len(lim.Permissions.Pub.Allow) == 0 {\n\t\tlim.Permissions.Pub.Deny.Add(\">\")\n\t}\n\treturn lim, nil\n}\n\nfunc (s *Server) processClientOrLeafAuthentication(c *client, opts *Options) (authorized bool) {\n\tvar (\n\t\tnkey *NkeyUser\n\t\tjuc  *jwt.UserClaims\n\t\tacc  *Account\n\t\tuser *User\n\t\tok   bool\n\t\terr  error\n\t\tao   bool // auth override\n\t)\n\n\t// Check if we have auth callouts enabled at the server level or in the bound account.\n\tdefer func() {\n\t\t// Default reason\n\t\treason := AuthenticationViolation.String()\n\t\t// No-op\n\t\tif juc == nil && opts.AuthCallout == nil {\n\t\t\tif !authorized {\n\t\t\t\ts.sendAccountAuthErrorEvent(c, c.acc, reason)\n\t\t\t}\n\t\t\treturn\n\t\t}\n\t\t// We have a juc, check if externally managed, i.e. should be delegated\n\t\t// to the auth callout service.\n\t\tif juc != nil && !acc.hasExternalAuth() {\n\t\t\tif !authorized {\n\t\t\t\ts.sendAccountAuthErrorEvent(c, c.acc, reason)\n\t\t\t}\n\t\t\treturn\n\t\t}\n\t\t// Check config-mode. The global account is a condition since users that\n\t\t// are not found in the config are implicitly bound to the global account.\n\t\t// This means those users should be implicitly delegated to auth callout\n\t\t// if configured. Exclude LEAF connections from this check.\n\t\tif c.kind != LEAF && juc == nil && opts.AuthCallout != nil && c.acc.Name != globalAccountName {\n\t\t\t// If no allowed accounts are defined, then all accounts are in scope.\n\t\t\t// Otherwise see if the account is in the list.\n\t\t\tdelegated := len(opts.AuthCallout.AllowedAccounts) == 0\n\t\t\tif !delegated {\n\t\t\t\tfor _, n := range opts.AuthCallout.AllowedAccounts {\n\t\t\t\t\tif n == c.acc.Name {\n\t\t\t\t\t\tdelegated = true\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// Not delegated, so return with previous authorized result.\n\t\t\tif !delegated {\n\t\t\t\tif !authorized {\n\t\t\t\t\ts.sendAccountAuthErrorEvent(c, c.acc, reason)\n\t\t\t\t}\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\n\t\t// We have auth callout set here.\n\t\tvar skip bool\n\t\t// Check if we are on the list of auth_users.\n\t\tuserID := c.getRawAuthUser()\n\t\tif juc != nil {\n\t\t\tskip = acc.isExternalAuthUser(userID)\n\t\t} else {\n\t\t\tfor _, u := range opts.AuthCallout.AuthUsers {\n\t\t\t\tif userID == u {\n\t\t\t\t\tskip = true\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t// If we are here we have an auth callout defined and we have failed auth so far\n\t\t// so we will callout to our auth backend for processing.\n\t\tif !skip {\n\t\t\tauthorized, reason = s.processClientOrLeafCallout(c, opts)\n\t\t}\n\t\t// Check if we are authorized and in the auth callout account, and if so add in deny publish permissions for the auth subject.\n\t\tif authorized {\n\t\t\tvar authAccountName string\n\t\t\tif juc == nil && opts.AuthCallout != nil {\n\t\t\t\tauthAccountName = opts.AuthCallout.Account\n\t\t\t} else if juc != nil {\n\t\t\t\tauthAccountName = acc.Name\n\t\t\t}\n\t\t\tc.mu.Lock()\n\t\t\tif c.acc != nil && c.acc.Name == authAccountName {\n\t\t\t\tc.mergeDenyPermissions(pub, []string{AuthCalloutSubject})\n\t\t\t}\n\t\t\tc.mu.Unlock()\n\t\t} else {\n\t\t\t// If we are here we failed external authorization.\n\t\t\t// Send an account scoped event. Server config mode acc will be nil,\n\t\t\t// so lookup the auth callout assigned account, that is where this will be sent.\n\t\t\tif acc == nil {\n\t\t\t\tacc, _ = s.lookupAccount(opts.AuthCallout.Account)\n\t\t\t}\n\t\t\ts.sendAccountAuthErrorEvent(c, acc, reason)\n\t\t}\n\t}()\n\n\ts.mu.Lock()\n\tauthRequired := s.info.AuthRequired\n\tif !authRequired {\n\t\t// If no auth required for regular clients, then check if\n\t\t// we have an override for MQTT or Websocket clients.\n\t\tswitch c.clientType() {\n\t\tcase MQTT:\n\t\t\tauthRequired = s.mqtt.authOverride\n\t\tcase WS:\n\t\t\tauthRequired = s.websocket.authOverride\n\t\t}\n\t}\n\tif !authRequired {\n\t\t// TODO(dlc) - If they send us credentials should we fail?\n\t\ts.mu.Unlock()\n\t\treturn true\n\t}\n\tvar (\n\t\tusername      string\n\t\tpassword      string\n\t\ttoken         string\n\t\tnoAuthUser    string\n\t\tpinnedAcounts map[string]struct{}\n\t)\n\ttlsMap := opts.TLSMap\n\tif c.kind == CLIENT {\n\t\tswitch c.clientType() {\n\t\tcase MQTT:\n\t\t\tmo := &opts.MQTT\n\t\t\t// Always override TLSMap.\n\t\t\ttlsMap = mo.TLSMap\n\t\t\t// The rest depends on if there was any auth override in\n\t\t\t// the mqtt's config.\n\t\t\tif s.mqtt.authOverride {\n\t\t\t\tnoAuthUser = mo.NoAuthUser\n\t\t\t\tusername = mo.Username\n\t\t\t\tpassword = mo.Password\n\t\t\t\ttoken = mo.Token\n\t\t\t\tao = true\n\t\t\t}\n\t\tcase WS:\n\t\t\two := &opts.Websocket\n\t\t\t// Always override TLSMap.\n\t\t\ttlsMap = wo.TLSMap\n\t\t\t// The rest depends on if there was any auth override in\n\t\t\t// the websocket's config.\n\t\t\tif s.websocket.authOverride {\n\t\t\t\tnoAuthUser = wo.NoAuthUser\n\t\t\t\tusername = wo.Username\n\t\t\t\tpassword = wo.Password\n\t\t\t\ttoken = wo.Token\n\t\t\t\tao = true\n\t\t\t}\n\t\t}\n\t} else {\n\t\ttlsMap = opts.LeafNode.TLSMap\n\t}\n\n\tif !ao {\n\t\tnoAuthUser = opts.NoAuthUser\n\t\t// If a leaf connects using websocket, and websocket{} block has a no_auth_user\n\t\t// use that one instead.\n\t\tif c.kind == LEAF && c.isWebsocket() && opts.Websocket.NoAuthUser != _EMPTY_ {\n\t\t\tnoAuthUser = opts.Websocket.NoAuthUser\n\t\t}\n\t\tusername = opts.Username\n\t\tpassword = opts.Password\n\t\ttoken = opts.Authorization\n\t}\n\n\t// Check if we have trustedKeys defined in the server. If so we require a user jwt.\n\tif s.trustedKeys != nil {\n\t\tif c.opts.JWT == _EMPTY_ && opts.DefaultSentinel != _EMPTY_ {\n\t\t\tc.opts.JWT = opts.DefaultSentinel\n\t\t}\n\t\tif c.opts.JWT == _EMPTY_ {\n\t\t\ts.mu.Unlock()\n\t\t\tc.Debugf(\"Authentication requires a user JWT\")\n\t\t\treturn false\n\t\t}\n\t\t// So we have a valid user jwt here.\n\t\tjuc, err = jwt.DecodeUserClaims(c.opts.JWT)\n\t\tif err != nil {\n\t\t\ts.mu.Unlock()\n\t\t\tc.Debugf(\"User JWT not valid: %v\", err)\n\t\t\treturn false\n\t\t}\n\t\tvr := jwt.CreateValidationResults()\n\t\tjuc.Validate(vr)\n\t\tif vr.IsBlocking(true) {\n\t\t\ts.mu.Unlock()\n\t\t\tc.Debugf(\"User JWT no longer valid: %+v\", vr)\n\t\t\treturn false\n\t\t}\n\t\tpinnedAcounts = opts.resolverPinnedAccounts\n\t}\n\n\t// Check if we have nkeys or users for client.\n\thasNkeys := len(s.nkeys) > 0\n\thasUsers := len(s.users) > 0\n\tif hasNkeys {\n\t\tif (c.kind == CLIENT || c.kind == LEAF) && noAuthUser != _EMPTY_ &&\n\t\t\tc.opts.Username == _EMPTY_ && c.opts.Password == _EMPTY_ && c.opts.Token == _EMPTY_ && c.opts.Nkey == _EMPTY_ {\n\t\t\tif _, exists := s.nkeys[noAuthUser]; exists {\n\t\t\t\tc.mu.Lock()\n\t\t\t\tc.opts.Nkey = noAuthUser\n\t\t\t\tc.mu.Unlock()\n\t\t\t}\n\t\t}\n\t\tif c.opts.Nkey != _EMPTY_ {\n\t\t\tnkey, ok = s.nkeys[c.opts.Nkey]\n\t\t\tif !ok || !c.connectionTypeAllowed(nkey.AllowedConnectionTypes) {\n\t\t\t\ts.mu.Unlock()\n\t\t\t\treturn false\n\t\t\t}\n\t\t}\n\t}\n\tif hasUsers && nkey == nil {\n\t\t// Check if we are tls verify and are mapping users from the client_certificate.\n\t\tif tlsMap {\n\t\t\tauthorized := checkClientTLSCertSubject(c, func(u string, certDN *ldap.DN, _ bool) (string, bool) {\n\t\t\t\t// First do literal lookup using the resulting string representation\n\t\t\t\t// of RDNSequence as implemented by the pkix package from Go.\n\t\t\t\tif u != _EMPTY_ {\n\t\t\t\t\tusr, ok := s.users[u]\n\t\t\t\t\tif !ok || !c.connectionTypeAllowed(usr.AllowedConnectionTypes) {\n\t\t\t\t\t\treturn _EMPTY_, false\n\t\t\t\t\t}\n\t\t\t\t\tuser = usr\n\t\t\t\t\treturn usr.Username, true\n\t\t\t\t}\n\n\t\t\t\tif certDN == nil {\n\t\t\t\t\treturn _EMPTY_, false\n\t\t\t\t}\n\n\t\t\t\t// Look through the accounts for a DN that is equal to the one\n\t\t\t\t// presented by the certificate.\n\t\t\t\tdns := make(map[*User]*ldap.DN)\n\t\t\t\tfor _, usr := range s.users {\n\t\t\t\t\tif !c.connectionTypeAllowed(usr.AllowedConnectionTypes) {\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\t\t\t\t\t// TODO: Use this utility to make a full validation pass\n\t\t\t\t\t// on start in case tlsmap feature is being used.\n\t\t\t\t\tinputDN, err := ldap.ParseDN(usr.Username)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\t\t\t\t\tif inputDN.Equal(certDN) {\n\t\t\t\t\t\tuser = usr\n\t\t\t\t\t\treturn usr.Username, true\n\t\t\t\t\t}\n\n\t\t\t\t\t// In case it did not match exactly, then collect the DNs\n\t\t\t\t\t// and try to match later in case the DN was reordered.\n\t\t\t\t\tdns[usr] = inputDN\n\t\t\t\t}\n\n\t\t\t\t// Check in case the DN was reordered.\n\t\t\t\tfor usr, inputDN := range dns {\n\t\t\t\t\tif inputDN.RDNsMatch(certDN) {\n\t\t\t\t\t\tuser = usr\n\t\t\t\t\t\treturn usr.Username, true\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\treturn _EMPTY_, false\n\t\t\t})\n\t\t\tif !authorized {\n\t\t\t\ts.mu.Unlock()\n\t\t\t\treturn false\n\t\t\t}\n\t\t\tif c.opts.Username != _EMPTY_ {\n\t\t\t\ts.Warnf(\"User %q found in connect proto, but user required from cert\", c.opts.Username)\n\t\t\t}\n\t\t\t// Already checked that the client didn't send a user in connect\n\t\t\t// but we set it here to be able to identify it in the logs.\n\t\t\tc.opts.Username = user.Username\n\t\t} else {\n\t\t\tif (c.kind == CLIENT || c.kind == LEAF) && noAuthUser != _EMPTY_ &&\n\t\t\t\tc.opts.Username == _EMPTY_ && c.opts.Password == _EMPTY_ && c.opts.Token == _EMPTY_ {\n\t\t\t\tif u, exists := s.users[noAuthUser]; exists {\n\t\t\t\t\tc.mu.Lock()\n\t\t\t\t\tc.opts.Username = u.Username\n\t\t\t\t\tc.opts.Password = u.Password\n\t\t\t\t\tc.mu.Unlock()\n\t\t\t\t}\n\t\t\t}\n\t\t\tif c.opts.Username != _EMPTY_ {\n\t\t\t\tuser, ok = s.users[c.opts.Username]\n\t\t\t\tif !ok || !c.connectionTypeAllowed(user.AllowedConnectionTypes) {\n\t\t\t\t\ts.mu.Unlock()\n\t\t\t\t\treturn false\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\ts.mu.Unlock()\n\n\t// If we have a jwt and a userClaim, make sure we have the Account, etc associated.\n\t// We need to look up the account. This will use an account resolver if one is present.\n\tif juc != nil {\n\t\tissuer := juc.Issuer\n\t\tif juc.IssuerAccount != _EMPTY_ {\n\t\t\tissuer = juc.IssuerAccount\n\t\t}\n\t\tif pinnedAcounts != nil {\n\t\t\tif _, ok := pinnedAcounts[issuer]; !ok {\n\t\t\t\tc.Debugf(\"Account %s not listed as operator pinned account\", issuer)\n\t\t\t\tatomic.AddUint64(&s.pinnedAccFail, 1)\n\t\t\t\treturn false\n\t\t\t}\n\t\t}\n\t\tif acc, err = s.LookupAccount(issuer); acc == nil {\n\t\t\tc.Debugf(\"Account JWT lookup error: %v\", err)\n\t\t\treturn false\n\t\t}\n\t\tacc.mu.RLock()\n\t\taissuer := acc.Issuer\n\t\tacc.mu.RUnlock()\n\t\tif !s.isTrustedIssuer(aissuer) {\n\t\t\tc.Debugf(\"Account JWT not signed by trusted operator\")\n\t\t\treturn false\n\t\t}\n\t\tif scope, ok := acc.hasIssuer(juc.Issuer); !ok {\n\t\t\tc.Debugf(\"User JWT issuer is not known\")\n\t\t\treturn false\n\t\t} else if scope != nil {\n\t\t\tif err := scope.ValidateScopedSigner(juc); err != nil {\n\t\t\t\tc.Debugf(\"User JWT is not valid: %v\", err)\n\t\t\t\treturn false\n\t\t\t} else if uSc, ok := scope.(*jwt.UserScope); !ok {\n\t\t\t\tc.Debugf(\"User JWT is not valid\")\n\t\t\t\treturn false\n\t\t\t} else if juc.UserPermissionLimits, err = processUserPermissionsTemplate(uSc.Template, juc, acc); err != nil {\n\t\t\t\tc.Debugf(\"User JWT generated invalid permissions\")\n\t\t\t\treturn false\n\t\t\t}\n\t\t}\n\t\tif acc.IsExpired() {\n\t\t\tc.Debugf(\"Account JWT has expired\")\n\t\t\treturn false\n\t\t}\n\t\tif juc.BearerToken && acc.failBearer() {\n\t\t\tc.Debugf(\"Account does not allow bearer tokens\")\n\t\t\treturn false\n\t\t}\n\t\t// We check the allowed connection types, but only after processing\n\t\t// of scoped signer (so that it updates `juc` with what is defined\n\t\t// in the account.\n\t\tallowedConnTypes, err := convertAllowedConnectionTypes(juc.AllowedConnectionTypes)\n\t\tif err != nil {\n\t\t\t// We got an error, which means some connection types were unknown. As long as\n\t\t\t// a valid one is returned, we proceed with auth. If not, we have to reject.\n\t\t\t// In other words, suppose that JWT allows \"WEBSOCKET\" in the array. No error\n\t\t\t// is returned and allowedConnTypes will contain \"WEBSOCKET\" only.\n\t\t\t// Client will be rejected if not a websocket client, or proceed with rest of\n\t\t\t// auth if it is.\n\t\t\t// Now suppose JWT allows \"WEBSOCKET, MQTT\" and say MQTT is not known by this\n\t\t\t// server. In this case, allowedConnTypes would contain \"WEBSOCKET\" and we\n\t\t\t// would get `err` indicating that \"MQTT\" is an unknown connection type.\n\t\t\t// If a websocket client connects, it should still be allowed, since after all\n\t\t\t// the admin wanted to allow websocket and mqtt connection types.\n\t\t\t// However, say that the JWT only allows \"MQTT\" (and again suppose this server\n\t\t\t// does not know about MQTT connection type), then since the allowedConnTypes\n\t\t\t// map would be empty (no valid types found), and since empty means allow-all,\n\t\t\t// then we should reject because the intent was to allow connections for this\n\t\t\t// user only as an MQTT client.\n\t\t\tc.Debugf(\"%v\", err)\n\t\t\tif len(allowedConnTypes) == 0 {\n\t\t\t\treturn false\n\t\t\t}\n\t\t}\n\t\tif !c.connectionTypeAllowed(allowedConnTypes) {\n\t\t\tc.Debugf(\"Connection type not allowed\")\n\t\t\treturn false\n\t\t}\n\t\t// skip validation of nonce when presented with a bearer token\n\t\t// FIXME: if BearerToken is only for WSS, need check for server with that port enabled\n\t\tif !juc.BearerToken {\n\t\t\t// Verify the signature against the nonce.\n\t\t\tif c.opts.Sig == _EMPTY_ {\n\t\t\t\tc.Debugf(\"Signature missing\")\n\t\t\t\treturn false\n\t\t\t}\n\t\t\tsig, err := base64.RawURLEncoding.DecodeString(c.opts.Sig)\n\t\t\tif err != nil {\n\t\t\t\t// Allow fallback to normal base64.\n\t\t\t\tsig, err = base64.StdEncoding.DecodeString(c.opts.Sig)\n\t\t\t\tif err != nil {\n\t\t\t\t\tc.Debugf(\"Signature not valid base64\")\n\t\t\t\t\treturn false\n\t\t\t\t}\n\t\t\t}\n\t\t\tpub, err := nkeys.FromPublicKey(juc.Subject)\n\t\t\tif err != nil {\n\t\t\t\tc.Debugf(\"User nkey not valid: %v\", err)\n\t\t\t\treturn false\n\t\t\t}\n\t\t\tif err := pub.Verify(c.nonce, sig); err != nil {\n\t\t\t\tc.Debugf(\"Signature not verified\")\n\t\t\t\treturn false\n\t\t\t}\n\t\t}\n\t\tif acc.checkUserRevoked(juc.Subject, juc.IssuedAt) {\n\t\t\tc.Debugf(\"User authentication revoked\")\n\t\t\treturn false\n\t\t}\n\t\tif !validateSrc(juc, c.host) {\n\t\t\tc.Errorf(\"Bad src Ip %s\", c.host)\n\t\t\treturn false\n\t\t}\n\t\tallowNow, validFor := validateTimes(juc)\n\t\tif !allowNow {\n\t\t\tc.Errorf(\"Outside connect times\")\n\t\t\treturn false\n\t\t}\n\n\t\tnkey = buildInternalNkeyUser(juc, allowedConnTypes, acc)\n\t\tif err := c.RegisterNkeyUser(nkey); err != nil {\n\t\t\treturn false\n\t\t}\n\n\t\t// Warn about JetStream restrictions\n\t\tif c.perms != nil {\n\t\t\tdeniedPub := []string{}\n\t\t\tdeniedSub := []string{}\n\t\t\tfor _, sub := range denyAllJs {\n\t\t\t\tif c.perms.pub.deny != nil {\n\t\t\t\t\tif c.perms.pub.deny.HasInterest(sub) {\n\t\t\t\t\t\tdeniedPub = append(deniedPub, sub)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif c.perms.sub.deny != nil {\n\t\t\t\t\tif c.perms.sub.deny.HasInterest(sub) {\n\t\t\t\t\t\tdeniedSub = append(deniedSub, sub)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tif len(deniedPub) > 0 || len(deniedSub) > 0 {\n\t\t\t\tc.Noticef(\"Connected %s has JetStream denied on pub: %v sub: %v\", c.kindString(), deniedPub, deniedSub)\n\t\t\t}\n\t\t}\n\n\t\t// Hold onto the user's public key.\n\t\tc.mu.Lock()\n\t\tc.pubKey = juc.Subject\n\t\tc.tags = juc.Tags\n\t\tc.nameTag = juc.Name\n\t\tc.mu.Unlock()\n\n\t\t// Check if we need to set an auth timer if the user jwt expires.\n\t\tc.setExpiration(juc.Claims(), validFor)\n\n\t\tacc.mu.RLock()\n\t\tc.Debugf(\"Authenticated JWT: %s %q (claim-name: %q, claim-tags: %q) \"+\n\t\t\t\"signed with %q by Account %q (claim-name: %q, claim-tags: %q) signed with %q has mappings %t accused %p\",\n\t\t\tc.kindString(), juc.Subject, juc.Name, juc.Tags, juc.Issuer, issuer, acc.nameTag, acc.tags, acc.Issuer, acc.hasMappings(), acc)\n\t\tacc.mu.RUnlock()\n\t\treturn true\n\t}\n\n\tif nkey != nil {\n\t\t// If we did not match noAuthUser check signature which is required.\n\t\tif nkey.Nkey != noAuthUser {\n\t\t\tif c.opts.Sig == _EMPTY_ {\n\t\t\t\tc.Debugf(\"Signature missing\")\n\t\t\t\treturn false\n\t\t\t}\n\t\t\tsig, err := base64.RawURLEncoding.DecodeString(c.opts.Sig)\n\t\t\tif err != nil {\n\t\t\t\t// Allow fallback to normal base64.\n\t\t\t\tsig, err = base64.StdEncoding.DecodeString(c.opts.Sig)\n\t\t\t\tif err != nil {\n\t\t\t\t\tc.Debugf(\"Signature not valid base64\")\n\t\t\t\t\treturn false\n\t\t\t\t}\n\t\t\t}\n\t\t\tpub, err := nkeys.FromPublicKey(c.opts.Nkey)\n\t\t\tif err != nil {\n\t\t\t\tc.Debugf(\"User nkey not valid: %v\", err)\n\t\t\t\treturn false\n\t\t\t}\n\t\t\tif err := pub.Verify(c.nonce, sig); err != nil {\n\t\t\t\tc.Debugf(\"Signature not verified\")\n\t\t\t\treturn false\n\t\t\t}\n\t\t}\n\t\tif err := c.RegisterNkeyUser(nkey); err != nil {\n\t\t\treturn false\n\t\t}\n\t\treturn true\n\t}\n\tif user != nil {\n\t\tok = comparePasswords(user.Password, c.opts.Password)\n\t\t// If we are authorized, register the user which will properly setup any permissions\n\t\t// for pub/sub authorizations.\n\t\tif ok {\n\t\t\tc.RegisterUser(user)\n\t\t}\n\t\treturn ok\n\t}\n\n\tif c.kind == CLIENT {\n\t\tif token != _EMPTY_ {\n\t\t\treturn comparePasswords(token, c.opts.Token)\n\t\t} else if username != _EMPTY_ {\n\t\t\tif username != c.opts.Username {\n\t\t\t\treturn false\n\t\t\t}\n\t\t\treturn comparePasswords(password, c.opts.Password)\n\t\t}\n\t} else if c.kind == LEAF {\n\t\t// There is no required username/password to connect and\n\t\t// there was no u/p in the CONNECT or none that matches the\n\t\t// know users. Register the leaf connection with global account\n\t\t// or the one specified in config (if provided).\n\t\treturn s.registerLeafWithAccount(c, opts.LeafNode.Account)\n\t}\n\treturn false\n}\n\nfunc getTLSAuthDCs(rdns *pkix.RDNSequence) string {\n\tdcOID := asn1.ObjectIdentifier{0, 9, 2342, 19200300, 100, 1, 25}\n\tdcs := []string{}\n\tfor _, rdn := range *rdns {\n\t\tif len(rdn) == 0 {\n\t\t\tcontinue\n\t\t}\n\t\tfor _, atv := range rdn {\n\t\t\tvalue, ok := atv.Value.(string)\n\t\t\tif !ok {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif atv.Type.Equal(dcOID) {\n\t\t\t\tdcs = append(dcs, \"DC=\"+value)\n\t\t\t}\n\t\t}\n\t}\n\treturn strings.Join(dcs, \",\")\n}\n\ntype tlsMapAuthFn func(string, *ldap.DN, bool) (string, bool)\n\nfunc checkClientTLSCertSubject(c *client, fn tlsMapAuthFn) bool {\n\ttlsState := c.GetTLSConnectionState()\n\tif tlsState == nil {\n\t\tc.Debugf(\"User required in cert, no TLS connection state\")\n\t\treturn false\n\t}\n\tif len(tlsState.PeerCertificates) == 0 {\n\t\tc.Debugf(\"User required in cert, no peer certificates found\")\n\t\treturn false\n\t}\n\tcert := tlsState.PeerCertificates[0]\n\tif len(tlsState.PeerCertificates) > 1 {\n\t\tc.Debugf(\"Multiple peer certificates found, selecting first\")\n\t}\n\n\thasSANs := len(cert.DNSNames) > 0\n\thasEmailAddresses := len(cert.EmailAddresses) > 0\n\thasSubject := len(cert.Subject.String()) > 0\n\thasURIs := len(cert.URIs) > 0\n\tif !hasEmailAddresses && !hasSubject && !hasURIs {\n\t\tc.Debugf(\"User required in cert, none found\")\n\t\treturn false\n\t}\n\n\tswitch {\n\tcase hasEmailAddresses:\n\t\tfor _, u := range cert.EmailAddresses {\n\t\t\tif match, ok := fn(u, nil, false); ok {\n\t\t\t\tc.Debugf(\"Using email found in cert for auth [%q]\", match)\n\t\t\t\treturn true\n\t\t\t}\n\t\t}\n\t\tfallthrough\n\tcase hasSANs:\n\t\tfor _, u := range cert.DNSNames {\n\t\t\tif match, ok := fn(u, nil, true); ok {\n\t\t\t\tc.Debugf(\"Using SAN found in cert for auth [%q]\", match)\n\t\t\t\treturn true\n\t\t\t}\n\t\t}\n\t\tfallthrough\n\tcase hasURIs:\n\t\tfor _, u := range cert.URIs {\n\t\t\tif match, ok := fn(u.String(), nil, false); ok {\n\t\t\t\tc.Debugf(\"Using URI found in cert for auth [%q]\", match)\n\t\t\t\treturn true\n\t\t\t}\n\t\t}\n\t}\n\n\t// Use the string representation of the full RDN Sequence including\n\t// the domain components in case there are any.\n\trdn := cert.Subject.ToRDNSequence().String()\n\n\t// Match using the raw subject to avoid ignoring attributes.\n\t// https://github.com/golang/go/issues/12342\n\tdn, err := ldap.FromRawCertSubject(cert.RawSubject)\n\tif err == nil {\n\t\tif match, ok := fn(_EMPTY_, dn, false); ok {\n\t\t\tc.Debugf(\"Using DistinguishedNameMatch for auth [%q]\", match)\n\t\t\treturn true\n\t\t}\n\t\tc.Debugf(\"DistinguishedNameMatch could not be used for auth [%q]\", rdn)\n\t}\n\n\tvar rdns pkix.RDNSequence\n\tif _, err := asn1.Unmarshal(cert.RawSubject, &rdns); err == nil {\n\t\t// If found domain components then include roughly following\n\t\t// the order from https://tools.ietf.org/html/rfc2253\n\t\t//\n\t\t// NOTE: The original sequence from string representation by ToRDNSequence does not follow\n\t\t// the correct ordering, so this addition ofdomainComponents would likely be deprecated in\n\t\t// another release in favor of using the correct ordered as parsed by the go-ldap library.\n\t\t//\n\t\tdcs := getTLSAuthDCs(&rdns)\n\t\tif len(dcs) > 0 {\n\t\t\tu := strings.Join([]string{rdn, dcs}, \",\")\n\t\t\tif match, ok := fn(u, nil, false); ok {\n\t\t\t\tc.Debugf(\"Using RDNSequence for auth [%q]\", match)\n\t\t\t\treturn true\n\t\t\t}\n\t\t\tc.Debugf(\"RDNSequence could not be used for auth [%q]\", u)\n\t\t}\n\t}\n\n\t// If no match, then use the string representation of the RDNSequence\n\t// from the subject without the domainComponents.\n\tif match, ok := fn(rdn, nil, false); ok {\n\t\tc.Debugf(\"Using certificate subject for auth [%q]\", match)\n\t\treturn true\n\t}\n\n\tc.Debugf(\"User in cert [%q], not found\", rdn)\n\treturn false\n}\n\nfunc dnsAltNameLabels(dnsAltName string) []string {\n\treturn strings.Split(strings.ToLower(dnsAltName), \".\")\n}\n\n// Check DNS name according to https://tools.ietf.org/html/rfc6125#section-6.4.1\nfunc dnsAltNameMatches(dnsAltNameLabels []string, urls []*url.URL) bool {\nURLS:\n\tfor _, url := range urls {\n\t\tif url == nil {\n\t\t\tcontinue URLS\n\t\t}\n\t\thostLabels := strings.Split(strings.ToLower(url.Hostname()), \".\")\n\t\t// Following https://tools.ietf.org/html/rfc6125#section-6.4.3, should not => will not, may => will not\n\t\t// The wildcard * never matches multiple label and only matches the left most label.\n\t\tif len(hostLabels) != len(dnsAltNameLabels) {\n\t\t\tcontinue URLS\n\t\t}\n\t\ti := 0\n\t\t// only match wildcard on left most label\n\t\tif dnsAltNameLabels[0] == \"*\" {\n\t\t\ti++\n\t\t}\n\t\tfor ; i < len(dnsAltNameLabels); i++ {\n\t\t\tif dnsAltNameLabels[i] != hostLabels[i] {\n\t\t\t\tcontinue URLS\n\t\t\t}\n\t\t}\n\t\treturn true\n\t}\n\treturn false\n}\n\n// checkRouterAuth checks optional router authorization which can be nil or username/password.\nfunc (s *Server) isRouterAuthorized(c *client) bool {\n\t// Snapshot server options.\n\topts := s.getOpts()\n\n\t// Check custom auth first, then TLS map if enabled\n\t// then single user/pass.\n\tif opts.CustomRouterAuthentication != nil {\n\t\treturn opts.CustomRouterAuthentication.Check(c)\n\t}\n\n\tif opts.Cluster.TLSMap || opts.Cluster.TLSCheckKnownURLs {\n\t\treturn checkClientTLSCertSubject(c, func(user string, _ *ldap.DN, isDNSAltName bool) (string, bool) {\n\t\t\tif user == _EMPTY_ {\n\t\t\t\treturn _EMPTY_, false\n\t\t\t}\n\t\t\tif opts.Cluster.TLSCheckKnownURLs && isDNSAltName {\n\t\t\t\tif dnsAltNameMatches(dnsAltNameLabels(user), opts.Routes) {\n\t\t\t\t\treturn _EMPTY_, true\n\t\t\t\t}\n\t\t\t}\n\t\t\tif opts.Cluster.TLSMap && opts.Cluster.Username == user {\n\t\t\t\treturn _EMPTY_, true\n\t\t\t}\n\t\t\treturn _EMPTY_, false\n\t\t})\n\t}\n\n\tif opts.Cluster.Username == _EMPTY_ {\n\t\treturn true\n\t}\n\n\tif opts.Cluster.Username != c.opts.Username {\n\t\treturn false\n\t}\n\tif !comparePasswords(opts.Cluster.Password, c.opts.Password) {\n\t\treturn false\n\t}\n\treturn true\n}\n\n// isGatewayAuthorized checks optional gateway authorization which can be nil or username/password.\nfunc (s *Server) isGatewayAuthorized(c *client) bool {\n\t// Snapshot server options.\n\topts := s.getOpts()\n\n\t// Check whether TLS map is enabled, otherwise use single user/pass.\n\tif opts.Gateway.TLSMap || opts.Gateway.TLSCheckKnownURLs {\n\t\treturn checkClientTLSCertSubject(c, func(user string, _ *ldap.DN, isDNSAltName bool) (string, bool) {\n\t\t\tif user == _EMPTY_ {\n\t\t\t\treturn _EMPTY_, false\n\t\t\t}\n\t\t\tif opts.Gateway.TLSCheckKnownURLs && isDNSAltName {\n\t\t\t\tlabels := dnsAltNameLabels(user)\n\t\t\t\tfor _, gw := range opts.Gateway.Gateways {\n\t\t\t\t\tif gw != nil && dnsAltNameMatches(labels, gw.URLs) {\n\t\t\t\t\t\treturn _EMPTY_, true\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tif opts.Gateway.TLSMap && opts.Gateway.Username == user {\n\t\t\t\treturn _EMPTY_, true\n\t\t\t}\n\t\t\treturn _EMPTY_, false\n\t\t})\n\t}\n\n\tif opts.Gateway.Username == _EMPTY_ {\n\t\treturn true\n\t}\n\n\tif opts.Gateway.Username != c.opts.Username {\n\t\treturn false\n\t}\n\treturn comparePasswords(opts.Gateway.Password, c.opts.Password)\n}\n\nfunc (s *Server) registerLeafWithAccount(c *client, account string) bool {\n\tvar err error\n\tacc := s.globalAccount()\n\tif account != _EMPTY_ {\n\t\tacc, err = s.lookupAccount(account)\n\t\tif err != nil {\n\t\t\ts.Errorf(\"authentication of user %q failed, unable to lookup account %q: %v\",\n\t\t\t\tc.opts.Username, account, err)\n\t\t\treturn false\n\t\t}\n\t}\n\tif err = c.registerWithAccount(acc); err != nil {\n\t\treturn false\n\t}\n\treturn true\n}\n\n// isLeafNodeAuthorized will check for auth for an inbound leaf node connection.\nfunc (s *Server) isLeafNodeAuthorized(c *client) bool {\n\topts := s.getOpts()\n\n\tisAuthorized := func(username, password, account string) bool {\n\t\tif username != c.opts.Username {\n\t\t\treturn false\n\t\t}\n\t\tif !comparePasswords(password, c.opts.Password) {\n\t\t\treturn false\n\t\t}\n\t\treturn s.registerLeafWithAccount(c, account)\n\t}\n\n\t// If leafnodes config has an authorization{} stanza, this takes precedence.\n\t// The user in CONNECT must match. We will bind to the account associated\n\t// with that user (from the leafnode's authorization{} config).\n\tif opts.LeafNode.Username != _EMPTY_ {\n\t\treturn isAuthorized(opts.LeafNode.Username, opts.LeafNode.Password, opts.LeafNode.Account)\n\t} else if opts.LeafNode.Nkey != _EMPTY_ {\n\t\tif c.opts.Nkey != opts.LeafNode.Nkey {\n\t\t\treturn false\n\t\t}\n\t\tif c.opts.Sig == _EMPTY_ {\n\t\t\tc.Debugf(\"Signature missing\")\n\t\t\treturn false\n\t\t}\n\t\tsig, err := base64.RawURLEncoding.DecodeString(c.opts.Sig)\n\t\tif err != nil {\n\t\t\t// Allow fallback to normal base64.\n\t\t\tsig, err = base64.StdEncoding.DecodeString(c.opts.Sig)\n\t\t\tif err != nil {\n\t\t\t\tc.Debugf(\"Signature not valid base64\")\n\t\t\t\treturn false\n\t\t\t}\n\t\t}\n\t\tpub, err := nkeys.FromPublicKey(c.opts.Nkey)\n\t\tif err != nil {\n\t\t\tc.Debugf(\"User nkey not valid: %v\", err)\n\t\t\treturn false\n\t\t}\n\t\tif err := pub.Verify(c.nonce, sig); err != nil {\n\t\t\tc.Debugf(\"Signature not verified\")\n\t\t\treturn false\n\t\t}\n\t\treturn s.registerLeafWithAccount(c, opts.LeafNode.Account)\n\t} else if len(opts.LeafNode.Users) > 0 {\n\t\tif opts.LeafNode.TLSMap {\n\t\t\tvar user *User\n\t\t\tfound := checkClientTLSCertSubject(c, func(u string, _ *ldap.DN, _ bool) (string, bool) {\n\t\t\t\t// This is expected to be a very small array.\n\t\t\t\tfor _, usr := range opts.LeafNode.Users {\n\t\t\t\t\tif u == usr.Username {\n\t\t\t\t\t\tuser = usr\n\t\t\t\t\t\treturn u, true\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\treturn _EMPTY_, false\n\t\t\t})\n\t\t\tif !found {\n\t\t\t\treturn false\n\t\t\t}\n\t\t\tif c.opts.Username != _EMPTY_ {\n\t\t\t\ts.Warnf(\"User %q found in connect proto, but user required from cert\", c.opts.Username)\n\t\t\t}\n\t\t\tc.opts.Username = user.Username\n\t\t\t// EMPTY will result in $G\n\t\t\taccName := _EMPTY_\n\t\t\tif user.Account != nil {\n\t\t\t\taccName = user.Account.GetName()\n\t\t\t}\n\t\t\t// This will authorize since are using an existing user,\n\t\t\t// but it will also register with proper account.\n\t\t\treturn isAuthorized(user.Username, user.Password, accName)\n\t\t}\n\n\t\t// This is expected to be a very small array.\n\t\tfor _, u := range opts.LeafNode.Users {\n\t\t\tif u.Username == c.opts.Username {\n\t\t\t\tvar accName string\n\t\t\t\tif u.Account != nil {\n\t\t\t\t\taccName = u.Account.Name\n\t\t\t\t}\n\t\t\t\treturn isAuthorized(u.Username, u.Password, accName)\n\t\t\t}\n\t\t}\n\t\treturn false\n\t}\n\n\t// We are here if we accept leafnode connections without any credentials.\n\n\t// Still, if the CONNECT has some user info, we will bind to the\n\t// user's account or to the specified default account (if provided)\n\t// or to the global account.\n\treturn s.isClientAuthorized(c)\n}\n\n// Support for bcrypt stored passwords and tokens.\nvar validBcryptPrefix = regexp.MustCompile(`^\\$2[abxy]\\$\\d{2}\\$.*`)\n\n// isBcrypt checks whether the given password or token is bcrypted.\nfunc isBcrypt(password string) bool {\n\tif strings.HasPrefix(password, \"$\") {\n\t\treturn validBcryptPrefix.MatchString(password)\n\t}\n\n\treturn false\n}\n\nfunc comparePasswords(serverPassword, clientPassword string) bool {\n\t// Check to see if the server password is a bcrypt hash\n\tif isBcrypt(serverPassword) {\n\t\tif err := bcrypt.CompareHashAndPassword([]byte(serverPassword), []byte(clientPassword)); err != nil {\n\t\t\treturn false\n\t\t}\n\t} else {\n\t\t// stringToBytes should be constant-time near enough compared to\n\t\t// turning a string into []byte normally.\n\t\tspass := stringToBytes(serverPassword)\n\t\tcpass := stringToBytes(clientPassword)\n\t\tif subtle.ConstantTimeCompare(spass, cpass) == 0 {\n\t\t\treturn false\n\t\t}\n\t}\n\treturn true\n}\n\nfunc validateAuth(o *Options) error {\n\tif err := validatePinnedCerts(o.TLSPinnedCerts); err != nil {\n\t\treturn err\n\t}\n\tfor _, u := range o.Users {\n\t\tif err := validateAllowedConnectionTypes(u.AllowedConnectionTypes); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\tfor _, u := range o.Nkeys {\n\t\tif err := validateAllowedConnectionTypes(u.AllowedConnectionTypes); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn validateNoAuthUser(o, o.NoAuthUser)\n}\n\nfunc validateAllowedConnectionTypes(m map[string]struct{}) error {\n\tfor ct := range m {\n\t\tctuc := strings.ToUpper(ct)\n\t\tswitch ctuc {\n\t\tcase jwt.ConnectionTypeStandard, jwt.ConnectionTypeWebsocket,\n\t\t\tjwt.ConnectionTypeLeafnode, jwt.ConnectionTypeLeafnodeWS,\n\t\t\tjwt.ConnectionTypeMqtt, jwt.ConnectionTypeMqttWS,\n\t\t\tjwt.ConnectionTypeInProcess:\n\t\tdefault:\n\t\t\treturn fmt.Errorf(\"unknown connection type %q\", ct)\n\t\t}\n\t\tif ctuc != ct {\n\t\t\tdelete(m, ct)\n\t\t\tm[ctuc] = struct{}{}\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc validateNoAuthUser(o *Options, noAuthUser string) error {\n\tif noAuthUser == _EMPTY_ {\n\t\treturn nil\n\t}\n\tif len(o.TrustedOperators) > 0 {\n\t\treturn fmt.Errorf(\"no_auth_user not compatible with Trusted Operator\")\n\t}\n\n\tif o.Nkeys == nil && o.Users == nil {\n\t\treturn fmt.Errorf(`no_auth_user: \"%s\" present, but users/nkeys are not defined`, noAuthUser)\n\t}\n\tfor _, u := range o.Users {\n\t\tif u.Username == noAuthUser {\n\t\t\treturn nil\n\t\t}\n\t}\n\tfor _, u := range o.Nkeys {\n\t\tif u.Nkey == noAuthUser {\n\t\t\treturn nil\n\t\t}\n\t}\n\treturn fmt.Errorf(\n\t\t`no_auth_user: \"%s\" not present as user or nkey in authorization block or account configuration`,\n\t\tnoAuthUser)\n}\n",
    "source_file": "server/auth.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2020-2025 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage server\n\nimport (\n\t\"encoding/json\"\n\t\"time\"\n)\n\n// publishAdvisory sends the given advisory into the account. Returns true if\n// it was sent, false if not (i.e. due to lack of interest or a marshal error).\nfunc (s *Server) publishAdvisory(acc *Account, subject string, adv any) bool {\n\tif acc == nil {\n\t\tacc = s.SystemAccount()\n\t\tif acc == nil {\n\t\t\treturn false\n\t\t}\n\t}\n\n\t// If there is no one listening for this advisory then save ourselves the effort\n\t// and don't bother encoding the JSON or sending it.\n\tif sl := acc.sl; (sl != nil && !sl.HasInterest(subject)) && !s.hasGatewayInterest(acc.Name, subject) {\n\t\treturn false\n\t}\n\n\tej, err := json.Marshal(adv)\n\tif err == nil {\n\t\terr = s.sendInternalAccountMsg(acc, subject, ej)\n\t\tif err != nil {\n\t\t\ts.Warnf(\"Advisory could not be sent for account %q: %v\", acc.Name, err)\n\t\t}\n\t} else {\n\t\ts.Warnf(\"Advisory could not be serialized for account %q: %v\", acc.Name, err)\n\t}\n\treturn err == nil\n}\n\n// JSAPIAudit is an advisory about administrative actions taken on JetStream\ntype JSAPIAudit struct {\n\tTypedEvent\n\tServer   string      `json:\"server\"`\n\tClient   *ClientInfo `json:\"client\"`\n\tSubject  string      `json:\"subject\"`\n\tRequest  string      `json:\"request,omitempty\"`\n\tResponse string      `json:\"response\"`\n\tDomain   string      `json:\"domain,omitempty\"`\n}\n\nconst JSAPIAuditType = \"io.nats.jetstream.advisory.v1.api_audit\"\n\n// ActionAdvisoryType indicates which action against a stream, consumer or template triggered an advisory\ntype ActionAdvisoryType string\n\nconst (\n\tCreateEvent ActionAdvisoryType = \"create\"\n\tDeleteEvent ActionAdvisoryType = \"delete\"\n\tModifyEvent ActionAdvisoryType = \"modify\"\n)\n\n// JSStreamActionAdvisory indicates that a stream was created, edited or deleted\ntype JSStreamActionAdvisory struct {\n\tTypedEvent\n\tStream   string             `json:\"stream\"`\n\tAction   ActionAdvisoryType `json:\"action\"`\n\tTemplate string             `json:\"template,omitempty\"`\n\tDomain   string             `json:\"domain,omitempty\"`\n}\n\nconst JSStreamActionAdvisoryType = \"io.nats.jetstream.advisory.v1.stream_action\"\n\n// JSConsumerActionAdvisory indicates that a consumer was created or deleted\ntype JSConsumerActionAdvisory struct {\n\tTypedEvent\n\tStream   string             `json:\"stream\"`\n\tConsumer string             `json:\"consumer\"`\n\tAction   ActionAdvisoryType `json:\"action\"`\n\tDomain   string             `json:\"domain,omitempty\"`\n}\n\nconst JSConsumerActionAdvisoryType = \"io.nats.jetstream.advisory.v1.consumer_action\"\n\n// JSConsumerPauseAdvisory indicates that a consumer was paused or unpaused\ntype JSConsumerPauseAdvisory struct {\n\tTypedEvent\n\tStream     string    `json:\"stream\"`\n\tConsumer   string    `json:\"consumer\"`\n\tPaused     bool      `json:\"paused\"`\n\tPauseUntil time.Time `json:\"pause_until,omitempty\"`\n\tDomain     string    `json:\"domain,omitempty\"`\n}\n\nconst JSConsumerPauseAdvisoryType = \"io.nats.jetstream.advisory.v1.consumer_pause\"\n\n// JSConsumerAckMetric is a metric published when a user acknowledges a message, the\n// number of these that will be published is dependent on SampleFrequency\ntype JSConsumerAckMetric struct {\n\tTypedEvent\n\tStream      string `json:\"stream\"`\n\tConsumer    string `json:\"consumer\"`\n\tConsumerSeq uint64 `json:\"consumer_seq\"`\n\tStreamSeq   uint64 `json:\"stream_seq\"`\n\tDelay       int64  `json:\"ack_time\"`\n\tDeliveries  uint64 `json:\"deliveries\"`\n\tDomain      string `json:\"domain,omitempty\"`\n}\n\n// JSConsumerAckMetricType is the schema type for JSConsumerAckMetricType\nconst JSConsumerAckMetricType = \"io.nats.jetstream.metric.v1.consumer_ack\"\n\n// JSConsumerDeliveryExceededAdvisory is an advisory informing that a message hit\n// its MaxDeliver threshold and so might be a candidate for DLQ handling\ntype JSConsumerDeliveryExceededAdvisory struct {\n\tTypedEvent\n\tStream     string `json:\"stream\"`\n\tConsumer   string `json:\"consumer\"`\n\tStreamSeq  uint64 `json:\"stream_seq\"`\n\tDeliveries uint64 `json:\"deliveries\"`\n\tDomain     string `json:\"domain,omitempty\"`\n}\n\n// JSConsumerDeliveryExceededAdvisoryType is the schema type for JSConsumerDeliveryExceededAdvisory\nconst JSConsumerDeliveryExceededAdvisoryType = \"io.nats.jetstream.advisory.v1.max_deliver\"\n\n// JSConsumerDeliveryNakAdvisory is an advisory informing that a message was\n// naked by the consumer\ntype JSConsumerDeliveryNakAdvisory struct {\n\tTypedEvent\n\tStream      string `json:\"stream\"`\n\tConsumer    string `json:\"consumer\"`\n\tConsumerSeq uint64 `json:\"consumer_seq\"`\n\tStreamSeq   uint64 `json:\"stream_seq\"`\n\tDeliveries  uint64 `json:\"deliveries\"`\n\tDomain      string `json:\"domain,omitempty\"`\n}\n\n// JSConsumerDeliveryNakAdvisoryType is the schema type for JSConsumerDeliveryNakAdvisory\nconst JSConsumerDeliveryNakAdvisoryType = \"io.nats.jetstream.advisory.v1.nak\"\n\n// JSConsumerDeliveryTerminatedAdvisory is an advisory informing that a message was\n// terminated by the consumer, so might be a candidate for DLQ handling\ntype JSConsumerDeliveryTerminatedAdvisory struct {\n\tTypedEvent\n\tStream      string `json:\"stream\"`\n\tConsumer    string `json:\"consumer\"`\n\tConsumerSeq uint64 `json:\"consumer_seq\"`\n\tStreamSeq   uint64 `json:\"stream_seq\"`\n\tDeliveries  uint64 `json:\"deliveries\"`\n\tReason      string `json:\"reason,omitempty\"`\n\tDomain      string `json:\"domain,omitempty\"`\n}\n\n// JSConsumerDeliveryTerminatedAdvisoryType is the schema type for JSConsumerDeliveryTerminatedAdvisory\nconst JSConsumerDeliveryTerminatedAdvisoryType = \"io.nats.jetstream.advisory.v1.terminated\"\n\n// JSSnapshotCreateAdvisory is an advisory sent after a snapshot is successfully started\ntype JSSnapshotCreateAdvisory struct {\n\tTypedEvent\n\tStream string      `json:\"stream\"`\n\tState  StreamState `json:\"state\"`\n\tClient *ClientInfo `json:\"client\"`\n\tDomain string      `json:\"domain,omitempty\"`\n}\n\n// JSSnapshotCreatedAdvisoryType is the schema type for JSSnapshotCreateAdvisory\nconst JSSnapshotCreatedAdvisoryType = \"io.nats.jetstream.advisory.v1.snapshot_create\"\n\n// JSSnapshotCompleteAdvisory is an advisory sent after a snapshot is successfully started\ntype JSSnapshotCompleteAdvisory struct {\n\tTypedEvent\n\tStream string      `json:\"stream\"`\n\tStart  time.Time   `json:\"start\"`\n\tEnd    time.Time   `json:\"end\"`\n\tClient *ClientInfo `json:\"client\"`\n\tDomain string      `json:\"domain,omitempty\"`\n}\n\n// JSSnapshotCompleteAdvisoryType is the schema type for JSSnapshotCreateAdvisory\nconst JSSnapshotCompleteAdvisoryType = \"io.nats.jetstream.advisory.v1.snapshot_complete\"\n\n// JSRestoreCreateAdvisory is an advisory sent after a snapshot is successfully started\ntype JSRestoreCreateAdvisory struct {\n\tTypedEvent\n\tStream string      `json:\"stream\"`\n\tClient *ClientInfo `json:\"client\"`\n\tDomain string      `json:\"domain,omitempty\"`\n}\n\n// JSRestoreCreateAdvisoryType is the schema type for JSSnapshotCreateAdvisory\nconst JSRestoreCreateAdvisoryType = \"io.nats.jetstream.advisory.v1.restore_create\"\n\n// JSRestoreCompleteAdvisory is an advisory sent after a snapshot is successfully started\ntype JSRestoreCompleteAdvisory struct {\n\tTypedEvent\n\tStream string      `json:\"stream\"`\n\tStart  time.Time   `json:\"start\"`\n\tEnd    time.Time   `json:\"end\"`\n\tBytes  int64       `json:\"bytes\"`\n\tClient *ClientInfo `json:\"client\"`\n\tDomain string      `json:\"domain,omitempty\"`\n}\n\n// JSRestoreCompleteAdvisoryType is the schema type for JSSnapshotCreateAdvisory\nconst JSRestoreCompleteAdvisoryType = \"io.nats.jetstream.advisory.v1.restore_complete\"\n\n// Clustering specific.\n\n// JSClusterLeaderElectedAdvisoryType is sent when the system elects a new meta leader.\nconst JSDomainLeaderElectedAdvisoryType = \"io.nats.jetstream.advisory.v1.domain_leader_elected\"\n\n// JSClusterLeaderElectedAdvisory indicates that a domain has elected a new leader.\ntype JSDomainLeaderElectedAdvisory struct {\n\tTypedEvent\n\tLeader   string      `json:\"leader\"`\n\tReplicas []*PeerInfo `json:\"replicas\"`\n\tCluster  string      `json:\"cluster\"`\n\tDomain   string      `json:\"domain,omitempty\"`\n}\n\n// JSStreamLeaderElectedAdvisoryType is sent when the system elects a new leader for a stream.\nconst JSStreamLeaderElectedAdvisoryType = \"io.nats.jetstream.advisory.v1.stream_leader_elected\"\n\n// JSStreamLeaderElectedAdvisory indicates that a stream has elected a new leader.\ntype JSStreamLeaderElectedAdvisory struct {\n\tTypedEvent\n\tAccount  string      `json:\"account,omitempty\"`\n\tStream   string      `json:\"stream\"`\n\tLeader   string      `json:\"leader\"`\n\tReplicas []*PeerInfo `json:\"replicas\"`\n\tDomain   string      `json:\"domain,omitempty\"`\n}\n\n// JSStreamQuorumLostAdvisoryType is sent when the system detects a clustered stream and\n// its consumers are stalled and unable to make progress.\nconst JSStreamQuorumLostAdvisoryType = \"io.nats.jetstream.advisory.v1.stream_quorum_lost\"\n\n// JSStreamQuorumLostAdvisory indicates that a stream has lost quorum and is stalled.\ntype JSStreamQuorumLostAdvisory struct {\n\tTypedEvent\n\tAccount  string      `json:\"account,omitempty\"`\n\tStream   string      `json:\"stream\"`\n\tReplicas []*PeerInfo `json:\"replicas\"`\n\tDomain   string      `json:\"domain,omitempty\"`\n}\n\n// JSConsumerLeaderElectedAdvisoryType is sent when the system elects a leader for a consumer.\nconst JSConsumerLeaderElectedAdvisoryType = \"io.nats.jetstream.advisory.v1.consumer_leader_elected\"\n\n// JSConsumerLeaderElectedAdvisory indicates that a consumer has elected a new leader.\ntype JSConsumerLeaderElectedAdvisory struct {\n\tTypedEvent\n\tAccount  string      `json:\"account,omitempty\"`\n\tStream   string      `json:\"stream\"`\n\tConsumer string      `json:\"consumer\"`\n\tLeader   string      `json:\"leader\"`\n\tReplicas []*PeerInfo `json:\"replicas\"`\n\tDomain   string      `json:\"domain,omitempty\"`\n}\n\n// JSConsumerQuorumLostAdvisoryType is sent when the system detects a clustered consumer and\n// is stalled and unable to make progress.\nconst JSConsumerQuorumLostAdvisoryType = \"io.nats.jetstream.advisory.v1.consumer_quorum_lost\"\n\n// JSConsumerQuorumLostAdvisory indicates that a consumer has lost quorum and is stalled.\ntype JSConsumerQuorumLostAdvisory struct {\n\tTypedEvent\n\tAccount  string      `json:\"account,omitempty\"`\n\tStream   string      `json:\"stream\"`\n\tConsumer string      `json:\"consumer\"`\n\tReplicas []*PeerInfo `json:\"replicas\"`\n\tDomain   string      `json:\"domain,omitempty\"`\n}\n\nconst JSConsumerGroupPinnedAdvisoryType = \"io.nats.jetstream.advisory.v1.consumer_group_pinned\"\n\n// JSConsumerGroupPinnedAdvisory that a group switched to a new pinned client\ntype JSConsumerGroupPinnedAdvisory struct {\n\tTypedEvent\n\tAccount        string `json:\"account,omitempty\"`\n\tStream         string `json:\"stream\"`\n\tConsumer       string `json:\"consumer\"`\n\tDomain         string `json:\"domain,omitempty\"`\n\tGroup          string `json:\"group\"`\n\tPinnedClientId string `json:\"pinned_id\"`\n}\n\nconst JSConsumerGroupUnpinnedAdvisoryType = \"io.nats.jetstream.advisory.v1.consumer_group_unpinned\"\n\n// JSConsumerGroupUnpinnedAdvisory indicates that a pin was lost\ntype JSConsumerGroupUnpinnedAdvisory struct {\n\tTypedEvent\n\tAccount  string `json:\"account,omitempty\"`\n\tStream   string `json:\"stream\"`\n\tConsumer string `json:\"consumer\"`\n\tDomain   string `json:\"domain,omitempty\"`\n\tGroup    string `json:\"group\"`\n\t// one of \"admin\" or \"timeout\", could be an enum up to the implementor to decide\n\tReason string `json:\"reason\"`\n}\n\n// JSServerOutOfStorageAdvisoryType is sent when the server is out of storage space.\nconst JSServerOutOfStorageAdvisoryType = \"io.nats.jetstream.advisory.v1.server_out_of_space\"\n\n// JSServerOutOfSpaceAdvisory indicates that a stream has lost quorum and is stalled.\ntype JSServerOutOfSpaceAdvisory struct {\n\tTypedEvent\n\tServer   string `json:\"server\"`\n\tServerID string `json:\"server_id\"`\n\tStream   string `json:\"stream,omitempty\"`\n\tCluster  string `json:\"cluster\"`\n\tDomain   string `json:\"domain,omitempty\"`\n}\n\n// JSServerRemovedAdvisoryType is sent when the server has been removed and JS disabled.\nconst JSServerRemovedAdvisoryType = \"io.nats.jetstream.advisory.v1.server_removed\"\n\n// JSServerRemovedAdvisory indicates that a stream has lost quorum and is stalled.\ntype JSServerRemovedAdvisory struct {\n\tTypedEvent\n\tServer   string `json:\"server\"`\n\tServerID string `json:\"server_id\"`\n\tCluster  string `json:\"cluster\"`\n\tDomain   string `json:\"domain,omitempty\"`\n}\n\n// JSAPILimitReachedAdvisoryType is sent when the JS API request queue limit is reached.\nconst JSAPILimitReachedAdvisoryType = \"io.nats.jetstream.advisory.v1.api_limit_reached\"\n\n// JSAPILimitReachedAdvisory is a advisory published when JetStream hits the queue length limit.\ntype JSAPILimitReachedAdvisory struct {\n\tTypedEvent\n\tServer  string `json:\"server\"`           // Server that created the event, name or ID\n\tDomain  string `json:\"domain,omitempty\"` // Domain the server belongs to\n\tDropped int64  `json:\"dropped\"`          // How many messages did we drop from the queue\n}\n",
    "source_file": "server/jetstream_events.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2019-2025 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage server\n\nimport (\n\t\"encoding/binary\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"os\"\n\t\"strings\"\n\t\"time\"\n\t\"unsafe\"\n\n\t\"github.com/nats-io/nats-server/v2/server/avl\"\n)\n\n// StorageType determines how messages are stored for retention.\ntype StorageType int\n\nconst (\n\t// File specifies on disk, designated by the JetStream config StoreDir.\n\tFileStorage = StorageType(22)\n\t// MemoryStorage specifies in memory only.\n\tMemoryStorage = StorageType(33)\n\t// Any is for internals.\n\tAnyStorage = StorageType(44)\n)\n\nvar (\n\t// ErrStoreClosed is returned when the store has been closed\n\tErrStoreClosed = errors.New(\"store is closed\")\n\t// ErrStoreMsgNotFound when message was not found but was expected to be.\n\tErrStoreMsgNotFound = errors.New(\"no message found\")\n\t// ErrStoreEOF is returned when message seq is greater than the last sequence.\n\tErrStoreEOF = errors.New(\"stream store EOF\")\n\t// ErrMaxMsgs is returned when we have discard new as a policy and we reached the message limit.\n\tErrMaxMsgs = errors.New(\"maximum messages exceeded\")\n\t// ErrMaxBytes is returned when we have discard new as a policy and we reached the bytes limit.\n\tErrMaxBytes = errors.New(\"maximum bytes exceeded\")\n\t// ErrMaxMsgsPerSubject is returned when we have discard new as a policy and we reached the message limit per subject.\n\tErrMaxMsgsPerSubject = errors.New(\"maximum messages per subject exceeded\")\n\t// ErrStoreSnapshotInProgress is returned when RemoveMsg or EraseMsg is called\n\t// while a snapshot is in progress.\n\tErrStoreSnapshotInProgress = errors.New(\"snapshot in progress\")\n\t// ErrMsgTooLarge is returned when a message is considered too large.\n\tErrMsgTooLarge = errors.New(\"message too large\")\n\t// ErrStoreWrongType is for when you access the wrong storage type.\n\tErrStoreWrongType = errors.New(\"wrong storage type\")\n\t// ErrNoAckPolicy is returned when trying to update a consumer's acks with no ack policy.\n\tErrNoAckPolicy = errors.New(\"ack policy is none\")\n\t// ErrInvalidSequence is returned when the sequence is not present in the stream store.\n\tErrInvalidSequence = errors.New(\"invalid sequence\")\n\t// ErrSequenceMismatch is returned when storing a raw message and the expected sequence is wrong.\n\tErrSequenceMismatch = errors.New(\"expected sequence does not match store\")\n\t// ErrCorruptStreamState\n\tErrCorruptStreamState = errors.New(\"stream state snapshot is corrupt\")\n\t// ErrTooManyResults\n\tErrTooManyResults = errors.New(\"too many matching results for request\")\n)\n\n// StoreMsg is the stored message format for messages that are retained by the Store layer.\ntype StoreMsg struct {\n\tsubj string\n\thdr  []byte\n\tmsg  []byte\n\tbuf  []byte\n\tseq  uint64\n\tts   int64\n}\n\n// Used to call back into the upper layers to report on changes in storage resources.\n// For the cases where its a single message we will also supply sequence number and subject.\ntype StorageUpdateHandler func(msgs, bytes int64, seq uint64, subj string)\n\n// Used to call back into the upper layers to remove a message.\ntype StorageRemoveMsgHandler func(seq uint64)\n\n// Used to call back into the upper layers to report on newly created subject delete markers.\ntype SubjectDeleteMarkerUpdateHandler func(*inMsg)\n\ntype StreamStore interface {\n\tStoreMsg(subject string, hdr, msg []byte, ttl int64) (uint64, int64, error)\n\tStoreRawMsg(subject string, hdr, msg []byte, seq uint64, ts int64, ttl int64) error\n\tSkipMsg() uint64\n\tSkipMsgs(seq uint64, num uint64) error\n\tLoadMsg(seq uint64, sm *StoreMsg) (*StoreMsg, error)\n\tLoadNextMsg(filter string, wc bool, start uint64, smp *StoreMsg) (sm *StoreMsg, skip uint64, err error)\n\tLoadNextMsgMulti(sl *Sublist, start uint64, smp *StoreMsg) (sm *StoreMsg, skip uint64, err error)\n\tLoadLastMsg(subject string, sm *StoreMsg) (*StoreMsg, error)\n\tLoadPrevMsg(start uint64, smp *StoreMsg) (sm *StoreMsg, err error)\n\tRemoveMsg(seq uint64) (bool, error)\n\tEraseMsg(seq uint64) (bool, error)\n\tPurge() (uint64, error)\n\tPurgeEx(subject string, seq, keep uint64) (uint64, error)\n\tCompact(seq uint64) (uint64, error)\n\tTruncate(seq uint64) error\n\tGetSeqFromTime(t time.Time) uint64\n\tFilteredState(seq uint64, subject string) SimpleState\n\tSubjectsState(filterSubject string) map[string]SimpleState\n\tSubjectsTotals(filterSubject string) map[string]uint64\n\tAllLastSeqs() ([]uint64, error)\n\tMultiLastSeqs(filters []string, maxSeq uint64, maxAllowed int) ([]uint64, error)\n\tSubjectForSeq(seq uint64) (string, error)\n\tNumPending(sseq uint64, filter string, lastPerSubject bool) (total, validThrough uint64)\n\tNumPendingMulti(sseq uint64, sl *Sublist, lastPerSubject bool) (total, validThrough uint64)\n\tState() StreamState\n\tFastState(*StreamState)\n\tEncodedStreamState(failed uint64) (enc []byte, err error)\n\tSyncDeleted(dbs DeleteBlocks)\n\tType() StorageType\n\tRegisterStorageUpdates(StorageUpdateHandler)\n\tRegisterStorageRemoveMsg(handler StorageRemoveMsgHandler)\n\tRegisterSubjectDeleteMarkerUpdates(SubjectDeleteMarkerUpdateHandler)\n\tUpdateConfig(cfg *StreamConfig) error\n\tDelete() error\n\tStop() error\n\tConsumerStore(name string, cfg *ConsumerConfig) (ConsumerStore, error)\n\tAddConsumer(o ConsumerStore) error\n\tRemoveConsumer(o ConsumerStore) error\n\tSnapshot(deadline time.Duration, includeConsumers, checkMsgs bool) (*SnapshotResult, error)\n\tUtilization() (total, reported uint64, err error)\n}\n\n// RetentionPolicy determines how messages in a set are retained.\ntype RetentionPolicy int\n\nconst (\n\t// LimitsPolicy (default) means that messages are retained until any given limit is reached.\n\t// This could be one of MaxMsgs, MaxBytes, or MaxAge.\n\tLimitsPolicy RetentionPolicy = iota\n\t// InterestPolicy specifies that when all known consumers have acknowledged a message it can be removed.\n\tInterestPolicy\n\t// WorkQueuePolicy specifies that when the first worker or subscriber acknowledges the message it can be removed.\n\tWorkQueuePolicy\n)\n\n// Discard Policy determines how we proceed when limits of messages or bytes are hit. The default, DicscardOld will\n// remove older messages. DiscardNew will fail to store the new message.\ntype DiscardPolicy int\n\nconst (\n\t// DiscardOld will remove older messages to return to the limits.\n\tDiscardOld = iota\n\t// DiscardNew will error on a StoreMsg call\n\tDiscardNew\n)\n\n// StreamState is information about the given stream.\ntype StreamState struct {\n\tMsgs        uint64            `json:\"messages\"`\n\tBytes       uint64            `json:\"bytes\"`\n\tFirstSeq    uint64            `json:\"first_seq\"`\n\tFirstTime   time.Time         `json:\"first_ts\"`\n\tLastSeq     uint64            `json:\"last_seq\"`\n\tLastTime    time.Time         `json:\"last_ts\"`\n\tNumSubjects int               `json:\"num_subjects,omitempty\"`\n\tSubjects    map[string]uint64 `json:\"subjects,omitempty\"`\n\tNumDeleted  int               `json:\"num_deleted,omitempty\"`\n\tDeleted     []uint64          `json:\"deleted,omitempty\"`\n\tLost        *LostStreamData   `json:\"lost,omitempty\"`\n\tConsumers   int               `json:\"consumer_count\"`\n}\n\n// SimpleState for filtered subject specific state.\ntype SimpleState struct {\n\tMsgs  uint64 `json:\"messages\"`\n\tFirst uint64 `json:\"first_seq\"`\n\tLast  uint64 `json:\"last_seq\"`\n\n\t// Internal usage for when the first needs to be updated before use.\n\tfirstNeedsUpdate bool\n\t// Internal usage for when the last needs to be updated before use.\n\tlastNeedsUpdate bool\n}\n\n// LostStreamData indicates msgs that have been lost.\ntype LostStreamData struct {\n\tMsgs  []uint64 `json:\"msgs\"`\n\tBytes uint64   `json:\"bytes\"`\n}\n\n// SnapshotResult contains information about the snapshot.\ntype SnapshotResult struct {\n\tReader io.ReadCloser\n\tState  StreamState\n\terrCh  chan string\n}\n\nconst (\n\t// Magic is used to identify stream state encodings.\n\tstreamStateMagic = uint8(42)\n\t// Version\n\tstreamStateVersion = uint8(1)\n\t// Magic / Identifier for run length encodings.\n\trunLengthMagic = uint8(33)\n\t// Magic / Identifier for AVL seqsets.\n\tseqSetMagic = uint8(22)\n)\n\n// Interface for DeleteBlock.\n// These will be of three types:\n// 1. AVL seqsets.\n// 2. Run length encoding of a deleted range.\n// 3. Legacy []uint64\ntype DeleteBlock interface {\n\tState() (first, last, num uint64)\n\tRange(f func(uint64) bool)\n}\n\ntype DeleteBlocks []DeleteBlock\n\n// StreamReplicatedState represents what is encoded in a binary stream snapshot used\n// for stream replication in an NRG.\ntype StreamReplicatedState struct {\n\tMsgs     uint64\n\tBytes    uint64\n\tFirstSeq uint64\n\tLastSeq  uint64\n\tFailed   uint64\n\tDeleted  DeleteBlocks\n}\n\n// Determine if this is an encoded stream state.\nfunc IsEncodedStreamState(buf []byte) bool {\n\treturn len(buf) >= hdrLen && buf[0] == streamStateMagic && buf[1] == streamStateVersion\n}\n\nvar ErrBadStreamStateEncoding = errors.New(\"bad stream state encoding\")\n\nfunc DecodeStreamState(buf []byte) (*StreamReplicatedState, error) {\n\tss := &StreamReplicatedState{}\n\tif len(buf) < hdrLen || buf[0] != streamStateMagic || buf[1] != streamStateVersion {\n\t\treturn nil, ErrBadStreamStateEncoding\n\t}\n\tvar bi = hdrLen\n\n\treadU64 := func() uint64 {\n\t\tif bi < 0 || bi >= len(buf) {\n\t\t\tbi = -1\n\t\t\treturn 0\n\t\t}\n\t\tnum, n := binary.Uvarint(buf[bi:])\n\t\tif n <= 0 {\n\t\t\tbi = -1\n\t\t\treturn 0\n\t\t}\n\t\tbi += n\n\t\treturn num\n\t}\n\n\tparserFailed := func() bool {\n\t\treturn bi < 0\n\t}\n\n\tss.Msgs = readU64()\n\tss.Bytes = readU64()\n\tss.FirstSeq = readU64()\n\tss.LastSeq = readU64()\n\tss.Failed = readU64()\n\n\tif parserFailed() {\n\t\treturn nil, ErrCorruptStreamState\n\t}\n\n\tif numDeleted := readU64(); numDeleted > 0 {\n\t\t// If we have some deleted blocks.\n\t\tfor l := len(buf); l > bi; {\n\t\t\tswitch buf[bi] {\n\t\t\tcase seqSetMagic:\n\t\t\t\tdmap, n, err := avl.Decode(buf[bi:])\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn nil, ErrCorruptStreamState\n\t\t\t\t}\n\t\t\t\tbi += n\n\t\t\t\tss.Deleted = append(ss.Deleted, dmap)\n\t\t\tcase runLengthMagic:\n\t\t\t\tbi++\n\t\t\t\tvar rl DeleteRange\n\t\t\t\trl.First = readU64()\n\t\t\t\trl.Num = readU64()\n\t\t\t\tif parserFailed() {\n\t\t\t\t\treturn nil, ErrCorruptStreamState\n\t\t\t\t}\n\t\t\t\tss.Deleted = append(ss.Deleted, &rl)\n\t\t\tdefault:\n\t\t\t\treturn nil, ErrCorruptStreamState\n\t\t\t}\n\t\t}\n\t}\n\n\treturn ss, nil\n}\n\n// DeleteRange is a run length encoded delete range.\ntype DeleteRange struct {\n\tFirst uint64\n\tNum   uint64\n}\n\nfunc (dr *DeleteRange) State() (first, last, num uint64) {\n\tdeletesAfterFirst := dr.Num\n\tif deletesAfterFirst > 0 {\n\t\tdeletesAfterFirst--\n\t}\n\treturn dr.First, dr.First + deletesAfterFirst, dr.Num\n}\n\n// Range will range over all the deleted sequences represented by this block.\nfunc (dr *DeleteRange) Range(f func(uint64) bool) {\n\tfor seq := dr.First; seq < dr.First+dr.Num; seq++ {\n\t\tif !f(seq) {\n\t\t\treturn\n\t\t}\n\t}\n}\n\n// Legacy []uint64\ntype DeleteSlice []uint64\n\nfunc (ds DeleteSlice) State() (first, last, num uint64) {\n\tif len(ds) == 0 {\n\t\treturn 0, 0, 0\n\t}\n\treturn ds[0], ds[len(ds)-1], uint64(len(ds))\n}\n\n// Range will range over all the deleted sequences represented by this []uint64.\nfunc (ds DeleteSlice) Range(f func(uint64) bool) {\n\tfor _, seq := range ds {\n\t\tif !f(seq) {\n\t\t\treturn\n\t\t}\n\t}\n}\n\nfunc (dbs DeleteBlocks) NumDeleted() (total uint64) {\n\tfor _, db := range dbs {\n\t\t_, _, num := db.State()\n\t\ttotal += num\n\t}\n\treturn total\n}\n\n// ConsumerStore stores state on consumers for streams.\ntype ConsumerStore interface {\n\tSetStarting(sseq uint64) error\n\tUpdateStarting(sseq uint64)\n\tHasState() bool\n\tUpdateDelivered(dseq, sseq, dc uint64, ts int64) error\n\tUpdateAcks(dseq, sseq uint64) error\n\tUpdateConfig(cfg *ConsumerConfig) error\n\tUpdate(*ConsumerState) error\n\tState() (*ConsumerState, error)\n\tBorrowState() (*ConsumerState, error)\n\tEncodedState() ([]byte, error)\n\tType() StorageType\n\tStop() error\n\tDelete() error\n\tStreamDelete() error\n}\n\n// SequencePair has both the consumer and the stream sequence. They point to same message.\ntype SequencePair struct {\n\tConsumer uint64 `json:\"consumer_seq\"`\n\tStream   uint64 `json:\"stream_seq\"`\n}\n\n// ConsumerState represents a stored state for a consumer.\ntype ConsumerState struct {\n\t// Delivered keeps track of last delivered sequence numbers for both the stream and the consumer.\n\tDelivered SequencePair `json:\"delivered\"`\n\t// AckFloor keeps track of the ack floors for both the stream and the consumer.\n\tAckFloor SequencePair `json:\"ack_floor\"`\n\t// These are both in stream sequence context.\n\t// Pending is for all messages pending and the timestamp for the delivered time.\n\t// This will only be present when the AckPolicy is ExplicitAck.\n\tPending map[uint64]*Pending `json:\"pending,omitempty\"`\n\t// This is for messages that have been redelivered, so count > 1.\n\tRedelivered map[uint64]uint64 `json:\"redelivered,omitempty\"`\n}\n\n// Encode consumer state.\nfunc encodeConsumerState(state *ConsumerState) []byte {\n\tvar hdr [seqsHdrSize]byte\n\tvar buf []byte\n\n\tmaxSize := seqsHdrSize\n\tif lp := len(state.Pending); lp > 0 {\n\t\tmaxSize += lp*(3*binary.MaxVarintLen64) + binary.MaxVarintLen64\n\t}\n\tif lr := len(state.Redelivered); lr > 0 {\n\t\tmaxSize += lr*(2*binary.MaxVarintLen64) + binary.MaxVarintLen64\n\t}\n\tif maxSize == seqsHdrSize {\n\t\tbuf = hdr[:seqsHdrSize]\n\t} else {\n\t\tbuf = make([]byte, maxSize)\n\t}\n\n\t// Write header\n\tbuf[0] = magic\n\tbuf[1] = 2\n\n\tn := hdrLen\n\tn += binary.PutUvarint(buf[n:], state.AckFloor.Consumer)\n\tn += binary.PutUvarint(buf[n:], state.AckFloor.Stream)\n\tn += binary.PutUvarint(buf[n:], state.Delivered.Consumer)\n\tn += binary.PutUvarint(buf[n:], state.Delivered.Stream)\n\tn += binary.PutUvarint(buf[n:], uint64(len(state.Pending)))\n\n\tasflr := state.AckFloor.Stream\n\tadflr := state.AckFloor.Consumer\n\n\t// These are optional, but always write len. This is to avoid a truncate inline.\n\tif len(state.Pending) > 0 {\n\t\t// To save space we will use now rounded to seconds to be our base timestamp.\n\t\tmints := time.Now().Round(time.Second).Unix()\n\t\t// Write minimum timestamp we found from above.\n\t\tn += binary.PutVarint(buf[n:], mints)\n\n\t\tfor k, v := range state.Pending {\n\t\t\tn += binary.PutUvarint(buf[n:], k-asflr)\n\t\t\tn += binary.PutUvarint(buf[n:], v.Sequence-adflr)\n\t\t\t// Downsample to seconds to save on space.\n\t\t\t// Subsecond resolution not needed for recovery etc.\n\t\t\tts := v.Timestamp / int64(time.Second)\n\t\t\tn += binary.PutVarint(buf[n:], mints-ts)\n\t\t}\n\t}\n\n\t// We always write the redelivered len.\n\tn += binary.PutUvarint(buf[n:], uint64(len(state.Redelivered)))\n\n\t// We expect these to be small.\n\tif len(state.Redelivered) > 0 {\n\t\tfor k, v := range state.Redelivered {\n\t\t\tn += binary.PutUvarint(buf[n:], k-asflr)\n\t\t\tn += binary.PutUvarint(buf[n:], v)\n\t\t}\n\t}\n\n\treturn buf[:n]\n}\n\n// Represents a pending message for explicit ack or ack all.\n// Sequence is the original consumer sequence.\ntype Pending struct {\n\tSequence  uint64\n\tTimestamp int64\n}\n\n// TemplateStore stores templates.\ntype TemplateStore interface {\n\tStore(*streamTemplate) error\n\tDelete(*streamTemplate) error\n}\n\nconst (\n\tlimitsPolicyJSONString    = `\"limits\"`\n\tinterestPolicyJSONString  = `\"interest\"`\n\tworkQueuePolicyJSONString = `\"workqueue\"`\n)\n\nvar (\n\tlimitsPolicyJSONBytes    = []byte(limitsPolicyJSONString)\n\tinterestPolicyJSONBytes  = []byte(interestPolicyJSONString)\n\tworkQueuePolicyJSONBytes = []byte(workQueuePolicyJSONString)\n)\n\nfunc (rp RetentionPolicy) String() string {\n\tswitch rp {\n\tcase LimitsPolicy:\n\t\treturn \"Limits\"\n\tcase InterestPolicy:\n\t\treturn \"Interest\"\n\tcase WorkQueuePolicy:\n\t\treturn \"WorkQueue\"\n\tdefault:\n\t\treturn \"Unknown Retention Policy\"\n\t}\n}\n\nfunc (rp RetentionPolicy) MarshalJSON() ([]byte, error) {\n\tswitch rp {\n\tcase LimitsPolicy:\n\t\treturn limitsPolicyJSONBytes, nil\n\tcase InterestPolicy:\n\t\treturn interestPolicyJSONBytes, nil\n\tcase WorkQueuePolicy:\n\t\treturn workQueuePolicyJSONBytes, nil\n\tdefault:\n\t\treturn nil, fmt.Errorf(\"can not marshal %v\", rp)\n\t}\n}\n\nfunc (rp *RetentionPolicy) UnmarshalJSON(data []byte) error {\n\tswitch string(data) {\n\tcase limitsPolicyJSONString:\n\t\t*rp = LimitsPolicy\n\tcase interestPolicyJSONString:\n\t\t*rp = InterestPolicy\n\tcase workQueuePolicyJSONString:\n\t\t*rp = WorkQueuePolicy\n\tdefault:\n\t\treturn fmt.Errorf(\"can not unmarshal %q\", data)\n\t}\n\treturn nil\n}\n\nfunc (dp DiscardPolicy) String() string {\n\tswitch dp {\n\tcase DiscardOld:\n\t\treturn \"DiscardOld\"\n\tcase DiscardNew:\n\t\treturn \"DiscardNew\"\n\tdefault:\n\t\treturn \"Unknown Discard Policy\"\n\t}\n}\n\nfunc (dp DiscardPolicy) MarshalJSON() ([]byte, error) {\n\tswitch dp {\n\tcase DiscardOld:\n\t\treturn []byte(`\"old\"`), nil\n\tcase DiscardNew:\n\t\treturn []byte(`\"new\"`), nil\n\tdefault:\n\t\treturn nil, fmt.Errorf(\"can not marshal %v\", dp)\n\t}\n}\n\nfunc (dp *DiscardPolicy) UnmarshalJSON(data []byte) error {\n\tswitch strings.ToLower(string(data)) {\n\tcase `\"old\"`:\n\t\t*dp = DiscardOld\n\tcase `\"new\"`:\n\t\t*dp = DiscardNew\n\tdefault:\n\t\treturn fmt.Errorf(\"can not unmarshal %q\", data)\n\t}\n\treturn nil\n}\n\nconst (\n\tmemoryStorageJSONString = `\"memory\"`\n\tfileStorageJSONString   = `\"file\"`\n\tanyStorageJSONString    = `\"any\"`\n)\n\nvar (\n\tmemoryStorageJSONBytes = []byte(memoryStorageJSONString)\n\tfileStorageJSONBytes   = []byte(fileStorageJSONString)\n\tanyStorageJSONBytes    = []byte(anyStorageJSONString)\n)\n\nfunc (st StorageType) String() string {\n\tswitch st {\n\tcase MemoryStorage:\n\t\treturn \"Memory\"\n\tcase FileStorage:\n\t\treturn \"File\"\n\tcase AnyStorage:\n\t\treturn \"Any\"\n\tdefault:\n\t\treturn \"Unknown Storage Type\"\n\t}\n}\n\nfunc (st StorageType) MarshalJSON() ([]byte, error) {\n\tswitch st {\n\tcase MemoryStorage:\n\t\treturn memoryStorageJSONBytes, nil\n\tcase FileStorage:\n\t\treturn fileStorageJSONBytes, nil\n\tcase AnyStorage:\n\t\treturn anyStorageJSONBytes, nil\n\tdefault:\n\t\treturn nil, fmt.Errorf(\"can not marshal %v\", st)\n\t}\n}\n\nfunc (st *StorageType) UnmarshalJSON(data []byte) error {\n\tswitch string(data) {\n\tcase memoryStorageJSONString:\n\t\t*st = MemoryStorage\n\tcase fileStorageJSONString:\n\t\t*st = FileStorage\n\tcase anyStorageJSONString:\n\t\t*st = AnyStorage\n\tdefault:\n\t\treturn fmt.Errorf(\"can not unmarshal %q\", data)\n\t}\n\treturn nil\n}\n\nconst (\n\tackNonePolicyJSONString     = `\"none\"`\n\tackAllPolicyJSONString      = `\"all\"`\n\tackExplicitPolicyJSONString = `\"explicit\"`\n)\n\nvar (\n\tackNonePolicyJSONBytes     = []byte(ackNonePolicyJSONString)\n\tackAllPolicyJSONBytes      = []byte(ackAllPolicyJSONString)\n\tackExplicitPolicyJSONBytes = []byte(ackExplicitPolicyJSONString)\n)\n\nfunc (ap AckPolicy) MarshalJSON() ([]byte, error) {\n\tswitch ap {\n\tcase AckNone:\n\t\treturn ackNonePolicyJSONBytes, nil\n\tcase AckAll:\n\t\treturn ackAllPolicyJSONBytes, nil\n\tcase AckExplicit:\n\t\treturn ackExplicitPolicyJSONBytes, nil\n\tdefault:\n\t\treturn nil, fmt.Errorf(\"can not marshal %v\", ap)\n\t}\n}\n\nfunc (ap *AckPolicy) UnmarshalJSON(data []byte) error {\n\tswitch string(data) {\n\tcase ackNonePolicyJSONString:\n\t\t*ap = AckNone\n\tcase ackAllPolicyJSONString:\n\t\t*ap = AckAll\n\tcase ackExplicitPolicyJSONString:\n\t\t*ap = AckExplicit\n\tdefault:\n\t\treturn fmt.Errorf(\"can not unmarshal %q\", data)\n\t}\n\treturn nil\n}\n\nconst (\n\treplayInstantPolicyJSONString  = `\"instant\"`\n\treplayOriginalPolicyJSONString = `\"original\"`\n)\n\nvar (\n\treplayInstantPolicyJSONBytes  = []byte(replayInstantPolicyJSONString)\n\treplayOriginalPolicyJSONBytes = []byte(replayOriginalPolicyJSONString)\n)\n\nfunc (rp ReplayPolicy) MarshalJSON() ([]byte, error) {\n\tswitch rp {\n\tcase ReplayInstant:\n\t\treturn replayInstantPolicyJSONBytes, nil\n\tcase ReplayOriginal:\n\t\treturn replayOriginalPolicyJSONBytes, nil\n\tdefault:\n\t\treturn nil, fmt.Errorf(\"can not marshal %v\", rp)\n\t}\n}\n\nfunc (rp *ReplayPolicy) UnmarshalJSON(data []byte) error {\n\tswitch string(data) {\n\tcase replayInstantPolicyJSONString:\n\t\t*rp = ReplayInstant\n\tcase replayOriginalPolicyJSONString:\n\t\t*rp = ReplayOriginal\n\tdefault:\n\t\treturn fmt.Errorf(\"can not unmarshal %q\", data)\n\t}\n\treturn nil\n}\n\nconst (\n\tdeliverAllPolicyJSONString       = `\"all\"`\n\tdeliverLastPolicyJSONString      = `\"last\"`\n\tdeliverNewPolicyJSONString       = `\"new\"`\n\tdeliverByStartSequenceJSONString = `\"by_start_sequence\"`\n\tdeliverByStartTimeJSONString     = `\"by_start_time\"`\n\tdeliverLastPerPolicyJSONString   = `\"last_per_subject\"`\n\tdeliverUndefinedJSONString       = `\"undefined\"`\n)\n\nvar (\n\tdeliverAllPolicyJSONBytes       = []byte(deliverAllPolicyJSONString)\n\tdeliverLastPolicyJSONBytes      = []byte(deliverLastPolicyJSONString)\n\tdeliverNewPolicyJSONBytes       = []byte(deliverNewPolicyJSONString)\n\tdeliverByStartSequenceJSONBytes = []byte(deliverByStartSequenceJSONString)\n\tdeliverByStartTimeJSONBytes     = []byte(deliverByStartTimeJSONString)\n\tdeliverLastPerPolicyJSONBytes   = []byte(deliverLastPerPolicyJSONString)\n\tdeliverUndefinedJSONBytes       = []byte(deliverUndefinedJSONString)\n)\n\nfunc (p *DeliverPolicy) UnmarshalJSON(data []byte) error {\n\tswitch string(data) {\n\tcase deliverAllPolicyJSONString, deliverUndefinedJSONString:\n\t\t*p = DeliverAll\n\tcase deliverLastPolicyJSONString:\n\t\t*p = DeliverLast\n\tcase deliverLastPerPolicyJSONString:\n\t\t*p = DeliverLastPerSubject\n\tcase deliverNewPolicyJSONString:\n\t\t*p = DeliverNew\n\tcase deliverByStartSequenceJSONString:\n\t\t*p = DeliverByStartSequence\n\tcase deliverByStartTimeJSONString:\n\t\t*p = DeliverByStartTime\n\tdefault:\n\t\treturn fmt.Errorf(\"can not unmarshal %q\", data)\n\t}\n\n\treturn nil\n}\n\nfunc (p DeliverPolicy) MarshalJSON() ([]byte, error) {\n\tswitch p {\n\tcase DeliverAll:\n\t\treturn deliverAllPolicyJSONBytes, nil\n\tcase DeliverLast:\n\t\treturn deliverLastPolicyJSONBytes, nil\n\tcase DeliverLastPerSubject:\n\t\treturn deliverLastPerPolicyJSONBytes, nil\n\tcase DeliverNew:\n\t\treturn deliverNewPolicyJSONBytes, nil\n\tcase DeliverByStartSequence:\n\t\treturn deliverByStartSequenceJSONBytes, nil\n\tcase DeliverByStartTime:\n\t\treturn deliverByStartTimeJSONBytes, nil\n\tdefault:\n\t\treturn deliverUndefinedJSONBytes, nil\n\t}\n}\n\nfunc isOutOfSpaceErr(err error) bool {\n\treturn err != nil && (strings.Contains(err.Error(), \"no space left\"))\n}\n\n// For when our upper layer catchup detects its missing messages from the beginning of the stream.\nvar errFirstSequenceMismatch = errors.New(\"first sequence mismatch\")\n\nfunc isClusterResetErr(err error) bool {\n\treturn err == errLastSeqMismatch || err == ErrStoreEOF || err == errFirstSequenceMismatch || errors.Is(err, errCatchupAbortedNoLeader) || err == errCatchupTooManyRetries\n}\n\n// Copy all fields.\nfunc (smo *StoreMsg) copy(sm *StoreMsg) {\n\tif sm.buf != nil {\n\t\tsm.buf = sm.buf[:0]\n\t}\n\tsm.buf = append(sm.buf, smo.buf...)\n\t// We set cap on header in case someone wants to expand it.\n\tsm.hdr, sm.msg = sm.buf[:len(smo.hdr):len(smo.hdr)], sm.buf[len(smo.hdr):]\n\tsm.subj, sm.seq, sm.ts = smo.subj, smo.seq, smo.ts\n}\n\n// Clear all fields except underlying buffer but reset that if present to [:0].\nfunc (sm *StoreMsg) clear() {\n\tif sm == nil {\n\t\treturn\n\t}\n\t*sm = StoreMsg{_EMPTY_, nil, nil, sm.buf, 0, 0}\n\tif len(sm.buf) > 0 {\n\t\tsm.buf = sm.buf[:0]\n\t}\n}\n\n// Note this will avoid a copy of the data used for the string, but it will also reference the existing slice's data pointer.\n// So this should be used sparingly when we know the encompassing byte slice's lifetime is the same.\nfunc bytesToString(b []byte) string {\n\tif len(b) == 0 {\n\t\treturn _EMPTY_\n\t}\n\tp := unsafe.SliceData(b)\n\treturn unsafe.String(p, len(b))\n}\n\n// Same in reverse. Used less often.\nfunc stringToBytes(s string) []byte {\n\tif len(s) == 0 {\n\t\treturn nil\n\t}\n\tp := unsafe.StringData(s)\n\tb := unsafe.Slice(p, len(s))\n\treturn b\n}\n\n// Forces a copy of a string, for use in the case that you might have been passed a value when bytesToString was used,\n// but now you need a separate copy of it to store for longer-term use.\nfunc copyString(s string) string {\n\tb := make([]byte, len(s))\n\tcopy(b, s)\n\treturn bytesToString(b)\n}\n\nfunc isPermissionError(err error) bool {\n\treturn err != nil && os.IsPermission(err)\n}\n",
    "source_file": "server/store.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2022-2025 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n//go:build wasm\n\npackage server\n\nfunc diskAvailable(storeDir string) int64 {\n\treturn JetStreamMaxStoreDefault\n}\n",
    "source_file": "server/disk_avail_wasm.go",
    "chunk_type": "code"
  },
  {
    "content": "//go:build ignore\n\npackage main\n\nimport (\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"log\"\n\t\"os\"\n\t\"os/exec\"\n\t\"regexp\"\n\t\"sort\"\n\t\"strings\"\n\t\"text/template\"\n\n\t\"github.com/nats-io/nats-server/v2/server\"\n)\n\nvar tagRe = regexp.MustCompile(\"\\\\{(.+?)}\")\n\nvar templ = `\n// Generated code, do not edit. See errors.json and run go generate to update\n\npackage server\n\nimport \"strings\"\n\nconst (\n{{- range $i, $error := . }}\n{{- if .Comment }}\n\t// {{ .Constant }} {{ .Comment }} ({{ .Description | print }})\n{{- else }}\n\t// {{ .Constant }} {{ .Description | print }}\n{{- end }}\n\t{{ .Constant }} ErrorIdentifier = {{ .ErrCode }}\n{{ end }}\n)\n\nvar (\n\tApiErrors = map[ErrorIdentifier]*ApiError{\n{{- range $i, $error := . }}\n\t\t{{ .Constant }}: {Code: {{ .Code }},ErrCode: {{ .ErrCode }},Description: {{ .Description | printf \"%q\" }}},{{- end }}\n\t}\n\n{{- range $i, $error := . }}\n{{- if .Deprecates }}\n// {{ .Deprecates }} Deprecated by {{ .Constant }} ApiError, use IsNatsError() for comparisons\n{{ .Deprecates }} = ApiErrors[{{ .Constant }}]\n{{- end }}\n{{- end }}\n)\n\n{{- range $i, $error := . }}\n// {{ .Constant | funcNameForConstant }} creates a new {{ .Constant }} error: {{ .Description | printf \"%q\" }}\nfunc {{ .Constant | funcNameForConstant }}({{ .Description | funcArgsForTags }}) *ApiError {\n    eopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n \t}\n{{ if .Description | hasTags }}\n\te:=ApiErrors[{{.Constant}}]\n\targs:=e.toReplacerArgs([]interface{}{ {{.Description | replacerArgsForTags }} })\n\treturn &ApiError{\n\t\tCode:        e.Code,\n\t\tErrCode:     e.ErrCode,\n\t\tDescription: strings.NewReplacer(args...).Replace(e.Description),\n\t}\n{{- else }}\n\treturn ApiErrors[{{.Constant}}]\n{{- end }}\n}\n\n{{- end }}\n`\n\nfunc panicIfErr(err error) {\n\tif err == nil {\n\t\treturn\n\t}\n\tpanic(err)\n}\n\nfunc goFmt(file string) error {\n\tc := exec.Command(\"go\", \"fmt\", file)\n\tout, err := c.CombinedOutput()\n\tif err != nil {\n\t\tlog.Printf(\"go fmt failed: %s\", string(out))\n\t}\n\n\treturn err\n}\n\nfunc checkIncrements(errs []server.ErrorsData) error {\n\tsort.Slice(errs, func(i, j int) bool {\n\t\treturn errs[i].ErrCode < errs[j].ErrCode\n\t})\n\n\tlast := errs[0].ErrCode\n\tgaps := []uint16{}\n\n\tfor i := 1; i < len(errs); i++ {\n\t\tif errs[i].ErrCode != last+1 {\n\t\t\tgaps = append(gaps, last)\n\t\t}\n\t\tlast = errs[i].ErrCode\n\t}\n\n\tif len(gaps) > 0 {\n\t\treturn fmt.Errorf(\"gaps found in sequences: %v\", gaps)\n\t}\n\n\treturn nil\n}\n\nfunc checkDupes(errs []server.ErrorsData) error {\n\tcodes := []uint16{}\n\thighest := uint16(0)\n\tfor _, err := range errs {\n\t\tcodes = append(codes, err.ErrCode)\n\t\tif highest < err.ErrCode {\n\t\t\thighest = err.ErrCode\n\t\t}\n\t}\n\n\tcodeKeys := make(map[uint16]bool)\n\tconstKeys := make(map[string]bool)\n\n\tfor _, entry := range errs {\n\t\tif _, found := codeKeys[entry.ErrCode]; found {\n\t\t\treturn fmt.Errorf(\"duplicate error code %+v, highest code is %d\", entry, highest)\n\t\t}\n\n\t\tif _, found := constKeys[entry.Constant]; found {\n\t\t\treturn fmt.Errorf(\"duplicate error constant %+v\", entry)\n\t\t}\n\n\t\tcodeKeys[entry.ErrCode] = true\n\t\tconstKeys[entry.Constant] = true\n\t}\n\n\treturn nil\n}\n\nfunc findTags(d string) []string {\n\ttags := []string{}\n\tfor _, tag := range tagRe.FindAllStringSubmatch(d, -1) {\n\t\tif len(tag) != 2 {\n\t\t\tcontinue\n\t\t}\n\n\t\ttags = append(tags, tag[1])\n\t}\n\n\tsort.Strings(tags)\n\n\treturn tags\n}\n\nfunc main() {\n\tej, err := os.ReadFile(\"server/errors.json\")\n\tpanicIfErr(err)\n\n\terrs := []server.ErrorsData{}\n\tpanicIfErr(json.Unmarshal(ej, &errs))\n\n\tpanicIfErr(checkDupes(errs))\n\tpanicIfErr(checkIncrements(errs))\n\n\tsort.Slice(errs, func(i, j int) bool {\n\t\treturn errs[i].Constant < errs[j].Constant\n\t})\n\n\tt := template.New(\"errors\").Funcs(\n\t\ttemplate.FuncMap{\n\t\t\t\"inc\": func(i int) int { return i + 1 },\n\t\t\t\"hasTags\": func(d string) bool {\n\t\t\t\treturn strings.Contains(d, \"{\") && strings.Contains(d, \"}\")\n\t\t\t},\n\t\t\t\"replacerArgsForTags\": func(d string) string {\n\t\t\t\tres := []string{}\n\t\t\t\tfor _, tag := range findTags(d) {\n\t\t\t\t\tres = append(res, `\"{`+tag+`}\"`)\n\t\t\t\t\tres = append(res, tag)\n\t\t\t\t}\n\n\t\t\t\treturn strings.Join(res, \", \")\n\t\t\t},\n\t\t\t\"funcArgsForTags\": func(d string) string {\n\t\t\t\tres := []string{}\n\t\t\t\tfor _, tag := range findTags(d) {\n\t\t\t\t\tif tag == \"err\" {\n\t\t\t\t\t\tres = append(res, \"err error\")\n\t\t\t\t\t} else if tag == \"seq\" {\n\t\t\t\t\t\tres = append(res, \"seq uint64\")\n\t\t\t\t\t} else {\n\t\t\t\t\t\tres = append(res, fmt.Sprintf(\"%s interface{}\", tag))\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\tres = append(res, \"opts ...ErrorOption\")\n\n\t\t\t\treturn strings.Join(res, \", \")\n\t\t\t},\n\t\t\t\"funcNameForConstant\": func(c string) string {\n\t\t\t\tres := \"\"\n\n\t\t\t\tswitch {\n\t\t\t\tcase strings.HasSuffix(c, \"ErrF\"):\n\t\t\t\t\tres = fmt.Sprintf(\"New%sError\", strings.TrimSuffix(c, \"ErrF\"))\n\t\t\t\tcase strings.HasSuffix(c, \"Err\"):\n\t\t\t\t\tres = fmt.Sprintf(\"New%sError\", strings.TrimSuffix(c, \"Err\"))\n\t\t\t\tcase strings.HasSuffix(c, \"ErrorF\"):\n\t\t\t\t\tres = fmt.Sprintf(\"New%s\", strings.TrimSuffix(c, \"F\"))\n\t\t\t\tcase strings.HasSuffix(c, \"F\"):\n\t\t\t\t\tres = fmt.Sprintf(\"New%sError\", strings.TrimSuffix(c, \"F\"))\n\t\t\t\tdefault:\n\t\t\t\t\tres = fmt.Sprintf(\"New%s\", c)\n\t\t\t\t}\n\n\t\t\t\tif !strings.HasSuffix(res, \"Error\") {\n\t\t\t\t\tres = fmt.Sprintf(\"%sError\", res)\n\t\t\t\t}\n\n\t\t\t\treturn res\n\t\t\t},\n\t\t})\n\tp, err := t.Parse(templ)\n\tpanicIfErr(err)\n\n\ttf, err := os.CreateTemp(\"\", \"\")\n\tpanicIfErr(err)\n\tdefer tf.Close()\n\n\tpanicIfErr(p.Execute(tf, errs))\n\n\tpanicIfErr(os.Rename(tf.Name(), \"server/jetstream_errors_generated.go\"))\n\tpanicIfErr(goFmt(\"server/jetstream_errors_generated.go\"))\n}\n",
    "source_file": "server/errors_gen.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2021-2025 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage server\n\nimport (\n\t\"errors\"\n\t\"sync\"\n\t\"sync/atomic\"\n)\n\nconst ipQueueDefaultMaxRecycleSize = 4 * 1024\n\n// This is a generic intra-process queue.\ntype ipQueue[T any] struct {\n\tinprogress int64\n\tsync.Mutex\n\tch   chan struct{}\n\telts []T\n\tpos  int\n\tpool *sync.Pool\n\tsz   uint64 // Calculated size (only if calc != nil)\n\tname string\n\tm    *sync.Map\n\tipQueueOpts[T]\n}\n\ntype ipQueueOpts[T any] struct {\n\tmrs  int              // Max recycle size\n\tcalc func(e T) uint64 // Calc function for tracking size\n\tmsz  uint64           // Limit by total calculated size\n\tmlen int              // Limit by number of entries\n}\n\ntype ipQueueOpt[T any] func(*ipQueueOpts[T])\n\n// This option allows to set the maximum recycle size when attempting\n// to put back a slice to the pool.\nfunc ipqMaxRecycleSize[T any](max int) ipQueueOpt[T] {\n\treturn func(o *ipQueueOpts[T]) {\n\t\to.mrs = max\n\t}\n}\n\n// This option enables total queue size counting by passing in a function\n// that evaluates the size of each entry as it is pushed/popped. This option\n// enables the size() function.\nfunc ipqSizeCalculation[T any](calc func(e T) uint64) ipQueueOpt[T] {\n\treturn func(o *ipQueueOpts[T]) {\n\t\to.calc = calc\n\t}\n}\n\n// This option allows setting the maximum queue size. Once the limit is\n// reached, then push() will stop returning true and no more entries will\n// be stored until some more are popped. The ipQueue_SizeCalculation must\n// be provided for this to work.\nfunc ipqLimitBySize[T any](max uint64) ipQueueOpt[T] {\n\treturn func(o *ipQueueOpts[T]) {\n\t\to.msz = max\n\t}\n}\n\n// This option allows setting the maximum queue length. Once the limit is\n// reached, then push() will stop returning true and no more entries will\n// be stored until some more are popped.\nfunc ipqLimitByLen[T any](max int) ipQueueOpt[T] {\n\treturn func(o *ipQueueOpts[T]) {\n\t\to.mlen = max\n\t}\n}\n\nvar errIPQLenLimitReached = errors.New(\"IPQ len limit reached\")\nvar errIPQSizeLimitReached = errors.New(\"IPQ size limit reached\")\n\nfunc newIPQueue[T any](s *Server, name string, opts ...ipQueueOpt[T]) *ipQueue[T] {\n\tq := &ipQueue[T]{\n\t\tch: make(chan struct{}, 1),\n\t\tpool: &sync.Pool{\n\t\t\tNew: func() any {\n\t\t\t\t// Reason we use pointer to slice instead of slice is explained\n\t\t\t\t// here: https://staticcheck.io/docs/checks#SA6002\n\t\t\t\tres := make([]T, 0, 32)\n\t\t\t\treturn &res\n\t\t\t},\n\t\t},\n\t\tname: name,\n\t\tm:    &s.ipQueues,\n\t\tipQueueOpts: ipQueueOpts[T]{\n\t\t\tmrs: ipQueueDefaultMaxRecycleSize,\n\t\t},\n\t}\n\tfor _, o := range opts {\n\t\to(&q.ipQueueOpts)\n\t}\n\ts.ipQueues.Store(name, q)\n\treturn q\n}\n\n// Add the element `e` to the queue, notifying the queue channel's `ch` if the\n// entry is the first to be added, and returns the length of the queue after\n// this element is added.\nfunc (q *ipQueue[T]) push(e T) (int, error) {\n\tq.Lock()\n\tl := len(q.elts) - q.pos\n\tif q.mlen > 0 && l == q.mlen {\n\t\tq.Unlock()\n\t\treturn l, errIPQLenLimitReached\n\t}\n\tif q.calc != nil {\n\t\tsz := q.calc(e)\n\t\tif q.msz > 0 && q.sz+sz > q.msz {\n\t\t\tq.Unlock()\n\t\t\treturn l, errIPQSizeLimitReached\n\t\t}\n\t\tq.sz += sz\n\t}\n\tif q.elts == nil {\n\t\t// What comes out of the pool is already of size 0, so no need for [:0].\n\t\tq.elts = *(q.pool.Get().(*[]T))\n\t}\n\tq.elts = append(q.elts, e)\n\tq.Unlock()\n\tif l == 0 {\n\t\tselect {\n\t\tcase q.ch <- struct{}{}:\n\t\tdefault:\n\t\t}\n\t}\n\treturn l + 1, nil\n}\n\n// Returns the whole list of elements currently present in the queue,\n// emptying the queue. This should be called after receiving a notification\n// from the queue's `ch` notification channel that indicates that there\n// is something in the queue.\n// However, in cases where `drain()` may be called from another go\n// routine, it is possible that a routine is notified that there is\n// something, but by the time it calls `pop()`, the drain() would have\n// emptied the queue. So the caller should never assume that pop() will\n// return a slice of 1 or more, it could return `nil`.\nfunc (q *ipQueue[T]) pop() []T {\n\tif q == nil {\n\t\treturn nil\n\t}\n\tq.Lock()\n\tif len(q.elts)-q.pos == 0 {\n\t\tq.Unlock()\n\t\treturn nil\n\t}\n\tvar elts []T\n\tif q.pos == 0 {\n\t\telts = q.elts\n\t} else {\n\t\telts = q.elts[q.pos:]\n\t}\n\tq.elts, q.pos, q.sz = nil, 0, 0\n\tatomic.AddInt64(&q.inprogress, int64(len(elts)))\n\tq.Unlock()\n\treturn elts\n}\n\n// Returns the first element from the queue, if any. See comment above\n// regarding calling after being notified that there is something and\n// the use of drain(). In short, the caller should always check the\n// boolean return value to ensure that the value is genuine and not a\n// default empty value.\nfunc (q *ipQueue[T]) popOne() (T, bool) {\n\tq.Lock()\n\tl := len(q.elts) - q.pos\n\tif l == 0 {\n\t\tq.Unlock()\n\t\tvar empty T\n\t\treturn empty, false\n\t}\n\te := q.elts[q.pos]\n\tif l--; l > 0 {\n\t\tq.pos++\n\t\tif q.calc != nil {\n\t\t\tq.sz -= q.calc(e)\n\t\t}\n\t\t// We need to re-signal\n\t\tselect {\n\t\tcase q.ch <- struct{}{}:\n\t\tdefault:\n\t\t}\n\t} else {\n\t\t// We have just emptied the queue, so we can reuse unless it is too big.\n\t\tif cap(q.elts) <= q.mrs {\n\t\t\tq.elts = q.elts[:0]\n\t\t} else {\n\t\t\tq.elts = nil\n\t\t}\n\t\tq.pos, q.sz = 0, 0\n\t}\n\tq.Unlock()\n\treturn e, true\n}\n\n// After a pop(), the slice can be recycled for the next push() when\n// a first element is added to the queue.\n// This will also decrement the \"in progress\" count with the length\n// of the slice.\n// WARNING: The caller MUST never reuse `elts`.\nfunc (q *ipQueue[T]) recycle(elts *[]T) {\n\t// If invoked with a nil list, nothing to do.\n\tif elts == nil || *elts == nil {\n\t\treturn\n\t}\n\t// Update the in progress count.\n\tif len(*elts) > 0 {\n\t\tatomic.AddInt64(&q.inprogress, int64(-(len(*elts))))\n\t}\n\t// We also don't want to recycle huge slices, so check against the max.\n\t// q.mrs is normally immutable but can be changed, in a safe way, in some tests.\n\tif cap(*elts) > q.mrs {\n\t\treturn\n\t}\n\t(*elts) = (*elts)[:0]\n\tq.pool.Put(elts)\n}\n\n// Returns the current length of the queue.\nfunc (q *ipQueue[T]) len() int {\n\tq.Lock()\n\tdefer q.Unlock()\n\treturn len(q.elts) - q.pos\n}\n\n// Returns the calculated size of the queue (if ipQueue_SizeCalculation has been\n// passed in), otherwise returns zero.\nfunc (q *ipQueue[T]) size() uint64 {\n\tq.Lock()\n\tdefer q.Unlock()\n\treturn q.sz\n}\n\n// Empty the queue and consumes the notification signal if present.\n// Returns the number of items that were drained from the queue.\n// Note that this could cause a reader go routine that has been\n// notified that there is something in the queue (reading from queue's `ch`)\n// may then get nothing if `drain()` is invoked before the `pop()` or `popOne()`.\nfunc (q *ipQueue[T]) drain() int {\n\tif q == nil {\n\t\treturn 0\n\t}\n\tq.Lock()\n\tolen := len(q.elts) - q.pos\n\tq.elts, q.pos, q.sz = nil, 0, 0\n\t// Consume the signal if it was present to reduce the chance of a reader\n\t// routine to be think that there is something in the queue...\n\tselect {\n\tcase <-q.ch:\n\tdefault:\n\t}\n\tq.Unlock()\n\treturn olen\n}\n\n// Since the length of the queue goes to 0 after a pop(), it is good to\n// have an insight on how many elements are yet to be processed after a pop().\n// For that reason, the queue maintains a count of elements returned through\n// the pop() API. When the caller will call q.recycle(), this count will\n// be reduced by the size of the slice returned by pop().\nfunc (q *ipQueue[T]) inProgress() int64 {\n\treturn atomic.LoadInt64(&q.inprogress)\n}\n\n// Remove this queue from the server's map of ipQueues.\n// All ipQueue operations (such as push/pop/etc..) are still possible.\nfunc (q *ipQueue[T]) unregister() {\n\tif q == nil {\n\t\treturn\n\t}\n\tq.m.Delete(q.name)\n}\n",
    "source_file": "server/ipqueue.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2012-2025 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage server\n\nimport (\n\t\"os\"\n\t\"time\"\n\n\t\"golang.org/x/sys/windows/svc\"\n)\n\nconst (\n\treopenLogCode   = 128\n\treopenLogCmd    = svc.Cmd(reopenLogCode)\n\tldmCode         = 129\n\tldmCmd          = svc.Cmd(ldmCode)\n\tacceptReopenLog = svc.Accepted(reopenLogCode)\n)\n\nvar serviceName = \"nats-server\"\n\n// SetServiceName allows setting a different service name\nfunc SetServiceName(name string) {\n\tserviceName = name\n}\n\n// winServiceWrapper implements the svc.Handler interface for implementing\n// nats-server as a Windows service.\ntype winServiceWrapper struct {\n\tserver *Server\n}\n\nvar dockerized = false\nvar startupDelay = 10 * time.Second\n\nfunc init() {\n\tif v, exists := os.LookupEnv(\"NATS_DOCKERIZED\"); exists && v == \"1\" {\n\t\tdockerized = true\n\t}\n}\n\n// Execute will be called by the package code at the start of\n// the service, and the service will exit once Execute completes.\n// Inside Execute you must read service change requests from r and\n// act accordingly. You must keep service control manager up to date\n// about state of your service by writing into s as required.\n// args contains service name followed by argument strings passed\n// to the service.\n// You can provide service exit code in exitCode return parameter,\n// with 0 being \"no error\". You can also indicate if exit code,\n// if any, is service specific or not by using svcSpecificEC\n// parameter.\nfunc (w *winServiceWrapper) Execute(args []string, changes <-chan svc.ChangeRequest,\n\tstatus chan<- svc.Status) (bool, uint32) {\n\n\tstatus <- svc.Status{State: svc.StartPending}\n\tgo w.server.Start()\n\n\tif v, exists := os.LookupEnv(\"NATS_STARTUP_DELAY\"); exists {\n\t\tif delay, err := time.ParseDuration(v); err == nil {\n\t\t\tstartupDelay = delay\n\t\t} else {\n\t\t\tw.server.Errorf(\"Failed to parse \\\"%v\\\" as a duration for startup: %s\", v, err)\n\t\t}\n\t}\n\t// Wait for accept loop(s) to be started\n\tif !w.server.ReadyForConnections(startupDelay) {\n\t\t// Failed to start.\n\t\treturn false, 1\n\t}\n\n\tstatus <- svc.Status{\n\t\tState:   svc.Running,\n\t\tAccepts: svc.AcceptStop | svc.AcceptShutdown | svc.AcceptParamChange | acceptReopenLog,\n\t}\n\nloop:\n\tfor change := range changes {\n\t\tswitch change.Cmd {\n\t\tcase svc.Interrogate:\n\t\t\tstatus <- change.CurrentStatus\n\t\tcase svc.Stop, svc.Shutdown:\n\t\t\tw.server.Shutdown()\n\t\t\tbreak loop\n\t\tcase reopenLogCmd:\n\t\t\t// File log re-open for rotating file logs.\n\t\t\tw.server.ReOpenLogFile()\n\t\tcase ldmCmd:\n\t\t\tgo w.server.lameDuckMode()\n\t\tcase svc.ParamChange:\n\t\t\tif err := w.server.Reload(); err != nil {\n\t\t\t\tw.server.Errorf(\"Failed to reload server configuration: %s\", err)\n\t\t\t}\n\t\tdefault:\n\t\t\tw.server.Debugf(\"Unexpected control request: %v\", change.Cmd)\n\t\t}\n\t}\n\n\tstatus <- svc.Status{State: svc.StopPending}\n\treturn false, 0\n}\n\n// Run starts the NATS server as a Windows service.\nfunc Run(server *Server) error {\n\tif dockerized {\n\t\tserver.Start()\n\t\treturn nil\n\t}\n\tisWindowsService, err := svc.IsWindowsService()\n\tif err != nil {\n\t\treturn err\n\t}\n\tif !isWindowsService {\n\t\tserver.Start()\n\t\treturn nil\n\t}\n\treturn svc.Run(serviceName, &winServiceWrapper{server})\n}\n\n// isWindowsService indicates if NATS is running as a Windows service.\nfunc isWindowsService() bool {\n\tif dockerized {\n\t\treturn false\n\t}\n\tisWindowsService, _ := svc.IsWindowsService()\n\treturn isWindowsService\n}\n",
    "source_file": "server/service_windows.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2019-2025 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage server\n\nimport (\n\t\"archive/tar\"\n\t\"bytes\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"math\"\n\t\"math/rand\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"reflect\"\n\t\"slices\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t\"github.com/klauspost/compress/s2\"\n\t\"github.com/nats-io/nats-server/v2/server/gsl\"\n\t\"github.com/nats-io/nuid\"\n)\n\n// StreamConfigRequest is used to create or update a stream.\ntype StreamConfigRequest struct {\n\tStreamConfig\n\t// This is not part of the StreamConfig, because its scoped to request,\n\t// and not to the stream itself.\n\tPedantic bool `json:\"pedantic,omitempty\"`\n}\n\n// StreamConfig will determine the name, subjects and retention policy\n// for a given stream. If subjects is empty the name will be used.\ntype StreamConfig struct {\n\tName         string           `json:\"name\"`\n\tDescription  string           `json:\"description,omitempty\"`\n\tSubjects     []string         `json:\"subjects,omitempty\"`\n\tRetention    RetentionPolicy  `json:\"retention\"`\n\tMaxConsumers int              `json:\"max_consumers\"`\n\tMaxMsgs      int64            `json:\"max_msgs\"`\n\tMaxBytes     int64            `json:\"max_bytes\"`\n\tMaxAge       time.Duration    `json:\"max_age\"`\n\tMaxMsgsPer   int64            `json:\"max_msgs_per_subject\"`\n\tMaxMsgSize   int32            `json:\"max_msg_size,omitempty\"`\n\tDiscard      DiscardPolicy    `json:\"discard\"`\n\tStorage      StorageType      `json:\"storage\"`\n\tReplicas     int              `json:\"num_replicas\"`\n\tNoAck        bool             `json:\"no_ack,omitempty\"`\n\tTemplate     string           `json:\"template_owner,omitempty\"`\n\tDuplicates   time.Duration    `json:\"duplicate_window,omitempty\"`\n\tPlacement    *Placement       `json:\"placement,omitempty\"`\n\tMirror       *StreamSource    `json:\"mirror,omitempty\"`\n\tSources      []*StreamSource  `json:\"sources,omitempty\"`\n\tCompression  StoreCompression `json:\"compression\"`\n\tFirstSeq     uint64           `json:\"first_seq,omitempty\"`\n\n\t// Allow applying a subject transform to incoming messages before doing anything else\n\tSubjectTransform *SubjectTransformConfig `json:\"subject_transform,omitempty\"`\n\n\t// Allow republish of the message after being sequenced and stored.\n\tRePublish *RePublish `json:\"republish,omitempty\"`\n\n\t// Allow higher performance, direct access to get individual messages. E.g. KeyValue\n\tAllowDirect bool `json:\"allow_direct\"`\n\t// Allow higher performance and unified direct access for mirrors as well.\n\tMirrorDirect bool `json:\"mirror_direct\"`\n\n\t// Allow KV like semantics to also discard new on a per subject basis\n\tDiscardNewPer bool `json:\"discard_new_per_subject,omitempty\"`\n\n\t// Optional qualifiers. These can not be modified after set to true.\n\n\t// Sealed will seal a stream so no messages can get out or in.\n\tSealed bool `json:\"sealed\"`\n\t// DenyDelete will restrict the ability to delete messages.\n\tDenyDelete bool `json:\"deny_delete\"`\n\t// DenyPurge will restrict the ability to purge messages.\n\tDenyPurge bool `json:\"deny_purge\"`\n\t// AllowRollup allows messages to be placed into the system and purge\n\t// all older messages using a special msg header.\n\tAllowRollup bool `json:\"allow_rollup_hdrs\"`\n\n\t// The following defaults will apply to consumers when created against\n\t// this stream, unless overridden manually.\n\t// TODO(nat): Can/should we name these better?\n\tConsumerLimits StreamConsumerLimits `json:\"consumer_limits\"`\n\n\t// AllowMsgTTL allows header initiated per-message TTLs. If disabled,\n\t// then the `NATS-TTL` header will be ignored.\n\tAllowMsgTTL bool `json:\"allow_msg_ttl\"`\n\n\t// SubjectDeleteMarkerTTL sets the TTL of delete marker messages left behind by\n\t// subject delete markers.\n\tSubjectDeleteMarkerTTL time.Duration `json:\"subject_delete_marker_ttl,omitempty\"`\n\n\t// Metadata is additional metadata for the Stream.\n\tMetadata map[string]string `json:\"metadata,omitempty\"`\n}\n\n// clone performs a deep copy of the StreamConfig struct, returning a new clone with\n// all values copied.\nfunc (cfg *StreamConfig) clone() *StreamConfig {\n\tclone := *cfg\n\tif cfg.Placement != nil {\n\t\tplacement := *cfg.Placement\n\t\tclone.Placement = &placement\n\t}\n\tif cfg.Mirror != nil {\n\t\tmirror := *cfg.Mirror\n\t\tclone.Mirror = &mirror\n\t}\n\tif len(cfg.Sources) > 0 {\n\t\tclone.Sources = make([]*StreamSource, len(cfg.Sources))\n\t\tfor i, cfgSource := range cfg.Sources {\n\t\t\tsource := *cfgSource\n\t\t\tclone.Sources[i] = &source\n\t\t}\n\t}\n\tif cfg.SubjectTransform != nil {\n\t\ttransform := *cfg.SubjectTransform\n\t\tclone.SubjectTransform = &transform\n\t}\n\tif cfg.RePublish != nil {\n\t\trePublish := *cfg.RePublish\n\t\tclone.RePublish = &rePublish\n\t}\n\tif cfg.Metadata != nil {\n\t\tclone.Metadata = make(map[string]string, len(cfg.Metadata))\n\t\tfor k, v := range cfg.Metadata {\n\t\t\tclone.Metadata[k] = v\n\t\t}\n\t}\n\treturn &clone\n}\n\ntype StreamConsumerLimits struct {\n\tInactiveThreshold time.Duration `json:\"inactive_threshold,omitempty\"`\n\tMaxAckPending     int           `json:\"max_ack_pending,omitempty\"`\n}\n\n// SubjectTransformConfig is for applying a subject transform (to matching messages) before doing anything else when a new message is received\ntype SubjectTransformConfig struct {\n\tSource      string `json:\"src\"`\n\tDestination string `json:\"dest\"`\n}\n\n// RePublish is for republishing messages once committed to a stream.\ntype RePublish struct {\n\tSource      string `json:\"src,omitempty\"`\n\tDestination string `json:\"dest\"`\n\tHeadersOnly bool   `json:\"headers_only,omitempty\"`\n}\n\n// JSPubAckResponse is a formal response to a publish operation.\ntype JSPubAckResponse struct {\n\tError *ApiError `json:\"error,omitempty\"`\n\t*PubAck\n}\n\n// ToError checks if the response has a error and if it does converts it to an error\n// avoiding the pitfalls described by https://yourbasic.org/golang/gotcha-why-nil-error-not-equal-nil/\nfunc (r *JSPubAckResponse) ToError() error {\n\tif r.Error == nil {\n\t\treturn nil\n\t}\n\treturn r.Error\n}\n\n// PubAck is the detail you get back from a publish to a stream that was successful.\n// e.g. +OK {\"stream\": \"Orders\", \"seq\": 22}\ntype PubAck struct {\n\tStream    string `json:\"stream\"`\n\tSequence  uint64 `json:\"seq\"`\n\tDomain    string `json:\"domain,omitempty\"`\n\tDuplicate bool   `json:\"duplicate,omitempty\"`\n}\n\n// StreamInfo shows config and current state for this stream.\ntype StreamInfo struct {\n\tConfig     StreamConfig        `json:\"config\"`\n\tCreated    time.Time           `json:\"created\"`\n\tState      StreamState         `json:\"state\"`\n\tDomain     string              `json:\"domain,omitempty\"`\n\tCluster    *ClusterInfo        `json:\"cluster,omitempty\"`\n\tMirror     *StreamSourceInfo   `json:\"mirror,omitempty\"`\n\tSources    []*StreamSourceInfo `json:\"sources,omitempty\"`\n\tAlternates []StreamAlternate   `json:\"alternates,omitempty\"`\n\t// TimeStamp indicates when the info was gathered\n\tTimeStamp time.Time `json:\"ts\"`\n}\n\ntype StreamAlternate struct {\n\tName    string `json:\"name\"`\n\tDomain  string `json:\"domain,omitempty\"`\n\tCluster string `json:\"cluster\"`\n}\n\n// ClusterInfo shows information about the underlying set of servers\n// that make up the stream or consumer.\ntype ClusterInfo struct {\n\tName      string      `json:\"name,omitempty\"`\n\tRaftGroup string      `json:\"raft_group,omitempty\"`\n\tLeader    string      `json:\"leader,omitempty\"`\n\tReplicas  []*PeerInfo `json:\"replicas,omitempty\"`\n}\n\n// PeerInfo shows information about all the peers in the cluster that\n// are supporting the stream or consumer.\ntype PeerInfo struct {\n\tName    string        `json:\"name\"`\n\tCurrent bool          `json:\"current\"`\n\tOffline bool          `json:\"offline,omitempty\"`\n\tActive  time.Duration `json:\"active\"`\n\tLag     uint64        `json:\"lag,omitempty\"`\n\tPeer    string        `json:\"peer\"`\n\t// For migrations.\n\tcluster string\n}\n\n// StreamSourceInfo shows information about an upstream stream source.\ntype StreamSourceInfo struct {\n\tName              string                   `json:\"name\"`\n\tExternal          *ExternalStream          `json:\"external,omitempty\"`\n\tLag               uint64                   `json:\"lag\"`\n\tActive            time.Duration            `json:\"active\"`\n\tError             *ApiError                `json:\"error,omitempty\"`\n\tFilterSubject     string                   `json:\"filter_subject,omitempty\"`\n\tSubjectTransforms []SubjectTransformConfig `json:\"subject_transforms,omitempty\"`\n}\n\n// StreamSource dictates how streams can source from other streams.\ntype StreamSource struct {\n\tName              string                   `json:\"name\"`\n\tOptStartSeq       uint64                   `json:\"opt_start_seq,omitempty\"`\n\tOptStartTime      *time.Time               `json:\"opt_start_time,omitempty\"`\n\tFilterSubject     string                   `json:\"filter_subject,omitempty\"`\n\tSubjectTransforms []SubjectTransformConfig `json:\"subject_transforms,omitempty\"`\n\tExternal          *ExternalStream          `json:\"external,omitempty\"`\n\n\t// Internal\n\tiname string // For indexing when stream names are the same for multiple sources.\n}\n\n// ExternalStream allows you to qualify access to a stream source in another account or domain.\ntype ExternalStream struct {\n\tApiPrefix     string `json:\"api\"`\n\tDeliverPrefix string `json:\"deliver\"`\n}\n\n// Will return the domain for this external stream.\nfunc (ext *ExternalStream) Domain() string {\n\tif ext == nil || ext.ApiPrefix == _EMPTY_ {\n\t\treturn _EMPTY_\n\t}\n\treturn tokenAt(ext.ApiPrefix, 2)\n}\n\n// For managing stream ingest.\nconst (\n\tstreamDefaultMaxQueueMsgs  = 10_000\n\tstreamDefaultMaxQueueBytes = 1024 * 1024 * 128\n)\n\n// Stream is a jetstream stream of messages. When we receive a message internally destined\n// for a Stream we will direct link from the client to this structure.\ntype stream struct {\n\tmu     sync.RWMutex // Read/write lock for the stream.\n\tjs     *jetStream   // The internal *jetStream for the account.\n\tjsa    *jsAccount   // The JetStream account-level information.\n\tacc    *Account     // The account this stream is defined in.\n\tsrv    *Server      // The server we are running in.\n\tclient *client      // The internal JetStream client.\n\tsysc   *client      // The internal JetStream system client.\n\n\t// The current last subscription ID for the subscriptions through `client`.\n\t// Those subscriptions are for the subjects filters being listened to and captured by the stream.\n\tsid atomic.Uint64\n\n\tpubAck    []byte                  // The template (prefix) to generate the pubAck responses for this stream quickly.\n\toutq      *jsOutQ                 // Queue of *jsPubMsg for sending messages.\n\tmsgs      *ipQueue[*inMsg]        // Intra-process queue for the ingress of messages.\n\tgets      *ipQueue[*directGetReq] // Intra-process queue for the direct get requests.\n\tstore     StreamStore             // The storage for this stream.\n\tackq      *ipQueue[uint64]        // Intra-process queue for acks.\n\tlseq      uint64                  // The sequence number of the last message stored in the stream.\n\tlmsgId    string                  // The de-duplication message ID of the last message stored in the stream.\n\tconsumers map[string]*consumer    // The consumers for this stream.\n\tnumFilter int                     // The number of filtered consumers.\n\tcfg       StreamConfig            // The stream's config.\n\tcfgMu     sync.RWMutex            // Config mutex used to solve some races with consumer code\n\tcreated   time.Time               // Time the stream was created.\n\tstype     StorageType             // The storage type.\n\ttier      string                  // The tier is the number of replicas for the stream (e.g. \"R1\" or \"R3\").\n\tddmap     map[string]*ddentry     // The dedupe map.\n\tddarr     []*ddentry              // The dedupe array.\n\tddindex   int                     // The dedupe index.\n\tddtmr     *time.Timer             // The dedupe timer.\n\tqch       chan struct{}           // The quit channel.\n\tmqch      chan struct{}           // The monitor's quit channel.\n\tactive    bool                    // Indicates that there are active internal subscriptions (for the subject filters)\n\t// and/or mirror/sources consumers are scheduled to be established or already started.\n\tddloaded bool        // set to true when the deduplication structures are been built.\n\tclosed   atomic.Bool // Set to true when stop() is called on the stream.\n\n\t// Mirror\n\tmirror *sourceInfo\n\n\t// Sources\n\tsources              map[string]*sourceInfo\n\tsourceSetupSchedules map[string]*time.Timer\n\tsourcesConsumerSetup *time.Timer\n\tsmsgs                *ipQueue[*inMsg] // Intra-process queue for all incoming sourced messages.\n\n\t// Indicates we have direct consumers.\n\tdirects int\n\n\t// For input subject transform.\n\titr *subjectTransform\n\n\t// For republishing.\n\ttr *subjectTransform\n\n\t// For processing consumers without main stream lock.\n\tclsMu sync.RWMutex\n\tcList []*consumer                    // Consumer list.\n\tsch   chan struct{}                  // Channel to signal consumers.\n\tsigq  *ipQueue[*cMsg]                // Intra-process queue for the messages to signal to the consumers.\n\tcsl   *gsl.GenericSublist[*consumer] // Consumer subscription list.\n\n\t// Leader will store seq/msgTrace in clustering mode. Used in applyStreamEntries\n\t// to know if trace event should be sent after processing.\n\tmt map[uint64]*msgTrace\n\n\t// For non limits policy streams when they process an ack before the actual msg.\n\t// Can happen in stretch clusters, multi-cloud, or during catchup for a restarted server.\n\tpreAcks map[uint64]map[*consumer]struct{}\n\n\t// TODO(dlc) - Hide everything below behind two pointers.\n\t// Clustered mode.\n\tsa        *streamAssignment // What the meta controller uses to assign streams to peers.\n\tnode      RaftNode          // Our RAFT node for the stream's group.\n\tcatchup   atomic.Bool       // Used to signal we are in catchup mode.\n\tcatchups  map[string]uint64 // The number of messages that need to be caught per peer.\n\tsyncSub   *subscription     // Internal subscription for sync messages (on \"$JSC.SYNC\").\n\tinfoSub   *subscription     // Internal subscription for stream info requests.\n\tclMu      sync.Mutex        // The mutex for clseq and clfs.\n\tclseq     uint64            // The current last seq being proposed to the NRG layer.\n\tclfs      uint64            // The count (offset) of the number of failed NRG sequences used to compute clseq.\n\tinflight  map[uint64]uint64 // Inflight message sizes per clseq.\n\tlqsent    time.Time         // The time at which the last lost quorum advisory was sent. Used to rate limit.\n\tuch       chan struct{}     // The channel to signal updates to the monitor routine.\n\tinMonitor bool              // True if the monitor routine has been started.\n\n\texpectedPerSubjectSequence  map[uint64]string   // Inflight 'expected per subject' subjects per clseq.\n\texpectedPerSubjectInProcess map[string]struct{} // Current 'expected per subject' subjects in process.\n\n\t// Direct get subscription.\n\tdirectSub *subscription\n\tlastBySub *subscription\n\n\tmonitorWg sync.WaitGroup // Wait group for the monitor routine.\n}\n\ntype sourceInfo struct {\n\tname  string        // The name of the stream being sourced.\n\tiname string        // The unique index name of this particular source.\n\tcname string        // The name of the current consumer for this source.\n\tsub   *subscription // The subscription to the consumer.\n\n\t// (mirrors only) The subscription to the direct get request subject for\n\t// the source stream's name on the `_sys_` queue group.\n\tdsub *subscription\n\n\t// (mirrors only) The subscription to the direct get last per subject request subject for\n\t// the source stream's name on the `_sys_` queue group.\n\tlbsub *subscription\n\n\tmsgs  *ipQueue[*inMsg]    // Intra-process queue for incoming messages.\n\tsseq  uint64              // Last stream message sequence number seen from the source.\n\tdseq  uint64              // Last delivery (i.e. consumer's) sequence number.\n\tlag   uint64              // 0 or number of messages pending (as last reported by the consumer) - 1.\n\terr   *ApiError           // The API error that caused the last consumer setup to fail.\n\tfails int                 // The number of times trying to setup the consumer failed.\n\tlast  atomic.Int64        // Time the consumer was created or of last message it received.\n\tlreq  time.Time           // The last time setupMirrorConsumer/setupSourceConsumer was called.\n\tqch   chan struct{}       // Quit channel.\n\tsip   bool                // Setup in progress.\n\twg    sync.WaitGroup      // WaitGroup for the consumer's go routine.\n\tsf    string              // The subject filter.\n\tsfs   []string            // The subject filters.\n\ttrs   []*subjectTransform // The subject transforms.\n}\n\n// For mirrors and direct get\nconst (\n\tdgetGroup          = sysGroup\n\tdgetCaughtUpThresh = 10\n)\n\n// Headers for published messages.\nconst (\n\tJSMsgId                   = \"Nats-Msg-Id\"\n\tJSExpectedStream          = \"Nats-Expected-Stream\"\n\tJSExpectedLastSeq         = \"Nats-Expected-Last-Sequence\"\n\tJSExpectedLastSubjSeq     = \"Nats-Expected-Last-Subject-Sequence\"\n\tJSExpectedLastSubjSeqSubj = \"Nats-Expected-Last-Subject-Sequence-Subject\"\n\tJSExpectedLastMsgId       = \"Nats-Expected-Last-Msg-Id\"\n\tJSStreamSource            = \"Nats-Stream-Source\"\n\tJSLastConsumerSeq         = \"Nats-Last-Consumer\"\n\tJSLastStreamSeq           = \"Nats-Last-Stream\"\n\tJSConsumerStalled         = \"Nats-Consumer-Stalled\"\n\tJSMsgRollup               = \"Nats-Rollup\"\n\tJSMsgSize                 = \"Nats-Msg-Size\"\n\tJSResponseType            = \"Nats-Response-Type\"\n\tJSMessageTTL              = \"Nats-TTL\"\n\tJSMarkerReason            = \"Nats-Marker-Reason\"\n)\n\n// Headers for republished messages and direct gets.\nconst (\n\tJSStream       = \"Nats-Stream\"\n\tJSSequence     = \"Nats-Sequence\"\n\tJSTimeStamp    = \"Nats-Time-Stamp\"\n\tJSSubject      = \"Nats-Subject\"\n\tJSLastSequence = \"Nats-Last-Sequence\"\n\tJSNumPending   = \"Nats-Num-Pending\"\n\tJSUpToSequence = \"Nats-UpTo-Sequence\"\n)\n\n// Rollups, can be subject only or all messages.\nconst (\n\tJSMsgRollupSubject = \"sub\"\n\tJSMsgRollupAll     = \"all\"\n)\n\n// Applied limits in the Nats-Applied-Limit header.\nconst (\n\tJSMarkerReasonMaxAge = \"MaxAge\"\n\tJSMarkerReasonPurge  = \"Purge\"\n\tJSMarkerReasonRemove = \"Remove\"\n)\n\nconst (\n\tjsCreateResponse = \"create\"\n)\n\n// Dedupe entry\ntype ddentry struct {\n\tid  string // The unique message ID provided by the client.\n\tseq uint64 // The sequence number of the message.\n\tts  int64  // The timestamp of the message.\n}\n\n// Replicas Range\nconst StreamMaxReplicas = 5\n\n// AddStream adds a stream for the given account.\nfunc (a *Account) addStream(config *StreamConfig) (*stream, error) {\n\treturn a.addStreamWithAssignment(config, nil, nil, false)\n}\n\n// AddStreamWithStore adds a stream for the given account with custome store config options.\nfunc (a *Account) addStreamWithStore(config *StreamConfig, fsConfig *FileStoreConfig) (*stream, error) {\n\treturn a.addStreamWithAssignment(config, fsConfig, nil, false)\n}\n\nfunc (a *Account) addStreamPedantic(config *StreamConfig, pedantic bool) (*stream, error) {\n\treturn a.addStreamWithAssignment(config, nil, nil, pedantic)\n}\n\nfunc (a *Account) addStreamWithAssignment(config *StreamConfig, fsConfig *FileStoreConfig, sa *streamAssignment, pedantic bool) (*stream, error) {\n\ts, jsa, err := a.checkForJetStream()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// If we do not have the stream currently assigned to us in cluster mode we will proceed but warn.\n\t// This can happen on startup with restored state where on meta replay we still do not have\n\t// the assignment. Running in single server mode this always returns true.\n\tif !jsa.streamAssigned(config.Name) {\n\t\ts.Debugf(\"Stream '%s > %s' does not seem to be assigned to this server\", a.Name, config.Name)\n\t}\n\n\t// Sensible defaults.\n\tccfg, apiErr := s.checkStreamCfg(config, a, pedantic)\n\tif apiErr != nil {\n\t\treturn nil, apiErr\n\t}\n\tcfg := &ccfg\n\n\tsingleServerMode := !s.JetStreamIsClustered() && s.standAloneMode()\n\tif singleServerMode && cfg.Replicas > 1 {\n\t\treturn nil, ApiErrors[JSStreamReplicasNotSupportedErr]\n\t}\n\n\t// Make sure we are ok when these are done in parallel.\n\t// We used to call Add(1) in the \"else\" clause of the \"if loaded\"\n\t// statement. This caused a data race because it was possible\n\t// that one go routine stores (with count==0) and another routine\n\t// gets \"loaded==true\" and calls wg.Wait() while the other routine\n\t// then calls wg.Add(1). It also could mean that two routines execute\n\t// the rest of the code concurrently.\n\tswg := &sync.WaitGroup{}\n\tswg.Add(1)\n\tv, loaded := jsa.inflight.LoadOrStore(cfg.Name, swg)\n\twg := v.(*sync.WaitGroup)\n\tif loaded {\n\t\twg.Wait()\n\t\t// This waitgroup is \"thrown away\" (since there was an existing one).\n\t\tswg.Done()\n\t} else {\n\t\tdefer func() {\n\t\t\tjsa.inflight.Delete(cfg.Name)\n\t\t\twg.Done()\n\t\t}()\n\t}\n\n\tjs, isClustered := jsa.jetStreamAndClustered()\n\tjsa.mu.Lock()\n\tif mset, ok := jsa.streams[cfg.Name]; ok {\n\t\tjsa.mu.Unlock()\n\t\t// Check to see if configs are same.\n\t\tocfg := mset.config()\n\n\t\t// set the index name on cfg since it would not contain a value for iname while the return from mset.config() does to ensure the DeepEqual works\n\t\tfor _, s := range cfg.Sources {\n\t\t\ts.setIndexName()\n\t\t}\n\n\t\t// Hold lock, because we'll be reading from and writing to a shared object.\n\t\tjs.mu.Lock()\n\t\tcopyStreamMetadata(cfg, &ocfg)\n\t\tdeepEqual := reflect.DeepEqual(cfg, &ocfg)\n\t\tjs.mu.Unlock()\n\t\tif deepEqual {\n\t\t\tif sa != nil {\n\t\t\t\tmset.setStreamAssignment(sa)\n\t\t\t}\n\t\t\treturn mset, nil\n\t\t} else {\n\t\t\treturn nil, ApiErrors[JSStreamNameExistErr]\n\t\t}\n\t}\n\tjsa.usageMu.RLock()\n\tselected, tier, hasTier := jsa.selectLimits(cfg.Replicas)\n\tjsa.usageMu.RUnlock()\n\treserved := int64(0)\n\tif !isClustered {\n\t\treserved = jsa.tieredReservation(tier, cfg)\n\t}\n\tjsa.mu.Unlock()\n\n\tif !hasTier {\n\t\treturn nil, NewJSNoLimitsError()\n\t}\n\tjs.mu.RLock()\n\tif isClustered {\n\t\t_, reserved = tieredStreamAndReservationCount(js.cluster.streams[a.Name], tier, cfg)\n\t}\n\tif err := js.checkAllLimits(&selected, cfg, reserved, 0); err != nil {\n\t\tjs.mu.RUnlock()\n\t\treturn nil, err\n\t}\n\tjs.mu.RUnlock()\n\tjsa.mu.Lock()\n\t// Check for template ownership if present.\n\tif cfg.Template != _EMPTY_ && jsa.account != nil {\n\t\tif !jsa.checkTemplateOwnership(cfg.Template, cfg.Name) {\n\t\t\tjsa.mu.Unlock()\n\t\t\treturn nil, fmt.Errorf(\"stream not owned by template\")\n\t\t}\n\t}\n\n\t// If mirror, check if the transforms (if any) are valid.\n\tif cfg.Mirror != nil {\n\t\tif len(cfg.Mirror.SubjectTransforms) == 0 {\n\t\t\tif cfg.Mirror.FilterSubject != _EMPTY_ && !IsValidSubject(cfg.Mirror.FilterSubject) {\n\t\t\t\tjsa.mu.Unlock()\n\t\t\t\treturn nil, fmt.Errorf(\"subject filter '%s' for the mirror %w\", cfg.Mirror.FilterSubject, ErrBadSubject)\n\t\t\t}\n\t\t} else {\n\t\t\tfor _, st := range cfg.Mirror.SubjectTransforms {\n\t\t\t\tif st.Source != _EMPTY_ && !IsValidSubject(st.Source) {\n\t\t\t\t\tjsa.mu.Unlock()\n\t\t\t\t\treturn nil, fmt.Errorf(\"invalid subject transform source '%s' for the mirror: %w\", st.Source, ErrBadSubject)\n\t\t\t\t}\n\t\t\t\t// check the transform, if any, is valid\n\t\t\t\tif st.Destination != _EMPTY_ {\n\t\t\t\t\tif _, err = NewSubjectTransform(st.Source, st.Destination); err != nil {\n\t\t\t\t\t\tjsa.mu.Unlock()\n\t\t\t\t\t\treturn nil, fmt.Errorf(\"subject transform from '%s' to '%s' for the mirror: %w\", st.Source, st.Destination, err)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\t// Setup our internal indexed names here for sources and check if the transforms (if any) are valid.\n\tfor _, ssi := range cfg.Sources {\n\t\tif len(ssi.SubjectTransforms) == 0 {\n\t\t\t// check the filter, if any, is valid\n\t\t\tif ssi.FilterSubject != _EMPTY_ && !IsValidSubject(ssi.FilterSubject) {\n\t\t\t\tjsa.mu.Unlock()\n\t\t\t\treturn nil, fmt.Errorf(\"subject filter '%s' for the source: %w\", ssi.FilterSubject, ErrBadSubject)\n\t\t\t}\n\t\t} else {\n\t\t\tfor _, st := range ssi.SubjectTransforms {\n\t\t\t\tif st.Source != _EMPTY_ && !IsValidSubject(st.Source) {\n\t\t\t\t\tjsa.mu.Unlock()\n\t\t\t\t\treturn nil, fmt.Errorf(\"subject filter '%s' for the source: %w\", st.Source, ErrBadSubject)\n\t\t\t\t}\n\t\t\t\t// check the transform, if any, is valid\n\t\t\t\tif st.Destination != _EMPTY_ {\n\t\t\t\t\tif _, err = NewSubjectTransform(st.Source, st.Destination); err != nil {\n\t\t\t\t\t\tjsa.mu.Unlock()\n\t\t\t\t\t\treturn nil, fmt.Errorf(\"subject transform from '%s' to '%s' for the source: %w\", st.Source, st.Destination, err)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\t// Check for overlapping subjects with other streams.\n\t// These are not allowed for now.\n\tif jsa.subjectsOverlap(cfg.Subjects, nil) {\n\t\tjsa.mu.Unlock()\n\t\treturn nil, NewJSStreamSubjectOverlapError()\n\t}\n\n\tif !hasTier {\n\t\tjsa.mu.Unlock()\n\t\treturn nil, fmt.Errorf(\"no applicable tier found\")\n\t}\n\n\t// Setup the internal clients.\n\tc := s.createInternalJetStreamClient()\n\tic := s.createInternalJetStreamClient()\n\n\t// Work out the stream ingest limits.\n\tmlen := s.opts.StreamMaxBufferedMsgs\n\tmsz := uint64(s.opts.StreamMaxBufferedSize)\n\tif mlen == 0 {\n\t\tmlen = streamDefaultMaxQueueMsgs\n\t}\n\tif msz == 0 {\n\t\tmsz = streamDefaultMaxQueueBytes\n\t}\n\n\tqpfx := fmt.Sprintf(\"[ACC:%s] stream '%s' \", a.Name, config.Name)\n\tmset := &stream{\n\t\tacc:       a,\n\t\tjsa:       jsa,\n\t\tcfg:       *cfg,\n\t\tjs:        js,\n\t\tsrv:       s,\n\t\tclient:    c,\n\t\tsysc:      ic,\n\t\ttier:      tier,\n\t\tstype:     cfg.Storage,\n\t\tconsumers: make(map[string]*consumer),\n\t\tmsgs: newIPQueue[*inMsg](s, qpfx+\"messages\",\n\t\t\tipqSizeCalculation(func(msg *inMsg) uint64 {\n\t\t\t\treturn uint64(len(msg.hdr) + len(msg.msg) + len(msg.rply) + len(msg.subj))\n\t\t\t}),\n\t\t\tipqLimitByLen[*inMsg](mlen),\n\t\t\tipqLimitBySize[*inMsg](msz),\n\t\t),\n\t\tgets: newIPQueue[*directGetReq](s, qpfx+\"direct gets\"),\n\t\tqch:  make(chan struct{}),\n\t\tmqch: make(chan struct{}),\n\t\tuch:  make(chan struct{}, 4),\n\t\tsch:  make(chan struct{}, 1),\n\t}\n\n\t// Start our signaling routine to process consumers.\n\tmset.sigq = newIPQueue[*cMsg](s, qpfx+\"obs\") // of *cMsg\n\tgo mset.signalConsumersLoop()\n\n\t// For no-ack consumers when we are interest retention.\n\tif cfg.Retention != LimitsPolicy {\n\t\tmset.ackq = newIPQueue[uint64](s, qpfx+\"acks\")\n\t}\n\n\t// Check for input subject transform\n\tif cfg.SubjectTransform != nil {\n\t\ttr, err := NewSubjectTransform(cfg.SubjectTransform.Source, cfg.SubjectTransform.Destination)\n\t\tif err != nil {\n\t\t\tjsa.mu.Unlock()\n\t\t\treturn nil, fmt.Errorf(\"stream subject transform from '%s' to '%s': %w\", cfg.SubjectTransform.Source, cfg.SubjectTransform.Destination, err)\n\t\t}\n\t\tmset.itr = tr\n\t}\n\n\t// Check for RePublish.\n\tif cfg.RePublish != nil {\n\t\ttr, err := NewSubjectTransform(cfg.RePublish.Source, cfg.RePublish.Destination)\n\t\tif err != nil {\n\t\t\tjsa.mu.Unlock()\n\t\t\treturn nil, fmt.Errorf(\"stream republish transform from '%s' to '%s': %w\", cfg.RePublish.Source, cfg.RePublish.Destination, err)\n\t\t}\n\t\t// Assign our transform for republishing.\n\t\tmset.tr = tr\n\t}\n\tstoreDir := filepath.Join(jsa.storeDir, streamsDir, cfg.Name)\n\tjsa.mu.Unlock()\n\n\t// Bind to the user account.\n\tc.registerWithAccount(a)\n\t// Bind to the system account.\n\tic.registerWithAccount(s.SystemAccount())\n\n\t// Create the appropriate storage\n\tfsCfg := fsConfig\n\tif fsCfg == nil {\n\t\tfsCfg = &FileStoreConfig{}\n\t\t// If we are file based and not explicitly configured\n\t\t// we may be able to auto-tune based on max msgs or bytes.\n\t\tif cfg.Storage == FileStorage {\n\t\t\tmset.autoTuneFileStorageBlockSize(fsCfg)\n\t\t}\n\t}\n\tfsCfg.StoreDir = storeDir\n\tfsCfg.AsyncFlush = false\n\t// Grab configured sync interval.\n\tfsCfg.SyncInterval = s.getOpts().SyncInterval\n\tfsCfg.SyncAlways = s.getOpts().SyncAlways\n\tfsCfg.Compression = config.Compression\n\n\tif err := mset.setupStore(fsCfg); err != nil {\n\t\tmset.stop(true, false)\n\t\treturn nil, NewJSStreamStoreFailedError(err)\n\t}\n\n\t// Create our pubAck template here. Better than json marshal each time on success.\n\tif domain := s.getOpts().JetStreamDomain; domain != _EMPTY_ {\n\t\tmset.pubAck = []byte(fmt.Sprintf(\"{%q:%q, %q:%q, %q:\", \"stream\", cfg.Name, \"domain\", domain, \"seq\"))\n\t} else {\n\t\tmset.pubAck = []byte(fmt.Sprintf(\"{%q:%q, %q:\", \"stream\", cfg.Name, \"seq\"))\n\t}\n\tend := len(mset.pubAck)\n\tmset.pubAck = mset.pubAck[:end:end]\n\n\t// Set our known last sequence.\n\tvar state StreamState\n\tmset.store.FastState(&state)\n\n\t// Possible race with consumer.setLeader during recovery.\n\tmset.mu.Lock()\n\tmset.lseq = state.LastSeq\n\tmset.mu.Unlock()\n\n\t// If no msgs (new stream), set dedupe state loaded to true.\n\tif state.Msgs == 0 {\n\t\tmset.ddloaded = true\n\t}\n\n\t// Set our stream assignment if in clustered mode.\n\treserveResources := true\n\tif sa != nil {\n\t\tmset.setStreamAssignment(sa)\n\n\t\t// If the stream is resetting we must not double-account resources, they were already accounted for.\n\t\tjs.mu.Lock()\n\t\tif sa.resetting {\n\t\t\treserveResources, sa.resetting = false, false\n\t\t}\n\t\tjs.mu.Unlock()\n\t}\n\n\t// Setup our internal send go routine.\n\tmset.setupSendCapabilities()\n\n\t// Reserve resources if MaxBytes present.\n\tif reserveResources {\n\t\tmset.js.reserveStreamResources(&mset.cfg)\n\t}\n\n\t// Call directly to set leader if not in clustered mode.\n\t// This can be called though before we actually setup clustering, so check both.\n\tif singleServerMode {\n\t\tif err := mset.setLeader(true); err != nil {\n\t\t\tmset.stop(true, false)\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\t// This is always true in single server mode.\n\tif mset.IsLeader() {\n\t\t// Send advisory.\n\t\tvar suppress bool\n\t\tif !s.standAloneMode() && sa == nil {\n\t\t\tif cfg.Replicas > 1 {\n\t\t\t\tsuppress = true\n\t\t\t}\n\t\t} else if sa != nil {\n\t\t\tsuppress = sa.responded\n\t\t}\n\t\tif !suppress {\n\t\t\tmset.sendCreateAdvisory()\n\t\t}\n\t}\n\n\t// Register with our account last.\n\tjsa.mu.Lock()\n\tjsa.streams[cfg.Name] = mset\n\tjsa.mu.Unlock()\n\n\treturn mset, nil\n}\n\n// Composes the index name. Contains the stream name, subject filter, and transform destination\n// when the stream is external we will use the api prefix as part of the index name\n// (as the same stream name could be used in multiple JS domains)\nfunc (ssi *StreamSource) composeIName() string {\n\tvar iName = ssi.Name\n\n\tif ssi.External != nil {\n\t\tiName = iName + \":\" + getHash(ssi.External.ApiPrefix)\n\t}\n\n\tsource := ssi.FilterSubject\n\tdestination := fwcs\n\n\tif len(ssi.SubjectTransforms) == 0 {\n\t\t// normalize filter and destination in case they are empty\n\t\tif source == _EMPTY_ {\n\t\t\tsource = fwcs\n\t\t}\n\t\tif destination == _EMPTY_ {\n\t\t\tdestination = fwcs\n\t\t}\n\t} else {\n\t\tvar sources, destinations []string\n\n\t\tfor _, tr := range ssi.SubjectTransforms {\n\t\t\ttrsrc, trdest := tr.Source, tr.Destination\n\t\t\tif trsrc == _EMPTY_ {\n\t\t\t\ttrsrc = fwcs\n\t\t\t}\n\t\t\tif trdest == _EMPTY_ {\n\t\t\t\ttrdest = fwcs\n\t\t\t}\n\t\t\tsources = append(sources, trsrc)\n\t\t\tdestinations = append(destinations, trdest)\n\t\t}\n\t\tsource = strings.Join(sources, \"\\f\")\n\t\tdestination = strings.Join(destinations, \"\\f\")\n\t}\n\n\treturn strings.Join([]string{iName, source, destination}, \" \")\n}\n\n// Sets the index name.\nfunc (ssi *StreamSource) setIndexName() {\n\tssi.iname = ssi.composeIName()\n}\n\nfunc (mset *stream) streamAssignment() *streamAssignment {\n\tmset.mu.RLock()\n\tdefer mset.mu.RUnlock()\n\treturn mset.sa\n}\n\nfunc (mset *stream) setStreamAssignment(sa *streamAssignment) {\n\tvar node RaftNode\n\tvar peers []string\n\n\tmset.mu.RLock()\n\tjs := mset.js\n\tmset.mu.RUnlock()\n\n\tif js != nil {\n\t\tjs.mu.RLock()\n\t\tif sa.Group != nil {\n\t\t\tnode = sa.Group.node\n\t\t\tpeers = sa.Group.Peers\n\t\t}\n\t\tjs.mu.RUnlock()\n\t}\n\n\tmset.mu.Lock()\n\tdefer mset.mu.Unlock()\n\n\tmset.sa = sa\n\tif sa == nil {\n\t\treturn\n\t}\n\n\t// Set our node.\n\tmset.node = node\n\tif mset.node != nil {\n\t\tmset.node.UpdateKnownPeers(peers)\n\t}\n\n\t// Setup our info sub here as well for all stream members. This is now by design.\n\tif mset.infoSub == nil {\n\t\tisubj := fmt.Sprintf(clusterStreamInfoT, mset.jsa.acc(), mset.cfg.Name)\n\t\t// Note below the way we subscribe here is so that we can send requests to ourselves.\n\t\tmset.infoSub, _ = mset.srv.systemSubscribe(isubj, _EMPTY_, false, mset.sysc, mset.handleClusterStreamInfoRequest)\n\t}\n\n\t// Trigger update chan.\n\tselect {\n\tcase mset.uch <- struct{}{}:\n\tdefault:\n\t}\n}\n\nfunc (mset *stream) monitorQuitC() <-chan struct{} {\n\tif mset == nil {\n\t\treturn nil\n\t}\n\tmset.mu.RLock()\n\tdefer mset.mu.RUnlock()\n\treturn mset.mqch\n}\n\nfunc (mset *stream) updateC() <-chan struct{} {\n\tif mset == nil {\n\t\treturn nil\n\t}\n\tmset.mu.RLock()\n\tdefer mset.mu.RUnlock()\n\treturn mset.uch\n}\n\n// IsLeader will return if we are the current leader.\nfunc (mset *stream) IsLeader() bool {\n\tmset.mu.RLock()\n\tdefer mset.mu.RUnlock()\n\treturn mset.isLeader()\n}\n\n// Lock should be held.\nfunc (mset *stream) isLeader() bool {\n\tif mset.isClustered() {\n\t\treturn mset.node.Leader()\n\t}\n\treturn true\n}\n\n// isLeaderNodeState should NOT be used normally, use isLeader instead.\n// Returns whether the node thinks it is the leader, regardless of whether applies are up-to-date yet\n// (unlike isLeader, which requires applies to be caught up).\n// May be used to respond to clients after a leader change, when applying entries from a former leader.\n// Lock should be held.\nfunc (mset *stream) isLeaderNodeState() bool {\n\tif mset.isClustered() {\n\t\treturn mset.node.State() == Leader\n\t}\n\treturn true\n}\n\n// TODO(dlc) - Check to see if we can accept being the leader or we should step down.\nfunc (mset *stream) setLeader(isLeader bool) error {\n\tmset.mu.Lock()\n\t// If we are here we have a change in leader status.\n\tif isLeader {\n\t\t// Make sure we are listening for sync requests.\n\t\t// TODO(dlc) - Original design was that all in sync members of the group would do DQ.\n\t\tif mset.isClustered() {\n\t\t\tmset.startClusterSubs()\n\t\t}\n\n\t\t// Setup subscriptions if we were not already the leader.\n\t\tif err := mset.subscribeToStream(); err != nil {\n\t\t\tif mset.isClustered() {\n\t\t\t\t// Stepdown since we have an error.\n\t\t\t\tmset.node.StepDown()\n\t\t\t}\n\t\t\tmset.mu.Unlock()\n\t\t\treturn err\n\t\t}\n\t} else {\n\t\t// cancel timer to create the source consumers if not fired yet\n\t\tif mset.sourcesConsumerSetup != nil {\n\t\t\tmset.sourcesConsumerSetup.Stop()\n\t\t\tmset.sourcesConsumerSetup = nil\n\t\t} else {\n\t\t\t// Stop any source consumers\n\t\t\tmset.stopSourceConsumers()\n\t\t}\n\n\t\t// Stop responding to sync requests.\n\t\tmset.stopClusterSubs()\n\t\t// Unsubscribe from direct stream.\n\t\tmset.unsubscribeToStream(false)\n\t\t// Clear catchup state\n\t\tmset.clearAllCatchupPeers()\n\t}\n\tmset.mu.Unlock()\n\n\t// If we are interest based make sure to check consumers.\n\t// This is to make sure we process any outstanding acks.\n\tmset.checkInterestState()\n\n\treturn nil\n}\n\n// Lock should be held.\nfunc (mset *stream) startClusterSubs() {\n\tif mset.syncSub == nil {\n\t\tmset.syncSub, _ = mset.srv.systemSubscribe(mset.sa.Sync, _EMPTY_, false, mset.sysc, mset.handleClusterSyncRequest)\n\t}\n}\n\n// Lock should be held.\nfunc (mset *stream) stopClusterSubs() {\n\tif mset.syncSub != nil {\n\t\tmset.srv.sysUnsubscribe(mset.syncSub)\n\t\tmset.syncSub = nil\n\t}\n}\n\n// account gets the account for this stream.\nfunc (mset *stream) account() *Account {\n\tmset.mu.RLock()\n\tjsa := mset.jsa\n\tmset.mu.RUnlock()\n\tif jsa == nil {\n\t\treturn nil\n\t}\n\treturn jsa.acc()\n}\n\n// Helper to determine the max msg size for this stream if file based.\nfunc (mset *stream) maxMsgSize() uint64 {\n\tmaxMsgSize := mset.cfg.MaxMsgSize\n\tif maxMsgSize <= 0 {\n\t\t// Pull from the account.\n\t\tif mset.jsa != nil {\n\t\t\tif acc := mset.jsa.acc(); acc != nil {\n\t\t\t\tacc.mu.RLock()\n\t\t\t\tmaxMsgSize = acc.mpay\n\t\t\t\tacc.mu.RUnlock()\n\t\t\t}\n\t\t}\n\t\t// If all else fails use default.\n\t\tif maxMsgSize <= 0 {\n\t\t\tmaxMsgSize = MAX_PAYLOAD_SIZE\n\t\t}\n\t}\n\t// Now determine an estimation for the subjects etc.\n\tmaxSubject := -1\n\tfor _, subj := range mset.cfg.Subjects {\n\t\tif subjectIsLiteral(subj) {\n\t\t\tif len(subj) > maxSubject {\n\t\t\t\tmaxSubject = len(subj)\n\t\t\t}\n\t\t}\n\t}\n\tif maxSubject < 0 {\n\t\tconst defaultMaxSubject = 256\n\t\tmaxSubject = defaultMaxSubject\n\t}\n\t// filestore will add in estimates for record headers, etc.\n\treturn fileStoreMsgSizeEstimate(maxSubject, int(maxMsgSize))\n}\n\n// If we are file based and the file storage config was not explicitly set\n// we can autotune block sizes to better match. Our target will be to store 125%\n// of the theoretical limit. We will round up to nearest 100 bytes as well.\nfunc (mset *stream) autoTuneFileStorageBlockSize(fsCfg *FileStoreConfig) {\n\tvar totalEstSize uint64\n\n\t// MaxBytes will take precedence for now.\n\tif mset.cfg.MaxBytes > 0 {\n\t\ttotalEstSize = uint64(mset.cfg.MaxBytes)\n\t} else if mset.cfg.MaxMsgs > 0 {\n\t\t// Determine max message size to estimate.\n\t\ttotalEstSize = mset.maxMsgSize() * uint64(mset.cfg.MaxMsgs)\n\t} else if mset.cfg.MaxMsgsPer > 0 {\n\t\tfsCfg.BlockSize = uint64(defaultKVBlockSize)\n\t\treturn\n\t} else {\n\t\t// If nothing set will let underlying filestore determine blkSize.\n\t\treturn\n\t}\n\n\tblkSize := (totalEstSize / 4) + 1 // (25% overhead)\n\t// Round up to nearest 100\n\tif m := blkSize % 100; m != 0 {\n\t\tblkSize += 100 - m\n\t}\n\tif blkSize <= FileStoreMinBlkSize {\n\t\tblkSize = FileStoreMinBlkSize\n\t} else if blkSize >= FileStoreMaxBlkSize {\n\t\tblkSize = FileStoreMaxBlkSize\n\t} else {\n\t\tblkSize = defaultMediumBlockSize\n\t}\n\tfsCfg.BlockSize = uint64(blkSize)\n}\n\n// rebuildDedupe will rebuild any dedupe structures needed after recovery of a stream.\n// Will be called lazily to avoid penalizing startup times.\n// TODO(dlc) - Might be good to know if this should be checked at all for streams with no\n// headers and msgId in them. Would need signaling from the storage layer.\n// Lock should be held.\nfunc (mset *stream) rebuildDedupe() {\n\tif mset.ddloaded {\n\t\treturn\n\t}\n\n\tmset.ddloaded = true\n\n\t// We have some messages. Lookup starting sequence by duplicate time window.\n\tsseq := mset.store.GetSeqFromTime(time.Now().Add(-mset.cfg.Duplicates))\n\tif sseq == 0 {\n\t\treturn\n\t}\n\n\tvar smv StoreMsg\n\tvar state StreamState\n\tmset.store.FastState(&state)\n\n\tfor seq := sseq; seq <= state.LastSeq; seq++ {\n\t\tsm, err := mset.store.LoadMsg(seq, &smv)\n\t\tif err != nil {\n\t\t\tcontinue\n\t\t}\n\t\tvar msgId string\n\t\tif len(sm.hdr) > 0 {\n\t\t\tif msgId = getMsgId(sm.hdr); msgId != _EMPTY_ {\n\t\t\t\tmset.storeMsgIdLocked(&ddentry{msgId, sm.seq, sm.ts})\n\t\t\t}\n\t\t}\n\t\tif seq == state.LastSeq {\n\t\t\tmset.lmsgId = msgId\n\t\t}\n\t}\n}\n\nfunc (mset *stream) lastSeqAndCLFS() (uint64, uint64) {\n\treturn mset.lastSeq(), mset.getCLFS()\n}\n\nfunc (mset *stream) getCLFS() uint64 {\n\tif mset == nil {\n\t\treturn 0\n\t}\n\tmset.clMu.Lock()\n\tdefer mset.clMu.Unlock()\n\treturn mset.clfs\n}\n\nfunc (mset *stream) setCLFS(clfs uint64) {\n\tmset.clMu.Lock()\n\tmset.clfs = clfs\n\tmset.clMu.Unlock()\n}\n\nfunc (mset *stream) lastSeq() uint64 {\n\tmset.mu.RLock()\n\tdefer mset.mu.RUnlock()\n\treturn mset.lseq\n}\n\n// Set last seq.\n// Write lock should be held.\nfunc (mset *stream) setLastSeq(lseq uint64) {\n\tmset.lseq = lseq\n}\n\nfunc (mset *stream) sendCreateAdvisory() {\n\tmset.mu.RLock()\n\tname := mset.cfg.Name\n\ttemplate := mset.cfg.Template\n\toutq := mset.outq\n\tsrv := mset.srv\n\tmset.mu.RUnlock()\n\n\tif outq == nil {\n\t\treturn\n\t}\n\n\t// finally send an event that this stream was created\n\tm := JSStreamActionAdvisory{\n\t\tTypedEvent: TypedEvent{\n\t\t\tType: JSStreamActionAdvisoryType,\n\t\t\tID:   nuid.Next(),\n\t\t\tTime: time.Now().UTC(),\n\t\t},\n\t\tStream:   name,\n\t\tAction:   CreateEvent,\n\t\tTemplate: template,\n\t\tDomain:   srv.getOpts().JetStreamDomain,\n\t}\n\n\tj, err := json.Marshal(m)\n\tif err != nil {\n\t\treturn\n\t}\n\n\tsubj := JSAdvisoryStreamCreatedPre + \".\" + name\n\toutq.sendMsg(subj, j)\n}\n\nfunc (mset *stream) sendDeleteAdvisoryLocked() {\n\tif mset.outq == nil {\n\t\treturn\n\t}\n\n\tm := JSStreamActionAdvisory{\n\t\tTypedEvent: TypedEvent{\n\t\t\tType: JSStreamActionAdvisoryType,\n\t\t\tID:   nuid.Next(),\n\t\t\tTime: time.Now().UTC(),\n\t\t},\n\t\tStream:   mset.cfg.Name,\n\t\tAction:   DeleteEvent,\n\t\tTemplate: mset.cfg.Template,\n\t\tDomain:   mset.srv.getOpts().JetStreamDomain,\n\t}\n\n\tj, err := json.Marshal(m)\n\tif err == nil {\n\t\tsubj := JSAdvisoryStreamDeletedPre + \".\" + mset.cfg.Name\n\t\tmset.outq.sendMsg(subj, j)\n\t}\n}\n\nfunc (mset *stream) sendUpdateAdvisoryLocked() {\n\tif mset.outq == nil {\n\t\treturn\n\t}\n\n\tm := JSStreamActionAdvisory{\n\t\tTypedEvent: TypedEvent{\n\t\t\tType: JSStreamActionAdvisoryType,\n\t\t\tID:   nuid.Next(),\n\t\t\tTime: time.Now().UTC(),\n\t\t},\n\t\tStream: mset.cfg.Name,\n\t\tAction: ModifyEvent,\n\t\tDomain: mset.srv.getOpts().JetStreamDomain,\n\t}\n\n\tj, err := json.Marshal(m)\n\tif err == nil {\n\t\tsubj := JSAdvisoryStreamUpdatedPre + \".\" + mset.cfg.Name\n\t\tmset.outq.sendMsg(subj, j)\n\t}\n}\n\n// Created returns created time.\nfunc (mset *stream) createdTime() time.Time {\n\tmset.mu.RLock()\n\tcreated := mset.created\n\tmset.mu.RUnlock()\n\treturn created\n}\n\n// Internal to allow creation time to be restored.\nfunc (mset *stream) setCreatedTime(created time.Time) {\n\tmset.mu.Lock()\n\tmset.created = created\n\tmset.mu.Unlock()\n}\n\n// subjectsOverlap to see if these subjects overlap with existing subjects.\n// Use only for non-clustered JetStream\n// RLock minimum should be held.\nfunc (jsa *jsAccount) subjectsOverlap(subjects []string, self *stream) bool {\n\tfor _, mset := range jsa.streams {\n\t\tif self != nil && mset == self {\n\t\t\tcontinue\n\t\t}\n\t\tfor _, subj := range mset.cfg.Subjects {\n\t\t\tfor _, tsubj := range subjects {\n\t\t\t\tif SubjectsCollide(tsubj, subj) {\n\t\t\t\t\treturn true\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn false\n}\n\n// StreamDefaultDuplicatesWindow default duplicates window.\nconst StreamDefaultDuplicatesWindow = 2 * time.Minute\n\nfunc (s *Server) checkStreamCfg(config *StreamConfig, acc *Account, pedantic bool) (StreamConfig, *ApiError) {\n\tlim := &s.getOpts().JetStreamLimits\n\n\tif config == nil {\n\t\treturn StreamConfig{}, NewJSStreamInvalidConfigError(fmt.Errorf(\"stream configuration invalid\"))\n\t}\n\tif !isValidName(config.Name) {\n\t\treturn StreamConfig{}, NewJSStreamInvalidConfigError(fmt.Errorf(\"stream name is required and can not contain '.', '*', '>'\"))\n\t}\n\tif len(config.Name) > JSMaxNameLen {\n\t\treturn StreamConfig{}, NewJSStreamInvalidConfigError(fmt.Errorf(\"stream name is too long, maximum allowed is %d\", JSMaxNameLen))\n\t}\n\tif len(config.Description) > JSMaxDescriptionLen {\n\t\treturn StreamConfig{}, NewJSStreamInvalidConfigError(fmt.Errorf(\"stream description is too long, maximum allowed is %d\", JSMaxDescriptionLen))\n\t}\n\n\tvar metadataLen int\n\tfor k, v := range config.Metadata {\n\t\tmetadataLen += len(k) + len(v)\n\t}\n\tif metadataLen > JSMaxMetadataLen {\n\t\treturn StreamConfig{}, NewJSStreamInvalidConfigError(fmt.Errorf(\"stream metadata exceeds maximum size of %d bytes\", JSMaxMetadataLen))\n\t}\n\n\tcfg := *config\n\n\t// Make file the default.\n\tif cfg.Storage == 0 {\n\t\tcfg.Storage = FileStorage\n\t}\n\tif cfg.Replicas == 0 {\n\t\tcfg.Replicas = 1\n\t}\n\tif cfg.Replicas > StreamMaxReplicas {\n\t\treturn cfg, NewJSStreamInvalidConfigError(fmt.Errorf(\"maximum replicas is %d\", StreamMaxReplicas))\n\t}\n\tif cfg.Replicas < 0 {\n\t\treturn cfg, NewJSReplicasCountCannotBeNegativeError()\n\t}\n\tif cfg.MaxMsgs == 0 {\n\t\tcfg.MaxMsgs = -1\n\t}\n\tif cfg.MaxMsgsPer == 0 {\n\t\tcfg.MaxMsgsPer = -1\n\t}\n\tif cfg.MaxBytes == 0 {\n\t\tcfg.MaxBytes = -1\n\t}\n\tif cfg.MaxMsgSize == 0 {\n\t\tcfg.MaxMsgSize = -1\n\t}\n\tif cfg.MaxConsumers == 0 {\n\t\tcfg.MaxConsumers = -1\n\t}\n\tif cfg.Duplicates == 0 && cfg.Mirror == nil {\n\t\tmaxWindow := StreamDefaultDuplicatesWindow\n\t\tif lim.Duplicates > 0 && maxWindow > lim.Duplicates {\n\t\t\tif pedantic {\n\t\t\t\treturn StreamConfig{}, NewJSPedanticError(fmt.Errorf(\"pedantic mode: duplicate window limits are higher than current limits\"))\n\t\t\t}\n\t\t\tmaxWindow = lim.Duplicates\n\t\t}\n\t\tif cfg.MaxAge != 0 && cfg.MaxAge < maxWindow {\n\t\t\tif pedantic {\n\t\t\t\treturn StreamConfig{}, NewJSPedanticError(fmt.Errorf(\"pedantic mode: duplicate window cannot be bigger than max age\"))\n\t\t\t}\n\t\t\tcfg.Duplicates = cfg.MaxAge\n\t\t} else {\n\t\t\tcfg.Duplicates = maxWindow\n\t\t}\n\t}\n\tif cfg.MaxAge > 0 && cfg.MaxAge < 100*time.Millisecond {\n\t\treturn StreamConfig{}, NewJSStreamInvalidConfigError(fmt.Errorf(\"max age needs to be >= 100ms\"))\n\t}\n\tif cfg.Duplicates < 0 {\n\t\treturn StreamConfig{}, NewJSStreamInvalidConfigError(fmt.Errorf(\"duplicates window can not be negative\"))\n\t}\n\t// Check that duplicates is not larger then age if set.\n\tif cfg.MaxAge != 0 && cfg.Duplicates > cfg.MaxAge {\n\t\treturn StreamConfig{}, NewJSStreamInvalidConfigError(fmt.Errorf(\"duplicates window can not be larger then max age\"))\n\t}\n\tif lim.Duplicates > 0 && cfg.Duplicates > lim.Duplicates {\n\t\treturn StreamConfig{}, NewJSStreamInvalidConfigError(fmt.Errorf(\"duplicates window can not be larger then server limit of %v\",\n\t\t\tlim.Duplicates.String()))\n\t}\n\tif cfg.Duplicates > 0 && cfg.Duplicates < 100*time.Millisecond {\n\t\treturn StreamConfig{}, NewJSStreamInvalidConfigError(fmt.Errorf(\"duplicates window needs to be >= 100ms\"))\n\t}\n\n\tif cfg.DenyPurge && cfg.AllowRollup {\n\t\treturn StreamConfig{}, NewJSStreamInvalidConfigError(fmt.Errorf(\"roll-ups require the purge permission\"))\n\t}\n\n\t// Check for new discard new per subject, we require the discard policy to also be new.\n\tif cfg.DiscardNewPer {\n\t\tif cfg.Discard != DiscardNew {\n\t\t\treturn StreamConfig{}, NewJSStreamInvalidConfigError(fmt.Errorf(\"discard new per subject requires discard new policy to be set\"))\n\t\t}\n\t\tif cfg.MaxMsgsPer <= 0 {\n\t\t\treturn StreamConfig{}, NewJSStreamInvalidConfigError(fmt.Errorf(\"discard new per subject requires max msgs per subject > 0\"))\n\t\t}\n\t}\n\n\tif cfg.SubjectDeleteMarkerTTL > 0 {\n\t\tif cfg.SubjectDeleteMarkerTTL < time.Second {\n\t\t\treturn StreamConfig{}, NewJSStreamInvalidConfigError(fmt.Errorf(\"subject delete marker TTL must be at least 1 second\"))\n\t\t}\n\t\tif !cfg.AllowMsgTTL {\n\t\t\tif pedantic {\n\t\t\t\treturn StreamConfig{}, NewJSStreamInvalidConfigError(fmt.Errorf(\"subject delete marker cannot be set if message TTLs are disabled\"))\n\t\t\t}\n\t\t\tcfg.AllowMsgTTL = true\n\t\t}\n\t\tif !cfg.AllowRollup {\n\t\t\tif pedantic {\n\t\t\t\treturn StreamConfig{}, NewJSStreamInvalidConfigError(fmt.Errorf(\"subject delete marker cannot be set if roll-ups are disabled\"))\n\t\t\t}\n\t\t\tcfg.AllowRollup, cfg.DenyPurge = true, false\n\t\t}\n\t} else if cfg.SubjectDeleteMarkerTTL < 0 {\n\t\treturn StreamConfig{}, NewJSStreamInvalidConfigError(fmt.Errorf(\"subject delete marker TTL must not be negative\"))\n\t}\n\n\tgetStream := func(streamName string) (bool, StreamConfig) {\n\t\tvar exists bool\n\t\tvar cfg StreamConfig\n\t\tif s.JetStreamIsClustered() {\n\t\t\tif js, _ := s.getJetStreamCluster(); js != nil {\n\t\t\t\tjs.mu.RLock()\n\t\t\t\tif sa := js.streamAssignment(acc.Name, streamName); sa != nil {\n\t\t\t\t\tcfg = *sa.Config.clone()\n\t\t\t\t\texists = true\n\t\t\t\t}\n\t\t\t\tjs.mu.RUnlock()\n\t\t\t}\n\t\t} else if mset, err := acc.lookupStream(streamName); err == nil {\n\t\t\tcfg = mset.cfg\n\t\t\texists = true\n\t\t}\n\t\treturn exists, cfg\n\t}\n\n\thasStream := func(streamName string) (bool, int32, []string) {\n\t\texists, cfg := getStream(streamName)\n\t\treturn exists, cfg.MaxMsgSize, cfg.Subjects\n\t}\n\n\tvar streamSubs []string\n\tvar deliveryPrefixes []string\n\tvar apiPrefixes []string\n\n\t// Do some pre-checking for mirror config to avoid cycles in clustered mode.\n\tif cfg.Mirror != nil {\n\t\tif cfg.FirstSeq > 0 {\n\t\t\treturn StreamConfig{}, NewJSMirrorWithFirstSeqError()\n\t\t}\n\t\tif len(cfg.Subjects) > 0 {\n\t\t\treturn StreamConfig{}, NewJSMirrorWithSubjectsError()\n\t\t}\n\t\tif len(cfg.Sources) > 0 {\n\t\t\treturn StreamConfig{}, NewJSMirrorWithSourcesError()\n\t\t}\n\t\tif cfg.Mirror.FilterSubject != _EMPTY_ && len(cfg.Mirror.SubjectTransforms) != 0 {\n\t\t\treturn StreamConfig{}, NewJSMirrorMultipleFiltersNotAllowedError()\n\t\t}\n\t\tif cfg.SubjectDeleteMarkerTTL > 0 {\n\t\t\t// Delete markers cannot be configured on a mirror as it would result in new\n\t\t\t// tombstones which would use up sequence numbers, diverging from the origin\n\t\t\t// stream.\n\t\t\treturn StreamConfig{}, NewJSStreamInvalidConfigError(fmt.Errorf(\"subject delete markers forbidden on mirrors\"))\n\t\t}\n\t\t// Check subject filters overlap.\n\t\tfor outer, tr := range cfg.Mirror.SubjectTransforms {\n\t\t\tif tr.Source != _EMPTY_ && !IsValidSubject(tr.Source) {\n\t\t\t\treturn StreamConfig{}, NewJSMirrorInvalidSubjectFilterError(fmt.Errorf(\"%w %s\", ErrBadSubject, tr.Source))\n\t\t\t}\n\n\t\t\terr := ValidateMapping(tr.Source, tr.Destination)\n\t\t\tif err != nil {\n\t\t\t\treturn StreamConfig{}, NewJSMirrorInvalidTransformDestinationError(err)\n\t\t\t}\n\n\t\t\tfor inner, innertr := range cfg.Mirror.SubjectTransforms {\n\t\t\t\tif inner != outer && SubjectsCollide(tr.Source, innertr.Source) {\n\t\t\t\t\treturn StreamConfig{}, NewJSMirrorOverlappingSubjectFiltersError()\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t// Do not perform checks if External is provided, as it could lead to\n\t\t// checking against itself (if sourced stream name is the same on different JetStream)\n\t\tif cfg.Mirror.External == nil {\n\t\t\tif !isValidName(cfg.Mirror.Name) {\n\t\t\t\treturn StreamConfig{}, NewJSMirrorInvalidStreamNameError()\n\t\t\t}\n\t\t\t// We do not require other stream to exist anymore, but if we can see it check payloads.\n\t\t\texists, maxMsgSize, subs := hasStream(cfg.Mirror.Name)\n\t\t\tif len(subs) > 0 {\n\t\t\t\tstreamSubs = append(streamSubs, subs...)\n\t\t\t}\n\t\t\tif exists {\n\t\t\t\tif cfg.MaxMsgSize > 0 && maxMsgSize > 0 && cfg.MaxMsgSize < maxMsgSize {\n\t\t\t\t\treturn StreamConfig{}, NewJSMirrorMaxMessageSizeTooBigError()\n\t\t\t\t}\n\t\t\t}\n\t\t\t// Determine if we are inheriting direct gets.\n\t\t\tif exists, ocfg := getStream(cfg.Mirror.Name); exists {\n\t\t\t\tif pedantic && cfg.MirrorDirect != ocfg.AllowDirect {\n\t\t\t\t\treturn StreamConfig{}, NewJSPedanticError(fmt.Errorf(\"origin stream has direct get set, mirror has it disabled\"))\n\t\t\t\t}\n\t\t\t\tcfg.MirrorDirect = ocfg.AllowDirect\n\t\t\t} else if js := s.getJetStream(); js != nil && js.isClustered() {\n\t\t\t\t// Could not find it here. If we are clustered we can look it up.\n\t\t\t\tjs.mu.RLock()\n\t\t\t\tif cc := js.cluster; cc != nil {\n\t\t\t\t\tif as := cc.streams[acc.Name]; as != nil {\n\t\t\t\t\t\tif sa := as[cfg.Mirror.Name]; sa != nil {\n\t\t\t\t\t\t\tif pedantic && cfg.MirrorDirect != sa.Config.AllowDirect {\n\t\t\t\t\t\t\t\tjs.mu.RUnlock()\n\t\t\t\t\t\t\t\treturn StreamConfig{}, NewJSPedanticError(fmt.Errorf(\"origin stream has direct get set, mirror has it disabled\"))\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tcfg.MirrorDirect = sa.Config.AllowDirect\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tjs.mu.RUnlock()\n\t\t\t}\n\t\t} else {\n\t\t\tif cfg.Mirror.External.DeliverPrefix != _EMPTY_ {\n\t\t\t\tdeliveryPrefixes = append(deliveryPrefixes, cfg.Mirror.External.DeliverPrefix)\n\t\t\t}\n\n\t\t\tif cfg.Mirror.External.ApiPrefix != _EMPTY_ {\n\t\t\t\tapiPrefixes = append(apiPrefixes, cfg.Mirror.External.ApiPrefix)\n\t\t\t}\n\t\t}\n\t}\n\n\t// check for duplicates\n\tvar iNames = make(map[string]struct{})\n\tfor _, src := range cfg.Sources {\n\t\tif !isValidName(src.Name) {\n\t\t\treturn StreamConfig{}, NewJSSourceInvalidStreamNameError()\n\t\t}\n\t\tif _, ok := iNames[src.composeIName()]; !ok {\n\t\t\tiNames[src.composeIName()] = struct{}{}\n\t\t} else {\n\t\t\treturn StreamConfig{}, NewJSSourceDuplicateDetectedError()\n\t\t}\n\t\t// Do not perform checks if External is provided, as it could lead to\n\t\t// checking against itself (if sourced stream name is the same on different JetStream)\n\t\tif src.External == nil {\n\t\t\texists, maxMsgSize, subs := hasStream(src.Name)\n\t\t\tif len(subs) > 0 {\n\t\t\t\tstreamSubs = append(streamSubs, subs...)\n\t\t\t}\n\t\t\tif exists {\n\t\t\t\tif cfg.MaxMsgSize > 0 && maxMsgSize > 0 && cfg.MaxMsgSize < maxMsgSize {\n\t\t\t\t\treturn StreamConfig{}, NewJSSourceMaxMessageSizeTooBigError()\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif src.FilterSubject != _EMPTY_ && len(src.SubjectTransforms) != 0 {\n\t\t\t\treturn StreamConfig{}, NewJSSourceMultipleFiltersNotAllowedError()\n\t\t\t}\n\n\t\t\tfor _, tr := range src.SubjectTransforms {\n\t\t\t\tif tr.Source != _EMPTY_ && !IsValidSubject(tr.Source) {\n\t\t\t\t\treturn StreamConfig{}, NewJSSourceInvalidSubjectFilterError(fmt.Errorf(\"%w %s\", ErrBadSubject, tr.Source))\n\t\t\t\t}\n\n\t\t\t\terr := ValidateMapping(tr.Source, tr.Destination)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn StreamConfig{}, NewJSSourceInvalidTransformDestinationError(err)\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// Check subject filters overlap.\n\t\t\tfor outer, tr := range src.SubjectTransforms {\n\t\t\t\tfor inner, innertr := range src.SubjectTransforms {\n\t\t\t\t\tif inner != outer && subjectIsSubsetMatch(tr.Source, innertr.Source) {\n\t\t\t\t\t\treturn StreamConfig{}, NewJSSourceOverlappingSubjectFiltersError()\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tcontinue\n\t\t} else {\n\t\t\tif src.External.DeliverPrefix != _EMPTY_ {\n\t\t\t\tdeliveryPrefixes = append(deliveryPrefixes, src.External.DeliverPrefix)\n\t\t\t}\n\t\t\tif src.External.ApiPrefix != _EMPTY_ {\n\t\t\t\tapiPrefixes = append(apiPrefixes, src.External.ApiPrefix)\n\t\t\t}\n\t\t}\n\t}\n\n\t// check prefix overlap with subjects\n\tfor _, pfx := range deliveryPrefixes {\n\t\tif !IsValidPublishSubject(pfx) {\n\t\t\treturn StreamConfig{}, NewJSStreamInvalidExternalDeliverySubjError(pfx)\n\t\t}\n\t\tfor _, sub := range streamSubs {\n\t\t\tif SubjectsCollide(sub, fmt.Sprintf(\"%s.%s\", pfx, sub)) {\n\t\t\t\treturn StreamConfig{}, NewJSStreamExternalDelPrefixOverlapsError(pfx, sub)\n\t\t\t}\n\t\t}\n\t}\n\t// check if api prefixes overlap\n\tfor _, apiPfx := range apiPrefixes {\n\t\tif !IsValidPublishSubject(apiPfx) {\n\t\t\treturn StreamConfig{}, NewJSStreamInvalidConfigError(\n\t\t\t\tfmt.Errorf(\"stream external api prefix %q must be a valid subject without wildcards\", apiPfx))\n\t\t}\n\t\tif SubjectsCollide(apiPfx, JSApiPrefix) {\n\t\t\treturn StreamConfig{}, NewJSStreamExternalApiOverlapError(apiPfx, JSApiPrefix)\n\t\t}\n\t}\n\n\t// cycle check for source cycle\n\ttoVisit := []*StreamConfig{&cfg}\n\tvisited := make(map[string]struct{})\n\toverlaps := func(subjects []string, filter string) bool {\n\t\tif filter == _EMPTY_ {\n\t\t\treturn true\n\t\t}\n\t\tfor _, subject := range subjects {\n\t\t\tif SubjectsCollide(subject, filter) {\n\t\t\t\treturn true\n\t\t\t}\n\t\t}\n\t\treturn false\n\t}\n\n\tfor len(toVisit) > 0 {\n\t\tcfg := toVisit[0]\n\t\ttoVisit = toVisit[1:]\n\t\tvisited[cfg.Name] = struct{}{}\n\t\tfor _, src := range cfg.Sources {\n\t\t\tif src.External != nil {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\t// We can detect a cycle between streams, but let's double check that the\n\t\t\t// subjects actually form a cycle.\n\t\t\tif _, ok := visited[src.Name]; ok {\n\t\t\t\tif overlaps(cfg.Subjects, src.FilterSubject) {\n\t\t\t\t\treturn StreamConfig{}, NewJSStreamInvalidConfigError(errors.New(\"detected cycle\"))\n\t\t\t\t}\n\t\t\t} else if exists, cfg := getStream(src.Name); exists {\n\t\t\t\ttoVisit = append(toVisit, &cfg)\n\t\t\t}\n\t\t}\n\t\t// Avoid cycles hiding behind mirrors\n\t\tif m := cfg.Mirror; m != nil {\n\t\t\tif m.External == nil {\n\t\t\t\tif _, ok := visited[m.Name]; ok {\n\t\t\t\t\treturn StreamConfig{}, NewJSStreamInvalidConfigError(errors.New(\"detected cycle\"))\n\t\t\t\t}\n\t\t\t\tif exists, cfg := getStream(m.Name); exists {\n\t\t\t\t\ttoVisit = append(toVisit, &cfg)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tif len(cfg.Subjects) == 0 {\n\t\tif cfg.Mirror == nil && len(cfg.Sources) == 0 {\n\t\t\tcfg.Subjects = append(cfg.Subjects, cfg.Name)\n\t\t}\n\t} else {\n\t\tif cfg.Mirror != nil {\n\t\t\treturn StreamConfig{}, NewJSMirrorWithSubjectsError()\n\t\t}\n\n\t\t// Check for literal duplication of subject interest in config\n\t\t// and no overlap with any JS or SYS API subject space.\n\t\tdset := make(map[string]struct{}, len(cfg.Subjects))\n\t\tfor _, subj := range cfg.Subjects {\n\t\t\t// Make sure the subject is valid. Check this first.\n\t\t\tif !IsValidSubject(subj) {\n\t\t\t\treturn StreamConfig{}, NewJSStreamInvalidConfigError(fmt.Errorf(\"invalid subject\"))\n\t\t\t}\n\t\t\tif _, ok := dset[subj]; ok {\n\t\t\t\treturn StreamConfig{}, NewJSStreamInvalidConfigError(fmt.Errorf(\"duplicate subjects detected\"))\n\t\t\t}\n\t\t\t// Check for trying to capture everything.\n\t\t\tif subj == fwcs {\n\t\t\t\tif !cfg.NoAck {\n\t\t\t\t\treturn StreamConfig{}, NewJSStreamInvalidConfigError(fmt.Errorf(\"capturing all subjects requires no-ack to be true\"))\n\t\t\t\t}\n\t\t\t\t// Capturing everything also will require R1.\n\t\t\t\tif cfg.Replicas != 1 {\n\t\t\t\t\treturn StreamConfig{}, NewJSStreamInvalidConfigError(fmt.Errorf(\"capturing all subjects requires replicas of 1\"))\n\t\t\t\t}\n\t\t\t}\n\t\t\t// Also check to make sure we do not overlap with our $JS API subjects.\n\t\t\tif !cfg.NoAck {\n\t\t\t\tfor _, namespace := range []string{\"$JS.>\", \"$JSC.>\", \"$NRG.>\"} {\n\t\t\t\t\tif SubjectsCollide(subj, namespace) {\n\t\t\t\t\t\t// We allow an exception for $JS.EVENT.> since these could have been created in the past.\n\t\t\t\t\t\tif !subjectIsSubsetMatch(subj, \"$JS.EVENT.>\") {\n\t\t\t\t\t\t\treturn StreamConfig{}, NewJSStreamInvalidConfigError(fmt.Errorf(\"subjects that overlap with jetstream api require no-ack to be true\"))\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif SubjectsCollide(subj, \"$SYS.>\") {\n\t\t\t\t\tif !subjectIsSubsetMatch(subj, \"$SYS.ACCOUNT.>\") {\n\t\t\t\t\t\treturn StreamConfig{}, NewJSStreamInvalidConfigError(fmt.Errorf(\"subjects that overlap with system api require no-ack to be true\"))\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\t// Mark for duplicate check.\n\t\t\tdset[subj] = struct{}{}\n\t\t}\n\t}\n\n\tif len(cfg.Subjects) == 0 && len(cfg.Sources) == 0 && cfg.Mirror == nil {\n\t\treturn StreamConfig{}, NewJSStreamInvalidConfigError(\n\t\t\tfmt.Errorf(\"stream needs at least one configured subject or be a source/mirror\"))\n\t}\n\n\t// Check for MaxBytes required and it's limit\n\tif required, limit := acc.maxBytesLimits(&cfg); required && cfg.MaxBytes <= 0 {\n\t\treturn StreamConfig{}, NewJSStreamMaxBytesRequiredError()\n\t} else if limit > 0 && cfg.MaxBytes > limit {\n\t\treturn StreamConfig{}, NewJSStreamMaxStreamBytesExceededError()\n\t}\n\n\t// Now check if we have multiple subjects they we do not overlap ourselves\n\t// which would cause duplicate entries (assuming no MsgID).\n\tif len(cfg.Subjects) > 1 {\n\t\tfor _, subj := range cfg.Subjects {\n\t\t\tfor _, tsubj := range cfg.Subjects {\n\t\t\t\tif tsubj != subj && SubjectsCollide(tsubj, subj) {\n\t\t\t\t\treturn StreamConfig{}, NewJSStreamInvalidConfigError(fmt.Errorf(\"subject %q overlaps with %q\", subj, tsubj))\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\t// If we have a republish directive check if we can create a transform here.\n\tif cfg.RePublish != nil {\n\t\t// Check to make sure source is a valid subset of the subjects we have.\n\t\t// Also make sure it does not form a cycle.\n\t\t// Empty same as all.\n\t\tif cfg.RePublish.Source == _EMPTY_ {\n\t\t\tif pedantic {\n\t\t\t\treturn StreamConfig{}, NewJSPedanticError(fmt.Errorf(\"pedantic mode: republish source can not be empty\"))\n\t\t\t}\n\t\t\tcfg.RePublish.Source = fwcs\n\t\t}\n\t\tvar formsCycle bool\n\t\tfor _, subj := range cfg.Subjects {\n\t\t\tif SubjectsCollide(cfg.RePublish.Destination, subj) {\n\t\t\t\tformsCycle = true\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\tif formsCycle {\n\t\t\treturn StreamConfig{}, NewJSStreamInvalidConfigError(fmt.Errorf(\"stream configuration for republish destination forms a cycle\"))\n\t\t}\n\t\tif _, err := NewSubjectTransform(cfg.RePublish.Source, cfg.RePublish.Destination); err != nil {\n\t\t\treturn StreamConfig{}, NewJSStreamInvalidConfigError(fmt.Errorf(\"stream configuration for republish with transform from '%s' to '%s' not valid\", cfg.RePublish.Source, cfg.RePublish.Destination))\n\t\t}\n\t}\n\n\t// Check the subject transform if any\n\tif cfg.SubjectTransform != nil {\n\t\tif cfg.SubjectTransform.Source != _EMPTY_ && !IsValidSubject(cfg.SubjectTransform.Source) {\n\t\t\treturn StreamConfig{}, NewJSStreamTransformInvalidSourceError(fmt.Errorf(\"%w %s\", ErrBadSubject, cfg.SubjectTransform.Source))\n\t\t}\n\n\t\terr := ValidateMapping(cfg.SubjectTransform.Source, cfg.SubjectTransform.Destination)\n\t\tif err != nil {\n\t\t\treturn StreamConfig{}, NewJSStreamTransformInvalidDestinationError(err)\n\t\t}\n\t}\n\n\t// For now don't allow preferred server in placement.\n\tif cfg.Placement != nil && cfg.Placement.Preferred != _EMPTY_ {\n\t\treturn StreamConfig{}, NewJSStreamInvalidConfigError(fmt.Errorf(\"preferred server not permitted in placement\"))\n\t}\n\n\treturn cfg, nil\n}\n\n// Config returns the stream's configuration.\nfunc (mset *stream) config() StreamConfig {\n\tmset.cfgMu.RLock()\n\tdefer mset.cfgMu.RUnlock()\n\treturn mset.cfg\n}\n\nfunc (mset *stream) fileStoreConfig() (FileStoreConfig, error) {\n\tmset.mu.Lock()\n\tdefer mset.mu.Unlock()\n\tfs, ok := mset.store.(*fileStore)\n\tif !ok {\n\t\treturn FileStoreConfig{}, ErrStoreWrongType\n\t}\n\treturn fs.fileStoreConfig(), nil\n}\n\n// Do not hold jsAccount or jetStream lock\nfunc (jsa *jsAccount) configUpdateCheck(old, new *StreamConfig, s *Server, pedantic bool) (*StreamConfig, error) {\n\tcfg, apiErr := s.checkStreamCfg(new, jsa.acc(), pedantic)\n\tif apiErr != nil {\n\t\treturn nil, apiErr\n\t}\n\n\t// Name must match.\n\tif cfg.Name != old.Name {\n\t\treturn nil, NewJSStreamInvalidConfigError(fmt.Errorf(\"stream configuration name must match original\"))\n\t}\n\t// Can't change MaxConsumers for now.\n\tif cfg.MaxConsumers != old.MaxConsumers {\n\t\treturn nil, NewJSStreamInvalidConfigError(fmt.Errorf(\"stream configuration update can not change MaxConsumers\"))\n\t}\n\t// Can't change storage types.\n\tif cfg.Storage != old.Storage {\n\t\treturn nil, NewJSStreamInvalidConfigError(fmt.Errorf(\"stream configuration update can not change storage type\"))\n\t}\n\t// Can only change retention from limits to interest or back, not to/from work queue for now.\n\tif cfg.Retention != old.Retention {\n\t\tif old.Retention == WorkQueuePolicy || cfg.Retention == WorkQueuePolicy {\n\t\t\treturn nil, NewJSStreamInvalidConfigError(fmt.Errorf(\"stream configuration update can not change retention policy to/from workqueue\"))\n\t\t}\n\t}\n\t// Can not have a template owner for now.\n\tif old.Template != _EMPTY_ {\n\t\treturn nil, NewJSStreamInvalidConfigError(fmt.Errorf(\"stream configuration update not allowed on template owned stream\"))\n\t}\n\tif cfg.Template != _EMPTY_ {\n\t\treturn nil, NewJSStreamInvalidConfigError(fmt.Errorf(\"stream configuration update can not be owned by a template\"))\n\t}\n\t// Can not change from true to false.\n\tif !cfg.Sealed && old.Sealed {\n\t\treturn nil, NewJSStreamInvalidConfigError(fmt.Errorf(\"stream configuration update can not unseal a sealed stream\"))\n\t}\n\t// Can not change from true to false.\n\tif !cfg.DenyDelete && old.DenyDelete {\n\t\treturn nil, NewJSStreamInvalidConfigError(fmt.Errorf(\"stream configuration update can not cancel deny message deletes\"))\n\t}\n\t// Can not change from true to false.\n\tif !cfg.DenyPurge && old.DenyPurge {\n\t\treturn nil, NewJSStreamInvalidConfigError(fmt.Errorf(\"stream configuration update can not cancel deny purge\"))\n\t}\n\t// Check for mirror changes which are not allowed.\n\tif !reflect.DeepEqual(cfg.Mirror, old.Mirror) {\n\t\treturn nil, NewJSStreamMirrorNotUpdatableError()\n\t}\n\n\t// Check on new discard new per subject.\n\tif cfg.DiscardNewPer {\n\t\tif cfg.Discard != DiscardNew {\n\t\t\treturn nil, NewJSStreamInvalidConfigError(fmt.Errorf(\"discard new per subject requires discard new policy to be set\"))\n\t\t}\n\t\tif cfg.MaxMsgsPer <= 0 {\n\t\t\treturn nil, NewJSStreamInvalidConfigError(fmt.Errorf(\"discard new per subject requires max msgs per subject > 0\"))\n\t\t}\n\t}\n\n\t// Check on the allowed message TTL status.\n\tif old.AllowMsgTTL && !cfg.AllowMsgTTL {\n\t\treturn nil, NewJSStreamInvalidConfigError(fmt.Errorf(\"message TTL status can not be disabled\"))\n\t}\n\n\t// Do some adjustments for being sealed.\n\t// Pedantic mode will allow those changes to be made, as they are deterministic and important to get a sealed stream.\n\tif cfg.Sealed {\n\t\tcfg.MaxAge = 0\n\t\tcfg.Discard = DiscardNew\n\t\tcfg.DenyDelete, cfg.DenyPurge = true, true\n\t\tcfg.AllowRollup = false\n\t}\n\n\t// Check limits. We need some extra handling to allow updating MaxBytes.\n\n\t// First, let's calculate the difference between the new and old MaxBytes.\n\tmaxBytesDiff := cfg.MaxBytes - old.MaxBytes\n\tif maxBytesDiff < 0 {\n\t\t// If we're updating to a lower MaxBytes (maxBytesDiff is negative),\n\t\t// then set to zero so checkBytesLimits doesn't set addBytes to 1.\n\t\tmaxBytesDiff = 0\n\t}\n\t// If maxBytesDiff == 0, then that means MaxBytes didn't change.\n\t// If maxBytesDiff > 0, then we want to reserve additional bytes.\n\n\t// Save the user configured MaxBytes.\n\tnewMaxBytes := cfg.MaxBytes\n\tmaxBytesOffset := int64(0)\n\n\t// We temporarily set cfg.MaxBytes to maxBytesDiff because checkAllLimits\n\t// adds cfg.MaxBytes to the current reserved limit and checks if we've gone\n\t// over. However, we don't want an addition cfg.MaxBytes, we only want to\n\t// reserve the difference between the new and the old values.\n\tcfg.MaxBytes = maxBytesDiff\n\n\t// Check limits.\n\tjs, isClustered := jsa.jetStreamAndClustered()\n\tjsa.mu.RLock()\n\tacc := jsa.account\n\tjsa.usageMu.RLock()\n\tselected, tier, hasTier := jsa.selectLimits(cfg.Replicas)\n\tif !hasTier && old.Replicas != cfg.Replicas {\n\t\tselected, tier, hasTier = jsa.selectLimits(old.Replicas)\n\t}\n\tjsa.usageMu.RUnlock()\n\treserved := int64(0)\n\tif !isClustered {\n\t\treserved = jsa.tieredReservation(tier, &cfg)\n\t}\n\tjsa.mu.RUnlock()\n\tif !hasTier {\n\t\treturn nil, NewJSNoLimitsError()\n\t}\n\tjs.mu.RLock()\n\tdefer js.mu.RUnlock()\n\tif isClustered {\n\t\t_, reserved = tieredStreamAndReservationCount(js.cluster.streams[acc.Name], tier, &cfg)\n\t}\n\t// reservation does not account for this stream, hence add the old value\n\tif tier == _EMPTY_ && old.Replicas > 1 {\n\t\treserved += old.MaxBytes * int64(old.Replicas)\n\t} else {\n\t\treserved += old.MaxBytes\n\t}\n\tif err := js.checkAllLimits(&selected, &cfg, reserved, maxBytesOffset); err != nil {\n\t\treturn nil, err\n\t}\n\t// Restore the user configured MaxBytes.\n\tcfg.MaxBytes = newMaxBytes\n\treturn &cfg, nil\n}\n\n// Update will allow certain configuration properties of an existing stream to be updated.\nfunc (mset *stream) update(config *StreamConfig) error {\n\treturn mset.updateWithAdvisory(config, true, false)\n}\n\nfunc (mset *stream) updatePedantic(config *StreamConfig, pedantic bool) error {\n\treturn mset.updateWithAdvisory(config, true, pedantic)\n}\n\n// Update will allow certain configuration properties of an existing stream to be updated.\nfunc (mset *stream) updateWithAdvisory(config *StreamConfig, sendAdvisory bool, pedantic bool) error {\n\t_, jsa, err := mset.acc.checkForJetStream()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tmset.mu.RLock()\n\tocfg := mset.cfg\n\ts := mset.srv\n\tmset.mu.RUnlock()\n\n\tcfg, err := mset.jsa.configUpdateCheck(&ocfg, config, s, pedantic)\n\tif err != nil {\n\t\treturn NewJSStreamInvalidConfigError(err, Unless(err))\n\t}\n\n\t// In the event that some of the stream-level limits have changed, yell appropriately\n\t// if any of the consumers exceed that limit.\n\tupdateLimits := ocfg.ConsumerLimits.InactiveThreshold != cfg.ConsumerLimits.InactiveThreshold ||\n\t\tocfg.ConsumerLimits.MaxAckPending != cfg.ConsumerLimits.MaxAckPending\n\tif updateLimits {\n\t\tvar errorConsumers []string\n\t\tconsumers := map[string]*ConsumerConfig{}\n\t\tif mset.js.isClustered() {\n\t\t\tfor _, c := range mset.sa.consumers {\n\t\t\t\tconsumers[c.Name] = c.Config\n\t\t\t}\n\t\t} else {\n\t\t\tfor _, c := range mset.consumers {\n\t\t\t\tconsumers[c.name] = &c.cfg\n\t\t\t}\n\t\t}\n\t\tfor name, ccfg := range consumers {\n\t\t\tif ccfg.InactiveThreshold > cfg.ConsumerLimits.InactiveThreshold ||\n\t\t\t\tccfg.MaxAckPending > cfg.ConsumerLimits.MaxAckPending {\n\t\t\t\terrorConsumers = append(errorConsumers, name)\n\t\t\t}\n\t\t}\n\t\tif len(errorConsumers) > 0 {\n\t\t\t// TODO(nat): Return a parsable error so that we can surface something\n\t\t\t// sensible through the JS API.\n\t\t\treturn fmt.Errorf(\"change to limits violates consumers: %s\", strings.Join(errorConsumers, \", \"))\n\t\t}\n\t}\n\n\tjsa.mu.RLock()\n\tif jsa.subjectsOverlap(cfg.Subjects, mset) {\n\t\tjsa.mu.RUnlock()\n\t\treturn NewJSStreamSubjectOverlapError()\n\t}\n\tjsa.mu.RUnlock()\n\n\tmset.mu.Lock()\n\tif mset.isLeader() {\n\t\t// Now check for subject interest differences.\n\t\tcurrent := make(map[string]struct{}, len(ocfg.Subjects))\n\t\tfor _, s := range ocfg.Subjects {\n\t\t\tcurrent[s] = struct{}{}\n\t\t}\n\t\t// Update config with new values. The store update will enforce any stricter limits.\n\n\t\t// Now walk new subjects. All of these need to be added, but we will check\n\t\t// the originals first, since if it is in there we can skip, already added.\n\t\tfor _, s := range cfg.Subjects {\n\t\t\tif _, ok := current[s]; !ok {\n\t\t\t\tif _, err := mset.subscribeInternal(s, mset.processInboundJetStreamMsg); err != nil {\n\t\t\t\t\tmset.mu.Unlock()\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t}\n\t\t\tdelete(current, s)\n\t\t}\n\t\t// What is left in current needs to be deleted.\n\t\tfor s := range current {\n\t\t\tif err := mset.unsubscribeInternal(s); err != nil {\n\t\t\t\tmset.mu.Unlock()\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\n\t\t// Check for the Duplicates\n\t\tif cfg.Duplicates != ocfg.Duplicates && mset.ddtmr != nil {\n\t\t\t// Let it fire right away, it will adjust properly on purge.\n\t\t\tmset.ddtmr.Reset(time.Microsecond)\n\t\t}\n\n\t\t// Check for Sources.\n\t\tif len(cfg.Sources) > 0 || len(ocfg.Sources) > 0 {\n\t\t\tcurrentIName := make(map[string]struct{})\n\t\t\tneedsStartingSeqNum := make(map[string]struct{})\n\n\t\t\tfor _, s := range ocfg.Sources {\n\t\t\t\tcurrentIName[s.iname] = struct{}{}\n\t\t\t}\n\t\t\tfor _, s := range cfg.Sources {\n\t\t\t\ts.setIndexName()\n\t\t\t\tif _, ok := currentIName[s.iname]; !ok {\n\t\t\t\t\t// new source\n\t\t\t\t\tif mset.sources == nil {\n\t\t\t\t\t\tmset.sources = make(map[string]*sourceInfo)\n\t\t\t\t\t}\n\t\t\t\t\tmset.cfg.Sources = append(mset.cfg.Sources, s)\n\n\t\t\t\t\tvar si *sourceInfo\n\n\t\t\t\t\tif len(s.SubjectTransforms) == 0 {\n\t\t\t\t\t\tsi = &sourceInfo{name: s.Name, iname: s.iname, sf: s.FilterSubject}\n\t\t\t\t\t} else {\n\t\t\t\t\t\tsi = &sourceInfo{name: s.Name, iname: s.iname}\n\t\t\t\t\t\tsi.trs = make([]*subjectTransform, len(s.SubjectTransforms))\n\t\t\t\t\t\tsi.sfs = make([]string, len(s.SubjectTransforms))\n\t\t\t\t\t\tfor i := range s.SubjectTransforms {\n\t\t\t\t\t\t\t// err can be ignored as already validated in config check\n\t\t\t\t\t\t\tsi.sfs[i] = s.SubjectTransforms[i].Source\n\t\t\t\t\t\t\tvar err error\n\t\t\t\t\t\t\tsi.trs[i], err = NewSubjectTransform(s.SubjectTransforms[i].Source, s.SubjectTransforms[i].Destination)\n\t\t\t\t\t\t\tif err != nil {\n\t\t\t\t\t\t\t\tmset.mu.Unlock()\n\t\t\t\t\t\t\t\treturn fmt.Errorf(\"unable to get subject transform for source: %v\", err)\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t\tmset.sources[s.iname] = si\n\t\t\t\t\tneedsStartingSeqNum[s.iname] = struct{}{}\n\t\t\t\t} else {\n\t\t\t\t\t// source already exists\n\t\t\t\t\tdelete(currentIName, s.iname)\n\t\t\t\t}\n\t\t\t}\n\t\t\t// What is left in currentIName needs to be deleted.\n\t\t\tfor iName := range currentIName {\n\t\t\t\tmset.cancelSourceConsumer(iName)\n\t\t\t\tdelete(mset.sources, iName)\n\t\t\t}\n\t\t\tneededCopy := make(map[string]struct{}, len(needsStartingSeqNum))\n\t\t\tfor iName := range needsStartingSeqNum {\n\t\t\t\tneededCopy[iName] = struct{}{}\n\t\t\t}\n\t\t\tmset.setStartingSequenceForSources(needsStartingSeqNum)\n\t\t\tfor iName := range neededCopy {\n\t\t\t\tmset.setupSourceConsumer(iName, mset.sources[iName].sseq+1, time.Time{})\n\t\t\t}\n\t\t}\n\t}\n\n\t// Check for a change in allow direct status.\n\t// These will run on all members, so just update as appropriate here.\n\t// We do make sure we are caught up under monitorStream() during initial startup.\n\tif cfg.AllowDirect != ocfg.AllowDirect {\n\t\tif cfg.AllowDirect {\n\t\t\tmset.subscribeToDirect()\n\t\t} else {\n\t\t\tmset.unsubscribeToDirect()\n\t\t}\n\t}\n\n\t// Check for changes to RePublish.\n\tif cfg.RePublish != nil {\n\t\t// Empty same as all.\n\t\tif cfg.RePublish.Source == _EMPTY_ {\n\t\t\tcfg.RePublish.Source = fwcs\n\t\t}\n\t\tif cfg.RePublish.Destination == _EMPTY_ {\n\t\t\tcfg.RePublish.Destination = fwcs\n\t\t}\n\t\ttr, err := NewSubjectTransform(cfg.RePublish.Source, cfg.RePublish.Destination)\n\t\tif err != nil {\n\t\t\tmset.mu.Unlock()\n\t\t\treturn fmt.Errorf(\"stream configuration for republish from '%s' to '%s': %w\", cfg.RePublish.Source, cfg.RePublish.Destination, err)\n\t\t}\n\t\t// Assign our transform for republishing.\n\t\tmset.tr = tr\n\t} else {\n\t\tmset.tr = nil\n\t}\n\n\t// Check for changes to subject transform\n\tif ocfg.SubjectTransform == nil && cfg.SubjectTransform != nil {\n\t\ttr, err := NewSubjectTransform(cfg.SubjectTransform.Source, cfg.SubjectTransform.Destination)\n\t\tif err != nil {\n\t\t\tmset.mu.Unlock()\n\t\t\treturn fmt.Errorf(\"stream configuration for subject transform from '%s' to '%s': %w\", cfg.SubjectTransform.Source, cfg.SubjectTransform.Destination, err)\n\t\t}\n\t\tmset.itr = tr\n\t} else if ocfg.SubjectTransform != nil && cfg.SubjectTransform != nil &&\n\t\t(ocfg.SubjectTransform.Source != cfg.SubjectTransform.Source || ocfg.SubjectTransform.Destination != cfg.SubjectTransform.Destination) {\n\t\ttr, err := NewSubjectTransform(cfg.SubjectTransform.Source, cfg.SubjectTransform.Destination)\n\t\tif err != nil {\n\t\t\tmset.mu.Unlock()\n\t\t\treturn fmt.Errorf(\"stream configuration for subject transform from '%s' to '%s': %w\", cfg.SubjectTransform.Source, cfg.SubjectTransform.Destination, err)\n\t\t}\n\t\tmset.itr = tr\n\t} else if ocfg.SubjectTransform != nil && cfg.SubjectTransform == nil {\n\t\tmset.itr = nil\n\t}\n\n\tjs := mset.js\n\n\tif targetTier := tierName(cfg.Replicas); mset.tier != targetTier {\n\t\t// In cases such as R1->R3, only one update is needed\n\t\tjsa.usageMu.RLock()\n\t\t_, ok := jsa.limits[targetTier]\n\t\tjsa.usageMu.RUnlock()\n\t\tif ok {\n\t\t\t// error never set\n\t\t\t_, reported, _ := mset.store.Utilization()\n\t\t\tjsa.updateUsage(mset.tier, mset.stype, -int64(reported))\n\t\t\tjsa.updateUsage(targetTier, mset.stype, int64(reported))\n\t\t\tmset.tier = targetTier\n\t\t}\n\t\t// else in case the new tier does not exist (say on move), keep the old tier around\n\t\t// a subsequent update to an existing tier will then move from existing past tier to existing new tier\n\t}\n\n\tif mset.isLeader() && mset.sa != nil && ocfg.Retention != cfg.Retention && cfg.Retention == InterestPolicy {\n\t\t// Before we can update the retention policy for the consumer, we need\n\t\t// the replica count of all consumers to match the stream.\n\t\tfor _, c := range mset.sa.consumers {\n\t\t\tif c.Config.Replicas > 0 && c.Config.Replicas != cfg.Replicas {\n\t\t\t\tmset.mu.Unlock()\n\t\t\t\treturn fmt.Errorf(\"consumer %q replica count must be %d\", c.Name, cfg.Replicas)\n\t\t\t}\n\t\t}\n\t}\n\n\t// Now update config and store's version of our config.\n\t// Although we are under the stream write lock, we will also assign the new\n\t// configuration under mset.cfgMu lock. This is so that in places where\n\t// mset.mu cannot be acquired (like many cases in consumer.go where code\n\t// is under the consumer's lock), and the stream's configuration needs to\n\t// be inspected, one can use mset.cfgMu's read lock to do that safely.\n\tmset.cfgMu.Lock()\n\tmset.cfg = *cfg\n\tmset.cfgMu.Unlock()\n\n\t// If we're changing retention and haven't errored because of consumer\n\t// replicas by now, whip through and update the consumer retention.\n\tif ocfg.Retention != cfg.Retention {\n\t\ttoUpdate := make([]*consumer, 0, len(mset.consumers))\n\t\tfor _, c := range mset.consumers {\n\t\t\ttoUpdate = append(toUpdate, c)\n\t\t}\n\t\tvar ss StreamState\n\t\tmset.store.FastState(&ss)\n\t\tmset.mu.Unlock()\n\t\tfor _, c := range toUpdate {\n\t\t\tc.mu.Lock()\n\t\t\tc.retention = cfg.Retention\n\t\t\tc.mu.Unlock()\n\t\t\tif c.retention == InterestPolicy {\n\t\t\t\t// If we're switching to interest, force a check of the\n\t\t\t\t// interest of existing stream messages.\n\t\t\t\tc.checkStateForInterestStream(&ss)\n\t\t\t}\n\t\t}\n\t\tmset.mu.Lock()\n\t}\n\n\t// If we are the leader never suppress update advisory, simply send.\n\tif mset.isLeader() && sendAdvisory {\n\t\tmset.sendUpdateAdvisoryLocked()\n\t}\n\tmset.mu.Unlock()\n\n\tif js != nil {\n\t\tmaxBytesDiff := cfg.MaxBytes - ocfg.MaxBytes\n\t\tif maxBytesDiff > 0 {\n\t\t\t// Reserve the difference\n\t\t\tjs.reserveStreamResources(&StreamConfig{\n\t\t\t\tMaxBytes: maxBytesDiff,\n\t\t\t\tStorage:  cfg.Storage,\n\t\t\t})\n\t\t} else if maxBytesDiff < 0 {\n\t\t\t// Release the difference\n\t\t\tjs.releaseStreamResources(&StreamConfig{\n\t\t\t\tMaxBytes: -maxBytesDiff,\n\t\t\t\tStorage:  ocfg.Storage,\n\t\t\t})\n\t\t}\n\t}\n\n\tmset.store.UpdateConfig(cfg)\n\n\treturn nil\n}\n\n// Small helper to return the Name field from mset.cfg, protected by\n// the mset.cfgMu mutex. This is simply because we have several places\n// in consumer.go where we need it.\nfunc (mset *stream) getCfgName() string {\n\tmset.cfgMu.RLock()\n\tdefer mset.cfgMu.RUnlock()\n\treturn mset.cfg.Name\n}\n\n// Purge will remove all messages from the stream and underlying store based on the request.\nfunc (mset *stream) purge(preq *JSApiStreamPurgeRequest) (purged uint64, err error) {\n\tmset.mu.RLock()\n\tif mset.closed.Load() {\n\t\tmset.mu.RUnlock()\n\t\treturn 0, errStreamClosed\n\t}\n\tif mset.cfg.Sealed {\n\t\tmset.mu.RUnlock()\n\t\treturn 0, errors.New(\"sealed stream\")\n\t}\n\tstore, mlseq := mset.store, mset.lseq\n\tmset.mu.RUnlock()\n\n\tif preq != nil {\n\t\tpurged, err = mset.store.PurgeEx(preq.Subject, preq.Sequence, preq.Keep)\n\t} else {\n\t\tpurged, err = mset.store.Purge()\n\t}\n\tif err != nil {\n\t\treturn purged, err\n\t}\n\n\t// Grab our stream state.\n\tvar state StreamState\n\tstore.FastState(&state)\n\tfseq, lseq := state.FirstSeq, state.LastSeq\n\n\tmset.mu.Lock()\n\t// Check if our last has moved past what our original last sequence was, if so reset.\n\tif lseq > mlseq {\n\t\tmset.setLastSeq(lseq)\n\t}\n\n\t// Clear any pending acks below first seq.\n\tmset.clearAllPreAcksBelowFloor(fseq)\n\tmset.mu.Unlock()\n\n\t// Purge consumers.\n\t// Check for filtered purge.\n\tif preq != nil && preq.Subject != _EMPTY_ {\n\t\tss := store.FilteredState(fseq, preq.Subject)\n\t\tfseq = ss.First\n\t}\n\n\t// Take a copy of cList to avoid o.purge() potentially taking the stream lock and\n\t// violating the lock ordering.\n\tmset.clsMu.RLock()\n\tcList := slices.Clone(mset.cList)\n\tmset.clsMu.RUnlock()\n\tfor _, o := range cList {\n\t\tstart := fseq\n\t\to.mu.RLock()\n\t\t// we update consumer sequences if:\n\t\t// no subject was specified, we can purge all consumers sequences\n\t\tdoPurge := preq == nil ||\n\t\t\tpreq.Subject == _EMPTY_ ||\n\t\t\t// consumer filter subject is equal to purged subject\n\t\t\t// or consumer filter subject is subset of purged subject,\n\t\t\t// but not the other way around.\n\t\t\to.isEqualOrSubsetMatch(preq.Subject)\n\t\t// Check if a consumer has a wider subject space than what we purged\n\t\tvar isWider bool\n\t\tif !doPurge && preq != nil && o.isFilteredMatch(preq.Subject) {\n\t\t\tdoPurge, isWider = true, true\n\t\t\tstart = state.FirstSeq\n\t\t}\n\t\to.mu.RUnlock()\n\t\tif doPurge {\n\t\t\to.purge(start, lseq, isWider)\n\t\t}\n\t}\n\n\treturn purged, nil\n}\n\n// RemoveMsg will remove a message from a stream.\n// FIXME(dlc) - Should pick one and be consistent.\nfunc (mset *stream) removeMsg(seq uint64) (bool, error) {\n\treturn mset.deleteMsg(seq)\n}\n\n// DeleteMsg will remove a message from a stream.\nfunc (mset *stream) deleteMsg(seq uint64) (bool, error) {\n\tif mset.closed.Load() {\n\t\treturn false, errStreamClosed\n\t}\n\tremoved, err := mset.store.RemoveMsg(seq)\n\tif err != nil {\n\t\treturn removed, err\n\t}\n\tmset.mu.Lock()\n\tmset.clearAllPreAcks(seq)\n\tmset.mu.Unlock()\n\treturn removed, err\n}\n\n// EraseMsg will securely remove a message and rewrite the data with random data.\nfunc (mset *stream) eraseMsg(seq uint64) (bool, error) {\n\tif mset.closed.Load() {\n\t\treturn false, errStreamClosed\n\t}\n\tremoved, err := mset.store.EraseMsg(seq)\n\tif err != nil {\n\t\treturn removed, err\n\t}\n\tmset.mu.Lock()\n\tmset.clearAllPreAcks(seq)\n\tmset.mu.Unlock()\n\treturn removed, err\n}\n\n// Are we a mirror?\nfunc (mset *stream) isMirror() bool {\n\tmset.cfgMu.RLock()\n\tdefer mset.cfgMu.RUnlock()\n\treturn mset.cfg.Mirror != nil\n}\n\nfunc (mset *stream) sourcesInfo() (sis []*StreamSourceInfo) {\n\tmset.mu.RLock()\n\tdefer mset.mu.RUnlock()\n\tsis = make([]*StreamSourceInfo, 0, len(mset.sources))\n\tfor _, si := range mset.sources {\n\t\tsis = append(sis, mset.sourceInfo(si))\n\t}\n\treturn sis\n}\n\n// Lock should be held\nfunc (mset *stream) sourceInfo(si *sourceInfo) *StreamSourceInfo {\n\tif si == nil {\n\t\treturn nil\n\t}\n\n\tvar ssi = StreamSourceInfo{Name: si.name, Lag: si.lag, Error: si.err, FilterSubject: si.sf}\n\n\ttrConfigs := make([]SubjectTransformConfig, len(si.sfs))\n\tfor i := range si.sfs {\n\t\tvar destination string\n\t\tif si.trs[i] != nil {\n\t\t\tdestination = si.trs[i].dest\n\t\t}\n\t\ttrConfigs[i] = SubjectTransformConfig{si.sfs[i], destination}\n\t}\n\n\tssi.SubjectTransforms = trConfigs\n\n\t// If we have not heard from the source, set Active to -1.\n\tif last := si.last.Load(); last == 0 {\n\t\tssi.Active = -1\n\t} else {\n\t\tssi.Active = time.Since(time.Unix(0, last))\n\t}\n\n\tvar ext *ExternalStream\n\tif mset.cfg.Mirror != nil {\n\t\text = mset.cfg.Mirror.External\n\t} else if ss := mset.streamSource(si.iname); ss != nil && ss.External != nil {\n\t\text = ss.External\n\t}\n\tif ext != nil {\n\t\tssi.External = &ExternalStream{\n\t\t\tApiPrefix:     ext.ApiPrefix,\n\t\t\tDeliverPrefix: ext.DeliverPrefix,\n\t\t}\n\t}\n\treturn &ssi\n}\n\n// Return our source info for our mirror.\nfunc (mset *stream) mirrorInfo() *StreamSourceInfo {\n\tmset.mu.RLock()\n\tdefer mset.mu.RUnlock()\n\treturn mset.sourceInfo(mset.mirror)\n}\n\n// retryDisconnectedSyncConsumers() will check if we have any disconnected\n// sync consumers for either mirror or a source and will reset and retry to connect.\nfunc (mset *stream) retryDisconnectedSyncConsumers(remoteDomain string) {\n\tmset.mu.Lock()\n\tdefer mset.mu.Unlock()\n\n\t// Only applicable if we are the stream leader.\n\tif !mset.isLeader() {\n\t\treturn\n\t}\n\n\t// Check mirrors first.\n\tif si := mset.mirror; si != nil {\n\t\tif si.sub == nil && !si.sip {\n\t\t\tif remoteDomain == _EMPTY_ || (mset.cfg.Mirror != nil && mset.cfg.Mirror.External.Domain() == remoteDomain) {\n\t\t\t\t// Need to reset\n\t\t\t\tsi.fails = 0\n\t\t\t\tmset.cancelSourceInfo(si)\n\t\t\t\tmset.scheduleSetupMirrorConsumerRetry()\n\t\t\t}\n\t\t}\n\t} else {\n\t\tfor _, si := range mset.sources {\n\t\t\tss := mset.streamSource(si.iname)\n\t\t\tif remoteDomain == _EMPTY_ || (ss != nil && ss.External.Domain() == remoteDomain) {\n\t\t\t\t// Need to reset\n\t\t\t\tsi.fails = 0\n\t\t\t\tmset.cancelSourceInfo(si)\n\t\t\t\tmset.setupSourceConsumer(si.iname, si.sseq+1, time.Time{})\n\t\t\t}\n\t\t}\n\t}\n}\n\nconst (\n\t// Our consumer HB interval.\n\tsourceHealthHB = 1 * time.Second\n\t// How often we check and our stalled interval.\n\tsourceHealthCheckInterval = 10 * time.Second\n)\n\n// Will run as a Go routine to process mirror consumer messages.\nfunc (mset *stream) processMirrorMsgs(mirror *sourceInfo, ready *sync.WaitGroup) {\n\ts := mset.srv\n\tdefer func() {\n\t\tmirror.wg.Done()\n\t\ts.grWG.Done()\n\t}()\n\n\t// Grab stream quit channel.\n\tmset.mu.Lock()\n\tmsgs, qch, siqch := mirror.msgs, mset.qch, mirror.qch\n\t// Set the last seen as now so that we don't fail at the first check.\n\tmirror.last.Store(time.Now().UnixNano())\n\tmset.mu.Unlock()\n\n\t// Signal the caller that we have captured the above fields.\n\tready.Done()\n\n\t// Make sure we have valid ipq for msgs.\n\tif msgs == nil {\n\t\tmset.mu.Lock()\n\t\tmset.cancelMirrorConsumer()\n\t\tmset.mu.Unlock()\n\t\treturn\n\t}\n\n\tt := time.NewTicker(sourceHealthCheckInterval)\n\tdefer t.Stop()\n\n\tfor {\n\t\tselect {\n\t\tcase <-s.quitCh:\n\t\t\treturn\n\t\tcase <-qch:\n\t\t\treturn\n\t\tcase <-siqch:\n\t\t\treturn\n\t\tcase <-msgs.ch:\n\t\t\tims := msgs.pop()\n\t\t\tfor _, im := range ims {\n\t\t\t\tif !mset.processInboundMirrorMsg(im) {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t\tim.returnToPool()\n\t\t\t}\n\t\t\tmsgs.recycle(&ims)\n\t\tcase <-t.C:\n\t\t\tmset.mu.RLock()\n\t\t\tvar stalled bool\n\t\t\tif mset.mirror != nil {\n\t\t\t\tstalled = time.Since(time.Unix(0, mset.mirror.last.Load())) > sourceHealthCheckInterval\n\t\t\t}\n\t\t\tisLeader := mset.isLeader()\n\t\t\tmset.mu.RUnlock()\n\t\t\t// No longer leader.\n\t\t\tif !isLeader {\n\t\t\t\tmset.mu.Lock()\n\t\t\t\tmset.cancelMirrorConsumer()\n\t\t\t\tmset.mu.Unlock()\n\t\t\t\treturn\n\t\t\t}\n\t\t\t// We are stalled.\n\t\t\tif stalled {\n\t\t\t\tmset.retryMirrorConsumer()\n\t\t\t}\n\t\t}\n\t}\n}\n\n// Checks that the message is from our current direct consumer. We can not depend on sub comparison\n// since cross account imports break.\nfunc (si *sourceInfo) isCurrentSub(reply string) bool {\n\treturn si.cname != _EMPTY_ && strings.HasPrefix(reply, jsAckPre) && si.cname == tokenAt(reply, 4)\n}\n\n// processInboundMirrorMsg handles processing messages bound for a stream.\nfunc (mset *stream) processInboundMirrorMsg(m *inMsg) bool {\n\tmset.mu.Lock()\n\tif mset.mirror == nil {\n\t\tmset.mu.Unlock()\n\t\treturn false\n\t}\n\tif !mset.isLeader() {\n\t\tmset.cancelMirrorConsumer()\n\t\tmset.mu.Unlock()\n\t\treturn false\n\t}\n\n\tisControl := m.isControlMsg()\n\n\t// Ignore from old subscriptions.\n\t// The reason we can not just compare subs is that on cross account imports they will not match.\n\tif !mset.mirror.isCurrentSub(m.rply) && !isControl {\n\t\tmset.mu.Unlock()\n\t\treturn false\n\t}\n\n\t// Check for heartbeats and flow control messages.\n\tif isControl {\n\t\tvar needsRetry bool\n\t\t// Flow controls have reply subjects.\n\t\tif m.rply != _EMPTY_ {\n\t\t\tmset.handleFlowControl(m)\n\t\t} else {\n\t\t\t// For idle heartbeats make sure we did not miss anything and check if we are considered stalled.\n\t\t\tif ldseq := parseInt64(getHeader(JSLastConsumerSeq, m.hdr)); ldseq > 0 && uint64(ldseq) != mset.mirror.dseq {\n\t\t\t\tneedsRetry = true\n\t\t\t} else if fcReply := getHeader(JSConsumerStalled, m.hdr); len(fcReply) > 0 {\n\t\t\t\t// Other side thinks we are stalled, so send flow control reply.\n\t\t\t\tmset.outq.sendMsg(string(fcReply), nil)\n\t\t\t}\n\t\t}\n\t\tmset.mu.Unlock()\n\t\tif needsRetry {\n\t\t\tmset.retryMirrorConsumer()\n\t\t}\n\t\treturn !needsRetry\n\t}\n\n\tsseq, dseq, dc, ts, pending := replyInfo(m.rply)\n\n\tif dc > 1 {\n\t\tmset.mu.Unlock()\n\t\treturn false\n\t}\n\n\t// Mirror info tracking.\n\tolag, osseq, odseq := mset.mirror.lag, mset.mirror.sseq, mset.mirror.dseq\n\tif sseq == mset.mirror.sseq+1 {\n\t\tmset.mirror.dseq = dseq\n\t\tmset.mirror.sseq++\n\t} else if sseq <= mset.mirror.sseq {\n\t\t// Ignore older messages.\n\t\tmset.mu.Unlock()\n\t\treturn true\n\t} else if mset.mirror.cname == _EMPTY_ {\n\t\tmset.mirror.cname = tokenAt(m.rply, 4)\n\t\tmset.mirror.dseq, mset.mirror.sseq = dseq, sseq\n\t} else {\n\t\t// If the deliver sequence matches then the upstream stream has expired or deleted messages.\n\t\tif dseq == mset.mirror.dseq+1 {\n\t\t\tmset.skipMsgs(mset.mirror.sseq+1, sseq-1)\n\t\t\tmset.mirror.dseq++\n\t\t\tmset.mirror.sseq = sseq\n\t\t} else {\n\t\t\tmset.mu.Unlock()\n\t\t\tmset.retryMirrorConsumer()\n\t\t\treturn false\n\t\t}\n\t}\n\n\tif pending == 0 {\n\t\tmset.mirror.lag = 0\n\t} else {\n\t\tmset.mirror.lag = pending - 1\n\t}\n\n\t// Check if we allow mirror direct here. If so check they we have mostly caught up.\n\t// The reason we do not require 0 is if the source is active we may always be slightly behind.\n\tif mset.cfg.MirrorDirect && mset.mirror.dsub == nil && pending < dgetCaughtUpThresh {\n\t\tif err := mset.subscribeToMirrorDirect(); err != nil {\n\t\t\t// Disable since we had problems above.\n\t\t\tmset.cfg.MirrorDirect = false\n\t\t}\n\t}\n\n\t// Do the subject transform if there's one\n\tif len(mset.mirror.trs) > 0 {\n\t\tfor _, tr := range mset.mirror.trs {\n\t\t\tif tr == nil {\n\t\t\t\tcontinue\n\t\t\t} else {\n\t\t\t\ttsubj, err := tr.Match(m.subj)\n\t\t\t\tif err == nil {\n\t\t\t\t\tm.subj = tsubj\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\ts, js, stype := mset.srv, mset.js, mset.cfg.Storage\n\tnode := mset.node\n\tmset.mu.Unlock()\n\n\tvar err error\n\tif node != nil {\n\t\tif js.limitsExceeded(stype) {\n\t\t\ts.resourcesExceededError()\n\t\t\terr = ApiErrors[JSInsufficientResourcesErr]\n\t\t} else {\n\t\t\terr = node.Propose(encodeStreamMsg(m.subj, _EMPTY_, m.hdr, m.msg, sseq-1, ts, true))\n\t\t}\n\t} else {\n\t\terr = mset.processJetStreamMsg(m.subj, _EMPTY_, m.hdr, m.msg, sseq-1, ts, nil, true)\n\t}\n\tif err != nil {\n\t\tif strings.Contains(err.Error(), \"no space left\") {\n\t\t\ts.Errorf(\"JetStream out of space, will be DISABLED\")\n\t\t\ts.DisableJetStream()\n\t\t\treturn false\n\t\t}\n\t\tif err != errLastSeqMismatch {\n\t\t\tmset.mu.RLock()\n\t\t\taccName, sname := mset.acc.Name, mset.cfg.Name\n\t\t\tmset.mu.RUnlock()\n\t\t\ts.RateLimitWarnf(\"Error processing inbound mirror message for '%s' > '%s': %v\",\n\t\t\t\taccName, sname, err)\n\t\t} else {\n\t\t\t// We may have missed messages, restart.\n\t\t\tif sseq <= mset.lastSeq() {\n\t\t\t\tmset.mu.Lock()\n\t\t\t\tmset.mirror.lag = olag\n\t\t\t\tmset.mirror.sseq = osseq\n\t\t\t\tmset.mirror.dseq = odseq\n\t\t\t\tmset.mu.Unlock()\n\t\t\t\treturn false\n\t\t\t} else {\n\t\t\t\tmset.mu.Lock()\n\t\t\t\tmset.mirror.dseq = odseq\n\t\t\t\tmset.mirror.sseq = osseq\n\t\t\t\tmset.mu.Unlock()\n\t\t\t\tmset.retryMirrorConsumer()\n\t\t\t}\n\t\t}\n\t}\n\treturn err == nil\n}\n\nfunc (mset *stream) setMirrorErr(err *ApiError) {\n\tmset.mu.Lock()\n\tif mset.mirror != nil {\n\t\tmset.mirror.err = err\n\t}\n\tmset.mu.Unlock()\n}\n\n// Cancels a mirror consumer.\n//\n// Lock held on entry\nfunc (mset *stream) cancelMirrorConsumer() {\n\tif mset.mirror == nil {\n\t\treturn\n\t}\n\tmset.cancelSourceInfo(mset.mirror)\n}\n\n// Similar to setupMirrorConsumer except that it will print a debug statement\n// indicating that there is a retry.\n//\n// Lock is acquired in this function\nfunc (mset *stream) retryMirrorConsumer() error {\n\tmset.mu.Lock()\n\tdefer mset.mu.Unlock()\n\tmset.srv.Debugf(\"Retrying mirror consumer for '%s > %s'\", mset.acc.Name, mset.cfg.Name)\n\tmset.cancelMirrorConsumer()\n\treturn mset.setupMirrorConsumer()\n}\n\n// Lock should be held.\nfunc (mset *stream) skipMsgs(start, end uint64) {\n\tnode, store := mset.node, mset.store\n\t// If we are not clustered we can short circuit now with store.SkipMsgs\n\tif node == nil {\n\t\tstore.SkipMsgs(start, end-start+1)\n\t\tmset.lseq = end\n\t\treturn\n\t}\n\n\t// FIXME (dlc) - We should allow proposals of DeleteRange, but would need to make sure all peers support.\n\t// With syncRequest was easy to add bool into request.\n\tvar entries []*Entry\n\tfor seq := start; seq <= end; seq++ {\n\t\tentries = append(entries, newEntry(EntryNormal, encodeStreamMsg(_EMPTY_, _EMPTY_, nil, nil, seq-1, 0, false)))\n\t\t// So a single message does not get too big.\n\t\tif len(entries) > 10_000 {\n\t\t\tnode.ProposeMulti(entries)\n\t\t\t// We need to re-create `entries` because there is a reference\n\t\t\t// to it in the node's pae map.\n\t\t\tentries = entries[:0]\n\t\t}\n\t}\n\t// Send all at once.\n\tif len(entries) > 0 {\n\t\tnode.ProposeMulti(entries)\n\t}\n}\n\nconst (\n\t// Base retry backoff duration.\n\tretryBackOff = 5 * time.Second\n\t// Maximum amount we will wait.\n\tretryMaximum = 2 * time.Minute\n)\n\n// Calculate our backoff based on number of failures.\nfunc calculateRetryBackoff(fails int) time.Duration {\n\tbackoff := time.Duration(retryBackOff) * time.Duration(fails*2)\n\tif backoff > retryMaximum {\n\t\tbackoff = retryMaximum\n\t}\n\treturn backoff\n}\n\n// This will schedule a call to setupMirrorConsumer, taking into account the last\n// time it was retried and determine the soonest setupMirrorConsumer can be called\n// without tripping the sourceConsumerRetryThreshold. We will also take into account\n// number of failures and will back off our retries.\n// The mset.mirror pointer has been verified to be not nil by the caller.\n//\n// Lock held on entry\nfunc (mset *stream) scheduleSetupMirrorConsumerRetry() {\n\t// We are trying to figure out how soon we can retry. setupMirrorConsumer will reject\n\t// a retry if last was done less than \"sourceConsumerRetryThreshold\" ago.\n\tnext := sourceConsumerRetryThreshold - time.Since(mset.mirror.lreq)\n\tif next < 0 {\n\t\t// It means that we have passed the threshold and so we are ready to go.\n\t\tnext = 0\n\t}\n\t// Take into account failures here.\n\tnext += calculateRetryBackoff(mset.mirror.fails)\n\n\t// Add some jitter.\n\tnext += time.Duration(rand.Intn(int(100*time.Millisecond))) + 100*time.Millisecond\n\n\ttime.AfterFunc(next, func() {\n\t\tmset.mu.Lock()\n\t\tmset.setupMirrorConsumer()\n\t\tmset.mu.Unlock()\n\t})\n}\n\n// How long we wait for a response from a consumer create request for a source or mirror.\nvar srcConsumerWaitTime = 30 * time.Second\n\n// Setup our mirror consumer.\n// Lock should be held.\nfunc (mset *stream) setupMirrorConsumer() error {\n\tif mset.closed.Load() {\n\t\treturn errStreamClosed\n\t}\n\tif mset.outq == nil {\n\t\treturn errors.New(\"outq required\")\n\t}\n\t// We use to prevent update of a mirror configuration in cluster\n\t// mode but not in standalone. This is now fixed. However, without\n\t// rejecting the update, it could be that if the source stream was\n\t// removed and then later the mirrored stream config changed to\n\t// remove mirror configuration, this function would panic when\n\t// accessing mset.cfg.Mirror fields. Adding this protection in case\n\t// we allow in the future the mirror config to be changed (removed).\n\tif mset.cfg.Mirror == nil {\n\t\treturn errors.New(\"invalid mirror configuration\")\n\t}\n\n\t// If this is the first time\n\tif mset.mirror == nil {\n\t\tmset.mirror = &sourceInfo{name: mset.cfg.Mirror.Name}\n\t} else {\n\t\tmset.cancelSourceInfo(mset.mirror)\n\t\tmset.mirror.sseq = mset.lseq\n\t}\n\n\t// If we are no longer the leader stop trying.\n\tif !mset.isLeader() {\n\t\treturn nil\n\t}\n\n\tmirror := mset.mirror\n\n\t// We want to throttle here in terms of how fast we request new consumers,\n\t// or if the previous is still in progress.\n\tif last := time.Since(mirror.lreq); last < sourceConsumerRetryThreshold || mirror.sip {\n\t\tmset.scheduleSetupMirrorConsumerRetry()\n\t\treturn nil\n\t}\n\tmirror.lreq = time.Now()\n\n\t// Determine subjects etc.\n\tvar deliverSubject string\n\text := mset.cfg.Mirror.External\n\n\tif ext != nil && ext.DeliverPrefix != _EMPTY_ {\n\t\tdeliverSubject = strings.ReplaceAll(ext.DeliverPrefix+syncSubject(\".M\"), \"..\", \".\")\n\t} else {\n\t\tdeliverSubject = syncSubject(\"$JS.M\")\n\t}\n\n\t// Now send off request to create/update our consumer. This will be all API based even in single server mode.\n\t// We calculate durable names apriori so we do not need to save them off.\n\n\tvar state StreamState\n\tmset.store.FastState(&state)\n\n\treq := &CreateConsumerRequest{\n\t\tStream: mset.cfg.Mirror.Name,\n\t\tConfig: ConsumerConfig{\n\t\t\tDeliverSubject:    deliverSubject,\n\t\t\tDeliverPolicy:     DeliverByStartSequence,\n\t\t\tOptStartSeq:       state.LastSeq + 1,\n\t\t\tAckPolicy:         AckNone,\n\t\t\tAckWait:           22 * time.Hour,\n\t\t\tMaxDeliver:        1,\n\t\t\tHeartbeat:         sourceHealthHB,\n\t\t\tFlowControl:       true,\n\t\t\tDirect:            true,\n\t\t\tInactiveThreshold: sourceHealthCheckInterval,\n\t\t},\n\t}\n\n\t// Only use start optionals on first time.\n\tif state.Msgs == 0 && state.FirstSeq == 0 {\n\t\treq.Config.OptStartSeq = 0\n\t\tif mset.cfg.Mirror.OptStartSeq > 0 {\n\t\t\treq.Config.OptStartSeq = mset.cfg.Mirror.OptStartSeq\n\t\t} else if mset.cfg.Mirror.OptStartTime != nil {\n\t\t\treq.Config.OptStartTime = mset.cfg.Mirror.OptStartTime\n\t\t\treq.Config.DeliverPolicy = DeliverByStartTime\n\t\t}\n\t}\n\tif req.Config.OptStartSeq == 0 && req.Config.OptStartTime == nil {\n\t\t// If starting out and lastSeq is 0.\n\t\treq.Config.DeliverPolicy = DeliverAll\n\t}\n\n\t// Filters\n\tif mset.cfg.Mirror.FilterSubject != _EMPTY_ {\n\t\treq.Config.FilterSubject = mset.cfg.Mirror.FilterSubject\n\t\tmirror.sf = mset.cfg.Mirror.FilterSubject\n\t}\n\n\tif lst := len(mset.cfg.Mirror.SubjectTransforms); lst > 0 {\n\t\tsfs := make([]string, lst)\n\t\ttrs := make([]*subjectTransform, lst)\n\n\t\tfor i, tr := range mset.cfg.Mirror.SubjectTransforms {\n\t\t\t// will not fail as already checked before that the transform will work\n\t\t\tsubjectTransform, err := NewSubjectTransform(tr.Source, tr.Destination)\n\t\t\tif err != nil {\n\t\t\t\tmset.srv.Errorf(\"Unable to get transform for mirror consumer: %v\", err)\n\t\t\t}\n\t\t\tsfs[i] = tr.Source\n\t\t\ttrs[i] = subjectTransform\n\t\t}\n\t\tmirror.sfs = sfs\n\t\tmirror.trs = trs\n\t\t// If there was no explicit FilterSubject defined and we have a single\n\t\t// subject transform, use Config.FilterSubject instead of FilterSubjects\n\t\t// so that we can use the extended consumer create API down below.\n\t\tif req.Config.FilterSubject == _EMPTY_ && len(sfs) == 1 {\n\t\t\treq.Config.FilterSubject = sfs[0]\n\t\t} else {\n\t\t\treq.Config.FilterSubjects = sfs\n\t\t}\n\t}\n\n\trespCh := make(chan *JSApiConsumerCreateResponse, 1)\n\treply := infoReplySubject()\n\tcrSub, err := mset.subscribeInternal(reply, func(sub *subscription, c *client, _ *Account, subject, reply string, rmsg []byte) {\n\t\tmset.unsubscribe(sub)\n\t\t_, msg := c.msgParts(rmsg)\n\n\t\tvar ccr JSApiConsumerCreateResponse\n\t\tif err := json.Unmarshal(msg, &ccr); err != nil {\n\t\t\tc.Warnf(\"JetStream bad mirror consumer create response: %q\", msg)\n\t\t\tmset.setMirrorErr(ApiErrors[JSInvalidJSONErr])\n\t\t\treturn\n\t\t}\n\t\tselect {\n\t\tcase respCh <- &ccr:\n\t\tdefault:\n\t\t}\n\t})\n\tif err != nil {\n\t\tmirror.err = NewJSMirrorConsumerSetupFailedError(err, Unless(err))\n\t\tmset.scheduleSetupMirrorConsumerRetry()\n\t\treturn nil\n\t}\n\n\tvar subject string\n\tif req.Config.FilterSubject != _EMPTY_ {\n\t\treq.Config.Name = fmt.Sprintf(\"mirror-%s\", createConsumerName())\n\t\tsubject = fmt.Sprintf(JSApiConsumerCreateExT, mset.cfg.Mirror.Name, req.Config.Name, req.Config.FilterSubject)\n\t} else {\n\t\tsubject = fmt.Sprintf(JSApiConsumerCreateT, mset.cfg.Mirror.Name)\n\t}\n\tif ext != nil {\n\t\tsubject = strings.Replace(subject, JSApiPrefix, ext.ApiPrefix, 1)\n\t\tsubject = strings.ReplaceAll(subject, \"..\", \".\")\n\t}\n\n\t// Marshal now that we are done with `req`.\n\tb, _ := json.Marshal(req)\n\n\t// Reset\n\tmirror.msgs = nil\n\tmirror.err = nil\n\tmirror.sip = true\n\n\t// Send the consumer create request\n\tmset.outq.send(newJSPubMsg(subject, _EMPTY_, reply, nil, b, nil, 0))\n\n\tgo func() {\n\n\t\tvar retry bool\n\t\tdefer func() {\n\t\t\tmset.mu.Lock()\n\t\t\t// Check that this is still valid and if so, clear the \"setup in progress\" flag.\n\t\t\tif mset.mirror != nil {\n\t\t\t\tmset.mirror.sip = false\n\t\t\t\t// If we need to retry, schedule now\n\t\t\t\tif retry {\n\t\t\t\t\tmset.mirror.fails++\n\t\t\t\t\t// Cancel here since we can not do anything with this consumer at this point.\n\t\t\t\t\tmset.cancelSourceInfo(mset.mirror)\n\t\t\t\t\tmset.scheduleSetupMirrorConsumerRetry()\n\t\t\t\t} else {\n\t\t\t\t\t// Clear on success.\n\t\t\t\t\tmset.mirror.fails = 0\n\t\t\t\t}\n\t\t\t}\n\t\t\tmset.mu.Unlock()\n\t\t}()\n\n\t\t// Wait for previous processMirrorMsgs go routine to be completely done.\n\t\t// If none is running, this will not block.\n\t\tmirror.wg.Wait()\n\n\t\tselect {\n\t\tcase ccr := <-respCh:\n\t\t\tmset.mu.Lock()\n\t\t\t// Mirror config has been removed.\n\t\t\tif mset.mirror == nil {\n\t\t\t\tmset.mu.Unlock()\n\t\t\t\treturn\n\t\t\t}\n\t\t\tready := sync.WaitGroup{}\n\t\t\tmirror := mset.mirror\n\t\t\tmirror.err = nil\n\t\t\tif ccr.Error != nil || ccr.ConsumerInfo == nil {\n\t\t\t\tmset.srv.Warnf(\"JetStream error response for create mirror consumer: %+v\", ccr.Error)\n\t\t\t\tmirror.err = ccr.Error\n\t\t\t\t// Let's retry as soon as possible, but we are gated by sourceConsumerRetryThreshold\n\t\t\t\tretry = true\n\t\t\t\tmset.mu.Unlock()\n\t\t\t\treturn\n\t\t\t} else {\n\t\t\t\t// Setup actual subscription to process messages from our source.\n\t\t\t\tqname := fmt.Sprintf(\"[ACC:%s] stream mirror '%s' of '%s' msgs\", mset.acc.Name, mset.cfg.Name, mset.cfg.Mirror.Name)\n\t\t\t\t// Create a new queue each time\n\t\t\t\tmirror.msgs = newIPQueue[*inMsg](mset.srv, qname)\n\t\t\t\tmsgs := mirror.msgs\n\t\t\t\tsub, err := mset.subscribeInternal(deliverSubject, func(sub *subscription, c *client, _ *Account, subject, reply string, rmsg []byte) {\n\t\t\t\t\thdr, msg := c.msgParts(copyBytes(rmsg)) // Need to copy.\n\t\t\t\t\tif len(hdr) > 0 {\n\t\t\t\t\t\t// Remove any Nats-Expected- headers as we don't want to validate them.\n\t\t\t\t\t\thdr = removeHeaderIfPrefixPresent(hdr, \"Nats-Expected-\")\n\t\t\t\t\t}\n\t\t\t\t\tmset.queueInbound(msgs, subject, reply, hdr, msg, nil, nil)\n\t\t\t\t\tmirror.last.Store(time.Now().UnixNano())\n\t\t\t\t})\n\t\t\t\tif err != nil {\n\t\t\t\t\tmirror.err = NewJSMirrorConsumerSetupFailedError(err, Unless(err))\n\t\t\t\t\tretry = true\n\t\t\t\t\tmset.mu.Unlock()\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t\t// Save our sub.\n\t\t\t\tmirror.sub = sub\n\n\t\t\t\t// When an upstream stream expires messages or in general has messages that we want\n\t\t\t\t// that are no longer available we need to adjust here.\n\t\t\t\tvar state StreamState\n\t\t\t\tmset.store.FastState(&state)\n\n\t\t\t\t// Check if we need to skip messages.\n\t\t\t\tif state.LastSeq != ccr.ConsumerInfo.Delivered.Stream {\n\t\t\t\t\t// Check to see if delivered is past our last and we have no msgs. This will help the\n\t\t\t\t\t// case when mirroring a stream that has a very high starting sequence number.\n\t\t\t\t\tif state.Msgs == 0 && ccr.ConsumerInfo.Delivered.Stream > state.LastSeq {\n\t\t\t\t\t\tmset.store.PurgeEx(_EMPTY_, ccr.ConsumerInfo.Delivered.Stream+1, 0)\n\t\t\t\t\t\tmset.lseq = ccr.ConsumerInfo.Delivered.Stream\n\t\t\t\t\t} else {\n\t\t\t\t\t\tmset.skipMsgs(state.LastSeq+1, ccr.ConsumerInfo.Delivered.Stream)\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\t// Capture consumer name.\n\t\t\t\tmirror.cname = ccr.ConsumerInfo.Name\n\t\t\t\tmirror.dseq = 0\n\t\t\t\tmirror.sseq = ccr.ConsumerInfo.Delivered.Stream\n\t\t\t\tmirror.qch = make(chan struct{})\n\t\t\t\tmirror.wg.Add(1)\n\t\t\t\tready.Add(1)\n\t\t\t\tif !mset.srv.startGoRoutine(\n\t\t\t\t\tfunc() { mset.processMirrorMsgs(mirror, &ready) },\n\t\t\t\t\tpprofLabels{\n\t\t\t\t\t\t\"type\":     \"mirror\",\n\t\t\t\t\t\t\"account\":  mset.acc.Name,\n\t\t\t\t\t\t\"stream\":   mset.cfg.Name,\n\t\t\t\t\t\t\"consumer\": mirror.cname,\n\t\t\t\t\t},\n\t\t\t\t) {\n\t\t\t\t\tready.Done()\n\t\t\t\t}\n\t\t\t}\n\t\t\tmset.mu.Unlock()\n\t\t\tready.Wait()\n\t\tcase <-time.After(srcConsumerWaitTime):\n\t\t\tmset.unsubscribe(crSub)\n\t\t\t// We already waited 30 seconds, let's retry now.\n\t\t\tretry = true\n\t\t}\n\t}()\n\n\treturn nil\n}\n\nfunc (mset *stream) streamSource(iname string) *StreamSource {\n\tfor _, ssi := range mset.cfg.Sources {\n\t\tif ssi.iname == iname {\n\t\t\treturn ssi\n\t\t}\n\t}\n\treturn nil\n}\n\n// Lock should be held.\nfunc (mset *stream) retrySourceConsumerAtSeq(iName string, seq uint64) {\n\ts := mset.srv\n\n\ts.Debugf(\"Retrying source consumer for '%s > %s'\", mset.acc.Name, mset.cfg.Name)\n\n\t// setupSourceConsumer will check that the source is still configured.\n\tmset.setupSourceConsumer(iName, seq, time.Time{})\n}\n\n// Lock should be held.\nfunc (mset *stream) cancelSourceConsumer(iname string) {\n\tif si := mset.sources[iname]; si != nil {\n\t\tmset.cancelSourceInfo(si)\n\t\tsi.sseq, si.dseq = 0, 0\n\t}\n}\n\n// The `si` has been verified to be not nil. The sourceInfo's sub will\n// be unsubscribed and set to nil (if not already done) and the\n// cname will be reset. The message processing's go routine quit channel\n// will be closed if still opened.\n//\n// Lock should be held\nfunc (mset *stream) cancelSourceInfo(si *sourceInfo) {\n\tif si.sub != nil {\n\t\tmset.unsubscribe(si.sub)\n\t\tsi.sub = nil\n\t}\n\t// In case we had a mirror direct subscription.\n\tif si.dsub != nil {\n\t\tmset.unsubscribe(si.dsub)\n\t\tsi.dsub = nil\n\t}\n\tmset.removeInternalConsumer(si)\n\tif si.qch != nil {\n\t\tclose(si.qch)\n\t\tsi.qch = nil\n\t}\n\tif si.msgs != nil {\n\t\tsi.msgs.drain()\n\t\tsi.msgs.unregister()\n\t}\n\t// If we have a schedule setup go ahead and delete that.\n\tif t := mset.sourceSetupSchedules[si.iname]; t != nil {\n\t\tt.Stop()\n\t\tdelete(mset.sourceSetupSchedules, si.iname)\n\t}\n}\n\nconst sourceConsumerRetryThreshold = 2 * time.Second\n\n// This is the main function to call when needing to setup a new consumer for the source.\n// It actually only does the scheduling of the execution of trySetupSourceConsumer in order to implement retry backoff\n// and throttle the number of requests.\n// Lock should be held.\nfunc (mset *stream) setupSourceConsumer(iname string, seq uint64, startTime time.Time) {\n\tif mset.sourceSetupSchedules == nil {\n\t\tmset.sourceSetupSchedules = map[string]*time.Timer{}\n\t}\n\n\tif _, ok := mset.sourceSetupSchedules[iname]; ok {\n\t\t// If there is already a timer scheduled, we don't need to do anything.\n\t\treturn\n\t}\n\n\tsi := mset.sources[iname]\n\tif si == nil || si.sip { // if sourceInfo was removed or setup is in progress, nothing to do\n\t\treturn\n\t}\n\n\t// First calculate the delay until the next time we can\n\tvar scheduleDelay time.Duration\n\n\tif !si.lreq.IsZero() { // it's not the very first time we are called, compute the delay\n\t\t// We want to throttle here in terms of how fast we request new consumers\n\t\tif sinceLast := time.Since(si.lreq); sinceLast < sourceConsumerRetryThreshold {\n\t\t\tscheduleDelay = sourceConsumerRetryThreshold - sinceLast\n\t\t}\n\t\t// Is it a retry? If so, add a backoff\n\t\tif si.fails > 0 {\n\t\t\tscheduleDelay += calculateRetryBackoff(si.fails)\n\t\t}\n\t}\n\n\t// Always add some jitter\n\tscheduleDelay += time.Duration(rand.Intn(int(100*time.Millisecond))) + 100*time.Millisecond\n\n\t// Schedule the call to trySetupSourceConsumer\n\tmset.sourceSetupSchedules[iname] = time.AfterFunc(scheduleDelay, func() {\n\t\tmset.mu.Lock()\n\t\tdefer mset.mu.Unlock()\n\n\t\tdelete(mset.sourceSetupSchedules, iname)\n\t\tmset.trySetupSourceConsumer(iname, seq, startTime)\n\t})\n}\n\n// This is where we will actually try to create a new consumer for the source\n// Lock should be held.\nfunc (mset *stream) trySetupSourceConsumer(iname string, seq uint64, startTime time.Time) {\n\t// Ignore if closed or not leader.\n\tif mset.closed.Load() || !mset.isLeader() {\n\t\treturn\n\t}\n\n\tsi := mset.sources[iname]\n\tif si == nil {\n\t\treturn\n\t}\n\n\t// Cancel previous instance if applicable\n\tmset.cancelSourceInfo(si)\n\n\tssi := mset.streamSource(iname)\n\tif ssi == nil {\n\t\treturn\n\t}\n\n\tsi.lreq = time.Now()\n\n\t// Determine subjects etc.\n\tvar deliverSubject string\n\text := ssi.External\n\n\tif ext != nil && ext.DeliverPrefix != _EMPTY_ {\n\t\tdeliverSubject = strings.ReplaceAll(ext.DeliverPrefix+syncSubject(\".S\"), \"..\", \".\")\n\t} else {\n\t\tdeliverSubject = syncSubject(\"$JS.S\")\n\t}\n\n\treq := &CreateConsumerRequest{\n\t\tStream: si.name,\n\t\tConfig: ConsumerConfig{\n\t\t\tDeliverSubject:    deliverSubject,\n\t\t\tAckPolicy:         AckNone,\n\t\t\tAckWait:           22 * time.Hour,\n\t\t\tMaxDeliver:        1,\n\t\t\tHeartbeat:         sourceHealthHB,\n\t\t\tFlowControl:       true,\n\t\t\tDirect:            true,\n\t\t\tInactiveThreshold: sourceHealthCheckInterval,\n\t\t},\n\t}\n\n\t// If starting, check any configs.\n\tif !startTime.IsZero() && seq > 1 {\n\t\treq.Config.OptStartTime = &startTime\n\t\treq.Config.DeliverPolicy = DeliverByStartTime\n\t} else if seq <= 1 {\n\t\tif ssi.OptStartSeq > 0 {\n\t\t\treq.Config.OptStartSeq = ssi.OptStartSeq\n\t\t\treq.Config.DeliverPolicy = DeliverByStartSequence\n\t\t} else {\n\t\t\t// We have not recovered state so check that configured time is less that our first seq time.\n\t\t\tvar state StreamState\n\t\t\tmset.store.FastState(&state)\n\t\t\tif ssi.OptStartTime != nil {\n\t\t\t\tif !state.LastTime.IsZero() && ssi.OptStartTime.Before(state.LastTime) {\n\t\t\t\t\treq.Config.OptStartTime = &state.LastTime\n\t\t\t\t} else {\n\t\t\t\t\treq.Config.OptStartTime = ssi.OptStartTime\n\t\t\t\t}\n\t\t\t\treq.Config.DeliverPolicy = DeliverByStartTime\n\t\t\t} else if state.FirstSeq > 1 && !state.LastTime.IsZero() {\n\t\t\t\treq.Config.OptStartTime = &state.LastTime\n\t\t\t\treq.Config.DeliverPolicy = DeliverByStartTime\n\t\t\t}\n\t\t}\n\n\t} else {\n\t\treq.Config.OptStartSeq = seq\n\t\treq.Config.DeliverPolicy = DeliverByStartSequence\n\t}\n\t// Filters\n\tif ssi.FilterSubject != _EMPTY_ {\n\t\treq.Config.FilterSubject = ssi.FilterSubject\n\t}\n\n\tvar filterSubjects []string\n\tfor _, tr := range ssi.SubjectTransforms {\n\t\tfilterSubjects = append(filterSubjects, tr.Source)\n\t}\n\treq.Config.FilterSubjects = filterSubjects\n\n\trespCh := make(chan *JSApiConsumerCreateResponse, 1)\n\treply := infoReplySubject()\n\tcrSub, err := mset.subscribeInternal(reply, func(sub *subscription, c *client, _ *Account, subject, reply string, rmsg []byte) {\n\t\tmset.unsubscribe(sub)\n\t\t_, msg := c.msgParts(rmsg)\n\t\tvar ccr JSApiConsumerCreateResponse\n\t\tif err := json.Unmarshal(msg, &ccr); err != nil {\n\t\t\tc.Warnf(\"JetStream bad source consumer create response: %q\", msg)\n\t\t\treturn\n\t\t}\n\t\tselect {\n\t\tcase respCh <- &ccr:\n\t\tdefault:\n\t\t}\n\t})\n\tif err != nil {\n\t\tsi.err = NewJSSourceConsumerSetupFailedError(err, Unless(err))\n\t\tmset.setupSourceConsumer(iname, seq, startTime)\n\t\treturn\n\t}\n\n\tvar subject string\n\tif req.Config.FilterSubject != _EMPTY_ {\n\t\treq.Config.Name = fmt.Sprintf(\"src-%s\", createConsumerName())\n\t\tsubject = fmt.Sprintf(JSApiConsumerCreateExT, si.name, req.Config.Name, req.Config.FilterSubject)\n\t} else if len(req.Config.FilterSubjects) == 1 {\n\t\treq.Config.Name = fmt.Sprintf(\"src-%s\", createConsumerName())\n\t\t// It is necessary to switch to using FilterSubject here as the extended consumer\n\t\t// create API checks for it, so as to not accidentally allow multiple filtered subjects.\n\t\treq.Config.FilterSubject = req.Config.FilterSubjects[0]\n\t\treq.Config.FilterSubjects = nil\n\t\tsubject = fmt.Sprintf(JSApiConsumerCreateExT, si.name, req.Config.Name, req.Config.FilterSubject)\n\t} else {\n\t\tsubject = fmt.Sprintf(JSApiConsumerCreateT, si.name)\n\t}\n\tif ext != nil {\n\t\tsubject = strings.Replace(subject, JSApiPrefix, ext.ApiPrefix, 1)\n\t\tsubject = strings.ReplaceAll(subject, \"..\", \".\")\n\t}\n\n\t// Marshal request.\n\tb, _ := json.Marshal(req)\n\n\t// Reset\n\tsi.msgs = nil\n\tsi.err = nil\n\tsi.sip = true\n\n\t// Send the consumer create request\n\tmset.outq.send(newJSPubMsg(subject, _EMPTY_, reply, nil, b, nil, 0))\n\n\tgo func() {\n\n\t\tvar retry bool\n\t\tdefer func() {\n\t\t\tmset.mu.Lock()\n\t\t\t// Check that this is still valid and if so, clear the \"setup in progress\" flag.\n\t\t\tif si := mset.sources[iname]; si != nil {\n\t\t\t\tsi.sip = false\n\t\t\t\t// If we need to retry, schedule now\n\t\t\t\tif retry {\n\t\t\t\t\tsi.fails++\n\t\t\t\t\t// Cancel here since we can not do anything with this consumer at this point.\n\t\t\t\t\tmset.cancelSourceInfo(si)\n\t\t\t\t\tmset.setupSourceConsumer(iname, seq, startTime)\n\t\t\t\t} else {\n\t\t\t\t\t// Clear on success.\n\t\t\t\t\tsi.fails = 0\n\t\t\t\t}\n\t\t\t}\n\t\t\tmset.mu.Unlock()\n\t\t}()\n\n\t\tselect {\n\t\tcase ccr := <-respCh:\n\t\t\tmset.mu.Lock()\n\t\t\t// Check that it has not been removed or canceled (si.sub would be nil)\n\t\t\tif si := mset.sources[iname]; si != nil {\n\t\t\t\tsi.err = nil\n\t\t\t\tif ccr.Error != nil || ccr.ConsumerInfo == nil {\n\t\t\t\t\t// Note: this warning can happen a few times when starting up the server when sourcing streams are\n\t\t\t\t\t// defined, this is normal as the streams are re-created in no particular order and it is possible\n\t\t\t\t\t// that a stream sourcing another could come up before all of its sources have been recreated.\n\t\t\t\t\tmset.srv.Warnf(\"JetStream error response for stream %s create source consumer %s: %+v\", mset.cfg.Name, si.name, ccr.Error)\n\t\t\t\t\tsi.err = ccr.Error\n\t\t\t\t\t// Let's retry as soon as possible, but we are gated by sourceConsumerRetryThreshold\n\t\t\t\t\tretry = true\n\t\t\t\t\tmset.mu.Unlock()\n\t\t\t\t\treturn\n\t\t\t\t} else {\n\t\t\t\t\t// Check if our shared msg queue and go routine is running or not.\n\t\t\t\t\tif mset.smsgs == nil {\n\t\t\t\t\t\tqname := fmt.Sprintf(\"[ACC:%s] stream sources '%s' msgs\", mset.acc.Name, mset.cfg.Name)\n\t\t\t\t\t\tmset.smsgs = newIPQueue[*inMsg](mset.srv, qname)\n\t\t\t\t\t\tmset.srv.startGoRoutine(func() { mset.processAllSourceMsgs() },\n\t\t\t\t\t\t\tpprofLabels{\n\t\t\t\t\t\t\t\t\"type\":    \"source\",\n\t\t\t\t\t\t\t\t\"account\": mset.acc.Name,\n\t\t\t\t\t\t\t\t\"stream\":  mset.cfg.Name,\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t)\n\t\t\t\t\t}\n\n\t\t\t\t\t// Setup actual subscription to process messages from our source.\n\t\t\t\t\tif si.sseq != ccr.ConsumerInfo.Delivered.Stream {\n\t\t\t\t\t\tsi.sseq = ccr.ConsumerInfo.Delivered.Stream + 1\n\t\t\t\t\t}\n\t\t\t\t\t// Capture consumer name.\n\t\t\t\t\tsi.cname = ccr.ConsumerInfo.Name\n\n\t\t\t\t\t// Do not set si.sseq to seq here. si.sseq will be set in processInboundSourceMsg\n\t\t\t\t\tsi.dseq = 0\n\t\t\t\t\tsi.qch = make(chan struct{})\n\t\t\t\t\t// Set the last seen as now so that we don't fail at the first check.\n\t\t\t\t\tsi.last.Store(time.Now().UnixNano())\n\n\t\t\t\t\tmsgs := mset.smsgs\n\t\t\t\t\tsub, err := mset.subscribeInternal(deliverSubject, func(sub *subscription, c *client, _ *Account, subject, reply string, rmsg []byte) {\n\t\t\t\t\t\thdr, msg := c.msgParts(copyBytes(rmsg)) // Need to copy.\n\t\t\t\t\t\tmset.queueInbound(msgs, subject, reply, hdr, msg, si, nil)\n\t\t\t\t\t\tsi.last.Store(time.Now().UnixNano())\n\t\t\t\t\t})\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\tsi.err = NewJSSourceConsumerSetupFailedError(err, Unless(err))\n\t\t\t\t\t\tretry = true\n\t\t\t\t\t\tmset.mu.Unlock()\n\t\t\t\t\t\treturn\n\t\t\t\t\t}\n\t\t\t\t\t// Save our sub.\n\t\t\t\t\tsi.sub = sub\n\t\t\t\t}\n\t\t\t}\n\t\t\tmset.mu.Unlock()\n\t\tcase <-time.After(srcConsumerWaitTime):\n\t\t\tmset.unsubscribe(crSub)\n\t\t\t// We already waited 30 seconds, let's retry now.\n\t\t\tretry = true\n\t\t}\n\t}()\n}\n\n// This will process all inbound source msgs.\n// We mux them into one go routine to avoid lock contention and high cpu and thread thrashing.\n// TODO(dlc) make this more then one and pin sources to one of a group.\nfunc (mset *stream) processAllSourceMsgs() {\n\ts := mset.srv\n\tdefer s.grWG.Done()\n\n\tmset.mu.RLock()\n\tmsgs, qch := mset.smsgs, mset.qch\n\tmset.mu.RUnlock()\n\n\tt := time.NewTicker(sourceHealthCheckInterval)\n\tdefer t.Stop()\n\n\t// When we detect we are no longer leader, we will cleanup.\n\t// Should always return right after this is called.\n\tcleanUp := func() {\n\t\tmset.mu.Lock()\n\t\tdefer mset.mu.Unlock()\n\t\tfor _, si := range mset.sources {\n\t\t\tmset.cancelSourceConsumer(si.iname)\n\t\t}\n\t\tmset.smsgs.drain()\n\t\tmset.smsgs.unregister()\n\t\tmset.smsgs = nil\n\t}\n\n\tfor {\n\t\tselect {\n\t\tcase <-s.quitCh:\n\t\t\treturn\n\t\tcase <-qch:\n\t\t\treturn\n\t\tcase <-msgs.ch:\n\t\t\tims := msgs.pop()\n\t\t\tfor _, im := range ims {\n\t\t\t\tif !mset.processInboundSourceMsg(im.si, im) {\n\t\t\t\t\t// If we are no longer leader bail.\n\t\t\t\t\tif !mset.IsLeader() {\n\t\t\t\t\t\tmsgs.recycle(&ims)\n\t\t\t\t\t\tcleanUp()\n\t\t\t\t\t\treturn\n\t\t\t\t\t}\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t\tim.returnToPool()\n\t\t\t}\n\t\t\tmsgs.recycle(&ims)\n\t\tcase <-t.C:\n\t\t\t// If we are no longer leader bail.\n\t\t\tif !mset.IsLeader() {\n\t\t\t\tcleanUp()\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\t// Check health of all sources.\n\t\t\tvar stalled []*sourceInfo\n\t\t\tmset.mu.RLock()\n\t\t\tfor _, si := range mset.sources {\n\t\t\t\tif time.Since(time.Unix(0, si.last.Load())) > sourceHealthCheckInterval {\n\t\t\t\t\tstalled = append(stalled, si)\n\t\t\t\t}\n\t\t\t}\n\t\t\tnumSources := len(mset.sources)\n\t\t\tmset.mu.RUnlock()\n\n\t\t\t// This can happen on an update when no longer have sources.\n\t\t\tif numSources == 0 {\n\t\t\t\tcleanUp()\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\t// We do not want to block here so do in separate Go routine.\n\t\t\tif len(stalled) > 0 {\n\t\t\t\tgo func() {\n\t\t\t\t\tmset.mu.Lock()\n\t\t\t\t\tdefer mset.mu.Unlock()\n\t\t\t\t\tfor _, si := range stalled {\n\t\t\t\t\t\tmset.setupSourceConsumer(si.iname, si.sseq+1, time.Time{})\n\t\t\t\t\t\tsi.last.Store(time.Now().UnixNano())\n\t\t\t\t\t}\n\t\t\t\t}()\n\t\t\t}\n\t\t}\n\t}\n}\n\n// isControlMsg determines if this is a control message.\nfunc (m *inMsg) isControlMsg() bool {\n\treturn len(m.msg) == 0 && len(m.hdr) > 0 && bytes.HasPrefix(m.hdr, []byte(\"NATS/1.0 100 \"))\n}\n\n// Sends a reply to a flow control request.\nfunc (mset *stream) sendFlowControlReply(reply string) {\n\tmset.mu.RLock()\n\tif mset.isLeader() && mset.outq != nil {\n\t\tmset.outq.sendMsg(reply, nil)\n\t}\n\tmset.mu.RUnlock()\n}\n\n// handleFlowControl will properly handle flow control messages for both R==1 and R>1.\n// Lock should be held.\nfunc (mset *stream) handleFlowControl(m *inMsg) {\n\t// If we are clustered we will send the flow control message through the replication stack.\n\tif mset.isClustered() {\n\t\tmset.node.Propose(encodeStreamMsg(_EMPTY_, m.rply, m.hdr, nil, 0, 0, false))\n\t} else {\n\t\tmset.outq.sendMsg(m.rply, nil)\n\t}\n}\n\n// processInboundSourceMsg handles processing other stream messages bound for this stream.\nfunc (mset *stream) processInboundSourceMsg(si *sourceInfo, m *inMsg) bool {\n\tmset.mu.Lock()\n\t// If we are no longer the leader cancel this subscriber.\n\tif !mset.isLeader() {\n\t\tmset.cancelSourceConsumer(si.iname)\n\t\tmset.mu.Unlock()\n\t\treturn false\n\t}\n\n\tisControl := m.isControlMsg()\n\n\t// Ignore from old subscriptions.\n\tif !si.isCurrentSub(m.rply) && !isControl {\n\t\tmset.mu.Unlock()\n\t\treturn false\n\t}\n\n\t// Check for heartbeats and flow control messages.\n\tif isControl {\n\t\tvar needsRetry bool\n\t\t// Flow controls have reply subjects.\n\t\tif m.rply != _EMPTY_ {\n\t\t\tmset.handleFlowControl(m)\n\t\t} else {\n\t\t\t// For idle heartbeats make sure we did not miss anything.\n\t\t\tif ldseq := parseInt64(getHeader(JSLastConsumerSeq, m.hdr)); ldseq > 0 && uint64(ldseq) != si.dseq {\n\t\t\t\tneedsRetry = true\n\t\t\t\tmset.retrySourceConsumerAtSeq(si.iname, si.sseq+1)\n\t\t\t} else if fcReply := getHeader(JSConsumerStalled, m.hdr); len(fcReply) > 0 {\n\t\t\t\t// Other side thinks we are stalled, so send flow control reply.\n\t\t\t\tmset.outq.sendMsg(string(fcReply), nil)\n\t\t\t}\n\t\t}\n\t\tmset.mu.Unlock()\n\t\treturn !needsRetry\n\t}\n\n\tsseq, dseq, dc, _, pending := replyInfo(m.rply)\n\n\tif dc > 1 {\n\t\tmset.mu.Unlock()\n\t\treturn false\n\t}\n\n\t// Tracking is done here.\n\tif dseq == si.dseq+1 {\n\t\tsi.dseq++\n\t\tsi.sseq = sseq\n\t} else if dseq > si.dseq {\n\t\tif si.cname == _EMPTY_ {\n\t\t\tsi.cname = tokenAt(m.rply, 4)\n\t\t\tsi.dseq, si.sseq = dseq, sseq\n\t\t} else {\n\t\t\tmset.retrySourceConsumerAtSeq(si.iname, si.sseq+1)\n\t\t\tmset.mu.Unlock()\n\t\t\treturn false\n\t\t}\n\t} else {\n\t\tmset.mu.Unlock()\n\t\treturn false\n\t}\n\n\tif pending == 0 {\n\t\tsi.lag = 0\n\t} else {\n\t\tsi.lag = pending - 1\n\t}\n\tnode := mset.node\n\tmset.mu.Unlock()\n\n\thdr, msg := m.hdr, m.msg\n\n\t// If we are daisy chained here make sure to remove the original one.\n\tif len(hdr) > 0 {\n\t\thdr = removeHeaderIfPresent(hdr, JSStreamSource)\n\t\t// Remove any Nats-Expected- headers as we don't want to validate them.\n\t\thdr = removeHeaderIfPrefixPresent(hdr, \"Nats-Expected-\")\n\t}\n\t// Hold onto the origin reply which has all the metadata.\n\thdr = genHeader(hdr, JSStreamSource, si.genSourceHeader(m.rply))\n\n\t// Do the subject transform for the source if there's one\n\tif len(si.trs) > 0 {\n\t\tfor _, tr := range si.trs {\n\t\t\tif tr == nil {\n\t\t\t\tcontinue\n\t\t\t} else {\n\t\t\t\ttsubj, err := tr.Match(m.subj)\n\t\t\t\tif err == nil {\n\t\t\t\t\tm.subj = tsubj\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tvar err error\n\t// If we are clustered we need to propose this message to the underlying raft group.\n\tif node != nil {\n\t\terr = mset.processClusteredInboundMsg(m.subj, _EMPTY_, hdr, msg, nil, true)\n\t} else {\n\t\terr = mset.processJetStreamMsg(m.subj, _EMPTY_, hdr, msg, 0, 0, nil, true)\n\t}\n\n\tif err != nil {\n\t\ts := mset.srv\n\t\tif strings.Contains(err.Error(), \"no space left\") {\n\t\t\ts.Errorf(\"JetStream out of space, will be DISABLED\")\n\t\t\ts.DisableJetStream()\n\t\t} else {\n\t\t\tmset.mu.RLock()\n\t\t\taccName, sname, iName := mset.acc.Name, mset.cfg.Name, si.iname\n\t\t\tmset.mu.RUnlock()\n\n\t\t\t// Can happen temporarily all the time during normal operations when the sourcing stream\n\t\t\t// is working queue/interest with a limit and discard new.\n\t\t\t// TODO - Improve sourcing to WQ with limit and new to use flow control rather than re-creating the consumer.\n\t\t\tif errors.Is(err, ErrMaxMsgs) || errors.Is(err, ErrMaxBytes) {\n\t\t\t\t// Do not need to do a full retry that includes finding the last sequence in the stream\n\t\t\t\t// for that source. Just re-create starting with the seq we couldn't store instead.\n\t\t\t\tmset.mu.Lock()\n\t\t\t\tmset.retrySourceConsumerAtSeq(iName, si.sseq)\n\t\t\t\tmset.mu.Unlock()\n\t\t\t} else {\n\t\t\t\t// Log some warning for errors other than errLastSeqMismatch or errMaxMsgs.\n\t\t\t\tif !errors.Is(err, errLastSeqMismatch) {\n\t\t\t\t\ts.RateLimitWarnf(\"Error processing inbound source %q for '%s' > '%s': %v\",\n\t\t\t\t\t\tiName, accName, sname, err)\n\t\t\t\t}\n\t\t\t\t// Retry in all type of errors if we are still leader.\n\t\t\t\tif mset.isLeader() {\n\t\t\t\t\t// This will make sure the source is still in mset.sources map,\n\t\t\t\t\t// find the last sequence and then call setupSourceConsumer.\n\t\t\t\t\tiNameMap := map[string]struct{}{iName: {}}\n\t\t\t\t\tmset.setStartingSequenceForSources(iNameMap)\n\t\t\t\t\tmset.mu.Lock()\n\t\t\t\t\tmset.retrySourceConsumerAtSeq(iName, si.sseq+1)\n\t\t\t\t\tmset.mu.Unlock()\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\treturn false\n\t}\n\n\treturn true\n}\n\n// Generate a new (2.10) style source header (stream name, sequence number, source filter, source destination transform).\nfunc (si *sourceInfo) genSourceHeader(reply string) string {\n\tvar b strings.Builder\n\tiNameParts := strings.Split(si.iname, \" \")\n\n\tb.WriteString(iNameParts[0])\n\tb.WriteByte(' ')\n\t// Grab sequence as text here from reply subject.\n\tvar tsa [expectedNumReplyTokens]string\n\tstart, tokens := 0, tsa[:0]\n\tfor i := 0; i < len(reply); i++ {\n\t\tif reply[i] == btsep {\n\t\t\ttokens, start = append(tokens, reply[start:i]), i+1\n\t\t}\n\t}\n\ttokens = append(tokens, reply[start:])\n\tseq := \"1\" // Default\n\tif len(tokens) == expectedNumReplyTokens && tokens[0] == \"$JS\" && tokens[1] == \"ACK\" {\n\t\tseq = tokens[5]\n\t}\n\tb.WriteString(seq)\n\n\tb.WriteByte(' ')\n\tb.WriteString(iNameParts[1])\n\tb.WriteByte(' ')\n\tb.WriteString(iNameParts[2])\n\treturn b.String()\n}\n\n// Original version of header that stored ack reply direct.\nfunc streamAndSeqFromAckReply(reply string) (string, string, uint64) {\n\ttsa := [expectedNumReplyTokens]string{}\n\tstart, tokens := 0, tsa[:0]\n\tfor i := 0; i < len(reply); i++ {\n\t\tif reply[i] == btsep {\n\t\t\ttokens, start = append(tokens, reply[start:i]), i+1\n\t\t}\n\t}\n\ttokens = append(tokens, reply[start:])\n\tif len(tokens) != expectedNumReplyTokens || tokens[0] != \"$JS\" || tokens[1] != \"ACK\" {\n\t\treturn _EMPTY_, _EMPTY_, 0\n\t}\n\treturn tokens[2], _EMPTY_, uint64(parseAckReplyNum(tokens[5]))\n}\n\n// Extract the stream name, the source index name and the message sequence number from the source header.\n// Uses the filter and transform arguments to provide backwards compatibility\nfunc streamAndSeq(shdr string) (string, string, uint64) {\n\tif strings.HasPrefix(shdr, jsAckPre) {\n\t\treturn streamAndSeqFromAckReply(shdr)\n\t}\n\t// New version which is stream index name <SPC> sequence\n\tfields := strings.Split(shdr, \" \")\n\tnFields := len(fields)\n\n\tif nFields != 2 && nFields <= 3 {\n\t\treturn _EMPTY_, _EMPTY_, 0\n\t}\n\n\tif nFields >= 4 {\n\t\treturn fields[0], strings.Join([]string{fields[0], fields[2], fields[3]}, \" \"), uint64(parseAckReplyNum(fields[1]))\n\t} else {\n\t\treturn fields[0], _EMPTY_, uint64(parseAckReplyNum(fields[1]))\n\t}\n\n}\n\n// Lock should be held.\nfunc (mset *stream) setStartingSequenceForSources(iNames map[string]struct{}) {\n\tvar state StreamState\n\tmset.store.FastState(&state)\n\n\t// Do not reset sseq here so we can remember when purge/expiration happens.\n\tif state.Msgs == 0 {\n\t\tfor iName := range iNames {\n\t\t\tsi := mset.sources[iName]\n\t\t\tif si == nil {\n\t\t\t\tcontinue\n\t\t\t} else {\n\t\t\t\tsi.dseq = 0\n\t\t\t}\n\t\t}\n\t\treturn\n\t}\n\n\tvar smv StoreMsg\n\tfor seq := state.LastSeq; seq >= state.FirstSeq; {\n\t\tsm, err := mset.store.LoadPrevMsg(seq, &smv)\n\t\tif err == ErrStoreEOF || err != nil {\n\t\t\tbreak\n\t\t}\n\t\tseq = sm.seq - 1\n\t\tif len(sm.hdr) == 0 {\n\t\t\tcontinue\n\t\t}\n\n\t\tss := getHeader(JSStreamSource, sm.hdr)\n\t\tif len(ss) == 0 {\n\t\t\tcontinue\n\t\t}\n\t\tstreamName, indexName, sseq := streamAndSeq(bytesToString(ss))\n\n\t\tif _, ok := iNames[indexName]; ok {\n\t\t\tsi := mset.sources[indexName]\n\t\t\tsi.sseq = sseq\n\t\t\tsi.dseq = 0\n\t\t\tdelete(iNames, indexName)\n\t\t} else if indexName == _EMPTY_ && streamName != _EMPTY_ {\n\t\t\tfor iName := range iNames {\n\t\t\t\t// TODO streamSource is a linear walk, to optimize later\n\t\t\t\tif si := mset.sources[iName]; si != nil && streamName == si.name ||\n\t\t\t\t\t(mset.streamSource(iName).External != nil && streamName == si.name+\":\"+getHash(mset.streamSource(iName).External.ApiPrefix)) {\n\t\t\t\t\tsi.sseq = sseq\n\t\t\t\t\tsi.dseq = 0\n\t\t\t\t\tdelete(iNames, iName)\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tif len(iNames) == 0 {\n\t\t\tbreak\n\t\t}\n\t}\n}\n\n// Resets the SourceInfo for all the sources\n// lock should be held.\nfunc (mset *stream) resetSourceInfo() {\n\t// Reset if needed.\n\tmset.stopSourceConsumers()\n\tmset.sources = make(map[string]*sourceInfo)\n\n\tfor _, ssi := range mset.cfg.Sources {\n\t\tif ssi.iname == _EMPTY_ {\n\t\t\tssi.setIndexName()\n\t\t}\n\n\t\tvar si *sourceInfo\n\n\t\tif len(ssi.SubjectTransforms) == 0 {\n\t\t\tsi = &sourceInfo{name: ssi.Name, iname: ssi.iname, sf: ssi.FilterSubject}\n\t\t} else {\n\t\t\tsfs := make([]string, len(ssi.SubjectTransforms))\n\t\t\ttrs := make([]*subjectTransform, len(ssi.SubjectTransforms))\n\t\t\tfor i, str := range ssi.SubjectTransforms {\n\t\t\t\ttr, err := NewSubjectTransform(str.Source, str.Destination)\n\t\t\t\tif err != nil {\n\t\t\t\t\tmset.srv.Errorf(\"Unable to get subject transform for source: %v\", err)\n\t\t\t\t}\n\t\t\t\tsfs[i] = str.Source\n\t\t\t\ttrs[i] = tr\n\t\t\t}\n\t\t\tsi = &sourceInfo{name: ssi.Name, iname: ssi.iname, sfs: sfs, trs: trs}\n\t\t}\n\t\tmset.sources[ssi.iname] = si\n\t}\n}\n\n// This will do a reverse scan on startup or leader election\n// searching for the starting sequence number.\n// This can be slow in degenerative cases.\n// Lock should be held.\nfunc (mset *stream) startingSequenceForSources() {\n\tif len(mset.cfg.Sources) == 0 {\n\t\treturn\n\t}\n\n\t// Always reset here.\n\tmset.resetSourceInfo()\n\n\tvar state StreamState\n\tmset.store.FastState(&state)\n\n\t// Bail if no messages, meaning no context.\n\tif state.Msgs == 0 {\n\t\treturn\n\t}\n\n\t// For short circuiting return.\n\texpected := len(mset.cfg.Sources)\n\tseqs := make(map[string]uint64)\n\n\t// Stamp our si seq records on the way out.\n\tdefer func() {\n\t\tfor sname, seq := range seqs {\n\t\t\t// Ignore if not set.\n\t\t\tif seq == 0 {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif si := mset.sources[sname]; si != nil {\n\t\t\t\tsi.sseq = seq\n\t\t\t\tsi.dseq = 0\n\t\t\t}\n\t\t}\n\t}()\n\n\tupdate := func(iName string, seq uint64) {\n\t\t// Only update active in case we have older ones in here that got configured out.\n\t\tif si := mset.sources[iName]; si != nil {\n\t\t\tif _, ok := seqs[iName]; !ok {\n\t\t\t\tseqs[iName] = seq\n\t\t\t}\n\t\t}\n\t}\n\n\tvar smv StoreMsg\n\tfor seq := state.LastSeq; ; {\n\t\tsm, err := mset.store.LoadPrevMsg(seq, &smv)\n\t\tif err == ErrStoreEOF || err != nil {\n\t\t\tbreak\n\t\t}\n\t\tseq = sm.seq - 1\n\t\tif len(sm.hdr) == 0 {\n\t\t\tcontinue\n\t\t}\n\t\tss := getHeader(JSStreamSource, sm.hdr)\n\t\tif len(ss) == 0 {\n\t\t\tcontinue\n\t\t}\n\n\t\tstreamName, iName, sseq := streamAndSeq(bytesToString(ss))\n\t\tif iName == _EMPTY_ { // Pre-2.10 message header means it's a match for any source using that stream name\n\t\t\tfor _, ssi := range mset.cfg.Sources {\n\t\t\t\tif streamName == ssi.Name || (ssi.External != nil && streamName == ssi.Name+\":\"+getHash(ssi.External.ApiPrefix)) {\n\t\t\t\t\tupdate(ssi.iname, sseq)\n\t\t\t\t}\n\t\t\t}\n\t\t} else {\n\t\t\tupdate(iName, sseq)\n\t\t}\n\t\tif len(seqs) == expected {\n\t\t\treturn\n\t\t}\n\t}\n}\n\n// Setup our source consumers.\n// Lock should be held.\nfunc (mset *stream) setupSourceConsumers() error {\n\tif mset.outq == nil {\n\t\treturn errors.New(\"outq required\")\n\t}\n\t// Reset if needed.\n\tfor _, si := range mset.sources {\n\t\tif si.sub != nil {\n\t\t\tmset.cancelSourceConsumer(si.iname)\n\t\t}\n\t}\n\n\t// If we are no longer the leader, give up\n\tif !mset.isLeader() {\n\t\treturn nil\n\t}\n\n\tmset.startingSequenceForSources()\n\n\t// Setup our consumers at the proper starting position.\n\tfor _, ssi := range mset.cfg.Sources {\n\t\tif si := mset.sources[ssi.iname]; si != nil {\n\t\t\tmset.setupSourceConsumer(ssi.iname, si.sseq+1, time.Time{})\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// Will create internal subscriptions for the stream.\n// Lock should be held.\nfunc (mset *stream) subscribeToStream() error {\n\tif mset.active {\n\t\treturn nil\n\t}\n\tfor _, subject := range mset.cfg.Subjects {\n\t\tif _, err := mset.subscribeInternal(subject, mset.processInboundJetStreamMsg); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\t// Check if we need to setup mirroring.\n\tif mset.cfg.Mirror != nil {\n\t\t// setup the initial mirror sourceInfo\n\t\tmset.mirror = &sourceInfo{name: mset.cfg.Mirror.Name}\n\t\tsfs := make([]string, len(mset.cfg.Mirror.SubjectTransforms))\n\t\ttrs := make([]*subjectTransform, len(mset.cfg.Mirror.SubjectTransforms))\n\n\t\tfor i, tr := range mset.cfg.Mirror.SubjectTransforms {\n\t\t\t// will not fail as already checked before that the transform will work\n\t\t\tsubjectTransform, err := NewSubjectTransform(tr.Source, tr.Destination)\n\t\t\tif err != nil {\n\t\t\t\tmset.srv.Errorf(\"Unable to get transform for mirror consumer: %v\", err)\n\t\t\t}\n\n\t\t\tsfs[i] = tr.Source\n\t\t\ttrs[i] = subjectTransform\n\t\t}\n\t\tmset.mirror.sfs = sfs\n\t\tmset.mirror.trs = trs\n\t\t// delay the actual mirror consumer creation for after a delay\n\t\tmset.scheduleSetupMirrorConsumerRetry()\n\t} else if len(mset.cfg.Sources) > 0 && mset.sourcesConsumerSetup == nil {\n\t\t// Setup the initial source infos for the sources\n\t\tmset.resetSourceInfo()\n\t\t// Delay the actual source consumer(s) creation(s) for after a delay if a replicated stream.\n\t\t// If it's an R1, this is done at startup and we will do inline.\n\t\tif mset.cfg.Replicas == 1 {\n\t\t\tmset.setupSourceConsumers()\n\t\t} else {\n\t\t\tmset.sourcesConsumerSetup = time.AfterFunc(time.Duration(rand.Intn(int(500*time.Millisecond)))+100*time.Millisecond, func() {\n\t\t\t\tmset.mu.Lock()\n\t\t\t\tmset.setupSourceConsumers()\n\t\t\t\tmset.mu.Unlock()\n\t\t\t})\n\t\t}\n\t}\n\t// Check for direct get access.\n\t// We spin up followers for clustered streams in monitorStream().\n\tif mset.cfg.AllowDirect {\n\t\tif err := mset.subscribeToDirect(); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\tmset.active = true\n\treturn nil\n}\n\n// Lock should be held.\nfunc (mset *stream) subscribeToDirect() error {\n\t// We will make this listen on a queue group by default, which can allow mirrors to participate on opt-in basis.\n\tif mset.directSub == nil {\n\t\tdsubj := fmt.Sprintf(JSDirectMsgGetT, mset.cfg.Name)\n\t\tif sub, err := mset.queueSubscribeInternal(dsubj, dgetGroup, mset.processDirectGetRequest); err == nil {\n\t\t\tmset.directSub = sub\n\t\t} else {\n\t\t\treturn err\n\t\t}\n\t}\n\t// Now the one that will have subject appended past stream name.\n\tif mset.lastBySub == nil {\n\t\tdsubj := fmt.Sprintf(JSDirectGetLastBySubjectT, mset.cfg.Name, fwcs)\n\t\t// We will make this listen on a queue group by default, which can allow mirrors to participate on opt-in basis.\n\t\tif sub, err := mset.queueSubscribeInternal(dsubj, dgetGroup, mset.processDirectGetLastBySubjectRequest); err == nil {\n\t\t\tmset.lastBySub = sub\n\t\t} else {\n\t\t\treturn err\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// Lock should be held.\nfunc (mset *stream) unsubscribeToDirect() {\n\tif mset.directSub != nil {\n\t\tmset.unsubscribe(mset.directSub)\n\t\tmset.directSub = nil\n\t}\n\tif mset.lastBySub != nil {\n\t\tmset.unsubscribe(mset.lastBySub)\n\t\tmset.lastBySub = nil\n\t}\n}\n\n// Lock should be held.\nfunc (mset *stream) subscribeToMirrorDirect() error {\n\tif mset.mirror == nil {\n\t\treturn nil\n\t}\n\n\t// We will make this listen on a queue group by default, which can allow mirrors to participate on opt-in basis.\n\tif mset.mirror.dsub == nil {\n\t\tdsubj := fmt.Sprintf(JSDirectMsgGetT, mset.mirror.name)\n\t\t// We will make this listen on a queue group by default, which can allow mirrors to participate on opt-in basis.\n\t\tif sub, err := mset.queueSubscribeInternal(dsubj, dgetGroup, mset.processDirectGetRequest); err == nil {\n\t\t\tmset.mirror.dsub = sub\n\t\t} else {\n\t\t\treturn err\n\t\t}\n\t}\n\t// Now the one that will have subject appended past stream name.\n\tif mset.mirror.lbsub == nil {\n\t\tdsubj := fmt.Sprintf(JSDirectGetLastBySubjectT, mset.mirror.name, fwcs)\n\t\t// We will make this listen on a queue group by default, which can allow mirrors to participate on opt-in basis.\n\t\tif sub, err := mset.queueSubscribeInternal(dsubj, dgetGroup, mset.processDirectGetLastBySubjectRequest); err == nil {\n\t\t\tmset.mirror.lbsub = sub\n\t\t} else {\n\t\t\treturn err\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// Stop our source consumers.\n// Lock should be held.\nfunc (mset *stream) stopSourceConsumers() {\n\tfor _, si := range mset.sources {\n\t\tmset.cancelSourceInfo(si)\n\t}\n}\n\n// Lock should be held.\nfunc (mset *stream) removeInternalConsumer(si *sourceInfo) {\n\tif si == nil || si.cname == _EMPTY_ {\n\t\treturn\n\t}\n\tsi.cname = _EMPTY_\n}\n\n// Will unsubscribe from the stream.\n// Lock should be held.\nfunc (mset *stream) unsubscribeToStream(stopping bool) error {\n\tfor _, subject := range mset.cfg.Subjects {\n\t\tmset.unsubscribeInternal(subject)\n\t}\n\tif mset.mirror != nil {\n\t\tmset.cancelSourceInfo(mset.mirror)\n\t\tmset.mirror = nil\n\t}\n\n\tif len(mset.sources) > 0 {\n\t\tmset.stopSourceConsumers()\n\t}\n\n\t// In case we had a direct get subscriptions.\n\tif stopping {\n\t\tmset.unsubscribeToDirect()\n\t}\n\n\tmset.active = false\n\treturn nil\n}\n\n// Lock does NOT need to be held, we set the client on setup and never change it at this point.\nfunc (mset *stream) subscribeInternal(subject string, cb msgHandler) (*subscription, error) {\n\tif mset.closed.Load() {\n\t\treturn nil, errStreamClosed\n\t}\n\tif cb == nil {\n\t\treturn nil, errInvalidMsgHandler\n\t}\n\tc := mset.client\n\tsid := int(mset.sid.Add(1))\n\t// Now create the subscription\n\treturn c.processSub([]byte(subject), nil, []byte(strconv.Itoa(sid)), cb, false)\n}\n\n// Lock does NOT need to be held, we set the client on setup and never change it at this point.\nfunc (mset *stream) queueSubscribeInternal(subject, group string, cb msgHandler) (*subscription, error) {\n\tif mset.closed.Load() {\n\t\treturn nil, errStreamClosed\n\t}\n\tif cb == nil {\n\t\treturn nil, errInvalidMsgHandler\n\t}\n\tc := mset.client\n\tsid := int(mset.sid.Add(1))\n\t// Now create the subscription\n\treturn c.processSub([]byte(subject), []byte(group), []byte(strconv.Itoa(sid)), cb, false)\n}\n\n// This will unsubscribe us from the exact subject given.\n// We do not currently track the subs so do not have the sid.\n// This should be called only on an update.\n// Lock does NOT need to be held, we set the client on setup and never change it at this point.\nfunc (mset *stream) unsubscribeInternal(subject string) error {\n\tif mset.closed.Load() {\n\t\treturn errStreamClosed\n\t}\n\tc := mset.client\n\tvar sid []byte\n\tc.mu.Lock()\n\tfor _, sub := range c.subs {\n\t\tif subject == string(sub.subject) {\n\t\t\tsid = sub.sid\n\t\t\tbreak\n\t\t}\n\t}\n\tc.mu.Unlock()\n\n\tif sid != nil {\n\t\treturn c.processUnsub(sid)\n\t}\n\treturn nil\n}\n\n// Lock should be held.\nfunc (mset *stream) unsubscribe(sub *subscription) {\n\tif sub == nil || mset.closed.Load() {\n\t\treturn\n\t}\n\tmset.client.processUnsub(sub.sid)\n}\n\nfunc (mset *stream) setupStore(fsCfg *FileStoreConfig) error {\n\tmset.mu.Lock()\n\tmset.created = time.Now().UTC()\n\n\tswitch mset.cfg.Storage {\n\tcase MemoryStorage:\n\t\tms, err := newMemStore(&mset.cfg)\n\t\tif err != nil {\n\t\t\tmset.mu.Unlock()\n\t\t\treturn err\n\t\t}\n\t\tmset.store = ms\n\tcase FileStorage:\n\t\ts := mset.srv\n\t\tprf := s.jsKeyGen(s.getOpts().JetStreamKey, mset.acc.Name)\n\t\tif prf != nil {\n\t\t\t// We are encrypted here, fill in correct cipher selection.\n\t\t\tfsCfg.Cipher = s.getOpts().JetStreamCipher\n\t\t}\n\t\toldprf := s.jsKeyGen(s.getOpts().JetStreamOldKey, mset.acc.Name)\n\t\tcfg := *fsCfg\n\t\tcfg.srv = s\n\t\tfs, err := newFileStoreWithCreated(cfg, mset.cfg, mset.created, prf, oldprf)\n\t\tif err != nil {\n\t\t\tmset.mu.Unlock()\n\t\t\treturn err\n\t\t}\n\t\tmset.store = fs\n\t}\n\t// This will fire the callback but we do not require the lock since md will be 0 here.\n\tmset.store.RegisterStorageUpdates(mset.storeUpdates)\n\tmset.store.RegisterStorageRemoveMsg(func(seq uint64) {\n\t\tif mset.IsClustered() {\n\t\t\tif mset.IsLeader() {\n\t\t\t\tmset.mu.RLock()\n\t\t\t\tmd := streamMsgDelete{Seq: seq, NoErase: true, Stream: mset.cfg.Name}\n\t\t\t\tmset.node.Propose(encodeMsgDelete(&md))\n\t\t\t\tmset.mu.RUnlock()\n\t\t\t}\n\t\t} else {\n\t\t\tmset.removeMsg(seq)\n\t\t}\n\t})\n\tmset.store.RegisterSubjectDeleteMarkerUpdates(func(im *inMsg) {\n\t\tif mset.IsClustered() {\n\t\t\tif mset.IsLeader() {\n\t\t\t\tmset.processClusteredInboundMsg(im.subj, im.rply, im.hdr, im.msg, im.mt, false)\n\t\t\t}\n\t\t} else {\n\t\t\tmset.processJetStreamMsg(im.subj, im.rply, im.hdr, im.msg, 0, 0, im.mt, false)\n\t\t}\n\t})\n\tmset.mu.Unlock()\n\n\treturn nil\n}\n\n// Called for any updates to the underlying stream. We pass through the bytes to the\n// jetstream account. We do local processing for stream pending for consumers, but only\n// for removals.\n// Lock should not be held.\nfunc (mset *stream) storeUpdates(md, bd int64, seq uint64, subj string) {\n\t// If we have a single negative update then we will process our consumers for stream pending.\n\t// Purge and Store handled separately inside individual calls.\n\tif md == -1 && seq > 0 && subj != _EMPTY_ {\n\t\t// We use our consumer list mutex here instead of the main stream lock since it may be held already.\n\t\tmset.clsMu.RLock()\n\t\tif mset.csl != nil {\n\t\t\tmset.csl.Match(subj, func(o *consumer) {\n\t\t\t\to.decStreamPending(seq, subj)\n\t\t\t})\n\t\t} else {\n\t\t\tfor _, o := range mset.cList {\n\t\t\t\to.decStreamPending(seq, subj)\n\t\t\t}\n\t\t}\n\t\tmset.clsMu.RUnlock()\n\t} else if md < 0 {\n\t\t// Batch decrements we need to force consumers to re-calculate num pending.\n\t\tmset.clsMu.RLock()\n\t\tfor _, o := range mset.cList {\n\t\t\to.streamNumPendingLocked()\n\t\t}\n\t\tmset.clsMu.RUnlock()\n\t}\n\n\tif mset.jsa != nil {\n\t\tmset.jsa.updateUsage(mset.tier, mset.stype, bd)\n\t}\n}\n\n// NumMsgIds returns the number of message ids being tracked for duplicate suppression.\nfunc (mset *stream) numMsgIds() int {\n\tmset.mu.Lock()\n\tdefer mset.mu.Unlock()\n\tif !mset.ddloaded {\n\t\tmset.rebuildDedupe()\n\t}\n\treturn len(mset.ddmap)\n}\n\n// checkMsgId will process and check for duplicates.\n// Lock should be held.\nfunc (mset *stream) checkMsgId(id string) *ddentry {\n\tif !mset.ddloaded {\n\t\tmset.rebuildDedupe()\n\t}\n\tif id == _EMPTY_ || len(mset.ddmap) == 0 {\n\t\treturn nil\n\t}\n\treturn mset.ddmap[id]\n}\n\n// Will purge the entries that are past the window.\n// Should be called from a timer.\nfunc (mset *stream) purgeMsgIds() {\n\tmset.mu.Lock()\n\tdefer mset.mu.Unlock()\n\n\tnow := time.Now().UnixNano()\n\ttmrNext := mset.cfg.Duplicates\n\twindow := int64(tmrNext)\n\n\tfor i, dde := range mset.ddarr[mset.ddindex:] {\n\t\tif now-dde.ts >= window {\n\t\t\tdelete(mset.ddmap, dde.id)\n\t\t} else {\n\t\t\tmset.ddindex += i\n\t\t\t// Check if we should garbage collect here if we are 1/3 total size.\n\t\t\tif cap(mset.ddarr) > 3*(len(mset.ddarr)-mset.ddindex) {\n\t\t\t\tmset.ddarr = append([]*ddentry(nil), mset.ddarr[mset.ddindex:]...)\n\t\t\t\tmset.ddindex = 0\n\t\t\t}\n\t\t\ttmrNext = time.Duration(window - (now - dde.ts))\n\t\t\tbreak\n\t\t}\n\t}\n\tif len(mset.ddmap) > 0 {\n\t\t// Make sure to not fire too quick\n\t\tconst minFire = 50 * time.Millisecond\n\t\tif tmrNext < minFire {\n\t\t\ttmrNext = minFire\n\t\t}\n\t\tif mset.ddtmr != nil {\n\t\t\tmset.ddtmr.Reset(tmrNext)\n\t\t} else {\n\t\t\tmset.ddtmr = time.AfterFunc(tmrNext, mset.purgeMsgIds)\n\t\t}\n\t} else {\n\t\tif mset.ddtmr != nil {\n\t\t\tmset.ddtmr.Stop()\n\t\t\tmset.ddtmr = nil\n\t\t}\n\t\tmset.ddmap = nil\n\t\tmset.ddarr = nil\n\t\tmset.ddindex = 0\n\t}\n}\n\n// storeMsgIdLocked will store the message id for duplicate detection.\n// Lock should be held.\nfunc (mset *stream) storeMsgIdLocked(dde *ddentry) {\n\tif mset.ddmap == nil {\n\t\tmset.ddmap = make(map[string]*ddentry)\n\t}\n\tmset.ddmap[dde.id] = dde\n\tmset.ddarr = append(mset.ddarr, dde)\n\tif mset.ddtmr == nil {\n\t\tmset.ddtmr = time.AfterFunc(mset.cfg.Duplicates, mset.purgeMsgIds)\n\t}\n}\n\n// Fast lookup of msgId.\nfunc getMsgId(hdr []byte) string {\n\treturn string(getHeader(JSMsgId, hdr))\n}\n\n// Fast lookup of expected last msgId.\nfunc getExpectedLastMsgId(hdr []byte) string {\n\treturn string(getHeader(JSExpectedLastMsgId, hdr))\n}\n\n// Fast lookup of expected stream.\nfunc getExpectedStream(hdr []byte) string {\n\treturn string(getHeader(JSExpectedStream, hdr))\n}\n\n// Fast lookup of expected stream.\nfunc getExpectedLastSeq(hdr []byte) (uint64, bool) {\n\tbseq := getHeader(JSExpectedLastSeq, hdr)\n\tif len(bseq) == 0 {\n\t\treturn 0, false\n\t}\n\treturn uint64(parseInt64(bseq)), true\n}\n\n// Fast lookup of rollups.\nfunc getRollup(hdr []byte) string {\n\tr := getHeader(JSMsgRollup, hdr)\n\tif len(r) == 0 {\n\t\treturn _EMPTY_\n\t}\n\treturn strings.ToLower(string(r))\n}\n\n// Fast lookup of expected stream sequence per subject.\nfunc getExpectedLastSeqPerSubject(hdr []byte) (uint64, bool) {\n\tbseq := getHeader(JSExpectedLastSubjSeq, hdr)\n\tif len(bseq) == 0 {\n\t\treturn 0, false\n\t}\n\treturn uint64(parseInt64(bseq)), true\n}\n\n// Fast lookup of expected subject for the expected stream sequence per subject.\nfunc getExpectedLastSeqPerSubjectForSubject(hdr []byte) string {\n\treturn string(getHeader(JSExpectedLastSubjSeqSubj, hdr))\n}\n\n// Fast lookup of the message TTL from headers:\n// - Positive return value: duration in seconds.\n// - Zero return value: no TTL or parse error.\n// - Negative return value: never expires.\nfunc getMessageTTL(hdr []byte) (int64, error) {\n\tttl := getHeader(JSMessageTTL, hdr)\n\tif len(ttl) == 0 {\n\t\treturn 0, nil\n\t}\n\treturn parseMessageTTL(bytesToString(ttl))\n}\n\n// - Positive return value: duration in seconds.\n// - Zero return value: no TTL or parse error.\n// - Negative return value: never expires.\nfunc parseMessageTTL(ttl string) (int64, error) {\n\tif strings.ToLower(ttl) == \"never\" {\n\t\treturn -1, nil\n\t}\n\tdur, err := time.ParseDuration(ttl)\n\tif err == nil {\n\t\tif dur < time.Second {\n\t\t\treturn 0, NewJSMessageTTLInvalidError()\n\t\t}\n\t\treturn int64(dur.Seconds()), nil\n\t}\n\tt := parseInt64(stringToBytes(ttl))\n\tif t < 0 {\n\t\t// This probably means a parse failure, hence why\n\t\t// we have a special case \"never\" for returning -1.\n\t\t// Otherwise we can't know if it's a genuine TTL\n\t\t// that says never expire or if it's a parse error.\n\t\treturn 0, NewJSMessageTTLInvalidError()\n\t}\n\treturn t, nil\n}\n\n// Signal if we are clustered. Will acquire rlock.\nfunc (mset *stream) IsClustered() bool {\n\tmset.mu.RLock()\n\tdefer mset.mu.RUnlock()\n\treturn mset.isClustered()\n}\n\n// Lock should be held.\nfunc (mset *stream) isClustered() bool {\n\treturn mset.node != nil\n}\n\n// Used if we have to queue things internally to avoid the route/gw path.\ntype inMsg struct {\n\tsubj string\n\trply string\n\thdr  []byte\n\tmsg  []byte\n\tsi   *sourceInfo\n\tmt   *msgTrace\n}\n\nvar inMsgPool = sync.Pool{\n\tNew: func() any {\n\t\treturn &inMsg{}\n\t},\n}\n\nfunc (im *inMsg) returnToPool() {\n\tim.subj, im.rply, im.hdr, im.msg, im.si, im.mt = _EMPTY_, _EMPTY_, nil, nil, nil, nil\n\tinMsgPool.Put(im)\n}\n\nfunc (mset *stream) queueInbound(ib *ipQueue[*inMsg], subj, rply string, hdr, msg []byte, si *sourceInfo, mt *msgTrace) {\n\tim := inMsgPool.Get().(*inMsg)\n\tim.subj, im.rply, im.hdr, im.msg, im.si, im.mt = subj, rply, hdr, msg, si, mt\n\tif _, err := ib.push(im); err != nil {\n\t\tim.returnToPool()\n\t\tstreamName := mset.cfg.Name\n\t\tmset.srv.RateLimitWarnf(\"Dropping messages due to excessive stream ingest rate on '%s' > '%s': %s\", mset.acc.Name, streamName, err)\n\t\tif rply != _EMPTY_ {\n\t\t\thdr := []byte(\"NATS/1.0 429 Too Many Requests\\r\\n\\r\\n\")\n\t\t\tb, _ := json.Marshal(&JSPubAckResponse{PubAck: &PubAck{Stream: streamName}, Error: NewJSStreamTooManyRequestsError()})\n\t\t\tmset.outq.send(newJSPubMsg(rply, _EMPTY_, _EMPTY_, hdr, b, nil, 0))\n\t\t}\n\t}\n}\n\nvar dgPool = sync.Pool{\n\tNew: func() any {\n\t\treturn &directGetReq{}\n\t},\n}\n\n// For when we need to not inline the request.\ntype directGetReq struct {\n\t// Copy of this is correct for this.\n\treq   JSApiMsgGetRequest\n\treply string\n}\n\n// processDirectGetRequest handles direct get request for stream messages.\nfunc (mset *stream) processDirectGetRequest(_ *subscription, c *client, _ *Account, subject, reply string, rmsg []byte) {\n\tif len(reply) == 0 {\n\t\treturn\n\t}\n\t_, msg := c.msgParts(rmsg)\n\tif len(msg) == 0 {\n\t\thdr := []byte(\"NATS/1.0 408 Empty Request\\r\\n\\r\\n\")\n\t\tmset.outq.send(newJSPubMsg(reply, _EMPTY_, _EMPTY_, hdr, nil, nil, 0))\n\t\treturn\n\t}\n\tvar req JSApiMsgGetRequest\n\terr := json.Unmarshal(msg, &req)\n\tif err != nil {\n\t\thdr := []byte(\"NATS/1.0 408 Malformed Request\\r\\n\\r\\n\")\n\t\tmset.outq.send(newJSPubMsg(reply, _EMPTY_, _EMPTY_, hdr, nil, nil, 0))\n\t\treturn\n\t}\n\t// Check if nothing set.\n\tif req.Seq == 0 && req.LastFor == _EMPTY_ && req.NextFor == _EMPTY_ && len(req.MultiLastFor) == 0 && req.StartTime == nil {\n\t\thdr := []byte(\"NATS/1.0 408 Empty Request\\r\\n\\r\\n\")\n\t\tmset.outq.send(newJSPubMsg(reply, _EMPTY_, _EMPTY_, hdr, nil, nil, 0))\n\t\treturn\n\t}\n\t// Check we don't have conflicting options set.\n\t// We do not allow batch mode for lastFor requests.\n\tif (req.Seq > 0 && req.LastFor != _EMPTY_) ||\n\t\t(req.Seq > 0 && req.StartTime != nil) ||\n\t\t(req.StartTime != nil && req.LastFor != _EMPTY_) ||\n\t\t(req.LastFor != _EMPTY_ && req.NextFor != _EMPTY_) ||\n\t\t(req.LastFor != _EMPTY_ && req.Batch > 0) ||\n\t\t(req.LastFor != _EMPTY_ && len(req.MultiLastFor) > 0) ||\n\t\t(req.NextFor != _EMPTY_ && len(req.MultiLastFor) > 0) ||\n\t\t(req.UpToSeq > 0 && req.UpToTime != nil) {\n\t\thdr := []byte(\"NATS/1.0 408 Bad Request\\r\\n\\r\\n\")\n\t\tmset.outq.send(newJSPubMsg(reply, _EMPTY_, _EMPTY_, hdr, nil, nil, 0))\n\t\treturn\n\t}\n\n\tinlineOk := c.kind != ROUTER && c.kind != GATEWAY && c.kind != LEAF\n\tif !inlineOk {\n\t\tdg := dgPool.Get().(*directGetReq)\n\t\tdg.req, dg.reply = req, reply\n\t\tmset.gets.push(dg)\n\t} else {\n\t\tmset.getDirectRequest(&req, reply)\n\t}\n}\n\n// This is for direct get by last subject which is part of the subject itself.\nfunc (mset *stream) processDirectGetLastBySubjectRequest(_ *subscription, c *client, _ *Account, subject, reply string, rmsg []byte) {\n\tif len(reply) == 0 {\n\t\treturn\n\t}\n\t_, msg := c.msgParts(rmsg)\n\t// This version expects no payload.\n\tif len(msg) != 0 {\n\t\thdr := []byte(\"NATS/1.0 408 Bad Request\\r\\n\\r\\n\")\n\t\tmset.outq.send(newJSPubMsg(reply, _EMPTY_, _EMPTY_, hdr, nil, nil, 0))\n\t\treturn\n\t}\n\t// Extract the key.\n\tvar key string\n\tfor i, n := 0, 0; i < len(subject); i++ {\n\t\tif subject[i] == btsep {\n\t\t\tif n == 4 {\n\t\t\t\tif start := i + 1; start < len(subject) {\n\t\t\t\t\tkey = subject[i+1:]\n\t\t\t\t}\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tn++\n\t\t}\n\t}\n\tif len(key) == 0 {\n\t\thdr := []byte(\"NATS/1.0 408 Bad Request\\r\\n\\r\\n\")\n\t\tmset.outq.send(newJSPubMsg(reply, _EMPTY_, _EMPTY_, hdr, nil, nil, 0))\n\t\treturn\n\t}\n\n\treq := JSApiMsgGetRequest{LastFor: key}\n\n\tinlineOk := c.kind != ROUTER && c.kind != GATEWAY && c.kind != LEAF\n\tif !inlineOk {\n\t\tdg := dgPool.Get().(*directGetReq)\n\t\tdg.req, dg.reply = req, reply\n\t\tmset.gets.push(dg)\n\t} else {\n\t\tmset.getDirectRequest(&req, reply)\n\t}\n}\n\n// For direct get batch and multi requests.\nconst (\n\tdg   = \"NATS/1.0\\r\\nNats-Stream: %s\\r\\nNats-Subject: %s\\r\\nNats-Sequence: %d\\r\\nNats-Time-Stamp: %s\\r\\n\\r\\n\"\n\tdgb  = \"NATS/1.0\\r\\nNats-Stream: %s\\r\\nNats-Subject: %s\\r\\nNats-Sequence: %d\\r\\nNats-Time-Stamp: %s\\r\\nNats-Num-Pending: %d\\r\\nNats-Last-Sequence: %d\\r\\n\\r\\n\"\n\teob  = \"NATS/1.0 204 EOB\\r\\nNats-Num-Pending: %d\\r\\nNats-Last-Sequence: %d\\r\\n\\r\\n\"\n\teobm = \"NATS/1.0 204 EOB\\r\\nNats-Num-Pending: %d\\r\\nNats-Last-Sequence: %d\\r\\nNats-UpTo-Sequence: %d\\r\\n\\r\\n\"\n)\n\n// Handle a multi request.\nfunc (mset *stream) getDirectMulti(req *JSApiMsgGetRequest, reply string) {\n\t// TODO(dlc) - Make configurable?\n\tconst maxAllowedResponses = 1024\n\n\t// We hold the lock here to try to avoid changes out from underneath of us.\n\tmset.mu.RLock()\n\tdefer mset.mu.RUnlock()\n\t// Grab store and name.\n\tstore, name, s := mset.store, mset.cfg.Name, mset.srv\n\n\t// Grab MaxBytes\n\tmb := req.MaxBytes\n\tif mb == 0 && s != nil {\n\t\t// Fill in with the server's MaxPending.\n\t\tmb = int(s.opts.MaxPending)\n\t}\n\n\tupToSeq := req.UpToSeq\n\t// If we have UpToTime set get the proper sequence.\n\tif req.UpToTime != nil {\n\t\tupToSeq = store.GetSeqFromTime((*req.UpToTime).UTC())\n\t\t// Avoid selecting a first sequence that will take us to before the stream first\n\t\t// sequence, otherwise we can return messages after the supplied UpToTime.\n\t\tif upToSeq <= mset.state().FirstSeq {\n\t\t\thdr := []byte(\"NATS/1.0 404 No Results\\r\\n\\r\\n\")\n\t\t\tmset.outq.send(newJSPubMsg(reply, _EMPTY_, _EMPTY_, hdr, nil, nil, 0))\n\t\t\treturn\n\t\t}\n\t\t// We need to back off one since this is used to determine start sequence normally,\n\t\t// whereas here we want it to be the ceiling.\n\t\tupToSeq--\n\t}\n\t// If not set, set to the last sequence and remember that for EOB.\n\tif upToSeq == 0 {\n\t\tvar state StreamState\n\t\tmset.store.FastState(&state)\n\t\tupToSeq = state.LastSeq\n\t}\n\n\tseqs, err := store.MultiLastSeqs(req.MultiLastFor, upToSeq, maxAllowedResponses)\n\tif err != nil {\n\t\tvar hdr []byte\n\t\tif err == ErrTooManyResults {\n\t\t\thdr = []byte(\"NATS/1.0 413 Too Many Results\\r\\n\\r\\n\")\n\t\t} else {\n\t\t\thdr = []byte(fmt.Sprintf(\"NATS/1.0 500 %v\\r\\n\\r\\n\", err))\n\t\t}\n\t\tmset.outq.send(newJSPubMsg(reply, _EMPTY_, _EMPTY_, hdr, nil, nil, 0))\n\t\treturn\n\t}\n\tif len(seqs) == 0 {\n\t\thdr := []byte(\"NATS/1.0 404 No Results\\r\\n\\r\\n\")\n\t\tmset.outq.send(newJSPubMsg(reply, _EMPTY_, _EMPTY_, hdr, nil, nil, 0))\n\t\treturn\n\t}\n\n\tnp, lseq, sentBytes, sent := uint64(len(seqs)-1), uint64(0), 0, 0\n\tfor _, seq := range seqs {\n\t\tif seq < req.Seq {\n\t\t\tif np > 0 {\n\t\t\t\tnp--\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\t\tvar svp StoreMsg\n\t\tsm, err := store.LoadMsg(seq, &svp)\n\t\tif err != nil {\n\t\t\thdr := []byte(\"NATS/1.0 404 Message Not Found\\r\\n\\r\\n\")\n\t\t\tmset.outq.send(newJSPubMsg(reply, _EMPTY_, _EMPTY_, hdr, nil, nil, 0))\n\t\t\treturn\n\t\t}\n\n\t\thdr := sm.hdr\n\t\tts := time.Unix(0, sm.ts).UTC()\n\n\t\tif len(hdr) == 0 {\n\t\t\thdr = fmt.Appendf(nil, dgb, name, sm.subj, sm.seq, ts.Format(time.RFC3339Nano), np, lseq)\n\t\t} else {\n\t\t\thdr = copyBytes(hdr)\n\t\t\thdr = genHeader(hdr, JSStream, name)\n\t\t\thdr = genHeader(hdr, JSSubject, sm.subj)\n\t\t\thdr = genHeader(hdr, JSSequence, strconv.FormatUint(sm.seq, 10))\n\t\t\thdr = genHeader(hdr, JSTimeStamp, ts.Format(time.RFC3339Nano))\n\t\t\thdr = genHeader(hdr, JSNumPending, strconv.FormatUint(np, 10))\n\t\t\thdr = genHeader(hdr, JSLastSequence, strconv.FormatUint(lseq, 10))\n\t\t}\n\t\t// Decrement num pending. This is optimization and we do not continue to look it up for these operations.\n\t\tif np > 0 {\n\t\t\tnp--\n\t\t}\n\t\t// Track our lseq\n\t\tlseq = sm.seq\n\t\t// Send out our message.\n\t\tmset.outq.send(newJSPubMsg(reply, _EMPTY_, _EMPTY_, hdr, sm.msg, nil, 0))\n\t\t// Check if we have exceeded max bytes.\n\t\tsentBytes += len(sm.subj) + len(sm.hdr) + len(sm.msg)\n\t\tif sentBytes >= mb {\n\t\t\tbreak\n\t\t}\n\t\tsent++\n\t\tif req.Batch > 0 && sent >= req.Batch {\n\t\t\tbreak\n\t\t}\n\t}\n\n\t// Send out EOB\n\thdr := fmt.Appendf(nil, eobm, np, lseq, upToSeq)\n\tmset.outq.send(newJSPubMsg(reply, _EMPTY_, _EMPTY_, hdr, nil, nil, 0))\n}\n\n// Do actual work on a direct msg request.\n// This could be called in a Go routine if we are inline for a non-client connection.\nfunc (mset *stream) getDirectRequest(req *JSApiMsgGetRequest, reply string) {\n\t// Handle multi in separate function.\n\tif len(req.MultiLastFor) > 0 {\n\t\tmset.getDirectMulti(req, reply)\n\t\treturn\n\t}\n\n\tmset.mu.RLock()\n\tstore, name, s := mset.store, mset.cfg.Name, mset.srv\n\tmset.mu.RUnlock()\n\n\tvar seq uint64\n\t// Lookup start seq if AsOfTime is set.\n\tif req.StartTime != nil {\n\t\tseq = store.GetSeqFromTime(*req.StartTime)\n\t} else {\n\t\tseq = req.Seq\n\t}\n\n\twc := subjectHasWildcard(req.NextFor)\n\t// For tracking num pending if we are batch.\n\tvar np, lseq, validThrough uint64\n\tvar isBatchRequest bool\n\tbatch := req.Batch\n\tif batch == 0 {\n\t\tbatch = 1\n\t} else {\n\t\t// This is a batch request, capture initial numPending.\n\t\tisBatchRequest = true\n\t\tnp, validThrough = store.NumPending(seq, req.NextFor, false)\n\t}\n\n\t// Grab MaxBytes\n\tmb := req.MaxBytes\n\tif mb == 0 && s != nil {\n\t\t// Fill in with the server's MaxPending.\n\t\tmb = int(s.opts.MaxPending)\n\t}\n\t// Track what we have sent.\n\tvar sentBytes int\n\n\t// Loop over batch, which defaults to 1.\n\tfor i := 0; i < batch; i++ {\n\t\tvar (\n\t\t\tsvp StoreMsg\n\t\t\tsm  *StoreMsg\n\t\t\terr error\n\t\t)\n\t\tif seq > 0 && req.NextFor == _EMPTY_ {\n\t\t\t// Only do direct lookup for first in a batch.\n\t\t\tif i == 0 {\n\t\t\t\tsm, err = store.LoadMsg(seq, &svp)\n\t\t\t} else {\n\t\t\t\t// We want to use load next with fwcs to step over deleted msgs.\n\t\t\t\tsm, seq, err = store.LoadNextMsg(fwcs, true, seq, &svp)\n\t\t\t}\n\t\t\t// Bump for next loop if applicable.\n\t\t\tseq++\n\t\t} else if req.NextFor != _EMPTY_ {\n\t\t\tsm, seq, err = store.LoadNextMsg(req.NextFor, wc, seq, &svp)\n\t\t\tseq++\n\t\t} else {\n\t\t\t// Batch is not applicable here, this is checked before we get here.\n\t\t\tsm, err = store.LoadLastMsg(req.LastFor, &svp)\n\t\t}\n\t\tif err != nil {\n\t\t\t// For batches, if we stop early we want to do EOB logic below.\n\t\t\tif batch > 1 && i > 0 {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\thdr := []byte(\"NATS/1.0 404 Message Not Found\\r\\n\\r\\n\")\n\t\t\tmset.outq.send(newJSPubMsg(reply, _EMPTY_, _EMPTY_, hdr, nil, nil, 0))\n\t\t\treturn\n\t\t}\n\n\t\thdr := sm.hdr\n\t\tts := time.Unix(0, sm.ts).UTC()\n\n\t\tif isBatchRequest {\n\t\t\tif len(hdr) == 0 {\n\t\t\t\thdr = fmt.Appendf(nil, dgb, name, sm.subj, sm.seq, ts.Format(time.RFC3339Nano), np, lseq)\n\t\t\t} else {\n\t\t\t\thdr = copyBytes(hdr)\n\t\t\t\thdr = genHeader(hdr, JSStream, name)\n\t\t\t\thdr = genHeader(hdr, JSSubject, sm.subj)\n\t\t\t\thdr = genHeader(hdr, JSSequence, strconv.FormatUint(sm.seq, 10))\n\t\t\t\thdr = genHeader(hdr, JSTimeStamp, ts.Format(time.RFC3339Nano))\n\t\t\t\thdr = genHeader(hdr, JSNumPending, strconv.FormatUint(np, 10))\n\t\t\t\thdr = genHeader(hdr, JSLastSequence, strconv.FormatUint(lseq, 10))\n\t\t\t}\n\t\t\t// Decrement num pending. This is optimization and we do not continue to look it up for these operations.\n\t\t\tnp--\n\t\t} else {\n\t\t\tif len(hdr) == 0 {\n\t\t\t\thdr = fmt.Appendf(nil, dg, name, sm.subj, sm.seq, ts.Format(time.RFC3339Nano))\n\t\t\t} else {\n\t\t\t\thdr = copyBytes(hdr)\n\t\t\t\thdr = genHeader(hdr, JSStream, name)\n\t\t\t\thdr = genHeader(hdr, JSSubject, sm.subj)\n\t\t\t\thdr = genHeader(hdr, JSSequence, strconv.FormatUint(sm.seq, 10))\n\t\t\t\thdr = genHeader(hdr, JSTimeStamp, ts.Format(time.RFC3339Nano))\n\t\t\t}\n\t\t}\n\t\t// Track our lseq\n\t\tlseq = sm.seq\n\t\t// Send out our message.\n\t\tmset.outq.send(newJSPubMsg(reply, _EMPTY_, _EMPTY_, hdr, sm.msg, nil, 0))\n\t\t// Check if we have exceeded max bytes.\n\t\tsentBytes += len(sm.subj) + len(sm.hdr) + len(sm.msg)\n\t\tif sentBytes >= mb {\n\t\t\tbreak\n\t\t}\n\t}\n\n\t// If batch was requested send EOB.\n\tif isBatchRequest {\n\t\t// Update if the stream's lasts sequence has moved past our validThrough.\n\t\tif mset.lastSeq() > validThrough {\n\t\t\tnp, _ = store.NumPending(seq, req.NextFor, false)\n\t\t}\n\t\thdr := fmt.Appendf(nil, eob, np, lseq)\n\t\tmset.outq.send(newJSPubMsg(reply, _EMPTY_, _EMPTY_, hdr, nil, nil, 0))\n\t}\n}\n\n// processInboundJetStreamMsg handles processing messages bound for a stream.\nfunc (mset *stream) processInboundJetStreamMsg(_ *subscription, c *client, _ *Account, subject, reply string, rmsg []byte) {\n\thdr, msg := c.msgParts(copyBytes(rmsg)) // Need to copy.\n\tif mt, traceOnly := c.isMsgTraceEnabled(); mt != nil {\n\t\t// If message is delivered, we need to disable the message trace headers\n\t\t// to prevent a trace event to be generated when a stored message\n\t\t// is delivered to a consumer and routed.\n\t\tif !traceOnly {\n\t\t\tdisableTraceHeaders(c, hdr)\n\t\t}\n\t\t// This will add the jetstream event while in the client read loop.\n\t\t// Since the event will be updated in a different go routine, the\n\t\t// tracing object will have a separate reference to the JS trace\n\t\t// object.\n\t\tmt.addJetStreamEvent(mset.name())\n\t}\n\tmset.queueInbound(mset.msgs, subject, reply, hdr, msg, nil, c.pa.trace)\n}\n\nvar (\n\terrLastSeqMismatch   = errors.New(\"last sequence mismatch\")\n\terrMsgIdDuplicate    = errors.New(\"msgid is duplicate\")\n\terrStreamClosed      = errors.New(\"stream closed\")\n\terrInvalidMsgHandler = errors.New(\"undefined message handler\")\n\terrStreamMismatch    = errors.New(\"expected stream does not match\")\n\terrMsgTTLDisabled    = errors.New(\"message TTL disabled\")\n)\n\n// processJetStreamMsg is where we try to actually process the stream msg.\nfunc (mset *stream) processJetStreamMsg(subject, reply string, hdr, msg []byte, lseq uint64, ts int64, mt *msgTrace, sourced bool) (retErr error) {\n\tif mt != nil {\n\t\t// Only the leader/standalone will have mt!=nil. On exit, send the\n\t\t// message trace event.\n\t\tdefer func() {\n\t\t\tmt.sendEventFromJetStream(retErr)\n\t\t}()\n\t}\n\n\tif mset.closed.Load() {\n\t\treturn errStreamClosed\n\t}\n\n\tmset.mu.Lock()\n\ts, store := mset.srv, mset.store\n\n\ttraceOnly := mt.traceOnly()\n\tbumpCLFS := func() {\n\t\t// Do not bump if tracing and not doing message delivery.\n\t\tif traceOnly {\n\t\t\treturn\n\t\t}\n\t\tmset.clMu.Lock()\n\t\tmset.clfs++\n\t\tmset.clMu.Unlock()\n\t}\n\n\t// Apply the input subject transform if any\n\tif mset.itr != nil {\n\t\tts, err := mset.itr.Match(subject)\n\t\tif err == nil {\n\t\t\t// no filtering: if the subject doesn't map the source of the transform, don't change it\n\t\t\tsubject = ts\n\t\t}\n\t}\n\n\tvar accName string\n\tif mset.acc != nil {\n\t\taccName = mset.acc.Name\n\t}\n\n\tjs, jsa, doAck := mset.js, mset.jsa, !mset.cfg.NoAck\n\tname, stype := mset.cfg.Name, mset.cfg.Storage\n\tmaxMsgSize := int(mset.cfg.MaxMsgSize)\n\tnumConsumers := len(mset.consumers)\n\tinterestRetention := mset.cfg.Retention == InterestPolicy\n\t// Snapshot if we are the leader and if we can respond.\n\tisLeader, isSealed := mset.isLeaderNodeState(), mset.cfg.Sealed\n\tcanRespond := doAck && len(reply) > 0 && isLeader\n\n\tvar resp = &JSPubAckResponse{}\n\n\t// Bail here if sealed.\n\tif isSealed {\n\t\toutq := mset.outq\n\t\tmset.mu.Unlock()\n\t\tbumpCLFS()\n\t\tif canRespond && outq != nil {\n\t\t\tresp.PubAck = &PubAck{Stream: name}\n\t\t\tresp.Error = ApiErrors[JSStreamSealedErr]\n\t\t\tb, _ := json.Marshal(resp)\n\t\t\toutq.sendMsg(reply, b)\n\t\t}\n\t\treturn ApiErrors[JSStreamSealedErr]\n\t}\n\n\tvar buf [256]byte\n\tpubAck := append(buf[:0], mset.pubAck...)\n\n\t// If this is a non-clustered msg and we are not considered active, meaning no active subscription, do not process.\n\tif lseq == 0 && ts == 0 && !mset.active {\n\t\tmset.mu.Unlock()\n\t\treturn nil\n\t}\n\n\t// For clustering the lower layers will pass our expected lseq. If it is present check for that here.\n\tvar clfs uint64\n\tif lseq > 0 {\n\t\tclfs = mset.getCLFS()\n\t\tif lseq != (mset.lseq + clfs) {\n\t\t\tisMisMatch := true\n\t\t\t// We may be able to recover here if we have no state whatsoever, or we are a mirror.\n\t\t\t// See if we have to adjust our starting sequence.\n\t\t\tif mset.lseq == 0 || mset.cfg.Mirror != nil {\n\t\t\t\tvar state StreamState\n\t\t\t\tmset.store.FastState(&state)\n\t\t\t\tif state.FirstSeq == 0 {\n\t\t\t\t\tmset.store.Compact(lseq + 1)\n\t\t\t\t\tmset.lseq = lseq\n\t\t\t\t\tisMisMatch = false\n\t\t\t\t}\n\t\t\t}\n\t\t\t// Really is a mismatch.\n\t\t\tif isMisMatch {\n\t\t\t\toutq := mset.outq\n\t\t\t\tmset.mu.Unlock()\n\t\t\t\tif canRespond && outq != nil {\n\t\t\t\t\tresp.PubAck = &PubAck{Stream: name}\n\t\t\t\t\tresp.Error = ApiErrors[JSStreamSequenceNotMatchErr]\n\t\t\t\t\tb, _ := json.Marshal(resp)\n\t\t\t\t\toutq.sendMsg(reply, b)\n\t\t\t\t}\n\t\t\t\treturn errLastSeqMismatch\n\t\t\t}\n\t\t}\n\t}\n\n\t// If we have received this message across an account we may have request information attached.\n\t// For now remove. TODO(dlc) - Should this be opt-in or opt-out?\n\tif len(hdr) > 0 {\n\t\thdr = removeHeaderIfPresent(hdr, ClientInfoHdr)\n\t}\n\n\t// Process additional msg headers if still present.\n\tvar msgId string\n\tvar rollupSub, rollupAll bool\n\tisClustered := mset.isClustered()\n\n\tif len(hdr) > 0 {\n\t\toutq := mset.outq\n\n\t\t// Certain checks have already been performed if in clustered mode, so only check if not.\n\t\t// Note, for cluster mode but with message tracing (without message delivery), we need\n\t\t// to do this check here since it was not done in processClusteredInboundMsg().\n\t\tif !isClustered || traceOnly {\n\t\t\t// Expected stream.\n\t\t\tif sname := getExpectedStream(hdr); sname != _EMPTY_ && sname != name {\n\t\t\t\tmset.mu.Unlock()\n\t\t\t\tbumpCLFS()\n\t\t\t\tif canRespond {\n\t\t\t\t\tresp.PubAck = &PubAck{Stream: name}\n\t\t\t\t\tresp.Error = NewJSStreamNotMatchError()\n\t\t\t\t\tb, _ := json.Marshal(resp)\n\t\t\t\t\toutq.sendMsg(reply, b)\n\t\t\t\t}\n\t\t\t\treturn errStreamMismatch\n\t\t\t}\n\t\t}\n\n\t\t// TTL'd messages are rejected entirely if TTLs are not enabled on the stream.\n\t\t// Shouldn't happen in clustered mode since we should have already caught this\n\t\t// in processClusteredInboundMsg, but needed here for non-clustered etc.\n\t\tif ttl, _ := getMessageTTL(hdr); !sourced && ttl != 0 && !mset.cfg.AllowMsgTTL {\n\t\t\tmset.mu.Unlock()\n\t\t\tbumpCLFS()\n\t\t\tif canRespond {\n\t\t\t\tresp.PubAck = &PubAck{Stream: name}\n\t\t\t\tresp.Error = NewJSMessageTTLDisabledError()\n\t\t\t\tb, _ := json.Marshal(resp)\n\t\t\t\toutq.sendMsg(reply, b)\n\t\t\t}\n\t\t\treturn errMsgTTLDisabled\n\t\t}\n\n\t\t// Dedupe detection. This is done at the cluster level for dedupe detectiom above the\n\t\t// lower layers. But we still need to pull out the msgId.\n\t\tif msgId = getMsgId(hdr); msgId != _EMPTY_ {\n\t\t\t// Do real check only if not clustered or traceOnly flag is set.\n\t\t\tif !isClustered || traceOnly {\n\t\t\t\tif dde := mset.checkMsgId(msgId); dde != nil {\n\t\t\t\t\tmset.mu.Unlock()\n\t\t\t\t\tbumpCLFS()\n\t\t\t\t\tif canRespond {\n\t\t\t\t\t\tresponse := append(pubAck, strconv.FormatUint(dde.seq, 10)...)\n\t\t\t\t\t\tresponse = append(response, \",\\\"duplicate\\\": true}\"...)\n\t\t\t\t\t\toutq.sendMsg(reply, response)\n\t\t\t\t\t}\n\t\t\t\t\treturn errMsgIdDuplicate\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t// Expected last sequence per subject.\n\t\tif seq, exists := getExpectedLastSeqPerSubject(hdr); exists {\n\t\t\t// Allow override of the subject used for the check.\n\t\t\tseqSubj := subject\n\t\t\tif optSubj := getExpectedLastSeqPerSubjectForSubject(hdr); optSubj != _EMPTY_ {\n\t\t\t\tseqSubj = optSubj\n\t\t\t}\n\n\t\t\t// TODO(dlc) - We could make a new store func that does this all in one.\n\t\t\tvar smv StoreMsg\n\t\t\tvar fseq uint64\n\t\t\tsm, err := store.LoadLastMsg(seqSubj, &smv)\n\t\t\tif sm != nil {\n\t\t\t\tfseq = sm.seq\n\t\t\t}\n\t\t\tif err == ErrStoreMsgNotFound {\n\t\t\t\tif seq == 0 {\n\t\t\t\t\tfseq, err = 0, nil\n\t\t\t\t} else if mset.isClustered() {\n\t\t\t\t\t// Do not bump clfs in case message was not found and could have been deleted.\n\t\t\t\t\tvar ss StreamState\n\t\t\t\t\tstore.FastState(&ss)\n\t\t\t\t\tif seq <= ss.LastSeq {\n\t\t\t\t\t\tfseq, err = seq, nil\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tif err != nil || fseq != seq {\n\t\t\t\tmset.mu.Unlock()\n\t\t\t\tbumpCLFS()\n\t\t\t\tif canRespond {\n\t\t\t\t\tresp.PubAck = &PubAck{Stream: name}\n\t\t\t\t\tresp.Error = NewJSStreamWrongLastSequenceError(fseq)\n\t\t\t\t\tb, _ := json.Marshal(resp)\n\t\t\t\t\toutq.sendMsg(reply, b)\n\t\t\t\t}\n\t\t\t\treturn fmt.Errorf(\"last sequence by subject mismatch: %d vs %d\", seq, fseq)\n\t\t\t}\n\t\t}\n\n\t\t// Expected last sequence.\n\t\tif seq, exists := getExpectedLastSeq(hdr); exists && seq != mset.lseq {\n\t\t\tmlseq := mset.lseq\n\t\t\tmset.mu.Unlock()\n\t\t\tbumpCLFS()\n\t\t\tif canRespond {\n\t\t\t\tresp.PubAck = &PubAck{Stream: name}\n\t\t\t\tresp.Error = NewJSStreamWrongLastSequenceError(mlseq)\n\t\t\t\tb, _ := json.Marshal(resp)\n\t\t\t\toutq.sendMsg(reply, b)\n\t\t\t}\n\t\t\treturn fmt.Errorf(\"last sequence mismatch: %d vs %d\", seq, mlseq)\n\t\t}\n\t\t// Expected last msgId.\n\t\tif lmsgId := getExpectedLastMsgId(hdr); lmsgId != _EMPTY_ {\n\t\t\tif mset.lmsgId == _EMPTY_ && !mset.ddloaded {\n\t\t\t\tmset.rebuildDedupe()\n\t\t\t}\n\t\t\tif lmsgId != mset.lmsgId {\n\t\t\t\tlast := mset.lmsgId\n\t\t\t\tmset.mu.Unlock()\n\t\t\t\tbumpCLFS()\n\t\t\t\tif canRespond {\n\t\t\t\t\tresp.PubAck = &PubAck{Stream: name}\n\t\t\t\t\tresp.Error = NewJSStreamWrongLastMsgIDError(last)\n\t\t\t\t\tb, _ := json.Marshal(resp)\n\t\t\t\t\toutq.sendMsg(reply, b)\n\t\t\t\t}\n\t\t\t\treturn fmt.Errorf(\"last msgid mismatch: %q vs %q\", lmsgId, last)\n\t\t\t}\n\t\t}\n\t\t// Check for any rollups.\n\t\tif rollup := getRollup(hdr); rollup != _EMPTY_ {\n\t\t\tif !mset.cfg.AllowRollup || mset.cfg.DenyPurge {\n\t\t\t\tmset.mu.Unlock()\n\t\t\t\tbumpCLFS()\n\t\t\t\tif canRespond {\n\t\t\t\t\tresp.PubAck = &PubAck{Stream: name}\n\t\t\t\t\tresp.Error = NewJSStreamRollupFailedError(errors.New(\"rollup not permitted\"))\n\t\t\t\t\tb, _ := json.Marshal(resp)\n\t\t\t\t\toutq.sendMsg(reply, b)\n\t\t\t\t}\n\t\t\t\treturn errors.New(\"rollup not permitted\")\n\t\t\t}\n\t\t\tswitch rollup {\n\t\t\tcase JSMsgRollupSubject:\n\t\t\t\trollupSub = true\n\t\t\tcase JSMsgRollupAll:\n\t\t\t\trollupAll = true\n\t\t\tdefault:\n\t\t\t\tmset.mu.Unlock()\n\t\t\t\tbumpCLFS()\n\t\t\t\terr := fmt.Errorf(\"rollup value invalid: %q\", rollup)\n\t\t\t\tif canRespond {\n\t\t\t\t\tresp.PubAck = &PubAck{Stream: name}\n\t\t\t\t\tresp.Error = NewJSStreamRollupFailedError(err)\n\t\t\t\t\tb, _ := json.Marshal(resp)\n\t\t\t\t\toutq.sendMsg(reply, b)\n\t\t\t\t}\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t}\n\n\t// Response Ack.\n\tvar (\n\t\tresponse []byte\n\t\tseq      uint64\n\t\terr      error\n\t)\n\n\t// Check to see if we are over the max msg size.\n\tif maxMsgSize >= 0 && (len(hdr)+len(msg)) > maxMsgSize {\n\t\tmset.mu.Unlock()\n\t\tbumpCLFS()\n\t\tif canRespond {\n\t\t\tresp.PubAck = &PubAck{Stream: name}\n\t\t\tresp.Error = NewJSStreamMessageExceedsMaximumError()\n\t\t\tresponse, _ = json.Marshal(resp)\n\t\t\tmset.outq.sendMsg(reply, response)\n\t\t}\n\t\treturn ErrMaxPayload\n\t}\n\n\tif len(hdr) > math.MaxUint16 {\n\t\tmset.mu.Unlock()\n\t\tbumpCLFS()\n\t\tif canRespond {\n\t\t\tresp.PubAck = &PubAck{Stream: name}\n\t\t\tresp.Error = NewJSStreamHeaderExceedsMaximumError()\n\t\t\tresponse, _ = json.Marshal(resp)\n\t\t\tmset.outq.sendMsg(reply, response)\n\t\t}\n\t\treturn ErrMaxPayload\n\t}\n\n\t// Check to see if we have exceeded our limits.\n\tif js.limitsExceeded(stype) {\n\t\ts.resourcesExceededError()\n\t\tmset.mu.Unlock()\n\t\tbumpCLFS()\n\t\tif canRespond {\n\t\t\tresp.PubAck = &PubAck{Stream: name}\n\t\t\tresp.Error = NewJSInsufficientResourcesError()\n\t\t\tresponse, _ = json.Marshal(resp)\n\t\t\tmset.outq.sendMsg(reply, response)\n\t\t}\n\t\t// Stepdown regardless.\n\t\tif node := mset.raftNode(); node != nil {\n\t\t\tnode.StepDown()\n\t\t}\n\t\treturn NewJSInsufficientResourcesError()\n\t}\n\n\tvar noInterest bool\n\n\t// If we are interest based retention and have no consumers then we can skip.\n\tif interestRetention {\n\t\tmset.clsMu.RLock()\n\t\tnoInterest = numConsumers == 0 || mset.csl == nil || !mset.csl.HasInterest(subject)\n\t\tmset.clsMu.RUnlock()\n\t}\n\n\t// Grab timestamp if not already set.\n\tif ts == 0 && lseq > 0 {\n\t\tts = time.Now().UnixNano()\n\t}\n\n\tmt.updateJetStreamEvent(subject, noInterest)\n\tif traceOnly {\n\t\tmset.mu.Unlock()\n\t\treturn nil\n\t}\n\n\t// Skip msg here.\n\tif noInterest {\n\t\tmset.lseq = store.SkipMsg()\n\t\tmset.lmsgId = msgId\n\t\t// If we have a msgId make sure to save.\n\t\tif msgId != _EMPTY_ {\n\t\t\tmset.storeMsgIdLocked(&ddentry{msgId, mset.lseq, ts})\n\t\t}\n\t\tif canRespond {\n\t\t\tresponse = append(pubAck, strconv.FormatUint(mset.lseq, 10)...)\n\t\t\tresponse = append(response, '}')\n\t\t\tmset.outq.sendMsg(reply, response)\n\t\t}\n\t\tmset.mu.Unlock()\n\t\treturn nil\n\t}\n\n\t// If here we will attempt to store the message.\n\t// Assume this will succeed.\n\tolmsgId := mset.lmsgId\n\tmset.lmsgId = msgId\n\tmset.lseq++\n\ttierName := mset.tier\n\n\t// Republish state if needed.\n\tvar tsubj string\n\tvar tlseq uint64\n\tvar thdrsOnly bool\n\tif mset.tr != nil {\n\t\ttsubj, _ = mset.tr.Match(subject)\n\t\tif mset.cfg.RePublish != nil {\n\t\t\tthdrsOnly = mset.cfg.RePublish.HeadersOnly\n\t\t}\n\t}\n\trepublish := tsubj != _EMPTY_ && isLeader\n\n\t// If we are republishing grab last sequence for this exact subject. Aids in gap detection for lightweight clients.\n\tif republish {\n\t\tvar smv StoreMsg\n\t\tif sm, _ := store.LoadLastMsg(subject, &smv); sm != nil {\n\t\t\ttlseq = sm.seq\n\t\t}\n\t}\n\n\t// If clustered this was already checked and we do not want to check here and possibly introduce skew.\n\tif !isClustered {\n\t\tif exceeded, err := jsa.wouldExceedLimits(stype, tierName, mset.cfg.Replicas, subject, hdr, msg); exceeded {\n\t\t\tif err == nil {\n\t\t\t\terr = NewJSAccountResourcesExceededError()\n\t\t\t}\n\t\t\ts.RateLimitWarnf(\"JetStream resource limits exceeded for account: %q\", accName)\n\t\t\tif canRespond {\n\t\t\t\tresp.PubAck = &PubAck{Stream: name}\n\t\t\t\tresp.Error = err\n\t\t\t\tresponse, _ = json.Marshal(resp)\n\t\t\t\tmset.outq.send(newJSPubMsg(reply, _EMPTY_, _EMPTY_, nil, response, nil, 0))\n\t\t\t}\n\t\t\tmset.mu.Unlock()\n\t\t\treturn err\n\t\t}\n\t}\n\n\t// Find the message TTL if any.\n\tttl, err := getMessageTTL(hdr)\n\tif err != nil {\n\t\tif canRespond {\n\t\t\tresp.PubAck = &PubAck{Stream: name}\n\t\t\tresp.Error = NewJSMessageTTLInvalidError()\n\t\t\tresponse, _ = json.Marshal(resp)\n\t\t\tmset.outq.send(newJSPubMsg(reply, _EMPTY_, _EMPTY_, nil, response, nil, 0))\n\t\t}\n\t\tmset.mu.Unlock()\n\t\treturn err\n\t}\n\n\t// If subject delete markers are used, ensure message TTL is that at minimum.\n\t// Otherwise, subject delete markers could be missed if one already exists for this subject.\n\t// MaxMsgsPer=1 is an exception, because we'll only ever have one message.\n\tif ttl > 0 && mset.cfg.SubjectDeleteMarkerTTL > 0 && mset.cfg.MaxMsgsPer != 1 {\n\t\tif minTtl := int64(mset.cfg.SubjectDeleteMarkerTTL.Seconds()); ttl < minTtl {\n\t\t\tttl = minTtl\n\t\t}\n\t}\n\n\t// Store actual msg.\n\tif lseq == 0 && ts == 0 {\n\t\tseq, ts, err = store.StoreMsg(subject, hdr, msg, ttl)\n\t} else {\n\t\t// Make sure to take into account any message assignments that we had to skip (clfs).\n\t\tseq = lseq + 1 - clfs\n\t\t// Check for preAcks and the need to clear it.\n\t\tif mset.hasAllPreAcks(seq, subject) {\n\t\t\tmset.clearAllPreAcks(seq)\n\t\t}\n\t\terr = store.StoreRawMsg(subject, hdr, msg, seq, ts, ttl)\n\t}\n\n\tif err != nil {\n\t\tif isPermissionError(err) {\n\t\t\tmset.mu.Unlock()\n\t\t\t// messages in block cache could be lost in the worst case.\n\t\t\t// In the clustered mode it is very highly unlikely as a result of replication.\n\t\t\tmset.srv.DisableJetStream()\n\t\t\tmset.srv.Warnf(\"Filesystem permission denied while writing msg, disabling JetStream: %v\", err)\n\t\t\treturn err\n\t\t}\n\t\t// If we did not succeed put those values back and increment clfs in case we are clustered.\n\t\tvar state StreamState\n\t\tmset.store.FastState(&state)\n\t\tmset.lseq = state.LastSeq\n\t\tmset.lmsgId = olmsgId\n\t\tmset.mu.Unlock()\n\t\tbumpCLFS()\n\n\t\tswitch err {\n\t\tcase ErrMaxMsgs, ErrMaxBytes, ErrMaxMsgsPerSubject, ErrMsgTooLarge:\n\t\t\ts.RateLimitDebugf(\"JetStream failed to store a msg on stream '%s > %s': %v\", accName, name, err)\n\t\tcase ErrStoreClosed:\n\t\tdefault:\n\t\t\ts.Errorf(\"JetStream failed to store a msg on stream '%s > %s': %v\", accName, name, err)\n\t\t}\n\n\t\tif canRespond {\n\t\t\tresp.PubAck = &PubAck{Stream: name}\n\t\t\tresp.Error = NewJSStreamStoreFailedError(err, Unless(err))\n\t\t\tresponse, _ = json.Marshal(resp)\n\t\t\tmset.outq.sendMsg(reply, response)\n\t\t}\n\t\treturn err\n\t}\n\n\t// If we have a msgId make sure to save.\n\t// This will replace our estimate from the cluster layer if we are clustered.\n\tif msgId != _EMPTY_ {\n\t\tif isClustered && isLeader && mset.ddmap != nil {\n\t\t\tif dde := mset.ddmap[msgId]; dde != nil {\n\t\t\t\tdde.seq, dde.ts = seq, ts\n\t\t\t} else {\n\t\t\t\tmset.storeMsgIdLocked(&ddentry{msgId, seq, ts})\n\t\t\t}\n\t\t} else {\n\t\t\t// R1 or not leader..\n\t\t\tmset.storeMsgIdLocked(&ddentry{msgId, seq, ts})\n\t\t}\n\t}\n\n\t// If here we succeeded in storing the message.\n\tmset.mu.Unlock()\n\n\t// No errors, this is the normal path.\n\tif rollupSub {\n\t\tmset.purge(&JSApiStreamPurgeRequest{Subject: subject, Keep: 1})\n\t} else if rollupAll {\n\t\tmset.purge(&JSApiStreamPurgeRequest{Keep: 1})\n\t}\n\n\t// Check for republish.\n\tif republish {\n\t\tconst ht = \"NATS/1.0\\r\\nNats-Stream: %s\\r\\nNats-Subject: %s\\r\\nNats-Sequence: %d\\r\\nNats-Time-Stamp: %s\\r\\nNats-Last-Sequence: %d\\r\\n\\r\\n\"\n\t\tconst htho = \"NATS/1.0\\r\\nNats-Stream: %s\\r\\nNats-Subject: %s\\r\\nNats-Sequence: %d\\r\\nNats-Time-Stamp: %s\\r\\nNats-Last-Sequence: %d\\r\\nNats-Msg-Size: %d\\r\\n\\r\\n\"\n\t\t// When adding to existing headers, will use the fmt.Append version so this skips the headers from above.\n\t\tconst hoff = 10\n\n\t\ttsStr := time.Unix(0, ts).UTC().Format(time.RFC3339Nano)\n\t\tvar rpMsg []byte\n\t\tif len(hdr) == 0 {\n\t\t\tif !thdrsOnly {\n\t\t\t\thdr = fmt.Appendf(nil, ht, name, subject, seq, tsStr, tlseq)\n\t\t\t\trpMsg = copyBytes(msg)\n\t\t\t} else {\n\t\t\t\thdr = fmt.Appendf(nil, htho, name, subject, seq, tsStr, tlseq, len(msg))\n\t\t\t}\n\t\t} else {\n\t\t\t// use hdr[:end:end] to make sure as we add we copy the original hdr.\n\t\t\tend := len(hdr) - LEN_CR_LF\n\t\t\tif !thdrsOnly {\n\t\t\t\thdr = fmt.Appendf(hdr[:end:end], ht[hoff:], name, subject, seq, tsStr, tlseq)\n\t\t\t\trpMsg = copyBytes(msg)\n\t\t\t} else {\n\t\t\t\thdr = fmt.Appendf(hdr[:end:end], htho[hoff:], name, subject, seq, tsStr, tlseq, len(msg))\n\t\t\t}\n\t\t}\n\t\tmset.outq.send(newJSPubMsg(tsubj, _EMPTY_, _EMPTY_, hdr, rpMsg, nil, seq))\n\t}\n\n\t// Send response here.\n\tif canRespond {\n\t\tresponse = append(pubAck, strconv.FormatUint(seq, 10)...)\n\t\tresponse = append(response, '}')\n\t\tmset.outq.sendMsg(reply, response)\n\t}\n\n\t// Signal consumers for new messages.\n\tif numConsumers > 0 {\n\t\tmset.sigq.push(newCMsg(subject, seq))\n\t\tselect {\n\t\tcase mset.sch <- struct{}{}:\n\t\tdefault:\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// Used to signal inbound message to registered consumers.\ntype cMsg struct {\n\tseq  uint64\n\tsubj string\n}\n\n// Pool to recycle consumer bound msgs.\nvar cMsgPool sync.Pool\n\n// Used to queue up consumer bound msgs for signaling.\nfunc newCMsg(subj string, seq uint64) *cMsg {\n\tvar m *cMsg\n\tcm := cMsgPool.Get()\n\tif cm != nil {\n\t\tm = cm.(*cMsg)\n\t} else {\n\t\tm = new(cMsg)\n\t}\n\tm.subj, m.seq = subj, seq\n\n\treturn m\n}\n\nfunc (m *cMsg) returnToPool() {\n\tif m == nil {\n\t\treturn\n\t}\n\tm.subj, m.seq = _EMPTY_, 0\n\tcMsgPool.Put(m)\n}\n\n// Go routine to signal consumers.\n// Offloaded from stream msg processing.\nfunc (mset *stream) signalConsumersLoop() {\n\tmset.mu.RLock()\n\ts, qch, sch, msgs := mset.srv, mset.qch, mset.sch, mset.sigq\n\tmset.mu.RUnlock()\n\n\tfor {\n\t\tselect {\n\t\tcase <-s.quitCh:\n\t\t\treturn\n\t\tcase <-qch:\n\t\t\treturn\n\t\tcase <-sch:\n\t\t\tcms := msgs.pop()\n\t\t\tfor _, m := range cms {\n\t\t\t\tseq, subj := m.seq, m.subj\n\t\t\t\tm.returnToPool()\n\t\t\t\t// Signal all appropriate consumers.\n\t\t\t\tmset.signalConsumers(subj, seq)\n\t\t\t}\n\t\t\tmsgs.recycle(&cms)\n\t\t}\n\t}\n}\n\n// This will update and signal all consumers that match.\nfunc (mset *stream) signalConsumers(subj string, seq uint64) {\n\tmset.clsMu.RLock()\n\tdefer mset.clsMu.RUnlock()\n\tcsl := mset.csl\n\tif csl == nil {\n\t\treturn\n\t}\n\tcsl.Match(subj, func(o *consumer) {\n\t\to.processStreamSignal(seq)\n\t})\n}\n\n// Internal message for use by jetstream subsystem.\ntype jsPubMsg struct {\n\tdsubj string // Subject to send to, e.g. _INBOX.xxx\n\treply string\n\tStoreMsg\n\to *consumer\n}\n\nvar jsPubMsgPool sync.Pool\n\nfunc newJSPubMsg(dsubj, subj, reply string, hdr, msg []byte, o *consumer, seq uint64) *jsPubMsg {\n\tvar m *jsPubMsg\n\tvar buf []byte\n\tpm := jsPubMsgPool.Get()\n\tif pm != nil {\n\t\tm = pm.(*jsPubMsg)\n\t\tbuf = m.buf[:0]\n\t\tif hdr != nil {\n\t\t\thdr = append(m.hdr[:0], hdr...)\n\t\t}\n\t} else {\n\t\tm = new(jsPubMsg)\n\t}\n\t// When getting something from a pool it is critical that all fields are\n\t// initialized. Doing this way guarantees that if someone adds a field to\n\t// the structure, the compiler will fail the build if this line is not updated.\n\t(*m) = jsPubMsg{dsubj, reply, StoreMsg{subj, hdr, msg, buf, seq, 0}, o}\n\n\treturn m\n}\n\n// Gets a jsPubMsg from the pool.\nfunc getJSPubMsgFromPool() *jsPubMsg {\n\tpm := jsPubMsgPool.Get()\n\tif pm != nil {\n\t\treturn pm.(*jsPubMsg)\n\t}\n\treturn new(jsPubMsg)\n}\n\nfunc (pm *jsPubMsg) returnToPool() {\n\tif pm == nil {\n\t\treturn\n\t}\n\tpm.subj, pm.dsubj, pm.reply, pm.hdr, pm.msg, pm.o = _EMPTY_, _EMPTY_, _EMPTY_, nil, nil, nil\n\tif len(pm.buf) > 0 {\n\t\tpm.buf = pm.buf[:0]\n\t}\n\tif len(pm.hdr) > 0 {\n\t\tpm.hdr = pm.hdr[:0]\n\t}\n\tjsPubMsgPool.Put(pm)\n}\n\nfunc (pm *jsPubMsg) size() int {\n\tif pm == nil {\n\t\treturn 0\n\t}\n\treturn len(pm.dsubj) + len(pm.reply) + len(pm.hdr) + len(pm.msg)\n}\n\n// Queue of *jsPubMsg for sending internal system messages.\ntype jsOutQ struct {\n\t*ipQueue[*jsPubMsg]\n}\n\nfunc (q *jsOutQ) sendMsg(subj string, msg []byte) {\n\tif q != nil {\n\t\tq.send(newJSPubMsg(subj, _EMPTY_, _EMPTY_, nil, msg, nil, 0))\n\t}\n}\n\nfunc (q *jsOutQ) send(msg *jsPubMsg) {\n\tif q == nil || msg == nil {\n\t\treturn\n\t}\n\tq.push(msg)\n}\n\nfunc (q *jsOutQ) unregister() {\n\tif q == nil {\n\t\treturn\n\t}\n\tq.ipQueue.unregister()\n}\n\n// StoredMsg is for raw access to messages in a stream.\ntype StoredMsg struct {\n\tSubject  string    `json:\"subject\"`\n\tSequence uint64    `json:\"seq\"`\n\tHeader   []byte    `json:\"hdrs,omitempty\"`\n\tData     []byte    `json:\"data,omitempty\"`\n\tTime     time.Time `json:\"time\"`\n}\n\n// This is similar to system semantics but did not want to overload the single system sendq,\n// or require system account when doing simple setup with jetstream.\nfunc (mset *stream) setupSendCapabilities() {\n\tmset.mu.Lock()\n\tdefer mset.mu.Unlock()\n\tif mset.outq != nil {\n\t\treturn\n\t}\n\tqname := fmt.Sprintf(\"[ACC:%s] stream '%s' sendQ\", mset.acc.Name, mset.cfg.Name)\n\tmset.outq = &jsOutQ{newIPQueue[*jsPubMsg](mset.srv, qname)}\n\tgo mset.internalLoop()\n}\n\n// Returns the associated account name.\nfunc (mset *stream) accName() string {\n\tif mset == nil {\n\t\treturn _EMPTY_\n\t}\n\tmset.mu.RLock()\n\tacc := mset.acc\n\tmset.mu.RUnlock()\n\treturn acc.Name\n}\n\n// Name returns the stream name.\nfunc (mset *stream) name() string {\n\tif mset == nil {\n\t\treturn _EMPTY_\n\t}\n\tmset.mu.RLock()\n\tdefer mset.mu.RUnlock()\n\treturn mset.cfg.Name\n}\n\nfunc (mset *stream) internalLoop() {\n\tmset.mu.RLock()\n\tsetGoRoutineLabels(pprofLabels{\n\t\t\"account\": mset.acc.Name,\n\t\t\"stream\":  mset.cfg.Name,\n\t})\n\ts := mset.srv\n\tc := s.createInternalJetStreamClient()\n\tc.registerWithAccount(mset.acc)\n\tdefer c.closeConnection(ClientClosed)\n\toutq, qch, msgs, gets := mset.outq, mset.qch, mset.msgs, mset.gets\n\n\t// For the ack msgs queue for interest retention.\n\tvar (\n\t\tamch chan struct{}\n\t\tackq *ipQueue[uint64]\n\t)\n\tif mset.ackq != nil {\n\t\tackq, amch = mset.ackq, mset.ackq.ch\n\t}\n\tmset.mu.RUnlock()\n\n\t// Raw scratch buffer.\n\t// This should be rarely used now so can be smaller.\n\tvar _r [1024]byte\n\n\t// To optimize for not converting a string to a []byte slice.\n\tvar (\n\t\tsubj  [256]byte\n\t\tdsubj [256]byte\n\t\trply  [256]byte\n\t\tszb   [10]byte\n\t\thdb   [10]byte\n\t)\n\n\tfor {\n\t\tselect {\n\t\tcase <-outq.ch:\n\t\t\tpms := outq.pop()\n\t\t\tfor _, pm := range pms {\n\t\t\t\tc.pa.subject = append(dsubj[:0], pm.dsubj...)\n\t\t\t\tc.pa.deliver = append(subj[:0], pm.subj...)\n\t\t\t\tc.pa.size = len(pm.msg) + len(pm.hdr)\n\t\t\t\tc.pa.szb = append(szb[:0], strconv.Itoa(c.pa.size)...)\n\t\t\t\tif len(pm.reply) > 0 {\n\t\t\t\t\tc.pa.reply = append(rply[:0], pm.reply...)\n\t\t\t\t} else {\n\t\t\t\t\tc.pa.reply = nil\n\t\t\t\t}\n\n\t\t\t\t// If we have an underlying buf that is the wire contents for hdr + msg, else construct on the fly.\n\t\t\t\tvar msg []byte\n\t\t\t\tif len(pm.buf) > 0 {\n\t\t\t\t\tmsg = pm.buf\n\t\t\t\t} else {\n\t\t\t\t\tif len(pm.hdr) > 0 {\n\t\t\t\t\t\tmsg = pm.hdr\n\t\t\t\t\t\tif len(pm.msg) > 0 {\n\t\t\t\t\t\t\tmsg = _r[:0]\n\t\t\t\t\t\t\tmsg = append(msg, pm.hdr...)\n\t\t\t\t\t\t\tmsg = append(msg, pm.msg...)\n\t\t\t\t\t\t}\n\t\t\t\t\t} else if len(pm.msg) > 0 {\n\t\t\t\t\t\t// We own this now from a low level buffer perspective so can use directly here.\n\t\t\t\t\t\tmsg = pm.msg\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\tif len(pm.hdr) > 0 {\n\t\t\t\t\tc.pa.hdr = len(pm.hdr)\n\t\t\t\t\tc.pa.hdb = []byte(strconv.Itoa(c.pa.hdr))\n\t\t\t\t\tc.pa.hdb = append(hdb[:0], strconv.Itoa(c.pa.hdr)...)\n\t\t\t\t} else {\n\t\t\t\t\tc.pa.hdr = -1\n\t\t\t\t\tc.pa.hdb = nil\n\t\t\t\t}\n\n\t\t\t\tmsg = append(msg, _CRLF_...)\n\n\t\t\t\tdidDeliver, _ := c.processInboundClientMsg(msg)\n\t\t\t\tc.pa.szb, c.pa.subject, c.pa.deliver = nil, nil, nil\n\n\t\t\t\t// Check to see if this is a delivery for a consumer and\n\t\t\t\t// we failed to deliver the message. If so alert the consumer.\n\t\t\t\tif pm.o != nil && pm.seq > 0 && !didDeliver {\n\t\t\t\t\tpm.o.didNotDeliver(pm.seq, pm.dsubj)\n\t\t\t\t}\n\t\t\t\tpm.returnToPool()\n\t\t\t}\n\t\t\t// TODO: Move in the for-loop?\n\t\t\tc.flushClients(0)\n\t\t\toutq.recycle(&pms)\n\t\tcase <-msgs.ch:\n\t\t\t// This can possibly change now so needs to be checked here.\n\t\t\tisClustered := mset.IsClustered()\n\t\t\tims := msgs.pop()\n\t\t\tfor _, im := range ims {\n\t\t\t\t// If we are clustered we need to propose this message to the underlying raft group.\n\t\t\t\tif isClustered {\n\t\t\t\t\tmset.processClusteredInboundMsg(im.subj, im.rply, im.hdr, im.msg, im.mt, false)\n\t\t\t\t} else {\n\t\t\t\t\tmset.processJetStreamMsg(im.subj, im.rply, im.hdr, im.msg, 0, 0, im.mt, false)\n\t\t\t\t}\n\t\t\t\tim.returnToPool()\n\t\t\t}\n\t\t\tmsgs.recycle(&ims)\n\t\tcase <-gets.ch:\n\t\t\tdgs := gets.pop()\n\t\t\tfor _, dg := range dgs {\n\t\t\t\tmset.getDirectRequest(&dg.req, dg.reply)\n\t\t\t\tdgPool.Put(dg)\n\t\t\t}\n\t\t\tgets.recycle(&dgs)\n\n\t\tcase <-amch:\n\t\t\tseqs := ackq.pop()\n\t\t\tfor _, seq := range seqs {\n\t\t\t\tmset.ackMsg(nil, seq)\n\t\t\t}\n\t\t\tackq.recycle(&seqs)\n\t\tcase <-qch:\n\t\t\treturn\n\t\tcase <-s.quitCh:\n\t\t\treturn\n\t\t}\n\t}\n}\n\n// Used to break consumers out of their monitorConsumer go routines.\nfunc (mset *stream) resetAndWaitOnConsumers() {\n\tmset.mu.RLock()\n\tconsumers := make([]*consumer, 0, len(mset.consumers))\n\tfor _, o := range mset.consumers {\n\t\tconsumers = append(consumers, o)\n\t}\n\tmset.mu.RUnlock()\n\n\tfor _, o := range consumers {\n\t\tif node := o.raftNode(); node != nil {\n\t\t\tnode.StepDown()\n\t\t\tnode.Stop()\n\t\t}\n\t\tif o.isMonitorRunning() {\n\t\t\to.monitorWg.Wait()\n\t\t}\n\t}\n}\n\n// Internal function to delete a stream.\nfunc (mset *stream) delete() error {\n\tif mset == nil {\n\t\treturn nil\n\t}\n\treturn mset.stop(true, true)\n}\n\n// Internal function to stop or delete the stream.\nfunc (mset *stream) stop(deleteFlag, advisory bool) error {\n\tmset.mu.RLock()\n\tjs, jsa, name := mset.js, mset.jsa, mset.cfg.Name\n\tmset.mu.RUnlock()\n\n\tif jsa == nil {\n\t\treturn NewJSNotEnabledForAccountError()\n\t}\n\n\t// Remove from our account map first.\n\tjsa.mu.Lock()\n\tdelete(jsa.streams, name)\n\taccName := jsa.account.Name\n\tjsa.mu.Unlock()\n\n\t// Kick monitor and collect consumers first.\n\tmset.mu.Lock()\n\n\t// Mark closed.\n\tmset.closed.Store(true)\n\n\t// Signal to the monitor loop.\n\t// Can't use qch here.\n\tif mset.mqch != nil {\n\t\tclose(mset.mqch)\n\t\tmset.mqch = nil\n\t}\n\n\t// Stop responding to sync requests.\n\tmset.stopClusterSubs()\n\t// Unsubscribe from direct stream.\n\tmset.unsubscribeToStream(true)\n\n\t// Our info sub if we spun it up.\n\tif mset.infoSub != nil {\n\t\tmset.srv.sysUnsubscribe(mset.infoSub)\n\t\tmset.infoSub = nil\n\t}\n\n\t// Clean up consumers.\n\tvar obs []*consumer\n\tfor _, o := range mset.consumers {\n\t\tobs = append(obs, o)\n\t}\n\tmset.clsMu.Lock()\n\tmset.consumers, mset.cList, mset.csl = nil, nil, nil\n\tmset.clsMu.Unlock()\n\n\t// Check if we are a mirror.\n\tif mset.mirror != nil && mset.mirror.sub != nil {\n\t\tmset.unsubscribe(mset.mirror.sub)\n\t\tmset.mirror.sub = nil\n\t\tmset.removeInternalConsumer(mset.mirror)\n\t}\n\t// Now check for sources.\n\tif len(mset.sources) > 0 {\n\t\tfor _, si := range mset.sources {\n\t\t\tmset.cancelSourceConsumer(si.iname)\n\t\t}\n\t}\n\tmset.mu.Unlock()\n\n\tisShuttingDown := js.isShuttingDown()\n\tfor _, o := range obs {\n\t\tif !o.isClosed() {\n\t\t\t// Third flag says do not broadcast a signal.\n\t\t\t// TODO(dlc) - If we have an err here we don't want to stop\n\t\t\t// but should we log?\n\t\t\to.stopWithFlags(deleteFlag, deleteFlag, false, advisory)\n\t\t\tif !isShuttingDown {\n\t\t\t\to.monitorWg.Wait()\n\t\t\t}\n\t\t}\n\t}\n\n\tmset.mu.Lock()\n\t// Send stream delete advisory after the consumers.\n\tif deleteFlag && advisory {\n\t\tmset.sendDeleteAdvisoryLocked()\n\t}\n\n\t// Quit channel, do this after sending the delete advisory\n\tif mset.qch != nil {\n\t\tclose(mset.qch)\n\t\tmset.qch = nil\n\t}\n\n\t// Cluster cleanup\n\tvar sa *streamAssignment\n\tif n := mset.node; n != nil {\n\t\tif deleteFlag {\n\t\t\tn.Delete()\n\t\t\tsa = mset.sa\n\t\t} else if !isShuttingDown {\n\t\t\t// Stop Raft, unless JetStream is already shutting down, in which case they'll be stopped separately.\n\t\t\tn.Stop()\n\t\t}\n\t}\n\n\t// Cleanup duplicate timer if running.\n\tif mset.ddtmr != nil {\n\t\tmset.ddtmr.Stop()\n\t\tmset.ddtmr = nil\n\t\tmset.ddmap = nil\n\t\tmset.ddarr = nil\n\t\tmset.ddindex = 0\n\t}\n\n\tsysc := mset.sysc\n\tmset.sysc = nil\n\n\tif deleteFlag {\n\t\t// Unregistering ipQueues do not prevent them from push/pop\n\t\t// just will remove them from the central monitoring map\n\t\tmset.msgs.unregister()\n\t\tmset.ackq.unregister()\n\t\tmset.outq.unregister()\n\t\tmset.sigq.unregister()\n\t\tmset.smsgs.unregister()\n\t}\n\n\t// Snapshot store.\n\tstore := mset.store\n\tc := mset.client\n\n\t// Clustered cleanup.\n\tmset.mu.Unlock()\n\n\t// Check if the stream assignment has the group node specified.\n\t// We need this cleared for if the stream gets reassigned here.\n\tif sa != nil {\n\t\tjs.mu.Lock()\n\t\tif sa.Group != nil {\n\t\t\tsa.Group.node = nil\n\t\t}\n\t\tjs.mu.Unlock()\n\t}\n\n\tif c != nil {\n\t\tc.closeConnection(ClientClosed)\n\t}\n\n\tif sysc != nil {\n\t\tsysc.closeConnection(ClientClosed)\n\t}\n\n\tif deleteFlag {\n\t\tif store != nil {\n\t\t\t// Ignore errors.\n\t\t\tstore.Delete()\n\t\t}\n\t\t// Release any resources.\n\t\tjs.releaseStreamResources(&mset.cfg)\n\t\t// cleanup directories after the stream\n\t\taccDir := filepath.Join(js.config.StoreDir, accName)\n\t\t// Do cleanup in separate go routine similar to how fs will use purge here..\n\t\tgo func() {\n\t\t\t// no op if not empty\n\t\t\tos.Remove(filepath.Join(accDir, streamsDir))\n\t\t\tos.Remove(accDir)\n\t\t}()\n\t} else if store != nil {\n\t\t// Ignore errors.\n\t\tstore.Stop()\n\t}\n\n\treturn nil\n}\n\nfunc (mset *stream) getMsg(seq uint64) (*StoredMsg, error) {\n\tvar smv StoreMsg\n\tsm, err := mset.store.LoadMsg(seq, &smv)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\t// This only used in tests directly so no need to pool etc.\n\treturn &StoredMsg{\n\t\tSubject:  sm.subj,\n\t\tSequence: sm.seq,\n\t\tHeader:   sm.hdr,\n\t\tData:     sm.msg,\n\t\tTime:     time.Unix(0, sm.ts).UTC(),\n\t}, nil\n}\n\n// getConsumers will return a copy of all the current consumers for this stream.\nfunc (mset *stream) getConsumers() []*consumer {\n\tmset.clsMu.RLock()\n\tdefer mset.clsMu.RUnlock()\n\treturn append([]*consumer(nil), mset.cList...)\n}\n\n// Lock should be held for this one.\nfunc (mset *stream) numPublicConsumers() int {\n\treturn len(mset.consumers) - mset.directs\n}\n\n// This returns all consumers that are not DIRECT.\nfunc (mset *stream) getPublicConsumers() []*consumer {\n\tmset.clsMu.RLock()\n\tdefer mset.clsMu.RUnlock()\n\n\tvar obs []*consumer\n\tfor _, o := range mset.cList {\n\t\tif !o.cfg.Direct {\n\t\t\tobs = append(obs, o)\n\t\t}\n\t}\n\treturn obs\n}\n\n// 2 minutes plus up to 30s jitter.\nconst (\n\tdefaultCheckInterestStateT = 2 * time.Minute\n\tdefaultCheckInterestStateJ = 30\n)\n\nvar (\n\tcheckInterestStateT = defaultCheckInterestStateT // Interval\n\tcheckInterestStateJ = defaultCheckInterestStateJ // Jitter (secs)\n)\n\n// Will check for interest retention and make sure messages\n// that have been acked are processed and removed.\n// This will check the ack floors of all consumers, and adjust our first sequence accordingly.\nfunc (mset *stream) checkInterestState() {\n\tif mset == nil || !mset.isInterestRetention() {\n\t\t// If we are limits based nothing to do.\n\t\treturn\n\t}\n\n\tvar ss StreamState\n\tmset.store.FastState(&ss)\n\n\tfor _, o := range mset.getConsumers() {\n\t\to.checkStateForInterestStream(&ss)\n\t}\n}\n\nfunc (mset *stream) isInterestRetention() bool {\n\tmset.mu.RLock()\n\tdefer mset.mu.RUnlock()\n\treturn mset.cfg.Retention != LimitsPolicy\n}\n\n// NumConsumers reports on number of active consumers for this stream.\nfunc (mset *stream) numConsumers() int {\n\tmset.mu.RLock()\n\tdefer mset.mu.RUnlock()\n\treturn len(mset.consumers)\n}\n\n// Lock should be held.\nfunc (mset *stream) setConsumer(o *consumer) {\n\tmset.consumers[o.name] = o\n\tif len(o.subjf) > 0 {\n\t\tmset.numFilter++\n\t}\n\tif o.cfg.Direct {\n\t\tmset.directs++\n\t}\n\t// Now update consumers list as well\n\tmset.clsMu.Lock()\n\tmset.cList = append(mset.cList, o)\n\tif mset.csl == nil {\n\t\tmset.csl = gsl.NewSublist[*consumer]()\n\t}\n\tfor _, sub := range o.signalSubs() {\n\t\tmset.csl.Insert(sub, o)\n\t}\n\tmset.clsMu.Unlock()\n}\n\n// Lock should be held.\nfunc (mset *stream) removeConsumer(o *consumer) {\n\tif o.cfg.FilterSubject != _EMPTY_ && mset.numFilter > 0 {\n\t\tmset.numFilter--\n\t}\n\tif o.cfg.Direct && mset.directs > 0 {\n\t\tmset.directs--\n\t}\n\tif mset.consumers != nil {\n\t\tdelete(mset.consumers, o.name)\n\t\t// Now update consumers list as well\n\t\tmset.clsMu.Lock()\n\t\tfor i, ol := range mset.cList {\n\t\t\tif ol == o {\n\t\t\t\tmset.cList = append(mset.cList[:i], mset.cList[i+1:]...)\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\t// Always remove from the leader sublist.\n\t\tif mset.csl != nil {\n\t\t\tfor _, sub := range o.signalSubs() {\n\t\t\t\tmset.csl.Remove(sub, o)\n\t\t\t}\n\t\t}\n\t\tmset.clsMu.Unlock()\n\t}\n}\n\n// swapSigSubs will update signal Subs for a new subject filter.\n// consumer lock should not be held.\nfunc (mset *stream) swapSigSubs(o *consumer, newFilters []string) {\n\tmset.clsMu.Lock()\n\to.mu.Lock()\n\n\tif o.closed || o.mset == nil {\n\t\to.mu.Unlock()\n\t\tmset.clsMu.Unlock()\n\t\treturn\n\t}\n\n\tif o.sigSubs != nil {\n\t\tif mset.csl != nil {\n\t\t\tfor _, sub := range o.sigSubs {\n\t\t\t\tmset.csl.Remove(sub, o)\n\t\t\t}\n\t\t}\n\t\to.sigSubs = nil\n\t}\n\n\tif o.isLeader() {\n\t\tif mset.csl == nil {\n\t\t\tmset.csl = gsl.NewSublist[*consumer]()\n\t\t}\n\t\t// If no filters are preset, add fwcs to sublist for that consumer.\n\t\tif newFilters == nil {\n\t\t\tmset.csl.Insert(fwcs, o)\n\t\t\to.sigSubs = append(o.sigSubs, fwcs)\n\t\t\t// If there are filters, add their subjects to sublist.\n\t\t} else {\n\t\t\tfor _, filter := range newFilters {\n\t\t\t\tmset.csl.Insert(filter, o)\n\t\t\t\to.sigSubs = append(o.sigSubs, filter)\n\t\t\t}\n\t\t}\n\t}\n\to.mu.Unlock()\n\tmset.clsMu.Unlock()\n\n\tmset.mu.Lock()\n\tdefer mset.mu.Unlock()\n\n\tif mset.numFilter > 0 && len(o.subjf) > 0 {\n\t\tmset.numFilter--\n\t}\n\tif len(newFilters) > 0 {\n\t\tmset.numFilter++\n\t}\n}\n\n// lookupConsumer will retrieve a consumer by name.\nfunc (mset *stream) lookupConsumer(name string) *consumer {\n\tmset.mu.RLock()\n\tdefer mset.mu.RUnlock()\n\treturn mset.consumers[name]\n}\n\nfunc (mset *stream) numDirectConsumers() (num int) {\n\tmset.clsMu.RLock()\n\tdefer mset.clsMu.RUnlock()\n\n\t// Consumers that are direct are not recorded at the store level.\n\tfor _, o := range mset.cList {\n\t\to.mu.RLock()\n\t\tif o.cfg.Direct {\n\t\t\tnum++\n\t\t}\n\t\to.mu.RUnlock()\n\t}\n\treturn num\n}\n\n// State will return the current state for this stream.\nfunc (mset *stream) state() StreamState {\n\treturn mset.stateWithDetail(false)\n}\n\nfunc (mset *stream) stateWithDetail(details bool) StreamState {\n\t// mset.store does not change once set, so ok to reference here directly.\n\t// We do this elsewhere as well.\n\tstore := mset.store\n\tif store == nil {\n\t\treturn StreamState{}\n\t}\n\n\t// Currently rely on store for details.\n\tif details {\n\t\treturn store.State()\n\t}\n\t// Here we do the fast version.\n\tvar state StreamState\n\tstore.FastState(&state)\n\treturn state\n}\n\nfunc (mset *stream) Store() StreamStore {\n\tmset.mu.RLock()\n\tdefer mset.mu.RUnlock()\n\treturn mset.store\n}\n\n// Determines if the new proposed partition is unique amongst all consumers.\n// Lock should be held.\nfunc (mset *stream) partitionUnique(name string, partitions []string) bool {\n\tfor _, partition := range partitions {\n\t\tfor n, o := range mset.consumers {\n\t\t\t// Skip the consumer being checked.\n\t\t\tif n == name {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif o.subjf == nil {\n\t\t\t\treturn false\n\t\t\t}\n\t\t\tfor _, filter := range o.subjf {\n\t\t\t\tif SubjectsCollide(partition, filter.subject) {\n\t\t\t\t\treturn false\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn true\n}\n\n// Lock should be held.\nfunc (mset *stream) potentialFilteredConsumers() bool {\n\tnumSubjects := len(mset.cfg.Subjects)\n\tif len(mset.consumers) == 0 || numSubjects == 0 {\n\t\treturn false\n\t}\n\tif numSubjects > 1 || subjectHasWildcard(mset.cfg.Subjects[0]) {\n\t\treturn true\n\t}\n\treturn false\n}\n\n// Check if there is no interest in this sequence number across our consumers.\n// The consumer passed is optional if we are processing the ack for that consumer.\n// Write lock should be held.\nfunc (mset *stream) noInterest(seq uint64, obs *consumer) bool {\n\treturn !mset.checkForInterest(seq, obs)\n}\n\n// Check if there is no interest in this sequence number and subject across our consumers.\n// The consumer passed is optional if we are processing the ack for that consumer.\n// Write lock should be held.\nfunc (mset *stream) noInterestWithSubject(seq uint64, subj string, obs *consumer) bool {\n\treturn !mset.checkForInterestWithSubject(seq, subj, obs)\n}\n\n// Write lock should be held here for the stream to avoid race conditions on state.\nfunc (mset *stream) checkForInterest(seq uint64, obs *consumer) bool {\n\tvar subj string\n\tif mset.potentialFilteredConsumers() {\n\t\tpmsg := getJSPubMsgFromPool()\n\t\tdefer pmsg.returnToPool()\n\t\tsm, err := mset.store.LoadMsg(seq, &pmsg.StoreMsg)\n\t\tif err != nil {\n\t\t\tif err == ErrStoreEOF {\n\t\t\t\t// Register this as a preAck.\n\t\t\t\tmset.registerPreAck(obs, seq)\n\t\t\t\treturn true\n\t\t\t}\n\t\t\tmset.clearAllPreAcks(seq)\n\t\t\treturn false\n\t\t}\n\t\tsubj = sm.subj\n\t}\n\treturn mset.checkForInterestWithSubject(seq, subj, obs)\n}\n\n// Checks for interest given a sequence and subject.\nfunc (mset *stream) checkForInterestWithSubject(seq uint64, subj string, obs *consumer) bool {\n\tfor _, o := range mset.consumers {\n\t\t// If this is us or we have a registered preAck for this consumer continue inspecting.\n\t\tif o == obs || mset.hasPreAck(o, seq) {\n\t\t\tcontinue\n\t\t}\n\t\t// Check if we need an ack.\n\t\tif o.needAck(seq, subj) {\n\t\t\treturn true\n\t\t}\n\t}\n\tmset.clearAllPreAcks(seq)\n\treturn false\n}\n\n// Check if we have a pre-registered ack for this sequence.\n// Write lock should be held.\nfunc (mset *stream) hasPreAck(o *consumer, seq uint64) bool {\n\tif o == nil || len(mset.preAcks) == 0 {\n\t\treturn false\n\t}\n\tconsumers := mset.preAcks[seq]\n\tif len(consumers) == 0 {\n\t\treturn false\n\t}\n\t_, found := consumers[o]\n\treturn found\n}\n\n// Check if we have all consumers pre-acked for this sequence and subject.\n// Write lock should be held.\nfunc (mset *stream) hasAllPreAcks(seq uint64, subj string) bool {\n\tif len(mset.preAcks) == 0 || len(mset.preAcks[seq]) == 0 {\n\t\treturn false\n\t}\n\t// Since these can be filtered and mutually exclusive,\n\t// if we have some preAcks we need to check all interest here.\n\treturn mset.noInterestWithSubject(seq, subj, nil)\n}\n\n// Check if we have all consumers pre-acked.\n// Write lock should be held.\nfunc (mset *stream) clearAllPreAcks(seq uint64) {\n\tdelete(mset.preAcks, seq)\n}\n\n// Clear all preAcks below floor.\n// Write lock should be held.\nfunc (mset *stream) clearAllPreAcksBelowFloor(floor uint64) {\n\tfor seq := range mset.preAcks {\n\t\tif seq < floor {\n\t\t\tdelete(mset.preAcks, seq)\n\t\t}\n\t}\n}\n\n// This will register an ack for a consumer if it arrives before the actual message.\nfunc (mset *stream) registerPreAckLock(o *consumer, seq uint64) {\n\tmset.mu.Lock()\n\tdefer mset.mu.Unlock()\n\tmset.registerPreAck(o, seq)\n}\n\n// This will register an ack for a consumer if it arrives before\n// the actual message.\n// Write lock should be held.\nfunc (mset *stream) registerPreAck(o *consumer, seq uint64) {\n\tif o == nil {\n\t\treturn\n\t}\n\tif mset.preAcks == nil {\n\t\tmset.preAcks = make(map[uint64]map[*consumer]struct{})\n\t}\n\tif mset.preAcks[seq] == nil {\n\t\tmset.preAcks[seq] = make(map[*consumer]struct{})\n\t}\n\tmset.preAcks[seq][o] = struct{}{}\n}\n\n// This will clear an ack for a consumer.\n// Write lock should be held.\nfunc (mset *stream) clearPreAck(o *consumer, seq uint64) {\n\tif o == nil || len(mset.preAcks) == 0 {\n\t\treturn\n\t}\n\tif consumers := mset.preAcks[seq]; len(consumers) > 0 {\n\t\tdelete(consumers, o)\n\t\tif len(consumers) == 0 {\n\t\t\tdelete(mset.preAcks, seq)\n\t\t}\n\t}\n}\n\n// ackMsg is called into from a consumer when we have a WorkQueue or Interest Retention Policy.\n// Returns whether the message at seq was removed as a result of the ACK.\n// (Or should be removed in the case of clustered streams, since it requires a message delete proposal)\nfunc (mset *stream) ackMsg(o *consumer, seq uint64) bool {\n\tif seq == 0 {\n\t\treturn false\n\t}\n\n\t// Don't make this RLock(). We need to have only 1 running at a time to gauge interest across all consumers.\n\tmset.mu.Lock()\n\tif mset.closed.Load() || mset.cfg.Retention == LimitsPolicy {\n\t\tmset.mu.Unlock()\n\t\treturn false\n\t}\n\n\tstore := mset.store\n\tvar state StreamState\n\tstore.FastState(&state)\n\n\t// If this has arrived before we have processed the message itself.\n\tif seq > state.LastSeq {\n\t\tmset.registerPreAck(o, seq)\n\t\tmset.mu.Unlock()\n\t\t// We have not removed the message, but should still signal so we could retry later\n\t\t// since we potentially need to remove it then.\n\t\treturn true\n\t}\n\n\t// Always clear pre-ack if here.\n\tmset.clearPreAck(o, seq)\n\n\t// Make sure this sequence is not below our first sequence.\n\tif seq < state.FirstSeq {\n\t\tmset.mu.Unlock()\n\t\treturn false\n\t}\n\n\tvar shouldRemove bool\n\tswitch mset.cfg.Retention {\n\tcase WorkQueuePolicy:\n\t\t// Normally we just remove a message when its ack'd here but if we have direct consumers\n\t\t// from sources and/or mirrors we need to make sure they have delivered the msg.\n\t\tshouldRemove = mset.directs <= 0 || mset.noInterest(seq, o)\n\tcase InterestPolicy:\n\t\tshouldRemove = mset.noInterest(seq, o)\n\t}\n\n\t// If nothing else to do.\n\tif !shouldRemove {\n\t\tmset.mu.Unlock()\n\t\treturn false\n\t}\n\n\tif !mset.isClustered() {\n\t\tmset.mu.Unlock()\n\t\t// If we are here we should attempt to remove.\n\t\tif _, err := store.RemoveMsg(seq); err == ErrStoreEOF {\n\t\t\t// This should not happen, but being pedantic.\n\t\t\tmset.registerPreAckLock(o, seq)\n\t\t}\n\t\treturn true\n\t}\n\n\t// Only propose message deletion to the stream if we're consumer leader, otherwise all followers would also propose.\n\t// We must be the consumer leader, since we know for sure we've stored the message and don't register as pre-ack.\n\tif o != nil && !o.IsLeader() {\n\t\tmset.mu.Unlock()\n\t\t// Must still mark as removal if follower. If we become leader later, we must be able to retry the proposal.\n\t\treturn true\n\t}\n\n\tmd := streamMsgDelete{Seq: seq, NoErase: true, Stream: mset.cfg.Name}\n\tmset.node.ForwardProposal(encodeMsgDelete(&md))\n\tmset.mu.Unlock()\n\treturn true\n}\n\n// Snapshot creates a snapshot for the stream and possibly consumers.\nfunc (mset *stream) snapshot(deadline time.Duration, checkMsgs, includeConsumers bool) (*SnapshotResult, error) {\n\tif mset.closed.Load() {\n\t\treturn nil, errStreamClosed\n\t}\n\tstore := mset.store\n\treturn store.Snapshot(deadline, checkMsgs, includeConsumers)\n}\n\nconst snapsDir = \"__snapshots__\"\n\n// RestoreStream will restore a stream from a snapshot.\nfunc (a *Account) RestoreStream(ncfg *StreamConfig, r io.Reader) (*stream, error) {\n\tif ncfg == nil {\n\t\treturn nil, errors.New(\"nil config on stream restore\")\n\t}\n\n\ts, jsa, err := a.checkForJetStream()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tjs := jsa.js\n\tif js == nil {\n\t\treturn nil, NewJSNotEnabledForAccountError()\n\t}\n\n\tcfg, apiErr := s.checkStreamCfg(ncfg, a, false)\n\tif apiErr != nil {\n\t\treturn nil, apiErr\n\t}\n\n\tsd := filepath.Join(jsa.storeDir, snapsDir)\n\tif _, err := os.Stat(sd); os.IsNotExist(err) {\n\t\tif err := os.MkdirAll(sd, defaultDirPerms); err != nil {\n\t\t\treturn nil, fmt.Errorf(\"could not create snapshots directory - %v\", err)\n\t\t}\n\t}\n\tsdir, err := os.MkdirTemp(sd, \"snap-\")\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif _, err := os.Stat(sdir); os.IsNotExist(err) {\n\t\tif err := os.MkdirAll(sdir, defaultDirPerms); err != nil {\n\t\t\treturn nil, fmt.Errorf(\"could not create snapshots directory - %v\", err)\n\t\t}\n\t}\n\tdefer os.RemoveAll(sdir)\n\n\tlogAndReturnError := func() error {\n\t\ta.mu.RLock()\n\t\terr := fmt.Errorf(\"unexpected content (account=%s)\", a.Name)\n\t\tif a.srv != nil {\n\t\t\ta.srv.Errorf(\"Stream restore failed due to %v\", err)\n\t\t}\n\t\ta.mu.RUnlock()\n\t\treturn err\n\t}\n\tsdirCheck := filepath.Clean(sdir) + string(os.PathSeparator)\n\n\t_, isClustered := jsa.jetStreamAndClustered()\n\tjsa.usageMu.RLock()\n\tselected, tier, hasTier := jsa.selectLimits(cfg.Replicas)\n\tjsa.usageMu.RUnlock()\n\treserved := int64(0)\n\tif hasTier {\n\t\tif isClustered {\n\t\t\tjs.mu.RLock()\n\t\t\t_, reserved = tieredStreamAndReservationCount(js.cluster.streams[a.Name], tier, &cfg)\n\t\t\tjs.mu.RUnlock()\n\t\t} else {\n\t\t\treserved = jsa.tieredReservation(tier, &cfg)\n\t\t}\n\t}\n\n\tvar bc int64\n\ttr := tar.NewReader(s2.NewReader(r))\n\tfor {\n\t\thdr, err := tr.Next()\n\t\tif err == io.EOF {\n\t\t\tbreak // End of snapshot\n\t\t}\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tif hdr.Typeflag != tar.TypeReg {\n\t\t\treturn nil, logAndReturnError()\n\t\t}\n\t\tbc += hdr.Size\n\t\tjs.mu.RLock()\n\t\terr = js.checkAllLimits(&selected, &cfg, reserved, bc)\n\t\tjs.mu.RUnlock()\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tfpath := filepath.Join(sdir, filepath.Clean(hdr.Name))\n\t\tif !strings.HasPrefix(fpath, sdirCheck) {\n\t\t\treturn nil, logAndReturnError()\n\t\t}\n\t\tos.MkdirAll(filepath.Dir(fpath), defaultDirPerms)\n\t\tfd, err := os.OpenFile(fpath, os.O_CREATE|os.O_RDWR, 0600)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\t_, err = io.Copy(fd, tr)\n\t\tfd.Close()\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\t// Check metadata.\n\t// The cfg passed in will be the new identity for the stream.\n\tvar fcfg FileStreamInfo\n\tb, err := os.ReadFile(filepath.Join(sdir, JetStreamMetaFile))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif err := json.Unmarshal(b, &fcfg); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Check to make sure names match.\n\tif fcfg.Name != cfg.Name {\n\t\treturn nil, errors.New(\"stream names do not match\")\n\t}\n\n\t// See if this stream already exists.\n\tif _, err := a.lookupStream(cfg.Name); err == nil {\n\t\treturn nil, NewJSStreamNameExistRestoreFailedError()\n\t}\n\t// Move into the correct place here.\n\tndir := filepath.Join(jsa.storeDir, streamsDir, cfg.Name)\n\t// Remove old one if for some reason it is still here.\n\tif _, err := os.Stat(ndir); err == nil {\n\t\tos.RemoveAll(ndir)\n\t}\n\t// Make sure our destination streams directory exists.\n\tif err := os.MkdirAll(filepath.Join(jsa.storeDir, streamsDir), defaultDirPerms); err != nil {\n\t\treturn nil, err\n\t}\n\t// Move into new location.\n\tif err := os.Rename(sdir, ndir); err != nil {\n\t\treturn nil, err\n\t}\n\n\tif cfg.Template != _EMPTY_ {\n\t\tif err := jsa.addStreamNameToTemplate(cfg.Template, cfg.Name); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\tmset, err := a.addStream(&cfg)\n\tif err != nil {\n\t\t// Make sure to clean up after ourselves here.\n\t\tos.RemoveAll(ndir)\n\t\treturn nil, err\n\t}\n\tif !fcfg.Created.IsZero() {\n\t\tmset.setCreatedTime(fcfg.Created)\n\t}\n\n\t// Make sure we do an update if the configs have changed.\n\tif !reflect.DeepEqual(fcfg.StreamConfig, cfg) {\n\t\tif err := mset.update(&cfg); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\t// Now do consumers.\n\todir := filepath.Join(ndir, consumerDir)\n\tofis, _ := os.ReadDir(odir)\n\tfor _, ofi := range ofis {\n\t\tmetafile := filepath.Join(odir, ofi.Name(), JetStreamMetaFile)\n\t\tmetasum := filepath.Join(odir, ofi.Name(), JetStreamMetaFileSum)\n\t\tif _, err := os.Stat(metafile); os.IsNotExist(err) {\n\t\t\tmset.stop(true, false)\n\t\t\treturn nil, fmt.Errorf(\"error restoring consumer [%q]: %v\", ofi.Name(), err)\n\t\t}\n\t\tbuf, err := os.ReadFile(metafile)\n\t\tif err != nil {\n\t\t\tmset.stop(true, false)\n\t\t\treturn nil, fmt.Errorf(\"error restoring consumer [%q]: %v\", ofi.Name(), err)\n\t\t}\n\t\tif _, err := os.Stat(metasum); os.IsNotExist(err) {\n\t\t\tmset.stop(true, false)\n\t\t\treturn nil, fmt.Errorf(\"error restoring consumer [%q]: %v\", ofi.Name(), err)\n\t\t}\n\t\tvar cfg FileConsumerInfo\n\t\tif err := json.Unmarshal(buf, &cfg); err != nil {\n\t\t\tmset.stop(true, false)\n\t\t\treturn nil, fmt.Errorf(\"error restoring consumer [%q]: %v\", ofi.Name(), err)\n\t\t}\n\t\tisEphemeral := !isDurableConsumer(&cfg.ConsumerConfig)\n\t\tif isEphemeral {\n\t\t\t// This is an ephermal consumer and this could fail on restart until\n\t\t\t// the consumer can reconnect. We will create it as a durable and switch it.\n\t\t\tcfg.ConsumerConfig.Durable = ofi.Name()\n\t\t}\n\t\tobs, err := mset.addConsumer(&cfg.ConsumerConfig)\n\t\tif err != nil {\n\t\t\tmset.stop(true, false)\n\t\t\treturn nil, fmt.Errorf(\"error restoring consumer [%q]: %v\", ofi.Name(), err)\n\t\t}\n\t\tif isEphemeral {\n\t\t\tobs.switchToEphemeral()\n\t\t}\n\t\tif !cfg.Created.IsZero() {\n\t\t\tobs.setCreatedTime(cfg.Created)\n\t\t}\n\t\tobs.mu.Lock()\n\t\terr = obs.readStoredState()\n\t\tobs.mu.Unlock()\n\t\tif err != nil {\n\t\t\tmset.stop(true, false)\n\t\t\treturn nil, fmt.Errorf(\"error restoring consumer [%q]: %v\", ofi.Name(), err)\n\t\t}\n\t}\n\treturn mset, nil\n}\n\n// This is to check for dangling messages on interest retention streams. Only called on account enable.\n// Issue https://github.com/nats-io/nats-server/issues/3612\nfunc (mset *stream) checkForOrphanMsgs() {\n\tmset.mu.RLock()\n\tconsumers := make([]*consumer, 0, len(mset.consumers))\n\tfor _, o := range mset.consumers {\n\t\tconsumers = append(consumers, o)\n\t}\n\taccName, stream := mset.acc.Name, mset.cfg.Name\n\n\tvar ss StreamState\n\tmset.store.FastState(&ss)\n\tmset.mu.RUnlock()\n\n\tfor _, o := range consumers {\n\t\tif err := o.checkStateForInterestStream(&ss); err == errAckFloorHigherThanLastSeq {\n\t\t\to.mu.RLock()\n\t\t\ts, consumer := o.srv, o.name\n\t\t\tstate, _ := o.store.State()\n\t\t\tasflr := state.AckFloor.Stream\n\t\t\to.mu.RUnlock()\n\t\t\t// Warn about stream state vs our ack floor.\n\t\t\ts.RateLimitWarnf(\"Detected consumer '%s > %s > %s' ack floor %d is ahead of stream's last sequence %d\",\n\t\t\t\taccName, stream, consumer, asflr, ss.LastSeq)\n\t\t}\n\t}\n}\n\n// Check on startup to make sure that consumers replication matches us.\n// Interest retention requires replication matches.\nfunc (mset *stream) checkConsumerReplication() {\n\tmset.mu.RLock()\n\tdefer mset.mu.RUnlock()\n\n\tif mset.cfg.Retention != InterestPolicy {\n\t\treturn\n\t}\n\n\ts, acc := mset.srv, mset.acc\n\tfor _, o := range mset.consumers {\n\t\to.mu.RLock()\n\t\t// Consumer replicas 0 can be a legit config for the replicas and we will inherit from the stream\n\t\t// when this is the case.\n\t\tif mset.cfg.Replicas != o.cfg.Replicas && o.cfg.Replicas != 0 {\n\t\t\ts.Errorf(\"consumer '%s > %s > %s' MUST match replication (%d vs %d) of stream with interest policy\",\n\t\t\t\tacc, mset.cfg.Name, o.cfg.Name, mset.cfg.Replicas, o.cfg.Replicas)\n\t\t}\n\t\to.mu.RUnlock()\n\t}\n}\n\n// Will check if we are running in the monitor already and if not set the appropriate flag.\nfunc (mset *stream) checkInMonitor() bool {\n\tmset.mu.Lock()\n\tdefer mset.mu.Unlock()\n\n\tif mset.inMonitor {\n\t\treturn true\n\t}\n\tmset.inMonitor = true\n\treturn false\n}\n\n// Clear us being in the monitor routine.\nfunc (mset *stream) clearMonitorRunning() {\n\tmset.mu.Lock()\n\tdefer mset.mu.Unlock()\n\tmset.inMonitor = false\n}\n\n// Check if our monitor is running.\nfunc (mset *stream) isMonitorRunning() bool {\n\tmset.mu.RLock()\n\tdefer mset.mu.RUnlock()\n\treturn mset.inMonitor\n}\n",
    "source_file": "server/stream.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2012-2025 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage server\n\nimport (\n\t\"bytes\"\n\t\"container/heap\"\n\t\"container/list\"\n\t\"crypto/sha256\"\n\t\"errors\"\n\t\"fmt\"\n\t\"math\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/nats-io/nkeys\"\n\n\t\"github.com/nats-io/jwt/v2\" // only used to decode, not for storage\n)\n\nconst (\n\tfileExtension = \".jwt\"\n)\n\n// validatePathExists checks that the provided path exists and is a dir if requested\nfunc validatePathExists(path string, dir bool) (string, error) {\n\tif path == _EMPTY_ {\n\t\treturn _EMPTY_, errors.New(\"path is not specified\")\n\t}\n\n\tabs, err := filepath.Abs(path)\n\tif err != nil {\n\t\treturn _EMPTY_, fmt.Errorf(\"error parsing path [%s]: %v\", abs, err)\n\t}\n\n\tvar finfo os.FileInfo\n\tif finfo, err = os.Stat(abs); os.IsNotExist(err) {\n\t\treturn _EMPTY_, fmt.Errorf(\"the path [%s] doesn't exist\", abs)\n\t}\n\n\tmode := finfo.Mode()\n\tif dir && mode.IsRegular() {\n\t\treturn _EMPTY_, fmt.Errorf(\"the path [%s] is not a directory\", abs)\n\t}\n\n\tif !dir && mode.IsDir() {\n\t\treturn _EMPTY_, fmt.Errorf(\"the path [%s] is not a file\", abs)\n\t}\n\n\treturn abs, nil\n}\n\n// ValidateDirPath checks that the provided path exists and is a dir\nfunc validateDirPath(path string) (string, error) {\n\treturn validatePathExists(path, true)\n}\n\n// JWTChanged functions are called when the store file watcher notices a JWT changed\ntype JWTChanged func(publicKey string)\n\n// DirJWTStore implements the JWT Store interface, keeping JWTs in an optionally sharded\n// directory structure\ntype DirJWTStore struct {\n\tsync.Mutex\n\tdirectory  string\n\tshard      bool\n\treadonly   bool\n\tdeleteType deleteType\n\toperator   map[string]struct{}\n\texpiration *expirationTracker\n\tchanged    JWTChanged\n\tdeleted    JWTChanged\n}\n\nfunc newDir(dirPath string, create bool) (string, error) {\n\tfullPath, err := validateDirPath(dirPath)\n\tif err != nil {\n\t\tif !create {\n\t\t\treturn _EMPTY_, err\n\t\t}\n\t\tif err = os.MkdirAll(dirPath, defaultDirPerms); err != nil {\n\t\t\treturn _EMPTY_, err\n\t\t}\n\t\tif fullPath, err = validateDirPath(dirPath); err != nil {\n\t\t\treturn _EMPTY_, err\n\t\t}\n\t}\n\treturn fullPath, nil\n}\n\n// future proofing in case new options will be added\ntype dirJWTStoreOption any\n\n// Creates a directory based jwt store.\n// Reads files only, does NOT watch directories and files.\nfunc NewImmutableDirJWTStore(dirPath string, shard bool, _ ...dirJWTStoreOption) (*DirJWTStore, error) {\n\ttheStore, err := NewDirJWTStore(dirPath, shard, false, nil)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\ttheStore.readonly = true\n\treturn theStore, nil\n}\n\n// Creates a directory based jwt store.\n// Operates on files only, does NOT watch directories and files.\nfunc NewDirJWTStore(dirPath string, shard bool, create bool, _ ...dirJWTStoreOption) (*DirJWTStore, error) {\n\tfullPath, err := newDir(dirPath, create)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\ttheStore := &DirJWTStore{\n\t\tdirectory: fullPath,\n\t\tshard:     shard,\n\t}\n\treturn theStore, nil\n}\n\ntype deleteType int\n\nconst (\n\tNoDelete deleteType = iota\n\tRenameDeleted\n\tHardDelete\n)\n\n// Creates a directory based jwt store.\n//\n// When ttl is set deletion of file is based on it and not on the jwt expiration\n// To completely disable expiration (including expiration in jwt) set ttl to max duration time.Duration(math.MaxInt64)\n//\n// limit defines how many files are allowed at any given time. Set to math.MaxInt64 to disable.\n// evictOnLimit determines the behavior once limit is reached.\n// * true - Evict based on lru strategy\n// * false - return an error\nfunc NewExpiringDirJWTStore(dirPath string, shard bool, create bool, delete deleteType, expireCheck time.Duration, limit int64,\n\tevictOnLimit bool, ttl time.Duration, changeNotification JWTChanged, _ ...dirJWTStoreOption) (*DirJWTStore, error) {\n\tfullPath, err := newDir(dirPath, create)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\ttheStore := &DirJWTStore{\n\t\tdirectory:  fullPath,\n\t\tshard:      shard,\n\t\tdeleteType: delete,\n\t\tchanged:    changeNotification,\n\t}\n\tif expireCheck <= 0 {\n\t\tif ttl != 0 {\n\t\t\texpireCheck = ttl / 2\n\t\t}\n\t\tif expireCheck == 0 || expireCheck > time.Minute {\n\t\t\texpireCheck = time.Minute\n\t\t}\n\t}\n\tif limit <= 0 {\n\t\tlimit = math.MaxInt64\n\t}\n\ttheStore.startExpiring(expireCheck, limit, evictOnLimit, ttl)\n\ttheStore.Lock()\n\terr = filepath.Walk(dirPath, func(path string, info os.FileInfo, err error) error {\n\t\tif strings.HasSuffix(path, fileExtension) {\n\t\t\tif theJwt, err := os.ReadFile(path); err == nil {\n\t\t\t\thash := sha256.Sum256(theJwt)\n\t\t\t\t_, file := filepath.Split(path)\n\t\t\t\ttheStore.expiration.track(strings.TrimSuffix(file, fileExtension), &hash, string(theJwt))\n\t\t\t}\n\t\t}\n\t\treturn nil\n\t})\n\ttheStore.Unlock()\n\tif err != nil {\n\t\ttheStore.Close()\n\t\treturn nil, err\n\t}\n\treturn theStore, err\n}\n\nfunc (store *DirJWTStore) IsReadOnly() bool {\n\treturn store.readonly\n}\n\nfunc (store *DirJWTStore) LoadAcc(publicKey string) (string, error) {\n\treturn store.load(publicKey)\n}\n\nfunc (store *DirJWTStore) SaveAcc(publicKey string, theJWT string) error {\n\treturn store.save(publicKey, theJWT)\n}\n\nfunc (store *DirJWTStore) LoadAct(hash string) (string, error) {\n\treturn store.load(hash)\n}\n\nfunc (store *DirJWTStore) SaveAct(hash string, theJWT string) error {\n\treturn store.save(hash, theJWT)\n}\n\nfunc (store *DirJWTStore) Close() {\n\tstore.Lock()\n\tdefer store.Unlock()\n\tif store.expiration != nil {\n\t\tstore.expiration.close()\n\t\tstore.expiration = nil\n\t}\n}\n\n// Pack up to maxJWTs into a package\nfunc (store *DirJWTStore) Pack(maxJWTs int) (string, error) {\n\tcount := 0\n\tvar pack []string\n\tif maxJWTs > 0 {\n\t\tpack = make([]string, 0, maxJWTs)\n\t} else {\n\t\tpack = []string{}\n\t}\n\tstore.Lock()\n\terr := filepath.Walk(store.directory, func(path string, info os.FileInfo, err error) error {\n\t\tif !info.IsDir() && strings.HasSuffix(path, fileExtension) { // this is a JWT\n\t\t\tif count == maxJWTs { // won't match negative\n\t\t\t\treturn nil\n\t\t\t}\n\t\t\tpubKey := strings.TrimSuffix(filepath.Base(path), fileExtension)\n\t\t\tif store.expiration != nil {\n\t\t\t\tif _, ok := store.expiration.idx[pubKey]; !ok {\n\t\t\t\t\treturn nil // only include indexed files\n\t\t\t\t}\n\t\t\t}\n\t\t\tjwtBytes, err := os.ReadFile(path)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tif store.expiration != nil {\n\t\t\t\tclaim, err := jwt.DecodeGeneric(string(jwtBytes))\n\t\t\t\tif err == nil && claim.Expires > 0 && claim.Expires < time.Now().Unix() {\n\t\t\t\t\treturn nil\n\t\t\t\t}\n\t\t\t}\n\t\t\tpack = append(pack, fmt.Sprintf(\"%s|%s\", pubKey, string(jwtBytes)))\n\t\t\tcount++\n\t\t}\n\t\treturn nil\n\t})\n\tstore.Unlock()\n\tif err != nil {\n\t\treturn _EMPTY_, err\n\t} else {\n\t\treturn strings.Join(pack, \"\\n\"), nil\n\t}\n}\n\n// Pack up to maxJWTs into a message and invoke callback with it\nfunc (store *DirJWTStore) PackWalk(maxJWTs int, cb func(partialPackMsg string)) error {\n\tif maxJWTs <= 0 || cb == nil {\n\t\treturn errors.New(\"bad arguments to PackWalk\")\n\t}\n\tvar packMsg []string\n\tstore.Lock()\n\tdir := store.directory\n\texp := store.expiration\n\tstore.Unlock()\n\terr := filepath.Walk(dir, func(path string, info os.FileInfo, err error) error {\n\t\tif info != nil && !info.IsDir() && strings.HasSuffix(path, fileExtension) { // this is a JWT\n\t\t\tpubKey := strings.TrimSuffix(filepath.Base(path), fileExtension)\n\t\t\tstore.Lock()\n\t\t\tif exp != nil {\n\t\t\t\tif _, ok := exp.idx[pubKey]; !ok {\n\t\t\t\t\tstore.Unlock()\n\t\t\t\t\treturn nil // only include indexed files\n\t\t\t\t}\n\t\t\t}\n\t\t\tstore.Unlock()\n\t\t\tjwtBytes, err := os.ReadFile(path)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tif len(jwtBytes) == 0 {\n\t\t\t\t// Skip if no contents in the JWT.\n\t\t\t\treturn nil\n\t\t\t}\n\t\t\tif exp != nil {\n\t\t\t\tclaim, err := jwt.DecodeGeneric(string(jwtBytes))\n\t\t\t\tif err == nil && claim.Expires > 0 && claim.Expires < time.Now().Unix() {\n\t\t\t\t\treturn nil\n\t\t\t\t}\n\t\t\t}\n\t\t\tpackMsg = append(packMsg, fmt.Sprintf(\"%s|%s\", pubKey, string(jwtBytes)))\n\t\t\tif len(packMsg) == maxJWTs { // won't match negative\n\t\t\t\tcb(strings.Join(packMsg, \"\\n\"))\n\t\t\t\tpackMsg = nil\n\t\t\t}\n\t\t}\n\t\treturn nil\n\t})\n\tif packMsg != nil {\n\t\tcb(strings.Join(packMsg, \"\\n\"))\n\t}\n\treturn err\n}\n\n// Merge takes the JWTs from package and adds them to the store\n// Merge is destructive in the sense that it doesn't check if the JWT\n// is newer or anything like that.\nfunc (store *DirJWTStore) Merge(pack string) error {\n\tnewJWTs := strings.Split(pack, \"\\n\")\n\tfor _, line := range newJWTs {\n\t\tif line == _EMPTY_ { // ignore blank lines\n\t\t\tcontinue\n\t\t}\n\t\tsplit := strings.Split(line, \"|\")\n\t\tif len(split) != 2 {\n\t\t\treturn fmt.Errorf(\"line in package didn't contain 2 entries: %q\", line)\n\t\t}\n\t\tpubKey := split[0]\n\t\tif !nkeys.IsValidPublicAccountKey(pubKey) {\n\t\t\treturn fmt.Errorf(\"key to merge is not a valid public account key\")\n\t\t}\n\t\tif err := store.saveIfNewer(pubKey, split[1]); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc (store *DirJWTStore) Reload() error {\n\tstore.Lock()\n\texp := store.expiration\n\tif exp == nil || store.readonly {\n\t\tstore.Unlock()\n\t\treturn nil\n\t}\n\tidx := exp.idx\n\tchanged := store.changed\n\tisCache := store.expiration.evictOnLimit\n\t// clear out indexing data structures\n\texp.heap = make([]*jwtItem, 0, len(exp.heap))\n\texp.idx = make(map[string]*list.Element)\n\texp.lru = list.New()\n\texp.hash = [sha256.Size]byte{}\n\tstore.Unlock()\n\treturn filepath.Walk(store.directory, func(path string, info os.FileInfo, err error) error {\n\t\tif strings.HasSuffix(path, fileExtension) {\n\t\t\tif theJwt, err := os.ReadFile(path); err == nil {\n\t\t\t\thash := sha256.Sum256(theJwt)\n\t\t\t\t_, file := filepath.Split(path)\n\t\t\t\tpkey := strings.TrimSuffix(file, fileExtension)\n\t\t\t\tnotify := isCache // for cache, issue cb even when file not present (may have been evicted)\n\t\t\t\tif i, ok := idx[pkey]; ok {\n\t\t\t\t\tnotify = !bytes.Equal(i.Value.(*jwtItem).hash[:], hash[:])\n\t\t\t\t}\n\t\t\t\tstore.Lock()\n\t\t\t\texp.track(pkey, &hash, string(theJwt))\n\t\t\t\tstore.Unlock()\n\t\t\t\tif notify && changed != nil {\n\t\t\t\t\tchanged(pkey)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\treturn nil\n\t})\n}\n\nfunc (store *DirJWTStore) pathForKey(publicKey string) string {\n\tif len(publicKey) < 2 {\n\t\treturn _EMPTY_\n\t}\n\tif !nkeys.IsValidPublicKey(publicKey) {\n\t\treturn _EMPTY_\n\t}\n\tfileName := fmt.Sprintf(\"%s%s\", publicKey, fileExtension)\n\tif store.shard {\n\t\tlast := publicKey[len(publicKey)-2:]\n\t\treturn filepath.Join(store.directory, last, fileName)\n\t} else {\n\t\treturn filepath.Join(store.directory, fileName)\n\t}\n}\n\n// Load checks the memory store and returns the matching JWT or an error\n// Assumes lock is NOT held\nfunc (store *DirJWTStore) load(publicKey string) (string, error) {\n\tstore.Lock()\n\tdefer store.Unlock()\n\tif path := store.pathForKey(publicKey); path == _EMPTY_ {\n\t\treturn _EMPTY_, fmt.Errorf(\"invalid public key\")\n\t} else if data, err := os.ReadFile(path); err != nil {\n\t\treturn _EMPTY_, err\n\t} else {\n\t\tif store.expiration != nil {\n\t\t\tstore.expiration.updateTrack(publicKey)\n\t\t}\n\t\treturn string(data), nil\n\t}\n}\n\n// write that keeps hash of all jwt in sync\n// Assumes the lock is held. Does return true or an error never both.\nfunc (store *DirJWTStore) write(path string, publicKey string, theJWT string) (bool, error) {\n\tif len(theJWT) == 0 {\n\t\treturn false, fmt.Errorf(\"invalid JWT\")\n\t}\n\tvar newHash *[sha256.Size]byte\n\tif store.expiration != nil {\n\t\th := sha256.Sum256([]byte(theJWT))\n\t\tnewHash = &h\n\t\tif v, ok := store.expiration.idx[publicKey]; ok {\n\t\t\tstore.expiration.updateTrack(publicKey)\n\t\t\t// this write is an update, move to back\n\t\t\tit := v.Value.(*jwtItem)\n\t\t\toldHash := it.hash[:]\n\t\t\tif bytes.Equal(oldHash, newHash[:]) {\n\t\t\t\treturn false, nil\n\t\t\t}\n\t\t} else if int64(store.expiration.Len()) >= store.expiration.limit {\n\t\t\tif !store.expiration.evictOnLimit {\n\t\t\t\treturn false, errors.New(\"jwt store is full\")\n\t\t\t}\n\t\t\t// this write is an add, pick the least recently used value for removal\n\t\t\ti := store.expiration.lru.Front().Value.(*jwtItem)\n\t\t\tif err := os.Remove(store.pathForKey(i.publicKey)); err != nil {\n\t\t\t\treturn false, err\n\t\t\t} else {\n\t\t\t\tstore.expiration.unTrack(i.publicKey)\n\t\t\t}\n\t\t}\n\t}\n\tif err := os.WriteFile(path, []byte(theJWT), defaultFilePerms); err != nil {\n\t\treturn false, err\n\t} else if store.expiration != nil {\n\t\tstore.expiration.track(publicKey, newHash, theJWT)\n\t}\n\treturn true, nil\n}\n\nfunc (store *DirJWTStore) delete(publicKey string) error {\n\tif store.readonly {\n\t\treturn fmt.Errorf(\"store is read-only\")\n\t} else if store.deleteType == NoDelete {\n\t\treturn fmt.Errorf(\"store is not set up to for delete\")\n\t}\n\tstore.Lock()\n\tdefer store.Unlock()\n\tname := store.pathForKey(publicKey)\n\tif store.deleteType == RenameDeleted {\n\t\tif err := os.Rename(name, name+\".deleted\"); err != nil {\n\t\t\tif os.IsNotExist(err) {\n\t\t\t\treturn nil\n\t\t\t}\n\t\t\treturn err\n\t\t}\n\t} else if err := os.Remove(name); err != nil {\n\t\tif os.IsNotExist(err) {\n\t\t\treturn nil\n\t\t}\n\t\treturn err\n\t}\n\tstore.expiration.unTrack(publicKey)\n\tstore.deleted(publicKey)\n\treturn nil\n}\n\n// Save puts the JWT in a map by public key and performs update callbacks\n// Assumes lock is NOT held\nfunc (store *DirJWTStore) save(publicKey string, theJWT string) error {\n\tif store.readonly {\n\t\treturn fmt.Errorf(\"store is read-only\")\n\t}\n\tstore.Lock()\n\tpath := store.pathForKey(publicKey)\n\tif path == _EMPTY_ {\n\t\tstore.Unlock()\n\t\treturn fmt.Errorf(\"invalid public key\")\n\t}\n\tdirPath := filepath.Dir(path)\n\tif _, err := validateDirPath(dirPath); err != nil {\n\t\tif err := os.MkdirAll(dirPath, defaultDirPerms); err != nil {\n\t\t\tstore.Unlock()\n\t\t\treturn err\n\t\t}\n\t}\n\tchanged, err := store.write(path, publicKey, theJWT)\n\tcb := store.changed\n\tstore.Unlock()\n\tif changed && cb != nil {\n\t\tcb(publicKey)\n\t}\n\treturn err\n}\n\n// Assumes the lock is NOT held, and only updates if the jwt is new, or the one on disk is older\n// When changed, invokes jwt changed callback\nfunc (store *DirJWTStore) saveIfNewer(publicKey string, theJWT string) error {\n\tif store.readonly {\n\t\treturn fmt.Errorf(\"store is read-only\")\n\t}\n\tpath := store.pathForKey(publicKey)\n\tif path == _EMPTY_ {\n\t\treturn fmt.Errorf(\"invalid public key\")\n\t}\n\tdirPath := filepath.Dir(path)\n\tif _, err := validateDirPath(dirPath); err != nil {\n\t\tif err := os.MkdirAll(dirPath, defaultDirPerms); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\tif _, err := os.Stat(path); err == nil {\n\t\tif newJWT, err := jwt.DecodeGeneric(theJWT); err != nil {\n\t\t\treturn err\n\t\t} else if existing, err := os.ReadFile(path); err != nil {\n\t\t\treturn err\n\t\t} else if existingJWT, err := jwt.DecodeGeneric(string(existing)); err != nil {\n\t\t\t// skip if it can't be decoded\n\t\t} else if existingJWT.ID == newJWT.ID {\n\t\t\treturn nil\n\t\t} else if existingJWT.IssuedAt > newJWT.IssuedAt {\n\t\t\treturn nil\n\t\t} else if newJWT.Subject != publicKey {\n\t\t\treturn fmt.Errorf(\"jwt subject nkey and provided nkey do not match\")\n\t\t} else if existingJWT.Subject != newJWT.Subject {\n\t\t\treturn fmt.Errorf(\"subject of existing and new jwt do not match\")\n\t\t}\n\t}\n\tstore.Lock()\n\tcb := store.changed\n\tchanged, err := store.write(path, publicKey, theJWT)\n\tstore.Unlock()\n\tif err != nil {\n\t\treturn err\n\t} else if changed && cb != nil {\n\t\tcb(publicKey)\n\t}\n\treturn nil\n}\n\nfunc xorAssign(lVal *[sha256.Size]byte, rVal [sha256.Size]byte) {\n\tfor i := range rVal {\n\t\t(*lVal)[i] ^= rVal[i]\n\t}\n}\n\n// returns a hash representing all indexed jwt\nfunc (store *DirJWTStore) Hash() [sha256.Size]byte {\n\tstore.Lock()\n\tdefer store.Unlock()\n\tif store.expiration == nil {\n\t\treturn [sha256.Size]byte{}\n\t} else {\n\t\treturn store.expiration.hash\n\t}\n}\n\n// An jwtItem is something managed by the priority queue\ntype jwtItem struct {\n\tindex      int\n\tpublicKey  string\n\texpiration int64 // consists of unix time of expiration (ttl when set or jwt expiration) in seconds\n\thash       [sha256.Size]byte\n}\n\n// A expirationTracker implements heap.Interface and holds Items.\ntype expirationTracker struct {\n\theap         []*jwtItem // sorted by jwtItem.expiration\n\tidx          map[string]*list.Element\n\tlru          *list.List // keep which jwt are least used\n\tlimit        int64      // limit how many jwt are being tracked\n\tevictOnLimit bool       // when limit is hit, error or evict using lru\n\tttl          time.Duration\n\thash         [sha256.Size]byte // xor of all jwtItem.hash in idx\n\tquit         chan struct{}\n\twg           sync.WaitGroup\n}\n\nfunc (q *expirationTracker) Len() int { return len(q.heap) }\n\nfunc (q *expirationTracker) Less(i, j int) bool {\n\tpq := q.heap\n\treturn pq[i].expiration < pq[j].expiration\n}\n\nfunc (q *expirationTracker) Swap(i, j int) {\n\tpq := q.heap\n\tpq[i], pq[j] = pq[j], pq[i]\n\tpq[i].index = i\n\tpq[j].index = j\n}\n\nfunc (q *expirationTracker) Push(x any) {\n\tn := len(q.heap)\n\titem := x.(*jwtItem)\n\titem.index = n\n\tq.heap = append(q.heap, item)\n\tq.idx[item.publicKey] = q.lru.PushBack(item)\n}\n\nfunc (q *expirationTracker) Pop() any {\n\told := q.heap\n\tn := len(old)\n\titem := old[n-1]\n\told[n-1] = nil // avoid memory leak\n\titem.index = -1\n\tq.heap = old[0 : n-1]\n\tq.lru.Remove(q.idx[item.publicKey])\n\tdelete(q.idx, item.publicKey)\n\treturn item\n}\n\nfunc (pq *expirationTracker) updateTrack(publicKey string) {\n\tif e, ok := pq.idx[publicKey]; ok {\n\t\ti := e.Value.(*jwtItem)\n\t\tif pq.ttl != 0 {\n\t\t\t// only update expiration when set\n\t\t\ti.expiration = time.Now().Add(pq.ttl).UnixNano()\n\t\t\theap.Fix(pq, i.index)\n\t\t}\n\t\tif pq.evictOnLimit {\n\t\t\tpq.lru.MoveToBack(e)\n\t\t}\n\t}\n}\n\nfunc (pq *expirationTracker) unTrack(publicKey string) {\n\tif it, ok := pq.idx[publicKey]; ok {\n\t\txorAssign(&pq.hash, it.Value.(*jwtItem).hash)\n\t\theap.Remove(pq, it.Value.(*jwtItem).index)\n\t\tdelete(pq.idx, publicKey)\n\t}\n}\n\nfunc (pq *expirationTracker) track(publicKey string, hash *[sha256.Size]byte, theJWT string) {\n\tvar exp int64\n\t// prioritize ttl over expiration\n\tif pq.ttl != 0 {\n\t\tif pq.ttl == time.Duration(math.MaxInt64) {\n\t\t\texp = math.MaxInt64\n\t\t} else {\n\t\t\texp = time.Now().Add(pq.ttl).UnixNano()\n\t\t}\n\t} else {\n\t\tif g, err := jwt.DecodeGeneric(theJWT); err == nil {\n\t\t\texp = time.Unix(g.Expires, 0).UnixNano()\n\t\t}\n\t\tif exp == 0 {\n\t\t\texp = math.MaxInt64 // default to indefinite\n\t\t}\n\t}\n\tif e, ok := pq.idx[publicKey]; ok {\n\t\ti := e.Value.(*jwtItem)\n\t\txorAssign(&pq.hash, i.hash) // remove old hash\n\t\ti.expiration = exp\n\t\ti.hash = *hash\n\t\theap.Fix(pq, i.index)\n\t} else {\n\t\theap.Push(pq, &jwtItem{-1, publicKey, exp, *hash})\n\t}\n\txorAssign(&pq.hash, *hash) // add in new hash\n}\n\nfunc (pq *expirationTracker) close() {\n\tif pq == nil || pq.quit == nil {\n\t\treturn\n\t}\n\tclose(pq.quit)\n\tpq.quit = nil\n}\n\nfunc (store *DirJWTStore) startExpiring(reCheck time.Duration, limit int64, evictOnLimit bool, ttl time.Duration) {\n\tstore.Lock()\n\tdefer store.Unlock()\n\tquit := make(chan struct{})\n\tpq := &expirationTracker{\n\t\tmake([]*jwtItem, 0, 10),\n\t\tmake(map[string]*list.Element),\n\t\tlist.New(),\n\t\tlimit,\n\t\tevictOnLimit,\n\t\tttl,\n\t\t[sha256.Size]byte{},\n\t\tquit,\n\t\tsync.WaitGroup{},\n\t}\n\tstore.expiration = pq\n\tpq.wg.Add(1)\n\tgo func() {\n\t\tt := time.NewTicker(reCheck)\n\t\tdefer t.Stop()\n\t\tdefer pq.wg.Done()\n\t\tfor {\n\t\t\tnow := time.Now().UnixNano()\n\t\t\tstore.Lock()\n\t\t\tif pq.Len() > 0 {\n\t\t\t\tif it := pq.heap[0]; it.expiration <= now {\n\t\t\t\t\tpath := store.pathForKey(it.publicKey)\n\t\t\t\t\tif err := os.Remove(path); err == nil {\n\t\t\t\t\t\theap.Pop(pq)\n\t\t\t\t\t\tpq.unTrack(it.publicKey)\n\t\t\t\t\t\txorAssign(&pq.hash, it.hash)\n\t\t\t\t\t\tstore.Unlock()\n\t\t\t\t\t\tcontinue // we removed an entry, check next one right away\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tstore.Unlock()\n\t\t\tselect {\n\t\t\tcase <-t.C:\n\t\t\tcase <-quit:\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}()\n}\n",
    "source_file": "server/dirstore.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2021-2025 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage server\n\nimport (\n\t\"sync\"\n\t\"time\"\n)\n\ntype rateCounter struct {\n\tlimit    int64\n\tcount    int64\n\tblocked  uint64\n\tend      time.Time\n\tinterval time.Duration\n\tmu       sync.Mutex\n}\n\nfunc newRateCounter(limit int64) *rateCounter {\n\treturn &rateCounter{\n\t\tlimit:    limit,\n\t\tinterval: time.Second,\n\t}\n}\n\nfunc (r *rateCounter) allow() bool {\n\tnow := time.Now()\n\n\tr.mu.Lock()\n\n\tif now.After(r.end) {\n\t\tr.count = 0\n\t\tr.end = now.Add(r.interval)\n\t} else {\n\t\tr.count++\n\t}\n\tallow := r.count < r.limit\n\tif !allow {\n\t\tr.blocked++\n\t}\n\n\tr.mu.Unlock()\n\n\treturn allow\n}\n\nfunc (r *rateCounter) countBlocked() uint64 {\n\tr.mu.Lock()\n\tblocked := r.blocked\n\tr.blocked = 0\n\tr.mu.Unlock()\n\n\treturn blocked\n}\n",
    "source_file": "server/rate_counter.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2020-2025 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage server\n\nimport (\n\t\"bytes\"\n\t\"cmp\"\n\t\"crypto/tls\"\n\t\"encoding/binary\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"net\"\n\t\"net/http\"\n\t\"slices\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\t\"unicode/utf8\"\n\n\t\"github.com/nats-io/nuid\"\n)\n\n// References to \"spec\" here is from https://docs.oasis-open.org/mqtt/mqtt/v3.1.1/os/mqtt-v3.1.1-os.pdf\n\nconst (\n\tmqttPacketConnect    = byte(0x10)\n\tmqttPacketConnectAck = byte(0x20)\n\tmqttPacketPub        = byte(0x30)\n\tmqttPacketPubAck     = byte(0x40)\n\tmqttPacketPubRec     = byte(0x50)\n\tmqttPacketPubRel     = byte(0x60)\n\tmqttPacketPubComp    = byte(0x70)\n\tmqttPacketSub        = byte(0x80)\n\tmqttPacketSubAck     = byte(0x90)\n\tmqttPacketUnsub      = byte(0xa0)\n\tmqttPacketUnsubAck   = byte(0xb0)\n\tmqttPacketPing       = byte(0xc0)\n\tmqttPacketPingResp   = byte(0xd0)\n\tmqttPacketDisconnect = byte(0xe0)\n\tmqttPacketMask       = byte(0xf0)\n\tmqttPacketFlagMask   = byte(0x0f)\n\n\tmqttProtoLevel = byte(0x4)\n\n\t// Connect flags\n\tmqttConnFlagReserved     = byte(0x1)\n\tmqttConnFlagCleanSession = byte(0x2)\n\tmqttConnFlagWillFlag     = byte(0x04)\n\tmqttConnFlagWillQoS      = byte(0x18)\n\tmqttConnFlagWillRetain   = byte(0x20)\n\tmqttConnFlagPasswordFlag = byte(0x40)\n\tmqttConnFlagUsernameFlag = byte(0x80)\n\n\t// Publish flags\n\tmqttPubFlagRetain = byte(0x01)\n\tmqttPubFlagQoS    = byte(0x06)\n\tmqttPubFlagDup    = byte(0x08)\n\tmqttPubQos1       = byte(0x1 << 1)\n\tmqttPubQoS2       = byte(0x2 << 1)\n\n\t// Subscribe flags\n\tmqttSubscribeFlags = byte(0x2)\n\tmqttSubAckFailure  = byte(0x80)\n\n\t// Unsubscribe flags\n\tmqttUnsubscribeFlags = byte(0x2)\n\n\t// ConnAck returned codes\n\tmqttConnAckRCConnectionAccepted          = byte(0x0)\n\tmqttConnAckRCUnacceptableProtocolVersion = byte(0x1)\n\tmqttConnAckRCIdentifierRejected          = byte(0x2)\n\tmqttConnAckRCServerUnavailable           = byte(0x3)\n\tmqttConnAckRCBadUserOrPassword           = byte(0x4)\n\tmqttConnAckRCNotAuthorized               = byte(0x5)\n\tmqttConnAckRCQoS2WillRejected            = byte(0x10)\n\n\t// Maximum payload size of a control packet\n\tmqttMaxPayloadSize = 0xFFFFFFF\n\n\t// Topic/Filter characters\n\tmqttTopicLevelSep = '/'\n\tmqttSingleLevelWC = '+'\n\tmqttMultiLevelWC  = '#'\n\tmqttReservedPre   = '$'\n\n\t// This is appended to the sid of a subscription that is\n\t// created on the upper level subject because of the MQTT\n\t// wildcard '#' semantic.\n\tmqttMultiLevelSidSuffix = \" fwc\"\n\n\t// This is the prefix for NATS subscriptions subjects associated as delivery\n\t// subject of JS consumer. We want to make them unique so will prevent users\n\t// MQTT subscriptions to start with this.\n\tmqttSubPrefix = \"$MQTT.sub.\"\n\n\t// Stream name for MQTT messages on a given account\n\tmqttStreamName          = \"$MQTT_msgs\"\n\tmqttStreamSubjectPrefix = \"$MQTT.msgs.\"\n\n\t// Stream name for MQTT retained messages on a given account\n\tmqttRetainedMsgsStreamName    = \"$MQTT_rmsgs\"\n\tmqttRetainedMsgsStreamSubject = \"$MQTT.rmsgs.\"\n\n\t// Stream name for MQTT sessions on a given account\n\tmqttSessStreamName          = \"$MQTT_sess\"\n\tmqttSessStreamSubjectPrefix = \"$MQTT.sess.\"\n\n\t// Stream name prefix for MQTT sessions on a given account\n\tmqttSessionsStreamNamePrefix = \"$MQTT_sess_\"\n\n\t// Stream name and subject for incoming MQTT QoS2 messages\n\tmqttQoS2IncomingMsgsStreamName          = \"$MQTT_qos2in\"\n\tmqttQoS2IncomingMsgsStreamSubjectPrefix = \"$MQTT.qos2.in.\"\n\n\t// Stream name and subjects for outgoing MQTT QoS (PUBREL) messages\n\tmqttOutStreamName               = \"$MQTT_out\"\n\tmqttOutSubjectPrefix            = \"$MQTT.out.\"\n\tmqttPubRelSubjectPrefix         = \"$MQTT.out.pubrel.\"\n\tmqttPubRelDeliverySubjectPrefix = \"$MQTT.deliver.pubrel.\"\n\tmqttPubRelConsumerDurablePrefix = \"$MQTT_PUBREL_\"\n\n\t// As per spec, MQTT server may not redeliver QoS 1 and 2 messages to\n\t// clients, except after client reconnects. However, NATS Server will\n\t// redeliver unacknowledged messages after this default interval. This can\n\t// be changed with the server.Options.MQTT.AckWait option.\n\tmqttDefaultAckWait = 30 * time.Second\n\n\t// This is the default for the outstanding number of pending QoS 1\n\t// messages sent to a session with QoS 1 subscriptions.\n\tmqttDefaultMaxAckPending = 1024\n\n\t// A session's list of subscriptions cannot have a cumulative MaxAckPending\n\t// of more than this limit.\n\tmqttMaxAckTotalLimit = 0xFFFF\n\n\t// Prefix of the reply subject for JS API requests.\n\tmqttJSARepliesPrefix = \"$MQTT.JSA.\"\n\n\t// Those are tokens that are used for the reply subject of JS API requests.\n\t// For instance \"$MQTT.JSA.<node id>.SC.<number>\" is the reply subject\n\t// for a request to create a stream (where <node id> is the server name hash),\n\t// while \"$MQTT.JSA.<node id>.SL.<number>\" is for a stream lookup, etc...\n\tmqttJSAIdTokenPos     = 3\n\tmqttJSATokenPos       = 4\n\tmqttJSAClientIDPos    = 5\n\tmqttJSAStreamCreate   = \"SC\"\n\tmqttJSAStreamUpdate   = \"SU\"\n\tmqttJSAStreamLookup   = \"SL\"\n\tmqttJSAStreamDel      = \"SD\"\n\tmqttJSAConsumerCreate = \"CC\"\n\tmqttJSAConsumerLookup = \"CL\"\n\tmqttJSAConsumerDel    = \"CD\"\n\tmqttJSAMsgStore       = \"MS\"\n\tmqttJSAMsgLoad        = \"ML\"\n\tmqttJSAMsgDelete      = \"MD\"\n\tmqttJSASessPersist    = \"SP\"\n\tmqttJSARetainedMsgDel = \"RD\"\n\tmqttJSAStreamNames    = \"SN\"\n\n\t// This is how long to keep a client in the flappers map before closing the\n\t// connection. This prevent quick reconnect from those clients that keep\n\t// wanting to connect with a client ID already in use.\n\tmqttSessFlappingJailDur = time.Second\n\n\t// This is how frequently the timer to cleanup the sessions flappers map is firing.\n\tmqttSessFlappingCleanupInterval = 5 * time.Second\n\n\t// Default retry delay if transfer of old session streams to new one fails\n\tmqttDefaultTransferRetry = 5 * time.Second\n\n\t// For Websocket URLs\n\tmqttWSPath = \"/mqtt\"\n\n\tmqttInitialPubHeader        = 16 // An overkill, should need 7 bytes max\n\tmqttProcessSubTooLong       = 100 * time.Millisecond\n\tmqttDefaultRetainedCacheTTL = 2 * time.Minute\n\tmqttRetainedTransferTimeout = 10 * time.Second\n\tmqttDefaultJSAPITimeout     = 5 * time.Second\n)\n\nconst (\n\tsparkbNBIRTH = \"NBIRTH\"\n\tsparkbDBIRTH = \"DBIRTH\"\n\tsparkbNDEATH = \"NDEATH\"\n\tsparkbDDEATH = \"DDEATH\"\n)\n\nvar (\n\tsparkbNamespaceTopicPrefix    = []byte(\"spBv1.0/\")\n\tsparkbCertificatesTopicPrefix = []byte(\"$sparkplug/certificates/\")\n)\n\nvar (\n\tmqttPingResponse     = []byte{mqttPacketPingResp, 0x0}\n\tmqttProtoName        = []byte(\"MQTT\")\n\tmqttOldProtoName     = []byte(\"MQIsdp\")\n\tmqttSessJailDur      = mqttSessFlappingJailDur\n\tmqttFlapCleanItvl    = mqttSessFlappingCleanupInterval\n\tmqttRetainedCacheTTL = mqttDefaultRetainedCacheTTL\n)\n\nvar (\n\terrMQTTNotWebsocketPort           = errors.New(\"MQTT clients over websocket must connect to the Websocket port, not the MQTT port\")\n\terrMQTTTopicFilterCannotBeEmpty   = errors.New(\"topic filter cannot be empty\")\n\terrMQTTMalformedVarInt            = errors.New(\"malformed variable int\")\n\terrMQTTSecondConnectPacket        = errors.New(\"received a second CONNECT packet\")\n\terrMQTTServerNameMustBeSet        = errors.New(\"mqtt requires server name to be explicitly set\")\n\terrMQTTUserMixWithUsersNKeys      = errors.New(\"mqtt authentication username not compatible with presence of users/nkeys\")\n\terrMQTTTokenMixWIthUsersNKeys     = errors.New(\"mqtt authentication token not compatible with presence of users/nkeys\")\n\terrMQTTAckWaitMustBePositive      = errors.New(\"ack wait must be a positive value\")\n\terrMQTTJSAPITimeoutMustBePositive = errors.New(\"JS API timeout must be a positive value\")\n\terrMQTTStandaloneNeedsJetStream   = errors.New(\"mqtt requires JetStream to be enabled if running in standalone mode\")\n\terrMQTTConnFlagReserved           = errors.New(\"connect flags reserved bit not set to 0\")\n\terrMQTTWillAndRetainFlag          = errors.New(\"if Will flag is set to 0, Will Retain flag must be 0 too\")\n\terrMQTTPasswordFlagAndNoUser      = errors.New(\"password flag set but username flag is not\")\n\terrMQTTCIDEmptyNeedsCleanFlag     = errors.New(\"when client ID is empty, clean session flag must be set to 1\")\n\terrMQTTEmptyWillTopic             = errors.New(\"empty Will topic not allowed\")\n\terrMQTTEmptyUsername              = errors.New(\"empty user name not allowed\")\n\terrMQTTTopicIsEmpty               = errors.New(\"topic cannot be empty\")\n\terrMQTTPacketIdentifierIsZero     = errors.New(\"packet identifier cannot be 0\")\n\terrMQTTUnsupportedCharacters      = errors.New(\"character ' ' not supported for MQTT topics\")\n\terrMQTTInvalidSession             = errors.New(\"invalid MQTT session\")\n)\n\ntype srvMQTT struct {\n\tlistener     net.Listener\n\tlistenerErr  error\n\tauthOverride bool\n\tsessmgr      mqttSessionManager\n}\n\ntype mqttSessionManager struct {\n\tmu       sync.RWMutex\n\tsessions map[string]*mqttAccountSessionManager // key is account name\n}\n\nvar testDisableRMSCache = false\n\ntype mqttAccountSessionManager struct {\n\tmu         sync.RWMutex\n\tsessions   map[string]*mqttSession        // key is MQTT client ID\n\tsessByHash map[string]*mqttSession        // key is MQTT client ID hash\n\tsessLocked map[string]struct{}            // key is MQTT client ID and indicate that a session can not be taken by a new client at this time\n\tflappers   map[string]int64               // When connection connects with client ID already in use\n\tflapTimer  *time.Timer                    // Timer to perform some cleanup of the flappers map\n\tsl         *Sublist                       // sublist allowing to find retained messages for given subscription\n\tretmsgs    map[string]*mqttRetainedMsgRef // retained messages\n\trmsCache   *sync.Map                      // map[subject]mqttRetainedMsg\n\tjsa        mqttJSA\n\trrmLastSeq uint64        // Restore retained messages expected last sequence\n\trrmDoneCh  chan struct{} // To notify the caller that all retained messages have been loaded\n\tdomainTk   string        // Domain (with trailing \".\"), or possibly empty. This is added to session subject.\n}\n\ntype mqttJSAResponse struct {\n\treply string // will be used to map to the original request in jsa.NewRequestExMulti\n\tvalue any\n}\n\ntype mqttJSA struct {\n\tmu        sync.Mutex\n\tid        string\n\tc         *client\n\tsendq     *ipQueue[*mqttJSPubMsg]\n\trplyr     string\n\treplies   sync.Map // [string]chan *mqttJSAResponse\n\tnuid      *nuid.NUID\n\tquitCh    chan struct{}\n\tdomain    string // Domain or possibly empty. This is added to session subject.\n\tdomainSet bool   // covers if domain was set, even to empty\n\ttimeout   time.Duration\n}\n\ntype mqttJSPubMsg struct {\n\tsubj  string\n\treply string\n\thdr   int\n\tmsg   []byte\n}\n\ntype mqttRetMsgDel struct {\n\tSubject string `json:\"subject\"`\n\tSeq     uint64 `json:\"seq\"`\n}\n\ntype mqttSession struct {\n\t// subsMu is a \"quick\" version of the session lock, sufficient for the QoS0\n\t// callback. It only guarantees that a new subscription is initialized, and\n\t// its retained messages if any have been queued up for delivery. The QoS12\n\t// callback uses the session lock.\n\tmu     sync.Mutex\n\tsubsMu sync.RWMutex\n\n\tid                     string // client ID\n\tidHash                 string // client ID hash\n\tc                      *client\n\tjsa                    *mqttJSA\n\tsubs                   map[string]byte // Key is MQTT SUBSCRIBE filter, value is the subscription QoS\n\tcons                   map[string]*ConsumerConfig\n\tpubRelConsumer         *ConsumerConfig\n\tpubRelSubscribed       bool\n\tpubRelDeliverySubject  string\n\tpubRelDeliverySubjectB []byte\n\tpubRelSubject          string\n\tseq                    uint64\n\n\t// pendingPublish maps packet identifiers (PI) to JetStream ACK subjects for\n\t// QoS1 and 2 PUBLISH messages pending delivery to the session's client.\n\tpendingPublish map[uint16]*mqttPending\n\n\t// pendingPubRel maps PIs to JetStream ACK subjects for QoS2 PUBREL\n\t// messages pending delivery to the session's client.\n\tpendingPubRel map[uint16]*mqttPending\n\n\t// cpending maps delivery attempts (that come with a JS ACK subject) to\n\t// existing PIs.\n\tcpending map[string]map[uint64]uint16 // composite key: jsDur, sseq\n\n\t// \"Last used\" publish packet identifier (PI). starting point searching for the next available.\n\tlast_pi uint16\n\n\t// Maximum number of pending acks for this session.\n\tmaxp     uint16\n\ttmaxack  int\n\tclean    bool\n\tdomainTk string\n}\n\ntype mqttPersistedSession struct {\n\tOrigin string                     `json:\"origin,omitempty\"`\n\tID     string                     `json:\"id,omitempty\"`\n\tClean  bool                       `json:\"clean,omitempty\"`\n\tSubs   map[string]byte            `json:\"subs,omitempty\"`\n\tCons   map[string]*ConsumerConfig `json:\"cons,omitempty\"`\n\tPubRel *ConsumerConfig            `json:\"pubrel,omitempty\"`\n}\n\ntype mqttRetainedMsg struct {\n\tOrigin  string `json:\"origin,omitempty\"`\n\tSubject string `json:\"subject,omitempty\"`\n\tTopic   string `json:\"topic,omitempty\"`\n\tMsg     []byte `json:\"msg,omitempty\"`\n\tFlags   byte   `json:\"flags,omitempty\"`\n\tSource  string `json:\"source,omitempty\"`\n\n\texpiresFromCache time.Time\n}\n\ntype mqttRetainedMsgRef struct {\n\tsseq  uint64\n\tfloor uint64\n\tsub   *subscription\n}\n\n// mqttSub contains fields associated with a MQTT subscription, and is added to\n// the main subscription struct for MQTT message delivery subscriptions. The\n// delivery callbacks may get invoked before sub.mqtt is set up, so they should\n// acquire either sess.mu or sess.subsMu before accessing it.\ntype mqttSub struct {\n\t// The sub's QOS and the JS durable name. They can change when\n\t// re-subscribing, and are used in the delivery callbacks. They can be\n\t// quickly accessed using sess.subsMu.RLock, or under the main session lock.\n\tqos   byte\n\tjsDur string\n\n\t// Pending serialization of retained messages to be sent when subscription\n\t// is registered. The sub's delivery callbacks must wait until `prm` is\n\t// ready (can block on sess.mu for that, too).\n\tprm [][]byte\n\n\t// If this subscription needs to be checked for being reserved. E.g. '#' or\n\t// '*' or '*/'.  It is set up at the time of subscription and is immutable\n\t// after that.\n\treserved bool\n}\n\ntype mqtt struct {\n\tr    *mqttReader\n\tcp   *mqttConnectProto\n\tpp   *mqttPublish\n\tasm  *mqttAccountSessionManager // quick reference to account session manager, immutable after processConnect()\n\tsess *mqttSession               // quick reference to session, immutable after processConnect()\n\tcid  string                     // client ID\n\n\t// rejectQoS2Pub tells the MQTT client to not accept QoS2 PUBLISH, instead\n\t// error and terminate the connection.\n\trejectQoS2Pub bool\n\n\t// downgradeQOS2Sub tells the MQTT client to downgrade QoS2 SUBSCRIBE\n\t// requests to QoS1.\n\tdowngradeQoS2Sub bool\n}\n\ntype mqttPending struct {\n\tsseq         uint64 // stream sequence\n\tjsAckSubject string // the ACK subject to send the ack to\n\tjsDur        string // JS durable name\n}\n\ntype mqttConnectProto struct {\n\trd    time.Duration\n\twill  *mqttWill\n\tflags byte\n}\n\ntype mqttIOReader interface {\n\tio.Reader\n\tSetReadDeadline(time.Time) error\n}\n\ntype mqttReader struct {\n\treader mqttIOReader\n\tbuf    []byte\n\tpos    int\n\tpstart int\n\tpbuf   []byte\n}\n\ntype mqttWriter struct {\n\tbytes.Buffer\n}\n\ntype mqttWill struct {\n\ttopic   []byte\n\tsubject []byte\n\tmapped  []byte\n\tmessage []byte\n\tqos     byte\n\tretain  bool\n}\n\ntype mqttFilter struct {\n\tfilter string\n\tqos    byte\n\t// Used only for tracing and should not be used after parsing of (un)sub protocols.\n\tttopic []byte\n}\n\ntype mqttPublish struct {\n\ttopic   []byte\n\tsubject []byte\n\tmapped  []byte\n\tmsg     []byte\n\tsz      int\n\tpi      uint16\n\tflags   byte\n}\n\n// When we re-encode incoming MQTT PUBLISH messages for NATS delivery, we add\n// the following headers:\n//   - \"Nmqtt-Pub\" (*always) indicates that the message originated from MQTT, and\n//     contains the original message QoS.\n//   - \"Nmqtt-Subject\" contains the original MQTT subject from mqttParsePub.\n//   - \"Nmqtt-Mapped\" contains the mapping during mqttParsePub.\n//\n// When we submit a PUBREL for delivery, we add a \"Nmqtt-PubRel\" header that\n// contains the PI.\nconst (\n\t// NATS header that indicates that the message originated from MQTT and\n\t// stores the published message QOS.\n\tmqttNatsHeader = \"Nmqtt-Pub\"\n\n\t// NATS headers to store retained message metadata (along with the original\n\t// message as binary).\n\tmqttNatsRetainedMessageTopic  = \"Nmqtt-RTopic\"\n\tmqttNatsRetainedMessageOrigin = \"Nmqtt-ROrigin\"\n\tmqttNatsRetainedMessageFlags  = \"Nmqtt-RFlags\"\n\tmqttNatsRetainedMessageSource = \"Nmqtt-RSource\"\n\n\t// NATS header that indicates that the message is an MQTT PubRel and stores\n\t// the PI.\n\tmqttNatsPubRelHeader = \"Nmqtt-PubRel\"\n\n\t// NATS headers to store the original MQTT subject and the subject mapping.\n\tmqttNatsHeaderSubject = \"Nmqtt-Subject\"\n\tmqttNatsHeaderMapped  = \"Nmqtt-Mapped\"\n)\n\ntype mqttParsedPublishNATSHeader struct {\n\tqos     byte\n\tsubject []byte\n\tmapped  []byte\n}\n\nfunc (s *Server) startMQTT() {\n\tif s.isShuttingDown() {\n\t\treturn\n\t}\n\n\tsopts := s.getOpts()\n\to := &sopts.MQTT\n\n\tvar hl net.Listener\n\tvar err error\n\n\tport := o.Port\n\tif port == -1 {\n\t\tport = 0\n\t}\n\thp := net.JoinHostPort(o.Host, strconv.Itoa(port))\n\ts.mu.Lock()\n\ts.mqtt.sessmgr.sessions = make(map[string]*mqttAccountSessionManager)\n\thl, err = net.Listen(\"tcp\", hp)\n\ts.mqtt.listenerErr = err\n\tif err != nil {\n\t\ts.mu.Unlock()\n\t\ts.Fatalf(\"Unable to listen for MQTT connections: %v\", err)\n\t\treturn\n\t}\n\tif port == 0 {\n\t\to.Port = hl.Addr().(*net.TCPAddr).Port\n\t}\n\ts.mqtt.listener = hl\n\tscheme := \"mqtt\"\n\tif o.TLSConfig != nil {\n\t\tscheme = \"tls\"\n\t}\n\ts.Noticef(\"Listening for MQTT clients on %s://%s:%d\", scheme, o.Host, o.Port)\n\tgo s.acceptConnections(hl, \"MQTT\", func(conn net.Conn) { s.createMQTTClient(conn, nil) }, nil)\n\ts.mu.Unlock()\n}\n\n// This is similar to createClient() but has some modifications specifi to MQTT clients.\n// The comments have been kept to minimum to reduce code size. Check createClient() for\n// more details.\nfunc (s *Server) createMQTTClient(conn net.Conn, ws *websocket) *client {\n\topts := s.getOpts()\n\n\tmaxPay := int32(opts.MaxPayload)\n\tmaxSubs := int32(opts.MaxSubs)\n\tif maxSubs == 0 {\n\t\tmaxSubs = -1\n\t}\n\tnow := time.Now()\n\n\tmqtt := &mqtt{\n\t\trejectQoS2Pub:    opts.MQTT.rejectQoS2Pub,\n\t\tdowngradeQoS2Sub: opts.MQTT.downgradeQoS2Sub,\n\t}\n\tc := &client{srv: s, nc: conn, mpay: maxPay, msubs: maxSubs, start: now, last: now, mqtt: mqtt, ws: ws}\n\tc.headers = true\n\tc.mqtt.pp = &mqttPublish{}\n\t// MQTT clients don't send NATS CONNECT protocols. So make it an \"echo\"\n\t// client, but disable verbose and pedantic (by not setting them).\n\tc.opts.Echo = true\n\n\tc.registerWithAccount(s.globalAccount())\n\n\ts.mu.Lock()\n\t// Check auth, override if applicable.\n\tauthRequired := s.info.AuthRequired || s.mqtt.authOverride\n\ts.totalClients++\n\ts.mu.Unlock()\n\n\tc.mu.Lock()\n\tif authRequired {\n\t\tc.flags.set(expectConnect)\n\t}\n\tc.initClient()\n\tc.Debugf(\"Client connection created\")\n\tc.mu.Unlock()\n\n\ts.mu.Lock()\n\tif !s.isRunning() || s.ldm {\n\t\tif s.isShuttingDown() {\n\t\t\tconn.Close()\n\t\t}\n\t\ts.mu.Unlock()\n\t\treturn c\n\t}\n\n\tif opts.MaxConn > 0 && len(s.clients) >= opts.MaxConn {\n\t\ts.mu.Unlock()\n\t\tc.maxConnExceeded()\n\t\treturn nil\n\t}\n\ts.clients[c.cid] = c\n\n\t// Websocket TLS handshake is already done when getting to this function.\n\ttlsRequired := opts.MQTT.TLSConfig != nil && ws == nil\n\ts.mu.Unlock()\n\n\tc.mu.Lock()\n\n\t// In case connection has already been closed\n\tif c.isClosed() {\n\t\tc.mu.Unlock()\n\t\tc.closeConnection(WriteError)\n\t\treturn nil\n\t}\n\n\tvar pre []byte\n\tif tlsRequired && opts.AllowNonTLS {\n\t\tpre = make([]byte, 4)\n\t\tc.nc.SetReadDeadline(time.Now().Add(secondsToDuration(opts.MQTT.TLSTimeout)))\n\t\tn, _ := io.ReadFull(c.nc, pre[:])\n\t\tc.nc.SetReadDeadline(time.Time{})\n\t\tpre = pre[:n]\n\t\tif n > 0 && pre[0] == 0x16 {\n\t\t\ttlsRequired = true\n\t\t} else {\n\t\t\ttlsRequired = false\n\t\t}\n\t}\n\n\tif tlsRequired {\n\t\tif len(pre) > 0 {\n\t\t\tc.nc = &tlsMixConn{c.nc, bytes.NewBuffer(pre)}\n\t\t\tpre = nil\n\t\t}\n\n\t\t// Perform server-side TLS handshake.\n\t\tif err := c.doTLSServerHandshake(tlsHandshakeMQTT, opts.MQTT.TLSConfig, opts.MQTT.TLSTimeout, opts.MQTT.TLSPinnedCerts); err != nil {\n\t\t\tc.mu.Unlock()\n\t\t\treturn nil\n\t\t}\n\t}\n\n\tif authRequired {\n\t\ttimeout := opts.AuthTimeout\n\t\t// Possibly override with MQTT specific value.\n\t\tif opts.MQTT.AuthTimeout != 0 {\n\t\t\ttimeout = opts.MQTT.AuthTimeout\n\t\t}\n\t\tc.setAuthTimer(secondsToDuration(timeout))\n\t}\n\n\t// No Ping timer for MQTT clients...\n\n\ts.startGoRoutine(func() { c.readLoop(pre) })\n\ts.startGoRoutine(func() { c.writeLoop() })\n\n\tif tlsRequired {\n\t\tc.Debugf(\"TLS handshake complete\")\n\t\tcs := c.nc.(*tls.Conn).ConnectionState()\n\t\tc.Debugf(\"TLS version %s, cipher suite %s\", tlsVersion(cs.Version), tlsCipher(cs.CipherSuite))\n\t}\n\n\tc.mu.Unlock()\n\n\treturn c\n}\n\n// Given the mqtt options, we check if any auth configuration\n// has been provided. If so, possibly create users/nkey users and\n// store them in s.mqtt.users/nkeys.\n// Also update a boolean that indicates if auth is required for\n// mqtt clients.\n// Server lock is held on entry.\nfunc (s *Server) mqttConfigAuth(opts *MQTTOpts) {\n\tmqtt := &s.mqtt\n\t// If any of those is specified, we consider that there is an override.\n\tmqtt.authOverride = opts.Username != _EMPTY_ || opts.Token != _EMPTY_ || opts.NoAuthUser != _EMPTY_\n}\n\n// Validate the mqtt related options.\nfunc validateMQTTOptions(o *Options) error {\n\tmo := &o.MQTT\n\t// If no port is defined, we don't care about other options\n\tif mo.Port == 0 {\n\t\treturn nil\n\t}\n\t// We have to force the server name to be explicitly set and be unique when\n\t// in cluster mode.\n\tif o.ServerName == _EMPTY_ && (o.Cluster.Port != 0 || o.Gateway.Port != 0) {\n\t\treturn errMQTTServerNameMustBeSet\n\t}\n\t// If there is a NoAuthUser, we need to have Users defined and\n\t// the user to be present.\n\tif mo.NoAuthUser != _EMPTY_ {\n\t\tif err := validateNoAuthUser(o, mo.NoAuthUser); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\t// Token/Username not possible if there are users/nkeys\n\tif len(o.Users) > 0 || len(o.Nkeys) > 0 {\n\t\tif mo.Username != _EMPTY_ {\n\t\t\treturn errMQTTUserMixWithUsersNKeys\n\t\t}\n\t\tif mo.Token != _EMPTY_ {\n\t\t\treturn errMQTTTokenMixWIthUsersNKeys\n\t\t}\n\t}\n\tif mo.AckWait < 0 {\n\t\treturn errMQTTAckWaitMustBePositive\n\t}\n\tif mo.JSAPITimeout < 0 {\n\t\treturn errMQTTJSAPITimeoutMustBePositive\n\t}\n\t// If strictly standalone and there is no JS enabled, then it won't work...\n\t// For leafnodes, we could either have remote(s) and it would be ok, or no\n\t// remote but accept from a remote side that has \"hub\" property set, which\n\t// then would ok too. So we fail only if we have no leafnode config at all.\n\tif !o.JetStream && o.Cluster.Port == 0 && o.Gateway.Port == 0 &&\n\t\to.LeafNode.Port == 0 && len(o.LeafNode.Remotes) == 0 {\n\t\treturn errMQTTStandaloneNeedsJetStream\n\t}\n\tif err := validatePinnedCerts(mo.TLSPinnedCerts); err != nil {\n\t\treturn fmt.Errorf(\"mqtt: %v\", err)\n\t}\n\tif mo.ConsumerReplicas > 0 && mo.StreamReplicas > 0 && mo.ConsumerReplicas > mo.StreamReplicas {\n\t\treturn fmt.Errorf(\"mqtt: consumer_replicas (%v) cannot be higher than stream_replicas (%v)\",\n\t\t\tmo.ConsumerReplicas, mo.StreamReplicas)\n\t}\n\treturn nil\n}\n\n// Returns true if this connection is from a MQTT client.\n// Lock held on entry.\nfunc (c *client) isMqtt() bool {\n\treturn c.mqtt != nil\n}\n\n// If this is an MQTT client, returns the session client ID,\n// otherwise returns the empty string.\n// Lock held on entry\nfunc (c *client) getMQTTClientID() string {\n\tif !c.isMqtt() {\n\t\treturn _EMPTY_\n\t}\n\treturn c.mqtt.cid\n}\n\n// Parse protocols inside the given buffer.\n// This is invoked from the readLoop.\nfunc (c *client) mqttParse(buf []byte) error {\n\tc.mu.Lock()\n\ts := c.srv\n\ttrace := c.trace\n\tconnected := c.flags.isSet(connectReceived)\n\tmqtt := c.mqtt\n\tr := mqtt.r\n\tvar rd time.Duration\n\tif mqtt.cp != nil {\n\t\trd = mqtt.cp.rd\n\t\tif rd > 0 {\n\t\t\tr.reader.SetReadDeadline(time.Time{})\n\t\t}\n\t}\n\thasMappings := c.in.flags.isSet(hasMappings)\n\tc.mu.Unlock()\n\n\tr.reset(buf)\n\n\tvar err error\n\tvar b byte\n\tvar pl int\n\tvar complete bool\n\n\tfor err == nil && r.hasMore() {\n\n\t\t// Keep track of the starting of the packet, in case we have a partial\n\t\tr.pstart = r.pos\n\n\t\t// Read packet type and flags\n\t\tif b, err = r.readByte(\"packet type\"); err != nil {\n\t\t\tbreak\n\t\t}\n\n\t\t// Packet type\n\t\tpt := b & mqttPacketMask\n\n\t\t// If client was not connected yet, the first packet must be\n\t\t// a mqttPacketConnect otherwise we fail the connection.\n\t\tif !connected && pt != mqttPacketConnect {\n\t\t\t// If the buffer indicates that it may be a websocket handshake\n\t\t\t// but the client is not websocket, it means that the client\n\t\t\t// connected to the MQTT port instead of the Websocket port.\n\t\t\tif bytes.HasPrefix(buf, []byte(\"GET \")) && !c.isWebsocket() {\n\t\t\t\terr = errMQTTNotWebsocketPort\n\t\t\t} else {\n\t\t\t\terr = fmt.Errorf(\"the first packet should be a CONNECT (%v), got %v\", mqttPacketConnect, pt)\n\t\t\t}\n\t\t\tbreak\n\t\t}\n\n\t\tpl, complete, err = r.readPacketLen()\n\t\tif err != nil || !complete {\n\t\t\tbreak\n\t\t}\n\n\t\tswitch pt {\n\t\t// Packets that we receive back when we act as the \"sender\": PUBACK,\n\t\t// PUBREC, PUBCOMP.\n\t\tcase mqttPacketPubAck:\n\t\t\tvar pi uint16\n\t\t\tpi, err = mqttParsePIPacket(r)\n\t\t\tif trace {\n\t\t\t\tc.traceInOp(\"PUBACK\", errOrTrace(err, fmt.Sprintf(\"pi=%v\", pi)))\n\t\t\t}\n\t\t\tif err == nil {\n\t\t\t\terr = c.mqttProcessPubAck(pi)\n\t\t\t}\n\n\t\tcase mqttPacketPubRec:\n\t\t\tvar pi uint16\n\t\t\tpi, err = mqttParsePIPacket(r)\n\t\t\tif trace {\n\t\t\t\tc.traceInOp(\"PUBREC\", errOrTrace(err, fmt.Sprintf(\"pi=%v\", pi)))\n\t\t\t}\n\t\t\tif err == nil {\n\t\t\t\terr = c.mqttProcessPubRec(pi)\n\t\t\t}\n\n\t\tcase mqttPacketPubComp:\n\t\t\tvar pi uint16\n\t\t\tpi, err = mqttParsePIPacket(r)\n\t\t\tif trace {\n\t\t\t\tc.traceInOp(\"PUBCOMP\", errOrTrace(err, fmt.Sprintf(\"pi=%v\", pi)))\n\t\t\t}\n\t\t\tif err == nil {\n\t\t\t\tc.mqttProcessPubComp(pi)\n\t\t\t}\n\n\t\t// Packets where we act as the \"receiver\": PUBLISH, PUBREL, SUBSCRIBE, UNSUBSCRIBE.\n\t\tcase mqttPacketPub:\n\t\t\tpp := c.mqtt.pp\n\t\t\tpp.flags = b & mqttPacketFlagMask\n\t\t\terr = c.mqttParsePub(r, pl, pp, hasMappings)\n\t\t\tif trace {\n\t\t\t\tc.traceInOp(\"PUBLISH\", errOrTrace(err, mqttPubTrace(pp)))\n\t\t\t\tif err == nil {\n\t\t\t\t\tc.mqttTraceMsg(pp.msg)\n\t\t\t\t}\n\t\t\t}\n\t\t\tif err == nil {\n\t\t\t\terr = s.mqttProcessPub(c, pp, trace)\n\t\t\t}\n\n\t\tcase mqttPacketPubRel:\n\t\t\tvar pi uint16\n\t\t\tpi, err = mqttParsePIPacket(r)\n\t\t\tif trace {\n\t\t\t\tc.traceInOp(\"PUBREL\", errOrTrace(err, fmt.Sprintf(\"pi=%v\", pi)))\n\t\t\t}\n\t\t\tif err == nil {\n\t\t\t\terr = s.mqttProcessPubRel(c, pi, trace)\n\t\t\t}\n\n\t\tcase mqttPacketSub:\n\t\t\tvar pi uint16 // packet identifier\n\t\t\tvar filters []*mqttFilter\n\t\t\tvar subs []*subscription\n\t\t\tpi, filters, err = c.mqttParseSubs(r, b, pl)\n\t\t\tif trace {\n\t\t\t\tc.traceInOp(\"SUBSCRIBE\", errOrTrace(err, mqttSubscribeTrace(pi, filters)))\n\t\t\t}\n\t\t\tif err == nil {\n\t\t\t\tsubs, err = c.mqttProcessSubs(filters)\n\t\t\t\tif err == nil && trace {\n\t\t\t\t\tc.traceOutOp(\"SUBACK\", []byte(fmt.Sprintf(\"pi=%v\", pi)))\n\t\t\t\t}\n\t\t\t}\n\t\t\tif err == nil {\n\t\t\t\tc.mqttEnqueueSubAck(pi, filters)\n\t\t\t\tc.mqttSendRetainedMsgsToNewSubs(subs)\n\t\t\t}\n\n\t\tcase mqttPacketUnsub:\n\t\t\tvar pi uint16 // packet identifier\n\t\t\tvar filters []*mqttFilter\n\t\t\tpi, filters, err = c.mqttParseUnsubs(r, b, pl)\n\t\t\tif trace {\n\t\t\t\tc.traceInOp(\"UNSUBSCRIBE\", errOrTrace(err, mqttUnsubscribeTrace(pi, filters)))\n\t\t\t}\n\t\t\tif err == nil {\n\t\t\t\terr = c.mqttProcessUnsubs(filters)\n\t\t\t\tif err == nil && trace {\n\t\t\t\t\tc.traceOutOp(\"UNSUBACK\", []byte(fmt.Sprintf(\"pi=%v\", pi)))\n\t\t\t\t}\n\t\t\t}\n\t\t\tif err == nil {\n\t\t\t\tc.mqttEnqueueUnsubAck(pi)\n\t\t\t}\n\n\t\t// Packets that we get both as a receiver and sender: PING, CONNECT, DISCONNECT\n\t\tcase mqttPacketPing:\n\t\t\tif trace {\n\t\t\t\tc.traceInOp(\"PINGREQ\", nil)\n\t\t\t}\n\t\t\tc.mqttEnqueuePingResp()\n\t\t\tif trace {\n\t\t\t\tc.traceOutOp(\"PINGRESP\", nil)\n\t\t\t}\n\n\t\tcase mqttPacketConnect:\n\t\t\t// It is an error to receive a second connect packet\n\t\t\tif connected {\n\t\t\t\terr = errMQTTSecondConnectPacket\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tvar rc byte\n\t\t\tvar cp *mqttConnectProto\n\t\t\tvar sessp bool\n\t\t\trc, cp, err = c.mqttParseConnect(r, hasMappings)\n\t\t\t// Add the client id to the client's string, regardless of error.\n\t\t\t// We may still get the client_id if the call above fails somewhere\n\t\t\t// after parsing the client ID itself.\n\t\t\tc.ncs.Store(fmt.Sprintf(\"%s - %q\", c, c.mqtt.cid))\n\t\t\tif trace && cp != nil {\n\t\t\t\tc.traceInOp(\"CONNECT\", errOrTrace(err, c.mqttConnectTrace(cp)))\n\t\t\t}\n\t\t\tif rc != 0 {\n\t\t\t\tc.mqttEnqueueConnAck(rc, sessp)\n\t\t\t\tif trace {\n\t\t\t\t\tc.traceOutOp(\"CONNACK\", []byte(fmt.Sprintf(\"sp=%v rc=%v\", sessp, rc)))\n\t\t\t\t}\n\t\t\t} else if err == nil {\n\t\t\t\tif err = s.mqttProcessConnect(c, cp, trace); err != nil {\n\t\t\t\t\terr = fmt.Errorf(\"unable to connect: %v\", err)\n\t\t\t\t} else {\n\t\t\t\t\t// Add this debug statement so users running in Debug mode\n\t\t\t\t\t// will have the client id printed here for the first time.\n\t\t\t\t\tc.Debugf(\"Client connected\")\n\t\t\t\t\tconnected = true\n\t\t\t\t\trd = cp.rd\n\t\t\t\t}\n\t\t\t}\n\n\t\tcase mqttPacketDisconnect:\n\t\t\tif trace {\n\t\t\t\tc.traceInOp(\"DISCONNECT\", nil)\n\t\t\t}\n\t\t\t// Normal disconnect, we need to discard the will.\n\t\t\t// Spec [MQTT-3.1.2-8]\n\t\t\tc.mu.Lock()\n\t\t\tif c.mqtt.cp != nil {\n\t\t\t\tc.mqtt.cp.will = nil\n\t\t\t}\n\t\t\tc.mu.Unlock()\n\t\t\ts.mqttHandleClosedClient(c)\n\t\t\tc.closeConnection(ClientClosed)\n\t\t\treturn nil\n\n\t\tdefault:\n\t\t\terr = fmt.Errorf(\"received unknown packet type %d\", pt>>4)\n\t\t}\n\t}\n\tif err == nil && rd > 0 {\n\t\tr.reader.SetReadDeadline(time.Now().Add(rd))\n\t}\n\treturn err\n}\n\nfunc (c *client) mqttTraceMsg(msg []byte) {\n\tmaxTrace := c.srv.getOpts().MaxTracedMsgLen\n\tif maxTrace > 0 && len(msg) > maxTrace {\n\t\tc.Tracef(\"<<- MSG_PAYLOAD: [\\\"%s...\\\"]\", msg[:maxTrace])\n\t} else {\n\t\tc.Tracef(\"<<- MSG_PAYLOAD: [%q]\", msg)\n\t}\n}\n\n// The MQTT client connection has been closed, or the DISCONNECT packet was received.\n// For a \"clean\" session, we will delete the session, otherwise, simply removing\n// the binding. We will also send the \"will\" message if applicable.\n//\n// Runs from the client's readLoop.\n// No lock held on entry.\nfunc (s *Server) mqttHandleClosedClient(c *client) {\n\tc.mu.Lock()\n\tasm := c.mqtt.asm\n\tsess := c.mqtt.sess\n\tc.mu.Unlock()\n\n\t// If asm or sess are nil, it means that we have failed a client\n\t// before it was associated with a session, so nothing more to do.\n\tif asm == nil || sess == nil {\n\t\treturn\n\t}\n\n\t// Add this session to the locked map for the rest of the execution.\n\tif err := asm.lockSession(sess, c); err != nil {\n\t\treturn\n\t}\n\tdefer asm.unlockSession(sess)\n\n\tasm.mu.Lock()\n\t// Clear the client from the session, but session may stay.\n\tsess.mu.Lock()\n\tsess.c = nil\n\tdoClean := sess.clean\n\tsess.mu.Unlock()\n\t// If it was a clean session, then we remove from the account manager,\n\t// and we will call clear() outside of any lock.\n\tif doClean {\n\t\tasm.removeSession(sess, false)\n\t}\n\t// Remove in case it was in the flappers map.\n\tasm.removeSessFromFlappers(sess.id)\n\tasm.mu.Unlock()\n\n\t// This needs to be done outside of any lock.\n\tif doClean {\n\t\tif err := sess.clear(true); err != nil {\n\t\t\tc.Errorf(err.Error())\n\t\t}\n\t}\n\n\t// Now handle the \"will\". This function will be a no-op if there is no \"will\" to send.\n\ts.mqttHandleWill(c)\n}\n\n// Updates the MaxAckPending for all MQTT sessions, updating the\n// JetStream consumers and updating their max ack pending and forcing\n// a expiration of pending messages.\n//\n// Runs from a server configuration reload routine.\n// No lock held on entry.\nfunc (s *Server) mqttUpdateMaxAckPending(newmaxp uint16) {\n\tmsm := &s.mqtt.sessmgr\n\ts.accounts.Range(func(k, _ any) bool {\n\t\taccName := k.(string)\n\t\tmsm.mu.RLock()\n\t\tasm := msm.sessions[accName]\n\t\tmsm.mu.RUnlock()\n\t\tif asm == nil {\n\t\t\t// Move to next account\n\t\t\treturn true\n\t\t}\n\t\tasm.mu.RLock()\n\t\tfor _, sess := range asm.sessions {\n\t\t\tsess.mu.Lock()\n\t\t\tsess.maxp = newmaxp\n\t\t\tsess.mu.Unlock()\n\t\t}\n\t\tasm.mu.RUnlock()\n\t\treturn true\n\t})\n}\n\nfunc (s *Server) mqttGetJSAForAccount(acc string) *mqttJSA {\n\tsm := &s.mqtt.sessmgr\n\n\tsm.mu.RLock()\n\tasm := sm.sessions[acc]\n\tsm.mu.RUnlock()\n\n\tif asm == nil {\n\t\treturn nil\n\t}\n\n\tasm.mu.RLock()\n\tjsa := &asm.jsa\n\tasm.mu.RUnlock()\n\treturn jsa\n}\n\nfunc (s *Server) mqttStoreQoSMsgForAccountOnNewSubject(hdr int, msg []byte, acc, subject string) {\n\tif s == nil || hdr <= 0 {\n\t\treturn\n\t}\n\th := mqttParsePublishNATSHeader(msg[:hdr])\n\tif h == nil || h.qos == 0 {\n\t\treturn\n\t}\n\tjsa := s.mqttGetJSAForAccount(acc)\n\tif jsa == nil {\n\t\treturn\n\t}\n\tjsa.storeMsg(mqttStreamSubjectPrefix+subject, hdr, msg)\n}\n\nfunc mqttParsePublishNATSHeader(headerBytes []byte) *mqttParsedPublishNATSHeader {\n\tif len(headerBytes) == 0 {\n\t\treturn nil\n\t}\n\n\tpubValue := getHeader(mqttNatsHeader, headerBytes)\n\tif len(pubValue) == 0 {\n\t\treturn nil\n\t}\n\treturn &mqttParsedPublishNATSHeader{\n\t\tqos:     pubValue[0] - '0',\n\t\tsubject: getHeader(mqttNatsHeaderSubject, headerBytes),\n\t\tmapped:  getHeader(mqttNatsHeaderMapped, headerBytes),\n\t}\n}\n\nfunc mqttParsePubRelNATSHeader(headerBytes []byte) uint16 {\n\tif len(headerBytes) == 0 {\n\t\treturn 0\n\t}\n\n\tpubrelValue := getHeader(mqttNatsPubRelHeader, headerBytes)\n\tif len(pubrelValue) == 0 {\n\t\treturn 0\n\t}\n\tpi, _ := strconv.Atoi(string(pubrelValue))\n\treturn uint16(pi)\n}\n\n// Returns the MQTT sessions manager for a given account.\n// If new, creates the required JetStream streams/consumers\n// for handling of sessions and messages.\nfunc (s *Server) getOrCreateMQTTAccountSessionManager(c *client) (*mqttAccountSessionManager, error) {\n\tsm := &s.mqtt.sessmgr\n\n\tc.mu.Lock()\n\tacc := c.acc\n\tc.mu.Unlock()\n\taccName := acc.GetName()\n\n\tsm.mu.RLock()\n\tasm, ok := sm.sessions[accName]\n\tsm.mu.RUnlock()\n\n\tif ok {\n\t\treturn asm, nil\n\t}\n\n\t// We will pass the quitCh to the account session manager if we happen to create it.\n\ts.mu.Lock()\n\tquitCh := s.quitCh\n\ts.mu.Unlock()\n\n\t// Not found, now take the write lock and check again\n\tsm.mu.Lock()\n\tdefer sm.mu.Unlock()\n\tasm, ok = sm.sessions[accName]\n\tif ok {\n\t\treturn asm, nil\n\t}\n\t// Need to create one here.\n\tasm, err := s.mqttCreateAccountSessionManager(acc, quitCh)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tsm.sessions[accName] = asm\n\treturn asm, nil\n}\n\n// Creates JS streams/consumers for handling of sessions and messages for this account.\n//\n// Global session manager lock is held on entry.\nfunc (s *Server) mqttCreateAccountSessionManager(acc *Account, quitCh chan struct{}) (*mqttAccountSessionManager, error) {\n\tvar err error\n\n\taccName := acc.GetName()\n\n\topts := s.getOpts()\n\tc := s.createInternalAccountClient()\n\tc.acc = acc\n\n\tid := s.NodeName()\n\n\tmqttJSAPITimeout := opts.MQTT.JSAPITimeout\n\tif mqttJSAPITimeout == 0 {\n\t\tmqttJSAPITimeout = mqttDefaultJSAPITimeout\n\t}\n\n\treplicas := opts.MQTT.StreamReplicas\n\tif replicas <= 0 {\n\t\treplicas = s.mqttDetermineReplicas()\n\t}\n\tqname := fmt.Sprintf(\"[ACC:%s] MQTT \", accName)\n\tas := &mqttAccountSessionManager{\n\t\tsessions:   make(map[string]*mqttSession),\n\t\tsessByHash: make(map[string]*mqttSession),\n\t\tsessLocked: make(map[string]struct{}),\n\t\tflappers:   make(map[string]int64),\n\t\tjsa: mqttJSA{\n\t\t\tid:      id,\n\t\t\tc:       c,\n\t\t\trplyr:   mqttJSARepliesPrefix + id + \".\",\n\t\t\tsendq:   newIPQueue[*mqttJSPubMsg](s, qname+\"send\"),\n\t\t\tnuid:    nuid.New(),\n\t\t\tquitCh:  quitCh,\n\t\t\ttimeout: mqttJSAPITimeout,\n\t\t},\n\t}\n\tif !testDisableRMSCache {\n\t\tas.rmsCache = &sync.Map{}\n\t}\n\t// TODO record domain name in as here\n\n\t// The domain to communicate with may be required for JS calls.\n\t// Search from specific (per account setting) to generic (mqtt setting)\n\tif opts.JsAccDefaultDomain != nil {\n\t\tif d, ok := opts.JsAccDefaultDomain[accName]; ok {\n\t\t\tif d != _EMPTY_ {\n\t\t\t\tas.jsa.domain = d\n\t\t\t}\n\t\t\tas.jsa.domainSet = true\n\t\t}\n\t\t// in case domain was set to empty, check if there are more generic domain overwrites\n\t}\n\tif as.jsa.domain == _EMPTY_ {\n\t\tif d := opts.MQTT.JsDomain; d != _EMPTY_ {\n\t\t\tas.jsa.domain = d\n\t\t\tas.jsa.domainSet = true\n\t\t}\n\t}\n\t// We need to include the domain in the subject prefix used to store sessions in the $MQTT_sess stream.\n\tif as.jsa.domainSet {\n\t\tif as.jsa.domain != _EMPTY_ {\n\t\t\tas.domainTk = as.jsa.domain + \".\"\n\t\t}\n\t} else if d := s.getOpts().JetStreamDomain; d != _EMPTY_ {\n\t\tas.domainTk = d + \".\"\n\t}\n\tif as.jsa.domainSet {\n\t\ts.Noticef(\"Creating MQTT streams/consumers with replicas %v for account %q in domain %q\", replicas, accName, as.jsa.domain)\n\t} else {\n\t\ts.Noticef(\"Creating MQTT streams/consumers with replicas %v for account %q\", replicas, accName)\n\t}\n\n\tvar subs []*subscription\n\tvar success bool\n\tcloseCh := make(chan struct{})\n\n\tdefer func() {\n\t\tif success {\n\t\t\treturn\n\t\t}\n\t\tfor _, sub := range subs {\n\t\t\tc.processUnsub(sub.sid)\n\t\t}\n\t\tclose(closeCh)\n\t}()\n\n\t// We create all subscriptions before starting the go routine that will do\n\t// sends otherwise we could get races.\n\t// Note that using two different clients (one for the subs, one for the\n\t// sends) would cause other issues such as registration of recent subs in\n\t// the \"sub\" client would be invisible to the check for GW routed replies\n\t// (shouldMapReplyForGatewaySend) since the client there would be the \"sender\".\n\n\tjsa := &as.jsa\n\tsid := int64(1)\n\t// This is a subscription that will process all JS API replies. We could split to\n\t// individual subscriptions if needed, but since there is a bit of common code,\n\t// that seemed like a good idea to be all in one place.\n\tif err := as.createSubscription(jsa.rplyr+\">\",\n\t\tas.processJSAPIReplies, &sid, &subs); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// We will listen for replies to session persist requests so that we can\n\t// detect the use of a session with the same client ID anywhere in the cluster.\n\t//   `$MQTT.JSA.{js-id}.SP.{client-id-hash}.{uuid}`\n\tif err := as.createSubscription(mqttJSARepliesPrefix+\"*.\"+mqttJSASessPersist+\".*.*\",\n\t\tas.processSessionPersist, &sid, &subs); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// We create the subscription on \"$MQTT.sub.<nuid>\" to limit the subjects\n\t// that a user would allow permissions on.\n\trmsubj := mqttSubPrefix + nuid.Next()\n\tif err := as.createSubscription(rmsubj, as.processRetainedMsg, &sid, &subs); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Create a subscription to be notified of retained messages delete requests.\n\trmdelsubj := mqttJSARepliesPrefix + \"*.\" + mqttJSARetainedMsgDel\n\tif err := as.createSubscription(rmdelsubj, as.processRetainedMsgDel, &sid, &subs); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// No more creation of subscriptions past this point otherwise RACEs may happen.\n\n\t// Start the go routine that will send JS API requests.\n\ts.startGoRoutine(func() {\n\t\tdefer s.grWG.Done()\n\t\tas.sendJSAPIrequests(s, c, accName, closeCh)\n\t})\n\n\t// Start the go routine that will clean up cached retained messages that expired.\n\tif as.rmsCache != nil {\n\t\ts.startGoRoutine(func() {\n\t\t\tdefer s.grWG.Done()\n\t\t\tas.cleanupRetainedMessageCache(s, closeCh)\n\t\t})\n\t}\n\n\tlookupStream := func(stream, txt string) (*StreamInfo, error) {\n\t\tsi, err := jsa.lookupStream(stream)\n\t\tif err != nil {\n\t\t\tif IsNatsErr(err, JSStreamNotFoundErr) {\n\t\t\t\treturn nil, nil\n\t\t\t}\n\t\t\treturn nil, fmt.Errorf(\"lookup %s stream for account %q: %v\", txt, accName, err)\n\t\t}\n\t\tif opts.MQTT.StreamReplicas == 0 {\n\t\t\treturn si, nil\n\t\t}\n\t\tsr := 1\n\t\tif si.Cluster != nil {\n\t\t\tsr += len(si.Cluster.Replicas)\n\t\t}\n\t\tif replicas != sr {\n\t\t\ts.Warnf(\"MQTT %s stream replicas mismatch: current is %v but configuration is %v for '%s > %s'\",\n\t\t\t\ttxt, sr, replicas, accName, stream)\n\t\t}\n\t\treturn si, nil\n\t}\n\n\tif si, err := lookupStream(mqttSessStreamName, \"sessions\"); err != nil {\n\t\treturn nil, err\n\t} else if si == nil {\n\t\t// Create the stream for the sessions.\n\t\tcfg := &StreamConfig{\n\t\t\tName:       mqttSessStreamName,\n\t\t\tSubjects:   []string{mqttSessStreamSubjectPrefix + as.domainTk + \">\"},\n\t\t\tStorage:    FileStorage,\n\t\t\tRetention:  LimitsPolicy,\n\t\t\tReplicas:   replicas,\n\t\t\tMaxMsgsPer: 1,\n\t\t}\n\t\tif _, created, err := jsa.createStream(cfg); err == nil && created {\n\t\t\tas.transferUniqueSessStreamsToMuxed(s)\n\t\t} else if isErrorOtherThan(err, JSStreamNameExistErr) {\n\t\t\treturn nil, fmt.Errorf(\"create sessions stream for account %q: %v\", accName, err)\n\t\t}\n\t}\n\n\tif si, err := lookupStream(mqttStreamName, \"messages\"); err != nil {\n\t\treturn nil, err\n\t} else if si == nil {\n\t\t// Create the stream for the messages.\n\t\tcfg := &StreamConfig{\n\t\t\tName:      mqttStreamName,\n\t\t\tSubjects:  []string{mqttStreamSubjectPrefix + \">\"},\n\t\t\tStorage:   FileStorage,\n\t\t\tRetention: InterestPolicy,\n\t\t\tReplicas:  replicas,\n\t\t}\n\t\tif _, _, err := jsa.createStream(cfg); isErrorOtherThan(err, JSStreamNameExistErr) {\n\t\t\treturn nil, fmt.Errorf(\"create messages stream for account %q: %v\", accName, err)\n\t\t}\n\t}\n\n\tif si, err := lookupStream(mqttQoS2IncomingMsgsStreamName, \"QoS2 incoming messages\"); err != nil {\n\t\treturn nil, err\n\t} else if si == nil {\n\t\t// Create the stream for the incoming QoS2 messages that have not been\n\t\t// PUBREL-ed by the sender. Subject is\n\t\t// \"$MQTT.qos2.<session>.<PI>\", the .PI is to achieve exactly\n\t\t// once for each PI.\n\t\tcfg := &StreamConfig{\n\t\t\tName:          mqttQoS2IncomingMsgsStreamName,\n\t\t\tSubjects:      []string{mqttQoS2IncomingMsgsStreamSubjectPrefix + \">\"},\n\t\t\tStorage:       FileStorage,\n\t\t\tRetention:     LimitsPolicy,\n\t\t\tDiscard:       DiscardNew,\n\t\t\tMaxMsgsPer:    1,\n\t\t\tDiscardNewPer: true,\n\t\t\tReplicas:      replicas,\n\t\t}\n\t\tif _, _, err := jsa.createStream(cfg); isErrorOtherThan(err, JSStreamNameExistErr) {\n\t\t\treturn nil, fmt.Errorf(\"create QoS2 incoming messages stream for account %q: %v\", accName, err)\n\t\t}\n\t}\n\n\tif si, err := lookupStream(mqttOutStreamName, \"QoS2 outgoing PUBREL\"); err != nil {\n\t\treturn nil, err\n\t} else if si == nil {\n\t\t// Create the stream for the incoming QoS2 messages that have not been\n\t\t// PUBREL-ed by the sender. NATS messages are submitted as\n\t\t// \"$MQTT.pubrel.<session hash>\"\n\t\tcfg := &StreamConfig{\n\t\t\tName:      mqttOutStreamName,\n\t\t\tSubjects:  []string{mqttOutSubjectPrefix + \">\"},\n\t\t\tStorage:   FileStorage,\n\t\t\tRetention: InterestPolicy,\n\t\t\tReplicas:  replicas,\n\t\t}\n\t\tif _, _, err := jsa.createStream(cfg); isErrorOtherThan(err, JSStreamNameExistErr) {\n\t\t\treturn nil, fmt.Errorf(\"create QoS2 outgoing PUBREL stream for account %q: %v\", accName, err)\n\t\t}\n\t}\n\n\t// This is the only case where we need \"si\" after lookup/create\n\tneedToTransfer := true\n\tsi, err := lookupStream(mqttRetainedMsgsStreamName, \"retained messages\")\n\tswitch {\n\tcase err != nil:\n\t\treturn nil, err\n\n\tcase si == nil:\n\t\t// Create the stream for retained messages.\n\t\tcfg := &StreamConfig{\n\t\t\tName:       mqttRetainedMsgsStreamName,\n\t\t\tSubjects:   []string{mqttRetainedMsgsStreamSubject + \">\"},\n\t\t\tStorage:    FileStorage,\n\t\t\tRetention:  LimitsPolicy,\n\t\t\tReplicas:   replicas,\n\t\t\tMaxMsgsPer: 1,\n\t\t}\n\t\t// We will need \"si\" outside of this block.\n\t\tsi, _, err = jsa.createStream(cfg)\n\t\tif err != nil {\n\t\t\tif isErrorOtherThan(err, JSStreamNameExistErr) {\n\t\t\t\treturn nil, fmt.Errorf(\"create retained messages stream for account %q: %v\", accName, err)\n\t\t\t}\n\t\t\t// Suppose we had a race and the stream was actually created by another\n\t\t\t// node, we really need \"si\" after that, so lookup the stream again here.\n\t\t\tsi, err = lookupStream(mqttRetainedMsgsStreamName, \"retained messages\")\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t}\n\t\tneedToTransfer = false\n\n\tdefault:\n\t\tneedToTransfer = si.Config.MaxMsgsPer != 1\n\t}\n\n\t// Doing this check outside of above if/else due to possible race when\n\t// creating the stream.\n\twantedSubj := mqttRetainedMsgsStreamSubject + \">\"\n\tif len(si.Config.Subjects) != 1 || si.Config.Subjects[0] != wantedSubj {\n\t\t// Update only the Subjects at this stage, not MaxMsgsPer yet.\n\t\tsi.Config.Subjects = []string{wantedSubj}\n\t\tif si, err = jsa.updateStream(&si.Config); err != nil {\n\t\t\treturn nil, fmt.Errorf(\"failed to update stream config: %w\", err)\n\t\t}\n\t}\n\n\ttransferRMS := func() error {\n\t\tif !needToTransfer {\n\t\t\treturn nil\n\t\t}\n\n\t\tas.transferRetainedToPerKeySubjectStream(s)\n\n\t\t// We need another lookup to have up-to-date si.State values in order\n\t\t// to load all retained messages.\n\t\tsi, err = lookupStream(mqttRetainedMsgsStreamName, \"retained messages\")\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tneedToTransfer = false\n\t\treturn nil\n\t}\n\n\t// Attempt to transfer all \"single subject\" retained messages to new\n\t// subjects. It may fail, will log its own error; ignore it the first time\n\t// and proceed to updating MaxMsgsPer. Then we invoke transferRMS() again,\n\t// which will get another chance to resolve the error; if not we bail there.\n\tif err = transferRMS(); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Now, if the stream does not have MaxMsgsPer set to 1, and there are no\n\t// more messages on the single $MQTT.rmsgs subject, update the stream again.\n\tif si.Config.MaxMsgsPer != 1 {\n\t\tsi.Config.MaxMsgsPer = 1\n\t\t// We will need an up-to-date si, so don't use local variable here.\n\t\tif si, err = jsa.updateStream(&si.Config); err != nil {\n\t\t\treturn nil, fmt.Errorf(\"failed to update stream config: %w\", err)\n\t\t}\n\t}\n\n\t// If we failed the first time, there is now at most one lingering message\n\t// in the old subject. Try again (it will be a NO-OP if succeeded the first\n\t// time).\n\tif err = transferRMS(); err != nil {\n\t\treturn nil, err\n\t}\n\n\tvar lastSeq uint64\n\tvar rmDoneCh chan struct{}\n\tst := si.State\n\tif st.Msgs > 0 {\n\t\tlastSeq = st.LastSeq\n\t\tif lastSeq > 0 {\n\t\t\trmDoneCh = make(chan struct{})\n\t\t\tas.rrmLastSeq = lastSeq\n\t\t\tas.rrmDoneCh = rmDoneCh\n\t\t}\n\t}\n\n\t// Opportunistically delete the old (legacy) consumer, from v2.10.10 and\n\t// before. Ignore any errors that might arise.\n\trmLegacyDurName := mqttRetainedMsgsStreamName + \"_\" + jsa.id\n\tjsa.deleteConsumer(mqttRetainedMsgsStreamName, rmLegacyDurName, true)\n\n\t// Create a new, uniquely names consumer for retained messages for this\n\t// server. The prior one will expire eventually.\n\tccfg := &CreateConsumerRequest{\n\t\tStream: mqttRetainedMsgsStreamName,\n\t\tConfig: ConsumerConfig{\n\t\t\tName:              mqttRetainedMsgsStreamName + \"_\" + nuid.Next(),\n\t\t\tFilterSubject:     mqttRetainedMsgsStreamSubject + \">\",\n\t\t\tDeliverSubject:    rmsubj,\n\t\t\tReplayPolicy:      ReplayInstant,\n\t\t\tAckPolicy:         AckNone,\n\t\t\tInactiveThreshold: 5 * time.Minute,\n\t\t},\n\t}\n\tif _, err := jsa.createEphemeralConsumer(ccfg); err != nil {\n\t\treturn nil, fmt.Errorf(\"create retained messages consumer for account %q: %v\", accName, err)\n\t}\n\n\tif lastSeq > 0 {\n\t\tttl := time.NewTimer(mqttJSAPITimeout)\n\t\tdefer ttl.Stop()\n\n\t\tselect {\n\t\tcase <-rmDoneCh:\n\t\tcase <-ttl.C:\n\t\t\ts.Warnf(\"Timing out waiting to load %v retained messages\", st.Msgs)\n\t\tcase <-quitCh:\n\t\t\treturn nil, ErrServerNotRunning\n\t\t}\n\t}\n\n\t// Set this so that on defer we don't cleanup.\n\tsuccess = true\n\n\treturn as, nil\n}\n\nfunc (s *Server) mqttDetermineReplicas() int {\n\t// If not clustered, then replica will be 1.\n\tif !s.JetStreamIsClustered() {\n\t\treturn 1\n\t}\n\topts := s.getOpts()\n\treplicas := 0\n\tfor _, u := range opts.Routes {\n\t\thost := u.Hostname()\n\t\t// If this is an IP just add one.\n\t\tif net.ParseIP(host) != nil {\n\t\t\treplicas++\n\t\t} else {\n\t\t\taddrs, _ := net.LookupHost(host)\n\t\t\treplicas += len(addrs)\n\t\t}\n\t}\n\tif replicas < 1 {\n\t\treplicas = 1\n\t} else if replicas > 3 {\n\t\treplicas = 3\n\t}\n\treturn replicas\n}\n\n//////////////////////////////////////////////////////////////////////////////\n//\n// JS APIs related functions\n//\n//////////////////////////////////////////////////////////////////////////////\n\nfunc (jsa *mqttJSA) newRequest(kind, subject string, hdr int, msg []byte) (any, error) {\n\treturn jsa.newRequestEx(kind, subject, _EMPTY_, hdr, msg)\n}\n\nfunc (jsa *mqttJSA) prefixDomain(subject string) string {\n\tif jsa.domain != _EMPTY_ {\n\t\t// rewrite js api prefix with domain\n\t\tif sub := strings.TrimPrefix(subject, JSApiPrefix+\".\"); sub != subject {\n\t\t\tsubject = fmt.Sprintf(\"$JS.%s.API.%s\", jsa.domain, sub)\n\t\t}\n\t}\n\treturn subject\n}\n\nfunc (jsa *mqttJSA) newRequestEx(kind, subject, cidHash string, hdr int, msg []byte) (any, error) {\n\tresponses, err := jsa.newRequestExMulti(kind, subject, cidHash, []int{hdr}, [][]byte{msg})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif len(responses) != 1 {\n\t\treturn nil, fmt.Errorf(\"unreachable: invalid number of responses (%d)\", len(responses))\n\t}\n\treturn responses[0].value, nil\n}\n\n// newRequestExMulti sends multiple messages on the same subject and waits for\n// all responses. It returns the same number of responses in the same order as\n// msgs parameter. In case of a timeout it returns an error as well as all\n// responses received as a sparsely populated array, matching msgs, with nils\n// for the values that have not yet been received.\n//\n// Note that each response may represent an error and should be inspected as\n// such by the caller.\nfunc (jsa *mqttJSA) newRequestExMulti(kind, subject, cidHash string, hdrs []int, msgs [][]byte) ([]*mqttJSAResponse, error) {\n\tif len(hdrs) != len(msgs) {\n\t\treturn nil, fmt.Errorf(\"unreachable: invalid number of messages (%d) or header offsets (%d)\", len(msgs), len(hdrs))\n\t}\n\tresponseCh := make(chan *mqttJSAResponse, len(msgs))\n\n\t// Generate and queue all outgoing requests, have all results reported to\n\t// responseCh, and store a map of reply subjects to the original subjects'\n\t// indices.\n\tr2i := map[string]int{}\n\tfor i, msg := range msgs {\n\t\thdr := hdrs[i]\n\t\tvar sb strings.Builder\n\t\t// Either we use nuid.Next() which uses a global lock, or our own nuid object, but\n\t\t// then it needs to be \"write\" protected. This approach will reduce across account\n\t\t// contention since we won't use the global nuid's lock.\n\t\tjsa.mu.Lock()\n\t\tuid := jsa.nuid.Next()\n\t\tsb.WriteString(jsa.rplyr)\n\t\tjsa.mu.Unlock()\n\n\t\tsb.WriteString(kind)\n\t\tsb.WriteByte(btsep)\n\t\tif cidHash != _EMPTY_ {\n\t\t\tsb.WriteString(cidHash)\n\t\t\tsb.WriteByte(btsep)\n\t\t}\n\t\tsb.WriteString(uid)\n\t\treply := sb.String()\n\n\t\t// Add responseCh to the reply channel map. It will be cleaned out on\n\t\t// timeout (see below), or in processJSAPIReplies upon receiving the\n\t\t// response.\n\t\tjsa.replies.Store(reply, responseCh)\n\n\t\tsubject = jsa.prefixDomain(subject)\n\t\tjsa.sendq.push(&mqttJSPubMsg{\n\t\t\tsubj:  subject,\n\t\t\treply: reply,\n\t\t\thdr:   hdr,\n\t\t\tmsg:   msg,\n\t\t})\n\t\tr2i[reply] = i\n\t}\n\n\t// Wait for all responses to come back, or for the timeout to expire. We\n\t// don't want to use time.After() which causes memory growth because the\n\t// timer can't be stopped and will need to expire to then be garbage\n\t// collected.\n\tc := 0\n\tresponses := make([]*mqttJSAResponse, len(msgs))\n\tstart := time.Now()\n\tt := time.NewTimer(jsa.timeout)\n\tdefer t.Stop()\n\tfor {\n\t\tselect {\n\t\tcase r := <-responseCh:\n\t\t\ti := r2i[r.reply]\n\t\t\tresponses[i] = r\n\t\t\tc++\n\t\t\tif c == len(msgs) {\n\t\t\t\treturn responses, nil\n\t\t\t}\n\n\t\tcase <-jsa.quitCh:\n\t\t\treturn nil, ErrServerNotRunning\n\n\t\tcase <-t.C:\n\t\t\tvar reply string\n\t\t\tnow := time.Now()\n\t\t\tfor reply = range r2i { // preserve the last value for Errorf\n\t\t\t\tjsa.replies.Delete(reply)\n\t\t\t}\n\n\t\t\tif len(msgs) == 1 {\n\t\t\t\treturn responses, fmt.Errorf(\"timeout after %v: request type %q on %q (reply=%q)\", now.Sub(start), kind, subject, reply)\n\t\t\t} else {\n\t\t\t\treturn responses, fmt.Errorf(\"timeout after %v: request type %q on %q: got %d out of %d\", now.Sub(start), kind, subject, c, len(msgs))\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc (jsa *mqttJSA) sendAck(ackSubject string) {\n\t// We pass -1 for the hdr so that the send loop does not need to\n\t// add the \"client info\" header. This is not a JS API request per se.\n\tjsa.sendMsg(ackSubject, nil)\n}\n\nfunc (jsa *mqttJSA) sendMsg(subj string, msg []byte) {\n\tif subj == _EMPTY_ {\n\t\treturn\n\t}\n\tjsa.sendq.push(&mqttJSPubMsg{subj: subj, msg: msg, hdr: -1})\n}\n\nfunc (jsa *mqttJSA) createEphemeralConsumer(cfg *CreateConsumerRequest) (*JSApiConsumerCreateResponse, error) {\n\tcfgb, err := json.Marshal(cfg)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tsubj := fmt.Sprintf(JSApiConsumerCreateT, cfg.Stream)\n\tccri, err := jsa.newRequest(mqttJSAConsumerCreate, subj, 0, cfgb)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tccr := ccri.(*JSApiConsumerCreateResponse)\n\treturn ccr, ccr.ToError()\n}\n\nfunc (jsa *mqttJSA) createDurableConsumer(cfg *CreateConsumerRequest) (*JSApiConsumerCreateResponse, error) {\n\tcfgb, err := json.Marshal(cfg)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tsubj := fmt.Sprintf(JSApiDurableCreateT, cfg.Stream, cfg.Config.Durable)\n\tccri, err := jsa.newRequest(mqttJSAConsumerCreate, subj, 0, cfgb)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tccr := ccri.(*JSApiConsumerCreateResponse)\n\treturn ccr, ccr.ToError()\n}\n\n// if noWait is specified, does not wait for the JS response, returns nil\nfunc (jsa *mqttJSA) deleteConsumer(streamName, consName string, noWait bool) (*JSApiConsumerDeleteResponse, error) {\n\tsubj := fmt.Sprintf(JSApiConsumerDeleteT, streamName, consName)\n\tif noWait {\n\t\tjsa.sendMsg(subj, nil)\n\t\treturn nil, nil\n\t}\n\n\tcdri, err := jsa.newRequest(mqttJSAConsumerDel, subj, 0, nil)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tcdr := cdri.(*JSApiConsumerDeleteResponse)\n\treturn cdr, cdr.ToError()\n}\n\nfunc (jsa *mqttJSA) createStream(cfg *StreamConfig) (*StreamInfo, bool, error) {\n\tcfgb, err := json.Marshal(cfg)\n\tif err != nil {\n\t\treturn nil, false, err\n\t}\n\tscri, err := jsa.newRequest(mqttJSAStreamCreate, fmt.Sprintf(JSApiStreamCreateT, cfg.Name), 0, cfgb)\n\tif err != nil {\n\t\treturn nil, false, err\n\t}\n\tscr := scri.(*JSApiStreamCreateResponse)\n\treturn scr.StreamInfo, scr.DidCreate, scr.ToError()\n}\n\nfunc (jsa *mqttJSA) updateStream(cfg *StreamConfig) (*StreamInfo, error) {\n\tcfgb, err := json.Marshal(cfg)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tscri, err := jsa.newRequest(mqttJSAStreamUpdate, fmt.Sprintf(JSApiStreamUpdateT, cfg.Name), 0, cfgb)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tscr := scri.(*JSApiStreamUpdateResponse)\n\treturn scr.StreamInfo, scr.ToError()\n}\n\nfunc (jsa *mqttJSA) lookupStream(name string) (*StreamInfo, error) {\n\tslri, err := jsa.newRequest(mqttJSAStreamLookup, fmt.Sprintf(JSApiStreamInfoT, name), 0, nil)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tslr := slri.(*JSApiStreamInfoResponse)\n\treturn slr.StreamInfo, slr.ToError()\n}\n\nfunc (jsa *mqttJSA) deleteStream(name string) (bool, error) {\n\tsdri, err := jsa.newRequest(mqttJSAStreamDel, fmt.Sprintf(JSApiStreamDeleteT, name), 0, nil)\n\tif err != nil {\n\t\treturn false, err\n\t}\n\tsdr := sdri.(*JSApiStreamDeleteResponse)\n\treturn sdr.Success, sdr.ToError()\n}\n\nfunc (jsa *mqttJSA) loadLastMsgFor(streamName string, subject string) (*StoredMsg, error) {\n\tmreq := &JSApiMsgGetRequest{LastFor: subject}\n\treq, err := json.Marshal(mreq)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tlmri, err := jsa.newRequest(mqttJSAMsgLoad, fmt.Sprintf(JSApiMsgGetT, streamName), 0, req)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tlmr := lmri.(*JSApiMsgGetResponse)\n\treturn lmr.Message, lmr.ToError()\n}\n\nfunc (jsa *mqttJSA) loadLastMsgForMulti(streamName string, subjects []string) ([]*JSApiMsgGetResponse, error) {\n\tmarshaled := make([][]byte, 0, len(subjects))\n\theaderBytes := make([]int, 0, len(subjects))\n\tfor _, subject := range subjects {\n\t\tmreq := &JSApiMsgGetRequest{LastFor: subject}\n\t\tbb, err := json.Marshal(mreq)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tmarshaled = append(marshaled, bb)\n\t\theaderBytes = append(headerBytes, 0)\n\t}\n\n\tall, err := jsa.newRequestExMulti(mqttJSAMsgLoad, fmt.Sprintf(JSApiMsgGetT, streamName), _EMPTY_, headerBytes, marshaled)\n\t// all has the same order as subjects, preserve it as we unmarshal\n\tresponses := make([]*JSApiMsgGetResponse, len(all))\n\tfor i, v := range all {\n\t\tif v != nil {\n\t\t\tresponses[i] = v.value.(*JSApiMsgGetResponse)\n\t\t}\n\t}\n\treturn responses, err\n}\n\nfunc (jsa *mqttJSA) loadNextMsgFor(streamName string, subject string) (*StoredMsg, error) {\n\tmreq := &JSApiMsgGetRequest{NextFor: subject}\n\treq, err := json.Marshal(mreq)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tlmri, err := jsa.newRequest(mqttJSAMsgLoad, fmt.Sprintf(JSApiMsgGetT, streamName), 0, req)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tlmr := lmri.(*JSApiMsgGetResponse)\n\treturn lmr.Message, lmr.ToError()\n}\n\nfunc (jsa *mqttJSA) loadMsg(streamName string, seq uint64) (*StoredMsg, error) {\n\tmreq := &JSApiMsgGetRequest{Seq: seq}\n\treq, err := json.Marshal(mreq)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tlmri, err := jsa.newRequest(mqttJSAMsgLoad, fmt.Sprintf(JSApiMsgGetT, streamName), 0, req)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tlmr := lmri.(*JSApiMsgGetResponse)\n\treturn lmr.Message, lmr.ToError()\n}\n\nfunc (jsa *mqttJSA) storeMsg(subject string, headers int, msg []byte) (*JSPubAckResponse, error) {\n\treturn jsa.storeMsgWithKind(mqttJSAMsgStore, subject, headers, msg)\n}\n\nfunc (jsa *mqttJSA) storeMsgWithKind(kind, subject string, headers int, msg []byte) (*JSPubAckResponse, error) {\n\tsmri, err := jsa.newRequest(kind, subject, headers, msg)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tsmr := smri.(*JSPubAckResponse)\n\treturn smr, smr.ToError()\n}\n\nfunc (jsa *mqttJSA) storeSessionMsg(domainTk, cidHash string, hdr int, msg []byte) (*JSPubAckResponse, error) {\n\t// Compute subject where the session is being stored\n\tsubject := mqttSessStreamSubjectPrefix + domainTk + cidHash\n\n\t// Passing cidHash will add it to the JS reply subject, so that we can use\n\t// it in processSessionPersist.\n\tsmri, err := jsa.newRequestEx(mqttJSASessPersist, subject, cidHash, hdr, msg)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tsmr := smri.(*JSPubAckResponse)\n\treturn smr, smr.ToError()\n}\n\nfunc (jsa *mqttJSA) loadSessionMsg(domainTk, cidHash string) (*StoredMsg, error) {\n\tsubject := mqttSessStreamSubjectPrefix + domainTk + cidHash\n\treturn jsa.loadLastMsgFor(mqttSessStreamName, subject)\n}\n\nfunc (jsa *mqttJSA) deleteMsg(stream string, seq uint64, wait bool) error {\n\tdreq := JSApiMsgDeleteRequest{Seq: seq, NoErase: true}\n\treq, _ := json.Marshal(dreq)\n\tsubj := jsa.prefixDomain(fmt.Sprintf(JSApiMsgDeleteT, stream))\n\tif !wait {\n\t\tjsa.sendq.push(&mqttJSPubMsg{\n\t\t\tsubj: subj,\n\t\t\tmsg:  req,\n\t\t})\n\t\treturn nil\n\t}\n\tdmi, err := jsa.newRequest(mqttJSAMsgDelete, subj, 0, req)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdm := dmi.(*JSApiMsgDeleteResponse)\n\treturn dm.ToError()\n}\n\n//////////////////////////////////////////////////////////////////////////////\n//\n// Account Sessions Manager related functions\n//\n//////////////////////////////////////////////////////////////////////////////\n\n// Returns true if `err` is not nil and does not match the api error with ErrorIdentifier id\nfunc isErrorOtherThan(err error, id ErrorIdentifier) bool {\n\treturn err != nil && !IsNatsErr(err, id)\n}\n\n// Process JS API replies.\n//\n// Can run from various go routines (consumer's loop, system send loop, etc..).\nfunc (as *mqttAccountSessionManager) processJSAPIReplies(_ *subscription, pc *client, _ *Account, subject, _ string, msg []byte) {\n\ttoken := tokenAt(subject, mqttJSATokenPos)\n\tif token == _EMPTY_ {\n\t\treturn\n\t}\n\tjsa := &as.jsa\n\tchi, ok := jsa.replies.Load(subject)\n\tif !ok {\n\t\treturn\n\t}\n\tjsa.replies.Delete(subject)\n\tch := chi.(chan *mqttJSAResponse)\n\tout := func(value any) {\n\t\tch <- &mqttJSAResponse{reply: subject, value: value}\n\t}\n\tswitch token {\n\tcase mqttJSAStreamCreate:\n\t\tvar resp = &JSApiStreamCreateResponse{}\n\t\tif err := json.Unmarshal(msg, resp); err != nil {\n\t\t\tresp.Error = NewJSInvalidJSONError(err)\n\t\t}\n\t\tout(resp)\n\tcase mqttJSAStreamUpdate:\n\t\tvar resp = &JSApiStreamUpdateResponse{}\n\t\tif err := json.Unmarshal(msg, resp); err != nil {\n\t\t\tresp.Error = NewJSInvalidJSONError(err)\n\t\t}\n\t\tout(resp)\n\tcase mqttJSAStreamLookup:\n\t\tvar resp = &JSApiStreamInfoResponse{}\n\t\tif err := json.Unmarshal(msg, &resp); err != nil {\n\t\t\tresp.Error = NewJSInvalidJSONError(err)\n\t\t}\n\t\tout(resp)\n\tcase mqttJSAStreamDel:\n\t\tvar resp = &JSApiStreamDeleteResponse{}\n\t\tif err := json.Unmarshal(msg, &resp); err != nil {\n\t\t\tresp.Error = NewJSInvalidJSONError(err)\n\t\t}\n\t\tout(resp)\n\tcase mqttJSAConsumerCreate:\n\t\tvar resp = &JSApiConsumerCreateResponse{}\n\t\tif err := json.Unmarshal(msg, resp); err != nil {\n\t\t\tresp.Error = NewJSInvalidJSONError(err)\n\t\t}\n\t\tout(resp)\n\tcase mqttJSAConsumerDel:\n\t\tvar resp = &JSApiConsumerDeleteResponse{}\n\t\tif err := json.Unmarshal(msg, resp); err != nil {\n\t\t\tresp.Error = NewJSInvalidJSONError(err)\n\t\t}\n\t\tout(resp)\n\tcase mqttJSAMsgStore, mqttJSASessPersist:\n\t\tvar resp = &JSPubAckResponse{}\n\t\tif err := json.Unmarshal(msg, resp); err != nil {\n\t\t\tresp.Error = NewJSInvalidJSONError(err)\n\t\t}\n\t\tout(resp)\n\tcase mqttJSAMsgLoad:\n\t\tvar resp = &JSApiMsgGetResponse{}\n\t\tif err := json.Unmarshal(msg, &resp); err != nil {\n\t\t\tresp.Error = NewJSInvalidJSONError(err)\n\t\t}\n\t\tout(resp)\n\tcase mqttJSAStreamNames:\n\t\tvar resp = &JSApiStreamNamesResponse{}\n\t\tif err := json.Unmarshal(msg, resp); err != nil {\n\t\t\tresp.Error = NewJSInvalidJSONError(err)\n\t\t}\n\t\tout(resp)\n\tcase mqttJSAMsgDelete:\n\t\tvar resp = &JSApiMsgDeleteResponse{}\n\t\tif err := json.Unmarshal(msg, resp); err != nil {\n\t\t\tresp.Error = NewJSInvalidJSONError(err)\n\t\t}\n\t\tout(resp)\n\tdefault:\n\t\tpc.Warnf(\"Unknown reply code %q\", token)\n\t}\n}\n\n// This will both load all retained messages and process updates from the cluster.\n//\n// Run from various go routines (JS consumer, etc..).\n// No lock held on entry.\nfunc (as *mqttAccountSessionManager) processRetainedMsg(_ *subscription, c *client, _ *Account, subject, reply string, rmsg []byte) {\n\th, m := c.msgParts(rmsg)\n\trm, err := mqttDecodeRetainedMessage(h, m)\n\tif err != nil {\n\t\treturn\n\t}\n\t// If lastSeq is 0 (nothing to recover, or done doing it) and this is\n\t// from our own server, ignore.\n\tas.mu.RLock()\n\tif as.rrmLastSeq == 0 && rm.Origin == as.jsa.id {\n\t\tas.mu.RUnlock()\n\t\treturn\n\t}\n\tas.mu.RUnlock()\n\n\t// At this point we either recover from our own server, or process a remote retained message.\n\tseq, _, _ := ackReplyInfo(reply)\n\n\t// Handle this retained message, no need to copy the bytes.\n\tas.handleRetainedMsg(rm.Subject, &mqttRetainedMsgRef{sseq: seq}, rm, false)\n\n\t// If we were recovering (lastSeq > 0), then check if we are done.\n\tas.mu.Lock()\n\tif as.rrmLastSeq > 0 && seq >= as.rrmLastSeq {\n\t\tas.rrmLastSeq = 0\n\t\tclose(as.rrmDoneCh)\n\t\tas.rrmDoneCh = nil\n\t}\n\tas.mu.Unlock()\n}\n\nfunc (as *mqttAccountSessionManager) processRetainedMsgDel(_ *subscription, c *client, _ *Account, subject, reply string, rmsg []byte) {\n\tidHash := tokenAt(subject, 3)\n\tif idHash == _EMPTY_ || idHash == as.jsa.id {\n\t\treturn\n\t}\n\t_, msg := c.msgParts(rmsg)\n\tif len(msg) < LEN_CR_LF {\n\t\treturn\n\t}\n\tvar drm mqttRetMsgDel\n\tif err := json.Unmarshal(msg, &drm); err != nil {\n\t\treturn\n\t}\n\tas.handleRetainedMsgDel(drm.Subject, drm.Seq)\n}\n\n// This will receive all JS API replies for a request to store a session record,\n// including the reply for our own server, which we will ignore.\n// This allows us to detect that some application somewhere else in the cluster\n// is connecting with the same client ID, and therefore we need to close the\n// connection that is currently using this client ID.\n//\n// Can run from various go routines (system send loop, etc..).\n// No lock held on entry.\nfunc (as *mqttAccountSessionManager) processSessionPersist(_ *subscription, pc *client, _ *Account, subject, _ string, rmsg []byte) {\n\t// Ignore our own responses here (they are handled elsewhere)\n\tif tokenAt(subject, mqttJSAIdTokenPos) == as.jsa.id {\n\t\treturn\n\t}\n\tcIDHash := tokenAt(subject, mqttJSAClientIDPos)\n\t_, msg := pc.msgParts(rmsg)\n\tif len(msg) < LEN_CR_LF {\n\t\treturn\n\t}\n\tvar par = &JSPubAckResponse{}\n\tif err := json.Unmarshal(msg, par); err != nil {\n\t\treturn\n\t}\n\tif err := par.Error; err != nil {\n\t\treturn\n\t}\n\tas.mu.RLock()\n\t// Note that as.domainTk includes a terminal '.', so strip to compare to PubAck.Domain.\n\tdl := len(as.domainTk)\n\tif dl > 0 {\n\t\tdl--\n\t}\n\tignore := par.Domain != as.domainTk[:dl]\n\tas.mu.RUnlock()\n\tif ignore {\n\t\treturn\n\t}\n\n\tas.mu.Lock()\n\tdefer as.mu.Unlock()\n\tsess, ok := as.sessByHash[cIDHash]\n\tif !ok {\n\t\treturn\n\t}\n\t// If our current session's stream sequence is higher, it means that this\n\t// update is stale, so we don't do anything here.\n\tsess.mu.Lock()\n\tignore = par.Sequence < sess.seq\n\tsess.mu.Unlock()\n\tif ignore {\n\t\treturn\n\t}\n\tas.removeSession(sess, false)\n\tsess.mu.Lock()\n\tif ec := sess.c; ec != nil {\n\t\tas.addSessToFlappers(sess.id)\n\t\tec.Warnf(\"Closing because a remote connection has started with the same client ID: %q\", sess.id)\n\t\t// Disassociate the client from the session so that on client close,\n\t\t// nothing will be done with regards to cleaning up the session,\n\t\t// such as deleting stream, etc..\n\t\tsess.c = nil\n\t\t// Remove in separate go routine.\n\t\tgo ec.closeConnection(DuplicateClientID)\n\t}\n\tsess.mu.Unlock()\n}\n\n// Adds this client ID to the flappers map, and if needed start the timer\n// for map cleanup.\n//\n// Lock held on entry.\nfunc (as *mqttAccountSessionManager) addSessToFlappers(clientID string) {\n\tas.flappers[clientID] = time.Now().UnixNano()\n\tif as.flapTimer == nil {\n\t\tas.flapTimer = time.AfterFunc(mqttFlapCleanItvl, func() {\n\t\t\tas.mu.Lock()\n\t\t\tdefer as.mu.Unlock()\n\t\t\t// In case of shutdown, this will be nil\n\t\t\tif as.flapTimer == nil {\n\t\t\t\treturn\n\t\t\t}\n\t\t\tnow := time.Now().UnixNano()\n\t\t\tfor cID, tm := range as.flappers {\n\t\t\t\tif now-tm > int64(mqttSessJailDur) {\n\t\t\t\t\tdelete(as.flappers, cID)\n\t\t\t\t}\n\t\t\t}\n\t\t\tas.flapTimer.Reset(mqttFlapCleanItvl)\n\t\t})\n\t}\n}\n\n// Remove this client ID from the flappers map.\n//\n// Lock held on entry.\nfunc (as *mqttAccountSessionManager) removeSessFromFlappers(clientID string) {\n\tdelete(as.flappers, clientID)\n\t// Do not stop/set timer to nil here. Better leave the timer run at its\n\t// regular interval and detect that there is nothing to do. The timer\n\t// will be stopped on shutdown.\n}\n\n// Helper to create a subscription. It updates the sid and array of subscriptions.\nfunc (as *mqttAccountSessionManager) createSubscription(subject string, cb msgHandler, sid *int64, subs *[]*subscription) error {\n\tsub, err := as.jsa.c.processSub([]byte(subject), nil, []byte(strconv.FormatInt(*sid, 10)), cb, false)\n\tif err != nil {\n\t\treturn err\n\t}\n\t*sid++\n\t*subs = append(*subs, sub)\n\treturn nil\n}\n\n// A timer loop to cleanup up expired cached retained messages for a given MQTT account.\n// The closeCh is used by the caller to be able to interrupt this routine\n// if the rest of the initialization fails, since the quitCh is really\n// only used when the server shutdown.\n//\n// No lock held on entry.\nfunc (as *mqttAccountSessionManager) cleanupRetainedMessageCache(s *Server, closeCh chan struct{}) {\n\ttt := time.NewTicker(mqttRetainedCacheTTL)\n\tdefer tt.Stop()\n\tfor {\n\t\tselect {\n\t\tcase <-tt.C:\n\t\t\t// Set a limit to the number of retained messages to scan since we\n\t\t\t// lock as for it. Since the map enumeration gives random order we\n\t\t\t// should eventually clean up everything.\n\t\t\ti, maxScan := 0, 10*1000\n\t\t\tnow := time.Now()\n\t\t\tas.rmsCache.Range(func(key, value any) bool {\n\t\t\t\trm := value.(*mqttRetainedMsg)\n\t\t\t\tif now.After(rm.expiresFromCache) {\n\t\t\t\t\tas.rmsCache.Delete(key)\n\t\t\t\t}\n\t\t\t\ti++\n\t\t\t\treturn i < maxScan\n\t\t\t})\n\n\t\tcase <-closeCh:\n\t\t\treturn\n\t\tcase <-s.quitCh:\n\t\t\treturn\n\t\t}\n\t}\n}\n\n// Loop to send JS API requests for a given MQTT account.\n// The closeCh is used by the caller to be able to interrupt this routine\n// if the rest of the initialization fails, since the quitCh is really\n// only used when the server shutdown.\n//\n// No lock held on entry.\nfunc (as *mqttAccountSessionManager) sendJSAPIrequests(s *Server, c *client, accName string, closeCh chan struct{}) {\n\tvar cluster string\n\tif s.JetStreamEnabled() && !as.jsa.domainSet {\n\t\t// Only request the own cluster when it is clear that\n\t\tcluster = s.cachedClusterName()\n\t}\n\tas.mu.RLock()\n\tsendq := as.jsa.sendq\n\tquitCh := as.jsa.quitCh\n\tci := ClientInfo{Account: accName, Cluster: cluster}\n\tas.mu.RUnlock()\n\n\t// The account session manager does not have a suhtdown API per-se, instead,\n\t// we will cleanup things when this go routine exits after detecting that the\n\t// server is shutdown or the initialization of the account manager failed.\n\tdefer func() {\n\t\tas.mu.Lock()\n\t\tif as.flapTimer != nil {\n\t\t\tas.flapTimer.Stop()\n\t\t\tas.flapTimer = nil\n\t\t}\n\t\tas.mu.Unlock()\n\t}()\n\n\tb, _ := json.Marshal(ci)\n\thdrStart := bytes.Buffer{}\n\thdrStart.WriteString(hdrLine)\n\thttp.Header{ClientInfoHdr: []string{string(b)}}.Write(&hdrStart)\n\thdrStart.WriteString(CR_LF)\n\thdrStart.WriteString(CR_LF)\n\thdrb := hdrStart.Bytes()\n\n\tfor {\n\t\tselect {\n\t\tcase <-sendq.ch:\n\t\t\tpmis := sendq.pop()\n\t\t\tfor _, r := range pmis {\n\t\t\t\tvar nsize int\n\n\t\t\t\tmsg := r.msg\n\t\t\t\t// If r.hdr is set to -1, it means that there is no need for any header.\n\t\t\t\tif r.hdr != -1 {\n\t\t\t\t\tbb := bytes.Buffer{}\n\t\t\t\t\tif r.hdr > 0 {\n\t\t\t\t\t\t// This means that the header has been set by the caller and is\n\t\t\t\t\t\t// already part of `msg`, so simply set c.pa.hdr to the given value.\n\t\t\t\t\t\tc.pa.hdr = r.hdr\n\t\t\t\t\t\tnsize = len(msg)\n\t\t\t\t\t\tmsg = append(msg, _CRLF_...)\n\t\t\t\t\t} else {\n\t\t\t\t\t\t// We need the ClientInfo header, so add it here.\n\t\t\t\t\t\tbb.Write(hdrb)\n\t\t\t\t\t\tc.pa.hdr = bb.Len()\n\t\t\t\t\t\tbb.Write(r.msg)\n\t\t\t\t\t\tnsize = bb.Len()\n\t\t\t\t\t\tbb.WriteString(_CRLF_)\n\t\t\t\t\t\tmsg = bb.Bytes()\n\t\t\t\t\t}\n\t\t\t\t\tc.pa.hdb = []byte(strconv.Itoa(c.pa.hdr))\n\t\t\t\t} else {\n\t\t\t\t\tc.pa.hdr = -1\n\t\t\t\t\tc.pa.hdb = nil\n\t\t\t\t\tnsize = len(msg)\n\t\t\t\t\tmsg = append(msg, _CRLF_...)\n\t\t\t\t}\n\n\t\t\t\tc.pa.subject = []byte(r.subj)\n\t\t\t\tc.pa.reply = []byte(r.reply)\n\t\t\t\tc.pa.size = nsize\n\t\t\t\tc.pa.szb = []byte(strconv.Itoa(nsize))\n\n\t\t\t\tc.processInboundClientMsg(msg)\n\t\t\t\tc.flushClients(0)\n\t\t\t}\n\t\t\tsendq.recycle(&pmis)\n\n\t\tcase <-closeCh:\n\t\t\treturn\n\t\tcase <-quitCh:\n\t\t\treturn\n\t\t}\n\t}\n}\n\n// Add/Replace this message from the retained messages map.\n// If a message for this topic already existed, the existing record is updated\n// with the provided information.\n// Lock not held on entry.\nfunc (as *mqttAccountSessionManager) handleRetainedMsg(key string, rf *mqttRetainedMsgRef, rm *mqttRetainedMsg, copyBytesToCache bool) {\n\tas.mu.Lock()\n\tdefer as.mu.Unlock()\n\tif as.retmsgs == nil {\n\t\tas.retmsgs = make(map[string]*mqttRetainedMsgRef)\n\t\tas.sl = NewSublistWithCache()\n\t} else {\n\t\t// Check if we already had one retained message. If so, update the existing one.\n\t\tif erm, exists := as.retmsgs[key]; exists {\n\t\t\t// If the new sequence is below the floor or the existing one,\n\t\t\t// then ignore the new one.\n\t\t\tif rf.sseq <= erm.sseq || rf.sseq <= erm.floor {\n\t\t\t\treturn\n\t\t\t}\n\t\t\t// Capture existing sequence number so we can return it as the old sequence.\n\t\t\term.sseq = rf.sseq\n\t\t\t// Clear the floor\n\t\t\term.floor = 0\n\t\t\t// If sub is nil, it means that it was removed from sublist following a\n\t\t\t// network delete. So need to add it now.\n\t\t\tif erm.sub == nil {\n\t\t\t\term.sub = &subscription{subject: []byte(key)}\n\t\t\t\tas.sl.Insert(erm.sub)\n\t\t\t}\n\n\t\t\t// Update the in-memory retained message cache but only for messages\n\t\t\t// that are already in the cache, i.e. have been (recently) used.\n\t\t\tas.setCachedRetainedMsg(key, rm, true, copyBytesToCache)\n\t\t\treturn\n\t\t}\n\t}\n\n\trf.sub = &subscription{subject: []byte(key)}\n\tas.retmsgs[key] = rf\n\tas.sl.Insert(rf.sub)\n}\n\n// Removes the retained message for the given `subject` if present, and returns the\n// stream sequence it was stored at. It will be 0 if no retained message was removed.\n// If a sequence is passed and not 0, then the retained message will be removed only\n// if the given sequence is equal or higher to what is stored.\n//\n// No lock held on entry.\nfunc (as *mqttAccountSessionManager) handleRetainedMsgDel(subject string, seq uint64) uint64 {\n\tvar seqToRemove uint64\n\tas.mu.Lock()\n\tif as.retmsgs == nil {\n\t\tas.retmsgs = make(map[string]*mqttRetainedMsgRef)\n\t\tas.sl = NewSublistWithCache()\n\t}\n\tif erm, ok := as.retmsgs[subject]; ok {\n\t\tif as.rmsCache != nil {\n\t\t\tas.rmsCache.Delete(subject)\n\t\t}\n\t\tif erm.sub != nil {\n\t\t\tas.sl.Remove(erm.sub)\n\t\t\term.sub = nil\n\t\t}\n\t\t// If processing a delete request from the network, then seq will be > 0.\n\t\t// If that is the case and it is greater or equal to what we have, we need\n\t\t// to record the floor for this subject.\n\t\tif seq != 0 && seq >= erm.sseq {\n\t\t\term.sseq = 0\n\t\t\term.floor = seq\n\t\t} else if seq == 0 {\n\t\t\tdelete(as.retmsgs, subject)\n\t\t\tseqToRemove = erm.sseq\n\t\t}\n\t} else if seq != 0 {\n\t\trf := &mqttRetainedMsgRef{floor: seq}\n\t\tas.retmsgs[subject] = rf\n\t}\n\tas.mu.Unlock()\n\treturn seqToRemove\n}\n\n// First check if this session's client ID is already in the \"locked\" map,\n// which if it is the case means that another client is now bound to this\n// session and this should return an error.\n// If not in the \"locked\" map, but the client is not bound with this session,\n// then same error is returned.\n// Finally, if all checks ok, then the session's ID is added to the \"locked\" map.\n//\n// No lock held on entry.\nfunc (as *mqttAccountSessionManager) lockSession(sess *mqttSession, c *client) error {\n\tas.mu.Lock()\n\tdefer as.mu.Unlock()\n\tvar fail bool\n\tif _, fail = as.sessLocked[sess.id]; !fail {\n\t\tsess.mu.Lock()\n\t\tfail = sess.c != c\n\t\tsess.mu.Unlock()\n\t}\n\tif fail {\n\t\treturn fmt.Errorf(\"another session is in use with client ID %q\", sess.id)\n\t}\n\tas.sessLocked[sess.id] = struct{}{}\n\treturn nil\n}\n\n// Remove the session from the \"locked\" map.\n//\n// No lock held on entry.\nfunc (as *mqttAccountSessionManager) unlockSession(sess *mqttSession) {\n\tas.mu.Lock()\n\tdelete(as.sessLocked, sess.id)\n\tas.mu.Unlock()\n}\n\n// Simply adds the session to the various sessions maps.\n// The boolean `lock` indicates if this function should acquire the lock\n// prior to adding to the maps.\n//\n// No lock held on entry.\nfunc (as *mqttAccountSessionManager) addSession(sess *mqttSession, lock bool) {\n\tif lock {\n\t\tas.mu.Lock()\n\t}\n\tas.sessions[sess.id] = sess\n\tas.sessByHash[sess.idHash] = sess\n\tif lock {\n\t\tas.mu.Unlock()\n\t}\n}\n\n// Simply removes the session from the various sessions maps.\n// The boolean `lock` indicates if this function should acquire the lock\n// prior to removing from the maps.\n//\n// No lock held on entry.\nfunc (as *mqttAccountSessionManager) removeSession(sess *mqttSession, lock bool) {\n\tif lock {\n\t\tas.mu.Lock()\n\t}\n\tdelete(as.sessions, sess.id)\n\tdelete(as.sessByHash, sess.idHash)\n\tif lock {\n\t\tas.mu.Unlock()\n\t}\n}\n\n// Helper to set the sub's mqtt fields and possibly serialize (pre-loaded)\n// retained messages.\n//\n// Session lock held on entry. Acquires the subs lock and holds it for\n// the duration. Non-MQTT messages coming into mqttDeliverMsgCbQoS0 will be\n// waiting.\nfunc (sess *mqttSession) processQOS12Sub(\n\tc *client, // subscribing client.\n\tsubject, sid []byte, isReserved bool, qos byte, jsDurName string, h msgHandler, // subscription parameters.\n) (*subscription, error) {\n\treturn sess.processSub(c, subject, sid, isReserved, qos, jsDurName, h, false, nil, false, nil)\n}\n\nfunc (sess *mqttSession) processSub(\n\tc *client, // subscribing client.\n\tsubject, sid []byte, isReserved bool, qos byte, jsDurName string, h msgHandler, // subscription parameters.\n\tinitShadow bool, // do we need to scan for shadow subscriptions? (not for QOS1+)\n\trms map[string]*mqttRetainedMsg, // preloaded rms (can be empty, or missing items if errors)\n\ttrace bool, // trace serialized retained messages in the log?\n\tas *mqttAccountSessionManager, // needed only for rms serialization.\n) (*subscription, error) {\n\tstart := time.Now()\n\tdefer func() {\n\t\telapsed := time.Since(start)\n\t\tif elapsed > mqttProcessSubTooLong {\n\t\t\tc.Warnf(\"Took too long to process subscription for %q: %v\", subject, elapsed)\n\t\t}\n\t}()\n\n\t// Hold subsMu to prevent QOS0 messages callback from doing anything until\n\t// the (MQTT) sub is initialized.\n\tsess.subsMu.Lock()\n\tdefer sess.subsMu.Unlock()\n\n\tsub, err := c.processSub(subject, nil, sid, h, false)\n\tif err != nil {\n\t\t// c.processSub already called c.Errorf(), so no need here.\n\t\treturn nil, err\n\t}\n\tsubs := []*subscription{sub}\n\tif initShadow {\n\t\tsubs = append(subs, sub.shadow...)\n\t}\n\tfor _, ss := range subs {\n\t\tif ss.mqtt == nil {\n\t\t\t// reserved is set only once and once the subscription has been\n\t\t\t// created it can be considered immutable.\n\t\t\tss.mqtt = &mqttSub{\n\t\t\t\treserved: isReserved,\n\t\t\t}\n\t\t}\n\t\t// QOS and jsDurName can be changed on an existing subscription, so\n\t\t// accessing it later requires a lock.\n\t\tss.mqtt.qos = qos\n\t\tss.mqtt.jsDur = jsDurName\n\t}\n\n\tif len(rms) > 0 {\n\t\tfor _, ss := range subs {\n\t\t\tas.serializeRetainedMsgsForSub(rms, sess, c, ss, trace)\n\t\t}\n\t}\n\n\treturn sub, nil\n}\n\n// Process subscriptions for the given session/client.\n//\n// When `fromSubProto` is false, it means that this is invoked from the CONNECT\n// protocol, when restoring subscriptions that were saved for this session.\n// In that case, there is no need to update the session record.\n//\n// When `fromSubProto` is true, it means that this call is invoked from the\n// processing of the SUBSCRIBE protocol, which means that the session needs to\n// be updated. It also means that if a subscription on same subject with same\n// QoS already exist, we should not be recreating the subscription/JS durable,\n// since it was already done when processing the CONNECT protocol.\n//\n// Runs from the client's readLoop.\n// Lock not held on entry, but session is in the locked map.\nfunc (as *mqttAccountSessionManager) processSubs(sess *mqttSession, c *client,\n\tfilters []*mqttFilter, fromSubProto, trace bool) ([]*subscription, error) {\n\n\tc.mu.Lock()\n\tacc := c.acc\n\tc.mu.Unlock()\n\n\t// Helper to determine if we need to create a separate top-level\n\t// subscription for a wildcard.\n\tfwc := func(subject string) (bool, string, string) {\n\t\tif !mqttNeedSubForLevelUp(subject) {\n\t\t\treturn false, _EMPTY_, _EMPTY_\n\t\t}\n\t\t// Say subject is \"foo.>\", remove the \".>\" so that it becomes \"foo\"\n\t\tfwcsubject := subject[:len(subject)-2]\n\t\t// Change the sid to \"foo fwc\"\n\t\tfwcsid := fwcsubject + mqttMultiLevelSidSuffix\n\n\t\treturn true, fwcsubject, fwcsid\n\t}\n\n\trmSubjects := map[string]struct{}{}\n\t// Preload retained messages for all requested subscriptions.  Also, since\n\t// it's the first iteration over the filter list, do some cleanup.\n\tfor _, f := range filters {\n\t\tif f.qos > 2 {\n\t\t\tf.qos = 2\n\t\t}\n\t\tif c.mqtt.downgradeQoS2Sub && f.qos == 2 {\n\t\t\tc.Warnf(\"Downgrading subscription QoS2 to QoS1 for %q, as configured\", f.filter)\n\t\t\tf.qos = 1\n\t\t}\n\n\t\t// Do not allow subscribing to our internal subjects.\n\t\t//\n\t\t// TODO: (levb: not sure why since one can subscribe to `#` and it'll\n\t\t// include everything; I guess this would discourage? Otherwise another\n\t\t// candidate for DO NOT DELIVER prefix list).\n\t\tif strings.HasPrefix(f.filter, mqttSubPrefix) {\n\t\t\tf.qos = mqttSubAckFailure\n\t\t\tcontinue\n\t\t}\n\n\t\tif f.qos == 2 {\n\t\t\tif err := sess.ensurePubRelConsumerSubscription(c); err != nil {\n\t\t\t\tc.Errorf(\"failed to initialize PUBREL processing: %v\", err)\n\t\t\t\tf.qos = mqttSubAckFailure\n\t\t\t\tcontinue\n\t\t\t}\n\t\t}\n\n\t\t// Find retained messages.\n\t\tif fromSubProto {\n\t\t\taddRMSubjects := func(subject string) error {\n\t\t\t\tsub := &subscription{\n\t\t\t\t\tclient:  c,\n\t\t\t\t\tsubject: []byte(subject),\n\t\t\t\t\tsid:     []byte(subject),\n\t\t\t\t}\n\t\t\t\tif err := c.addShadowSubscriptions(acc, sub, false); err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\n\t\t\t\tfor _, sub := range append([]*subscription{sub}, sub.shadow...) {\n\t\t\t\t\tas.addRetainedSubjectsForSubject(rmSubjects, bytesToString(sub.subject))\n\t\t\t\t\tfor _, ss := range sub.shadow {\n\t\t\t\t\t\tas.addRetainedSubjectsForSubject(rmSubjects, bytesToString(ss.subject))\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\treturn nil\n\t\t\t}\n\n\t\t\tif err := addRMSubjects(f.filter); err != nil {\n\t\t\t\tf.qos = mqttSubAckFailure\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif need, subject, _ := fwc(f.filter); need {\n\t\t\t\tif err := addRMSubjects(subject); err != nil {\n\t\t\t\t\tf.qos = mqttSubAckFailure\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tserializeRMS := len(rmSubjects) > 0\n\tvar rms map[string]*mqttRetainedMsg\n\tif serializeRMS {\n\t\t// Make the best effort to load retained messages. We will identify\n\t\t// errors in the next pass.\n\t\trms = as.loadRetainedMessages(rmSubjects, c)\n\t}\n\n\t// Small helper to add the consumer config to the session.\n\taddJSConsToSess := func(sid string, cc *ConsumerConfig) {\n\t\tif cc == nil {\n\t\t\treturn\n\t\t}\n\t\tif sess.cons == nil {\n\t\t\tsess.cons = make(map[string]*ConsumerConfig)\n\t\t}\n\t\tsess.cons[sid] = cc\n\t}\n\n\tvar err error\n\tsubs := make([]*subscription, 0, len(filters))\n\tfor _, f := range filters {\n\t\t// Skip what's already been identified as a failure.\n\t\tif f.qos == mqttSubAckFailure {\n\t\t\tcontinue\n\t\t}\n\t\tsubject := f.filter\n\t\tbsubject := []byte(subject)\n\t\tsid := subject\n\t\tbsid := bsubject\n\t\tisReserved := isMQTTReservedSubscription(subject)\n\n\t\tvar jscons *ConsumerConfig\n\t\tvar jssub *subscription\n\n\t\t// Note that if a subscription already exists on this subject, the\n\t\t// existing sub is returned. Need to update the qos.\n\t\tvar sub *subscription\n\t\tvar err error\n\n\t\tconst processShadowSubs = true\n\n\t\tas.mu.Lock()\n\t\tsess.mu.Lock()\n\t\tsub, err = sess.processSub(c,\n\t\t\tbsubject, bsid, isReserved, f.qos, // main subject\n\t\t\t_EMPTY_, mqttDeliverMsgCbQoS0, // no jsDur for QOS0\n\t\t\tprocessShadowSubs,\n\t\t\trms, trace, as)\n\t\tsess.mu.Unlock()\n\t\tas.mu.Unlock()\n\n\t\tif err != nil {\n\t\t\tf.qos = mqttSubAckFailure\n\t\t\tsess.cleanupFailedSub(c, sub, jscons, jssub)\n\t\t\tcontinue\n\t\t}\n\n\t\t// This will create (if not already exist) a JS consumer for\n\t\t// subscriptions of QoS >= 1. But if a JS consumer already exists and\n\t\t// the subscription for same subject is now a QoS==0, then the JS\n\t\t// consumer will be deleted.\n\t\tjscons, jssub, err = sess.processJSConsumer(c, subject, sid, f.qos, fromSubProto)\n\t\tif err != nil {\n\t\t\tf.qos = mqttSubAckFailure\n\t\t\tsess.cleanupFailedSub(c, sub, jscons, jssub)\n\t\t\tcontinue\n\t\t}\n\n\t\t// Process the wildcard subject if needed.\n\t\tif need, fwcsubject, fwcsid := fwc(subject); need {\n\t\t\tvar fwjscons *ConsumerConfig\n\t\t\tvar fwjssub *subscription\n\t\t\tvar fwcsub *subscription\n\n\t\t\t// See note above about existing subscription.\n\t\t\tas.mu.Lock()\n\t\t\tsess.mu.Lock()\n\t\t\tfwcsub, err = sess.processSub(c,\n\t\t\t\t[]byte(fwcsubject), []byte(fwcsid), isReserved, f.qos, // FWC (top-level wildcard) subject\n\t\t\t\t_EMPTY_, mqttDeliverMsgCbQoS0, // no jsDur for QOS0\n\t\t\t\tprocessShadowSubs,\n\t\t\t\trms, trace, as)\n\t\t\tsess.mu.Unlock()\n\t\t\tas.mu.Unlock()\n\t\t\tif err != nil {\n\t\t\t\t// c.processSub already called c.Errorf(), so no need here.\n\t\t\t\tf.qos = mqttSubAckFailure\n\t\t\t\tsess.cleanupFailedSub(c, sub, jscons, jssub)\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tfwjscons, fwjssub, err = sess.processJSConsumer(c, fwcsubject, fwcsid, f.qos, fromSubProto)\n\t\t\tif err != nil {\n\t\t\t\t// c.processSub already called c.Errorf(), so no need here.\n\t\t\t\tf.qos = mqttSubAckFailure\n\t\t\t\tsess.cleanupFailedSub(c, sub, jscons, jssub)\n\t\t\t\tsess.cleanupFailedSub(c, fwcsub, fwjscons, fwjssub)\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tsubs = append(subs, fwcsub)\n\t\t\taddJSConsToSess(fwcsid, fwjscons)\n\t\t}\n\n\t\tsubs = append(subs, sub)\n\t\taddJSConsToSess(sid, jscons)\n\t}\n\n\tif fromSubProto {\n\t\terr = sess.update(filters, true)\n\t}\n\n\treturn subs, err\n}\n\n// Retained publish messages matching this subscription are serialized in the\n// subscription's `prm` mqtt writer. This buffer will be queued for outbound\n// after the subscription is processed and SUBACK is sent or possibly when\n// server processes an incoming published message matching the newly\n// registered subscription.\n//\n// Runs from the client's readLoop.\n// Account session manager lock held on entry.\n// Session lock held on entry.\nfunc (as *mqttAccountSessionManager) serializeRetainedMsgsForSub(rms map[string]*mqttRetainedMsg, sess *mqttSession, c *client, sub *subscription, trace bool) error {\n\tif len(as.retmsgs) == 0 || len(rms) == 0 {\n\t\treturn nil\n\t}\n\tresult := as.sl.ReverseMatch(string(sub.subject))\n\tif len(result.psubs) == 0 {\n\t\treturn nil\n\t}\n\ttoTrace := []mqttPublish{}\n\tfor _, psub := range result.psubs {\n\n\t\trm := rms[string(psub.subject)]\n\t\tif rm == nil {\n\t\t\t// This should not happen since we pre-load messages into rms before\n\t\t\t// calling serialize.\n\t\t\tcontinue\n\t\t}\n\t\tvar pi uint16\n\t\tqos := mqttGetQoS(rm.Flags)\n\t\tif qos > sub.mqtt.qos {\n\t\t\tqos = sub.mqtt.qos\n\t\t}\n\t\tif c.mqtt.rejectQoS2Pub && qos == 2 {\n\t\t\tc.Warnf(\"Rejecting retained message with QoS2 for subscription %q, as configured\", sub.subject)\n\t\t\tcontinue\n\t\t}\n\t\tif qos > 0 {\n\t\t\tpi = sess.trackPublishRetained()\n\n\t\t\t// If we failed to get a PI for this message, send it as a QoS0, the\n\t\t\t// best we can do?\n\t\t\tif pi == 0 {\n\t\t\t\tqos = 0\n\t\t\t}\n\t\t}\n\n\t\t// Need to use the subject for the retained message, not the `sub` subject.\n\t\t// We can find the published retained message in rm.sub.subject.\n\t\t// Set the RETAIN flag: [MQTT-3.3.1-8].\n\t\tflags, headerBytes := mqttMakePublishHeader(pi, qos, false, true, []byte(rm.Topic), len(rm.Msg))\n\t\tc.mu.Lock()\n\t\tsub.mqtt.prm = append(sub.mqtt.prm, headerBytes, rm.Msg)\n\t\tc.mu.Unlock()\n\t\tif trace {\n\t\t\ttoTrace = append(toTrace, mqttPublish{\n\t\t\t\ttopic: []byte(rm.Topic),\n\t\t\t\tflags: flags,\n\t\t\t\tpi:    pi,\n\t\t\t\tsz:    len(rm.Msg),\n\t\t\t})\n\t\t}\n\t}\n\tfor _, pp := range toTrace {\n\t\tc.traceOutOp(\"PUBLISH\", []byte(mqttPubTrace(&pp)))\n\t}\n\treturn nil\n}\n\n// Appends the stored message subjects for all retained message records that\n// match the given subscription's `subject` (which could have wildcards).\n//\n// Account session manager NOT lock held on entry.\nfunc (as *mqttAccountSessionManager) addRetainedSubjectsForSubject(list map[string]struct{}, topSubject string) bool {\n\tas.mu.RLock()\n\tif len(as.retmsgs) == 0 {\n\t\tas.mu.RUnlock()\n\t\treturn false\n\t}\n\tresult := as.sl.ReverseMatch(topSubject)\n\tas.mu.RUnlock()\n\n\tadded := false\n\tfor _, sub := range result.psubs {\n\t\tsubject := string(sub.subject)\n\t\tif _, ok := list[subject]; ok {\n\t\t\tcontinue\n\t\t}\n\t\tlist[subject] = struct{}{}\n\t\tadded = true\n\t}\n\n\treturn added\n}\n\ntype warner interface {\n\tWarnf(format string, v ...any)\n}\n\n// Loads a list of retained messages given a list of stored message subjects.\nfunc (as *mqttAccountSessionManager) loadRetainedMessages(subjects map[string]struct{}, w warner) map[string]*mqttRetainedMsg {\n\trms := make(map[string]*mqttRetainedMsg, len(subjects))\n\tss := []string{}\n\tfor s := range subjects {\n\t\tif rm := as.getCachedRetainedMsg(s); rm != nil {\n\t\t\trms[s] = rm\n\t\t} else {\n\t\t\tss = append(ss, mqttRetainedMsgsStreamSubject+s)\n\t\t}\n\t}\n\n\tif len(ss) == 0 {\n\t\treturn rms\n\t}\n\n\tresults, err := as.jsa.loadLastMsgForMulti(mqttRetainedMsgsStreamName, ss)\n\t// If an error occurred, warn, but then proceed with what we got.\n\tif err != nil {\n\t\tw.Warnf(\"error loading retained messages: %v\", err)\n\t}\n\tfor i, result := range results {\n\t\tif result == nil {\n\t\t\tcontinue // skip requests that timed out\n\t\t}\n\t\tif result.ToError() != nil {\n\t\t\tw.Warnf(\"failed to load retained message for subject %q: %v\", ss[i], err)\n\t\t\tcontinue\n\t\t}\n\t\trm, err := mqttDecodeRetainedMessage(result.Message.Header, result.Message.Data)\n\t\tif err != nil {\n\t\t\tw.Warnf(\"failed to decode retained message for subject %q: %v\", ss[i], err)\n\t\t\tcontinue\n\t\t}\n\n\t\t// Add the loaded retained message to the cache, and to the results map.\n\t\tkey := ss[i][len(mqttRetainedMsgsStreamSubject):]\n\t\tas.setCachedRetainedMsg(key, rm, false, false)\n\t\trms[key] = rm\n\t}\n\treturn rms\n}\n\n// Composes a NATS message for a storeable mqttRetainedMsg.\nfunc mqttEncodeRetainedMessage(rm *mqttRetainedMsg) (natsMsg []byte, headerLen int) {\n\t// No need to encode the subject, we can restore it from topic.\n\tl := len(hdrLine)\n\tl += len(mqttNatsRetainedMessageTopic) + 1 + len(rm.Topic) + 2 // 1 byte for ':', 2 bytes for CRLF\n\tif rm.Origin != _EMPTY_ {\n\t\tl += len(mqttNatsRetainedMessageOrigin) + 1 + len(rm.Origin) + 2 // 1 byte for ':', 2 bytes for CRLF\n\t}\n\tif rm.Source != _EMPTY_ {\n\t\tl += len(mqttNatsRetainedMessageSource) + 1 + len(rm.Source) + 2 // 1 byte for ':', 2 bytes for CRLF\n\t}\n\tl += len(mqttNatsRetainedMessageFlags) + 1 + 2 + 2 // 1 byte for ':', 2 bytes for the flags, 2 bytes for CRLF\n\tl += 2                                             // 2 bytes for the extra CRLF after the header\n\tl += len(rm.Msg)\n\n\tbuf := bytes.NewBuffer(make([]byte, 0, l))\n\n\tbuf.WriteString(hdrLine)\n\n\tbuf.WriteString(mqttNatsRetainedMessageTopic)\n\tbuf.WriteByte(':')\n\tbuf.WriteString(rm.Topic)\n\tbuf.WriteString(_CRLF_)\n\n\tbuf.WriteString(mqttNatsRetainedMessageFlags)\n\tbuf.WriteByte(':')\n\tbuf.WriteString(strconv.FormatUint(uint64(rm.Flags), 16))\n\tbuf.WriteString(_CRLF_)\n\n\tif rm.Origin != _EMPTY_ {\n\t\tbuf.WriteString(mqttNatsRetainedMessageOrigin)\n\t\tbuf.WriteByte(':')\n\t\tbuf.WriteString(rm.Origin)\n\t\tbuf.WriteString(_CRLF_)\n\t}\n\tif rm.Source != _EMPTY_ {\n\t\tbuf.WriteString(mqttNatsRetainedMessageSource)\n\t\tbuf.WriteByte(':')\n\t\tbuf.WriteString(rm.Source)\n\t\tbuf.WriteString(_CRLF_)\n\t}\n\n\t// End of header, finalize\n\tbuf.WriteString(_CRLF_)\n\theaderLen = buf.Len()\n\tbuf.Write(rm.Msg)\n\treturn buf.Bytes(), headerLen\n}\n\nfunc mqttDecodeRetainedMessage(h, m []byte) (*mqttRetainedMsg, error) {\n\tfHeader := getHeader(mqttNatsRetainedMessageFlags, h)\n\tif len(fHeader) > 0 {\n\t\tflags, err := strconv.ParseUint(string(fHeader), 16, 8)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"invalid retained message flags: %v\", err)\n\t\t}\n\t\ttopic := getHeader(mqttNatsRetainedMessageTopic, h)\n\t\tsubj, _ := mqttToNATSSubjectConversion(topic, false)\n\t\treturn &mqttRetainedMsg{\n\t\t\tFlags:   byte(flags),\n\t\t\tSubject: string(subj),\n\t\t\tTopic:   string(topic),\n\t\t\tOrigin:  string(getHeader(mqttNatsRetainedMessageOrigin, h)),\n\t\t\tSource:  string(getHeader(mqttNatsRetainedMessageSource, h)),\n\t\t\tMsg:     m,\n\t\t}, nil\n\t} else {\n\t\tvar rm mqttRetainedMsg\n\t\tif err := json.Unmarshal(m, &rm); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn &rm, nil\n\t}\n}\n\n// Creates the session stream (limit msgs of 1) for this client ID if it does\n// not already exist. If it exists, recover the single record to rebuild the\n// state of the session. If there is a session record but this session is not\n// registered in the runtime of this server, then a request is made to the\n// owner to close the client associated with this session since specification\n// [MQTT-3.1.4-2] specifies that if the ClientId represents a Client already\n// connected to the Server then the Server MUST disconnect the existing client.\n//\n// Runs from the client's readLoop.\n// Lock not held on entry, but session is in the locked map.\nfunc (as *mqttAccountSessionManager) createOrRestoreSession(clientID string, opts *Options) (*mqttSession, bool, error) {\n\tjsa := &as.jsa\n\tformatError := func(errTxt string, err error) (*mqttSession, bool, error) {\n\t\taccName := jsa.c.acc.GetName()\n\t\treturn nil, false, fmt.Errorf(\"%s for account %q, session %q: %v\", errTxt, accName, clientID, err)\n\t}\n\n\thash := getHash(clientID)\n\tsmsg, err := jsa.loadSessionMsg(as.domainTk, hash)\n\tif err != nil {\n\t\tif isErrorOtherThan(err, JSNoMessageFoundErr) {\n\t\t\treturn formatError(\"loading session record\", err)\n\t\t}\n\t\t// Message not found, so reate the session...\n\t\t// Create a session and indicate that this session did not exist.\n\t\tsess := mqttSessionCreate(jsa, clientID, hash, 0, opts)\n\t\tsess.domainTk = as.domainTk\n\t\treturn sess, false, nil\n\t}\n\t// We need to recover the existing record now.\n\tps := &mqttPersistedSession{}\n\tif err := json.Unmarshal(smsg.Data, ps); err != nil {\n\t\treturn formatError(fmt.Sprintf(\"unmarshal of session record at sequence %v\", smsg.Sequence), err)\n\t}\n\n\t// Restore this session (even if we don't own it), the caller will do the right thing.\n\tsess := mqttSessionCreate(jsa, clientID, hash, smsg.Sequence, opts)\n\tsess.domainTk = as.domainTk\n\tsess.clean = ps.Clean\n\tsess.subs = ps.Subs\n\tsess.cons = ps.Cons\n\tsess.pubRelConsumer = ps.PubRel\n\tas.addSession(sess, true)\n\treturn sess, true, nil\n}\n\n// Sends a request to delete a message, but does not wait for the response.\n//\n// No lock held on entry.\nfunc (as *mqttAccountSessionManager) deleteRetainedMsg(seq uint64) {\n\tas.jsa.deleteMsg(mqttRetainedMsgsStreamName, seq, false)\n}\n\n// Sends a message indicating that a retained message on a given subject and stream sequence\n// is being removed.\nfunc (as *mqttAccountSessionManager) notifyRetainedMsgDeleted(subject string, seq uint64) {\n\treq := mqttRetMsgDel{\n\t\tSubject: subject,\n\t\tSeq:     seq,\n\t}\n\tb, _ := json.Marshal(&req)\n\tjsa := &as.jsa\n\tjsa.sendq.push(&mqttJSPubMsg{\n\t\tsubj: jsa.rplyr + mqttJSARetainedMsgDel,\n\t\tmsg:  b,\n\t})\n}\n\nfunc (as *mqttAccountSessionManager) transferUniqueSessStreamsToMuxed(log *Server) {\n\t// Set retry to true, will be set to false on success.\n\tretry := true\n\tdefer func() {\n\t\tif retry {\n\t\t\tnext := mqttDefaultTransferRetry\n\t\t\tlog.Warnf(\"Failed to transfer all MQTT session streams, will try again in %v\", next)\n\t\t\ttime.AfterFunc(next, func() { as.transferUniqueSessStreamsToMuxed(log) })\n\t\t}\n\t}()\n\n\tjsa := &as.jsa\n\tsni, err := jsa.newRequestEx(mqttJSAStreamNames, JSApiStreams, _EMPTY_, 0, nil)\n\tif err != nil {\n\t\tlog.Errorf(\"Unable to transfer MQTT session streams: %v\", err)\n\t\treturn\n\t}\n\tsnames := sni.(*JSApiStreamNamesResponse)\n\tif snames.Error != nil {\n\t\tlog.Errorf(\"Unable to transfer MQTT session streams: %v\", snames.ToError())\n\t\treturn\n\t}\n\tvar oldMQTTSessStreams []string\n\tfor _, sn := range snames.Streams {\n\t\tif strings.HasPrefix(sn, mqttSessionsStreamNamePrefix) {\n\t\t\toldMQTTSessStreams = append(oldMQTTSessStreams, sn)\n\t\t}\n\t}\n\tns := len(oldMQTTSessStreams)\n\tif ns == 0 {\n\t\t// Nothing to do\n\t\tretry = false\n\t\treturn\n\t}\n\tlog.Noticef(\"Transferring %v MQTT session streams...\", ns)\n\tfor _, sn := range oldMQTTSessStreams {\n\t\tlog.Noticef(\"  Transferring stream %q to %q\", sn, mqttSessStreamName)\n\t\tsmsg, err := jsa.loadLastMsgFor(sn, sn)\n\t\tif err != nil {\n\t\t\tlog.Errorf(\"   Unable to load session record: %v\", err)\n\t\t\treturn\n\t\t}\n\t\tps := &mqttPersistedSession{}\n\t\tif err := json.Unmarshal(smsg.Data, ps); err != nil {\n\t\t\tlog.Warnf(\"    Unable to unmarshal the content of this stream, may not be a legitimate MQTT session stream, skipping\")\n\t\t\tcontinue\n\t\t}\n\t\t// Store record to MQTT session stream\n\t\tif _, err := jsa.storeSessionMsg(as.domainTk, getHash(ps.ID), 0, smsg.Data); err != nil {\n\t\t\tlog.Errorf(\"    Unable to transfer the session record: %v\", err)\n\t\t\treturn\n\t\t}\n\t\tjsa.deleteStream(sn)\n\t}\n\tlog.Noticef(\"Transfer of %v MQTT session streams done!\", ns)\n\tretry = false\n}\n\nfunc (as *mqttAccountSessionManager) transferRetainedToPerKeySubjectStream(log *Server) error {\n\tjsa := &as.jsa\n\tvar processed int\n\tvar transferred int\n\n\tstart := time.Now()\n\tdeadline := start.Add(mqttRetainedTransferTimeout)\n\tfor {\n\t\t// Try and look up messages on the original undivided \"$MQTT.rmsgs\" subject.\n\t\t// If nothing is returned here, we assume to have migrated all old messages.\n\t\tsmsg, err := jsa.loadNextMsgFor(mqttRetainedMsgsStreamName, \"$MQTT.rmsgs\")\n\t\tif IsNatsErr(err, JSNoMessageFoundErr) {\n\t\t\t// We've ran out of messages to transfer, done.\n\t\t\tbreak\n\t\t}\n\t\tif err != nil {\n\t\t\tlog.Warnf(\"    Unable to transfer a retained message: failed to load from '$MQTT.rmsgs': %s\", err)\n\t\t\treturn err\n\t\t}\n\n\t\t// Unmarshal the message so that we can obtain the subject name. Do not\n\t\t// use mqttDecodeRetainedMessage() here because these messages are from\n\t\t// older versions, and contain the full JSON encoding in payload.\n\t\tvar rmsg mqttRetainedMsg\n\t\tif err = json.Unmarshal(smsg.Data, &rmsg); err == nil {\n\t\t\t// Store the message again, this time with the new per-key subject.\n\t\t\tsubject := mqttRetainedMsgsStreamSubject + rmsg.Subject\n\t\t\tif _, err = jsa.storeMsg(subject, 0, smsg.Data); err != nil {\n\t\t\t\tlog.Errorf(\"    Unable to transfer the retained message with sequence %d: %v\", smsg.Sequence, err)\n\t\t\t}\n\t\t\ttransferred++\n\t\t} else {\n\t\t\tlog.Warnf(\"    Unable to unmarshal retained message with sequence %d, skipping\", smsg.Sequence)\n\t\t}\n\n\t\t// Delete the original message.\n\t\tif err := jsa.deleteMsg(mqttRetainedMsgsStreamName, smsg.Sequence, true); err != nil {\n\t\t\tlog.Errorf(\"    Unable to clean up the retained message with sequence %d: %v\", smsg.Sequence, err)\n\t\t\treturn err\n\t\t}\n\t\tprocessed++\n\n\t\tnow := time.Now()\n\t\tif now.After(deadline) {\n\t\t\terr := fmt.Errorf(\"timed out while transferring retained messages from '$MQTT.rmsgs' after %v, %d processed, %d successfully transferred\", now.Sub(start), processed, transferred)\n\t\t\tlog.Noticef(err.Error())\n\t\t\treturn err\n\t\t}\n\t}\n\tif processed > 0 {\n\t\tlog.Noticef(\"Processed %d messages from '$MQTT.rmsgs', successfully transferred %d in %v\", processed, transferred, time.Since(start))\n\t} else {\n\t\tlog.Debugf(\"No messages found to transfer from '$MQTT.rmsgs'\")\n\t}\n\treturn nil\n}\n\nfunc (as *mqttAccountSessionManager) getCachedRetainedMsg(subject string) *mqttRetainedMsg {\n\tif as.rmsCache == nil {\n\t\treturn nil\n\t}\n\tv, ok := as.rmsCache.Load(subject)\n\tif !ok {\n\t\treturn nil\n\t}\n\trm := v.(*mqttRetainedMsg)\n\tif rm.expiresFromCache.Before(time.Now()) {\n\t\tas.rmsCache.Delete(subject)\n\t\treturn nil\n\t}\n\treturn rm\n}\n\nfunc (as *mqttAccountSessionManager) setCachedRetainedMsg(subject string, rm *mqttRetainedMsg, onlyReplace bool, copyBytesToCache bool) {\n\tif as.rmsCache == nil || rm == nil {\n\t\treturn\n\t}\n\trm.expiresFromCache = time.Now().Add(mqttRetainedCacheTTL)\n\tif onlyReplace {\n\t\tif _, ok := as.rmsCache.Load(subject); !ok {\n\t\t\treturn\n\t\t}\n\t}\n\tif copyBytesToCache {\n\t\trm.Msg = copyBytes(rm.Msg)\n\t}\n\tas.rmsCache.Store(subject, rm)\n}\n\n//////////////////////////////////////////////////////////////////////////////\n//\n// MQTT session related functions\n//\n//////////////////////////////////////////////////////////////////////////////\n\n// Returns a new mqttSession object with max ack pending set based on\n// option or use mqttDefaultMaxAckPending if no option set.\nfunc mqttSessionCreate(jsa *mqttJSA, id, idHash string, seq uint64, opts *Options) *mqttSession {\n\tmaxp := opts.MQTT.MaxAckPending\n\tif maxp == 0 {\n\t\tmaxp = mqttDefaultMaxAckPending\n\t}\n\n\treturn &mqttSession{\n\t\tjsa:                    jsa,\n\t\tid:                     id,\n\t\tidHash:                 idHash,\n\t\tseq:                    seq,\n\t\tmaxp:                   maxp,\n\t\tpubRelSubject:          mqttPubRelSubjectPrefix + idHash,\n\t\tpubRelDeliverySubject:  mqttPubRelDeliverySubjectPrefix + idHash,\n\t\tpubRelDeliverySubjectB: []byte(mqttPubRelDeliverySubjectPrefix + idHash),\n\t}\n}\n\n// Persists a session. Note that if the session's current client does not match\n// the given client, nothing is done.\n//\n// Lock not held on entry.\nfunc (sess *mqttSession) save() error {\n\tsess.mu.Lock()\n\tps := mqttPersistedSession{\n\t\tOrigin: sess.jsa.id,\n\t\tID:     sess.id,\n\t\tClean:  sess.clean,\n\t\tSubs:   sess.subs,\n\t\tCons:   sess.cons,\n\t\tPubRel: sess.pubRelConsumer,\n\t}\n\tb, _ := json.Marshal(&ps)\n\n\tdomainTk, cidHash := sess.domainTk, sess.idHash\n\tseq := sess.seq\n\tsess.mu.Unlock()\n\n\tvar hdr int\n\tif seq != 0 {\n\t\tbb := bytes.Buffer{}\n\t\tbb.WriteString(hdrLine)\n\t\tbb.WriteString(JSExpectedLastSubjSeq)\n\t\tbb.WriteString(\":\")\n\t\tbb.WriteString(strconv.FormatInt(int64(seq), 10))\n\t\tbb.WriteString(CR_LF)\n\t\tbb.WriteString(CR_LF)\n\t\thdr = bb.Len()\n\t\tbb.Write(b)\n\t\tb = bb.Bytes()\n\t}\n\n\tresp, err := sess.jsa.storeSessionMsg(domainTk, cidHash, hdr, b)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"unable to persist session %q (seq=%v): %v\", ps.ID, seq, err)\n\t}\n\tsess.mu.Lock()\n\tsess.seq = resp.Sequence\n\tsess.mu.Unlock()\n\treturn nil\n}\n\n// Clear the session.\n//\n// Runs from the client's readLoop.\n// Lock not held on entry, but session is in the locked map.\nfunc (sess *mqttSession) clear(noWait bool) error {\n\tvar durs []string\n\tvar pubRelDur string\n\n\tsess.mu.Lock()\n\tid := sess.id\n\tseq := sess.seq\n\tif l := len(sess.cons); l > 0 {\n\t\tdurs = make([]string, 0, l)\n\t}\n\tfor sid, cc := range sess.cons {\n\t\tdelete(sess.cons, sid)\n\t\tdurs = append(durs, cc.Durable)\n\t}\n\tif sess.pubRelConsumer != nil {\n\t\tpubRelDur = sess.pubRelConsumer.Durable\n\t}\n\n\tsess.subs = nil\n\tsess.pendingPublish = nil\n\tsess.pendingPubRel = nil\n\tsess.cpending = nil\n\tsess.pubRelConsumer = nil\n\tsess.seq = 0\n\tsess.tmaxack = 0\n\tsess.mu.Unlock()\n\n\tfor _, dur := range durs {\n\t\tif _, err := sess.jsa.deleteConsumer(mqttStreamName, dur, noWait); isErrorOtherThan(err, JSConsumerNotFoundErr) {\n\t\t\treturn fmt.Errorf(\"unable to delete consumer %q for session %q: %v\", dur, sess.id, err)\n\t\t}\n\t}\n\tif pubRelDur != _EMPTY_ {\n\t\t_, err := sess.jsa.deleteConsumer(mqttOutStreamName, pubRelDur, noWait)\n\t\tif isErrorOtherThan(err, JSConsumerNotFoundErr) {\n\t\t\treturn fmt.Errorf(\"unable to delete consumer %q for session %q: %v\", pubRelDur, sess.id, err)\n\t\t}\n\t}\n\n\tif seq > 0 {\n\t\terr := sess.jsa.deleteMsg(mqttSessStreamName, seq, !noWait)\n\t\t// Ignore the various errors indicating that the message (or sequence)\n\t\t// is already deleted, can happen in a cluster.\n\t\tif isErrorOtherThan(err, JSSequenceNotFoundErrF) {\n\t\t\tif isErrorOtherThan(err, JSStreamMsgDeleteFailedF) || !strings.Contains(err.Error(), ErrStoreMsgNotFound.Error()) {\n\t\t\t\treturn fmt.Errorf(\"unable to delete session %q record at sequence %v: %v\", id, seq, err)\n\t\t\t}\n\t\t}\n\t}\n\treturn nil\n}\n\n// This will update the session record for this client in the account's MQTT\n// sessions stream if the session had any change in the subscriptions.\n//\n// Runs from the client's readLoop.\n// Lock not held on entry, but session is in the locked map.\nfunc (sess *mqttSession) update(filters []*mqttFilter, add bool) error {\n\t// Evaluate if we need to persist anything.\n\tvar needUpdate bool\n\tfor _, f := range filters {\n\t\tif add {\n\t\t\tif f.qos == mqttSubAckFailure {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif qos, ok := sess.subs[f.filter]; !ok || qos != f.qos {\n\t\t\t\tif sess.subs == nil {\n\t\t\t\t\tsess.subs = make(map[string]byte)\n\t\t\t\t}\n\t\t\t\tsess.subs[f.filter] = f.qos\n\t\t\t\tneedUpdate = true\n\t\t\t}\n\t\t} else {\n\t\t\tif _, ok := sess.subs[f.filter]; ok {\n\t\t\t\tdelete(sess.subs, f.filter)\n\t\t\t\tneedUpdate = true\n\t\t\t}\n\t\t}\n\t}\n\tvar err error\n\tif needUpdate {\n\t\terr = sess.save()\n\t}\n\treturn err\n}\n\nfunc (sess *mqttSession) bumpPI() uint16 {\n\tvar avail bool\n\tnext := sess.last_pi\n\tfor i := 0; i < 0xFFFF; i++ {\n\t\tnext++\n\t\tif next == 0 {\n\t\t\tnext = 1\n\t\t}\n\n\t\t_, usedInPublish := sess.pendingPublish[next]\n\t\t_, usedInPubRel := sess.pendingPubRel[next]\n\t\tif !usedInPublish && !usedInPubRel {\n\t\t\tsess.last_pi = next\n\t\t\tavail = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif !avail {\n\t\treturn 0\n\t}\n\treturn sess.last_pi\n}\n\n// trackPublishRetained is invoked when a retained (QoS) message is published.\n// It need a new PI to be allocated, so we add it to the pendingPublish map,\n// with an empty value. Since cpending (not pending) is used to serialize the PI\n// mappings, we need to add this PI there as well. Make a unique key by using\n// mqttRetainedMsgsStreamName for the durable name, and PI for sseq.\n//\n// Lock held on entry\nfunc (sess *mqttSession) trackPublishRetained() uint16 {\n\t// Make sure we initialize the tracking maps.\n\tif sess.pendingPublish == nil {\n\t\tsess.pendingPublish = make(map[uint16]*mqttPending)\n\t}\n\tif sess.cpending == nil {\n\t\tsess.cpending = make(map[string]map[uint64]uint16)\n\t}\n\n\tpi := sess.bumpPI()\n\tif pi == 0 {\n\t\treturn 0\n\t}\n\tsess.pendingPublish[pi] = &mqttPending{}\n\n\treturn pi\n}\n\n// trackPublish is invoked when a (QoS) PUBLISH message is to be delivered. It\n// detects an untracked (new) message based on its sequence extracted from its\n// delivery-time jsAckSubject, and adds it to the tracking maps. Returns a PI to\n// use for the message (new, or previously used), and whether this is a\n// duplicate delivery attempt.\n//\n// Lock held on entry\nfunc (sess *mqttSession) trackPublish(jsDur, jsAckSubject string) (uint16, bool) {\n\tvar dup bool\n\tvar pi uint16\n\n\tif jsAckSubject == _EMPTY_ || jsDur == _EMPTY_ {\n\t\treturn 0, false\n\t}\n\n\t// Make sure we initialize the tracking maps.\n\tif sess.pendingPublish == nil {\n\t\tsess.pendingPublish = make(map[uint16]*mqttPending)\n\t}\n\tif sess.cpending == nil {\n\t\tsess.cpending = make(map[string]map[uint64]uint16)\n\t}\n\n\t// Get the stream sequence and duplicate flag from the ack reply subject.\n\tsseq, _, dcount := ackReplyInfo(jsAckSubject)\n\tif dcount > 1 {\n\t\tdup = true\n\t}\n\n\tvar ack *mqttPending\n\tsseqToPi, ok := sess.cpending[jsDur]\n\tif !ok {\n\t\tsseqToPi = make(map[uint64]uint16)\n\t\tsess.cpending[jsDur] = sseqToPi\n\t} else {\n\t\tpi = sseqToPi[sseq]\n\t}\n\n\tif pi != 0 {\n\t\t// There is a possible race between a PUBLISH re-delivery calling us,\n\t\t// and a PUBREC received already having submitting a PUBREL into JS . If\n\t\t// so, indicate no need for (re-)delivery by returning a PI of 0.\n\t\t_, usedForPubRel := sess.pendingPubRel[pi]\n\t\tif /*dup && */ usedForPubRel {\n\t\t\treturn 0, false\n\t\t}\n\n\t\t// We should have a pending JS ACK for this PI.\n\t\tack = sess.pendingPublish[pi]\n\t} else {\n\t\t// sess.maxp will always have a value > 0.\n\t\tif len(sess.pendingPublish) >= int(sess.maxp) {\n\t\t\t// Indicate that we did not assign a packet identifier.\n\t\t\t// The caller will not send the message to the subscription\n\t\t\t// and JS will redeliver later, based on consumer's AckWait.\n\t\t\treturn 0, false\n\t\t}\n\n\t\tpi = sess.bumpPI()\n\t\tif pi == 0 {\n\t\t\treturn 0, false\n\t\t}\n\n\t\tsseqToPi[sseq] = pi\n\t}\n\n\tif ack == nil {\n\t\tsess.pendingPublish[pi] = &mqttPending{\n\t\t\tjsDur:        jsDur,\n\t\t\tsseq:         sseq,\n\t\t\tjsAckSubject: jsAckSubject,\n\t\t}\n\t} else {\n\t\tack.jsAckSubject = jsAckSubject\n\t\tack.sseq = sseq\n\t\tack.jsDur = jsDur\n\t}\n\n\treturn pi, dup\n}\n\n// Stops a PI from being tracked as a PUBLISH. It can still be in use for a\n// pending PUBREL.\n//\n// Lock held on entry\nfunc (sess *mqttSession) untrackPublish(pi uint16) (jsAckSubject string) {\n\tack, ok := sess.pendingPublish[pi]\n\tif !ok {\n\t\treturn _EMPTY_\n\t}\n\n\tdelete(sess.pendingPublish, pi)\n\tif len(sess.pendingPublish) == 0 {\n\t\tsess.last_pi = 0\n\t}\n\n\tif len(sess.cpending) != 0 && ack.jsDur != _EMPTY_ {\n\t\tif sseqToPi := sess.cpending[ack.jsDur]; sseqToPi != nil {\n\t\t\tdelete(sseqToPi, ack.sseq)\n\t\t}\n\t}\n\n\treturn ack.jsAckSubject\n}\n\n// trackAsPubRel is invoked in 2 cases: (a) when we receive a PUBREC and we need\n// to change from tracking the PI as a PUBLISH to a PUBREL; and (b) when we\n// attempt to deliver the PUBREL to record the JS ack subject for it.\n//\n// Lock held on entry\nfunc (sess *mqttSession) trackAsPubRel(pi uint16, jsAckSubject string) {\n\tif sess.pubRelConsumer == nil {\n\t\t// The cosumer MUST be set up already.\n\t\treturn\n\t}\n\tjsDur := sess.pubRelConsumer.Durable\n\n\tif sess.pendingPubRel == nil {\n\t\tsess.pendingPubRel = make(map[uint16]*mqttPending)\n\t}\n\n\tif jsAckSubject == _EMPTY_ {\n\t\tsess.pendingPubRel[pi] = &mqttPending{\n\t\t\tjsDur: jsDur,\n\t\t}\n\t\treturn\n\t}\n\n\tsseq, _, _ := ackReplyInfo(jsAckSubject)\n\n\tif sess.cpending == nil {\n\t\tsess.cpending = make(map[string]map[uint64]uint16)\n\t}\n\tsseqToPi := sess.cpending[jsDur]\n\tif sseqToPi == nil {\n\t\tsseqToPi = make(map[uint64]uint16)\n\t\tsess.cpending[jsDur] = sseqToPi\n\t}\n\tsseqToPi[sseq] = pi\n\tsess.pendingPubRel[pi] = &mqttPending{\n\t\tjsDur:        sess.pubRelConsumer.Durable,\n\t\tsseq:         sseq,\n\t\tjsAckSubject: jsAckSubject,\n\t}\n}\n\n// Stops a PI from being tracked as a PUBREL.\n//\n// Lock held on entry\nfunc (sess *mqttSession) untrackPubRel(pi uint16) (jsAckSubject string) {\n\tack, ok := sess.pendingPubRel[pi]\n\tif !ok {\n\t\treturn _EMPTY_\n\t}\n\n\tdelete(sess.pendingPubRel, pi)\n\n\tif sess.pubRelConsumer != nil && len(sess.cpending) > 0 {\n\t\tif sseqToPi := sess.cpending[ack.jsDur]; sseqToPi != nil {\n\t\t\tdelete(sseqToPi, ack.sseq)\n\t\t}\n\t}\n\n\treturn ack.jsAckSubject\n}\n\n// Sends a consumer delete request, but does not wait for response.\n//\n// Lock not held on entry.\nfunc (sess *mqttSession) deleteConsumer(cc *ConsumerConfig) {\n\tsess.mu.Lock()\n\tsess.tmaxack -= cc.MaxAckPending\n\tsess.jsa.deleteConsumer(mqttStreamName, cc.Durable, true)\n\tsess.mu.Unlock()\n}\n\n//////////////////////////////////////////////////////////////////////////////\n//\n// CONNECT protocol related functions\n//\n//////////////////////////////////////////////////////////////////////////////\n\n// Parse the MQTT connect protocol\nfunc (c *client) mqttParseConnect(r *mqttReader, hasMappings bool) (byte, *mqttConnectProto, error) {\n\t// Protocol name\n\tproto, err := r.readBytes(\"protocol name\", false)\n\tif err != nil {\n\t\treturn 0, nil, err\n\t}\n\n\t// Spec [MQTT-3.1.2-1]\n\tif !bytes.Equal(proto, mqttProtoName) {\n\t\t// Check proto name against v3.1 to report better error\n\t\tif bytes.Equal(proto, mqttOldProtoName) {\n\t\t\treturn 0, nil, fmt.Errorf(\"older protocol %q not supported\", proto)\n\t\t}\n\t\treturn 0, nil, fmt.Errorf(\"expected connect packet with protocol name %q, got %q\", mqttProtoName, proto)\n\t}\n\n\t// Protocol level\n\tlevel, err := r.readByte(\"protocol level\")\n\tif err != nil {\n\t\treturn 0, nil, err\n\t}\n\t// Spec [MQTT-3.1.2-2]\n\tif level != mqttProtoLevel {\n\t\treturn mqttConnAckRCUnacceptableProtocolVersion, nil, fmt.Errorf(\"unacceptable protocol version of %v\", level)\n\t}\n\n\tcp := &mqttConnectProto{}\n\t// Connect flags\n\tcp.flags, err = r.readByte(\"flags\")\n\tif err != nil {\n\t\treturn 0, nil, err\n\t}\n\n\t// Spec [MQTT-3.1.2-3]\n\tif cp.flags&mqttConnFlagReserved != 0 {\n\t\treturn 0, nil, errMQTTConnFlagReserved\n\t}\n\n\tvar hasWill bool\n\twqos := (cp.flags & mqttConnFlagWillQoS) >> 3\n\twretain := cp.flags&mqttConnFlagWillRetain != 0\n\t// Spec [MQTT-3.1.2-11]\n\tif cp.flags&mqttConnFlagWillFlag == 0 {\n\t\t// Spec [MQTT-3.1.2-13]\n\t\tif wqos != 0 {\n\t\t\treturn 0, nil, fmt.Errorf(\"if Will flag is set to 0, Will QoS must be 0 too, got %v\", wqos)\n\t\t}\n\t\t// Spec [MQTT-3.1.2-15]\n\t\tif wretain {\n\t\t\treturn 0, nil, errMQTTWillAndRetainFlag\n\t\t}\n\t} else {\n\t\t// Spec [MQTT-3.1.2-14]\n\t\tif wqos == 3 {\n\t\t\treturn 0, nil, fmt.Errorf(\"if Will flag is set to 1, Will QoS can be 0, 1 or 2, got %v\", wqos)\n\t\t}\n\t\thasWill = true\n\t}\n\n\tif c.mqtt.rejectQoS2Pub && hasWill && wqos == 2 {\n\t\treturn mqttConnAckRCQoS2WillRejected, nil, fmt.Errorf(\"server does not accept QoS2 for Will messages\")\n\t}\n\n\t// Spec [MQTT-3.1.2-19]\n\thasUser := cp.flags&mqttConnFlagUsernameFlag != 0\n\t// Spec [MQTT-3.1.2-21]\n\thasPassword := cp.flags&mqttConnFlagPasswordFlag != 0\n\t// Spec [MQTT-3.1.2-22]\n\tif !hasUser && hasPassword {\n\t\treturn 0, nil, errMQTTPasswordFlagAndNoUser\n\t}\n\n\t// Keep alive\n\tvar ka uint16\n\tka, err = r.readUint16(\"keep alive\")\n\tif err != nil {\n\t\treturn 0, nil, err\n\t}\n\t// Spec [MQTT-3.1.2-24]\n\tif ka > 0 {\n\t\tcp.rd = time.Duration(float64(ka)*1.5) * time.Second\n\t}\n\n\t// Payload starts here and order is mandated by:\n\t// Spec [MQTT-3.1.3-1]: client ID, will topic, will message, username, password\n\n\t// Client ID\n\tc.mqtt.cid, err = r.readString(\"client ID\")\n\tif err != nil {\n\t\treturn 0, nil, err\n\t}\n\t// Spec [MQTT-3.1.3-7]\n\tif c.mqtt.cid == _EMPTY_ {\n\t\tif cp.flags&mqttConnFlagCleanSession == 0 {\n\t\t\treturn mqttConnAckRCIdentifierRejected, nil, errMQTTCIDEmptyNeedsCleanFlag\n\t\t}\n\t\t// Spec [MQTT-3.1.3-6]\n\t\tc.mqtt.cid = nuid.Next()\n\t}\n\t// Spec [MQTT-3.1.3-4] and [MQTT-3.1.3-9]\n\tif !utf8.ValidString(c.mqtt.cid) {\n\t\treturn mqttConnAckRCIdentifierRejected, nil, fmt.Errorf(\"invalid utf8 for client ID: %q\", c.mqtt.cid)\n\t}\n\n\tif hasWill {\n\t\tcp.will = &mqttWill{\n\t\t\tqos:    wqos,\n\t\t\tretain: wretain,\n\t\t}\n\t\tvar topic []byte\n\t\t// Need to make a copy since we need to hold to this topic after the\n\t\t// parsing of this protocol.\n\t\ttopic, err = r.readBytes(\"Will topic\", true)\n\t\tif err != nil {\n\t\t\treturn 0, nil, err\n\t\t}\n\t\tif len(topic) == 0 {\n\t\t\treturn 0, nil, errMQTTEmptyWillTopic\n\t\t}\n\t\tif !utf8.Valid(topic) {\n\t\t\treturn 0, nil, fmt.Errorf(\"invalid utf8 for Will topic %q\", topic)\n\t\t}\n\t\t// Convert MQTT topic to NATS subject\n\t\tcp.will.subject, err = mqttTopicToNATSPubSubject(topic)\n\t\tif err != nil {\n\t\t\treturn 0, nil, err\n\t\t}\n\t\t// Check for subject mapping.\n\t\tif hasMappings {\n\t\t\t// For selectMappedSubject to work, we need to have c.pa.subject set.\n\t\t\t// If there is a change, c.pa.mapped will be set after the call.\n\t\t\tc.pa.subject = cp.will.subject\n\t\t\tif changed := c.selectMappedSubject(); changed {\n\t\t\t\t// We need to keep track of the NATS subject/mapped in the `cp` structure.\n\t\t\t\tcp.will.subject = c.pa.subject\n\t\t\t\tcp.will.mapped = c.pa.mapped\n\t\t\t\t// We also now need to map the original MQTT topic to the new topic\n\t\t\t\t// based on the new subject.\n\t\t\t\ttopic = natsSubjectToMQTTTopic(cp.will.subject)\n\t\t\t}\n\t\t\t// Reset those now.\n\t\t\tc.pa.subject, c.pa.mapped = nil, nil\n\t\t}\n\t\tcp.will.topic = topic\n\t\t// Now \"will\" message.\n\t\t// Ask for a copy since we need to hold to this after parsing of this protocol.\n\t\tcp.will.message, err = r.readBytes(\"Will message\", true)\n\t\tif err != nil {\n\t\t\treturn 0, nil, err\n\t\t}\n\t}\n\n\tif hasUser {\n\t\tc.opts.Username, err = r.readString(\"user name\")\n\t\tif err != nil {\n\t\t\treturn 0, nil, err\n\t\t}\n\t\tif c.opts.Username == _EMPTY_ {\n\t\t\treturn mqttConnAckRCBadUserOrPassword, nil, errMQTTEmptyUsername\n\t\t}\n\t\t// Spec [MQTT-3.1.3-11]\n\t\tif !utf8.ValidString(c.opts.Username) {\n\t\t\treturn mqttConnAckRCBadUserOrPassword, nil, fmt.Errorf(\"invalid utf8 for user name %q\", c.opts.Username)\n\t\t}\n\t}\n\n\tif hasPassword {\n\t\tc.opts.Password, err = r.readString(\"password\")\n\t\tif err != nil {\n\t\t\treturn 0, nil, err\n\t\t}\n\t\tc.opts.Token = c.opts.Password\n\t\tc.opts.JWT = c.opts.Password\n\t}\n\treturn 0, cp, nil\n}\n\nfunc (c *client) mqttConnectTrace(cp *mqttConnectProto) string {\n\ttrace := fmt.Sprintf(\"clientID=%s\", c.mqtt.cid)\n\tif cp.rd > 0 {\n\t\ttrace += fmt.Sprintf(\" keepAlive=%v\", cp.rd)\n\t}\n\tif cp.will != nil {\n\t\ttrace += fmt.Sprintf(\" will=(topic=%s QoS=%v retain=%v)\",\n\t\t\tcp.will.topic, cp.will.qos, cp.will.retain)\n\t}\n\tif cp.flags&mqttConnFlagCleanSession != 0 {\n\t\ttrace += \" clean\"\n\t}\n\tif c.opts.Username != _EMPTY_ {\n\t\ttrace += fmt.Sprintf(\" username=%s\", c.opts.Username)\n\t}\n\tif c.opts.Password != _EMPTY_ {\n\t\ttrace += \" password=****\"\n\t}\n\treturn trace\n}\n\n// Process the CONNECT packet.\n//\n// For the first session on the account, an account session manager will be created,\n// along with the JetStream streams/consumer necessary for the working of MQTT.\n//\n// The session, identified by a client ID, will be registered, or if already existing,\n// will be resumed. If the session exists but is associated with an existing client,\n// the old client is evicted, as per the specifications.\n//\n// Due to specific locking requirements around JS API requests, we cannot hold some\n// locks for the entire duration of processing of some protocols, therefore, we use\n// a map that registers the client ID in a \"locked\" state. If a different client tries\n// to connect and the server detects that the client ID is in that map, it will try\n// a little bit until it is not, or fail the new client, since we can't protect\n// processing of protocols in the original client. This is not expected to happen often.\n//\n// Runs from the client's readLoop.\n// No lock held on entry.\nfunc (s *Server) mqttProcessConnect(c *client, cp *mqttConnectProto, trace bool) error {\n\tsendConnAck := func(rc byte, sessp bool) {\n\t\tc.mqttEnqueueConnAck(rc, sessp)\n\t\tif trace {\n\t\t\tc.traceOutOp(\"CONNACK\", []byte(fmt.Sprintf(\"sp=%v rc=%v\", sessp, rc)))\n\t\t}\n\t}\n\n\tc.mu.Lock()\n\tcid := c.mqtt.cid\n\tc.clearAuthTimer()\n\tc.mu.Unlock()\n\tif !s.isClientAuthorized(c) {\n\t\tif trace {\n\t\t\tc.traceOutOp(\"CONNACK\", []byte(fmt.Sprintf(\"sp=%v rc=%v\", false, mqttConnAckRCNotAuthorized)))\n\t\t}\n\t\tc.authViolation()\n\t\treturn ErrAuthentication\n\t}\n\t// Now that we are authenticated, we have the client bound to the account.\n\t// Get the account's level MQTT sessions manager. If it does not exists yet,\n\t// this will create it along with the streams where sessions and messages\n\t// are stored.\n\tasm, err := s.getOrCreateMQTTAccountSessionManager(c)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Most of the session state is altered only in the readLoop so does not\n\t// need locking. For things that can be access in the readLoop and in\n\t// callbacks, we will use explicit locking.\n\t// To prevent other clients to connect with the same client ID, we will\n\t// add the client ID to a \"locked\" map so that the connect somewhere else\n\t// is put on hold.\n\t// This keep track of how many times this client is detecting that its\n\t// client ID is in the locked map. After a short amount, the server will\n\t// fail this inbound client.\n\tlocked := 0\n\nCHECK:\n\tasm.mu.Lock()\n\t// Check if different applications keep trying to connect with the same\n\t// client ID at the same time.\n\tif tm, ok := asm.flappers[cid]; ok {\n\t\t// If the last time it tried to connect was more than 1 sec ago,\n\t\t// then accept and remove from flappers map.\n\t\tif time.Now().UnixNano()-tm > int64(mqttSessJailDur) {\n\t\t\tasm.removeSessFromFlappers(cid)\n\t\t} else {\n\t\t\t// Will hold this client for a second and then close it. We\n\t\t\t// do this so that if the client has a reconnect feature we\n\t\t\t// don't end-up with very rapid flapping between apps.\n\t\t\t// We need to wait in place and not schedule the connection\n\t\t\t// close because if this is a misbehaved client that does\n\t\t\t// not wait for the CONNACK and sends other protocols, the\n\t\t\t// server would not have a fully setup client and may panic.\n\t\t\tasm.mu.Unlock()\n\t\t\tselect {\n\t\t\tcase <-s.quitCh:\n\t\t\tcase <-time.After(mqttSessJailDur):\n\t\t\t}\n\t\t\tc.closeConnection(DuplicateClientID)\n\t\t\treturn ErrConnectionClosed\n\t\t}\n\t}\n\t// If an existing session is in the process of processing some packet, we can't\n\t// evict the old client just yet. So try again to see if the state clears, but\n\t// if it does not, then we have no choice but to fail the new client instead of\n\t// the old one.\n\tif _, ok := asm.sessLocked[cid]; ok {\n\t\tasm.mu.Unlock()\n\t\tif locked++; locked == 10 {\n\t\t\treturn fmt.Errorf(\"other session with client ID %q is in the process of connecting\", cid)\n\t\t}\n\t\ttime.Sleep(100 * time.Millisecond)\n\t\tgoto CHECK\n\t}\n\n\t// Register this client ID the \"locked\" map for the duration if this function.\n\tasm.sessLocked[cid] = struct{}{}\n\t// And remove it on exit, regardless of error or not.\n\tdefer func() {\n\t\tasm.mu.Lock()\n\t\tdelete(asm.sessLocked, cid)\n\t\tasm.mu.Unlock()\n\t}()\n\n\t// Is the client requesting a clean session or not.\n\tcleanSess := cp.flags&mqttConnFlagCleanSession != 0\n\t// Session present? Assume false, will be set to true only when applicable.\n\tsessp := false\n\t// Do we have an existing session for this client ID\n\tes, exists := asm.sessions[cid]\n\tasm.mu.Unlock()\n\n\t// The session is not in the map, but may be on disk, so try to recover\n\t// or create the stream if not.\n\tif !exists {\n\t\tes, exists, err = asm.createOrRestoreSession(cid, s.getOpts())\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\tif exists {\n\t\t// Clear the session if client wants a clean session.\n\t\t// Also, Spec [MQTT-3.2.2-1]: don't report session present\n\t\tif cleanSess || es.clean {\n\t\t\t// Spec [MQTT-3.1.2-6]: If CleanSession is set to 1, the Client and\n\t\t\t// Server MUST discard any previous Session and start a new one.\n\t\t\t// This Session lasts as long as the Network Connection. State data\n\t\t\t// associated with this Session MUST NOT be reused in any subsequent\n\t\t\t// Session.\n\t\t\tif err := es.clear(false); err != nil {\n\t\t\t\tasm.removeSession(es, true)\n\t\t\t\treturn err\n\t\t\t}\n\t\t} else {\n\t\t\t// Report to the client that the session was present\n\t\t\tsessp = true\n\t\t}\n\t\t// Spec [MQTT-3.1.4-2]. If the ClientId represents a Client already\n\t\t// connected to the Server then the Server MUST disconnect the existing\n\t\t// client.\n\t\t// Bind with the new client. This needs to be protected because can be\n\t\t// accessed outside of the readLoop.\n\t\tes.mu.Lock()\n\t\tec := es.c\n\t\tes.c = c\n\t\tes.clean = cleanSess\n\t\tes.mu.Unlock()\n\t\tif ec != nil {\n\t\t\t// Remove \"will\" of existing client before closing\n\t\t\tec.mu.Lock()\n\t\t\tec.mqtt.cp.will = nil\n\t\t\tec.mu.Unlock()\n\t\t\t// Add to the map of the flappers\n\t\t\tasm.mu.Lock()\n\t\t\tasm.addSessToFlappers(cid)\n\t\t\tasm.mu.Unlock()\n\t\t\tc.Warnf(\"Replacing old client %q since both have the same client ID %q\", ec, cid)\n\t\t\t// Close old client in separate go routine\n\t\t\tgo ec.closeConnection(DuplicateClientID)\n\t\t}\n\t} else {\n\t\t// Spec [MQTT-3.2.2-3]: if the Server does not have stored Session state,\n\t\t// it MUST set Session Present to 0 in the CONNACK packet.\n\t\tes.mu.Lock()\n\t\tes.c, es.clean = c, cleanSess\n\t\tes.mu.Unlock()\n\t\t// Now add this new session into the account sessions\n\t\tasm.addSession(es, true)\n\t}\n\t// We would need to save only if it did not exist previously, but we save\n\t// always in case we are running in cluster mode. This will notify other\n\t// running servers that this session is being used.\n\tif err := es.save(); err != nil {\n\t\tasm.removeSession(es, true)\n\t\treturn err\n\t}\n\tc.mu.Lock()\n\tc.flags.set(connectReceived)\n\tc.mqtt.cp = cp\n\tc.mqtt.asm = asm\n\tc.mqtt.sess = es\n\tc.mu.Unlock()\n\n\t// Spec [MQTT-3.2.0-1]: CONNACK must be the first protocol sent to the session.\n\tsendConnAck(mqttConnAckRCConnectionAccepted, sessp)\n\n\t// Process possible saved subscriptions.\n\tif l := len(es.subs); l > 0 {\n\t\tfilters := make([]*mqttFilter, 0, l)\n\t\tfor subject, qos := range es.subs {\n\t\t\tfilters = append(filters, &mqttFilter{filter: subject, qos: qos})\n\t\t}\n\t\tif _, err := asm.processSubs(es, c, filters, false, trace); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc (c *client) mqttEnqueueConnAck(rc byte, sessionPresent bool) {\n\tproto := [4]byte{mqttPacketConnectAck, 2, 0, rc}\n\tc.mu.Lock()\n\t// Spec [MQTT-3.2.2-4]. If return code is different from 0, then\n\t// session present flag must be set to 0.\n\tif rc == 0 {\n\t\tif sessionPresent {\n\t\t\tproto[2] = 1\n\t\t}\n\t}\n\tc.enqueueProto(proto[:])\n\tc.mu.Unlock()\n}\n\nfunc (s *Server) mqttHandleWill(c *client) {\n\tc.mu.Lock()\n\tif c.mqtt.cp == nil {\n\t\tc.mu.Unlock()\n\t\treturn\n\t}\n\twill := c.mqtt.cp.will\n\tif will == nil {\n\t\tc.mu.Unlock()\n\t\treturn\n\t}\n\tpp := c.mqtt.pp\n\tpp.topic = will.topic\n\tpp.subject = will.subject\n\tpp.mapped = will.mapped\n\tpp.msg = will.message\n\tpp.sz = len(will.message)\n\tpp.pi = 0\n\tpp.flags = will.qos << 1\n\tif will.retain {\n\t\tpp.flags |= mqttPubFlagRetain\n\t}\n\tc.mu.Unlock()\n\ts.mqttInitiateMsgDelivery(c, pp)\n\tc.flushClients(0)\n}\n\n//////////////////////////////////////////////////////////////////////////////\n//\n// PUBLISH protocol related functions\n//\n//////////////////////////////////////////////////////////////////////////////\n\nfunc (c *client) mqttParsePub(r *mqttReader, pl int, pp *mqttPublish, hasMappings bool) error {\n\tqos := mqttGetQoS(pp.flags)\n\tif qos > 2 {\n\t\treturn fmt.Errorf(\"QoS=%v is invalid in MQTT\", qos)\n\t}\n\n\tif c.mqtt.rejectQoS2Pub && qos == 2 {\n\t\treturn fmt.Errorf(\"QoS=2 is disabled for PUBLISH messages\")\n\t}\n\n\t// Keep track of where we are when starting to read the variable header\n\tstart := r.pos\n\n\tvar err error\n\tpp.topic, err = r.readBytes(\"topic\", false)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif len(pp.topic) == 0 {\n\t\treturn errMQTTTopicIsEmpty\n\t}\n\t// Convert the topic to a NATS subject. This call will also check that\n\t// there is no MQTT wildcards (Spec [MQTT-3.3.2-2] and [MQTT-4.7.1-1])\n\t// Note that this may not result in a copy if there is no conversion.\n\t// It is good because after the message is processed we won't have a\n\t// reference to the buffer and we save a copy.\n\tpp.subject, err = mqttTopicToNATSPubSubject(pp.topic)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Check for subject mapping.\n\tif hasMappings {\n\t\t// For selectMappedSubject to work, we need to have c.pa.subject set.\n\t\t// If there is a change, c.pa.mapped will be set after the call.\n\t\tc.pa.subject = pp.subject\n\t\tif changed := c.selectMappedSubject(); changed {\n\t\t\t// We need to keep track of the NATS subject/mapped in the `pp` structure.\n\t\t\tpp.subject = c.pa.subject\n\t\t\tpp.mapped = c.pa.mapped\n\t\t\t// We also now need to map the original MQTT topic to the new topic\n\t\t\t// based on the new subject.\n\t\t\tpp.topic = natsSubjectToMQTTTopic(pp.subject)\n\t\t}\n\t\t// Reset those now.\n\t\tc.pa.subject, c.pa.mapped = nil, nil\n\t}\n\n\tif qos > 0 {\n\t\tpp.pi, err = r.readUint16(\"packet identifier\")\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif pp.pi == 0 {\n\t\t\treturn fmt.Errorf(\"with QoS=%v, packet identifier cannot be 0\", qos)\n\t\t}\n\t} else {\n\t\tpp.pi = 0\n\t}\n\n\t// The message payload will be the total packet length minus\n\t// what we have consumed for the variable header\n\tpp.sz = pl - (r.pos - start)\n\tif pp.sz > 0 {\n\t\tstart = r.pos\n\t\tr.pos += pp.sz\n\t\tpp.msg = r.buf[start:r.pos]\n\t} else {\n\t\tpp.msg = nil\n\t}\n\treturn nil\n}\n\nfunc mqttPubTrace(pp *mqttPublish) string {\n\tdup := pp.flags&mqttPubFlagDup != 0\n\tqos := mqttGetQoS(pp.flags)\n\tretain := mqttIsRetained(pp.flags)\n\tvar piStr string\n\tif pp.pi > 0 {\n\t\tpiStr = fmt.Sprintf(\" pi=%v\", pp.pi)\n\t}\n\treturn fmt.Sprintf(\"%s dup=%v QoS=%v retain=%v size=%v%s\",\n\t\tpp.topic, dup, qos, retain, pp.sz, piStr)\n}\n\n// Composes a NATS message from a MQTT PUBLISH packet. The message includes an\n// internal header containint the original packet's QoS, and for QoS2 packets\n// the original subject.\n//\n// Example (QoS2, subject: \"foo.bar\"):\n//\n//\tNATS/1.0\\r\\n\n//\tNmqtt-Pub:2foo.bar\\r\\n\n//\t\\r\\n\nfunc mqttNewDeliverableMessage(pp *mqttPublish, encodePP bool) (natsMsg []byte, headerLen int) {\n\tsize := len(hdrLine) +\n\t\tlen(mqttNatsHeader) + 2 + 2 + // 2 for ':<qos>', and 2 for CRLF\n\t\t2 + // end-of-header CRLF\n\t\tpp.sz\n\tif encodePP {\n\t\tsize += len(mqttNatsHeaderSubject) + 1 + // +1 for ':'\n\t\t\tlen(pp.subject) + 2 // 2 for CRLF\n\n\t\tif len(pp.mapped) > 0 {\n\t\t\tsize += len(mqttNatsHeaderMapped) + 1 + // +1 for ':'\n\t\t\t\tlen(pp.mapped) + 2 // 2 for CRLF\n\t\t}\n\t}\n\tbuf := bytes.NewBuffer(make([]byte, 0, size))\n\n\tqos := mqttGetQoS(pp.flags)\n\n\tbuf.WriteString(hdrLine)\n\tbuf.WriteString(mqttNatsHeader)\n\tbuf.WriteByte(':')\n\tbuf.WriteByte(qos + '0')\n\tbuf.WriteString(_CRLF_)\n\n\tif encodePP {\n\t\tbuf.WriteString(mqttNatsHeaderSubject)\n\t\tbuf.WriteByte(':')\n\t\tbuf.Write(pp.subject)\n\t\tbuf.WriteString(_CRLF_)\n\n\t\tif len(pp.mapped) > 0 {\n\t\t\tbuf.WriteString(mqttNatsHeaderMapped)\n\t\t\tbuf.WriteByte(':')\n\t\t\tbuf.Write(pp.mapped)\n\t\t\tbuf.WriteString(_CRLF_)\n\t\t}\n\t}\n\n\t// End of header\n\tbuf.WriteString(_CRLF_)\n\n\theaderLen = buf.Len()\n\n\tbuf.Write(pp.msg)\n\treturn buf.Bytes(), headerLen\n}\n\n// Composes a NATS message for a pending PUBREL packet. The message includes an\n// internal header containing the PI for PUBREL/PUBCOMP.\n//\n// Example (PI:123):\n//\n//\t\tNATS/1.0\\r\\n\n//\t \tNmqtt-PubRel:123\\r\\n\n//\t\t\\r\\n\nfunc mqttNewDeliverablePubRel(pi uint16) (natsMsg []byte, headerLen int) {\n\tsize := len(hdrLine) +\n\t\tlen(mqttNatsPubRelHeader) + 6 + 2 + // 6 for ':65535', and 2 for CRLF\n\t\t2 // end-of-header CRLF\n\tbuf := bytes.NewBuffer(make([]byte, 0, size))\n\tbuf.WriteString(hdrLine)\n\tbuf.WriteString(mqttNatsPubRelHeader)\n\tbuf.WriteByte(':')\n\tbuf.WriteString(strconv.FormatInt(int64(pi), 10))\n\tbuf.WriteString(_CRLF_)\n\tbuf.WriteString(_CRLF_)\n\treturn buf.Bytes(), buf.Len()\n}\n\n// Process the PUBLISH packet.\n//\n// Runs from the client's readLoop.\n// No lock held on entry.\nfunc (s *Server) mqttProcessPub(c *client, pp *mqttPublish, trace bool) error {\n\tqos := mqttGetQoS(pp.flags)\n\n\tswitch qos {\n\tcase 0:\n\t\treturn s.mqttInitiateMsgDelivery(c, pp)\n\n\tcase 1:\n\t\t// [MQTT-4.3.2-2]. Initiate onward delivery of the Application Message,\n\t\t// Send PUBACK.\n\t\t//\n\t\t// The receiver is not required to complete delivery of the Application\n\t\t// Message before sending the PUBACK. When its original sender receives\n\t\t// the PUBACK packet, ownership of the Application Message is\n\t\t// transferred to the receiver.\n\t\terr := s.mqttInitiateMsgDelivery(c, pp)\n\t\tif err == nil {\n\t\t\tc.mqttEnqueuePubResponse(mqttPacketPubAck, pp.pi, trace)\n\t\t}\n\t\treturn err\n\n\tcase 2:\n\t\t// [MQTT-4.3.3-2]. Method A, Store message, send PUBREC.\n\t\t//\n\t\t// The receiver is not required to complete delivery of the Application\n\t\t// Message before sending the PUBREC or PUBCOMP. When its original\n\t\t// sender receives the PUBREC packet, ownership of the Application\n\t\t// Message is transferred to the receiver.\n\t\terr := s.mqttStoreQoS2MsgOnce(c, pp)\n\t\tif err == nil {\n\t\t\tc.mqttEnqueuePubResponse(mqttPacketPubRec, pp.pi, trace)\n\t\t}\n\t\treturn err\n\n\tdefault:\n\t\treturn fmt.Errorf(\"unreachable: invalid QoS in mqttProcessPub: %v\", qos)\n\t}\n}\n\nfunc (s *Server) mqttInitiateMsgDelivery(c *client, pp *mqttPublish) error {\n\tnatsMsg, headerLen := mqttNewDeliverableMessage(pp, false)\n\n\t// Set the client's pubarg for processing.\n\tc.pa.subject = pp.subject\n\tc.pa.mapped = pp.mapped\n\tc.pa.reply = nil\n\tc.pa.hdr = headerLen\n\tc.pa.hdb = []byte(strconv.FormatInt(int64(c.pa.hdr), 10))\n\tc.pa.size = len(natsMsg)\n\tc.pa.szb = []byte(strconv.FormatInt(int64(c.pa.size), 10))\n\tdefer func() {\n\t\tc.pa.subject = nil\n\t\tc.pa.mapped = nil\n\t\tc.pa.reply = nil\n\t\tc.pa.hdr = -1\n\t\tc.pa.hdb = nil\n\t\tc.pa.size = 0\n\t\tc.pa.szb = nil\n\t}()\n\n\t_, permIssue := c.processInboundClientMsg(natsMsg)\n\tif permIssue {\n\t\treturn nil\n\t}\n\n\t// If QoS 0 messages don't need to be stored, other (1 and 2) do. Store them\n\t// JetStream under \"$MQTT.msgs.<delivery-subject>\"\n\tif qos := mqttGetQoS(pp.flags); qos == 0 {\n\t\treturn nil\n\t}\n\n\t// We need to call flushClients now since this we may have called c.addToPCD\n\t// with destination clients (possibly a route). Without calling flushClients\n\t// the following call may then be stuck waiting for a reply that may never\n\t// come because the destination is not flushed (due to c.out.fsp > 0,\n\t// see addToPCD and writeLoop for details).\n\tc.flushClients(0)\n\n\t_, err := c.mqtt.sess.jsa.storeMsg(mqttStreamSubjectPrefix+string(c.pa.subject), headerLen, natsMsg)\n\n\treturn err\n}\n\nvar mqttMaxMsgErrPattern = fmt.Sprintf(\"%s (%v)\", ErrMaxMsgsPerSubject.Error(), JSStreamStoreFailedF)\n\nfunc (s *Server) mqttStoreQoS2MsgOnce(c *client, pp *mqttPublish) error {\n\t// `true` means encode the MQTT PUBLISH packet in the NATS message header.\n\tnatsMsg, headerLen := mqttNewDeliverableMessage(pp, true)\n\n\t// Do not broadcast the message until it has been deduplicated and released\n\t// by the sender. Instead store this QoS2 message as\n\t// \"$MQTT.qos2.<client-id>.<PI>\". If the message is a duplicate, we get back\n\t// a ErrMaxMsgsPerSubject, otherwise it does not change the flow, still need\n\t// to send a PUBREC back to the client. The original subject (translated\n\t// from MQTT topic) is included in the NATS header of the stored message to\n\t// use for latter delivery.\n\t_, err := c.mqtt.sess.jsa.storeMsg(c.mqttQoS2InternalSubject(pp.pi), headerLen, natsMsg)\n\n\t// TODO: would prefer a more robust and performant way of checking the\n\t// error, but it comes back wrapped as an API result.\n\tif err != nil &&\n\t\t(isErrorOtherThan(err, JSStreamStoreFailedF) || err.Error() != mqttMaxMsgErrPattern) {\n\t\treturn err\n\t}\n\n\treturn nil\n}\n\nfunc (c *client) mqttQoS2InternalSubject(pi uint16) string {\n\treturn mqttQoS2IncomingMsgsStreamSubjectPrefix + c.mqtt.cid + \".\" + strconv.FormatUint(uint64(pi), 10)\n}\n\n// Process a PUBREL packet (QoS2, acting as Receiver).\n//\n// Runs from the client's readLoop.\n// No lock held on entry.\nfunc (s *Server) mqttProcessPubRel(c *client, pi uint16, trace bool) error {\n\t// Once done with the processing, send a PUBCOMP back to the client.\n\tdefer c.mqttEnqueuePubResponse(mqttPacketPubComp, pi, trace)\n\n\t// See if there is a message pending for this pi. All failures are treated\n\t// as \"not found\".\n\tasm := c.mqtt.asm\n\tstored, _ := asm.jsa.loadLastMsgFor(mqttQoS2IncomingMsgsStreamName, c.mqttQoS2InternalSubject(pi))\n\n\tif stored == nil {\n\t\t// No message found, nothing to do.\n\t\treturn nil\n\t}\n\t// Best attempt to delete the message from the QoS2 stream.\n\tasm.jsa.deleteMsg(mqttQoS2IncomingMsgsStreamName, stored.Sequence, true)\n\n\t// only MQTT QoS2 messages should be here, and they must have a subject.\n\th := mqttParsePublishNATSHeader(stored.Header)\n\tif h == nil || h.qos != 2 || len(h.subject) == 0 {\n\t\treturn errors.New(\"invalid message in QoS2 PUBREL stream\")\n\t}\n\n\tpp := &mqttPublish{\n\t\ttopic:   natsSubjectToMQTTTopic(h.subject),\n\t\tsubject: h.subject,\n\t\tmapped:  h.mapped,\n\t\tmsg:     stored.Data,\n\t\tsz:      len(stored.Data),\n\t\tpi:      pi,\n\t\tflags:   h.qos << 1,\n\t}\n\n\treturn s.mqttInitiateMsgDelivery(c, pp)\n}\n\n// Invoked when processing an inbound client message. If the \"retain\" flag is\n// set, the message is stored so it can be later resent to (re)starting\n// subscriptions that match the subject.\n//\n// Invoked from the MQTT publisher's readLoop. No client lock is held on entry.\nfunc (c *client) mqttHandlePubRetain() {\n\tpp := c.mqtt.pp\n\tretainMQTT := mqttIsRetained(pp.flags)\n\tisBirth, _, isCertificate := sparkbParseBirthDeathTopic(pp.topic)\n\tretainSparkbBirth := isBirth && !isCertificate\n\n\t// [tck-id-topics-nbirth-mqtt] NBIRTH messages MUST be published with MQTT\n\t// QoS equal to 0 and retain equal to false.\n\t//\n\t// [tck-id-conformance-mqtt-aware-nbirth-mqtt-retain] A Sparkplug Aware MQTT\n\t// Server MUST make NBIRTH messages available on the topic:\n\t// $sparkplug/certificates/namespace/group_id/NBIRTH/edge_node_id with the\n\t// MQTT retain flag set to true.\n\tif retainMQTT == retainSparkbBirth {\n\t\t// (retainSparkbBirth && retainMQTT) : not valid, so ignore altogether.\n\t\t// (!retainSparkbBirth && !retainMQTT) : nothing to do.\n\t\treturn\n\t}\n\n\tasm := c.mqtt.asm\n\tkey := string(pp.subject)\n\n\t// Always clear the retain flag to deliver a normal published message.\n\tdefer func() {\n\t\tpp.flags &= ^mqttPubFlagRetain\n\t}()\n\n\t// Spec [MQTT-3.3.1-11]. Payload of size 0 removes the retained message, but\n\t// should still be delivered as a normal message.\n\tif pp.sz == 0 {\n\t\tif seqToRemove := asm.handleRetainedMsgDel(key, 0); seqToRemove > 0 {\n\t\t\tasm.deleteRetainedMsg(seqToRemove)\n\t\t\tasm.notifyRetainedMsgDeleted(key, seqToRemove)\n\t\t}\n\t\treturn\n\t}\n\n\trm := &mqttRetainedMsg{\n\t\tOrigin: asm.jsa.id,\n\t\tMsg:    pp.msg, // will copy these bytes later as we process rm.\n\t\tFlags:  pp.flags,\n\t\tSource: c.opts.Username,\n\t}\n\n\tif retainSparkbBirth {\n\t\t// [tck-id-conformance-mqtt-aware-store] A Sparkplug Aware MQTT Server\n\t\t// MUST store NBIRTH and DBIRTH messages as they pass through the MQTT\n\t\t// Server.\n\t\t//\n\t\t// [tck-id-conformance-mqtt-aware-nbirth-mqtt-topic]. A Sparkplug Aware\n\t\t// MQTT Server MUST make NBIRTH messages available on a topic of the\n\t\t// form: $sparkplug/certificates/namespace/group_id/NBIRTH/edge_node_id\n\t\t//\n\t\t// [tck-id-conformance-mqtt-aware-dbirth-mqtt-topic] A Sparkplug Aware\n\t\t// MQTT Server MUST make DBIRTH messages available on a topic of the\n\t\t// form:\n\t\t// $sparkplug/certificates/namespace/group_id/DBIRTH/edge_node_id/device_id\n\t\ttopic := append(sparkbCertificatesTopicPrefix, pp.topic...)\n\t\tsubject, _ := mqttTopicToNATSPubSubject(topic)\n\t\trm.Topic = string(topic)\n\t\trm.Subject = string(subject)\n\n\t\t// will use to save the retained message.\n\t\tkey = string(subject)\n\n\t\t// Store the retained message with the RETAIN flag set.\n\t\trm.Flags |= mqttPubFlagRetain\n\n\t\t// Copy the payload out of pp since we will be sending the message\n\t\t// asynchronously.\n\t\tmsg := make([]byte, pp.sz)\n\t\tcopy(msg, pp.msg[:pp.sz])\n\t\tasm.jsa.sendMsg(key, msg)\n\n\t} else { // isRetained\n\t\t// Spec [MQTT-3.3.1-5]. Store the retained message with its QoS.\n\t\t//\n\t\t// When coming from a publish protocol, `pp` is referencing a stack\n\t\t// variable that itself possibly references the read buffer.\n\t\trm.Topic = string(pp.topic)\n\t}\n\n\t// Set the key to the subject of the message for retained, or the composed\n\t// $sparkplug subject for sparkB.\n\trm.Subject = key\n\trmBytes, hdr := mqttEncodeRetainedMessage(rm) // will copy the payload bytes\n\tsmr, err := asm.jsa.storeMsg(mqttRetainedMsgsStreamSubject+key, hdr, rmBytes)\n\tif err == nil {\n\t\t// Update the new sequence.\n\t\trf := &mqttRetainedMsgRef{\n\t\t\tsseq: smr.Sequence,\n\t\t}\n\t\t// Add/update the map. `true` to copy the payload bytes if needs to\n\t\t// update rmsCache.\n\t\tasm.handleRetainedMsg(key, rf, rm, true)\n\t} else {\n\t\tc.mu.Lock()\n\t\tacc := c.acc\n\t\tc.mu.Unlock()\n\t\tc.Errorf(\"unable to store retained message for account %q, subject %q: %v\",\n\t\t\tacc.GetName(), key, err)\n\t}\n}\n\n// After a config reload, it is possible that the source of a publish retained\n// message is no longer allowed to publish on the given topic. If that is the\n// case, the retained message is removed from the map and will no longer be\n// sent to (re)starting subscriptions.\n//\n// Server lock MUST NOT be held on entry.\nfunc (s *Server) mqttCheckPubRetainedPerms() {\n\tsm := &s.mqtt.sessmgr\n\tsm.mu.RLock()\n\tdone := len(sm.sessions) == 0\n\tsm.mu.RUnlock()\n\n\tif done {\n\t\treturn\n\t}\n\n\ts.mu.Lock()\n\tusers := make(map[string]*User, len(s.users))\n\tfor un, u := range s.users {\n\t\tusers[un] = u\n\t}\n\ts.mu.Unlock()\n\n\t// First get a list of all of the sessions.\n\tsm.mu.RLock()\n\tasms := make([]*mqttAccountSessionManager, 0, len(sm.sessions))\n\tfor _, asm := range sm.sessions {\n\t\tasms = append(asms, asm)\n\t}\n\tsm.mu.RUnlock()\n\n\ttype retainedMsg struct {\n\t\tsubj string\n\t\trmsg *mqttRetainedMsgRef\n\t}\n\n\t// For each session we will obtain a list of retained messages.\n\tvar _rms [128]retainedMsg\n\trms := _rms[:0]\n\tfor _, asm := range asms {\n\t\t// Get all of the retained messages. Then we will sort them so\n\t\t// that they are in sequence order, which should help the file\n\t\t// store to not have to load out-of-order blocks so often.\n\t\tasm.mu.RLock()\n\t\trms = rms[:0] // reuse slice\n\t\tfor subj, rf := range asm.retmsgs {\n\t\t\trms = append(rms, retainedMsg{\n\t\t\t\tsubj: subj,\n\t\t\t\trmsg: rf,\n\t\t\t})\n\t\t}\n\t\tasm.mu.RUnlock()\n\t\tslices.SortFunc(rms, func(i, j retainedMsg) int { return cmp.Compare(i.rmsg.sseq, j.rmsg.sseq) })\n\n\t\tperms := map[string]*perm{}\n\t\tdeletes := map[string]uint64{}\n\t\tfor _, rf := range rms {\n\t\t\tjsm, err := asm.jsa.loadMsg(mqttRetainedMsgsStreamName, rf.rmsg.sseq)\n\t\t\tif err != nil || jsm == nil {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\trm, err := mqttDecodeRetainedMessage(jsm.Header, jsm.Data)\n\t\t\tif err != nil {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif rm.Source == _EMPTY_ {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\t// Lookup source from global users.\n\t\t\tu := users[rm.Source]\n\t\t\tif u != nil {\n\t\t\t\tp, ok := perms[rm.Source]\n\t\t\t\tif !ok {\n\t\t\t\t\tp = generatePubPerms(u.Permissions)\n\t\t\t\t\tperms[rm.Source] = p\n\t\t\t\t}\n\t\t\t\t// If there is permission and no longer allowed to publish in\n\t\t\t\t// the subject, remove the publish retained message from the map.\n\t\t\t\tif p != nil && !pubAllowed(p, rf.subj) {\n\t\t\t\t\tu = nil\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// Not present or permissions have changed such that the source can't\n\t\t\t// publish on that subject anymore: remove it from the map.\n\t\t\tif u == nil {\n\t\t\t\tasm.mu.Lock()\n\t\t\t\tdelete(asm.retmsgs, rf.subj)\n\t\t\t\tasm.sl.Remove(rf.rmsg.sub)\n\t\t\t\tasm.mu.Unlock()\n\t\t\t\tdeletes[rf.subj] = rf.rmsg.sseq\n\t\t\t}\n\t\t}\n\n\t\tfor subject, seq := range deletes {\n\t\t\tasm.deleteRetainedMsg(seq)\n\t\t\tasm.notifyRetainedMsgDeleted(subject, seq)\n\t\t}\n\t}\n}\n\n// Helper to generate only pub permissions from a Permissions object\nfunc generatePubPerms(perms *Permissions) *perm {\n\tvar p *perm\n\tif perms.Publish.Allow != nil {\n\t\tp = &perm{}\n\t\tp.allow = NewSublistWithCache()\n\t\tfor _, pubSubject := range perms.Publish.Allow {\n\t\t\tsub := &subscription{subject: []byte(pubSubject)}\n\t\t\tp.allow.Insert(sub)\n\t\t}\n\t}\n\tif len(perms.Publish.Deny) > 0 {\n\t\tif p == nil {\n\t\t\tp = &perm{}\n\t\t}\n\t\tp.deny = NewSublistWithCache()\n\t\tfor _, pubSubject := range perms.Publish.Deny {\n\t\t\tsub := &subscription{subject: []byte(pubSubject)}\n\t\t\tp.deny.Insert(sub)\n\t\t}\n\t}\n\treturn p\n}\n\n// Helper that checks if given `perms` allow to publish on the given `subject`\nfunc pubAllowed(perms *perm, subject string) bool {\n\tallowed := true\n\tif perms.allow != nil {\n\t\tnp, _ := perms.allow.NumInterest(subject)\n\t\tallowed = np != 0\n\t}\n\t// If we have a deny list and are currently allowed, check that as well.\n\tif allowed && perms.deny != nil {\n\t\tnp, _ := perms.deny.NumInterest(subject)\n\t\tallowed = np == 0\n\t}\n\treturn allowed\n}\n\nfunc (c *client) mqttEnqueuePubResponse(packetType byte, pi uint16, trace bool) {\n\tproto := [4]byte{packetType, 0x2, 0, 0}\n\tproto[2] = byte(pi >> 8)\n\tproto[3] = byte(pi)\n\n\t// Bits 3,2,1 and 0 of the fixed header in the PUBREL Control Packet are\n\t// reserved and MUST be set to 0,0,1 and 0 respectively. The Server MUST treat\n\t// any other value as malformed and close the Network Connection [MQTT-3.6.1-1].\n\tif packetType == mqttPacketPubRel {\n\t\tproto[0] |= 0x2\n\t}\n\n\tc.mu.Lock()\n\tc.enqueueProto(proto[:4])\n\tc.mu.Unlock()\n\n\tif trace {\n\t\tname := \"(???)\"\n\t\tswitch packetType {\n\t\tcase mqttPacketPubAck:\n\t\t\tname = \"PUBACK\"\n\t\tcase mqttPacketPubRec:\n\t\t\tname = \"PUBREC\"\n\t\tcase mqttPacketPubRel:\n\t\t\tname = \"PUBREL\"\n\t\tcase mqttPacketPubComp:\n\t\t\tname = \"PUBCOMP\"\n\t\t}\n\t\tc.traceOutOp(name, []byte(fmt.Sprintf(\"pi=%v\", pi)))\n\t}\n}\n\nfunc mqttParsePIPacket(r *mqttReader) (uint16, error) {\n\tpi, err := r.readUint16(\"packet identifier\")\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\tif pi == 0 {\n\t\treturn 0, errMQTTPacketIdentifierIsZero\n\t}\n\treturn pi, nil\n}\n\n// Process a PUBACK (QoS1) or a PUBREC (QoS2) packet, acting as Sender. Set\n// isPubRec to false to process as a PUBACK.\n//\n// Runs from the client's readLoop. No lock held on entry.\nfunc (c *client) mqttProcessPublishReceived(pi uint16, isPubRec bool) (err error) {\n\tsess := c.mqtt.sess\n\tif sess == nil {\n\t\treturn errMQTTInvalidSession\n\t}\n\n\tvar jsAckSubject string\n\tsess.mu.Lock()\n\t// Must be the same client, and the session must have been setup for QoS2.\n\tif sess.c != c {\n\t\tsess.mu.Unlock()\n\t\treturn errMQTTInvalidSession\n\t}\n\tif isPubRec {\n\t\t// The JS ACK subject for the PUBREL will be filled in at the delivery\n\t\t// attempt.\n\t\tsess.trackAsPubRel(pi, _EMPTY_)\n\t}\n\tjsAckSubject = sess.untrackPublish(pi)\n\tsess.mu.Unlock()\n\n\tif isPubRec {\n\t\tnatsMsg, headerLen := mqttNewDeliverablePubRel(pi)\n\t\t_, err = sess.jsa.storeMsg(sess.pubRelSubject, headerLen, natsMsg)\n\t\tif err != nil {\n\t\t\t// Failure to send out PUBREL will terminate the connection.\n\t\t\treturn err\n\t\t}\n\t}\n\n\t// Send the ack to JS to remove the pending message from the consumer.\n\tsess.jsa.sendAck(jsAckSubject)\n\treturn nil\n}\n\nfunc (c *client) mqttProcessPubAck(pi uint16) error {\n\treturn c.mqttProcessPublishReceived(pi, false)\n}\n\nfunc (c *client) mqttProcessPubRec(pi uint16) error {\n\treturn c.mqttProcessPublishReceived(pi, true)\n}\n\n// Runs from the client's readLoop. No lock held on entry.\nfunc (c *client) mqttProcessPubComp(pi uint16) {\n\tsess := c.mqtt.sess\n\tif sess == nil {\n\t\treturn\n\t}\n\n\tvar jsAckSubject string\n\tsess.mu.Lock()\n\tif sess.c != c {\n\t\tsess.mu.Unlock()\n\t\treturn\n\t}\n\tjsAckSubject = sess.untrackPubRel(pi)\n\tsess.mu.Unlock()\n\n\t// Send the ack to JS to remove the pending message from the consumer.\n\tsess.jsa.sendAck(jsAckSubject)\n}\n\n// Return the QoS from the given PUBLISH protocol's flags\nfunc mqttGetQoS(flags byte) byte {\n\treturn flags & mqttPubFlagQoS >> 1\n}\n\nfunc mqttIsRetained(flags byte) bool {\n\treturn flags&mqttPubFlagRetain != 0\n}\n\nfunc sparkbParseBirthDeathTopic(topic []byte) (isBirth, isDeath, isCertificate bool) {\n\tif bytes.HasPrefix(topic, sparkbCertificatesTopicPrefix) {\n\t\tisCertificate = true\n\t\ttopic = topic[len(sparkbCertificatesTopicPrefix):]\n\t}\n\tif !bytes.HasPrefix(topic, sparkbNamespaceTopicPrefix) {\n\t\treturn false, false, false\n\t}\n\ttopic = topic[len(sparkbNamespaceTopicPrefix):]\n\n\tparts := bytes.Split(topic, []byte{'/'})\n\tif len(parts) < 3 || len(parts) > 4 {\n\t\treturn false, false, false\n\t}\n\ttyp := bytesToString(parts[1])\n\tswitch typ {\n\tcase sparkbNBIRTH, sparkbDBIRTH:\n\t\tisBirth = true\n\tcase sparkbNDEATH, sparkbDDEATH:\n\t\tisDeath = true\n\tdefault:\n\t\treturn false, false, false\n\t}\n\treturn isBirth, isDeath, isCertificate\n}\n\n//////////////////////////////////////////////////////////////////////////////\n//\n// SUBSCRIBE related functions\n//\n//////////////////////////////////////////////////////////////////////////////\n\nfunc (c *client) mqttParseSubs(r *mqttReader, b byte, pl int) (uint16, []*mqttFilter, error) {\n\treturn c.mqttParseSubsOrUnsubs(r, b, pl, true)\n}\n\nfunc (c *client) mqttParseSubsOrUnsubs(r *mqttReader, b byte, pl int, sub bool) (uint16, []*mqttFilter, error) {\n\tvar expectedFlag byte\n\tvar action string\n\tif sub {\n\t\texpectedFlag = mqttSubscribeFlags\n\t} else {\n\t\texpectedFlag = mqttUnsubscribeFlags\n\t\taction = \"un\"\n\t}\n\t// Spec [MQTT-3.8.1-1], [MQTT-3.10.1-1]\n\tif rf := b & 0xf; rf != expectedFlag {\n\t\treturn 0, nil, fmt.Errorf(\"wrong %ssubscribe reserved flags: %x\", action, rf)\n\t}\n\tpi, err := r.readUint16(\"packet identifier\")\n\tif err != nil {\n\t\treturn 0, nil, fmt.Errorf(\"reading packet identifier: %v\", err)\n\t}\n\tend := r.pos + (pl - 2)\n\tvar filters []*mqttFilter\n\tfor r.pos < end {\n\t\t// Don't make a copy now because, this will happen during conversion\n\t\t// or when processing the sub.\n\t\ttopic, err := r.readBytes(\"topic filter\", false)\n\t\tif err != nil {\n\t\t\treturn 0, nil, err\n\t\t}\n\t\tif len(topic) == 0 {\n\t\t\treturn 0, nil, errMQTTTopicFilterCannotBeEmpty\n\t\t}\n\t\t// Spec [MQTT-3.8.3-1], [MQTT-3.10.3-1]\n\t\tif !utf8.Valid(topic) {\n\t\t\treturn 0, nil, fmt.Errorf(\"invalid utf8 for topic filter %q\", topic)\n\t\t}\n\t\tvar qos byte\n\t\t// We are going to report if we had an error during the conversion,\n\t\t// but we don't fail the parsing. When processing the sub, we will\n\t\t// have an error then, and the processing of subs code will send\n\t\t// the proper mqttSubAckFailure flag for this given subscription.\n\t\tfilter, err := mqttFilterToNATSSubject(topic)\n\t\tif err != nil {\n\t\t\tc.Errorf(\"invalid topic %q: %v\", topic, err)\n\t\t}\n\t\tif sub {\n\t\t\tqos, err = r.readByte(\"QoS\")\n\t\t\tif err != nil {\n\t\t\t\treturn 0, nil, err\n\t\t\t}\n\t\t\t// Spec [MQTT-3-8.3-4].\n\t\t\tif qos > 2 {\n\t\t\t\treturn 0, nil, fmt.Errorf(\"subscribe QoS value must be 0, 1 or 2, got %v\", qos)\n\t\t\t}\n\t\t}\n\t\tf := &mqttFilter{ttopic: topic, filter: string(filter), qos: qos}\n\t\tfilters = append(filters, f)\n\t}\n\t// Spec [MQTT-3.8.3-3], [MQTT-3.10.3-2]\n\tif len(filters) == 0 {\n\t\treturn 0, nil, fmt.Errorf(\"%ssubscribe protocol must contain at least 1 topic filter\", action)\n\t}\n\treturn pi, filters, nil\n}\n\nfunc mqttSubscribeTrace(pi uint16, filters []*mqttFilter) string {\n\tvar sep string\n\tsb := &strings.Builder{}\n\tsb.WriteString(\"[\")\n\tfor i, f := range filters {\n\t\tsb.WriteString(sep)\n\t\tsb.Write(f.ttopic)\n\t\tsb.WriteString(\" (\")\n\t\tsb.WriteString(f.filter)\n\t\tsb.WriteString(\") QoS=\")\n\t\tsb.WriteString(fmt.Sprintf(\"%v\", f.qos))\n\t\tif i == 0 {\n\t\t\tsep = \", \"\n\t\t}\n\t}\n\tsb.WriteString(fmt.Sprintf(\"] pi=%v\", pi))\n\treturn sb.String()\n}\n\n// For a MQTT QoS0 subscription, we create a single NATS subscription on the\n// actual subject, for instance \"foo.bar\".\n//\n// For a MQTT QoS1+ subscription, we create 2 subscriptions, one on \"foo.bar\"\n// (as for QoS0, but sub.mqtt.qos will be 1 or 2), and one on the subject\n// \"$MQTT.sub.<uid>\" which is the delivery subject of the JS durable consumer\n// with the filter subject \"$MQTT.msgs.foo.bar\".\n//\n// This callback delivers messages to the client as QoS0 messages, either\n// because: (a) they have been produced as MQTT QoS0 messages (and therefore\n// only this callback can receive them); (b) they are MQTT QoS1+ published\n// messages but this callback is for a subscription that is QoS0; or (c) the\n// published messages come from (other) NATS publishers on the subject.\n//\n// This callback must reject a message if it is known to be a QoS1+ published\n// message and this is the callback for a QoS1+ subscription because in that\n// case, it will be handled by the other callback. This avoid getting duplicate\n// deliveries.\nfunc mqttDeliverMsgCbQoS0(sub *subscription, pc *client, _ *Account, subject, reply string, rmsg []byte) {\n\tif pc.kind == JETSTREAM && len(reply) > 0 && strings.HasPrefix(reply, jsAckPre) {\n\t\treturn\n\t}\n\n\t// This is the client associated with the subscription.\n\tcc := sub.client\n\n\t// This is immutable\n\tsess := cc.mqtt.sess\n\n\t// Lock here, otherwise we may be called with sub.mqtt == nil. Ignore\n\t// wildcard subscriptions if this subject starts with '$', per Spec\n\t// [MQTT-4.7.2-1].\n\tsess.subsMu.RLock()\n\tsubQoS := sub.mqtt.qos\n\tignore := mqttMustIgnoreForReservedSub(sub, subject)\n\tsess.subsMu.RUnlock()\n\n\tif ignore {\n\t\treturn\n\t}\n\n\thdr, msg := pc.msgParts(rmsg)\n\tvar topic []byte\n\tif pc.isMqtt() {\n\t\t// This is an MQTT publisher directly connected to this server.\n\n\t\t// Check the subscription's QoS. If the message was published with a\n\t\t// QoS>0 and the sub has the QoS>0 then the message will be delivered by\n\t\t// mqttDeliverMsgCbQoS12.\n\t\tmsgQoS := mqttGetQoS(pc.mqtt.pp.flags)\n\t\tif subQoS > 0 && msgQoS > 0 {\n\t\t\treturn\n\t\t}\n\t\ttopic = pc.mqtt.pp.topic\n\t\t// Check for service imports where subject mapping is in play.\n\t\tif len(pc.pa.mapped) > 0 && len(pc.pa.psi) > 0 {\n\t\t\ttopic = natsSubjectStrToMQTTTopic(subject)\n\t\t}\n\n\t} else {\n\t\t// Non MQTT client, could be NATS publisher, or ROUTER, etc..\n\t\th := mqttParsePublishNATSHeader(hdr)\n\n\t\t// Check the subscription's QoS. If the message was published with a\n\t\t// QoS>0 (in the header) and the sub has the QoS>0 then the message will\n\t\t// be delivered by mqttDeliverMsgCbQoS12.\n\t\tif subQoS > 0 && h != nil && h.qos > 0 {\n\t\t\treturn\n\t\t}\n\n\t\t// If size is more than what a MQTT client can handle, we should probably reject,\n\t\t// for now just truncate.\n\t\tif len(msg) > mqttMaxPayloadSize {\n\t\t\tmsg = msg[:mqttMaxPayloadSize]\n\t\t}\n\t\ttopic = natsSubjectStrToMQTTTopic(subject)\n\t}\n\n\t// Message never has a packet identifier nor is marked as duplicate.\n\tpc.mqttEnqueuePublishMsgTo(cc, sub, 0, 0, false, topic, msg)\n}\n\n// This is the callback attached to a JS durable subscription for a MQTT QoS 1+\n// sub. Only JETSTREAM should be sending a message to this subject (the delivery\n// subject associated with the JS durable consumer), but in cluster mode, this\n// can be coming from a route, gw, etc... We make sure that if this is the case,\n// the message contains a NATS/MQTT header that indicates that this is a\n// published QoS1+ message.\nfunc mqttDeliverMsgCbQoS12(sub *subscription, pc *client, _ *Account, subject, reply string, rmsg []byte) {\n\t// Message on foo.bar is stored under $MQTT.msgs.foo.bar, so the subject has to be\n\t// at least as long as the stream subject prefix \"$MQTT.msgs.\", and after removing\n\t// the prefix, has to be at least 1 character long.\n\tif len(subject) < len(mqttStreamSubjectPrefix)+1 {\n\t\treturn\n\t}\n\n\thdr, msg := pc.msgParts(rmsg)\n\th := mqttParsePublishNATSHeader(hdr)\n\tif pc.kind != JETSTREAM && (h == nil || h.qos == 0) {\n\t\t// MQTT QoS 0 messages must be ignored, they will be delivered by the\n\t\t// other callback, the direct NATS subscription. All JETSTREAM messages\n\t\t// will have the header.\n\t\treturn\n\t}\n\n\t// This is the client associated with the subscription.\n\tcc := sub.client\n\n\t// This is immutable\n\tsess := cc.mqtt.sess\n\n\t// We lock to check some of the subscription's fields and if we need to keep\n\t// track of pending acks, etc. There is no need to acquire the subsMu RLock\n\t// since sess.Lock is overarching for modifying subscriptions.\n\tsess.mu.Lock()\n\tif sess.c != cc || sub.mqtt == nil {\n\t\tsess.mu.Unlock()\n\t\treturn\n\t}\n\n\t// In this callback we handle only QoS-published messages to QoS\n\t// subscriptions. Ignore if either is 0, will be delivered by the other\n\t// callback, mqttDeliverMsgCbQos1.\n\tvar qos byte\n\tif h != nil {\n\t\tqos = h.qos\n\t}\n\tif qos > sub.mqtt.qos {\n\t\tqos = sub.mqtt.qos\n\t}\n\tif qos == 0 {\n\t\tsess.mu.Unlock()\n\t\treturn\n\t}\n\n\t// Check for reserved subject violation. If so, we will send the ack to\n\t// remove the message, and do nothing else.\n\tstrippedSubj := subject[len(mqttStreamSubjectPrefix):]\n\tif mqttMustIgnoreForReservedSub(sub, strippedSubj) {\n\t\tsess.mu.Unlock()\n\t\tsess.jsa.sendAck(reply)\n\t\treturn\n\t}\n\n\tpi, dup := sess.trackPublish(sub.mqtt.jsDur, reply)\n\tsess.mu.Unlock()\n\n\tif pi == 0 {\n\t\t// We have reached max pending, don't send the message now.\n\t\t// JS will cause a redelivery and if by then the number of pending\n\t\t// messages has fallen below threshold, the message will be resent.\n\t\treturn\n\t}\n\n\toriginalTopic := natsSubjectStrToMQTTTopic(strippedSubj)\n\tpc.mqttEnqueuePublishMsgTo(cc, sub, pi, qos, dup, originalTopic, msg)\n}\n\nfunc mqttDeliverPubRelCb(sub *subscription, pc *client, _ *Account, subject, reply string, rmsg []byte) {\n\tif sub.client.mqtt == nil || sub.client.mqtt.sess == nil || reply == _EMPTY_ {\n\t\treturn\n\t}\n\n\thdr, _ := pc.msgParts(rmsg)\n\tpi := mqttParsePubRelNATSHeader(hdr)\n\tif pi == 0 {\n\t\treturn\n\t}\n\n\t// This is the client associated with the subscription.\n\tcc := sub.client\n\n\t// This is immutable\n\tsess := cc.mqtt.sess\n\n\tsess.mu.Lock()\n\tif sess.c != cc || sess.pubRelConsumer == nil {\n\t\tsess.mu.Unlock()\n\t\treturn\n\t}\n\tsess.trackAsPubRel(pi, reply)\n\ttrace := cc.trace\n\tsess.mu.Unlock()\n\n\tcc.mqttEnqueuePubResponse(mqttPacketPubRel, pi, trace)\n}\n\n// The MQTT Server MUST NOT match Topic Filters starting with a wildcard\n// character (# or +) with Topic Names beginning with a $ character, Spec\n// [MQTT-4.7.2-1]. We will return true if there is a violation.\n//\n// Session or subMu lock must be held on entry to protect access to sub.mqtt.\nfunc mqttMustIgnoreForReservedSub(sub *subscription, subject string) bool {\n\t// If the subject does not start with $ nothing to do here.\n\tif !sub.mqtt.reserved || len(subject) == 0 || subject[0] != mqttReservedPre {\n\t\treturn false\n\t}\n\treturn true\n}\n\n// Check if a sub is a reserved wildcard. E.g. '#', '*', or '*/\" prefix.\nfunc isMQTTReservedSubscription(subject string) bool {\n\tif len(subject) == 1 && (subject[0] == fwc || subject[0] == pwc) {\n\t\treturn true\n\t}\n\t// Match \"*.<>\"\n\tif len(subject) > 1 && (subject[0] == pwc && subject[1] == btsep) {\n\t\treturn true\n\t}\n\treturn false\n}\n\nfunc sparkbReplaceDeathTimestamp(msg []byte) []byte {\n\tconst VARINT = 0\n\tconst TIMESTAMP = 1\n\n\torig := msg\n\tbuf := bytes.NewBuffer(make([]byte, 0, len(msg)+16)) // 16 bytes should be enough if we need to add a timestamp\n\twriteDeathTimestamp := func() {\n\t\t// [tck-id-conformance-mqtt-aware-ndeath-timestamp] A Sparkplug Aware\n\t\t// MQTT Server MAY replace the timestamp of NDEATH messages. If it does,\n\t\t// it MUST set the timestamp to the UTC time at which it attempts to\n\t\t// deliver the NDEATH to subscribed clients\n\t\t//\n\t\t// sparkB spec: 6.4.1. Google Protocol Buffer Schema\n\t\t//      optional uint64 timestamp = 1; // Timestamp at message sending time\n\t\t//\n\t\t// SparkplugB timestamps are milliseconds since epoch, represented as\n\t\t// uint64 in go, transmitted as protobuf varint.\n\t\tts := uint64(time.Now().UnixMilli())\n\t\tbuf.Write(protoEncodeVarint(TIMESTAMP<<3 | VARINT))\n\t\tbuf.Write(protoEncodeVarint(ts))\n\t}\n\n\tfor len(msg) > 0 {\n\t\tfieldNumericID, fieldType, size, err := protoScanField(msg)\n\t\tif err != nil {\n\t\t\treturn orig\n\t\t}\n\t\tif fieldType != VARINT || fieldNumericID != TIMESTAMP {\n\t\t\t// Add the field as is\n\t\t\tbuf.Write(msg[:size])\n\t\t\tmsg = msg[size:]\n\t\t\tcontinue\n\t\t}\n\n\t\twriteDeathTimestamp()\n\n\t\t// Add the rest of the message as is, we are done\n\t\tbuf.Write(msg[size:])\n\t\treturn buf.Bytes()\n\t}\n\n\t// Add timestamp if we did not find one.\n\twriteDeathTimestamp()\n\n\treturn buf.Bytes()\n}\n\n// Common function to mqtt delivery callbacks to serialize and send the message\n// to the `cc` client.\nfunc (c *client) mqttEnqueuePublishMsgTo(cc *client, sub *subscription, pi uint16, qos byte, dup bool, topic, msg []byte) {\n\t// [tck-id-conformance-mqtt-aware-nbirth-mqtt-retain] A Sparkplug Aware\n\t// MQTT Server MUST make NBIRTH messages available on the topic:\n\t// $sparkplug/certificates/namespace/group_id/NBIRTH/edge_node_id with\n\t// the MQTT retain flag set to true\n\t//\n\t// [tck-id-conformance-mqtt-aware-dbirth-mqtt-retain] A Sparkplug Aware\n\t// MQTT Server MUST make DBIRTH messages available on the topic:\n\t// $sparkplug/certificates/namespace/group_id/DBIRTH/edge_node_id/device_id\n\t// with the MQTT retain flag set to true\n\t//\n\t// $sparkplug/certificates messages are sent as NATS messages, so we\n\t// need to add the retain flag when sending them to MQTT clients.\n\n\tretain := false\n\tisBirth, isDeath, isCertificate := sparkbParseBirthDeathTopic(topic)\n\tif isBirth && qos == 0 {\n\t\tretain = isCertificate\n\t} else if isDeath && !isCertificate {\n\t\tmsg = sparkbReplaceDeathTimestamp(msg)\n\t}\n\n\tflags, headerBytes := mqttMakePublishHeader(pi, qos, dup, retain, topic, len(msg))\n\n\tcc.mu.Lock()\n\tif sub.mqtt.prm != nil {\n\t\tfor _, data := range sub.mqtt.prm {\n\t\t\tcc.queueOutbound(data)\n\t\t}\n\t\tsub.mqtt.prm = nil\n\t}\n\tcc.queueOutbound(headerBytes)\n\tcc.queueOutbound(msg)\n\tc.addToPCD(cc)\n\ttrace := cc.trace\n\tcc.mu.Unlock()\n\n\tif trace {\n\t\tpp := mqttPublish{\n\t\t\ttopic: topic,\n\t\t\tflags: flags,\n\t\t\tpi:    pi,\n\t\t\tsz:    len(msg),\n\t\t}\n\t\tcc.traceOutOp(\"PUBLISH\", []byte(mqttPubTrace(&pp)))\n\t}\n}\n\n// Serializes to the given writer the message for the given subject.\nfunc (w *mqttWriter) WritePublishHeader(pi uint16, qos byte, dup, retained bool, topic []byte, msgLen int) byte {\n\t// Compute len (will have to add packet id if message is sent as QoS>=1)\n\tpkLen := 2 + len(topic) + msgLen\n\tvar flags byte\n\n\t// Set flags for dup/retained/qos1\n\tif dup {\n\t\tflags |= mqttPubFlagDup\n\t}\n\tif retained {\n\t\tflags |= mqttPubFlagRetain\n\t}\n\tif qos > 0 {\n\t\tpkLen += 2\n\t\tflags |= qos << 1\n\t}\n\n\tw.WriteByte(mqttPacketPub | flags)\n\tw.WriteVarInt(pkLen)\n\tw.WriteBytes(topic)\n\tif qos > 0 {\n\t\tw.WriteUint16(pi)\n\t}\n\n\treturn flags\n}\n\n// Serializes to the given writer the message for the given subject.\nfunc mqttMakePublishHeader(pi uint16, qos byte, dup, retained bool, topic []byte, msgLen int) (byte, []byte) {\n\theaderBuf := newMQTTWriter(mqttInitialPubHeader + len(topic))\n\tflags := headerBuf.WritePublishHeader(pi, qos, dup, retained, topic, msgLen)\n\treturn flags, headerBuf.Bytes()\n}\n\n// Process the SUBSCRIBE packet.\n//\n// Process the list of subscriptions and update the given filter\n// with the QoS that has been accepted (or failure).\n//\n// Spec [MQTT-3.8.4-3] says that if an exact same subscription is\n// found, it needs to be replaced with the new one (possibly updating\n// the qos) and that the flow of publications must not be interrupted,\n// which I read as the replacement cannot be a \"remove then add\" if there\n// is a chance that in between the 2 actions, published messages\n// would be \"lost\" because there would not be any matching subscription.\n//\n// Run from client's readLoop.\n// No lock held on entry.\nfunc (c *client) mqttProcessSubs(filters []*mqttFilter) ([]*subscription, error) {\n\t// Those things are immutable, but since processing subs is not\n\t// really in the fast path, let's get them under the client lock.\n\tc.mu.Lock()\n\tasm := c.mqtt.asm\n\tsess := c.mqtt.sess\n\ttrace := c.trace\n\tc.mu.Unlock()\n\n\tif err := asm.lockSession(sess, c); err != nil {\n\t\treturn nil, err\n\t}\n\tdefer asm.unlockSession(sess)\n\treturn asm.processSubs(sess, c, filters, true, trace)\n}\n\n// Cleanup that is performed in processSubs if there was an error.\n//\n// Runs from client's readLoop.\n// Lock not held on entry, but session is in the locked map.\nfunc (sess *mqttSession) cleanupFailedSub(c *client, sub *subscription, cc *ConsumerConfig, jssub *subscription) {\n\tif sub != nil {\n\t\tc.processUnsub(sub.sid)\n\t}\n\tif jssub != nil {\n\t\tc.processUnsub(jssub.sid)\n\t}\n\tif cc != nil {\n\t\tsess.deleteConsumer(cc)\n\t}\n}\n\n// Make sure we are set up to deliver PUBREL messages to this QoS2-subscribed\n// session.\nfunc (sess *mqttSession) ensurePubRelConsumerSubscription(c *client) error {\n\topts := c.srv.getOpts()\n\tackWait := opts.MQTT.AckWait\n\tif ackWait == 0 {\n\t\tackWait = mqttDefaultAckWait\n\t}\n\tmaxAckPending := int(opts.MQTT.MaxAckPending)\n\tif maxAckPending == 0 {\n\t\tmaxAckPending = mqttDefaultMaxAckPending\n\t}\n\n\tsess.mu.Lock()\n\tpubRelSubscribed := sess.pubRelSubscribed\n\tpubRelSubject := sess.pubRelSubject\n\tpubRelDeliverySubjectB := sess.pubRelDeliverySubjectB\n\tpubRelDeliverySubject := sess.pubRelDeliverySubject\n\tpubRelConsumer := sess.pubRelConsumer\n\ttmaxack := sess.tmaxack\n\tidHash := sess.idHash\n\tid := sess.id\n\tsess.mu.Unlock()\n\n\t// Subscribe before the consumer is created so we don't loose any messages.\n\tif !pubRelSubscribed {\n\t\t_, err := c.processSub(pubRelDeliverySubjectB, nil, pubRelDeliverySubjectB,\n\t\t\tmqttDeliverPubRelCb, false)\n\t\tif err != nil {\n\t\t\tc.Errorf(\"Unable to create subscription for JetStream consumer on %q: %v\", pubRelDeliverySubject, err)\n\t\t\treturn err\n\t\t}\n\t\tpubRelSubscribed = true\n\t}\n\n\t// Create the consumer if needed.\n\tif pubRelConsumer == nil {\n\t\t// Check that the limit of subs' maxAckPending are not going over the limit\n\t\tif after := tmaxack + maxAckPending; after > mqttMaxAckTotalLimit {\n\t\t\treturn fmt.Errorf(\"max_ack_pending for all consumers would be %v which exceeds the limit of %v\",\n\t\t\t\tafter, mqttMaxAckTotalLimit)\n\t\t}\n\n\t\tccr := &CreateConsumerRequest{\n\t\t\tStream: mqttOutStreamName,\n\t\t\tConfig: ConsumerConfig{\n\t\t\t\tDeliverSubject: pubRelDeliverySubject,\n\t\t\t\tDurable:        mqttPubRelConsumerDurablePrefix + idHash,\n\t\t\t\tAckPolicy:      AckExplicit,\n\t\t\t\tDeliverPolicy:  DeliverNew,\n\t\t\t\tFilterSubject:  pubRelSubject,\n\t\t\t\tAckWait:        ackWait,\n\t\t\t\tMaxAckPending:  maxAckPending,\n\t\t\t\tMemoryStorage:  opts.MQTT.ConsumerMemoryStorage,\n\t\t\t},\n\t\t}\n\t\tif opts.MQTT.ConsumerInactiveThreshold > 0 {\n\t\t\tccr.Config.InactiveThreshold = opts.MQTT.ConsumerInactiveThreshold\n\t\t}\n\t\tif _, err := sess.jsa.createDurableConsumer(ccr); err != nil {\n\t\t\tc.Errorf(\"Unable to add JetStream consumer for PUBREL for client %q: err=%v\", id, err)\n\t\t\treturn err\n\t\t}\n\t\tpubRelConsumer = &ccr.Config\n\t\ttmaxack += maxAckPending\n\t}\n\n\tsess.mu.Lock()\n\tsess.pubRelSubscribed = pubRelSubscribed\n\tsess.pubRelConsumer = pubRelConsumer\n\tsess.tmaxack = tmaxack\n\tsess.mu.Unlock()\n\n\treturn nil\n}\n\n// When invoked with a QoS of 0, looks for an existing JS durable consumer for\n// the given sid and if one is found, delete the JS durable consumer and unsub\n// the NATS subscription on the delivery subject.\n//\n// With a QoS > 0, creates or update the existing JS durable consumer along with\n// its NATS subscription on a delivery subject.\n//\n// Session lock is acquired and released as needed. Session is in the locked\n// map.\nfunc (sess *mqttSession) processJSConsumer(c *client, subject, sid string,\n\tqos byte, fromSubProto bool) (*ConsumerConfig, *subscription, error) {\n\n\tsess.mu.Lock()\n\tcc, exists := sess.cons[sid]\n\ttmaxack := sess.tmaxack\n\tidHash := sess.idHash\n\tsess.mu.Unlock()\n\n\t// Check if we are already a JS consumer for this SID.\n\tif exists {\n\t\t// If current QoS is 0, it means that we need to delete the existing\n\t\t// one (that was QoS > 0)\n\t\tif qos == 0 {\n\t\t\t// The JS durable consumer's delivery subject is on a NUID of\n\t\t\t// the form: mqttSubPrefix + <nuid>. It is also used as the sid\n\t\t\t// for the NATS subscription, so use that for the lookup.\n\t\t\tc.mu.Lock()\n\t\t\tsub := c.subs[cc.DeliverSubject]\n\t\t\tc.mu.Unlock()\n\n\t\t\tsess.mu.Lock()\n\t\t\tdelete(sess.cons, sid)\n\t\t\tsess.mu.Unlock()\n\n\t\t\tsess.deleteConsumer(cc)\n\t\t\tif sub != nil {\n\t\t\t\tc.processUnsub(sub.sid)\n\t\t\t}\n\t\t\treturn nil, nil, nil\n\t\t}\n\t\t// If this is called when processing SUBSCRIBE protocol, then if\n\t\t// the JS consumer already exists, we are done (it was created\n\t\t// during the processing of CONNECT).\n\t\tif fromSubProto {\n\t\t\treturn nil, nil, nil\n\t\t}\n\t}\n\t// Here it means we don't have a JS consumer and if we are QoS 0,\n\t// we have nothing to do.\n\tif qos == 0 {\n\t\treturn nil, nil, nil\n\t}\n\tvar err error\n\tvar inbox string\n\tif exists {\n\t\tinbox = cc.DeliverSubject\n\t} else {\n\t\tinbox = mqttSubPrefix + nuid.Next()\n\t\topts := c.srv.getOpts()\n\t\tackWait := opts.MQTT.AckWait\n\t\tif ackWait == 0 {\n\t\t\tackWait = mqttDefaultAckWait\n\t\t}\n\t\tmaxAckPending := int(opts.MQTT.MaxAckPending)\n\t\tif maxAckPending == 0 {\n\t\t\tmaxAckPending = mqttDefaultMaxAckPending\n\t\t}\n\n\t\t// Check that the limit of subs' maxAckPending are not going over the limit\n\t\tif after := tmaxack + maxAckPending; after > mqttMaxAckTotalLimit {\n\t\t\treturn nil, nil, fmt.Errorf(\"max_ack_pending for all consumers would be %v which exceeds the limit of %v\",\n\t\t\t\tafter, mqttMaxAckTotalLimit)\n\t\t}\n\n\t\tdurName := idHash + \"_\" + nuid.Next()\n\t\tccr := &CreateConsumerRequest{\n\t\t\tStream: mqttStreamName,\n\t\t\tConfig: ConsumerConfig{\n\t\t\t\tDeliverSubject: inbox,\n\t\t\t\tDurable:        durName,\n\t\t\t\tAckPolicy:      AckExplicit,\n\t\t\t\tDeliverPolicy:  DeliverNew,\n\t\t\t\tFilterSubject:  mqttStreamSubjectPrefix + subject,\n\t\t\t\tAckWait:        ackWait,\n\t\t\t\tMaxAckPending:  maxAckPending,\n\t\t\t\tMemoryStorage:  opts.MQTT.ConsumerMemoryStorage,\n\t\t\t},\n\t\t}\n\t\tif opts.MQTT.ConsumerInactiveThreshold > 0 {\n\t\t\tccr.Config.InactiveThreshold = opts.MQTT.ConsumerInactiveThreshold\n\t\t}\n\t\tif _, err := sess.jsa.createDurableConsumer(ccr); err != nil {\n\t\t\tc.Errorf(\"Unable to add JetStream consumer for subscription on %q: err=%v\", subject, err)\n\t\t\treturn nil, nil, err\n\t\t}\n\t\tcc = &ccr.Config\n\t\ttmaxack += maxAckPending\n\t}\n\n\t// This is an internal subscription on subject like \"$MQTT.sub.<nuid>\" that is setup\n\t// for the JS durable's deliver subject.\n\tsess.mu.Lock()\n\tsess.tmaxack = tmaxack\n\tsub, err := sess.processQOS12Sub(c, []byte(inbox), []byte(inbox),\n\t\tisMQTTReservedSubscription(subject), qos, cc.Durable, mqttDeliverMsgCbQoS12)\n\tsess.mu.Unlock()\n\n\tif err != nil {\n\t\tsess.deleteConsumer(cc)\n\t\tc.Errorf(\"Unable to create subscription for JetStream consumer on %q: %v\", subject, err)\n\t\treturn nil, nil, err\n\t}\n\treturn cc, sub, nil\n}\n\n// Queues the published retained messages for each subscription and signals\n// the writeLoop.\nfunc (c *client) mqttSendRetainedMsgsToNewSubs(subs []*subscription) {\n\tc.mu.Lock()\n\tfor _, sub := range subs {\n\t\tif sub.mqtt != nil && sub.mqtt.prm != nil {\n\t\t\tfor _, data := range sub.mqtt.prm {\n\t\t\t\tc.queueOutbound(data)\n\t\t\t}\n\t\t\tsub.mqtt.prm = nil\n\t\t}\n\t}\n\tc.flushSignal()\n\tc.mu.Unlock()\n}\n\nfunc (c *client) mqttEnqueueSubAck(pi uint16, filters []*mqttFilter) {\n\tw := newMQTTWriter(7 + len(filters))\n\tw.WriteByte(mqttPacketSubAck)\n\t// packet length is 2 (for packet identifier) and 1 byte per filter.\n\tw.WriteVarInt(2 + len(filters))\n\tw.WriteUint16(pi)\n\tfor _, f := range filters {\n\t\tw.WriteByte(f.qos)\n\t}\n\tc.mu.Lock()\n\tc.enqueueProto(w.Bytes())\n\tc.mu.Unlock()\n}\n\n//////////////////////////////////////////////////////////////////////////////\n//\n// UNSUBSCRIBE related functions\n//\n//////////////////////////////////////////////////////////////////////////////\n\nfunc (c *client) mqttParseUnsubs(r *mqttReader, b byte, pl int) (uint16, []*mqttFilter, error) {\n\treturn c.mqttParseSubsOrUnsubs(r, b, pl, false)\n}\n\n// Process the UNSUBSCRIBE packet.\n//\n// Given the list of topics, this is going to unsubscribe the low level NATS subscriptions\n// and delete the JS durable consumers when applicable.\n//\n// Runs from the client's readLoop.\n// No lock held on entry.\nfunc (c *client) mqttProcessUnsubs(filters []*mqttFilter) error {\n\t// Those things are immutable, but since processing unsubs is not\n\t// really in the fast path, let's get them under the client lock.\n\tc.mu.Lock()\n\tasm := c.mqtt.asm\n\tsess := c.mqtt.sess\n\tc.mu.Unlock()\n\n\tif err := asm.lockSession(sess, c); err != nil {\n\t\treturn err\n\t}\n\tdefer asm.unlockSession(sess)\n\n\tremoveJSCons := func(sid string) {\n\t\tcc, ok := sess.cons[sid]\n\t\tif ok {\n\t\t\tdelete(sess.cons, sid)\n\t\t\tsess.deleteConsumer(cc)\n\t\t\t// Need lock here since these are accessed by callbacks\n\t\t\tsess.mu.Lock()\n\t\t\tif seqPis, ok := sess.cpending[cc.Durable]; ok {\n\t\t\t\tdelete(sess.cpending, cc.Durable)\n\t\t\t\tfor _, pi := range seqPis {\n\t\t\t\t\tdelete(sess.pendingPublish, pi)\n\t\t\t\t}\n\t\t\t\tif len(sess.pendingPublish) == 0 {\n\t\t\t\t\tsess.last_pi = 0\n\t\t\t\t}\n\t\t\t}\n\t\t\tsess.mu.Unlock()\n\t\t}\n\t}\n\tfor _, f := range filters {\n\t\tsid := f.filter\n\t\t// Remove JS Consumer if one exists for this sid\n\t\tremoveJSCons(sid)\n\t\tif err := c.processUnsub([]byte(sid)); err != nil {\n\t\t\tc.Errorf(\"error unsubscribing from %q: %v\", sid, err)\n\t\t}\n\t\tif mqttNeedSubForLevelUp(sid) {\n\t\t\tsubject := sid[:len(sid)-2]\n\t\t\tsid = subject + mqttMultiLevelSidSuffix\n\t\t\tremoveJSCons(sid)\n\t\t\tif err := c.processUnsub([]byte(sid)); err != nil {\n\t\t\t\tc.Errorf(\"error unsubscribing from %q: %v\", subject, err)\n\t\t\t}\n\t\t}\n\t}\n\treturn sess.update(filters, false)\n}\n\nfunc (c *client) mqttEnqueueUnsubAck(pi uint16) {\n\tw := newMQTTWriter(4)\n\tw.WriteByte(mqttPacketUnsubAck)\n\tw.WriteVarInt(2)\n\tw.WriteUint16(pi)\n\tc.mu.Lock()\n\tc.enqueueProto(w.Bytes())\n\tc.mu.Unlock()\n}\n\nfunc mqttUnsubscribeTrace(pi uint16, filters []*mqttFilter) string {\n\tvar sep string\n\tsb := strings.Builder{}\n\tsb.WriteString(\"[\")\n\tfor i, f := range filters {\n\t\tsb.WriteString(sep)\n\t\tsb.Write(f.ttopic)\n\t\tsb.WriteString(\" (\")\n\t\tsb.WriteString(f.filter)\n\t\tsb.WriteString(\")\")\n\t\tif i == 0 {\n\t\t\tsep = \", \"\n\t\t}\n\t}\n\tsb.WriteString(fmt.Sprintf(\"] pi=%v\", pi))\n\treturn sb.String()\n}\n\n//////////////////////////////////////////////////////////////////////////////\n//\n// PINGREQ/PINGRESP related functions\n//\n//////////////////////////////////////////////////////////////////////////////\n\nfunc (c *client) mqttEnqueuePingResp() {\n\tc.mu.Lock()\n\tc.enqueueProto(mqttPingResponse)\n\tc.mu.Unlock()\n}\n\n//////////////////////////////////////////////////////////////////////////////\n//\n// Trace functions\n//\n//////////////////////////////////////////////////////////////////////////////\n\nfunc errOrTrace(err error, trace string) []byte {\n\tif err != nil {\n\t\treturn []byte(err.Error())\n\t}\n\treturn []byte(trace)\n}\n\n//////////////////////////////////////////////////////////////////////////////\n//\n// Subject/Topic conversion functions\n//\n//////////////////////////////////////////////////////////////////////////////\n\n// Converts an MQTT Topic Name to a NATS Subject (used by PUBLISH)\n// See mqttToNATSSubjectConversion() for details.\nfunc mqttTopicToNATSPubSubject(mt []byte) ([]byte, error) {\n\treturn mqttToNATSSubjectConversion(mt, false)\n}\n\n// Converts an MQTT Topic Filter to a NATS Subject (used by SUBSCRIBE)\n// See mqttToNATSSubjectConversion() for details.\nfunc mqttFilterToNATSSubject(filter []byte) ([]byte, error) {\n\treturn mqttToNATSSubjectConversion(filter, true)\n}\n\n// Converts an MQTT Topic Name or Filter to a NATS Subject.\n// In MQTT:\n// - a Topic Name does not have wildcard (PUBLISH uses only topic names).\n// - a Topic Filter can include wildcards (SUBSCRIBE uses those).\n// - '+' and '#' are wildcard characters (single and multiple levels respectively)\n// - '/' is the topic level separator.\n//\n// Conversion that occurs:\n//   - '/' is replaced with '/.' if it is the first character in mt\n//   - '/' is replaced with './' if the last or next character in mt is '/'\n//     For instance, foo//bar would become foo./.bar\n//   - '/' is replaced with '.' for all other conditions (foo/bar -> foo.bar)\n//   - '.' is replaced with '//'.\n//   - ' ' cause an error to be returned.\n//\n// If there is no need to convert anything (say \"foo\" remains \"foo\"), then\n// the no memory is allocated and the returned slice is the original `mt`.\nfunc mqttToNATSSubjectConversion(mt []byte, wcOk bool) ([]byte, error) {\n\tvar cp bool\n\tvar j int\n\tres := mt\n\n\tmakeCopy := func(i int) {\n\t\tcp = true\n\t\tres = make([]byte, 0, len(mt)+10)\n\t\tif i > 0 {\n\t\t\tres = append(res, mt[:i]...)\n\t\t}\n\t}\n\n\tend := len(mt) - 1\n\tfor i := 0; i < len(mt); i++ {\n\t\tswitch mt[i] {\n\t\tcase mqttTopicLevelSep:\n\t\t\tif i == 0 || res[j-1] == btsep {\n\t\t\t\tif !cp {\n\t\t\t\t\tmakeCopy(0)\n\t\t\t\t}\n\t\t\t\tres = append(res, mqttTopicLevelSep, btsep)\n\t\t\t\tj++\n\t\t\t} else if i == end || mt[i+1] == mqttTopicLevelSep {\n\t\t\t\tif !cp {\n\t\t\t\t\tmakeCopy(i)\n\t\t\t\t}\n\t\t\t\tres = append(res, btsep, mqttTopicLevelSep)\n\t\t\t\tj++\n\t\t\t} else {\n\t\t\t\tif !cp {\n\t\t\t\t\tmakeCopy(i)\n\t\t\t\t}\n\t\t\t\tres = append(res, btsep)\n\t\t\t}\n\t\tcase ' ':\n\t\t\t// As of now, we cannot support ' ' in the MQTT topic/filter.\n\t\t\treturn nil, errMQTTUnsupportedCharacters\n\t\tcase btsep:\n\t\t\tif !cp {\n\t\t\t\tmakeCopy(i)\n\t\t\t}\n\t\t\tres = append(res, mqttTopicLevelSep, mqttTopicLevelSep)\n\t\t\tj++\n\t\tcase mqttSingleLevelWC, mqttMultiLevelWC:\n\t\t\tif !wcOk {\n\t\t\t\t// Spec [MQTT-3.3.2-2] and [MQTT-4.7.1-1]\n\t\t\t\t// The wildcard characters can be used in Topic Filters, but MUST NOT be used within a Topic Name\n\t\t\t\treturn nil, fmt.Errorf(\"wildcards not allowed in publish's topic: %q\", mt)\n\t\t\t}\n\t\t\tif !cp {\n\t\t\t\tmakeCopy(i)\n\t\t\t}\n\t\t\tif mt[i] == mqttSingleLevelWC {\n\t\t\t\tres = append(res, pwc)\n\t\t\t} else {\n\t\t\t\tres = append(res, fwc)\n\t\t\t}\n\t\tdefault:\n\t\t\tif cp {\n\t\t\t\tres = append(res, mt[i])\n\t\t\t}\n\t\t}\n\t\tj++\n\t}\n\tif cp && res[j-1] == btsep {\n\t\tres = append(res, mqttTopicLevelSep)\n\t\tj++\n\t}\n\treturn res[:j], nil\n}\n\n// Converts a NATS subject to MQTT topic. This is for publish\n// messages only, so there is no checking for wildcards.\n// Rules are reversed of mqttToNATSSubjectConversion.\nfunc natsSubjectStrToMQTTTopic(subject string) []byte {\n\treturn natsSubjectToMQTTTopic(stringToBytes(subject))\n}\n\nfunc natsSubjectToMQTTTopic(subject []byte) []byte {\n\ttopic := make([]byte, len(subject))\n\tend := len(subject) - 1\n\tvar j int\n\tfor i := 0; i < len(subject); i++ {\n\t\tswitch subject[i] {\n\t\tcase mqttTopicLevelSep:\n\t\t\tif i < end {\n\t\t\t\tswitch c := subject[i+1]; c {\n\t\t\t\tcase btsep, mqttTopicLevelSep:\n\t\t\t\t\tif c == btsep {\n\t\t\t\t\t\ttopic[j] = mqttTopicLevelSep\n\t\t\t\t\t} else {\n\t\t\t\t\t\ttopic[j] = btsep\n\t\t\t\t\t}\n\t\t\t\t\tj++\n\t\t\t\t\ti++\n\t\t\t\tdefault:\n\t\t\t\t}\n\t\t\t}\n\t\tcase btsep:\n\t\t\ttopic[j] = mqttTopicLevelSep\n\t\t\tj++\n\t\tdefault:\n\t\t\ttopic[j] = subject[i]\n\t\t\tj++\n\t\t}\n\t}\n\treturn topic[:j]\n}\n\n// Returns true if the subject has more than 1 token and ends with \".>\"\nfunc mqttNeedSubForLevelUp(subject string) bool {\n\tif len(subject) < 3 {\n\t\treturn false\n\t}\n\tend := len(subject)\n\tif subject[end-2] == '.' && subject[end-1] == fwc {\n\t\treturn true\n\t}\n\treturn false\n}\n\n//////////////////////////////////////////////////////////////////////////////\n//\n// MQTT Reader functions\n//\n//////////////////////////////////////////////////////////////////////////////\n\nfunc (r *mqttReader) reset(buf []byte) {\n\tif l := len(r.pbuf); l > 0 {\n\t\ttmp := make([]byte, l+len(buf))\n\t\tcopy(tmp, r.pbuf)\n\t\tcopy(tmp[l:], buf)\n\t\tbuf = tmp\n\t\tr.pbuf = nil\n\t}\n\tr.buf = buf\n\tr.pos = 0\n\tr.pstart = 0\n}\n\nfunc (r *mqttReader) hasMore() bool {\n\treturn r.pos != len(r.buf)\n}\n\nfunc (r *mqttReader) readByte(field string) (byte, error) {\n\tif r.pos == len(r.buf) {\n\t\treturn 0, fmt.Errorf(\"error reading %s: %v\", field, io.EOF)\n\t}\n\tb := r.buf[r.pos]\n\tr.pos++\n\treturn b, nil\n}\n\nfunc (r *mqttReader) readPacketLen() (int, bool, error) {\n\treturn r.readPacketLenWithCheck(true)\n}\n\nfunc (r *mqttReader) readPacketLenWithCheck(check bool) (int, bool, error) {\n\tm := 1\n\tv := 0\n\tfor {\n\t\tvar b byte\n\t\tif r.pos != len(r.buf) {\n\t\t\tb = r.buf[r.pos]\n\t\t\tr.pos++\n\t\t} else {\n\t\t\tbreak\n\t\t}\n\t\tv += int(b&0x7f) * m\n\t\tif (b & 0x80) == 0 {\n\t\t\tif check && r.pos+v > len(r.buf) {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\treturn v, true, nil\n\t\t}\n\t\tm *= 0x80\n\t\tif m > 0x200000 {\n\t\t\treturn 0, false, errMQTTMalformedVarInt\n\t\t}\n\t}\n\tr.pbuf = make([]byte, len(r.buf)-r.pstart)\n\tcopy(r.pbuf, r.buf[r.pstart:])\n\treturn 0, false, nil\n}\n\nfunc (r *mqttReader) readString(field string) (string, error) {\n\tvar s string\n\tbs, err := r.readBytes(field, false)\n\tif err == nil {\n\t\ts = string(bs)\n\t}\n\treturn s, err\n}\n\nfunc (r *mqttReader) readBytes(field string, cp bool) ([]byte, error) {\n\tluint, err := r.readUint16(field)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tl := int(luint)\n\tif l == 0 {\n\t\treturn nil, nil\n\t}\n\tstart := r.pos\n\tif start+l > len(r.buf) {\n\t\treturn nil, fmt.Errorf(\"error reading %s: %v\", field, io.ErrUnexpectedEOF)\n\t}\n\tr.pos += l\n\tb := r.buf[start:r.pos]\n\tif cp {\n\t\tb = copyBytes(b)\n\t}\n\treturn b, nil\n}\n\nfunc (r *mqttReader) readUint16(field string) (uint16, error) {\n\tif len(r.buf)-r.pos < 2 {\n\t\treturn 0, fmt.Errorf(\"error reading %s: %v\", field, io.ErrUnexpectedEOF)\n\t}\n\tstart := r.pos\n\tr.pos += 2\n\treturn binary.BigEndian.Uint16(r.buf[start:r.pos]), nil\n}\n\n//////////////////////////////////////////////////////////////////////////////\n//\n// MQTT Writer functions\n//\n//////////////////////////////////////////////////////////////////////////////\n\nfunc (w *mqttWriter) WriteUint16(i uint16) {\n\tw.WriteByte(byte(i >> 8))\n\tw.WriteByte(byte(i))\n}\n\nfunc (w *mqttWriter) WriteString(s string) {\n\tw.WriteBytes([]byte(s))\n}\n\nfunc (w *mqttWriter) WriteBytes(bs []byte) {\n\tw.WriteUint16(uint16(len(bs)))\n\tw.Write(bs)\n}\n\nfunc (w *mqttWriter) WriteVarInt(value int) {\n\tfor {\n\t\tb := byte(value & 0x7f)\n\t\tvalue >>= 7\n\t\tif value > 0 {\n\t\t\tb |= 0x80\n\t\t}\n\t\tw.WriteByte(b)\n\t\tif value == 0 {\n\t\t\tbreak\n\t\t}\n\t}\n}\n\nfunc newMQTTWriter(cap int) *mqttWriter {\n\tw := &mqttWriter{}\n\tw.Grow(cap)\n\treturn w\n}\n",
    "source_file": "server/mqtt.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2023-2025 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage server\n\nimport (\n\t\"crypto/tls\"\n\t\"crypto/x509\"\n\t\"errors\"\n\t\"fmt\"\n\t\"strings\"\n\t\"time\"\n\n\t\"golang.org/x/crypto/ocsp\"\n\n\t\"github.com/nats-io/nats-server/v2/server/certidp\"\n)\n\nfunc parseOCSPPeer(v any) (pcfg *certidp.OCSPPeerConfig, retError error) {\n\tvar lt token\n\tdefer convertPanicToError(&lt, &retError)\n\ttk, v := unwrapValue(v, &lt)\n\tcm, ok := v.(map[string]any)\n\tif !ok {\n\t\treturn nil, &configErr{tk, fmt.Sprintf(certidp.ErrIllegalPeerOptsConfig, v)}\n\t}\n\tpcfg = certidp.NewOCSPPeerConfig()\n\tretError = nil\n\tfor mk, mv := range cm {\n\t\ttk, mv = unwrapValue(mv, &lt)\n\t\tswitch strings.ToLower(mk) {\n\t\tcase \"verify\":\n\t\t\tverify, ok := mv.(bool)\n\t\t\tif !ok {\n\t\t\t\treturn nil, &configErr{tk, fmt.Sprintf(certidp.ErrParsingPeerOptFieldGeneric, mk)}\n\t\t\t}\n\t\t\tpcfg.Verify = verify\n\t\tcase \"allowed_clockskew\":\n\t\t\tat := float64(0)\n\t\t\tswitch mv := mv.(type) {\n\t\t\tcase int64:\n\t\t\t\tat = float64(mv)\n\t\t\tcase float64:\n\t\t\t\tat = mv\n\t\t\tcase string:\n\t\t\t\td, err := time.ParseDuration(mv)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn nil, &configErr{tk, fmt.Sprintf(certidp.ErrParsingPeerOptFieldTypeConversion, \"unexpected type\")}\n\t\t\t\t}\n\t\t\t\tat = d.Seconds()\n\t\t\tdefault:\n\t\t\t\treturn nil, &configErr{tk, fmt.Sprintf(certidp.ErrParsingPeerOptFieldTypeConversion, \"unexpected type\")}\n\t\t\t}\n\t\t\tif at >= 0 {\n\t\t\t\tpcfg.ClockSkew = at\n\t\t\t}\n\t\tcase \"ca_timeout\":\n\t\t\tat := float64(0)\n\t\t\tswitch mv := mv.(type) {\n\t\t\tcase int64:\n\t\t\t\tat = float64(mv)\n\t\t\tcase float64:\n\t\t\t\tat = mv\n\t\t\tcase string:\n\t\t\t\td, err := time.ParseDuration(mv)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn nil, &configErr{tk, fmt.Sprintf(certidp.ErrParsingPeerOptFieldTypeConversion, err)}\n\t\t\t\t}\n\t\t\t\tat = d.Seconds()\n\t\t\tdefault:\n\t\t\t\treturn nil, &configErr{tk, fmt.Sprintf(certidp.ErrParsingPeerOptFieldTypeConversion, \"unexpected type\")}\n\t\t\t}\n\t\t\tif at >= 0 {\n\t\t\t\tpcfg.Timeout = at\n\t\t\t}\n\t\tcase \"cache_ttl_when_next_update_unset\":\n\t\t\tat := float64(0)\n\t\t\tswitch mv := mv.(type) {\n\t\t\tcase int64:\n\t\t\t\tat = float64(mv)\n\t\t\tcase float64:\n\t\t\t\tat = mv\n\t\t\tcase string:\n\t\t\t\td, err := time.ParseDuration(mv)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn nil, &configErr{tk, fmt.Sprintf(certidp.ErrParsingPeerOptFieldTypeConversion, err)}\n\t\t\t\t}\n\t\t\t\tat = d.Seconds()\n\t\t\tdefault:\n\t\t\t\treturn nil, &configErr{tk, fmt.Sprintf(certidp.ErrParsingPeerOptFieldTypeConversion, \"unexpected type\")}\n\t\t\t}\n\t\t\tif at >= 0 {\n\t\t\t\tpcfg.TTLUnsetNextUpdate = at\n\t\t\t}\n\t\tcase \"warn_only\":\n\t\t\twarnOnly, ok := mv.(bool)\n\t\t\tif !ok {\n\t\t\t\treturn nil, &configErr{tk, fmt.Sprintf(certidp.ErrParsingPeerOptFieldGeneric, mk)}\n\t\t\t}\n\t\t\tpcfg.WarnOnly = warnOnly\n\t\tcase \"unknown_is_good\":\n\t\t\tunknownIsGood, ok := mv.(bool)\n\t\t\tif !ok {\n\t\t\t\treturn nil, &configErr{tk, fmt.Sprintf(certidp.ErrParsingPeerOptFieldGeneric, mk)}\n\t\t\t}\n\t\t\tpcfg.UnknownIsGood = unknownIsGood\n\t\tcase \"allow_when_ca_unreachable\":\n\t\t\tallowWhenCAUnreachable, ok := mv.(bool)\n\t\t\tif !ok {\n\t\t\t\treturn nil, &configErr{tk, fmt.Sprintf(certidp.ErrParsingPeerOptFieldGeneric, mk)}\n\t\t\t}\n\t\t\tpcfg.AllowWhenCAUnreachable = allowWhenCAUnreachable\n\t\tdefault:\n\t\t\treturn nil, &configErr{tk, fmt.Sprintf(certidp.ErrParsingPeerOptFieldGeneric, mk)}\n\t\t}\n\t}\n\treturn pcfg, nil\n}\n\nfunc peerFromVerifiedChains(chains [][]*x509.Certificate) *x509.Certificate {\n\tif len(chains) == 0 || len(chains[0]) == 0 {\n\t\treturn nil\n\t}\n\treturn chains[0][0]\n}\n\n// plugTLSOCSPPeer will plug the TLS handshake lifecycle for client mTLS connections and Leaf connections\nfunc (s *Server) plugTLSOCSPPeer(config *tlsConfigKind) (*tls.Config, bool, error) {\n\tif config == nil || config.tlsConfig == nil {\n\t\treturn nil, false, errors.New(certidp.ErrUnableToPlugTLSEmptyConfig)\n\t}\n\tkind := config.kind\n\tisSpoke := config.isLeafSpoke\n\ttcOpts := config.tlsOpts\n\tif tcOpts == nil || tcOpts.OCSPPeerConfig == nil || !tcOpts.OCSPPeerConfig.Verify {\n\t\treturn nil, false, nil\n\t}\n\ts.Debugf(certidp.DbgPlugTLSForKind, config.kind)\n\t// peer is a tls client\n\tif kind == kindStringMap[CLIENT] || (kind == kindStringMap[LEAF] && !isSpoke) {\n\t\tif !tcOpts.Verify {\n\t\t\treturn nil, false, errors.New(certidp.ErrMTLSRequired)\n\t\t}\n\t\treturn s.plugClientTLSOCSPPeer(config)\n\t}\n\t// peer is a tls server\n\tif kind == kindStringMap[LEAF] && isSpoke {\n\t\treturn s.plugServerTLSOCSPPeer(config)\n\t}\n\treturn nil, false, nil\n}\n\nfunc (s *Server) plugClientTLSOCSPPeer(config *tlsConfigKind) (*tls.Config, bool, error) {\n\tif config == nil || config.tlsConfig == nil || config.tlsOpts == nil {\n\t\treturn nil, false, errors.New(certidp.ErrUnableToPlugTLSClient)\n\t}\n\ttc := config.tlsConfig\n\ttcOpts := config.tlsOpts\n\tkind := config.kind\n\tif tcOpts.OCSPPeerConfig == nil || !tcOpts.OCSPPeerConfig.Verify {\n\t\treturn tc, false, nil\n\t}\n\ttc.VerifyConnection = func(cs tls.ConnectionState) error {\n\t\tif !s.tlsClientOCSPValid(cs.VerifiedChains, tcOpts.OCSPPeerConfig) {\n\t\t\ts.sendOCSPPeerRejectEvent(kind, peerFromVerifiedChains(cs.VerifiedChains), certidp.MsgTLSClientRejectConnection)\n\t\t\treturn errors.New(certidp.MsgTLSClientRejectConnection)\n\t\t}\n\t\treturn nil\n\t}\n\treturn tc, true, nil\n}\n\nfunc (s *Server) plugServerTLSOCSPPeer(config *tlsConfigKind) (*tls.Config, bool, error) {\n\tif config == nil || config.tlsConfig == nil || config.tlsOpts == nil {\n\t\treturn nil, false, errors.New(certidp.ErrUnableToPlugTLSServer)\n\t}\n\ttc := config.tlsConfig\n\ttcOpts := config.tlsOpts\n\tkind := config.kind\n\tif tcOpts.OCSPPeerConfig == nil || !tcOpts.OCSPPeerConfig.Verify {\n\t\treturn tc, false, nil\n\t}\n\ttc.VerifyConnection = func(cs tls.ConnectionState) error {\n\t\tif !s.tlsServerOCSPValid(cs.VerifiedChains, tcOpts.OCSPPeerConfig) {\n\t\t\ts.sendOCSPPeerRejectEvent(kind, peerFromVerifiedChains(cs.VerifiedChains), certidp.MsgTLSServerRejectConnection)\n\t\t\treturn errors.New(certidp.MsgTLSServerRejectConnection)\n\t\t}\n\t\treturn nil\n\t}\n\treturn tc, true, nil\n}\n\n// tlsServerOCSPValid evaluates verified chains (post successful TLS handshake) against OCSP\n// eligibility. A verified chain is considered OCSP Valid if either none of the links are\n// OCSP eligible, or current \"good\" responses from the CA can be obtained for each eligible link.\n// Upon first OCSP Valid chain found, the Server is deemed OCSP Valid. If none of the chains are\n// OCSP Valid, the Server is deemed OCSP Invalid. A verified self-signed certificate (chain length 1)\n// is also considered OCSP Valid.\nfunc (s *Server) tlsServerOCSPValid(chains [][]*x509.Certificate, opts *certidp.OCSPPeerConfig) bool {\n\ts.Debugf(certidp.DbgNumServerChains, len(chains))\n\treturn s.peerOCSPValid(chains, opts)\n}\n\n// tlsClientOCSPValid evaluates verified chains (post successful TLS handshake) against OCSP\n// eligibility. A verified chain is considered OCSP Valid if either none of the links are\n// OCSP eligible, or current \"good\" responses from the CA can be obtained for each eligible link.\n// Upon first OCSP Valid chain found, the Client is deemed OCSP Valid. If none of the chains are\n// OCSP Valid, the Client is deemed OCSP Invalid. A verified self-signed certificate (chain length 1)\n// is also considered OCSP Valid.\nfunc (s *Server) tlsClientOCSPValid(chains [][]*x509.Certificate, opts *certidp.OCSPPeerConfig) bool {\n\ts.Debugf(certidp.DbgNumClientChains, len(chains))\n\treturn s.peerOCSPValid(chains, opts)\n}\n\nfunc (s *Server) peerOCSPValid(chains [][]*x509.Certificate, opts *certidp.OCSPPeerConfig) bool {\n\tpeer := peerFromVerifiedChains(chains)\n\tif peer == nil {\n\t\ts.Errorf(certidp.ErrPeerEmptyAutoReject)\n\t\treturn false\n\t}\n\tfor ci, chain := range chains {\n\t\ts.Debugf(certidp.DbgLinksInChain, ci, len(chain))\n\t\t// Self-signed certificate is Client OCSP Valid (no CA)\n\t\tif len(chain) == 1 {\n\t\t\ts.Debugf(certidp.DbgSelfSignedValid, ci)\n\t\t\treturn true\n\t\t}\n\t\t// Check if any of the links in the chain are OCSP eligible\n\t\tchainEligible := false\n\t\tvar eligibleLinks []*certidp.ChainLink\n\t\t// Iterate over links skipping the root cert which is not OCSP eligible (self == issuer)\n\t\tfor linkPos := 0; linkPos < len(chain)-1; linkPos++ {\n\t\t\tcert := chain[linkPos]\n\t\t\tlink := &certidp.ChainLink{\n\t\t\t\tLeaf: cert,\n\t\t\t}\n\t\t\tif certidp.CertOCSPEligible(link) {\n\t\t\t\tchainEligible = true\n\t\t\t\tissuerCert := certidp.GetLeafIssuerCert(chain, linkPos)\n\t\t\t\tif issuerCert == nil {\n\t\t\t\t\t// unexpected chain condition, reject Client as OCSP Invalid\n\t\t\t\t\treturn false\n\t\t\t\t}\n\t\t\t\tlink.Issuer = issuerCert\n\t\t\t\teligibleLinks = append(eligibleLinks, link)\n\t\t\t}\n\t\t}\n\t\t// A trust-store verified chain that is not OCSP eligible is always OCSP Valid\n\t\tif !chainEligible {\n\t\t\ts.Debugf(certidp.DbgValidNonOCSPChain, ci)\n\t\t\treturn true\n\t\t}\n\t\ts.Debugf(certidp.DbgChainIsOCSPEligible, ci, len(eligibleLinks))\n\t\t// Chain has at least one OCSP eligible link, so check each eligible link;\n\t\t// any link with a !good OCSP response chain OCSP Invalid\n\t\tchainValid := true\n\t\tfor _, link := range eligibleLinks {\n\t\t\t// if option selected, good could reflect either ocsp.Good or ocsp.Unknown\n\t\t\tif badReason, good := s.certOCSPGood(link, opts); !good {\n\t\t\t\ts.Debugf(badReason)\n\t\t\t\ts.sendOCSPPeerChainlinkInvalidEvent(peer, link.Leaf, badReason)\n\t\t\t\tchainValid = false\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\tif chainValid {\n\t\t\ts.Debugf(certidp.DbgChainIsOCSPValid, ci)\n\t\t\treturn true\n\t\t}\n\t}\n\t// If we are here, all chains had OCSP eligible links, but none of the chains achieved OCSP valid\n\ts.Debugf(certidp.DbgNoOCSPValidChains)\n\treturn false\n}\n\nfunc (s *Server) certOCSPGood(link *certidp.ChainLink, opts *certidp.OCSPPeerConfig) (string, bool) {\n\tif link == nil || link.Leaf == nil || link.Issuer == nil || link.OCSPWebEndpoints == nil || len(*link.OCSPWebEndpoints) < 1 {\n\t\treturn \"Empty chainlink found\", false\n\t}\n\tvar err error\n\tsLogs := &certidp.Log{\n\t\tDebugf:  s.Debugf,\n\t\tNoticef: s.Noticef,\n\t\tWarnf:   s.Warnf,\n\t\tErrorf:  s.Errorf,\n\t\tTracef:  s.Tracef,\n\t}\n\tfingerprint := certidp.GenerateFingerprint(link.Leaf)\n\t// Used for debug/operator only, not match\n\tsubj := certidp.GetSubjectDNForm(link.Leaf)\n\tvar rawResp []byte\n\tvar ocspr *ocsp.Response\n\tvar useCachedResp bool\n\tvar rc = s.ocsprc\n\tvar cachedRevocation bool\n\t// Check our cache before calling out to the CA OCSP responder\n\ts.Debugf(certidp.DbgCheckingCacheForCert, subj, fingerprint)\n\tif rawResp = rc.Get(fingerprint, sLogs); len(rawResp) > 0 {\n\t\t// Signature validation of CA's OCSP response occurs in ParseResponse\n\t\tocspr, err = ocsp.ParseResponse(rawResp, link.Issuer)\n\t\tif err == nil && ocspr != nil {\n\t\t\t// Check if OCSP Response delegation present and if so is valid\n\t\t\tif !certidp.ValidDelegationCheck(link.Issuer, ocspr) {\n\t\t\t\t// Invalid delegation was already in cache, purge it and don't use it\n\t\t\t\ts.Debugf(certidp.MsgCachedOCSPResponseInvalid, subj)\n\t\t\t\trc.Delete(fingerprint, true, sLogs)\n\t\t\t\tgoto AFTERCACHE\n\t\t\t}\n\t\t\tif certidp.OCSPResponseCurrent(ocspr, opts, sLogs) {\n\t\t\t\ts.Debugf(certidp.DbgCurrentResponseCached, certidp.GetStatusAssertionStr(ocspr.Status))\n\t\t\t\tuseCachedResp = true\n\t\t\t} else {\n\t\t\t\t// Cached response is not current, delete it and tidy runtime stats to reflect a miss;\n\t\t\t\t// if preserve_revoked is enabled, the cache will not delete the cached response\n\t\t\t\ts.Debugf(certidp.DbgExpiredResponseCached, certidp.GetStatusAssertionStr(ocspr.Status))\n\t\t\t\trc.Delete(fingerprint, true, sLogs)\n\t\t\t}\n\t\t\t// Regardless of currency, record a cached revocation found in case AllowWhenCAUnreachable is set\n\t\t\tif ocspr.Status == ocsp.Revoked {\n\t\t\t\tcachedRevocation = true\n\t\t\t}\n\t\t} else {\n\t\t\t// Bogus cached assertion, purge it and don't use it\n\t\t\ts.Debugf(certidp.MsgCachedOCSPResponseInvalid, subj, fingerprint)\n\t\t\trc.Delete(fingerprint, true, sLogs)\n\t\t\tgoto AFTERCACHE\n\t\t}\n\t}\nAFTERCACHE:\n\tif !useCachedResp {\n\t\t// CA OCSP responder callout needed\n\t\trawResp, err = certidp.FetchOCSPResponse(link, opts, sLogs)\n\t\tif err != nil || rawResp == nil || len(rawResp) == 0 {\n\t\t\ts.Warnf(certidp.ErrCAResponderCalloutFail, subj, err)\n\t\t\tif opts.WarnOnly {\n\t\t\t\ts.Warnf(certidp.MsgAllowWarnOnlyOccurred, subj)\n\t\t\t\treturn _EMPTY_, true\n\t\t\t}\n\t\t\tif opts.AllowWhenCAUnreachable && !cachedRevocation {\n\t\t\t\t// Link has no cached history of revocation, so allow it to pass\n\t\t\t\ts.Warnf(certidp.MsgAllowWhenCAUnreachableOccurred, subj)\n\t\t\t\treturn _EMPTY_, true\n\t\t\t} else if opts.AllowWhenCAUnreachable {\n\t\t\t\t// Link has cached but expired revocation so reject when CA is unreachable\n\t\t\t\ts.Warnf(certidp.MsgAllowWhenCAUnreachableOccurredCachedRevoke, subj)\n\t\t\t}\n\t\t\treturn certidp.MsgFailedOCSPResponseFetch, false\n\t\t}\n\t\t// Signature validation of CA's OCSP response occurs in ParseResponse\n\t\tocspr, err = ocsp.ParseResponse(rawResp, link.Issuer)\n\t\tif err == nil && ocspr != nil {\n\t\t\t// Check if OCSP Response delegation present and if so is valid\n\t\t\tif !certidp.ValidDelegationCheck(link.Issuer, ocspr) {\n\t\t\t\ts.Warnf(certidp.MsgOCSPResponseDelegationInvalid, subj)\n\t\t\t\tif opts.WarnOnly {\n\t\t\t\t\t// Can't use bogus assertion, but warn-only set so allow link to pass\n\t\t\t\t\ts.Warnf(certidp.MsgAllowWarnOnlyOccurred, subj)\n\t\t\t\t\treturn _EMPTY_, true\n\t\t\t\t}\n\t\t\t\treturn fmt.Sprintf(certidp.MsgOCSPResponseDelegationInvalid, subj), false\n\t\t\t}\n\t\t\tif !certidp.OCSPResponseCurrent(ocspr, opts, sLogs) {\n\t\t\t\ts.Warnf(certidp.ErrNewCAResponseNotCurrent, subj)\n\t\t\t\tif opts.WarnOnly {\n\t\t\t\t\t// Can't use non-effective assertion, but warn-only set so allow link to pass\n\t\t\t\t\ts.Warnf(certidp.MsgAllowWarnOnlyOccurred, subj)\n\t\t\t\t\treturn _EMPTY_, true\n\t\t\t\t}\n\t\t\t\treturn certidp.MsgOCSPResponseNotEffective, false\n\t\t\t}\n\t\t} else {\n\t\t\ts.Errorf(certidp.ErrCAResponseParseFailed, subj, err)\n\t\t\tif opts.WarnOnly {\n\t\t\t\t// Can't use bogus assertion, but warn-only set so allow link to pass\n\t\t\t\ts.Warnf(certidp.MsgAllowWarnOnlyOccurred, subj)\n\t\t\t\treturn _EMPTY_, true\n\t\t\t}\n\t\t\treturn certidp.MsgFailedOCSPResponseParse, false\n\t\t}\n\t\t// cache the valid fetched CA OCSP Response\n\t\trc.Put(fingerprint, ocspr, subj, sLogs)\n\t}\n\n\t// Whether through valid cache response available or newly fetched valid response, now check the status\n\tif ocspr.Status == ocsp.Revoked || (ocspr.Status == ocsp.Unknown && !opts.UnknownIsGood) {\n\t\ts.Warnf(certidp.ErrOCSPInvalidPeerLink, subj, certidp.GetStatusAssertionStr(ocspr.Status))\n\t\tif opts.WarnOnly {\n\t\t\ts.Warnf(certidp.MsgAllowWarnOnlyOccurred, subj)\n\t\t\treturn _EMPTY_, true\n\t\t}\n\t\treturn fmt.Sprintf(certidp.MsgOCSPResponseInvalidStatus, certidp.GetStatusAssertionStr(ocspr.Status)), false\n\t}\n\ts.Debugf(certidp.DbgOCSPValidPeerLink, subj)\n\treturn _EMPTY_, true\n}\n",
    "source_file": "server/ocsp_peer.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2020-2025 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage server\n\nimport (\n\t\"strconv\"\n\t\"sync\"\n)\n\ntype outMsg struct {\n\tsubj string\n\trply string\n\thdr  []byte\n\tmsg  []byte\n}\n\ntype sendq struct {\n\tmu sync.Mutex\n\tq  *ipQueue[*outMsg]\n\ts  *Server\n\ta  *Account\n}\n\nfunc (s *Server) newSendQ(acc *Account) *sendq {\n\tsq := &sendq{s: s, q: newIPQueue[*outMsg](s, \"SendQ\"), a: acc}\n\ts.startGoRoutine(sq.internalLoop)\n\treturn sq\n}\n\nfunc (sq *sendq) internalLoop() {\n\tsq.mu.Lock()\n\ts, q := sq.s, sq.q\n\tsq.mu.Unlock()\n\n\tdefer s.grWG.Done()\n\n\tc := s.createInternalSystemClient()\n\tc.registerWithAccount(sq.a)\n\tc.noIcb = true\n\n\tdefer c.closeConnection(ClientClosed)\n\n\t// To optimize for not converting a string to a []byte slice.\n\tvar (\n\t\tsubj [256]byte\n\t\trply [256]byte\n\t\tszb  [10]byte\n\t\thdb  [10]byte\n\t\t_msg [4096]byte\n\t\tmsg  = _msg[:0]\n\t)\n\n\tfor s.isRunning() {\n\t\tselect {\n\t\tcase <-s.quitCh:\n\t\t\treturn\n\t\tcase <-q.ch:\n\t\t\tpms := q.pop()\n\t\t\tfor _, pm := range pms {\n\t\t\t\tc.pa.subject = append(subj[:0], pm.subj...)\n\t\t\t\tc.pa.size = len(pm.msg) + len(pm.hdr)\n\t\t\t\tc.pa.szb = append(szb[:0], strconv.Itoa(c.pa.size)...)\n\t\t\t\tif len(pm.rply) > 0 {\n\t\t\t\t\tc.pa.reply = append(rply[:0], pm.rply...)\n\t\t\t\t} else {\n\t\t\t\t\tc.pa.reply = nil\n\t\t\t\t}\n\t\t\t\tmsg = msg[:0]\n\t\t\t\tif len(pm.hdr) > 0 {\n\t\t\t\t\tc.pa.hdr = len(pm.hdr)\n\t\t\t\t\tc.pa.hdb = append(hdb[:0], strconv.Itoa(c.pa.hdr)...)\n\t\t\t\t\tmsg = append(msg, pm.hdr...)\n\t\t\t\t\tmsg = append(msg, pm.msg...)\n\t\t\t\t\tmsg = append(msg, _CRLF_...)\n\t\t\t\t} else {\n\t\t\t\t\tc.pa.hdr = -1\n\t\t\t\t\tc.pa.hdb = nil\n\t\t\t\t\tmsg = append(msg, pm.msg...)\n\t\t\t\t\tmsg = append(msg, _CRLF_...)\n\t\t\t\t}\n\t\t\t\tc.processInboundClientMsg(msg)\n\t\t\t\tc.pa.szb = nil\n\t\t\t\toutMsgPool.Put(pm)\n\t\t\t}\n\t\t\t// TODO: should this be in the for-loop instead?\n\t\t\tc.flushClients(0)\n\t\t\tq.recycle(&pms)\n\t\t}\n\t}\n}\n\nvar outMsgPool = sync.Pool{\n\tNew: func() any {\n\t\treturn &outMsg{}\n\t},\n}\n\nfunc (sq *sendq) send(subj, rply string, hdr, msg []byte) {\n\tif sq == nil {\n\t\treturn\n\t}\n\tout := outMsgPool.Get().(*outMsg)\n\tout.subj, out.rply = subj, rply\n\tout.hdr = append(out.hdr[:0], hdr...)\n\tout.msg = append(out.msg[:0], msg...)\n\tsq.q.push(out)\n}\n",
    "source_file": "server/sendq.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2021-2025 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n//go:build openbsd\n\npackage server\n\nimport (\n\t\"os\"\n\t\"syscall\"\n)\n\nfunc diskAvailable(storeDir string) int64 {\n\tvar ba int64\n\tif _, err := os.Stat(storeDir); os.IsNotExist(err) {\n\t\tos.MkdirAll(storeDir, defaultDirPerms)\n\t}\n\tvar fs syscall.Statfs_t\n\tif err := syscall.Statfs(storeDir, &fs); err == nil {\n\t\t// Estimate 75% of available storage.\n\t\tba = int64(uint64(fs.F_bavail) * uint64(fs.F_bsize) / 4 * 3)\n\t} else {\n\t\t// Used 1TB default as a guess if all else fails.\n\t\tba = JetStreamMaxStoreDefault\n\t}\n\treturn ba\n}\n",
    "source_file": "server/disk_avail_openbsd.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2012-2025 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage server\n\nimport (\n\t\"fmt\"\n\t\"os\"\n\t\"os/signal\"\n\t\"syscall\"\n\t\"time\"\n\n\t\"golang.org/x/sys/windows/svc\"\n\t\"golang.org/x/sys/windows/svc/mgr\"\n)\n\n// Signal Handling\nfunc (s *Server) handleSignals() {\n\tif s.getOpts().NoSigs {\n\t\treturn\n\t}\n\tc := make(chan os.Signal, 1)\n\n\tsignal.Notify(c, os.Interrupt, syscall.SIGTERM)\n\n\tgo func() {\n\t\tfor {\n\t\t\tselect {\n\t\t\tcase sig := <-c:\n\t\t\t\ts.Debugf(\"Trapped %q signal\", sig)\n\t\t\t\ts.Shutdown()\n\t\t\t\tos.Exit(0)\n\t\t\tcase <-s.quitCh:\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}()\n}\n\n// ProcessSignal sends the given signal command to the running nats-server service.\n// If service is empty, this signals the \"nats-server\" service. This returns an\n// error is the given service is not running or the command is invalid.\nfunc ProcessSignal(command Command, service string) error {\n\tif service == \"\" {\n\t\tservice = serviceName\n\t}\n\n\tm, err := mgr.Connect()\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer m.Disconnect()\n\n\ts, err := m.OpenService(service)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"could not access service: %v\", err)\n\t}\n\tdefer s.Close()\n\n\tvar (\n\t\tcmd svc.Cmd\n\t\tto  svc.State\n\t)\n\n\tswitch command {\n\tcase CommandStop, CommandQuit:\n\t\tcmd = svc.Stop\n\t\tto = svc.Stopped\n\tcase CommandReopen:\n\t\tcmd = reopenLogCmd\n\t\tto = svc.Running\n\tcase CommandReload:\n\t\tcmd = svc.ParamChange\n\t\tto = svc.Running\n\tcase commandLDMode:\n\t\tcmd = ldmCmd\n\t\tto = svc.Running\n\tdefault:\n\t\treturn fmt.Errorf(\"unknown signal %q\", command)\n\t}\n\n\tstatus, err := s.Control(cmd)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"could not send control=%d: %v\", cmd, err)\n\t}\n\n\ttimeout := time.Now().Add(10 * time.Second)\n\tfor status.State != to {\n\t\tif timeout.Before(time.Now()) {\n\t\t\treturn fmt.Errorf(\"timeout waiting for service to go to state=%d\", to)\n\t\t}\n\t\ttime.Sleep(300 * time.Millisecond)\n\t\tstatus, err = s.Query()\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"could not retrieve service status: %v\", err)\n\t\t}\n\t}\n\n\treturn nil\n}\n",
    "source_file": "server/signal_windows.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2022-2025 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n//go:build wasm\n\npackage server\n\nfunc (s *Server) handleSignals() {\n\n}\n\nfunc ProcessSignal(command Command, service string) error {\n\treturn nil\n}\n",
    "source_file": "server/signal_wasm.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2016-2025 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage server\n\nimport (\n\t\"bytes\"\n\t\"errors\"\n\t\"strings\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"unicode/utf8\"\n\n\t\"github.com/nats-io/nats-server/v2/server/stree\"\n)\n\n// Sublist is a routing mechanism to handle subject distribution and\n// provides a facility to match subjects from published messages to\n// interested subscribers. Subscribers can have wildcard subjects to\n// match multiple published subjects.\n\n// Common byte variables for wildcards and token separator.\nconst (\n\tpwc   = '*'\n\tpwcs  = \"*\"\n\tfwc   = '>'\n\tfwcs  = \">\"\n\ttsep  = \".\"\n\tbtsep = '.'\n)\n\n// Sublist related errors\nvar (\n\tErrInvalidSubject    = errors.New(\"sublist: invalid subject\")\n\tErrNotFound          = errors.New(\"sublist: no matches found\")\n\tErrNilChan           = errors.New(\"sublist: nil channel\")\n\tErrAlreadyRegistered = errors.New(\"sublist: notification already registered\")\n)\n\nconst (\n\t// cacheMax is used to bound limit the frontend cache\n\tslCacheMax = 1024\n\t// If we run a sweeper we will drain to this count.\n\tslCacheSweep = 256\n\t// plistMin is our lower bounds to create a fast plist for Match.\n\tplistMin = 256\n)\n\n// SublistResult is a result structure better optimized for queue subs.\ntype SublistResult struct {\n\tpsubs []*subscription\n\tqsubs [][]*subscription // don't make this a map, too expensive to iterate\n}\n\n// A Sublist stores and efficiently retrieves subscriptions.\ntype Sublist struct {\n\tsync.RWMutex\n\tgenid     uint64\n\tmatches   uint64\n\tcacheHits uint64\n\tinserts   uint64\n\tremoves   uint64\n\troot      *level\n\tcache     map[string]*SublistResult\n\tccSweep   int32\n\tnotify    *notifyMaps\n\tcount     uint32\n}\n\n// notifyMaps holds maps of arrays of channels for notifications\n// on a change of interest.\ntype notifyMaps struct {\n\tinsert map[string][]chan<- bool\n\tremove map[string][]chan<- bool\n}\n\n// A node contains subscriptions and a pointer to the next level.\ntype node struct {\n\tnext  *level\n\tpsubs map[*subscription]struct{}\n\tqsubs map[string]map[*subscription]struct{}\n\tplist []*subscription\n}\n\n// A level represents a group of nodes and special pointers to\n// wildcard nodes.\ntype level struct {\n\tnodes    map[string]*node\n\tpwc, fwc *node\n}\n\n// Create a new default node.\nfunc newNode() *node {\n\treturn &node{psubs: make(map[*subscription]struct{})}\n}\n\n// Create a new default level.\nfunc newLevel() *level {\n\treturn &level{nodes: make(map[string]*node)}\n}\n\n// In general caching is recommended however in some extreme cases where\n// interest changes are high, suppressing the cache can help.\n// https://github.com/nats-io/nats-server/issues/941\n// FIXME(dlc) - should be more dynamic at some point based on cache thrashing.\n\n// NewSublist will create a default sublist with caching enabled per the flag.\nfunc NewSublist(enableCache bool) *Sublist {\n\tif enableCache {\n\t\treturn &Sublist{root: newLevel(), cache: make(map[string]*SublistResult)}\n\t}\n\treturn &Sublist{root: newLevel()}\n}\n\n// NewSublistWithCache will create a default sublist with caching enabled.\nfunc NewSublistWithCache() *Sublist {\n\treturn NewSublist(true)\n}\n\n// NewSublistNoCache will create a default sublist with caching disabled.\nfunc NewSublistNoCache() *Sublist {\n\treturn NewSublist(false)\n}\n\n// CacheEnabled returns whether or not caching is enabled for this sublist.\nfunc (s *Sublist) CacheEnabled() bool {\n\ts.RLock()\n\tenabled := s.cache != nil\n\ts.RUnlock()\n\treturn enabled\n}\n\n// RegisterNotification will register for notifications when interest for the given\n// subject changes. The subject must be a literal publish type subject.\n// The notification is true for when the first interest for a subject is inserted,\n// and false when all interest in the subject is removed. Note that this interest\n// needs to be exact and that wildcards will not trigger the notifications. The sublist\n// will not block when trying to send the notification. Its up to the caller to make\n// sure the channel send will not block.\nfunc (s *Sublist) RegisterNotification(subject string, notify chan<- bool) error {\n\treturn s.registerNotification(subject, _EMPTY_, notify)\n}\n\nfunc (s *Sublist) RegisterQueueNotification(subject, queue string, notify chan<- bool) error {\n\treturn s.registerNotification(subject, queue, notify)\n}\n\nfunc (s *Sublist) registerNotification(subject, queue string, notify chan<- bool) error {\n\tif subjectHasWildcard(subject) {\n\t\treturn ErrInvalidSubject\n\t}\n\tif notify == nil {\n\t\treturn ErrNilChan\n\t}\n\n\tvar hasInterest bool\n\tr := s.Match(subject)\n\n\tif len(r.psubs)+len(r.qsubs) > 0 {\n\t\tif queue == _EMPTY_ {\n\t\t\tfor _, sub := range r.psubs {\n\t\t\t\tif string(sub.subject) == subject {\n\t\t\t\t\thasInterest = true\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t} else {\n\t\t\tfor _, qsub := range r.qsubs {\n\t\t\t\tqs := qsub[0]\n\t\t\t\tif string(qs.subject) == subject && string(qs.queue) == queue {\n\t\t\t\t\thasInterest = true\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tkey := keyFromSubjectAndQueue(subject, queue)\n\tvar err error\n\n\ts.Lock()\n\tif s.notify == nil {\n\t\ts.notify = &notifyMaps{\n\t\t\tinsert: make(map[string][]chan<- bool),\n\t\t\tremove: make(map[string][]chan<- bool),\n\t\t}\n\t}\n\t// Check which list to add us to.\n\tif hasInterest {\n\t\terr = s.addRemoveNotify(key, notify)\n\t} else {\n\t\terr = s.addInsertNotify(key, notify)\n\t}\n\ts.Unlock()\n\n\tif err == nil {\n\t\tsendNotification(notify, hasInterest)\n\t}\n\treturn err\n}\n\n// Lock should be held.\nfunc chkAndRemove(key string, notify chan<- bool, ms map[string][]chan<- bool) bool {\n\tchs := ms[key]\n\tfor i, ch := range chs {\n\t\tif ch == notify {\n\t\t\tchs[i] = chs[len(chs)-1]\n\t\t\tchs = chs[:len(chs)-1]\n\t\t\tif len(chs) == 0 {\n\t\t\t\tdelete(ms, key)\n\t\t\t}\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\nfunc (s *Sublist) ClearNotification(subject string, notify chan<- bool) bool {\n\treturn s.clearNotification(subject, _EMPTY_, notify)\n}\n\nfunc (s *Sublist) ClearQueueNotification(subject, queue string, notify chan<- bool) bool {\n\treturn s.clearNotification(subject, queue, notify)\n}\n\nfunc (s *Sublist) clearNotification(subject, queue string, notify chan<- bool) bool {\n\ts.Lock()\n\tif s.notify == nil {\n\t\ts.Unlock()\n\t\treturn false\n\t}\n\tkey := keyFromSubjectAndQueue(subject, queue)\n\t// Check both, start with remove.\n\tdidRemove := chkAndRemove(key, notify, s.notify.remove)\n\tdidRemove = didRemove || chkAndRemove(key, notify, s.notify.insert)\n\t// Check if everything is gone\n\tif len(s.notify.remove)+len(s.notify.insert) == 0 {\n\t\ts.notify = nil\n\t}\n\ts.Unlock()\n\treturn didRemove\n}\n\nfunc sendNotification(ch chan<- bool, hasInterest bool) {\n\tselect {\n\tcase ch <- hasInterest:\n\tdefault:\n\t}\n}\n\n// Add a new channel for notification in insert map.\n// Write lock should be held.\nfunc (s *Sublist) addInsertNotify(subject string, notify chan<- bool) error {\n\treturn s.addNotify(s.notify.insert, subject, notify)\n}\n\n// Add a new channel for notification in removal map.\n// Write lock should be held.\nfunc (s *Sublist) addRemoveNotify(subject string, notify chan<- bool) error {\n\treturn s.addNotify(s.notify.remove, subject, notify)\n}\n\n// Add a new channel for notification.\n// Write lock should be held.\nfunc (s *Sublist) addNotify(m map[string][]chan<- bool, subject string, notify chan<- bool) error {\n\tchs := m[subject]\n\tif len(chs) > 0 {\n\t\t// Check to see if this chan is already registered.\n\t\tfor _, ch := range chs {\n\t\t\tif ch == notify {\n\t\t\t\treturn ErrAlreadyRegistered\n\t\t\t}\n\t\t}\n\t}\n\n\tm[subject] = append(chs, notify)\n\treturn nil\n}\n\n// To generate a key from subject and queue. We just add spc.\nfunc keyFromSubjectAndQueue(subject, queue string) string {\n\tif len(queue) == 0 {\n\t\treturn subject\n\t}\n\tvar sb strings.Builder\n\tsb.WriteString(subject)\n\tsb.WriteString(\" \")\n\tsb.WriteString(queue)\n\treturn sb.String()\n}\n\n// chkForInsertNotification will check to see if we need to notify on this subject.\n// Write lock should be held.\nfunc (s *Sublist) chkForInsertNotification(subject, queue string) {\n\tkey := keyFromSubjectAndQueue(subject, queue)\n\n\t// All notify subjects are also literal so just do a hash lookup here.\n\tif chs := s.notify.insert[key]; len(chs) > 0 {\n\t\tfor _, ch := range chs {\n\t\t\tsendNotification(ch, true)\n\t\t}\n\t\t// Move from the insert map to the remove map.\n\t\ts.notify.remove[key] = append(s.notify.remove[key], chs...)\n\t\tdelete(s.notify.insert, key)\n\t}\n}\n\n// chkForRemoveNotification will check to see if we need to notify on this subject.\n// Write lock should be held.\nfunc (s *Sublist) chkForRemoveNotification(subject, queue string) {\n\tkey := keyFromSubjectAndQueue(subject, queue)\n\tif chs := s.notify.remove[key]; len(chs) > 0 {\n\t\t// We need to always check that we have no interest anymore.\n\t\tvar hasInterest bool\n\t\tr := s.matchNoLock(subject)\n\n\t\tif len(r.psubs)+len(r.qsubs) > 0 {\n\t\t\tif queue == _EMPTY_ {\n\t\t\t\tfor _, sub := range r.psubs {\n\t\t\t\t\tif string(sub.subject) == subject {\n\t\t\t\t\t\thasInterest = true\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tfor _, qsub := range r.qsubs {\n\t\t\t\t\tqs := qsub[0]\n\t\t\t\t\tif string(qs.subject) == subject && string(qs.queue) == queue {\n\t\t\t\t\t\thasInterest = true\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tif !hasInterest {\n\t\t\tfor _, ch := range chs {\n\t\t\t\tsendNotification(ch, false)\n\t\t\t}\n\t\t\t// Move from the remove map to the insert map.\n\t\t\ts.notify.insert[key] = append(s.notify.insert[key], chs...)\n\t\t\tdelete(s.notify.remove, key)\n\t\t}\n\t}\n}\n\n// Insert adds a subscription into the sublist\nfunc (s *Sublist) Insert(sub *subscription) error {\n\t// copy the subject since we hold this and this might be part of a large byte slice.\n\tsubject := string(sub.subject)\n\ttsa := [32]string{}\n\ttokens := tsa[:0]\n\tstart := 0\n\tfor i := 0; i < len(subject); i++ {\n\t\tif subject[i] == btsep {\n\t\t\ttokens = append(tokens, subject[start:i])\n\t\t\tstart = i + 1\n\t\t}\n\t}\n\ttokens = append(tokens, subject[start:])\n\n\ts.Lock()\n\n\tvar sfwc, haswc, isnew bool\n\tvar n *node\n\tl := s.root\n\n\tfor _, t := range tokens {\n\t\tlt := len(t)\n\t\tif lt == 0 || sfwc {\n\t\t\ts.Unlock()\n\t\t\treturn ErrInvalidSubject\n\t\t}\n\n\t\tif lt > 1 {\n\t\t\tn = l.nodes[t]\n\t\t} else {\n\t\t\tswitch t[0] {\n\t\t\tcase pwc:\n\t\t\t\tn = l.pwc\n\t\t\t\thaswc = true\n\t\t\tcase fwc:\n\t\t\t\tn = l.fwc\n\t\t\t\thaswc, sfwc = true, true\n\t\t\tdefault:\n\t\t\t\tn = l.nodes[t]\n\t\t\t}\n\t\t}\n\t\tif n == nil {\n\t\t\tn = newNode()\n\t\t\tif lt > 1 {\n\t\t\t\tl.nodes[t] = n\n\t\t\t} else {\n\t\t\t\tswitch t[0] {\n\t\t\t\tcase pwc:\n\t\t\t\t\tl.pwc = n\n\t\t\t\tcase fwc:\n\t\t\t\t\tl.fwc = n\n\t\t\t\tdefault:\n\t\t\t\t\tl.nodes[t] = n\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tif n.next == nil {\n\t\t\tn.next = newLevel()\n\t\t}\n\t\tl = n.next\n\t}\n\tif sub.queue == nil {\n\t\tn.psubs[sub] = struct{}{}\n\t\tisnew = len(n.psubs) == 1\n\t\tif n.plist != nil {\n\t\t\tn.plist = append(n.plist, sub)\n\t\t} else if len(n.psubs) > plistMin {\n\t\t\tn.plist = make([]*subscription, 0, len(n.psubs))\n\t\t\t// Populate\n\t\t\tfor psub := range n.psubs {\n\t\t\t\tn.plist = append(n.plist, psub)\n\t\t\t}\n\t\t}\n\t} else {\n\t\tif n.qsubs == nil {\n\t\t\tn.qsubs = make(map[string]map[*subscription]struct{})\n\t\t}\n\t\tqname := string(sub.queue)\n\t\t// This is a queue subscription\n\t\tsubs, ok := n.qsubs[qname]\n\t\tif !ok {\n\t\t\tsubs = make(map[*subscription]struct{})\n\t\t\tn.qsubs[qname] = subs\n\t\t\tisnew = true\n\t\t}\n\t\tsubs[sub] = struct{}{}\n\t}\n\n\ts.count++\n\ts.inserts++\n\n\ts.addToCache(subject, sub)\n\tatomic.AddUint64(&s.genid, 1)\n\n\tif s.notify != nil && isnew && !haswc && len(s.notify.insert) > 0 {\n\t\ts.chkForInsertNotification(subject, string(sub.queue))\n\t}\n\ts.Unlock()\n\n\treturn nil\n}\n\n// Deep copy\nfunc copyResult(r *SublistResult) *SublistResult {\n\tnr := &SublistResult{}\n\tnr.psubs = append([]*subscription(nil), r.psubs...)\n\tfor _, qr := range r.qsubs {\n\t\tnqr := append([]*subscription(nil), qr...)\n\t\tnr.qsubs = append(nr.qsubs, nqr)\n\t}\n\treturn nr\n}\n\n// Adds a new sub to an existing result.\nfunc (r *SublistResult) addSubToResult(sub *subscription) *SublistResult {\n\t// Copy since others may have a reference.\n\tnr := copyResult(r)\n\tif sub.queue == nil {\n\t\tnr.psubs = append(nr.psubs, sub)\n\t} else {\n\t\tif i := findQSlot(sub.queue, nr.qsubs); i >= 0 {\n\t\t\tnr.qsubs[i] = append(nr.qsubs[i], sub)\n\t\t} else {\n\t\t\tnr.qsubs = append(nr.qsubs, []*subscription{sub})\n\t\t}\n\t}\n\treturn nr\n}\n\n// addToCache will add the new entry to the existing cache\n// entries if needed. Assumes write lock is held.\n// Assumes write lock is held.\nfunc (s *Sublist) addToCache(subject string, sub *subscription) {\n\tif s.cache == nil {\n\t\treturn\n\t}\n\t// If literal we can direct match.\n\tif subjectIsLiteral(subject) {\n\t\tif r := s.cache[subject]; r != nil {\n\t\t\ts.cache[subject] = r.addSubToResult(sub)\n\t\t}\n\t\treturn\n\t}\n\tfor key, r := range s.cache {\n\t\tif matchLiteral(key, subject) {\n\t\t\ts.cache[key] = r.addSubToResult(sub)\n\t\t}\n\t}\n}\n\n// removeFromCache will remove the sub from any active cache entries.\n// Assumes write lock is held.\nfunc (s *Sublist) removeFromCache(subject string) {\n\tif s.cache == nil {\n\t\treturn\n\t}\n\t// If literal we can direct match.\n\tif subjectIsLiteral(subject) {\n\t\tdelete(s.cache, subject)\n\t\treturn\n\t}\n\t// Wildcard here.\n\tfor key := range s.cache {\n\t\tif matchLiteral(key, subject) {\n\t\t\tdelete(s.cache, key)\n\t\t}\n\t}\n}\n\n// a place holder for an empty result.\nvar emptyResult = &SublistResult{}\n\n// Match will match all entries to the literal subject.\n// It will return a set of results for both normal and queue subscribers.\nfunc (s *Sublist) Match(subject string) *SublistResult {\n\treturn s.match(subject, true, false)\n}\n\n// MatchBytes will match all entries to the literal subject.\n// It will return a set of results for both normal and queue subscribers.\nfunc (s *Sublist) MatchBytes(subject []byte) *SublistResult {\n\treturn s.match(bytesToString(subject), true, true)\n}\n\n// HasInterest will return whether or not there is any interest in the subject.\n// In cases where more detail is not required, this may be faster than Match.\nfunc (s *Sublist) HasInterest(subject string) bool {\n\treturn s.hasInterest(subject, true, nil, nil)\n}\n\n// NumInterest will return the number of subs/qsubs interested in the subject.\n// In cases where more detail is not required, this may be faster than Match.\nfunc (s *Sublist) NumInterest(subject string) (np, nq int) {\n\ts.hasInterest(subject, true, &np, &nq)\n\treturn\n}\n\nfunc (s *Sublist) matchNoLock(subject string) *SublistResult {\n\treturn s.match(subject, false, false)\n}\n\nfunc (s *Sublist) match(subject string, doLock bool, doCopyOnCache bool) *SublistResult {\n\tatomic.AddUint64(&s.matches, 1)\n\n\t// Check cache first.\n\tif doLock {\n\t\ts.RLock()\n\t}\n\tcacheEnabled := s.cache != nil\n\tr, ok := s.cache[subject]\n\tif doLock {\n\t\ts.RUnlock()\n\t}\n\tif ok {\n\t\tatomic.AddUint64(&s.cacheHits, 1)\n\t\treturn r\n\t}\n\n\ttsa := [32]string{}\n\ttokens := tsa[:0]\n\tstart := 0\n\tfor i := 0; i < len(subject); i++ {\n\t\tif subject[i] == btsep {\n\t\t\tif i-start == 0 {\n\t\t\t\treturn emptyResult\n\t\t\t}\n\t\t\ttokens = append(tokens, subject[start:i])\n\t\t\tstart = i + 1\n\t\t}\n\t}\n\tif start >= len(subject) {\n\t\treturn emptyResult\n\t}\n\ttokens = append(tokens, subject[start:])\n\n\t// FIXME(dlc) - Make shared pool between sublist and client readLoop?\n\tresult := &SublistResult{}\n\n\t// Get result from the main structure and place into the shared cache.\n\t// Hold the read lock to avoid race between match and store.\n\tvar n int\n\n\tif doLock {\n\t\tif cacheEnabled {\n\t\t\ts.Lock()\n\t\t} else {\n\t\t\ts.RLock()\n\t\t}\n\t}\n\n\tmatchLevel(s.root, tokens, result)\n\t// Check for empty result.\n\tif len(result.psubs) == 0 && len(result.qsubs) == 0 {\n\t\tresult = emptyResult\n\t}\n\tif cacheEnabled {\n\t\tif doCopyOnCache {\n\t\t\tsubject = copyString(subject)\n\t\t}\n\t\ts.cache[subject] = result\n\t\tn = len(s.cache)\n\t}\n\tif doLock {\n\t\tif cacheEnabled {\n\t\t\ts.Unlock()\n\t\t} else {\n\t\t\ts.RUnlock()\n\t\t}\n\t}\n\n\t// Reduce the cache count if we have exceeded our set maximum.\n\tif cacheEnabled && n > slCacheMax && atomic.CompareAndSwapInt32(&s.ccSweep, 0, 1) {\n\t\tgo s.reduceCacheCount()\n\t}\n\n\treturn result\n}\n\nfunc (s *Sublist) hasInterest(subject string, doLock bool, np, nq *int) bool {\n\t// Check cache first.\n\tif doLock {\n\t\ts.RLock()\n\t}\n\tvar matched bool\n\tif s.cache != nil {\n\t\tif r, ok := s.cache[subject]; ok {\n\t\t\tif np != nil && nq != nil {\n\t\t\t\t*np += len(r.psubs)\n\t\t\t\tfor _, qsub := range r.qsubs {\n\t\t\t\t\t*nq += len(qsub)\n\t\t\t\t}\n\t\t\t}\n\t\t\tmatched = len(r.psubs)+len(r.qsubs) > 0\n\t\t}\n\t}\n\tif doLock {\n\t\ts.RUnlock()\n\t}\n\tif matched {\n\t\tatomic.AddUint64(&s.cacheHits, 1)\n\t\treturn true\n\t}\n\n\ttsa := [32]string{}\n\ttokens := tsa[:0]\n\tstart := 0\n\tfor i := 0; i < len(subject); i++ {\n\t\tif subject[i] == btsep {\n\t\t\tif i-start == 0 {\n\t\t\t\treturn false\n\t\t\t}\n\t\t\ttokens = append(tokens, subject[start:i])\n\t\t\tstart = i + 1\n\t\t}\n\t}\n\tif start >= len(subject) {\n\t\treturn false\n\t}\n\ttokens = append(tokens, subject[start:])\n\n\tif doLock {\n\t\ts.RLock()\n\t\tdefer s.RUnlock()\n\t}\n\treturn matchLevelForAny(s.root, tokens, np, nq)\n}\n\n// Remove entries in the cache until we are under the maximum.\n// TODO(dlc) this could be smarter now that its not inline.\nfunc (s *Sublist) reduceCacheCount() {\n\tdefer atomic.StoreInt32(&s.ccSweep, 0)\n\t// If we are over the cache limit randomly drop until under the limit.\n\ts.Lock()\n\tfor key := range s.cache {\n\t\tdelete(s.cache, key)\n\t\tif len(s.cache) <= slCacheSweep {\n\t\t\tbreak\n\t\t}\n\t}\n\ts.Unlock()\n}\n\n// Helper function for auto-expanding remote qsubs.\nfunc isRemoteQSub(sub *subscription) bool {\n\treturn sub != nil && sub.queue != nil && sub.client != nil && (sub.client.kind == ROUTER || sub.client.kind == LEAF)\n}\n\n// UpdateRemoteQSub should be called when we update the weight of an existing\n// remote queue sub.\nfunc (s *Sublist) UpdateRemoteQSub(sub *subscription) {\n\t// We could search to make sure we find it, but probably not worth\n\t// it unless we are thrashing the cache. Just remove from our L2 and update\n\t// the genid so L1 will be flushed.\n\ts.Lock()\n\ts.removeFromCache(string(sub.subject))\n\tatomic.AddUint64(&s.genid, 1)\n\ts.Unlock()\n}\n\n// This will add in a node's results to the total results.\nfunc addNodeToResults(n *node, results *SublistResult) {\n\t// Normal subscriptions\n\tif n.plist != nil {\n\t\tresults.psubs = append(results.psubs, n.plist...)\n\t} else {\n\t\tfor psub := range n.psubs {\n\t\t\tresults.psubs = append(results.psubs, psub)\n\t\t}\n\t}\n\t// Queue subscriptions\n\tfor qname, qr := range n.qsubs {\n\t\tif len(qr) == 0 {\n\t\t\tcontinue\n\t\t}\n\t\t// Need to find matching list in results\n\t\tvar i int\n\t\tif i = findQSlot([]byte(qname), results.qsubs); i < 0 {\n\t\t\ti = len(results.qsubs)\n\t\t\tnqsub := make([]*subscription, 0, len(qr))\n\t\t\tresults.qsubs = append(results.qsubs, nqsub)\n\t\t}\n\t\tfor sub := range qr {\n\t\t\tif isRemoteQSub(sub) {\n\t\t\t\tns := atomic.LoadInt32(&sub.qw)\n\t\t\t\t// Shadow these subscriptions\n\t\t\t\tfor n := 0; n < int(ns); n++ {\n\t\t\t\t\tresults.qsubs[i] = append(results.qsubs[i], sub)\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tresults.qsubs[i] = append(results.qsubs[i], sub)\n\t\t\t}\n\t\t}\n\t}\n}\n\n// We do not use a map here since we want iteration to be past when\n// processing publishes in L1 on client. So we need to walk sequentially\n// for now. Keep an eye on this in case we start getting large number of\n// different queue subscribers for the same subject.\nfunc findQSlot(queue []byte, qsl [][]*subscription) int {\n\tif queue == nil {\n\t\treturn -1\n\t}\n\tfor i, qr := range qsl {\n\t\tif len(qr) > 0 && bytes.Equal(queue, qr[0].queue) {\n\t\t\treturn i\n\t\t}\n\t}\n\treturn -1\n}\n\n// matchLevel is used to recursively descend into the trie.\nfunc matchLevel(l *level, toks []string, results *SublistResult) {\n\tvar pwc, n *node\n\tfor i, t := range toks {\n\t\tif l == nil {\n\t\t\treturn\n\t\t}\n\t\tif l.fwc != nil {\n\t\t\taddNodeToResults(l.fwc, results)\n\t\t}\n\t\tif pwc = l.pwc; pwc != nil {\n\t\t\tmatchLevel(pwc.next, toks[i+1:], results)\n\t\t}\n\t\tn = l.nodes[t]\n\t\tif n != nil {\n\t\t\tl = n.next\n\t\t} else {\n\t\t\tl = nil\n\t\t}\n\t}\n\tif n != nil {\n\t\taddNodeToResults(n, results)\n\t}\n\tif pwc != nil {\n\t\taddNodeToResults(pwc, results)\n\t}\n}\n\nfunc matchLevelForAny(l *level, toks []string, np, nq *int) bool {\n\tvar pwc, n *node\n\tfor i, t := range toks {\n\t\tif l == nil {\n\t\t\treturn false\n\t\t}\n\t\tif l.fwc != nil {\n\t\t\tif np != nil && nq != nil {\n\t\t\t\t*np += len(l.fwc.psubs)\n\t\t\t\tfor _, qsub := range l.fwc.qsubs {\n\t\t\t\t\t*nq += len(qsub)\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn true\n\t\t}\n\t\tif pwc = l.pwc; pwc != nil {\n\t\t\tif match := matchLevelForAny(pwc.next, toks[i+1:], np, nq); match {\n\t\t\t\treturn true\n\t\t\t}\n\t\t}\n\t\tn = l.nodes[t]\n\t\tif n != nil {\n\t\t\tl = n.next\n\t\t} else {\n\t\t\tl = nil\n\t\t}\n\t}\n\tif n != nil {\n\t\tif np != nil && nq != nil {\n\t\t\t*np += len(n.psubs)\n\t\t\tfor _, qsub := range n.qsubs {\n\t\t\t\t*nq += len(qsub)\n\t\t\t}\n\t\t}\n\t\treturn len(n.plist) > 0 || len(n.psubs) > 0 || len(n.qsubs) > 0\n\t}\n\tif pwc != nil {\n\t\tif np != nil && nq != nil {\n\t\t\t*np += len(pwc.psubs)\n\t\t\tfor _, qsub := range pwc.qsubs {\n\t\t\t\t*nq += len(qsub)\n\t\t\t}\n\t\t}\n\t\treturn len(pwc.plist) > 0 || len(pwc.psubs) > 0 || len(pwc.qsubs) > 0\n\t}\n\treturn false\n}\n\n// lnt is used to track descent into levels for a removal for pruning.\ntype lnt struct {\n\tl *level\n\tn *node\n\tt string\n}\n\n// Raw low level remove, can do batches with lock held outside.\nfunc (s *Sublist) remove(sub *subscription, shouldLock bool, doCacheUpdates bool) error {\n\tsubject := string(sub.subject)\n\ttsa := [32]string{}\n\ttokens := tsa[:0]\n\tstart := 0\n\tfor i := 0; i < len(subject); i++ {\n\t\tif subject[i] == btsep {\n\t\t\ttokens = append(tokens, subject[start:i])\n\t\t\tstart = i + 1\n\t\t}\n\t}\n\ttokens = append(tokens, subject[start:])\n\n\tif shouldLock {\n\t\ts.Lock()\n\t\tdefer s.Unlock()\n\t}\n\n\tvar sfwc, haswc bool\n\tvar n *node\n\tl := s.root\n\n\t// Track levels for pruning\n\tvar lnts [32]lnt\n\tlevels := lnts[:0]\n\n\tfor _, t := range tokens {\n\t\tlt := len(t)\n\t\tif lt == 0 || sfwc {\n\t\t\treturn ErrInvalidSubject\n\t\t}\n\t\tif l == nil {\n\t\t\treturn ErrNotFound\n\t\t}\n\t\tif lt > 1 {\n\t\t\tn = l.nodes[t]\n\t\t} else {\n\t\t\tswitch t[0] {\n\t\t\tcase pwc:\n\t\t\t\tn = l.pwc\n\t\t\t\thaswc = true\n\t\t\tcase fwc:\n\t\t\t\tn = l.fwc\n\t\t\t\thaswc, sfwc = true, true\n\t\t\tdefault:\n\t\t\t\tn = l.nodes[t]\n\t\t\t}\n\t\t}\n\t\tif n != nil {\n\t\t\tlevels = append(levels, lnt{l, n, t})\n\t\t\tl = n.next\n\t\t} else {\n\t\t\tl = nil\n\t\t}\n\t}\n\tremoved, last := s.removeFromNode(n, sub)\n\tif !removed {\n\t\treturn ErrNotFound\n\t}\n\n\ts.count--\n\ts.removes++\n\n\tfor i := len(levels) - 1; i >= 0; i-- {\n\t\tl, n, t := levels[i].l, levels[i].n, levels[i].t\n\t\tif n.isEmpty() {\n\t\t\tl.pruneNode(n, t)\n\t\t}\n\t}\n\tif doCacheUpdates {\n\t\ts.removeFromCache(subject)\n\t\tatomic.AddUint64(&s.genid, 1)\n\t}\n\n\tif s.notify != nil && last && !haswc && len(s.notify.remove) > 0 {\n\t\ts.chkForRemoveNotification(subject, string(sub.queue))\n\t}\n\n\treturn nil\n}\n\n// Remove will remove a subscription.\nfunc (s *Sublist) Remove(sub *subscription) error {\n\treturn s.remove(sub, true, true)\n}\n\n// RemoveBatch will remove a list of subscriptions.\nfunc (s *Sublist) RemoveBatch(subs []*subscription) error {\n\tif len(subs) == 0 {\n\t\treturn nil\n\t}\n\n\ts.Lock()\n\tdefer s.Unlock()\n\n\t// TODO(dlc) - We could try to be smarter here for a client going away but the account\n\t// has a large number of subscriptions compared to this client. Quick and dirty testing\n\t// though said just disabling all the time best for now.\n\n\t// Turn off our cache if enabled.\n\twasEnabled := s.cache != nil\n\ts.cache = nil\n\t// We will try to remove all subscriptions but will report the first that caused\n\t// an error. In other words, we don't bail out at the first error which would\n\t// possibly leave a bunch of subscriptions that could have been removed.\n\tvar err error\n\tfor _, sub := range subs {\n\t\tif lerr := s.remove(sub, false, false); lerr != nil && err == nil {\n\t\t\terr = lerr\n\t\t}\n\t}\n\t// Turn caching back on here.\n\tatomic.AddUint64(&s.genid, 1)\n\tif wasEnabled {\n\t\ts.cache = make(map[string]*SublistResult)\n\t}\n\treturn err\n}\n\n// pruneNode is used to prune an empty node from the tree.\nfunc (l *level) pruneNode(n *node, t string) {\n\tif n == nil {\n\t\treturn\n\t}\n\tif n == l.fwc {\n\t\tl.fwc = nil\n\t} else if n == l.pwc {\n\t\tl.pwc = nil\n\t} else {\n\t\tdelete(l.nodes, t)\n\t}\n}\n\n// isEmpty will test if the node has any entries. Used\n// in pruning.\nfunc (n *node) isEmpty() bool {\n\tif len(n.psubs) == 0 && len(n.qsubs) == 0 {\n\t\tif n.next == nil || n.next.numNodes() == 0 {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\n// Return the number of nodes for the given level.\nfunc (l *level) numNodes() int {\n\tnum := len(l.nodes)\n\tif l.pwc != nil {\n\t\tnum++\n\t}\n\tif l.fwc != nil {\n\t\tnum++\n\t}\n\treturn num\n}\n\n// Remove the sub for the given node.\nfunc (s *Sublist) removeFromNode(n *node, sub *subscription) (found, last bool) {\n\tif n == nil {\n\t\treturn false, true\n\t}\n\tif sub.queue == nil {\n\t\t_, found = n.psubs[sub]\n\t\tdelete(n.psubs, sub)\n\t\tif found && n.plist != nil {\n\t\t\t// This will brute force remove the plist to perform\n\t\t\t// correct behavior. Will get re-populated on a call\n\t\t\t// to Match as needed.\n\t\t\tn.plist = nil\n\t\t}\n\t\treturn found, len(n.psubs) == 0\n\t}\n\n\t// We have a queue group subscription here\n\tqsub := n.qsubs[string(sub.queue)]\n\t_, found = qsub[sub]\n\tdelete(qsub, sub)\n\tif len(qsub) == 0 {\n\t\t// This is the last queue subscription interest when len(qsub) == 0, not\n\t\t// when n.qsubs is empty.\n\t\tlast = true\n\t\tdelete(n.qsubs, string(sub.queue))\n\t}\n\treturn found, last\n}\n\n// Count returns the number of subscriptions.\nfunc (s *Sublist) Count() uint32 {\n\ts.RLock()\n\tdefer s.RUnlock()\n\treturn s.count\n}\n\n// CacheCount returns the number of result sets in the cache.\nfunc (s *Sublist) CacheCount() int {\n\ts.RLock()\n\tcc := len(s.cache)\n\ts.RUnlock()\n\treturn cc\n}\n\n// SublistStats are public stats for the sublist\ntype SublistStats struct {\n\tNumSubs      uint32  `json:\"num_subscriptions\"`\n\tNumCache     uint32  `json:\"num_cache\"`\n\tNumInserts   uint64  `json:\"num_inserts\"`\n\tNumRemoves   uint64  `json:\"num_removes\"`\n\tNumMatches   uint64  `json:\"num_matches\"`\n\tCacheHitRate float64 `json:\"cache_hit_rate\"`\n\tMaxFanout    uint32  `json:\"max_fanout\"`\n\tAvgFanout    float64 `json:\"avg_fanout\"`\n\ttotFanout    int\n\tcacheCnt     int\n\tcacheHits    uint64\n}\n\nfunc (s *SublistStats) add(stat *SublistStats) {\n\ts.NumSubs += stat.NumSubs\n\ts.NumCache += stat.NumCache\n\ts.NumInserts += stat.NumInserts\n\ts.NumRemoves += stat.NumRemoves\n\ts.NumMatches += stat.NumMatches\n\ts.cacheHits += stat.cacheHits\n\tif s.MaxFanout < stat.MaxFanout {\n\t\ts.MaxFanout = stat.MaxFanout\n\t}\n\n\t// ignore slStats.AvgFanout, collect the values\n\t// it's based on instead\n\ts.totFanout += stat.totFanout\n\ts.cacheCnt += stat.cacheCnt\n\tif s.totFanout > 0 {\n\t\ts.AvgFanout = float64(s.totFanout) / float64(s.cacheCnt)\n\t}\n\tif s.NumMatches > 0 {\n\t\ts.CacheHitRate = float64(s.cacheHits) / float64(s.NumMatches)\n\t}\n}\n\n// Stats will return a stats structure for the current state.\nfunc (s *Sublist) Stats() *SublistStats {\n\tst := &SublistStats{}\n\n\ts.RLock()\n\tcache := s.cache\n\tcc := len(s.cache)\n\tst.NumSubs = s.count\n\tst.NumInserts = s.inserts\n\tst.NumRemoves = s.removes\n\ts.RUnlock()\n\n\tst.NumCache = uint32(cc)\n\tst.NumMatches = atomic.LoadUint64(&s.matches)\n\tst.cacheHits = atomic.LoadUint64(&s.cacheHits)\n\tif st.NumMatches > 0 {\n\t\tst.CacheHitRate = float64(st.cacheHits) / float64(st.NumMatches)\n\t}\n\n\t// whip through cache for fanout stats, this can be off if cache is full and doing evictions.\n\t// If this is called frequently, which it should not be, this could hurt performance.\n\tif cache != nil {\n\t\ttot, max, clen := 0, 0, 0\n\t\ts.RLock()\n\t\tfor _, r := range s.cache {\n\t\t\tclen++\n\t\t\tl := len(r.psubs) + len(r.qsubs)\n\t\t\ttot += l\n\t\t\tif l > max {\n\t\t\t\tmax = l\n\t\t\t}\n\t\t}\n\t\ts.RUnlock()\n\t\tst.totFanout = tot\n\t\tst.cacheCnt = clen\n\t\tst.MaxFanout = uint32(max)\n\t\tif tot > 0 {\n\t\t\tst.AvgFanout = float64(tot) / float64(clen)\n\t\t}\n\t}\n\treturn st\n}\n\n// numLevels will return the maximum number of levels\n// contained in the Sublist tree.\nfunc (s *Sublist) numLevels() int {\n\treturn visitLevel(s.root, 0)\n}\n\n// visitLevel is used to descend the Sublist tree structure\n// recursively.\nfunc visitLevel(l *level, depth int) int {\n\tif l == nil || l.numNodes() == 0 {\n\t\treturn depth\n\t}\n\n\tdepth++\n\tmaxDepth := depth\n\n\tfor _, n := range l.nodes {\n\t\tif n == nil {\n\t\t\tcontinue\n\t\t}\n\t\tnewDepth := visitLevel(n.next, depth)\n\t\tif newDepth > maxDepth {\n\t\t\tmaxDepth = newDepth\n\t\t}\n\t}\n\tif l.pwc != nil {\n\t\tpwcDepth := visitLevel(l.pwc.next, depth)\n\t\tif pwcDepth > maxDepth {\n\t\t\tmaxDepth = pwcDepth\n\t\t}\n\t}\n\tif l.fwc != nil {\n\t\tfwcDepth := visitLevel(l.fwc.next, depth)\n\t\tif fwcDepth > maxDepth {\n\t\t\tmaxDepth = fwcDepth\n\t\t}\n\t}\n\treturn maxDepth\n}\n\n// Determine if a subject has any wildcard tokens.\nfunc subjectHasWildcard(subject string) bool {\n\t// This one exits earlier then !subjectIsLiteral(subject)\n\tfor i, c := range subject {\n\t\tif c == pwc || c == fwc {\n\t\t\tif (i == 0 || subject[i-1] == btsep) &&\n\t\t\t\t(i+1 == len(subject) || subject[i+1] == btsep) {\n\t\t\t\treturn true\n\t\t\t}\n\t\t}\n\t}\n\treturn false\n}\n\n// Determine if the subject has any wildcards. Fast version, does not check for\n// valid subject. Used in caching layer.\nfunc subjectIsLiteral(subject string) bool {\n\tfor i, c := range subject {\n\t\tif c == pwc || c == fwc {\n\t\t\tif (i == 0 || subject[i-1] == btsep) &&\n\t\t\t\t(i+1 == len(subject) || subject[i+1] == btsep) {\n\t\t\t\treturn false\n\t\t\t}\n\t\t}\n\t}\n\treturn true\n}\n\n// IsValidPublishSubject returns true if a subject is valid and a literal, false otherwise\nfunc IsValidPublishSubject(subject string) bool {\n\treturn IsValidSubject(subject) && subjectIsLiteral(subject)\n}\n\n// IsValidSubject returns true if a subject is valid, false otherwise\nfunc IsValidSubject(subject string) bool {\n\treturn isValidSubject(subject, false)\n}\n\nfunc isValidSubject(subject string, checkRunes bool) bool {\n\tif subject == _EMPTY_ {\n\t\treturn false\n\t}\n\tif checkRunes {\n\t\t// Check if we have embedded nulls.\n\t\tif bytes.IndexByte(stringToBytes(subject), 0) >= 0 {\n\t\t\treturn false\n\t\t}\n\t\t// Since casting to a string will always produce valid UTF-8, we need to look for replacement runes.\n\t\t// This signals something is off or corrupt.\n\t\tfor _, r := range subject {\n\t\t\tif r == utf8.RuneError {\n\t\t\t\treturn false\n\t\t\t}\n\t\t}\n\t}\n\tsfwc := false\n\ttokens := strings.Split(subject, tsep)\n\tfor _, t := range tokens {\n\t\tlength := len(t)\n\t\tif length == 0 || sfwc {\n\t\t\treturn false\n\t\t}\n\t\tif length > 1 {\n\t\t\tif strings.ContainsAny(t, \"\\t\\n\\f\\r \") {\n\t\t\t\treturn false\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\t\tswitch t[0] {\n\t\tcase fwc:\n\t\t\tsfwc = true\n\t\tcase ' ', '\\t', '\\n', '\\r', '\\f':\n\t\t\treturn false\n\t\t}\n\t}\n\treturn true\n}\n\n// IsValidLiteralSubject returns true if a subject is valid and literal (no wildcards), false otherwise\nfunc IsValidLiteralSubject(subject string) bool {\n\treturn isValidLiteralSubject(strings.Split(subject, tsep))\n}\n\n// isValidLiteralSubject returns true if the tokens are valid and literal (no wildcards), false otherwise\nfunc isValidLiteralSubject(tokens []string) bool {\n\tfor _, t := range tokens {\n\t\tif len(t) == 0 {\n\t\t\treturn false\n\t\t}\n\t\tif len(t) > 1 {\n\t\t\tcontinue\n\t\t}\n\t\tswitch t[0] {\n\t\tcase pwc, fwc:\n\t\t\treturn false\n\t\t}\n\t}\n\treturn true\n}\n\n// ValidateMapping returns nil error if the subject is a valid subject mapping destination subject\nfunc ValidateMapping(src string, dest string) error {\n\tif dest == _EMPTY_ {\n\t\treturn nil\n\t}\n\tsubjectTokens := strings.Split(dest, tsep)\n\tsfwc := false\n\tfor _, t := range subjectTokens {\n\t\tlength := len(t)\n\t\tif length == 0 || sfwc {\n\t\t\treturn &mappingDestinationErr{t, ErrInvalidMappingDestinationSubject}\n\t\t}\n\n\t\t// if it looks like it contains a mapping function, it should be a valid mapping function\n\t\tif length > 4 && t[0] == '{' && t[1] == '{' && t[length-2] == '}' && t[length-1] == '}' {\n\t\t\tif !partitionMappingFunctionRegEx.MatchString(t) &&\n\t\t\t\t!wildcardMappingFunctionRegEx.MatchString(t) &&\n\t\t\t\t!splitFromLeftMappingFunctionRegEx.MatchString(t) &&\n\t\t\t\t!splitFromRightMappingFunctionRegEx.MatchString(t) &&\n\t\t\t\t!sliceFromLeftMappingFunctionRegEx.MatchString(t) &&\n\t\t\t\t!sliceFromRightMappingFunctionRegEx.MatchString(t) &&\n\t\t\t\t!splitMappingFunctionRegEx.MatchString(t) {\n\t\t\t\treturn &mappingDestinationErr{t, ErrUnknownMappingDestinationFunction}\n\t\t\t} else {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t}\n\n\t\tif length == 1 && t[0] == fwc {\n\t\t\tsfwc = true\n\t\t} else if strings.ContainsAny(t, \"\\t\\n\\f\\r \") {\n\t\t\treturn ErrInvalidMappingDestinationSubject\n\t\t}\n\t}\n\n\t// Finally, verify that the transform can actually be created from the source and destination\n\t_, err := NewSubjectTransform(src, dest)\n\treturn err\n}\n\n// Will check tokens and report back if the have any partial or full wildcards.\nfunc analyzeTokens(tokens []string) (hasPWC, hasFWC bool) {\n\tfor _, t := range tokens {\n\t\tif lt := len(t); lt == 0 || lt > 1 {\n\t\t\tcontinue\n\t\t}\n\t\tswitch t[0] {\n\t\tcase pwc:\n\t\t\thasPWC = true\n\t\tcase fwc:\n\t\t\thasFWC = true\n\t\t}\n\t}\n\treturn\n}\n\n// Check on a token basis if they could match.\nfunc tokensCanMatch(t1, t2 string) bool {\n\tif len(t1) == 0 || len(t2) == 0 {\n\t\treturn false\n\t}\n\tt1c, t2c := t1[0], t2[0]\n\tif t1c == pwc || t2c == pwc || t1c == fwc || t2c == fwc {\n\t\treturn true\n\t}\n\treturn t1 == t2\n}\n\n// SubjectsCollide will determine if two subjects could both match a single literal subject.\nfunc SubjectsCollide(subj1, subj2 string) bool {\n\tif subj1 == subj2 {\n\t\treturn true\n\t}\n\ttoks1 := strings.Split(subj1, tsep)\n\ttoks2 := strings.Split(subj2, tsep)\n\tpwc1, fwc1 := analyzeTokens(toks1)\n\tpwc2, fwc2 := analyzeTokens(toks2)\n\t// if both literal just string compare.\n\tl1, l2 := !(pwc1 || fwc1), !(pwc2 || fwc2)\n\tif l1 && l2 {\n\t\treturn subj1 == subj2\n\t}\n\t// So one or both have wildcards. If one is literal than we can do subset matching.\n\tif l1 && !l2 {\n\t\treturn isSubsetMatch(toks1, subj2)\n\t} else if l2 && !l1 {\n\t\treturn isSubsetMatch(toks2, subj1)\n\t}\n\t// Both have wildcards.\n\t// If they only have partials then the lengths must match.\n\tif !fwc1 && !fwc2 && len(toks1) != len(toks2) {\n\t\treturn false\n\t}\n\tif lt1, lt2 := len(toks1), len(toks2); lt1 != lt2 {\n\t\t// If the shorter one only has partials then these will not collide.\n\t\tif lt1 < lt2 && !fwc1 || lt2 < lt1 && !fwc2 {\n\t\t\treturn false\n\t\t}\n\t}\n\n\tstop := len(toks1)\n\tif len(toks2) < stop {\n\t\tstop = len(toks2)\n\t}\n\n\t// We look for reasons to say no.\n\tfor i := 0; i < stop; i++ {\n\t\tt1, t2 := toks1[i], toks2[i]\n\t\tif !tokensCanMatch(t1, t2) {\n\t\t\treturn false\n\t\t}\n\t}\n\n\treturn true\n}\n\n// Returns number of tokens in the subject.\nfunc numTokens(subject string) int {\n\tvar numTokens int\n\tif len(subject) == 0 {\n\t\treturn 0\n\t}\n\tfor i := 0; i < len(subject); i++ {\n\t\tif subject[i] == btsep {\n\t\t\tnumTokens++\n\t\t}\n\t}\n\treturn numTokens + 1\n}\n\n// Fast way to return an indexed token.\n// This is one based, so first token is TokenAt(subject, 1)\nfunc tokenAt(subject string, index uint8) string {\n\tti, start := uint8(1), 0\n\tfor i := 0; i < len(subject); i++ {\n\t\tif subject[i] == btsep {\n\t\t\tif ti == index {\n\t\t\t\treturn subject[start:i]\n\t\t\t}\n\t\t\tstart = i + 1\n\t\t\tti++\n\t\t}\n\t}\n\tif ti == index {\n\t\treturn subject[start:]\n\t}\n\treturn _EMPTY_\n}\n\n// use similar to append. meaning, the updated slice will be returned\nfunc tokenizeSubjectIntoSlice(tts []string, subject string) []string {\n\tstart := 0\n\tfor i := 0; i < len(subject); i++ {\n\t\tif subject[i] == btsep {\n\t\t\ttts = append(tts, subject[start:i])\n\t\t\tstart = i + 1\n\t\t}\n\t}\n\ttts = append(tts, subject[start:])\n\treturn tts\n}\n\n// Calls into the function isSubsetMatch()\nfunc subjectIsSubsetMatch(subject, test string) bool {\n\ttsa := [32]string{}\n\ttts := tokenizeSubjectIntoSlice(tsa[:0], subject)\n\treturn isSubsetMatch(tts, test)\n}\n\n// This will test a subject as an array of tokens against a test subject\n// Calls into the function isSubsetMatchTokenized\nfunc isSubsetMatch(tokens []string, test string) bool {\n\ttsa := [32]string{}\n\ttts := tokenizeSubjectIntoSlice(tsa[:0], test)\n\treturn isSubsetMatchTokenized(tokens, tts)\n}\n\n// This will test a subject as an array of tokens against a test subject (also encoded as array of tokens)\n// and determine if the tokens are matched. Both test subject and tokens\n// may contain wildcards. So foo.* is a subset match of [\">\", \"*.*\", \"foo.*\"],\n// but not of foo.bar, etc.\nfunc isSubsetMatchTokenized(tokens, test []string) bool {\n\t// Walk the target tokens\n\tfor i, t2 := range test {\n\t\tif i >= len(tokens) {\n\t\t\treturn false\n\t\t}\n\t\tl := len(t2)\n\t\tif l == 0 {\n\t\t\treturn false\n\t\t}\n\t\tif t2[0] == fwc && l == 1 {\n\t\t\treturn true\n\t\t}\n\t\tt1 := tokens[i]\n\n\t\tl = len(t1)\n\t\tif l == 0 || t1[0] == fwc && l == 1 {\n\t\t\treturn false\n\t\t}\n\n\t\tif t1[0] == pwc && len(t1) == 1 {\n\t\t\tm := t2[0] == pwc && len(t2) == 1\n\t\t\tif !m {\n\t\t\t\treturn false\n\t\t\t}\n\t\t\tif i >= len(test) {\n\t\t\t\treturn true\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\t\tif t2[0] != pwc && strings.Compare(t1, t2) != 0 {\n\t\t\treturn false\n\t\t}\n\t}\n\treturn len(tokens) == len(test)\n}\n\n// matchLiteral is used to test literal subjects, those that do not have any\n// wildcards, with a target subject. This is used in the cache layer.\nfunc matchLiteral(literal, subject string) bool {\n\tli := 0\n\tll := len(literal)\n\tls := len(subject)\n\tfor i := 0; i < ls; i++ {\n\t\tif li >= ll {\n\t\t\treturn false\n\t\t}\n\t\t// This function has been optimized for speed.\n\t\t// For instance, do not set b:=subject[i] here since\n\t\t// we may bump `i` in this loop to avoid `continue` or\n\t\t// skipping common test in a particular test.\n\t\t// Run Benchmark_SublistMatchLiteral before making any change.\n\t\tswitch subject[i] {\n\t\tcase pwc:\n\t\t\t// NOTE: This is not testing validity of a subject, instead ensures\n\t\t\t// that wildcards are treated as such if they follow some basic rules,\n\t\t\t// namely that they are a token on their own.\n\t\t\tif i == 0 || subject[i-1] == btsep {\n\t\t\t\tif i == ls-1 {\n\t\t\t\t\t// There is no more token in the subject after this wildcard.\n\t\t\t\t\t// Skip token in literal and expect to not find a separator.\n\t\t\t\t\tfor {\n\t\t\t\t\t\t// End of literal, this is a match.\n\t\t\t\t\t\tif li >= ll {\n\t\t\t\t\t\t\treturn true\n\t\t\t\t\t\t}\n\t\t\t\t\t\t// Presence of separator, this can't be a match.\n\t\t\t\t\t\tif literal[li] == btsep {\n\t\t\t\t\t\t\treturn false\n\t\t\t\t\t\t}\n\t\t\t\t\t\tli++\n\t\t\t\t\t}\n\t\t\t\t} else if subject[i+1] == btsep {\n\t\t\t\t\t// There is another token in the subject after this wildcard.\n\t\t\t\t\t// Skip token in literal and expect to get a separator.\n\t\t\t\t\tfor {\n\t\t\t\t\t\t// We found the end of the literal before finding a separator,\n\t\t\t\t\t\t// this can't be a match.\n\t\t\t\t\t\tif li >= ll {\n\t\t\t\t\t\t\treturn false\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif literal[li] == btsep {\n\t\t\t\t\t\t\tbreak\n\t\t\t\t\t\t}\n\t\t\t\t\t\tli++\n\t\t\t\t\t}\n\t\t\t\t\t// Bump `i` since we know there is a `.` following, we are\n\t\t\t\t\t// safe. The common test below is going to check `.` with `.`\n\t\t\t\t\t// which is good. A `continue` here is too costly.\n\t\t\t\t\ti++\n\t\t\t\t}\n\t\t\t}\n\t\tcase fwc:\n\t\t\t// For `>` to be a wildcard, it means being the only or last character\n\t\t\t// in the string preceded by a `.`\n\t\t\tif (i == 0 || subject[i-1] == btsep) && i == ls-1 {\n\t\t\t\treturn true\n\t\t\t}\n\t\t}\n\t\tif subject[i] != literal[li] {\n\t\t\treturn false\n\t\t}\n\t\tli++\n\t}\n\t// Make sure we have processed all of the literal's chars..\n\treturn li >= ll\n}\n\nfunc addLocalSub(sub *subscription, subs *[]*subscription, includeLeafHubs bool) {\n\tif sub != nil && sub.client != nil {\n\t\tkind := sub.client.kind\n\t\tif kind == CLIENT || kind == SYSTEM || kind == JETSTREAM || kind == ACCOUNT ||\n\t\t\t(includeLeafHubs && sub.client.isHubLeafNode() /* implied kind==LEAF */) {\n\t\t\t*subs = append(*subs, sub)\n\t\t}\n\t}\n}\n\nfunc (s *Sublist) addNodeToSubs(n *node, subs *[]*subscription, includeLeafHubs bool) {\n\t// Normal subscriptions\n\tif n.plist != nil {\n\t\tfor _, sub := range n.plist {\n\t\t\taddLocalSub(sub, subs, includeLeafHubs)\n\t\t}\n\t} else {\n\t\tfor sub := range n.psubs {\n\t\t\taddLocalSub(sub, subs, includeLeafHubs)\n\t\t}\n\t}\n\t// Queue subscriptions\n\tfor _, qr := range n.qsubs {\n\t\tfor sub := range qr {\n\t\t\taddLocalSub(sub, subs, includeLeafHubs)\n\t\t}\n\t}\n}\n\nfunc (s *Sublist) collectLocalSubs(l *level, subs *[]*subscription, includeLeafHubs bool) {\n\tfor _, n := range l.nodes {\n\t\ts.addNodeToSubs(n, subs, includeLeafHubs)\n\t\ts.collectLocalSubs(n.next, subs, includeLeafHubs)\n\t}\n\tif l.pwc != nil {\n\t\ts.addNodeToSubs(l.pwc, subs, includeLeafHubs)\n\t\ts.collectLocalSubs(l.pwc.next, subs, includeLeafHubs)\n\t}\n\tif l.fwc != nil {\n\t\ts.addNodeToSubs(l.fwc, subs, includeLeafHubs)\n\t\ts.collectLocalSubs(l.fwc.next, subs, includeLeafHubs)\n\t}\n}\n\n// Return all local client subscriptions. Use the supplied slice.\nfunc (s *Sublist) localSubs(subs *[]*subscription, includeLeafHubs bool) {\n\ts.RLock()\n\ts.collectLocalSubs(s.root, subs, includeLeafHubs)\n\ts.RUnlock()\n}\n\n// All is used to collect all subscriptions.\nfunc (s *Sublist) All(subs *[]*subscription) {\n\ts.RLock()\n\ts.collectAllSubs(s.root, subs)\n\ts.RUnlock()\n}\n\nfunc (s *Sublist) addAllNodeToSubs(n *node, subs *[]*subscription) {\n\t// Normal subscriptions\n\tif n.plist != nil {\n\t\t*subs = append(*subs, n.plist...)\n\t} else {\n\t\tfor sub := range n.psubs {\n\t\t\t*subs = append(*subs, sub)\n\t\t}\n\t}\n\t// Queue subscriptions\n\tfor _, qr := range n.qsubs {\n\t\tfor sub := range qr {\n\t\t\t*subs = append(*subs, sub)\n\t\t}\n\t}\n}\n\nfunc (s *Sublist) collectAllSubs(l *level, subs *[]*subscription) {\n\tfor _, n := range l.nodes {\n\t\ts.addAllNodeToSubs(n, subs)\n\t\ts.collectAllSubs(n.next, subs)\n\t}\n\tif l.pwc != nil {\n\t\ts.addAllNodeToSubs(l.pwc, subs)\n\t\ts.collectAllSubs(l.pwc.next, subs)\n\t}\n\tif l.fwc != nil {\n\t\ts.addAllNodeToSubs(l.fwc, subs)\n\t\ts.collectAllSubs(l.fwc.next, subs)\n\t}\n}\n\n// For a given subject (which may contain wildcards), this call returns all\n// subscriptions that would match that subject. For instance, suppose that\n// the sublist contains: foo.bar, foo.bar.baz and foo.baz, ReverseMatch(\"foo.*\")\n// would return foo.bar and foo.baz.\n// This is used in situations where the sublist is likely to contain only\n// literals and one wants to get all the subjects that would have been a match\n// to a subscription on `subject`.\nfunc (s *Sublist) ReverseMatch(subject string) *SublistResult {\n\ttsa := [32]string{}\n\ttokens := tsa[:0]\n\tstart := 0\n\tfor i := 0; i < len(subject); i++ {\n\t\tif subject[i] == btsep {\n\t\t\ttokens = append(tokens, subject[start:i])\n\t\t\tstart = i + 1\n\t\t}\n\t}\n\ttokens = append(tokens, subject[start:])\n\n\tresult := &SublistResult{}\n\n\ts.RLock()\n\treverseMatchLevel(s.root, tokens, nil, result)\n\t// Check for empty result.\n\tif len(result.psubs) == 0 && len(result.qsubs) == 0 {\n\t\tresult = emptyResult\n\t}\n\ts.RUnlock()\n\n\treturn result\n}\n\nfunc reverseMatchLevel(l *level, toks []string, n *node, results *SublistResult) {\n\tif l == nil {\n\t\treturn\n\t}\n\tfor i, t := range toks {\n\t\tif len(t) == 1 {\n\t\t\tif t[0] == fwc {\n\t\t\t\tgetAllNodes(l, results)\n\t\t\t\treturn\n\t\t\t} else if t[0] == pwc {\n\t\t\t\tfor _, n := range l.nodes {\n\t\t\t\t\treverseMatchLevel(n.next, toks[i+1:], n, results)\n\t\t\t\t}\n\t\t\t\tif l.pwc != nil {\n\t\t\t\t\treverseMatchLevel(l.pwc.next, toks[i+1:], n, results)\n\t\t\t\t}\n\t\t\t\tif l.fwc != nil {\n\t\t\t\t\tgetAllNodes(l, results)\n\t\t\t\t}\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t\t// If the sub tree has a fwc at this position, match as well.\n\t\tif l.fwc != nil {\n\t\t\tgetAllNodes(l, results)\n\t\t\treturn\n\t\t} else if l.pwc != nil {\n\t\t\treverseMatchLevel(l.pwc.next, toks[i+1:], n, results)\n\t\t}\n\t\tn = l.nodes[t]\n\t\tif n == nil {\n\t\t\tbreak\n\t\t}\n\t\tl = n.next\n\t}\n\tif n != nil {\n\t\taddNodeToResults(n, results)\n\t}\n}\n\nfunc getAllNodes(l *level, results *SublistResult) {\n\tif l == nil {\n\t\treturn\n\t}\n\tif l.pwc != nil {\n\t\taddNodeToResults(l.pwc, results)\n\t}\n\tif l.fwc != nil {\n\t\taddNodeToResults(l.fwc, results)\n\t}\n\tfor _, n := range l.nodes {\n\t\taddNodeToResults(n, results)\n\t\tgetAllNodes(n.next, results)\n\t}\n}\n\n// IntersectStree will match all items in the given subject tree that\n// have interest expressed in the given sublist. The callback will only be called\n// once for each subject, regardless of overlapping subscriptions in the sublist.\nfunc IntersectStree[T any](st *stree.SubjectTree[T], sl *Sublist, cb func(subj []byte, entry *T)) {\n\tvar _subj [255]byte\n\tintersectStree(st, sl.root, _subj[:0], cb)\n}\n\nfunc intersectStree[T any](st *stree.SubjectTree[T], r *level, subj []byte, cb func(subj []byte, entry *T)) {\n\tnsubj := subj\n\tif len(nsubj) > 0 {\n\t\tnsubj = append(subj, '.')\n\t}\n\tswitch {\n\tcase r.fwc != nil:\n\t\t// We've reached a full wildcard, do a FWC match on the stree at this point\n\t\t// and don't keep iterating downward.\n\t\tnsubj := append(nsubj, '>')\n\t\tst.Match(nsubj, cb)\n\tcase r.pwc != nil:\n\t\t// We've found a partial wildcard. We'll keep iterating downwards, but first\n\t\t// check whether there's interest at this level (without triggering dupes) and\n\t\t// match if so.\n\t\tnsubj := append(nsubj, '*')\n\t\tif len(r.pwc.psubs)+len(r.pwc.qsubs) > 0 {\n\t\t\tst.Match(nsubj, cb)\n\t\t}\n\t\tif r.pwc.next != nil && r.pwc.next.numNodes() > 0 {\n\t\t\tintersectStree(st, r.pwc.next, nsubj, cb)\n\t\t}\n\tdefault:\n\t\t// Normal node with subject literals, keep iterating.\n\t\tfor t, n := range r.nodes {\n\t\t\tnsubj := append(nsubj, t...)\n\t\t\tif len(n.psubs)+len(n.qsubs) > 0 {\n\t\t\t\tif subjectHasWildcard(bytesToString(nsubj)) {\n\t\t\t\t\tst.Match(nsubj, cb)\n\t\t\t\t} else {\n\t\t\t\t\tif e, ok := st.Find(nsubj); ok {\n\t\t\t\t\t\tcb(nsubj, e)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tif n.next != nil && n.next.numNodes() > 0 {\n\t\t\t\tintersectStree(st, n.next, nsubj, cb)\n\t\t\t}\n\t\t}\n\t}\n}\n",
    "source_file": "server/sublist.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2013-2025 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage server\n\nimport (\n\t\"bytes\"\n\t\"crypto/tls\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"hash/fnv\"\n\t\"math/rand\"\n\t\"net\"\n\t\"net/url\"\n\t\"reflect\"\n\t\"runtime\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t\"github.com/klauspost/compress/s2\"\n)\n\n// RouteType designates the router type\ntype RouteType int\n\n// Type of Route\nconst (\n\t// This route we learned from speaking to other routes.\n\tImplicit RouteType = iota\n\t// This route was explicitly configured.\n\tExplicit\n)\n\n// Include the space for the proto\nvar (\n\taSubBytes   = []byte{'A', '+', ' '}\n\taUnsubBytes = []byte{'A', '-', ' '}\n\trSubBytes   = []byte{'R', 'S', '+', ' '}\n\trUnsubBytes = []byte{'R', 'S', '-', ' '}\n\tlSubBytes   = []byte{'L', 'S', '+', ' '}\n\tlUnsubBytes = []byte{'L', 'S', '-', ' '}\n)\n\ntype route struct {\n\tremoteID     string\n\tremoteName   string\n\tdidSolicit   bool\n\tretry        bool\n\tlnoc         bool\n\tlnocu        bool\n\trouteType    RouteType\n\turl          *url.URL\n\tauthRequired bool\n\ttlsRequired  bool\n\tjetstream    bool\n\tconnectURLs  []string\n\twsConnURLs   []string\n\tgatewayURL   string\n\tleafnodeURL  string\n\thash         string\n\tidHash       string\n\t// Location of the route in the slice: s.routes[remoteID][]*client.\n\t// Initialized to -1 on creation, as to indicate that it is not\n\t// added to the list.\n\tpoolIdx int\n\t// If this is set, it means that the route is dedicated for this\n\t// account and the account name will not be included in protocols.\n\taccName []byte\n\t// This is set to true if this is a route connection to an old\n\t// server or a server that has pooling completely disabled.\n\tnoPool bool\n\t// Selected compression mode, which may be different from the\n\t// server configured mode.\n\tcompression string\n\t// Transient value used to set the Info.GossipMode when initiating\n\t// an implicit route and sending to the remote.\n\tgossipMode byte\n}\n\n// Do not change the values/order since they are exchanged between servers.\nconst (\n\tgossipDefault = byte(iota)\n\tgossipDisabled\n\tgossipOverride\n)\n\ntype connectInfo struct {\n\tEcho     bool   `json:\"echo\"`\n\tVerbose  bool   `json:\"verbose\"`\n\tPedantic bool   `json:\"pedantic\"`\n\tUser     string `json:\"user,omitempty\"`\n\tPass     string `json:\"pass,omitempty\"`\n\tTLS      bool   `json:\"tls_required\"`\n\tHeaders  bool   `json:\"headers\"`\n\tName     string `json:\"name\"`\n\tCluster  string `json:\"cluster\"`\n\tDynamic  bool   `json:\"cluster_dynamic,omitempty\"`\n\tLNOC     bool   `json:\"lnoc,omitempty\"`\n\tLNOCU    bool   `json:\"lnocu,omitempty\"` // Support for LS- with origin cluster name\n\tGateway  string `json:\"gateway,omitempty\"`\n}\n\n// Route protocol constants\nconst (\n\tConProto  = \"CONNECT %s\" + _CRLF_\n\tInfoProto = \"INFO %s\" + _CRLF_\n)\n\nconst (\n\t// Warning when user configures cluster TLS insecure\n\tclusterTLSInsecureWarning = \"TLS certificate chain and hostname of solicited routes will not be verified. DO NOT USE IN PRODUCTION!\"\n\n\t// The default ping interval is set to 2 minutes, which is fine for client\n\t// connections, etc.. but for route compression, the CompressionS2Auto\n\t// mode uses RTT measurements (ping/pong) to decide which compression level\n\t// to use, we want the interval to not be that high.\n\tdefaultRouteMaxPingInterval = 30 * time.Second\n)\n\n// Can be changed for tests\nvar (\n\trouteConnectDelay    = DEFAULT_ROUTE_CONNECT\n\trouteMaxPingInterval = defaultRouteMaxPingInterval\n)\n\n// removeReplySub is called when we trip the max on remoteReply subs.\nfunc (c *client) removeReplySub(sub *subscription) {\n\tif sub == nil {\n\t\treturn\n\t}\n\t// Lookup the account based on sub.sid.\n\tif i := bytes.Index(sub.sid, []byte(\" \")); i > 0 {\n\t\t// First part of SID for route is account name.\n\t\tif v, ok := c.srv.accounts.Load(bytesToString(sub.sid[:i])); ok {\n\t\t\t(v.(*Account)).sl.Remove(sub)\n\t\t}\n\t\tc.mu.Lock()\n\t\tdelete(c.subs, bytesToString(sub.sid))\n\t\tc.mu.Unlock()\n\t}\n}\n\nfunc (c *client) processAccountSub(arg []byte) error {\n\tif c.kind == GATEWAY {\n\t\treturn c.processGatewayAccountSub(string(arg))\n\t}\n\treturn nil\n}\n\nfunc (c *client) processAccountUnsub(arg []byte) {\n\tif c.kind == GATEWAY {\n\t\tc.processGatewayAccountUnsub(string(arg))\n\t}\n}\n\n// Process an inbound LMSG specification from the remote route. This means\n// we have an origin cluster and we force header semantics.\nfunc (c *client) processRoutedOriginClusterMsgArgs(arg []byte) error {\n\t// Unroll splitArgs to avoid runtime/heap issues\n\ta := [MAX_HMSG_ARGS + 1][]byte{}\n\targs := a[:0]\n\tstart := -1\n\tfor i, b := range arg {\n\t\tswitch b {\n\t\tcase ' ', '\\t', '\\r', '\\n':\n\t\t\tif start >= 0 {\n\t\t\t\targs = append(args, arg[start:i])\n\t\t\t\tstart = -1\n\t\t\t}\n\t\tdefault:\n\t\t\tif start < 0 {\n\t\t\t\tstart = i\n\t\t\t}\n\t\t}\n\t}\n\tif start >= 0 {\n\t\targs = append(args, arg[start:])\n\t}\n\n\tvar an []byte\n\tif c.kind == ROUTER {\n\t\tif an = c.route.accName; len(an) > 0 && len(args) > 2 {\n\t\t\targs = append(args[:2], args[1:]...)\n\t\t\targs[1] = an\n\t\t}\n\t}\n\tc.pa.arg = arg\n\tswitch len(args) {\n\tcase 0, 1, 2, 3, 4:\n\t\treturn fmt.Errorf(\"processRoutedOriginClusterMsgArgs Parse Error: '%s'\", args)\n\tcase 5:\n\t\tc.pa.reply = nil\n\t\tc.pa.queues = nil\n\t\tc.pa.hdb = args[3]\n\t\tc.pa.hdr = parseSize(args[3])\n\t\tc.pa.szb = args[4]\n\t\tc.pa.size = parseSize(args[4])\n\tcase 6:\n\t\tc.pa.reply = args[3]\n\t\tc.pa.queues = nil\n\t\tc.pa.hdb = args[4]\n\t\tc.pa.hdr = parseSize(args[4])\n\t\tc.pa.szb = args[5]\n\t\tc.pa.size = parseSize(args[5])\n\tdefault:\n\t\t// args[2] is our reply indicator. Should be + or | normally.\n\t\tif len(args[3]) != 1 {\n\t\t\treturn fmt.Errorf(\"processRoutedOriginClusterMsgArgs Bad or Missing Reply Indicator: '%s'\", args[3])\n\t\t}\n\t\tswitch args[3][0] {\n\t\tcase '+':\n\t\t\tc.pa.reply = args[4]\n\t\tcase '|':\n\t\t\tc.pa.reply = nil\n\t\tdefault:\n\t\t\treturn fmt.Errorf(\"processRoutedOriginClusterMsgArgs Bad or Missing Reply Indicator: '%s'\", args[3])\n\t\t}\n\n\t\t// Grab header size.\n\t\tc.pa.hdb = args[len(args)-2]\n\t\tc.pa.hdr = parseSize(c.pa.hdb)\n\n\t\t// Grab size.\n\t\tc.pa.szb = args[len(args)-1]\n\t\tc.pa.size = parseSize(c.pa.szb)\n\n\t\t// Grab queue names.\n\t\tif c.pa.reply != nil {\n\t\t\tc.pa.queues = args[5 : len(args)-2]\n\t\t} else {\n\t\t\tc.pa.queues = args[4 : len(args)-2]\n\t\t}\n\t}\n\tif c.pa.hdr < 0 {\n\t\treturn fmt.Errorf(\"processRoutedOriginClusterMsgArgs Bad or Missing Header Size: '%s'\", arg)\n\t}\n\tif c.pa.size < 0 {\n\t\treturn fmt.Errorf(\"processRoutedOriginClusterMsgArgs Bad or Missing Size: '%s'\", args)\n\t}\n\tif c.pa.hdr > c.pa.size {\n\t\treturn fmt.Errorf(\"processRoutedOriginClusterMsgArgs Header Size larger then TotalSize: '%s'\", arg)\n\t}\n\n\t// Common ones processed after check for arg length\n\tc.pa.origin = args[0]\n\tc.pa.account = args[1]\n\tc.pa.subject = args[2]\n\tif len(an) > 0 {\n\t\tc.pa.pacache = c.pa.subject\n\t} else {\n\t\tc.pa.pacache = arg[len(args[0])+1 : len(args[0])+len(args[1])+len(args[2])+2]\n\t}\n\treturn nil\n}\n\n// Process an inbound HMSG specification from the remote route.\nfunc (c *client) processRoutedHeaderMsgArgs(arg []byte) error {\n\t// Unroll splitArgs to avoid runtime/heap issues\n\ta := [MAX_HMSG_ARGS][]byte{}\n\targs := a[:0]\n\tvar an []byte\n\tif c.kind == ROUTER {\n\t\tif an = c.route.accName; len(an) > 0 {\n\t\t\targs = append(args, an)\n\t\t}\n\t}\n\tstart := -1\n\tfor i, b := range arg {\n\t\tswitch b {\n\t\tcase ' ', '\\t', '\\r', '\\n':\n\t\t\tif start >= 0 {\n\t\t\t\targs = append(args, arg[start:i])\n\t\t\t\tstart = -1\n\t\t\t}\n\t\tdefault:\n\t\t\tif start < 0 {\n\t\t\t\tstart = i\n\t\t\t}\n\t\t}\n\t}\n\tif start >= 0 {\n\t\targs = append(args, arg[start:])\n\t}\n\n\tc.pa.arg = arg\n\tswitch len(args) {\n\tcase 0, 1, 2, 3:\n\t\treturn fmt.Errorf(\"processRoutedHeaderMsgArgs Parse Error: '%s'\", args)\n\tcase 4:\n\t\tc.pa.reply = nil\n\t\tc.pa.queues = nil\n\t\tc.pa.hdb = args[2]\n\t\tc.pa.hdr = parseSize(args[2])\n\t\tc.pa.szb = args[3]\n\t\tc.pa.size = parseSize(args[3])\n\tcase 5:\n\t\tc.pa.reply = args[2]\n\t\tc.pa.queues = nil\n\t\tc.pa.hdb = args[3]\n\t\tc.pa.hdr = parseSize(args[3])\n\t\tc.pa.szb = args[4]\n\t\tc.pa.size = parseSize(args[4])\n\tdefault:\n\t\t// args[2] is our reply indicator. Should be + or | normally.\n\t\tif len(args[2]) != 1 {\n\t\t\treturn fmt.Errorf(\"processRoutedHeaderMsgArgs Bad or Missing Reply Indicator: '%s'\", args[2])\n\t\t}\n\t\tswitch args[2][0] {\n\t\tcase '+':\n\t\t\tc.pa.reply = args[3]\n\t\tcase '|':\n\t\t\tc.pa.reply = nil\n\t\tdefault:\n\t\t\treturn fmt.Errorf(\"processRoutedHeaderMsgArgs Bad or Missing Reply Indicator: '%s'\", args[2])\n\t\t}\n\n\t\t// Grab header size.\n\t\tc.pa.hdb = args[len(args)-2]\n\t\tc.pa.hdr = parseSize(c.pa.hdb)\n\n\t\t// Grab size.\n\t\tc.pa.szb = args[len(args)-1]\n\t\tc.pa.size = parseSize(c.pa.szb)\n\n\t\t// Grab queue names.\n\t\tif c.pa.reply != nil {\n\t\t\tc.pa.queues = args[4 : len(args)-2]\n\t\t} else {\n\t\t\tc.pa.queues = args[3 : len(args)-2]\n\t\t}\n\t}\n\tif c.pa.hdr < 0 {\n\t\treturn fmt.Errorf(\"processRoutedHeaderMsgArgs Bad or Missing Header Size: '%s'\", arg)\n\t}\n\tif c.pa.size < 0 {\n\t\treturn fmt.Errorf(\"processRoutedHeaderMsgArgs Bad or Missing Size: '%s'\", args)\n\t}\n\tif c.pa.hdr > c.pa.size {\n\t\treturn fmt.Errorf(\"processRoutedHeaderMsgArgs Header Size larger then TotalSize: '%s'\", arg)\n\t}\n\n\t// Common ones processed after check for arg length\n\tc.pa.account = args[0]\n\tc.pa.subject = args[1]\n\tif len(an) > 0 {\n\t\tc.pa.pacache = c.pa.subject\n\t} else {\n\t\tc.pa.pacache = arg[:len(args[0])+len(args[1])+1]\n\t}\n\treturn nil\n}\n\n// Process an inbound RMSG or LMSG specification from the remote route.\nfunc (c *client) processRoutedMsgArgs(arg []byte) error {\n\t// Unroll splitArgs to avoid runtime/heap issues\n\ta := [MAX_RMSG_ARGS][]byte{}\n\targs := a[:0]\n\tvar an []byte\n\tif c.kind == ROUTER {\n\t\tif an = c.route.accName; len(an) > 0 {\n\t\t\targs = append(args, an)\n\t\t}\n\t}\n\tstart := -1\n\tfor i, b := range arg {\n\t\tswitch b {\n\t\tcase ' ', '\\t', '\\r', '\\n':\n\t\t\tif start >= 0 {\n\t\t\t\targs = append(args, arg[start:i])\n\t\t\t\tstart = -1\n\t\t\t}\n\t\tdefault:\n\t\t\tif start < 0 {\n\t\t\t\tstart = i\n\t\t\t}\n\t\t}\n\t}\n\tif start >= 0 {\n\t\targs = append(args, arg[start:])\n\t}\n\n\tc.pa.arg = arg\n\tswitch len(args) {\n\tcase 0, 1, 2:\n\t\treturn fmt.Errorf(\"processRoutedMsgArgs Parse Error: '%s'\", args)\n\tcase 3:\n\t\tc.pa.reply = nil\n\t\tc.pa.queues = nil\n\t\tc.pa.szb = args[2]\n\t\tc.pa.size = parseSize(args[2])\n\tcase 4:\n\t\tc.pa.reply = args[2]\n\t\tc.pa.queues = nil\n\t\tc.pa.szb = args[3]\n\t\tc.pa.size = parseSize(args[3])\n\tdefault:\n\t\t// args[2] is our reply indicator. Should be + or | normally.\n\t\tif len(args[2]) != 1 {\n\t\t\treturn fmt.Errorf(\"processRoutedMsgArgs Bad or Missing Reply Indicator: '%s'\", args[2])\n\t\t}\n\t\tswitch args[2][0] {\n\t\tcase '+':\n\t\t\tc.pa.reply = args[3]\n\t\tcase '|':\n\t\t\tc.pa.reply = nil\n\t\tdefault:\n\t\t\treturn fmt.Errorf(\"processRoutedMsgArgs Bad or Missing Reply Indicator: '%s'\", args[2])\n\t\t}\n\t\t// Grab size.\n\t\tc.pa.szb = args[len(args)-1]\n\t\tc.pa.size = parseSize(c.pa.szb)\n\n\t\t// Grab queue names.\n\t\tif c.pa.reply != nil {\n\t\t\tc.pa.queues = args[4 : len(args)-1]\n\t\t} else {\n\t\t\tc.pa.queues = args[3 : len(args)-1]\n\t\t}\n\t}\n\tif c.pa.size < 0 {\n\t\treturn fmt.Errorf(\"processRoutedMsgArgs Bad or Missing Size: '%s'\", args)\n\t}\n\n\t// Common ones processed after check for arg length\n\tc.pa.account = args[0]\n\tc.pa.subject = args[1]\n\tif len(an) > 0 {\n\t\tc.pa.pacache = c.pa.subject\n\t} else {\n\t\tc.pa.pacache = arg[:len(args[0])+len(args[1])+1]\n\t}\n\treturn nil\n}\n\n// processInboundRoutedMsg is called to process an inbound msg from a route.\nfunc (c *client) processInboundRoutedMsg(msg []byte) {\n\t// Update statistics\n\tc.in.msgs++\n\t// The msg includes the CR_LF, so pull back out for accounting.\n\tsize := len(msg) - LEN_CR_LF\n\tc.in.bytes += int32(size)\n\n\tif c.opts.Verbose {\n\t\tc.sendOK()\n\t}\n\n\t// Mostly under testing scenarios.\n\tif c.srv == nil {\n\t\treturn\n\t}\n\n\t// If the subject (c.pa.subject) has the gateway prefix, this function will handle it.\n\tif c.handleGatewayReply(msg) {\n\t\t// We are done here.\n\t\treturn\n\t}\n\n\tacc, r := c.getAccAndResultFromCache()\n\tif acc == nil {\n\t\tc.Debugf(\"Unknown account %q for routed message on subject: %q\", c.pa.account, c.pa.subject)\n\t\treturn\n\t}\n\n\tacc.stats.Lock()\n\tacc.stats.inMsgs++\n\tacc.stats.inBytes += int64(size)\n\tacc.stats.rt.inMsgs++\n\tacc.stats.rt.inBytes += int64(size)\n\tacc.stats.Unlock()\n\n\t// Check for no interest, short circuit if so.\n\t// This is the fanout scale.\n\tif len(r.psubs)+len(r.qsubs) > 0 {\n\t\tc.processMsgResults(acc, r, msg, nil, c.pa.subject, c.pa.reply, pmrNoFlag)\n\t}\n}\n\n// Lock should be held entering here.\nfunc (c *client) sendRouteConnect(clusterName string, tlsRequired bool) error {\n\tvar user, pass string\n\tif userInfo := c.route.url.User; userInfo != nil {\n\t\tuser = userInfo.Username()\n\t\tpass, _ = userInfo.Password()\n\t}\n\ts := c.srv\n\tcinfo := connectInfo{\n\t\tEcho:     true,\n\t\tVerbose:  false,\n\t\tPedantic: false,\n\t\tUser:     user,\n\t\tPass:     pass,\n\t\tTLS:      tlsRequired,\n\t\tName:     s.info.ID,\n\t\tHeaders:  s.supportsHeaders(),\n\t\tCluster:  clusterName,\n\t\tDynamic:  s.isClusterNameDynamic(),\n\t\tLNOC:     true,\n\t}\n\n\tb, err := json.Marshal(cinfo)\n\tif err != nil {\n\t\tc.Errorf(\"Error marshaling CONNECT to route: %v\\n\", err)\n\t\treturn err\n\t}\n\tc.enqueueProto([]byte(fmt.Sprintf(ConProto, b)))\n\treturn nil\n}\n\n// Returns a route pool index for this account based on the given pool size.\n// If `poolSize` is smaller or equal to 1, the returned value will always\n// be 0, regardless of the account name. If not, the returned value will\n// be in the range [0..poolSize-1]. The value for a given account name\n// is constant and same on all servers (given the same `poolSize` value).\nfunc computeRoutePoolIdx(poolSize int, an string) int {\n\tif poolSize <= 1 {\n\t\treturn 0\n\t}\n\th := fnv.New32a()\n\th.Write([]byte(an))\n\tsum32 := h.Sum32()\n\treturn int((sum32 % uint32(poolSize)))\n}\n\n// Process the info message if we are a route.\nfunc (c *client) processRouteInfo(info *Info) {\n\n\tsupportsHeaders := c.srv.supportsHeaders()\n\tclusterName := c.srv.ClusterName()\n\tsrvName := c.srv.Name()\n\n\tc.mu.Lock()\n\t// Connection can be closed at any time (by auth timeout, etc).\n\t// Does not make sense to continue here if connection is gone.\n\tif c.route == nil || c.isClosed() {\n\t\tc.mu.Unlock()\n\t\treturn\n\t}\n\n\ts := c.srv\n\n\t// Detect route to self.\n\tif info.ID == s.info.ID {\n\t\t// Need to set this so that the close does the right thing\n\t\tc.route.remoteID = info.ID\n\t\tc.mu.Unlock()\n\t\tc.closeConnection(DuplicateRoute)\n\t\treturn\n\t}\n\n\t// Detect if we have a mismatch of cluster names.\n\tif info.Cluster != \"\" && info.Cluster != clusterName {\n\t\tc.mu.Unlock()\n\t\t// If we are dynamic we may update our cluster name.\n\t\t// Use other if remote is non dynamic or their name is \"bigger\"\n\t\tif s.isClusterNameDynamic() && (!info.Dynamic || (strings.Compare(clusterName, info.Cluster) < 0)) {\n\t\t\ts.setClusterName(info.Cluster)\n\t\t\ts.removeAllRoutesExcept(info.ID)\n\t\t\tc.mu.Lock()\n\t\t} else {\n\t\t\tc.closeConnection(ClusterNameConflict)\n\t\t\treturn\n\t\t}\n\t}\n\n\topts := s.getOpts()\n\n\tdidSolicit := c.route.didSolicit\n\n\t// If this is an async INFO from an existing route...\n\tif c.flags.isSet(infoReceived) {\n\t\tremoteID := c.route.remoteID\n\n\t\t// Check if this is an INFO about adding a per-account route during\n\t\t// a configuration reload.\n\t\tif info.RouteAccReqID != _EMPTY_ {\n\t\t\tc.mu.Unlock()\n\n\t\t\t// If there is an account name, then the remote server is telling\n\t\t\t// us that this account will now have its dedicated route.\n\t\t\tif an := info.RouteAccount; an != _EMPTY_ {\n\t\t\t\tacc, err := s.LookupAccount(an)\n\t\t\t\tif err != nil {\n\t\t\t\t\ts.Errorf(\"Error looking up account %q: %v\", an, err)\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t\ts.mu.Lock()\n\t\t\t\t// If running without system account and adding a dedicated\n\t\t\t\t// route for an account for the first time, it could be that\n\t\t\t\t// the map is nil. If so, create it.\n\t\t\t\tif s.accRoutes == nil {\n\t\t\t\t\ts.accRoutes = make(map[string]map[string]*client)\n\t\t\t\t}\n\t\t\t\tif _, ok := s.accRoutes[an]; !ok {\n\t\t\t\t\ts.accRoutes[an] = make(map[string]*client)\n\t\t\t\t}\n\t\t\t\tacc.mu.Lock()\n\t\t\t\tsl := acc.sl\n\t\t\t\trpi := acc.routePoolIdx\n\t\t\t\t// Make sure that the account was not already switched.\n\t\t\t\tif rpi >= 0 {\n\t\t\t\t\ts.setRouteInfo(acc)\n\t\t\t\t\t// Change the route pool index to indicate that this\n\t\t\t\t\t// account is actually transitioning. This will be used\n\t\t\t\t\t// to suppress possible remote subscription interest coming\n\t\t\t\t\t// in while the transition is happening.\n\t\t\t\t\tacc.routePoolIdx = accTransitioningToDedicatedRoute\n\t\t\t\t}\n\t\t\t\tacc.mu.Unlock()\n\t\t\t\t// Since v2.11.0, we support remotes with a different pool size\n\t\t\t\t// (for rolling upgrades), so we need to use the remote route\n\t\t\t\t// pool index (based on the remote configured pool size) since\n\t\t\t\t// the remote subscriptions will be attached to the route at\n\t\t\t\t// that index, not at our account's route pool index. But we\n\t\t\t\t// need to compute only if rpi is negative or the pool sizes\n\t\t\t\t// are different.\n\t\t\t\tif rpi <= 0 || info.RoutePoolSize != s.routesPoolSize {\n\t\t\t\t\trpi = computeRoutePoolIdx(info.RoutePoolSize, an)\n\t\t\t\t}\n\t\t\t\t// Go over each remote's route at pool index `rpi` and remove\n\t\t\t\t// remote subs for this account.\n\t\t\t\ts.forEachRouteIdx(rpi, func(r *client) bool {\n\t\t\t\t\tr.mu.Lock()\n\t\t\t\t\t// Exclude routes to servers that don't support pooling.\n\t\t\t\t\tif !r.route.noPool {\n\t\t\t\t\t\tif subs := r.removeRemoteSubsForAcc(an); len(subs) > 0 {\n\t\t\t\t\t\t\tsl.RemoveBatch(subs)\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tr.mu.Unlock()\n\t\t\t\t\treturn true\n\t\t\t\t})\n\t\t\t\t// Respond to the remote by clearing the RouteAccount field.\n\t\t\t\tinfo.RouteAccount = _EMPTY_\n\t\t\t\tproto := generateInfoJSON(info)\n\t\t\t\tc.mu.Lock()\n\t\t\t\tc.enqueueProto(proto)\n\t\t\t\tc.mu.Unlock()\n\t\t\t\ts.mu.Unlock()\n\t\t\t} else {\n\t\t\t\t// If no account name is specified, this is a response from the\n\t\t\t\t// remote. Simply send to the communication channel, if the\n\t\t\t\t// request ID matches the current one.\n\t\t\t\ts.mu.Lock()\n\t\t\t\tif info.RouteAccReqID == s.accAddedReqID && s.accAddedCh != nil {\n\t\t\t\t\tselect {\n\t\t\t\t\tcase s.accAddedCh <- struct{}{}:\n\t\t\t\t\tdefault:\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\ts.mu.Unlock()\n\t\t\t}\n\t\t\t// In both cases, we are done here.\n\t\t\treturn\n\t\t}\n\n\t\t// Check if this is an INFO for gateways...\n\t\tif info.Gateway != _EMPTY_ {\n\t\t\tc.mu.Unlock()\n\t\t\t// If this server has no gateway configured, report error and return.\n\t\t\tif !s.gateway.enabled {\n\t\t\t\t// FIXME: Should this be a Fatalf()?\n\t\t\t\ts.Errorf(\"Received information about gateway %q from %s, but gateway is not configured\",\n\t\t\t\t\tinfo.Gateway, remoteID)\n\t\t\t\treturn\n\t\t\t}\n\t\t\ts.processGatewayInfoFromRoute(info, remoteID)\n\t\t\treturn\n\t\t}\n\n\t\t// We receive an INFO from a server that informs us about another server,\n\t\t// so the info.ID in the INFO protocol does not match the ID of this route.\n\t\tif remoteID != _EMPTY_ && remoteID != info.ID {\n\t\t\t// We want to know if the existing route supports pooling/pinned-account\n\t\t\t// or not when processing the implicit route.\n\t\t\tnoPool := c.route.noPool\n\t\t\tc.mu.Unlock()\n\n\t\t\t// Process this implicit route. We will check that it is not an explicit\n\t\t\t// route and/or that it has not been connected already.\n\t\t\ts.processImplicitRoute(info, noPool)\n\t\t\treturn\n\t\t}\n\n\t\tvar connectURLs []string\n\t\tvar wsConnectURLs []string\n\t\tvar updateRoutePerms bool\n\n\t\t// If we are notified that the remote is going into LDM mode, capture route's connectURLs.\n\t\tif info.LameDuckMode {\n\t\t\tconnectURLs = c.route.connectURLs\n\t\t\twsConnectURLs = c.route.wsConnURLs\n\t\t} else {\n\t\t\t// Update only if we detect a difference\n\t\t\tupdateRoutePerms = !reflect.DeepEqual(c.opts.Import, info.Import) || !reflect.DeepEqual(c.opts.Export, info.Export)\n\t\t}\n\t\tc.mu.Unlock()\n\n\t\tif updateRoutePerms {\n\t\t\ts.updateRemoteRoutePerms(c, info)\n\t\t}\n\n\t\t// If the remote is going into LDM and there are client connect URLs\n\t\t// associated with this route and we are allowed to advertise, remove\n\t\t// those URLs and update our clients.\n\t\tif (len(connectURLs) > 0 || len(wsConnectURLs) > 0) && !opts.Cluster.NoAdvertise {\n\t\t\ts.mu.Lock()\n\t\t\ts.removeConnectURLsAndSendINFOToClients(connectURLs, wsConnectURLs)\n\t\t\ts.mu.Unlock()\n\t\t}\n\t\treturn\n\t}\n\n\t// Check if remote has same server name than this server.\n\tif !didSolicit && info.Name == srvName {\n\t\tc.mu.Unlock()\n\t\t// This is now an error and we close the connection. We need unique names for JetStream clustering.\n\t\tc.Errorf(\"Remote server has a duplicate name: %q\", info.Name)\n\t\tc.closeConnection(DuplicateServerName)\n\t\treturn\n\t}\n\n\tvar sendDelayedInfo bool\n\n\t// First INFO, check if this server is configured for compression because\n\t// if that is the case, we need to negotiate it with the remote server.\n\tif needsCompression(opts.Cluster.Compression.Mode) {\n\t\taccName := bytesToString(c.route.accName)\n\t\t// If we did not yet negotiate...\n\t\tcompNeg := c.flags.isSet(compressionNegotiated)\n\t\tif !compNeg {\n\t\t\t// Prevent from getting back here.\n\t\t\tc.flags.set(compressionNegotiated)\n\t\t\t// Release client lock since following function will need server lock.\n\t\t\tc.mu.Unlock()\n\t\t\tcompress, err := s.negotiateRouteCompression(c, didSolicit, accName, info.Compression, opts)\n\t\t\tif err != nil {\n\t\t\t\tc.sendErrAndErr(err.Error())\n\t\t\t\tc.closeConnection(ProtocolViolation)\n\t\t\t\treturn\n\t\t\t}\n\t\t\tif compress {\n\t\t\t\t// Done for now, will get back another INFO protocol...\n\t\t\t\treturn\n\t\t\t}\n\t\t\t// No compression because one side does not want/can't, so proceed.\n\t\t\tc.mu.Lock()\n\t\t\t// Check that the connection did not close if the lock was released.\n\t\t\tif c.isClosed() {\n\t\t\t\tc.mu.Unlock()\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t\t// We can set the ping timer after we just negotiated compression above,\n\t\t// or for solicited routes if we already negotiated.\n\t\tif !compNeg || didSolicit {\n\t\t\tc.setFirstPingTimer()\n\t\t}\n\t\t// When compression is configured, we delay the initial INFO for any\n\t\t// solicited route. So we need to send the delayed INFO simply based\n\t\t// on the didSolicit boolean.\n\t\tsendDelayedInfo = didSolicit\n\t} else {\n\t\t// Coming from an old server, the Compression field would be the empty\n\t\t// string. For servers that are configured with CompressionNotSupported,\n\t\t// this makes them behave as old servers.\n\t\tif info.Compression == _EMPTY_ || opts.Cluster.Compression.Mode == CompressionNotSupported {\n\t\t\tc.route.compression = CompressionNotSupported\n\t\t} else {\n\t\t\tc.route.compression = CompressionOff\n\t\t}\n\t\t// When compression is not configured, we delay the initial INFO only\n\t\t// for solicited pooled routes, so use the same check that we did when\n\t\t// we decided to delay in createRoute().\n\t\tsendDelayedInfo = didSolicit && routeShouldDelayInfo(bytesToString(c.route.accName), opts)\n\t}\n\n\t// Mark that the INFO protocol has been received, so we can detect updates.\n\tc.flags.set(infoReceived)\n\n\t// Get the route's proto version. It will be used to check if the connection\n\t// supports certain features, such as message tracing.\n\tc.opts.Protocol = info.Proto\n\n\t// Headers\n\tc.headers = supportsHeaders && info.Headers\n\n\t// Copy over important information.\n\tc.route.remoteID = info.ID\n\tc.route.authRequired = info.AuthRequired\n\tc.route.tlsRequired = info.TLSRequired\n\tc.route.gatewayURL = info.GatewayURL\n\tc.route.remoteName = info.Name\n\tc.route.lnoc = info.LNOC\n\tc.route.lnocu = info.LNOCU\n\tc.route.jetstream = info.JetStream\n\n\t// When sent through route INFO, if the field is set, it should be of size 1.\n\tif len(info.LeafNodeURLs) == 1 {\n\t\tc.route.leafnodeURL = info.LeafNodeURLs[0]\n\t}\n\t// Compute the hash of this route based on remote server name\n\tc.route.hash = getHash(info.Name)\n\t// Same with remote server ID (used for GW mapped replies routing).\n\t// Use getGWHash since we don't use the same hash len for that\n\t// for backward compatibility.\n\tc.route.idHash = string(getGWHash(info.ID))\n\n\t// Copy over permissions as well.\n\tc.opts.Import = info.Import\n\tc.opts.Export = info.Export\n\n\t// If we do not know this route's URL, construct one on the fly\n\t// from the information provided.\n\tif c.route.url == nil {\n\t\t// Add in the URL from host and port\n\t\thp := net.JoinHostPort(info.Host, strconv.Itoa(info.Port))\n\t\turl, err := url.Parse(fmt.Sprintf(\"nats-route://%s/\", hp))\n\t\tif err != nil {\n\t\t\tc.Errorf(\"Error parsing URL from INFO: %v\\n\", err)\n\t\t\tc.mu.Unlock()\n\t\t\tc.closeConnection(ParseError)\n\t\t\treturn\n\t\t}\n\t\tc.route.url = url\n\t}\n\t// The incoming INFO from the route will have IP set\n\t// if it has Cluster.Advertise. In that case, use that\n\t// otherwise construct it from the remote TCP address.\n\tif info.IP == _EMPTY_ {\n\t\t// Need to get the remote IP address.\n\t\tswitch conn := c.nc.(type) {\n\t\tcase *net.TCPConn, *tls.Conn:\n\t\t\taddr := conn.RemoteAddr().(*net.TCPAddr)\n\t\t\tinfo.IP = fmt.Sprintf(\"nats-route://%s/\", net.JoinHostPort(addr.IP.String(),\n\t\t\t\tstrconv.Itoa(info.Port)))\n\t\tdefault:\n\t\t\tinfo.IP = c.route.url.String()\n\t\t}\n\t}\n\t// For accounts that are configured to have their own route:\n\t// If this is a solicited route, we already have c.route.accName set in createRoute.\n\t// For non solicited route (the accept side), we will set the account name that\n\t// is present in the INFO protocol.\n\tif didSolicit && len(c.route.accName) > 0 {\n\t\t// Set it in the info.RouteAccount so that addRoute can use that\n\t\t// and we properly gossip that this is a route for an account.\n\t\tinfo.RouteAccount = string(c.route.accName)\n\t} else if !didSolicit && info.RouteAccount != _EMPTY_ {\n\t\tc.route.accName = []byte(info.RouteAccount)\n\t}\n\taccName := string(c.route.accName)\n\n\t// Capture the noGossip value and reset it here.\n\tgossipMode := c.route.gossipMode\n\tc.route.gossipMode = 0\n\n\t// Check to see if we have this remote already registered.\n\t// This can happen when both servers have routes to each other.\n\tc.mu.Unlock()\n\n\tif added := s.addRoute(c, didSolicit, sendDelayedInfo, gossipMode, info, accName); added {\n\t\tif accName != _EMPTY_ {\n\t\t\tc.Debugf(\"Registering remote route %q for account %q\", info.ID, accName)\n\t\t} else {\n\t\t\tc.Debugf(\"Registering remote route %q\", info.ID)\n\t\t}\n\t} else {\n\t\tc.Debugf(\"Detected duplicate remote route %q\", info.ID)\n\t\tc.closeConnection(DuplicateRoute)\n\t}\n}\n\nfunc (s *Server) negotiateRouteCompression(c *client, didSolicit bool, accName, infoCompression string, opts *Options) (bool, error) {\n\t// Negotiate the appropriate compression mode (or no compression)\n\tcm, err := selectCompressionMode(opts.Cluster.Compression.Mode, infoCompression)\n\tif err != nil {\n\t\treturn false, err\n\t}\n\tc.mu.Lock()\n\t// For \"auto\" mode, set the initial compression mode based on RTT\n\tif cm == CompressionS2Auto {\n\t\tif c.rttStart.IsZero() {\n\t\t\tc.rtt = computeRTT(c.start)\n\t\t}\n\t\tcm = selectS2AutoModeBasedOnRTT(c.rtt, opts.Cluster.Compression.RTTThresholds)\n\t}\n\t// Keep track of the negotiated compression mode.\n\tc.route.compression = cm\n\tc.mu.Unlock()\n\n\t// If we end-up doing compression...\n\tif needsCompression(cm) {\n\t\t// Generate an INFO with the chosen compression mode.\n\t\ts.mu.Lock()\n\t\tinfoProto := s.generateRouteInitialInfoJSON(accName, cm, 0, gossipDefault)\n\t\ts.mu.Unlock()\n\n\t\t// If we solicited, then send this INFO protocol BEFORE switching\n\t\t// to compression writer. However, if we did not, we send it after.\n\t\tc.mu.Lock()\n\t\tif didSolicit {\n\t\t\tc.enqueueProto(infoProto)\n\t\t\t// Make sure it is completely flushed (the pending bytes goes to\n\t\t\t// 0) before proceeding.\n\t\t\tfor c.out.pb > 0 && !c.isClosed() {\n\t\t\t\tc.flushOutbound()\n\t\t\t}\n\t\t}\n\t\t// This is to notify the readLoop that it should switch to a\n\t\t// (de)compression reader.\n\t\tc.in.flags.set(switchToCompression)\n\t\t// Create the compress writer before queueing the INFO protocol for\n\t\t// a route that did not solicit. It will make sure that that proto\n\t\t// is sent with compression on.\n\t\tc.out.cw = s2.NewWriter(nil, s2WriterOptions(cm)...)\n\t\tif !didSolicit {\n\t\t\tc.enqueueProto(infoProto)\n\t\t}\n\t\t// We can now set the ping timer.\n\t\tc.setFirstPingTimer()\n\t\tc.mu.Unlock()\n\t\treturn true, nil\n\t}\n\treturn false, nil\n}\n\n// Possibly sends local subscriptions interest to this route\n// based on changes in the remote's Export permissions.\nfunc (s *Server) updateRemoteRoutePerms(c *client, info *Info) {\n\tc.mu.Lock()\n\t// Interested only on Export permissions for the remote server.\n\t// Create \"fake\" clients that we will use to check permissions\n\t// using the old permissions...\n\toldPerms := &RoutePermissions{Export: c.opts.Export}\n\toldPermsTester := &client{}\n\toldPermsTester.setRoutePermissions(oldPerms)\n\t// and the new ones.\n\tnewPerms := &RoutePermissions{Export: info.Export}\n\tnewPermsTester := &client{}\n\tnewPermsTester.setRoutePermissions(newPerms)\n\n\tc.opts.Import = info.Import\n\tc.opts.Export = info.Export\n\n\trouteAcc, poolIdx, noPool := bytesToString(c.route.accName), c.route.poolIdx, c.route.noPool\n\tc.mu.Unlock()\n\n\tvar (\n\t\t_localSubs [4096]*subscription\n\t\t_allSubs   [4096]*subscription\n\t\tallSubs    = _allSubs[:0]\n\t)\n\n\ts.accounts.Range(func(_, v any) bool {\n\t\tacc := v.(*Account)\n\t\tacc.mu.RLock()\n\t\taccName, sl, accPoolIdx := acc.Name, acc.sl, acc.routePoolIdx\n\t\tacc.mu.RUnlock()\n\n\t\t// Do this only for accounts handled by this route\n\t\tif (accPoolIdx >= 0 && accPoolIdx == poolIdx) || (routeAcc == accName) || noPool {\n\t\t\tlocalSubs := _localSubs[:0]\n\t\t\tsl.localSubs(&localSubs, false)\n\t\t\tif len(localSubs) > 0 {\n\t\t\t\tallSubs = append(allSubs, localSubs...)\n\t\t\t}\n\t\t}\n\t\treturn true\n\t})\n\n\tif len(allSubs) == 0 {\n\t\treturn\n\t}\n\n\tc.mu.Lock()\n\tc.sendRouteSubProtos(allSubs, false, func(sub *subscription) bool {\n\t\tsubj := string(sub.subject)\n\t\t// If the remote can now export but could not before, and this server can import this\n\t\t// subject, then send SUB protocol.\n\t\tif newPermsTester.canExport(subj) && !oldPermsTester.canExport(subj) && c.canImport(subj) {\n\t\t\treturn true\n\t\t}\n\t\treturn false\n\t})\n\tc.mu.Unlock()\n}\n\n// sendAsyncInfoToClients sends an INFO protocol to all\n// connected clients that accept async INFO updates.\n// The server lock is held on entry.\nfunc (s *Server) sendAsyncInfoToClients(regCli, wsCli bool) {\n\t// If there are no clients supporting async INFO protocols, we are done.\n\t// Also don't send if we are shutting down...\n\tif s.cproto == 0 || s.isShuttingDown() {\n\t\treturn\n\t}\n\tinfo := s.copyInfo()\n\n\tfor _, c := range s.clients {\n\t\tc.mu.Lock()\n\t\t// Here, we are going to send only to the clients that are fully\n\t\t// registered (server has received CONNECT and first PING). For\n\t\t// clients that are not at this stage, this will happen in the\n\t\t// processing of the first PING (see client.processPing)\n\t\tif ((regCli && !c.isWebsocket()) || (wsCli && c.isWebsocket())) &&\n\t\t\tc.opts.Protocol >= ClientProtoInfo &&\n\t\t\tc.flags.isSet(firstPongSent) {\n\t\t\t// sendInfo takes care of checking if the connection is still\n\t\t\t// valid or not, so don't duplicate tests here.\n\t\t\tc.enqueueProto(c.generateClientInfoJSON(info))\n\t\t}\n\t\tc.mu.Unlock()\n\t}\n}\n\n// This will process implicit route information received from another server.\n// We will check to see if we have configured or are already connected,\n// and if so we will ignore. Otherwise we will attempt to connect.\nfunc (s *Server) processImplicitRoute(info *Info, routeNoPool bool) {\n\tremoteID := info.ID\n\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\n\t// Don't connect to ourself\n\tif remoteID == s.info.ID {\n\t\treturn\n\t}\n\n\t// Snapshot server options.\n\topts := s.getOpts()\n\n\t// Check if this route already exists\n\tif accName := info.RouteAccount; accName != _EMPTY_ {\n\t\t// If we don't support pooling/pinned account, bail.\n\t\tif opts.Cluster.PoolSize <= 0 {\n\t\t\treturn\n\t\t}\n\t\tif remotes, ok := s.accRoutes[accName]; ok {\n\t\t\tif r := remotes[remoteID]; r != nil {\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t} else if _, exists := s.routes[remoteID]; exists {\n\t\treturn\n\t}\n\t// Check if we have this route as a configured route\n\tif s.hasThisRouteConfigured(info) {\n\t\treturn\n\t}\n\n\t// Initiate the connection, using info.IP instead of info.URL here...\n\tr, err := url.Parse(info.IP)\n\tif err != nil {\n\t\ts.Errorf(\"Error parsing URL from INFO: %v\\n\", err)\n\t\treturn\n\t}\n\n\tif info.AuthRequired {\n\t\tr.User = url.UserPassword(opts.Cluster.Username, opts.Cluster.Password)\n\t}\n\ts.startGoRoutine(func() { s.connectToRoute(r, Implicit, true, info.GossipMode, info.RouteAccount) })\n\t// If we are processing an implicit route from a route that does not\n\t// support pooling/pinned-accounts, we won't receive an INFO for each of\n\t// the pinned-accounts that we would normally receive. In that case, just\n\t// initiate routes for all our configured pinned accounts.\n\tif routeNoPool && info.RouteAccount == _EMPTY_ && len(opts.Cluster.PinnedAccounts) > 0 {\n\t\t// Copy since we are going to pass as closure to a go routine.\n\t\trURL := r\n\t\tfor _, an := range opts.Cluster.PinnedAccounts {\n\t\t\taccName := an\n\t\t\ts.startGoRoutine(func() { s.connectToRoute(rURL, Implicit, true, info.GossipMode, accName) })\n\t\t}\n\t}\n}\n\n// hasThisRouteConfigured returns true if info.Host:info.Port is present\n// in the server's opts.Routes, false otherwise.\n// Server lock is assumed to be held by caller.\nfunc (s *Server) hasThisRouteConfigured(info *Info) bool {\n\troutes := s.getOpts().Routes\n\tif len(routes) == 0 {\n\t\treturn false\n\t}\n\t// This could possibly be a 0.0.0.0 host so we will also construct a second\n\t// url with the host section of the `info.IP` (if present).\n\tsPort := strconv.Itoa(info.Port)\n\turlOne := strings.ToLower(net.JoinHostPort(info.Host, sPort))\n\tvar urlTwo string\n\tif info.IP != _EMPTY_ {\n\t\tif u, _ := url.Parse(info.IP); u != nil {\n\t\t\turlTwo = strings.ToLower(net.JoinHostPort(u.Hostname(), sPort))\n\t\t\t// Ignore if same than the first\n\t\t\tif urlTwo == urlOne {\n\t\t\t\turlTwo = _EMPTY_\n\t\t\t}\n\t\t}\n\t}\n\tfor _, ri := range routes {\n\t\trHost := strings.ToLower(ri.Host)\n\t\tif rHost == urlOne {\n\t\t\treturn true\n\t\t}\n\t\tif urlTwo != _EMPTY_ && rHost == urlTwo {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\n// forwardNewRouteInfoToKnownServers possibly sends the INFO protocol of the\n// new route to all routes known by this server. In turn, each server will\n// contact this new route.\n// Server lock held on entry.\nfunc (s *Server) forwardNewRouteInfoToKnownServers(info *Info, rtype RouteType, didSolicit bool, localGossipMode byte) {\n\t// Determine if this connection is resulting from a gossip notification.\n\tfromGossip := didSolicit && rtype == Implicit\n\t// If from gossip (but we are not overriding it) or if the remote disabled gossip, bail out.\n\tif (fromGossip && localGossipMode != gossipOverride) || info.GossipMode == gossipDisabled {\n\t\treturn\n\t}\n\n\t// Note: nonce is not used in routes.\n\t// That being said, the info we get is the initial INFO which\n\t// contains a nonce, but we now forward this to existing routes,\n\t// so clear it now.\n\tinfo.Nonce = _EMPTY_\n\n\tvar (\n\t\tinfoGMDefault  []byte\n\t\tinfoGMDisabled []byte\n\t\tinfoGMOverride []byte\n\t)\n\n\tgenerateJSON := func(gm byte) []byte {\n\t\tinfo.GossipMode = gm\n\t\tb, _ := json.Marshal(info)\n\t\treturn []byte(fmt.Sprintf(InfoProto, b))\n\t}\n\n\tgetJSON := func(r *client) []byte {\n\t\tif (!didSolicit && r.route.routeType == Explicit) || (didSolicit && rtype == Explicit) {\n\t\t\tif infoGMOverride == nil {\n\t\t\t\tinfoGMOverride = generateJSON(gossipOverride)\n\t\t\t}\n\t\t\treturn infoGMOverride\n\t\t} else if !didSolicit {\n\t\t\tif infoGMDisabled == nil {\n\t\t\t\tinfoGMDisabled = generateJSON(gossipDisabled)\n\t\t\t}\n\t\t\treturn infoGMDisabled\n\t\t}\n\t\tif infoGMDefault == nil {\n\t\t\tinfoGMDefault = generateJSON(0)\n\t\t}\n\t\treturn infoGMDefault\n\t}\n\n\tvar accRemotes map[string]*client\n\tpinnedAccount := info.RouteAccount != _EMPTY_\n\t// If this is for a pinned account, we will try to send the gossip\n\t// through our pinned account routes, but fall back to the other\n\t// routes in case we don't have one for a given remote.\n\tif pinnedAccount {\n\t\tvar ok bool\n\t\tif accRemotes, ok = s.accRoutes[info.RouteAccount]; ok {\n\t\t\tfor remoteID, r := range accRemotes {\n\t\t\t\tif r == nil {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tr.mu.Lock()\n\t\t\t\t// Do not send to a remote that does not support pooling/pinned-accounts.\n\t\t\t\tif remoteID != info.ID && !r.route.noPool {\n\t\t\t\t\tr.enqueueProto(getJSON(r))\n\t\t\t\t}\n\t\t\t\tr.mu.Unlock()\n\t\t\t}\n\t\t}\n\t}\n\n\ts.forEachRemote(func(r *client) {\n\t\tr.mu.Lock()\n\t\tremoteID := r.route.remoteID\n\t\tif pinnedAccount {\n\t\t\tif _, processed := accRemotes[remoteID]; processed {\n\t\t\t\tr.mu.Unlock()\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t\t// If this is a new route for a given account, do not send to a server\n\t\t// that does not support pooling/pinned-accounts.\n\t\tif remoteID != info.ID && (!pinnedAccount || !r.route.noPool) {\n\t\t\tr.enqueueProto(getJSON(r))\n\t\t}\n\t\tr.mu.Unlock()\n\t})\n}\n\n// canImport is whether or not we will send a SUB for interest to the other side.\n// This is for ROUTER connections only.\n// Lock is held on entry.\nfunc (c *client) canImport(subject string) bool {\n\t// Use pubAllowed() since this checks Publish permissions which\n\t// is what Import maps to.\n\treturn c.pubAllowedFullCheck(subject, false, true)\n}\n\n// canExport is whether or not we will accept a SUB from the remote for a given subject.\n// This is for ROUTER connections only.\n// Lock is held on entry\nfunc (c *client) canExport(subject string) bool {\n\t// Use canSubscribe() since this checks Subscribe permissions which\n\t// is what Export maps to.\n\treturn c.canSubscribe(subject)\n}\n\n// Initialize or reset cluster's permissions.\n// This is for ROUTER connections only.\n// Client lock is held on entry\nfunc (c *client) setRoutePermissions(perms *RoutePermissions) {\n\t// Reset if some were set\n\tif perms == nil {\n\t\tc.perms = nil\n\t\tc.mperms = nil\n\t\treturn\n\t}\n\t// Convert route permissions to user permissions.\n\t// The Import permission is mapped to Publish\n\t// and Export permission is mapped to Subscribe.\n\t// For meaning of Import/Export, see canImport and canExport.\n\tp := &Permissions{\n\t\tPublish:   perms.Import,\n\t\tSubscribe: perms.Export,\n\t}\n\tc.setPermissions(p)\n}\n\n// Type used to hold a list of subs on a per account basis.\ntype asubs struct {\n\tacc  *Account\n\tsubs []*subscription\n}\n\n// Returns the account name from the subscription's key.\n// This is invoked knowing that the key contains an account name, so for a sub\n// that is not from a pinned-account route.\n// The `keyHasSubType` boolean indicates that the key starts with the indicator\n// for leaf or regular routed subscriptions.\nfunc getAccNameFromRoutedSubKey(sub *subscription, key string, keyHasSubType bool) string {\n\tvar accIdx int\n\tif keyHasSubType {\n\t\t// Start after the sub type indicator.\n\t\taccIdx = 1\n\t\t// But if there is an origin, bump its index.\n\t\tif len(sub.origin) > 0 {\n\t\t\taccIdx = 2\n\t\t}\n\t}\n\treturn strings.Fields(key)[accIdx]\n}\n\n// Returns if the route is dedicated to an account, its name, and a boolean\n// that indicates if this route uses the routed subscription indicator at\n// the beginning of the subscription key.\n// Lock held on entry.\nfunc (c *client) getRoutedSubKeyInfo() (bool, string, bool) {\n\tvar accName string\n\tif an := c.route.accName; len(an) > 0 {\n\t\taccName = string(an)\n\t}\n\treturn accName != _EMPTY_, accName, c.route.lnocu\n}\n\n// removeRemoteSubs will walk the subs and remove them from the appropriate account.\nfunc (c *client) removeRemoteSubs() {\n\t// We need to gather these on a per account basis.\n\t// FIXME(dlc) - We should be smarter about this..\n\tas := map[string]*asubs{}\n\tc.mu.Lock()\n\tsrv := c.srv\n\tsubs := c.subs\n\tc.subs = nil\n\tpa, accountName, hasSubType := c.getRoutedSubKeyInfo()\n\tc.mu.Unlock()\n\n\tfor key, sub := range subs {\n\t\tc.mu.Lock()\n\t\tsub.max = 0\n\t\tc.mu.Unlock()\n\t\t// If not a pinned-account route, we need to find the account\n\t\t// name from the sub's key.\n\t\tif !pa {\n\t\t\taccountName = getAccNameFromRoutedSubKey(sub, key, hasSubType)\n\t\t}\n\t\tase := as[accountName]\n\t\tif ase == nil {\n\t\t\tif v, ok := srv.accounts.Load(accountName); ok {\n\t\t\t\tase = &asubs{acc: v.(*Account), subs: []*subscription{sub}}\n\t\t\t\tas[accountName] = ase\n\t\t\t} else {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t} else {\n\t\t\tase.subs = append(ase.subs, sub)\n\t\t}\n\t\tdelta := int32(1)\n\t\tif len(sub.queue) > 0 {\n\t\t\tdelta = sub.qw\n\t\t}\n\t\tif srv.gateway.enabled {\n\t\t\tsrv.gatewayUpdateSubInterest(accountName, sub, -delta)\n\t\t}\n\t\tase.acc.updateLeafNodes(sub, -delta)\n\t}\n\n\t// Now remove the subs by batch for each account sublist.\n\tfor _, ase := range as {\n\t\tc.Debugf(\"Removing %d subscriptions for account %q\", len(ase.subs), ase.acc.Name)\n\t\tase.acc.mu.Lock()\n\t\tase.acc.sl.RemoveBatch(ase.subs)\n\t\tase.acc.mu.Unlock()\n\t}\n}\n\n// Removes (and returns) the subscriptions from this route's subscriptions map\n// that belong to the given account.\n// Lock is held on entry\nfunc (c *client) removeRemoteSubsForAcc(name string) []*subscription {\n\tvar subs []*subscription\n\t_, _, hasSubType := c.getRoutedSubKeyInfo()\n\tfor key, sub := range c.subs {\n\t\tan := getAccNameFromRoutedSubKey(sub, key, hasSubType)\n\t\tif an == name {\n\t\t\tsub.max = 0\n\t\t\tsubs = append(subs, sub)\n\t\t\tdelete(c.subs, key)\n\t\t}\n\t}\n\treturn subs\n}\n\nfunc (c *client) parseUnsubProto(arg []byte, accInProto, hasOrigin bool) ([]byte, string, []byte, []byte, error) {\n\t// Indicate any activity, so pub and sub or unsubs.\n\tc.in.subs++\n\n\targs := splitArg(arg)\n\n\tvar (\n\t\torigin      []byte\n\t\taccountName string\n\t\tqueue       []byte\n\t\tsubjIdx     int\n\t)\n\t// If `hasOrigin` is true, then it means this is a LS- with origin in proto.\n\tif hasOrigin {\n\t\t// We would not be here if there was not at least 1 field.\n\t\torigin = args[0]\n\t\tsubjIdx = 1\n\t}\n\t// If there is an account in the protocol, bump the subject index.\n\tif accInProto {\n\t\tsubjIdx++\n\t}\n\n\tswitch len(args) {\n\tcase subjIdx + 1:\n\tcase subjIdx + 2:\n\t\tqueue = args[subjIdx+1]\n\tdefault:\n\t\treturn nil, _EMPTY_, nil, nil, fmt.Errorf(\"parse error: '%s'\", arg)\n\t}\n\tif accInProto {\n\t\t// If there is an account in the protocol, it is before the subject.\n\t\taccountName = string(args[subjIdx-1])\n\t}\n\treturn origin, accountName, args[subjIdx], queue, nil\n}\n\n// Indicates no more interest in the given account/subject for the remote side.\nfunc (c *client) processRemoteUnsub(arg []byte, leafUnsub bool) (err error) {\n\tsrv := c.srv\n\tif srv == nil {\n\t\treturn nil\n\t}\n\n\tvar accountName string\n\t// Assume the account will be in the protocol.\n\taccInProto := true\n\n\tc.mu.Lock()\n\toriginSupport := c.route.lnocu\n\tif c.route != nil && len(c.route.accName) > 0 {\n\t\taccountName, accInProto = string(c.route.accName), false\n\t}\n\tc.mu.Unlock()\n\n\thasOrigin := leafUnsub && originSupport\n\t_, accNameFromProto, subject, _, err := c.parseUnsubProto(arg, accInProto, hasOrigin)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"processRemoteUnsub %s\", err.Error())\n\t}\n\tif accInProto {\n\t\taccountName = accNameFromProto\n\t}\n\t// Lookup the account\n\tvar acc *Account\n\tif v, ok := srv.accounts.Load(accountName); ok {\n\t\tacc = v.(*Account)\n\t} else {\n\t\tc.Debugf(\"Unknown account %q for subject %q\", accountName, subject)\n\t\treturn nil\n\t}\n\n\tc.mu.Lock()\n\tif c.isClosed() {\n\t\tc.mu.Unlock()\n\t\treturn nil\n\t}\n\n\t_keya := [128]byte{}\n\t_key := _keya[:0]\n\n\tvar key string\n\tif !originSupport {\n\t\t// If it is an LS- or RS-, we use the protocol as-is as the key.\n\t\tkey = bytesToString(arg)\n\t} else {\n\t\t// We need to prefix with the sub type.\n\t\tif leafUnsub {\n\t\t\t_key = append(_key, keyRoutedLeafSubByte)\n\t\t} else {\n\t\t\t_key = append(_key, keyRoutedSubByte)\n\t\t}\n\t\t_key = append(_key, ' ')\n\t\t_key = append(_key, arg...)\n\t\tkey = bytesToString(_key)\n\t}\n\tdelta := int32(1)\n\tsub, ok := c.subs[key]\n\tif ok {\n\t\tdelete(c.subs, key)\n\t\tacc.sl.Remove(sub)\n\t\tif len(sub.queue) > 0 {\n\t\t\tdelta = sub.qw\n\t\t}\n\t}\n\tc.mu.Unlock()\n\n\t// Update gateways and leaf nodes only if the subscription was found.\n\tif ok {\n\t\tif srv.gateway.enabled {\n\t\t\tsrv.gatewayUpdateSubInterest(accountName, sub, -delta)\n\t\t}\n\n\t\t// Now check on leafnode updates.\n\t\tacc.updateLeafNodes(sub, -delta)\n\t}\n\n\tif c.opts.Verbose {\n\t\tc.sendOK()\n\t}\n\treturn nil\n}\n\nfunc (c *client) processRemoteSub(argo []byte, hasOrigin bool) (err error) {\n\t// Indicate activity.\n\tc.in.subs++\n\n\tsrv := c.srv\n\tif srv == nil {\n\t\treturn nil\n\t}\n\n\t// We copy `argo` to not reference the read buffer. However, we will\n\t// prefix with a code that says if the remote sub is for a leaf\n\t// (hasOrigin == true) or not to prevent key collisions. Imagine:\n\t// \"RS+ foo bar baz 1\\r\\n\" => \"foo bar baz\" (a routed queue sub)\n\t// \"LS+ foo bar baz\\r\\n\"   => \"foo bar baz\" (a route leaf sub on \"baz\",\n\t// for account \"bar\" with origin \"foo\").\n\t//\n\t// The sub.sid/key will be set respectively to \"R foo bar baz\" and\n\t// \"L foo bar baz\".\n\t//\n\t// We also no longer add the account if it was not present (due to\n\t// pinned-account route) since there is no need really.\n\t//\n\t// For routes to older server, we will still create the \"arg\" with\n\t// the above layout, but we will create the sub.sid/key as before,\n\t// that is, not including the origin for LS+ because older server\n\t// only send LS- without origin, so we would not be able to find\n\t// the sub in the map.\n\tc.mu.Lock()\n\taccountName := string(c.route.accName)\n\toldStyle := !c.route.lnocu\n\tc.mu.Unlock()\n\n\t// Indicate if the account name should be in the protocol. It would be the\n\t// case if accountName is empty.\n\taccInProto := accountName == _EMPTY_\n\n\t// Copy so we do not reference a potentially large buffer.\n\t// Add 2 more bytes for the routed sub type.\n\targ := make([]byte, 0, 2+len(argo))\n\tif hasOrigin {\n\t\targ = append(arg, keyRoutedLeafSubByte)\n\t} else {\n\t\targ = append(arg, keyRoutedSubByte)\n\t}\n\targ = append(arg, ' ')\n\targ = append(arg, argo...)\n\n\t// Now split to get all fields. Unroll splitArgs to avoid runtime/heap issues.\n\ta := [MAX_RSUB_ARGS][]byte{}\n\targs := a[:0]\n\tstart := -1\n\tfor i, b := range arg {\n\t\tswitch b {\n\t\tcase ' ', '\\t', '\\r', '\\n':\n\t\t\tif start >= 0 {\n\t\t\t\targs = append(args, arg[start:i])\n\t\t\t\tstart = -1\n\t\t\t}\n\t\tdefault:\n\t\t\tif start < 0 {\n\t\t\t\tstart = i\n\t\t\t}\n\t\t}\n\t}\n\tif start >= 0 {\n\t\targs = append(args, arg[start:])\n\t}\n\n\tdelta := int32(1)\n\tsub := &subscription{client: c}\n\n\t// There will always be at least a subject, but its location will depend\n\t// on if there is an origin, an account name, etc.. Since we know that\n\t// we have added the sub type indicator as the first field, the subject\n\t// position will be at minimum at index 1.\n\tsubjIdx := 1\n\tif hasOrigin {\n\t\tsubjIdx++\n\t}\n\tif accInProto {\n\t\tsubjIdx++\n\t}\n\tswitch len(args) {\n\tcase subjIdx + 1:\n\t\tsub.queue = nil\n\tcase subjIdx + 3:\n\t\tsub.queue = args[subjIdx+1]\n\t\tsub.qw = int32(parseSize(args[subjIdx+2]))\n\t\t// TODO: (ik) We should have a non empty queue name and a queue\n\t\t// weight >= 1. For 2.11, we may want to return an error if that\n\t\t// is not the case, but for now just overwrite `delta` if queue\n\t\t// weight is greater than 1 (it is possible after a reconnect/\n\t\t// server restart to receive a queue weight > 1 for a new sub).\n\t\tif sub.qw > 1 {\n\t\t\tdelta = sub.qw\n\t\t}\n\tdefault:\n\t\treturn fmt.Errorf(\"processRemoteSub Parse Error: '%s'\", arg)\n\t}\n\t// We know that the number of fields is correct. So we can access args[] based\n\t// on where we expect the fields to be.\n\n\t// If there is an origin, it will be at index 1.\n\tif hasOrigin {\n\t\tsub.origin = args[1]\n\t}\n\t// For subject, use subjIdx.\n\tsub.subject = args[subjIdx]\n\t// If the account name is in the protocol, it will be before the subject.\n\tif accInProto {\n\t\taccountName = bytesToString(args[subjIdx-1])\n\t}\n\t// Now set the sub.sid from the arg slice. However, we will have a different\n\t// one if we use the origin or not.\n\tstart = 0\n\tend := len(arg)\n\tif sub.queue != nil {\n\t\t// Remove the ' <weight>' from the arg length.\n\t\tend -= 1 + len(args[subjIdx+2])\n\t}\n\tif oldStyle {\n\t\t// We will start at the account (if present) or at the subject.\n\t\t// We first skip the \"R \" or \"L \"\n\t\tstart = 2\n\t\t// And if there is an origin skip that.\n\t\tif hasOrigin {\n\t\t\tstart += len(sub.origin) + 1\n\t\t}\n\t\t// Here we are pointing at the account (if present), or at the subject.\n\t}\n\tsub.sid = arg[start:end]\n\n\t// Lookup account while avoiding fetch.\n\t// A slow fetch delays subsequent remote messages. It also avoids the expired check (see below).\n\t// With all but memory resolver lookup can be delayed or fail.\n\t// It is also possible that the account can't be resolved yet.\n\t// This does not apply to the memory resolver.\n\t// When used, perform the fetch.\n\tstaticResolver := true\n\tif res := srv.AccountResolver(); res != nil {\n\t\tif _, ok := res.(*MemAccResolver); !ok {\n\t\t\tstaticResolver = false\n\t\t}\n\t}\n\tvar acc *Account\n\tif staticResolver {\n\t\tacc, _ = srv.LookupAccount(accountName)\n\t} else if v, ok := srv.accounts.Load(accountName); ok {\n\t\tacc = v.(*Account)\n\t}\n\tif acc == nil {\n\t\t// if the option of retrieving accounts later exists, create an expired one.\n\t\t// When a client comes along, expiration will prevent it from being used,\n\t\t// cause a fetch and update the account to what is should be.\n\t\tif staticResolver {\n\t\t\tc.Errorf(\"Unknown account %q for remote subject %q\", accountName, sub.subject)\n\t\t\treturn\n\t\t}\n\t\tc.Debugf(\"Unknown account %q for remote subject %q\", accountName, sub.subject)\n\n\t\tvar isNew bool\n\t\tif acc, isNew = srv.LookupOrRegisterAccount(accountName); isNew {\n\t\t\tacc.mu.Lock()\n\t\t\tacc.expired.Store(true)\n\t\t\tacc.incomplete = true\n\t\t\tacc.mu.Unlock()\n\t\t}\n\t}\n\n\tc.mu.Lock()\n\tif c.isClosed() {\n\t\tc.mu.Unlock()\n\t\treturn nil\n\t}\n\n\t// Check permissions if applicable.\n\tif c.perms != nil && !c.canExport(string(sub.subject)) {\n\t\tc.mu.Unlock()\n\t\tc.Debugf(\"Can not export %q, ignoring remote subscription request\", sub.subject)\n\t\treturn nil\n\t}\n\n\t// Check if we have a maximum on the number of subscriptions.\n\tif c.subsAtLimit() {\n\t\tc.mu.Unlock()\n\t\tc.maxSubsExceeded()\n\t\treturn nil\n\t}\n\n\tacc.mu.RLock()\n\t// For routes (this can be called by leafnodes), check if the account is\n\t// transitioning (from pool to dedicated route) and this route is not a\n\t// per-account route (route.poolIdx >= 0). If so, ignore this subscription.\n\t// Exclude \"no pool\" routes from this check.\n\tif c.kind == ROUTER && !c.route.noPool &&\n\t\tacc.routePoolIdx == accTransitioningToDedicatedRoute && c.route.poolIdx >= 0 {\n\t\tacc.mu.RUnlock()\n\t\tc.mu.Unlock()\n\t\t// Do not return an error, which would cause the connection to be closed.\n\t\treturn nil\n\t}\n\tsl := acc.sl\n\tacc.mu.RUnlock()\n\n\t// We use the sub.sid for the key of the c.subs map.\n\tkey := bytesToString(sub.sid)\n\tosub := c.subs[key]\n\tif osub == nil {\n\t\tc.subs[key] = sub\n\t\t// Now place into the account sl.\n\t\tif err = sl.Insert(sub); err != nil {\n\t\t\tdelete(c.subs, key)\n\t\t\tc.mu.Unlock()\n\t\t\tc.Errorf(\"Could not insert subscription: %v\", err)\n\t\t\tc.sendErr(\"Invalid Subscription\")\n\t\t\treturn nil\n\t\t}\n\t} else if sub.queue != nil {\n\t\t// For a queue we need to update the weight.\n\t\tdelta = sub.qw - atomic.LoadInt32(&osub.qw)\n\t\tatomic.StoreInt32(&osub.qw, sub.qw)\n\t\tsl.UpdateRemoteQSub(osub)\n\t}\n\tc.mu.Unlock()\n\n\tif srv.gateway.enabled {\n\t\tsrv.gatewayUpdateSubInterest(acc.Name, sub, delta)\n\t}\n\n\t// Now check on leafnode updates.\n\tacc.updateLeafNodes(sub, delta)\n\n\tif c.opts.Verbose {\n\t\tc.sendOK()\n\t}\n\n\treturn nil\n}\n\n// Lock is held on entry\nfunc (c *client) addRouteSubOrUnsubProtoToBuf(buf []byte, accName string, sub *subscription, isSubProto bool) []byte {\n\t// If we have an origin cluster and the other side supports leafnode origin clusters\n\t// send an LS+/LS- version instead.\n\tif len(sub.origin) > 0 && c.route.lnoc {\n\t\tif isSubProto {\n\t\t\tbuf = append(buf, lSubBytes...)\n\t\t\tbuf = append(buf, sub.origin...)\n\t\t\tbuf = append(buf, ' ')\n\t\t} else {\n\t\t\tbuf = append(buf, lUnsubBytes...)\n\t\t\tif c.route.lnocu {\n\t\t\t\tbuf = append(buf, sub.origin...)\n\t\t\t\tbuf = append(buf, ' ')\n\t\t\t}\n\t\t}\n\t} else {\n\t\tif isSubProto {\n\t\t\tbuf = append(buf, rSubBytes...)\n\t\t} else {\n\t\t\tbuf = append(buf, rUnsubBytes...)\n\t\t}\n\t}\n\tif len(c.route.accName) == 0 {\n\t\tbuf = append(buf, accName...)\n\t\tbuf = append(buf, ' ')\n\t}\n\tbuf = append(buf, sub.subject...)\n\tif len(sub.queue) > 0 {\n\t\tbuf = append(buf, ' ')\n\t\tbuf = append(buf, sub.queue...)\n\t\t// Send our weight if we are a sub proto\n\t\tif isSubProto {\n\t\t\tbuf = append(buf, ' ')\n\t\t\tvar b [12]byte\n\t\t\tvar i = len(b)\n\t\t\tfor l := sub.qw; l > 0; l /= 10 {\n\t\t\t\ti--\n\t\t\t\tb[i] = digits[l%10]\n\t\t\t}\n\t\t\tbuf = append(buf, b[i:]...)\n\t\t}\n\t}\n\tbuf = append(buf, CR_LF...)\n\treturn buf\n}\n\n// sendSubsToRoute will send over our subject interest to\n// the remote side. For each account we will send the\n// complete interest for all subjects, both normal as a binary\n// and queue group weights.\n//\n// Server lock held on entry.\nfunc (s *Server) sendSubsToRoute(route *client, idx int, account string) {\n\tvar noPool bool\n\tif idx >= 0 {\n\t\t// We need to check if this route is \"no_pool\" in which case we\n\t\t// need to select all accounts.\n\t\troute.mu.Lock()\n\t\tnoPool = route.route.noPool\n\t\troute.mu.Unlock()\n\t}\n\t// Estimated size of all protocols. It does not have to be accurate at all.\n\tvar eSize int\n\testimateProtosSize := func(a *Account, addAccountName bool) {\n\t\tif ns := len(a.rm); ns > 0 {\n\t\t\tvar accSize int\n\t\t\tif addAccountName {\n\t\t\t\taccSize = len(a.Name) + 1\n\t\t\t}\n\t\t\t// Proto looks like: \"RS+ [<account name> ]<subject>[ <queue> <weight>]\\r\\n\"\n\t\t\teSize += ns * (len(rSubBytes) + 1 + accSize)\n\t\t\tfor key := range a.rm {\n\t\t\t\t// Key contains \"<subject>[ <queue>]\"\n\t\t\t\teSize += len(key)\n\t\t\t\t// In case this is a queue, just add some bytes for the queue weight.\n\t\t\t\t// If we want to be accurate, would have to check if \"key\" has a space,\n\t\t\t\t// if so, then figure out how many bytes we need to represent the weight.\n\t\t\t\teSize += 5\n\t\t\t}\n\t\t}\n\t}\n\t// Send over our account subscriptions.\n\taccs := make([]*Account, 0, 1024)\n\tif idx < 0 || account != _EMPTY_ {\n\t\tif ai, ok := s.accounts.Load(account); ok {\n\t\t\ta := ai.(*Account)\n\t\t\ta.mu.RLock()\n\t\t\t// Estimate size and add account name in protocol if idx is not -1\n\t\t\testimateProtosSize(a, idx >= 0)\n\t\t\taccs = append(accs, a)\n\t\t\ta.mu.RUnlock()\n\t\t}\n\t} else {\n\t\ts.accounts.Range(func(k, v any) bool {\n\t\t\ta := v.(*Account)\n\t\t\ta.mu.RLock()\n\t\t\t// We are here for regular or pooled routes (not per-account).\n\t\t\t// So we collect all accounts whose routePoolIdx matches the\n\t\t\t// one for this route, or only the account provided, or all\n\t\t\t// accounts if dealing with a \"no pool\" route.\n\t\t\tif a.routePoolIdx == idx || noPool {\n\t\t\t\testimateProtosSize(a, true)\n\t\t\t\taccs = append(accs, a)\n\t\t\t}\n\t\t\ta.mu.RUnlock()\n\t\t\treturn true\n\t\t})\n\t}\n\n\tbuf := make([]byte, 0, eSize)\n\n\troute.mu.Lock()\n\tfor _, a := range accs {\n\t\ta.mu.RLock()\n\t\tfor key, n := range a.rm {\n\t\t\tvar origin, qn []byte\n\t\t\ts := strings.Fields(key)\n\t\t\t// Subject will always be the second field (index 1).\n\t\t\tsubj := stringToBytes(s[1])\n\t\t\t// Check if the key is for a leaf (will be field 0).\n\t\t\tforLeaf := s[0] == keyRoutedLeafSub\n\t\t\t// For queue, if not for a leaf, we need 3 fields \"R foo bar\",\n\t\t\t// but if for a leaf, we need 4 fields \"L foo bar leaf_origin\".\n\t\t\tif l := len(s); (!forLeaf && l == 3) || (forLeaf && l == 4) {\n\t\t\t\tqn = stringToBytes(s[2])\n\t\t\t}\n\t\t\tif forLeaf {\n\t\t\t\t// The leaf origin will be the last field.\n\t\t\t\torigin = stringToBytes(s[len(s)-1])\n\t\t\t}\n\t\t\t// s[1] is the subject and already as a string, so use that\n\t\t\t// instead of converting back `subj` to a string.\n\t\t\tif !route.canImport(s[1]) {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tsub := subscription{origin: origin, subject: subj, queue: qn, qw: n}\n\t\t\tbuf = route.addRouteSubOrUnsubProtoToBuf(buf, a.Name, &sub, true)\n\t\t}\n\t\ta.mu.RUnlock()\n\t}\n\tif len(buf) > 0 {\n\t\troute.enqueueProto(buf)\n\t\troute.Debugf(\"Sent local subscriptions to route\")\n\t}\n\troute.mu.Unlock()\n}\n\n// Sends SUBs protocols for the given subscriptions. If a filter is specified, it is\n// invoked for each subscription. If the filter returns false, the subscription is skipped.\n// This function may release the route's lock due to flushing of outbound data. A boolean\n// is returned to indicate if the connection has been closed during this call.\n// Lock is held on entry.\nfunc (c *client) sendRouteSubProtos(subs []*subscription, trace bool, filter func(sub *subscription) bool) {\n\tc.sendRouteSubOrUnSubProtos(subs, true, trace, filter)\n}\n\n// Sends UNSUBs protocols for the given subscriptions. If a filter is specified, it is\n// invoked for each subscription. If the filter returns false, the subscription is skipped.\n// This function may release the route's lock due to flushing of outbound data. A boolean\n// is returned to indicate if the connection has been closed during this call.\n// Lock is held on entry.\nfunc (c *client) sendRouteUnSubProtos(subs []*subscription, trace bool, filter func(sub *subscription) bool) {\n\tc.sendRouteSubOrUnSubProtos(subs, false, trace, filter)\n}\n\n// Low-level function that sends RS+ or RS- protocols for the given subscriptions.\n// This can now also send LS+ and LS- for origin cluster based leafnode subscriptions for cluster no-echo.\n// Use sendRouteSubProtos or sendRouteUnSubProtos instead for clarity.\n// Lock is held on entry.\nfunc (c *client) sendRouteSubOrUnSubProtos(subs []*subscription, isSubProto, trace bool, filter func(sub *subscription) bool) {\n\tvar (\n\t\t_buf [1024]byte\n\t\tbuf  = _buf[:0]\n\t)\n\n\tfor _, sub := range subs {\n\t\tif filter != nil && !filter(sub) {\n\t\t\tcontinue\n\t\t}\n\t\t// Determine the account. If sub has an ImportMap entry, use that, otherwise scoped to\n\t\t// client. Default to global if all else fails.\n\t\tvar accName string\n\t\tif sub.client != nil && sub.client != c {\n\t\t\tsub.client.mu.Lock()\n\t\t}\n\t\tif sub.im != nil {\n\t\t\taccName = sub.im.acc.Name\n\t\t} else if sub.client != nil && sub.client.acc != nil {\n\t\t\taccName = sub.client.acc.Name\n\t\t} else {\n\t\t\tc.Debugf(\"Falling back to default account for sending subs\")\n\t\t\taccName = globalAccountName\n\t\t}\n\t\tif sub.client != nil && sub.client != c {\n\t\t\tsub.client.mu.Unlock()\n\t\t}\n\n\t\tas := len(buf)\n\t\tbuf = c.addRouteSubOrUnsubProtoToBuf(buf, accName, sub, isSubProto)\n\t\tif trace {\n\t\t\tc.traceOutOp(\"\", buf[as:len(buf)-LEN_CR_LF])\n\t\t}\n\t}\n\tc.enqueueProto(buf)\n}\n\nfunc (s *Server) createRoute(conn net.Conn, rURL *url.URL, rtype RouteType, gossipMode byte, accName string) *client {\n\t// Snapshot server options.\n\topts := s.getOpts()\n\n\tdidSolicit := rURL != nil\n\tr := &route{routeType: rtype, didSolicit: didSolicit, poolIdx: -1, gossipMode: gossipMode}\n\n\tc := &client{srv: s, nc: conn, opts: ClientOpts{}, kind: ROUTER, msubs: -1, mpay: -1, route: r, start: time.Now()}\n\n\t// Is the server configured for compression?\n\tcompressionConfigured := needsCompression(opts.Cluster.Compression.Mode)\n\n\tvar infoJSON []byte\n\t// Grab server variables and generates route INFO Json. Note that we set\n\t// and reset some of s.routeInfo fields when that happens, so we need\n\t// the server write lock.\n\ts.mu.Lock()\n\t// If we are creating a pooled connection and this is the server soliciting\n\t// the connection, we will delay sending the INFO after we have processed\n\t// the incoming INFO from the remote. Also delay if configured for compression.\n\tdelayInfo := didSolicit && (compressionConfigured || routeShouldDelayInfo(accName, opts))\n\tif !delayInfo {\n\t\tinfoJSON = s.generateRouteInitialInfoJSON(accName, opts.Cluster.Compression.Mode, 0, gossipMode)\n\t}\n\tauthRequired := s.routeInfo.AuthRequired\n\ttlsRequired := s.routeInfo.TLSRequired\n\tclusterName := s.info.Cluster\n\ttlsName := s.routeTLSName\n\ts.mu.Unlock()\n\n\t// Grab lock\n\tc.mu.Lock()\n\n\t// Initialize\n\tc.initClient()\n\n\tif didSolicit {\n\t\t// Do this before the TLS code, otherwise, in case of failure\n\t\t// and if route is explicit, it would try to reconnect to 'nil'...\n\t\tr.url = rURL\n\t\tr.accName = []byte(accName)\n\t} else {\n\t\tc.flags.set(expectConnect)\n\t}\n\n\t// Check for TLS\n\tif tlsRequired {\n\t\ttlsConfig := opts.Cluster.TLSConfig\n\t\tif didSolicit {\n\t\t\t// Copy off the config to add in ServerName if we need to.\n\t\t\ttlsConfig = tlsConfig.Clone()\n\t\t}\n\t\t// Perform (server or client side) TLS handshake.\n\t\tif resetTLSName, err := c.doTLSHandshake(\"route\", didSolicit, rURL, tlsConfig, tlsName, opts.Cluster.TLSTimeout, opts.Cluster.TLSPinnedCerts); err != nil {\n\t\t\tc.mu.Unlock()\n\t\t\tif resetTLSName {\n\t\t\t\ts.mu.Lock()\n\t\t\t\ts.routeTLSName = _EMPTY_\n\t\t\t\ts.mu.Unlock()\n\t\t\t}\n\t\t\treturn nil\n\t\t}\n\t}\n\n\t// Do final client initialization\n\n\t// Initialize the per-account cache.\n\tc.in.pacache = make(map[string]*perAccountCache)\n\tif didSolicit {\n\t\t// Set permissions associated with the route user (if applicable).\n\t\t// No lock needed since we are already under client lock.\n\t\tc.setRoutePermissions(opts.Cluster.Permissions)\n\t}\n\n\t// We can't safely send the pings until we have negotiated compression\n\t// with the remote, but we want to protect against a connection that\n\t// does not perform the handshake. We will start a timer that will close\n\t// the connection as stale based on the ping interval and max out values,\n\t// but without actually sending pings.\n\tif compressionConfigured {\n\t\tpingInterval := opts.PingInterval\n\t\tpingMax := opts.MaxPingsOut\n\t\tif opts.Cluster.PingInterval > 0 {\n\t\t\tpingInterval = opts.Cluster.PingInterval\n\t\t}\n\t\tif opts.Cluster.MaxPingsOut > 0 {\n\t\t\tpingMax = opts.MaxPingsOut\n\t\t}\n\t\tc.watchForStaleConnection(adjustPingInterval(ROUTER, pingInterval), pingMax)\n\t} else {\n\t\t// Set the Ping timer\n\t\tc.setFirstPingTimer()\n\t}\n\n\t// For routes, the \"client\" is added to s.routes only when processing\n\t// the INFO protocol, that is much later.\n\t// In the meantime, if the server shutsdown, there would be no reference\n\t// to the client (connection) to be closed, leaving this readLoop\n\t// uinterrupted, causing the Shutdown() to wait indefinitively.\n\t// We need to store the client in a special map, under a special lock.\n\tif !s.addToTempClients(c.cid, c) {\n\t\tc.mu.Unlock()\n\t\tc.setNoReconnect()\n\t\tc.closeConnection(ServerShutdown)\n\t\treturn nil\n\t}\n\n\t// Check for Auth required state for incoming connections.\n\t// Make sure to do this before spinning up readLoop.\n\tif authRequired && !didSolicit {\n\t\tttl := secondsToDuration(opts.Cluster.AuthTimeout)\n\t\tc.setAuthTimer(ttl)\n\t}\n\n\t// Spin up the read loop.\n\ts.startGoRoutine(func() { c.readLoop(nil) })\n\n\t// Spin up the write loop.\n\ts.startGoRoutine(func() { c.writeLoop() })\n\n\tif tlsRequired {\n\t\tc.Debugf(\"TLS handshake complete\")\n\t\tcs := c.nc.(*tls.Conn).ConnectionState()\n\t\tc.Debugf(\"TLS version %s, cipher suite %s\", tlsVersion(cs.Version), tlsCipher(cs.CipherSuite))\n\t}\n\n\t// Queue Connect proto if we solicited the connection.\n\tif didSolicit {\n\t\tc.Debugf(\"Route connect msg sent\")\n\t\tif err := c.sendRouteConnect(clusterName, tlsRequired); err != nil {\n\t\t\tc.mu.Unlock()\n\t\t\tc.closeConnection(ProtocolViolation)\n\t\t\treturn nil\n\t\t}\n\t}\n\n\tif !delayInfo {\n\t\t// Send our info to the other side.\n\t\t// Our new version requires dynamic information for accounts and a nonce.\n\t\tc.enqueueProto(infoJSON)\n\t}\n\tc.mu.Unlock()\n\n\tc.Noticef(\"Route connection created\")\n\treturn c\n}\n\nfunc routeShouldDelayInfo(accName string, opts *Options) bool {\n\treturn accName == _EMPTY_ && opts.Cluster.PoolSize >= 1\n}\n\n// Generates a nonce and set some route info's fields before marshal'ing into JSON.\n// To be used only when a route is created (to send the initial INFO protocol).\n//\n// Server lock held on entry.\nfunc (s *Server) generateRouteInitialInfoJSON(accName, compression string, poolIdx int, gossipMode byte) []byte {\n\t// New proto wants a nonce (although not used in routes, that is, not signed in CONNECT)\n\tvar raw [nonceLen]byte\n\tnonce := raw[:]\n\ts.generateNonce(nonce)\n\tri := &s.routeInfo\n\t// Override compression with s2_auto instead of actual compression level.\n\tif s.getOpts().Cluster.Compression.Mode == CompressionS2Auto {\n\t\tcompression = CompressionS2Auto\n\t}\n\tri.Nonce, ri.RouteAccount, ri.RoutePoolIdx, ri.Compression, ri.GossipMode = string(nonce), accName, poolIdx, compression, gossipMode\n\tinfoJSON := generateInfoJSON(&s.routeInfo)\n\t// Clear now that it has been serialized. Will prevent nonce to be included in async INFO that we may send.\n\t// Same for some other fields.\n\tri.Nonce, ri.RouteAccount, ri.RoutePoolIdx, ri.Compression, ri.GossipMode = _EMPTY_, _EMPTY_, 0, _EMPTY_, 0\n\treturn infoJSON\n}\n\nconst (\n\t_CRLF_  = \"\\r\\n\"\n\t_EMPTY_ = \"\"\n)\n\nfunc (s *Server) addRoute(c *client, didSolicit, sendDelayedInfo bool, gossipMode byte, info *Info, accName string) bool {\n\tid := info.ID\n\n\tvar acc *Account\n\tif accName != _EMPTY_ {\n\t\tvar err error\n\t\tacc, err = s.LookupAccount(accName)\n\t\tif err != nil {\n\t\t\tc.sendErrAndErr(fmt.Sprintf(\"Unable to lookup account %q: %v\", accName, err))\n\t\t\tc.closeConnection(MissingAccount)\n\t\t\treturn false\n\t\t}\n\t}\n\n\ts.mu.Lock()\n\tif !s.isRunning() || s.routesReject {\n\t\ts.mu.Unlock()\n\t\treturn false\n\t}\n\tvar invProtoErr string\n\n\topts := s.getOpts()\n\n\t// Assume we are in pool mode if info.RoutePoolSize is set. We may disable\n\t// in some cases.\n\tpool := info.RoutePoolSize > 0\n\t// This is used to prevent a server with pooling to constantly trying\n\t// to connect to a server with no pooling (for instance old server) after\n\t// the first connection is established.\n\tvar noReconnectForOldServer bool\n\n\t// To allow rolling updates, we now allow servers with different pool sizes\n\t// so we will use as the effective pool size here, the max between our\n\t// configured size and the size we receive in the info protocol.\n\teffectivePoolSize := max(s.routesPoolSize, info.RoutePoolSize)\n\n\t// If the remote is an old server, info.RoutePoolSize will be 0, or if\n\t// this server's Cluster.PoolSize is negative, we will behave as an old\n\t// server and need to handle things differently.\n\tif info.RoutePoolSize <= 0 || opts.Cluster.PoolSize < 0 {\n\t\tif accName != _EMPTY_ {\n\t\t\tinvProtoErr = fmt.Sprintf(\"Not possible to have a dedicated route for account %q between those servers\", accName)\n\t\t\t// In this case, make sure this route does not attempt to reconnect\n\t\t\tc.setNoReconnect()\n\t\t} else {\n\t\t\t// We will accept, but treat this remote has \"no pool\"\n\t\t\tpool, noReconnectForOldServer = false, true\n\t\t\tc.mu.Lock()\n\t\t\tc.route.poolIdx = 0\n\t\t\tc.route.noPool = true\n\t\t\tc.mu.Unlock()\n\t\t\t// Keep track of number of routes like that. We will use that when\n\t\t\t// sending subscriptions over routes.\n\t\t\ts.routesNoPool++\n\t\t}\n\t} else if didSolicit {\n\t\t// For solicited route, the incoming's RoutePoolIdx should not be set.\n\t\tif info.RoutePoolIdx != 0 {\n\t\t\tinvProtoErr = fmt.Sprintf(\"Route pool index should not be set but is set to %v\", info.RoutePoolIdx)\n\t\t}\n\t} else if info.RoutePoolIdx < 0 || info.RoutePoolIdx >= effectivePoolSize {\n\t\t// For non solicited routes, if the remote sends a RoutePoolIdx, make\n\t\t// sure it is a valid one (in range of the pool size).\n\t\tinvProtoErr = fmt.Sprintf(\"Invalid route pool index: %v - pool size is %v\", info.RoutePoolIdx, info.RoutePoolSize)\n\t}\n\tif invProtoErr != _EMPTY_ {\n\t\ts.mu.Unlock()\n\t\tc.sendErrAndErr(invProtoErr)\n\t\tc.closeConnection(ProtocolViolation)\n\t\treturn false\n\t}\n\t// If accName is set, we are dealing with a per-account connection.\n\tif accName != _EMPTY_ {\n\t\t// When an account has its own route, it will be an error if the given\n\t\t// account name is not found in s.accRoutes map.\n\t\tconns, exists := s.accRoutes[accName]\n\t\tif !exists {\n\t\t\ts.mu.Unlock()\n\t\t\tc.sendErrAndErr(fmt.Sprintf(\"No route for account %q\", accName))\n\t\t\tc.closeConnection(ProtocolViolation)\n\t\t\treturn false\n\t\t}\n\t\tremote, exists := conns[id]\n\t\tif !exists {\n\t\t\tconns[id] = c\n\t\t\tc.mu.Lock()\n\t\t\tidHash := c.route.idHash\n\t\t\tcid := c.cid\n\t\t\trtype := c.route.routeType\n\t\t\tif sendDelayedInfo {\n\t\t\t\tcm := compressionModeForInfoProtocol(&opts.Cluster.Compression, c.route.compression)\n\t\t\t\tc.enqueueProto(s.generateRouteInitialInfoJSON(accName, cm, 0, gossipMode))\n\t\t\t}\n\t\t\tif c.last.IsZero() {\n\t\t\t\tc.last = time.Now()\n\t\t\t}\n\t\t\tif acc != nil {\n\t\t\t\tc.acc = acc\n\t\t\t}\n\t\t\tc.mu.Unlock()\n\n\t\t\t// Store this route with key being the route id hash + account name\n\t\t\ts.storeRouteByHash(idHash+accName, c)\n\n\t\t\t// Now that we have registered the route, we can remove from the temp map.\n\t\t\ts.removeFromTempClients(cid)\n\n\t\t\t// We don't need to send if the only route is the one we just accepted.\n\t\t\tif len(conns) > 1 {\n\t\t\t\ts.forwardNewRouteInfoToKnownServers(info, rtype, didSolicit, gossipMode)\n\t\t\t}\n\n\t\t\t// Send subscription interest\n\t\t\ts.sendSubsToRoute(c, -1, accName)\n\t\t} else {\n\t\t\thandleDuplicateRoute(remote, c, true)\n\t\t}\n\t\ts.mu.Unlock()\n\t\treturn !exists\n\t}\n\tvar remote *client\n\t// That will be the position of the connection in the slice, we initialize\n\t// to -1 to indicate that no space was found.\n\tidx := -1\n\t// This will be the size (or number of connections) in a given slice.\n\tsz := 0\n\t// Check if we know about the remote server\n\tconns, exists := s.routes[id]\n\tif !exists {\n\t\t// Now, create a slice for route connections of the size of the pool\n\t\t// or 1 when not in pool mode.\n\t\tconns = make([]*client, effectivePoolSize)\n\t\t// Track this slice for this remote server.\n\t\ts.routes[id] = conns\n\t\t// Set the index to info.RoutePoolIdx because if this is a solicited\n\t\t// route, this value will be 0, which is what we want, otherwise, we\n\t\t// will use whatever index the remote has chosen.\n\t\tidx = info.RoutePoolIdx\n\t} else if pool {\n\t\t// The remote could have done a config reload and increased the pool size.\n\t\t// It will close the connections before soliciting again, however, if\n\t\t// on this side, one of the route is not yet fully removed, but the\n\t\t// first one is, it would accept the new connection (with a greater pool\n\t\t// size) and we would not go through the phase of `!exists` above creating\n\t\t// the slice with the right size. So we need to check here and add new empty\n\t\t// entries to complete the effective pool size.\n\t\tif n := effectivePoolSize - len(conns); n > 0 {\n\t\t\tfor range n {\n\t\t\t\tconns = append(conns, nil)\n\t\t\t}\n\t\t\ts.routes[id] = conns\n\t\t}\n\t\t// The remote was found. If this is a non solicited route, we will place\n\t\t// the connection in the pool at the index given by info.RoutePoolIdx.\n\t\t// But if there is already one, close this incoming connection as a\n\t\t// duplicate.\n\t\tif !didSolicit {\n\t\t\tidx = info.RoutePoolIdx\n\t\t\tif remote = conns[idx]; remote != nil {\n\t\t\t\thandleDuplicateRoute(remote, c, false)\n\t\t\t\ts.mu.Unlock()\n\t\t\t\treturn false\n\t\t\t}\n\t\t\t// Look if there is a solicited route in the pool. If there is one,\n\t\t\t// they should all be, so stop at the first.\n\t\t\tif url, rtype, hasSolicited := hasSolicitedRoute(conns); hasSolicited {\n\t\t\t\tupgradeRouteToSolicited(c, url, rtype)\n\t\t\t}\n\t\t} else {\n\t\t\t// If we solicit, upgrade to solicited all non-solicited routes that\n\t\t\t// we may have registered.\n\t\t\tc.mu.Lock()\n\t\t\turl := c.route.url\n\t\t\trtype := c.route.routeType\n\t\t\tc.mu.Unlock()\n\t\t\tfor _, r := range conns {\n\t\t\t\tupgradeRouteToSolicited(r, url, rtype)\n\t\t\t}\n\t\t}\n\t\t// For all cases (solicited and not) we need to count how many connections\n\t\t// we already have, and for solicited route, we will find a free spot in\n\t\t// the slice.\n\t\tfor i, r := range conns {\n\t\t\tif idx == -1 && r == nil {\n\t\t\t\tidx = i\n\t\t\t} else if r != nil {\n\t\t\t\tsz++\n\t\t\t}\n\t\t}\n\t} else {\n\t\tremote = conns[0]\n\t}\n\t// If there is a spot, idx will be greater or equal to 0.\n\tif idx >= 0 {\n\t\tc.mu.Lock()\n\t\tc.route.connectURLs = info.ClientConnectURLs\n\t\tc.route.wsConnURLs = info.WSConnectURLs\n\t\tc.route.poolIdx = idx\n\t\trtype := c.route.routeType\n\t\tcid := c.cid\n\t\tidHash := c.route.idHash\n\t\trHash := c.route.hash\n\t\trn := c.route.remoteName\n\t\turl := c.route.url\n\t\tif sendDelayedInfo {\n\t\t\tcm := compressionModeForInfoProtocol(&opts.Cluster.Compression, c.route.compression)\n\t\t\tc.enqueueProto(s.generateRouteInitialInfoJSON(_EMPTY_, cm, idx, gossipMode))\n\t\t}\n\t\tif c.last.IsZero() {\n\t\t\tc.last = time.Now()\n\t\t}\n\t\tc.mu.Unlock()\n\n\t\t// With pooling, we keep track of the remote's configured route pool size.\n\t\t// We do so when adding the connection in the first slot, not when `sz == 1`\n\t\t// because there could be situations where we have old connections that have\n\t\t// not yet been removed and so we would not have `sz == `. However, we will\n\t\t// always have the condition where we are adding the new connection at `idx==0`\n\t\t// so use that as the condition to store the remote pool size.\n\t\tif pool && idx == 0 {\n\t\t\tif s.remoteRoutePoolSize == nil {\n\t\t\t\ts.remoteRoutePoolSize = make(map[string]int)\n\t\t\t}\n\t\t\ts.remoteRoutePoolSize[id] = info.RoutePoolSize\n\t\t}\n\n\t\t// Add to the slice and bump the count of connections for this remote\n\t\tconns[idx] = c\n\t\tsz++\n\t\t// This boolean will indicate that we are registering the only\n\t\t// connection in non pooled situation or we stored the very first\n\t\t// connection for a given remote server.\n\t\tdoOnce := !pool || sz == 1\n\t\tif doOnce {\n\t\t\t// check to be consistent and future proof. but will be same domain\n\t\t\tif s.sameDomain(info.Domain) {\n\t\t\t\ts.nodeToInfo.Store(rHash,\n\t\t\t\t\tnodeInfo{rn, s.info.Version, s.info.Cluster, info.Domain, id, nil, nil, nil, false, info.JetStream, false, false})\n\t\t\t}\n\t\t}\n\n\t\t// Store this route using the hash as the key\n\t\tif pool {\n\t\t\tidHash += strconv.Itoa(idx)\n\t\t}\n\t\ts.storeRouteByHash(idHash, c)\n\n\t\t// Now that we have registered the route, we can remove from the temp map.\n\t\ts.removeFromTempClients(cid)\n\n\t\tif doOnce {\n\t\t\t// If the INFO contains a Gateway URL, add it to the list for our cluster.\n\t\t\tif info.GatewayURL != _EMPTY_ && s.addGatewayURL(info.GatewayURL) {\n\t\t\t\ts.sendAsyncGatewayInfo()\n\t\t\t}\n\n\t\t\t// We don't need to send if the only route is the one we just accepted.\n\t\t\tif len(s.routes) > 1 {\n\t\t\t\ts.forwardNewRouteInfoToKnownServers(info, rtype, didSolicit, gossipMode)\n\t\t\t}\n\n\t\t\t// Send info about the known gateways to this route.\n\t\t\ts.sendGatewayConfigsToRoute(c)\n\n\t\t\t// Unless disabled, possibly update the server's INFO protocol\n\t\t\t// and send to clients that know how to handle async INFOs.\n\t\t\tif !opts.Cluster.NoAdvertise {\n\t\t\t\ts.addConnectURLsAndSendINFOToClients(info.ClientConnectURLs, info.WSConnectURLs)\n\t\t\t}\n\n\t\t\t// Add the remote's leafnodeURL to our list of URLs and send the update\n\t\t\t// to all LN connections. (Note that when coming from a route, LeafNodeURLs\n\t\t\t// is an array of size 1 max).\n\t\t\tif len(info.LeafNodeURLs) == 1 && s.addLeafNodeURL(info.LeafNodeURLs[0]) {\n\t\t\t\ts.sendAsyncLeafNodeInfo()\n\t\t\t}\n\t\t}\n\n\t\t// Send the subscriptions interest.\n\t\ts.sendSubsToRoute(c, idx, _EMPTY_)\n\n\t\t// In pool mode, if we did not yet reach the cap, try to connect a new connection\n\t\tif pool && didSolicit && sz != effectivePoolSize {\n\t\t\ts.startGoRoutine(func() {\n\t\t\t\tselect {\n\t\t\t\tcase <-time.After(time.Duration(rand.Intn(100)) * time.Millisecond):\n\t\t\t\tcase <-s.quitCh:\n\t\t\t\t\t// Doing this here and not as a defer because connectToRoute is also\n\t\t\t\t\t// calling s.grWG.Done() on exit, so we do this only if we don't\n\t\t\t\t\t// invoke connectToRoute().\n\t\t\t\t\ts.grWG.Done()\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t\ts.connectToRoute(url, rtype, true, gossipMode, _EMPTY_)\n\t\t\t})\n\t\t}\n\t}\n\ts.mu.Unlock()\n\tif pool {\n\t\tif idx == -1 {\n\t\t\t// Was full, so need to close connection\n\t\t\tc.Debugf(\"Route pool size reached, closing extra connection to %q\", id)\n\t\t\thandleDuplicateRoute(nil, c, true)\n\t\t\treturn false\n\t\t}\n\t\treturn true\n\t}\n\t// This is for non-pool mode at this point.\n\tif exists {\n\t\thandleDuplicateRoute(remote, c, noReconnectForOldServer)\n\t}\n\n\treturn !exists\n}\n\nfunc hasSolicitedRoute(conns []*client) (*url.URL, RouteType, bool) {\n\tvar url *url.URL\n\tvar rtype RouteType\n\tfor _, r := range conns {\n\t\tif r == nil {\n\t\t\tcontinue\n\t\t}\n\t\tr.mu.Lock()\n\t\tif r.route.didSolicit {\n\t\t\turl = r.route.url\n\t\t\trtype = r.route.routeType\n\t\t}\n\t\tr.mu.Unlock()\n\t\tif url != nil {\n\t\t\treturn url, rtype, true\n\t\t}\n\t}\n\treturn nil, 0, false\n}\n\nfunc upgradeRouteToSolicited(r *client, url *url.URL, rtype RouteType) {\n\tif r == nil {\n\t\treturn\n\t}\n\tr.mu.Lock()\n\tif !r.route.didSolicit {\n\t\tr.route.didSolicit = true\n\t\tr.route.url = url\n\t}\n\tif rtype == Explicit {\n\t\tr.route.routeType = Explicit\n\t}\n\tr.mu.Unlock()\n}\n\nfunc handleDuplicateRoute(remote, c *client, setNoReconnect bool) {\n\t// We used to clear some fields when closing a duplicate connection\n\t// to prevent sending INFO protocols for the remotes to update\n\t// their leafnode/gateway URLs. This is no longer needed since\n\t// removeRoute() now does the right thing of doing that only when\n\t// the closed connection was an added route connection.\n\tc.mu.Lock()\n\tdidSolicit := c.route.didSolicit\n\turl := c.route.url\n\trtype := c.route.routeType\n\tif setNoReconnect {\n\t\tc.flags.set(noReconnect)\n\t}\n\tc.mu.Unlock()\n\n\tif remote == nil {\n\t\treturn\n\t}\n\n\tremote.mu.Lock()\n\tif didSolicit && !remote.route.didSolicit {\n\t\tremote.route.didSolicit = true\n\t\tremote.route.url = url\n\t}\n\t// The extra route might be an configured explicit route\n\t// so keep the state that the remote was configured.\n\tif rtype == Explicit {\n\t\tremote.route.routeType = rtype\n\t}\n\t// This is to mitigate the issue where both sides add the route\n\t// on the opposite connection, and therefore end-up with both\n\t// connections being dropped.\n\tremote.route.retry = true\n\tremote.mu.Unlock()\n}\n\n// Import filter check.\nfunc (c *client) importFilter(sub *subscription) bool {\n\tif c.perms == nil {\n\t\treturn true\n\t}\n\treturn c.canImport(string(sub.subject))\n}\n\n// updateRouteSubscriptionMap will make sure to update the route map for the subscription. Will\n// also forward to all routes if needed.\nfunc (s *Server) updateRouteSubscriptionMap(acc *Account, sub *subscription, delta int32) {\n\tif acc == nil || sub == nil {\n\t\treturn\n\t}\n\n\t// We only store state on local subs for transmission across all other routes.\n\tif sub.client == nil || sub.client.kind == ROUTER || sub.client.kind == GATEWAY {\n\t\treturn\n\t}\n\n\tif sub.si {\n\t\treturn\n\t}\n\n\t// Copy to hold outside acc lock.\n\tvar n int32\n\tvar ok bool\n\n\tisq := len(sub.queue) > 0\n\n\taccLock := func() {\n\t\t// Not required for code correctness, but helps reduce the number of\n\t\t// updates sent to the routes when processing high number of concurrent\n\t\t// queue subscriptions updates (sub/unsub).\n\t\t// See https://github.com/nats-io/nats-server/pull/1126 for more details.\n\t\tif isq {\n\t\t\tacc.sqmu.Lock()\n\t\t}\n\t\tacc.mu.Lock()\n\t}\n\taccUnlock := func() {\n\t\tacc.mu.Unlock()\n\t\tif isq {\n\t\t\tacc.sqmu.Unlock()\n\t\t}\n\t}\n\n\taccLock()\n\n\t// This is non-nil when we know we are in cluster mode.\n\trm, lqws := acc.rm, acc.lqws\n\tif rm == nil {\n\t\taccUnlock()\n\t\treturn\n\t}\n\n\t// Create the subscription key which will prevent collisions between regular\n\t// and leaf routed subscriptions. See keyFromSubWithOrigin() for details.\n\tkey := keyFromSubWithOrigin(sub)\n\n\t// Decide whether we need to send an update out to all the routes.\n\tupdate := isq\n\n\t// This is where we do update to account. For queues we need to take\n\t// special care that this order of updates is same as what is sent out\n\t// over routes.\n\tif n, ok = rm[key]; ok {\n\t\tn += delta\n\t\tif n <= 0 {\n\t\t\tdelete(rm, key)\n\t\t\tif isq {\n\t\t\t\tdelete(lqws, key)\n\t\t\t}\n\t\t\tupdate = true // Update for deleting (N->0)\n\t\t} else {\n\t\t\trm[key] = n\n\t\t}\n\t} else if delta > 0 {\n\t\tn = delta\n\t\trm[key] = delta\n\t\tupdate = true // Adding a new entry for normal sub means update (0->1)\n\t}\n\n\taccUnlock()\n\n\tif !update {\n\t\treturn\n\t}\n\n\t// If we are sending a queue sub, make a copy and place in the queue weight.\n\t// FIXME(dlc) - We can be smarter here and avoid copying and acquiring the lock.\n\tif isq {\n\t\tsub.client.mu.Lock()\n\t\tnsub := *sub\n\t\tsub.client.mu.Unlock()\n\t\tnsub.qw = n\n\t\tsub = &nsub\n\t}\n\n\t// We need to send out this update. Gather routes\n\tvar _routes [32]*client\n\troutes := _routes[:0]\n\n\ts.mu.RLock()\n\t// The account's routePoolIdx field is set/updated under the server lock\n\t// (but also the account's lock). So we don't need to acquire the account's\n\t// lock here to get the value.\n\tif poolIdx := acc.routePoolIdx; poolIdx < 0 {\n\t\tif conns, ok := s.accRoutes[acc.Name]; ok {\n\t\t\tfor _, r := range conns {\n\t\t\t\troutes = append(routes, r)\n\t\t\t}\n\t\t}\n\t\tif s.routesNoPool > 0 {\n\t\t\t// We also need to look for \"no pool\" remotes (that is, routes to older\n\t\t\t// servers or servers that have explicitly disabled pooling).\n\t\t\ts.forEachRemote(func(r *client) {\n\t\t\t\tr.mu.Lock()\n\t\t\t\tif r.route.noPool {\n\t\t\t\t\troutes = append(routes, r)\n\t\t\t\t}\n\t\t\t\tr.mu.Unlock()\n\t\t\t})\n\t\t}\n\t} else {\n\t\t// We can't use s.forEachRouteIdx here since we want to check/get the\n\t\t// \"no pool\" route ONLY if we don't find a route at the given `poolIdx`.\n\t\tfor _, conns := range s.routes {\n\t\t\tif r := conns[poolIdx]; r != nil {\n\t\t\t\troutes = append(routes, r)\n\t\t\t} else if s.routesNoPool > 0 {\n\t\t\t\t// Check if we have a \"no pool\" route at index 0, and if so, it\n\t\t\t\t// means that for this remote, we have a single connection because\n\t\t\t\t// that server does not have pooling.\n\t\t\t\tif r := conns[0]; r != nil {\n\t\t\t\t\tr.mu.Lock()\n\t\t\t\t\tif r.route.noPool {\n\t\t\t\t\t\troutes = append(routes, r)\n\t\t\t\t\t}\n\t\t\t\t\tr.mu.Unlock()\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\ttrace := atomic.LoadInt32(&s.logging.trace) == 1\n\ts.mu.RUnlock()\n\n\t// If we are a queue subscriber we need to make sure our updates are serialized from\n\t// potential multiple connections. We want to make sure that the order above is preserved\n\t// here but not necessarily all updates need to be sent. We need to block and recheck the\n\t// n count with the lock held through sending here. We will suppress duplicate sends of same qw.\n\tif isq {\n\t\t// However, we can't hold the acc.mu lock since we allow client.mu.Lock -> acc.mu.Lock\n\t\t// but not the opposite. So use a dedicated lock while holding the route's lock.\n\t\tacc.sqmu.Lock()\n\t\tdefer acc.sqmu.Unlock()\n\n\t\tacc.mu.Lock()\n\t\tn = rm[key]\n\t\tsub.qw = n\n\t\t// Check the last sent weight here. If same, then someone\n\t\t// beat us to it and we can just return here. Otherwise update\n\t\tif ls, ok := lqws[key]; ok && ls == n {\n\t\t\tacc.mu.Unlock()\n\t\t\treturn\n\t\t} else if n > 0 {\n\t\t\tlqws[key] = n\n\t\t}\n\t\tacc.mu.Unlock()\n\t}\n\n\t// Snapshot into array\n\tsubs := []*subscription{sub}\n\n\t// Deliver to all routes.\n\tfor _, route := range routes {\n\t\troute.mu.Lock()\n\t\t// Note that queue unsubs where n > 0 are still\n\t\t// subscribes with a smaller weight.\n\t\troute.sendRouteSubOrUnSubProtos(subs, n > 0, trace, route.importFilter)\n\t\troute.mu.Unlock()\n\t}\n}\n\n// This starts the route accept loop in a go routine, unless it\n// is detected that the server has already been shutdown.\n// It will also start soliciting explicit routes.\nfunc (s *Server) startRouteAcceptLoop() {\n\tif s.isShuttingDown() {\n\t\treturn\n\t}\n\n\t// Snapshot server options.\n\topts := s.getOpts()\n\n\t// Snapshot server options.\n\tport := opts.Cluster.Port\n\n\tif port == -1 {\n\t\tport = 0\n\t}\n\n\t// This requires lock, so do this outside of may block.\n\tclusterName := s.ClusterName()\n\n\ts.mu.Lock()\n\ts.Noticef(\"Cluster name is %s\", clusterName)\n\tif s.isClusterNameDynamic() {\n\t\ts.Warnf(\"Cluster name was dynamically generated, consider setting one\")\n\t}\n\n\thp := net.JoinHostPort(opts.Cluster.Host, strconv.Itoa(port))\n\tl, e := natsListen(\"tcp\", hp)\n\ts.routeListenerErr = e\n\tif e != nil {\n\t\ts.mu.Unlock()\n\t\ts.Fatalf(\"Error listening on router port: %d - %v\", opts.Cluster.Port, e)\n\t\treturn\n\t}\n\ts.Noticef(\"Listening for route connections on %s\",\n\t\tnet.JoinHostPort(opts.Cluster.Host, strconv.Itoa(l.Addr().(*net.TCPAddr).Port)))\n\n\t// Check for TLSConfig\n\ttlsReq := opts.Cluster.TLSConfig != nil\n\tinfo := Info{\n\t\tID:           s.info.ID,\n\t\tName:         s.info.Name,\n\t\tVersion:      s.info.Version,\n\t\tGoVersion:    runtime.Version(),\n\t\tAuthRequired: false,\n\t\tTLSRequired:  tlsReq,\n\t\tTLSVerify:    tlsReq,\n\t\tMaxPayload:   s.info.MaxPayload,\n\t\tJetStream:    s.info.JetStream,\n\t\tProto:        s.getServerProto(),\n\t\tGatewayURL:   s.getGatewayURL(),\n\t\tHeaders:      s.supportsHeaders(),\n\t\tCluster:      s.info.Cluster,\n\t\tDomain:       s.info.Domain,\n\t\tDynamic:      s.isClusterNameDynamic(),\n\t\tLNOC:         true,\n\t\tLNOCU:        true,\n\t}\n\t// For tests that want to simulate old servers, do not set the compression\n\t// on the INFO protocol if configured with CompressionNotSupported.\n\tif cm := opts.Cluster.Compression.Mode; cm != CompressionNotSupported {\n\t\tinfo.Compression = cm\n\t}\n\tif ps := opts.Cluster.PoolSize; ps > 0 {\n\t\tinfo.RoutePoolSize = ps\n\t}\n\t// Set this if only if advertise is not disabled\n\tif !opts.Cluster.NoAdvertise {\n\t\tinfo.ClientConnectURLs = s.clientConnectURLs\n\t\tinfo.WSConnectURLs = s.websocket.connectURLs\n\t}\n\t// If we have selected a random port...\n\tif port == 0 {\n\t\t// Write resolved port back to options.\n\t\topts.Cluster.Port = l.Addr().(*net.TCPAddr).Port\n\t}\n\t// Check for Auth items\n\tif opts.Cluster.Username != \"\" {\n\t\tinfo.AuthRequired = true\n\t}\n\t// Check for permissions.\n\tif opts.Cluster.Permissions != nil {\n\t\tinfo.Import = opts.Cluster.Permissions.Import\n\t\tinfo.Export = opts.Cluster.Permissions.Export\n\t}\n\t// If this server has a LeafNode accept loop, s.leafNodeInfo.IP is,\n\t// at this point, set to the host:port for the leafnode accept URL,\n\t// taking into account possible advertise setting. Use the LeafNodeURLs\n\t// and set this server's leafnode accept URL. This will be sent to\n\t// routed servers.\n\tif !opts.LeafNode.NoAdvertise && s.leafNodeInfo.IP != _EMPTY_ {\n\t\tinfo.LeafNodeURLs = []string{s.leafNodeInfo.IP}\n\t}\n\ts.routeInfo = info\n\t// Possibly override Host/Port and set IP based on Cluster.Advertise\n\tif err := s.setRouteInfoHostPortAndIP(); err != nil {\n\t\ts.Fatalf(\"Error setting route INFO with Cluster.Advertise value of %s, err=%v\", opts.Cluster.Advertise, err)\n\t\tl.Close()\n\t\ts.mu.Unlock()\n\t\treturn\n\t}\n\t// Setup state that can enable shutdown\n\ts.routeListener = l\n\t// Warn if using Cluster.Insecure\n\tif tlsReq && opts.Cluster.TLSConfig.InsecureSkipVerify {\n\t\ts.Warnf(clusterTLSInsecureWarning)\n\t}\n\n\t// Now that we have the port, keep track of all ip:port that resolve to this server.\n\tif interfaceAddr, err := net.InterfaceAddrs(); err == nil {\n\t\tvar localIPs []string\n\t\tfor i := 0; i < len(interfaceAddr); i++ {\n\t\t\tinterfaceIP, _, _ := net.ParseCIDR(interfaceAddr[i].String())\n\t\t\tipStr := interfaceIP.String()\n\t\t\tif net.ParseIP(ipStr) != nil {\n\t\t\t\tlocalIPs = append(localIPs, ipStr)\n\t\t\t}\n\t\t}\n\t\tvar portStr = strconv.FormatInt(int64(s.routeInfo.Port), 10)\n\t\tfor _, ip := range localIPs {\n\t\t\tipPort := net.JoinHostPort(ip, portStr)\n\t\t\ts.routesToSelf[ipPort] = struct{}{}\n\t\t}\n\t}\n\n\t// Start the accept loop in a different go routine.\n\tgo s.acceptConnections(l, \"Route\", func(conn net.Conn) { s.createRoute(conn, nil, Implicit, gossipDefault, _EMPTY_) }, nil)\n\n\t// Solicit Routes if applicable. This will not block.\n\ts.solicitRoutes(opts.Routes, opts.Cluster.PinnedAccounts)\n\n\ts.mu.Unlock()\n}\n\n// Similar to setInfoHostPortAndGenerateJSON, but for routeInfo.\nfunc (s *Server) setRouteInfoHostPortAndIP() error {\n\topts := s.getOpts()\n\tif opts.Cluster.Advertise != _EMPTY_ {\n\t\tadvHost, advPort, err := parseHostPort(opts.Cluster.Advertise, opts.Cluster.Port)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\ts.routeInfo.Host = advHost\n\t\ts.routeInfo.Port = advPort\n\t\ts.routeInfo.IP = fmt.Sprintf(\"nats-route://%s/\", net.JoinHostPort(advHost, strconv.Itoa(advPort)))\n\t} else {\n\t\ts.routeInfo.Host = opts.Cluster.Host\n\t\ts.routeInfo.Port = opts.Cluster.Port\n\t\ts.routeInfo.IP = \"\"\n\t}\n\treturn nil\n}\n\n// StartRouting will start the accept loop on the cluster host:port\n// and will actively try to connect to listed routes.\nfunc (s *Server) StartRouting(clientListenReady chan struct{}) {\n\tdefer s.grWG.Done()\n\n\t// Wait for the client and leafnode listen ports to be opened,\n\t// and the possible ephemeral ports to be selected.\n\t<-clientListenReady\n\n\t// Start the accept loop and solicitation of explicit routes (if applicable)\n\ts.startRouteAcceptLoop()\n\n}\n\nfunc (s *Server) reConnectToRoute(rURL *url.URL, rtype RouteType, accName string) {\n\t// If A connects to B, and B to A (regardless if explicit or\n\t// implicit - due to auto-discovery), and if each server first\n\t// registers the route on the opposite TCP connection, the\n\t// two connections will end-up being closed.\n\t// Add some random delay to reduce risk of repeated failures.\n\tdelay := time.Duration(rand.Intn(100)) * time.Millisecond\n\tif rtype == Explicit {\n\t\tdelay += DEFAULT_ROUTE_RECONNECT\n\t}\n\tselect {\n\tcase <-time.After(delay):\n\tcase <-s.quitCh:\n\t\ts.grWG.Done()\n\t\treturn\n\t}\n\ts.connectToRoute(rURL, rtype, false, gossipDefault, accName)\n}\n\n// Checks to make sure the route is still valid.\nfunc (s *Server) routeStillValid(rURL *url.URL) bool {\n\tfor _, ri := range s.getOpts().Routes {\n\t\tif urlsAreEqual(ri, rURL) {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\nfunc (s *Server) connectToRoute(rURL *url.URL, rtype RouteType, firstConnect bool, gossipMode byte, accName string) {\n\tdefer s.grWG.Done()\n\tif rURL == nil {\n\t\treturn\n\t}\n\t// For explicit routes, we will try to connect until we succeed. For implicit\n\t// we will try only based on the number of ConnectRetries optin.\n\ttryForEver := rtype == Explicit\n\n\t// Snapshot server options.\n\topts := s.getOpts()\n\n\tconst connErrFmt = \"Error trying to connect to route (attempt %v): %v\"\n\n\ts.mu.RLock()\n\tresolver := s.routeResolver\n\texcludedAddresses := s.routesToSelf\n\ts.mu.RUnlock()\n\n\tfor attempts := 0; s.isRunning(); {\n\t\tif tryForEver {\n\t\t\tif !s.routeStillValid(rURL) {\n\t\t\t\treturn\n\t\t\t}\n\t\t\tif accName != _EMPTY_ {\n\t\t\t\ts.mu.RLock()\n\t\t\t\t_, valid := s.accRoutes[accName]\n\t\t\t\ts.mu.RUnlock()\n\t\t\t\tif !valid {\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tvar conn net.Conn\n\t\taddress, err := s.getRandomIP(resolver, rURL.Host, excludedAddresses)\n\t\tif err == errNoIPAvail {\n\t\t\t// This is ok, we are done.\n\t\t\treturn\n\t\t}\n\t\tif err == nil {\n\t\t\ts.Debugf(\"Trying to connect to route on %s (%s)\", rURL.Host, address)\n\t\t\tconn, err = natsDialTimeout(\"tcp\", address, DEFAULT_ROUTE_DIAL)\n\t\t}\n\t\tif err != nil {\n\t\t\tattempts++\n\t\t\tif s.shouldReportConnectErr(firstConnect, attempts) {\n\t\t\t\ts.Errorf(connErrFmt, attempts, err)\n\t\t\t} else {\n\t\t\t\ts.Debugf(connErrFmt, attempts, err)\n\t\t\t}\n\t\t\tif !tryForEver {\n\t\t\t\tif opts.Cluster.ConnectRetries <= 0 {\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t\tif attempts > opts.Cluster.ConnectRetries {\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}\n\t\t\tselect {\n\t\t\tcase <-s.quitCh:\n\t\t\t\treturn\n\t\t\tcase <-time.After(routeConnectDelay):\n\t\t\t\tcontinue\n\t\t\t}\n\t\t}\n\n\t\tif tryForEver && !s.routeStillValid(rURL) {\n\t\t\tconn.Close()\n\t\t\treturn\n\t\t}\n\n\t\t// We have a route connection here.\n\t\t// Go ahead and create it and exit this func.\n\t\ts.createRoute(conn, rURL, rtype, gossipMode, accName)\n\t\treturn\n\t}\n}\n\nfunc (c *client) isSolicitedRoute() bool {\n\tc.mu.Lock()\n\tdefer c.mu.Unlock()\n\treturn c.kind == ROUTER && c.route != nil && c.route.didSolicit\n}\n\n// Save the first hostname found in route URLs. This will be used in gossip mode\n// when trying to create a TLS connection by setting the tlsConfig.ServerName.\n// Lock is held on entry\nfunc (s *Server) saveRouteTLSName(routes []*url.URL) {\n\tfor _, u := range routes {\n\t\tif s.routeTLSName == _EMPTY_ && net.ParseIP(u.Hostname()) == nil {\n\t\t\ts.routeTLSName = u.Hostname()\n\t\t}\n\t}\n}\n\n// Start connection process to provided routes. Each route connection will\n// be started in a dedicated go routine.\n// Lock is held on entry\nfunc (s *Server) solicitRoutes(routes []*url.URL, accounts []string) {\n\ts.saveRouteTLSName(routes)\n\tfor _, r := range routes {\n\t\troute := r\n\t\ts.startGoRoutine(func() { s.connectToRoute(route, Explicit, true, gossipDefault, _EMPTY_) })\n\t}\n\t// Now go over possible per-account routes and create them.\n\tfor _, an := range accounts {\n\t\tfor _, r := range routes {\n\t\t\troute, accName := r, an\n\t\t\ts.startGoRoutine(func() { s.connectToRoute(route, Explicit, true, gossipDefault, accName) })\n\t\t}\n\t}\n}\n\nfunc (c *client) processRouteConnect(srv *Server, arg []byte, lang string) error {\n\t// Way to detect clients that incorrectly connect to the route listen\n\t// port. Client provide Lang in the CONNECT protocol while ROUTEs don't.\n\tif lang != \"\" {\n\t\tc.sendErrAndErr(ErrClientConnectedToRoutePort.Error())\n\t\tc.closeConnection(WrongPort)\n\t\treturn ErrClientConnectedToRoutePort\n\t}\n\t// Unmarshal as a route connect protocol\n\tproto := &connectInfo{}\n\n\tif err := json.Unmarshal(arg, proto); err != nil {\n\t\treturn err\n\t}\n\t// Reject if this has Gateway which means that it would be from a gateway\n\t// connection that incorrectly connects to the Route port.\n\tif proto.Gateway != \"\" {\n\t\terrTxt := fmt.Sprintf(\"Rejecting connection from gateway %q on the Route port\", proto.Gateway)\n\t\tc.Errorf(errTxt)\n\t\tc.sendErr(errTxt)\n\t\tc.closeConnection(WrongGateway)\n\t\treturn ErrWrongGateway\n\t}\n\n\tif srv == nil {\n\t\treturn ErrServerNotRunning\n\t}\n\n\tperms := srv.getOpts().Cluster.Permissions\n\tclusterName := srv.ClusterName()\n\n\t// If we have a cluster name set, make sure it matches ours.\n\tif proto.Cluster != clusterName {\n\t\tshouldReject := true\n\t\t// If we have a dynamic name we will do additional checks.\n\t\tif srv.isClusterNameDynamic() {\n\t\t\tif !proto.Dynamic || strings.Compare(clusterName, proto.Cluster) < 0 {\n\t\t\t\t// We will take on their name since theirs is configured or higher then ours.\n\t\t\t\tsrv.setClusterName(proto.Cluster)\n\t\t\t\tif !proto.Dynamic {\n\t\t\t\t\tsrv.optsMu.Lock()\n\t\t\t\t\tsrv.opts.Cluster.Name = proto.Cluster\n\t\t\t\t\tsrv.optsMu.Unlock()\n\t\t\t\t}\n\t\t\t\tc.mu.Lock()\n\t\t\t\tremoteID := c.opts.Name\n\t\t\t\tc.mu.Unlock()\n\t\t\t\tsrv.removeAllRoutesExcept(remoteID)\n\t\t\t\tshouldReject = false\n\t\t\t}\n\t\t}\n\t\tif shouldReject {\n\t\t\terrTxt := fmt.Sprintf(\"Rejecting connection, cluster name %q does not match %q\", proto.Cluster, clusterName)\n\t\t\tc.Errorf(errTxt)\n\t\t\tc.sendErr(errTxt)\n\t\t\tc.closeConnection(ClusterNameConflict)\n\t\t\treturn ErrClusterNameRemoteConflict\n\t\t}\n\t}\n\n\tsupportsHeaders := c.srv.supportsHeaders()\n\n\t// Grab connection name of remote route.\n\tc.mu.Lock()\n\tc.route.remoteID = c.opts.Name\n\tc.route.lnoc = proto.LNOC\n\tc.route.lnocu = proto.LNOCU\n\tc.setRoutePermissions(perms)\n\tc.headers = supportsHeaders && proto.Headers\n\tc.mu.Unlock()\n\treturn nil\n}\n\n// Called when we update our cluster name during negotiations with remotes.\nfunc (s *Server) removeAllRoutesExcept(remoteID string) {\n\ts.mu.Lock()\n\troutes := make([]*client, 0, s.numRoutes())\n\tfor rID, conns := range s.routes {\n\t\tif rID == remoteID {\n\t\t\tcontinue\n\t\t}\n\t\tfor _, r := range conns {\n\t\t\tif r != nil {\n\t\t\t\troutes = append(routes, r)\n\t\t\t}\n\t\t}\n\t}\n\tfor _, conns := range s.accRoutes {\n\t\tfor rID, r := range conns {\n\t\t\tif rID == remoteID {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\troutes = append(routes, r)\n\t\t}\n\t}\n\ts.mu.Unlock()\n\n\tfor _, r := range routes {\n\t\tr.closeConnection(ClusterNameConflict)\n\t}\n}\n\nfunc (s *Server) removeRoute(c *client) {\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\n\tvar (\n\t\trID           string\n\t\tlnURL         string\n\t\tgwURL         string\n\t\tidHash        string\n\t\taccName       string\n\t\tpoolIdx       = -1\n\t\tconnectURLs   []string\n\t\twsConnectURLs []string\n\t\topts          = s.getOpts()\n\t\trURL          *url.URL\n\t\tnoPool        bool\n\t\trtype         RouteType\n\t)\n\tc.mu.Lock()\n\tcid := c.cid\n\tr := c.route\n\tif r != nil {\n\t\trID = r.remoteID\n\t\tlnURL = r.leafnodeURL\n\t\tidHash = r.idHash\n\t\tgwURL = r.gatewayURL\n\t\tpoolIdx = r.poolIdx\n\t\taccName = bytesToString(r.accName)\n\t\tif r.noPool {\n\t\t\ts.routesNoPool--\n\t\t\tnoPool = true\n\t\t}\n\t\tconnectURLs = r.connectURLs\n\t\twsConnectURLs = r.wsConnURLs\n\t\trURL = r.url\n\t\trtype = r.routeType\n\t}\n\tc.mu.Unlock()\n\tif accName != _EMPTY_ {\n\t\tif conns, ok := s.accRoutes[accName]; ok {\n\t\t\tif r := conns[rID]; r == c {\n\t\t\t\ts.removeRouteByHash(idHash + accName)\n\t\t\t\tdelete(conns, rID)\n\t\t\t\t// Do not remove or set to nil when all remotes have been\n\t\t\t\t// removed from the map. The configured accounts must always\n\t\t\t\t// be in the accRoutes map and addRoute expects \"conns\" map\n\t\t\t\t// to be created.\n\t\t\t}\n\t\t}\n\t}\n\t// If this is still -1, it means that it was not added to the routes\n\t// so simply remove from temp clients and we are done.\n\tif poolIdx == -1 || accName != _EMPTY_ {\n\t\ts.removeFromTempClients(cid)\n\t\treturn\n\t}\n\tif conns, ok := s.routes[rID]; ok {\n\t\t// If this route was not the one stored, simply remove from the\n\t\t// temporary map and be done.\n\t\tif conns[poolIdx] != c {\n\t\t\ts.removeFromTempClients(cid)\n\t\t\treturn\n\t\t}\n\t\tconns[poolIdx] = nil\n\t\t// Now check if this was the last connection to be removed.\n\t\tempty := true\n\t\tfor _, c := range conns {\n\t\t\tif c != nil {\n\t\t\t\tempty = false\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\t// This was the last route for this remote. Remove the remote entry\n\t\t// and possibly send some async INFO protocols regarding gateway\n\t\t// and leafnode URLs.\n\t\tif empty {\n\t\t\tdelete(s.routes, rID)\n\n\t\t\t// Since this is the last route for this remote, possibly update\n\t\t\t// the client connect URLs and send an update to connected\n\t\t\t// clients.\n\t\t\tif (len(connectURLs) > 0 || len(wsConnectURLs) > 0) && !opts.Cluster.NoAdvertise {\n\t\t\t\ts.removeConnectURLsAndSendINFOToClients(connectURLs, wsConnectURLs)\n\t\t\t}\n\t\t\t// Remove the remote's gateway URL from our list and\n\t\t\t// send update to inbound Gateway connections.\n\t\t\tif gwURL != _EMPTY_ && s.removeGatewayURL(gwURL) {\n\t\t\t\ts.sendAsyncGatewayInfo()\n\t\t\t}\n\t\t\t// Remove the remote's leafNode URL from\n\t\t\t// our list and send update to LN connections.\n\t\t\tif lnURL != _EMPTY_ && s.removeLeafNodeURL(lnURL) {\n\t\t\t\ts.sendAsyncLeafNodeInfo()\n\t\t\t}\n\t\t\t// We can remove the configured route pool size of this remote.\n\t\t\tdelete(s.remoteRoutePoolSize, rID)\n\t\t\t// If this server has pooling/pinned accounts and the route for\n\t\t\t// this remote was a \"no pool\" route, attempt to reconnect.\n\t\t\tif noPool {\n\t\t\t\tif s.routesPoolSize > 1 {\n\t\t\t\t\ts.startGoRoutine(func() { s.connectToRoute(rURL, rtype, true, gossipDefault, _EMPTY_) })\n\t\t\t\t}\n\t\t\t\tif len(opts.Cluster.PinnedAccounts) > 0 {\n\t\t\t\t\tfor _, an := range opts.Cluster.PinnedAccounts {\n\t\t\t\t\t\taccName := an\n\t\t\t\t\t\ts.startGoRoutine(func() { s.connectToRoute(rURL, rtype, true, gossipDefault, accName) })\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t// This is for gateway code. Remove this route from a map that uses\n\t\t// the route hash in combination with the pool index as the key.\n\t\tif s.routesPoolSize > 1 {\n\t\t\tidHash += strconv.Itoa(poolIdx)\n\t\t}\n\t\ts.removeRouteByHash(idHash)\n\t}\n\ts.removeFromTempClients(cid)\n}\n\nfunc (s *Server) isDuplicateServerName(name string) bool {\n\tif name == _EMPTY_ {\n\t\treturn false\n\t}\n\ts.mu.RLock()\n\tdefer s.mu.RUnlock()\n\n\tif s.info.Name == name {\n\t\treturn true\n\t}\n\tfor _, conns := range s.routes {\n\t\tfor _, r := range conns {\n\t\t\tif r != nil {\n\t\t\t\tr.mu.Lock()\n\t\t\t\tduplicate := r.route.remoteName == name\n\t\t\t\tr.mu.Unlock()\n\t\t\t\tif duplicate {\n\t\t\t\t\treturn true\n\t\t\t\t}\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t}\n\treturn false\n}\n\n// Goes over each non-nil route connection for all remote servers\n// and invokes the function `f`. It does not go over per-account\n// routes.\n// Server lock is held on entry.\nfunc (s *Server) forEachNonPerAccountRoute(f func(r *client)) {\n\tfor _, conns := range s.routes {\n\t\tfor _, r := range conns {\n\t\t\tif r != nil {\n\t\t\t\tf(r)\n\t\t\t}\n\t\t}\n\t}\n}\n\n// Goes over each non-nil route connection for all remote servers\n// and invokes the function `f`. This also includes the per-account\n// routes.\n// Server lock is held on entry.\nfunc (s *Server) forEachRoute(f func(r *client)) {\n\ts.forEachNonPerAccountRoute(f)\n\tfor _, conns := range s.accRoutes {\n\t\tfor _, r := range conns {\n\t\t\tf(r)\n\t\t}\n\t}\n}\n\n// Goes over each non-nil route connection at the given pool index\n// location in the slice and invokes the function `f`. If the\n// callback returns `true`, this function moves to the next remote.\n// Otherwise, the iteration over removes stops.\n// This does not include per-account routes.\n// Server lock is held on entry.\nfunc (s *Server) forEachRouteIdx(idx int, f func(r *client) bool) {\n\tfor _, conns := range s.routes {\n\t\tif r := conns[idx]; r != nil {\n\t\t\tif !f(r) {\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}\n}\n\n// Goes over each remote and for the first non nil route connection,\n// invokes the function `f`.\n// Server lock is held on entry.\nfunc (s *Server) forEachRemote(f func(r *client)) {\n\tfor _, conns := range s.routes {\n\t\tfor _, r := range conns {\n\t\t\tif r != nil {\n\t\t\t\tf(r)\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t}\n}\n",
    "source_file": "server/route.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2012-2025 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage server\n\nimport (\n\t\"bytes\"\n\t\"crypto/sha256\"\n\t\"crypto/tls\"\n\t\"crypto/x509\"\n\t\"encoding/hex\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"math/rand\"\n\t\"net\"\n\t\"net/http\"\n\t\"net/url\"\n\t\"regexp\"\n\t\"runtime\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t\"github.com/klauspost/compress/s2\"\n\t\"github.com/nats-io/jwt/v2\"\n\t\"github.com/nats-io/nats-server/v2/internal/fastrand\"\n)\n\n// Type of client connection.\nconst (\n\t// CLIENT is an end user.\n\tCLIENT = iota\n\t// ROUTER represents another server in the cluster.\n\tROUTER\n\t// GATEWAY is a link between 2 clusters.\n\tGATEWAY\n\t// SYSTEM is an internal system client.\n\tSYSTEM\n\t// LEAF is for leaf node connections.\n\tLEAF\n\t// JETSTREAM is an internal jetstream client.\n\tJETSTREAM\n\t// ACCOUNT is for the internal client for accounts.\n\tACCOUNT\n)\n\n// Internal clients. kind should be SYSTEM, JETSTREAM or ACCOUNT\nfunc isInternalClient(kind int) bool {\n\treturn kind == SYSTEM || kind == JETSTREAM || kind == ACCOUNT\n}\n\n// Extended type of a CLIENT connection. This is returned by c.clientType()\n// and indicate what type of client connection we are dealing with.\n// If invoked on a non CLIENT connection, NON_CLIENT type is returned.\nconst (\n\t// If the connection is not a CLIENT connection.\n\tNON_CLIENT = iota\n\t// Regular NATS client.\n\tNATS\n\t// MQTT client.\n\tMQTT\n\t// Websocket client.\n\tWS\n)\n\nconst (\n\t// ClientProtoZero is the original Client protocol from 2009.\n\t// http://nats.io/documentation/internals/nats-protocol/\n\tClientProtoZero = iota\n\t// ClientProtoInfo signals a client can receive more then the original INFO block.\n\t// This can be used to update clients on other cluster members, etc.\n\tClientProtoInfo\n)\n\nconst (\n\tpingProto = \"PING\" + _CRLF_\n\tpongProto = \"PONG\" + _CRLF_\n\terrProto  = \"-ERR '%s'\" + _CRLF_\n\tokProto   = \"+OK\" + _CRLF_\n)\n\n// TLS Hanshake client types\nconst (\n\ttlsHandshakeLeaf = \"leafnode\"\n\ttlsHandshakeMQTT = \"mqtt\"\n)\n\nconst (\n\t// Scratch buffer size for the processMsg() calls.\n\tmsgScratchSize  = 1024\n\tmsgHeadProto    = \"RMSG \"\n\tmsgHeadProtoLen = len(msgHeadProto)\n\n\t// For controlling dynamic buffer sizes.\n\tstartBufSize    = 512   // For INFO/CONNECT block\n\tminBufSize      = 64    // Smallest to shrink to for PING/PONG\n\tmaxBufSize      = 65536 // 64k\n\tshortsToShrink  = 2     // Trigger to shrink dynamic buffers\n\tmaxFlushPending = 10    // Max fsps to have in order to wait for writeLoop\n\treadLoopReport  = 2 * time.Second\n\n\t// Server should not send a PING (for RTT) before the first PONG has\n\t// been sent to the client. However, in case some client libs don't\n\t// send CONNECT+PING, cap the maximum time before server can send\n\t// the RTT PING.\n\tmaxNoRTTPingBeforeFirstPong = 2 * time.Second\n\n\t// For stalling fast producers\n\tstallClientMinDuration = 2 * time.Millisecond\n\tstallClientMaxDuration = 5 * time.Millisecond\n\tstallTotalAllowed      = 10 * time.Millisecond\n)\n\nvar readLoopReportThreshold = readLoopReport\n\n// Represent client booleans with a bitmask\ntype clientFlag uint16\n\nconst (\n\thdrLine      = \"NATS/1.0\\r\\n\"\n\temptyHdrLine = \"NATS/1.0\\r\\n\\r\\n\"\n)\n\n// Some client state represented as flags\nconst (\n\tconnectReceived        clientFlag = 1 << iota // The CONNECT proto has been received\n\tinfoReceived                                  // The INFO protocol has been received\n\tfirstPongSent                                 // The first PONG has been sent\n\thandshakeComplete                             // For TLS clients, indicate that the handshake is complete\n\tflushOutbound                                 // Marks client as having a flushOutbound call in progress.\n\tnoReconnect                                   // Indicate that on close, this connection should not attempt a reconnect\n\tcloseConnection                               // Marks that closeConnection has already been called.\n\tconnMarkedClosed                              // Marks that markConnAsClosed has already been called.\n\twriteLoopStarted                              // Marks that the writeLoop has been started.\n\tskipFlushOnClose                              // Marks that flushOutbound() should not be called on connection close.\n\texpectConnect                                 // Marks if this connection is expected to send a CONNECT\n\tconnectProcessFinished                        // Marks if this connection has finished the connect process.\n\tcompressionNegotiated                         // Marks if this connection has negotiated compression level with remote.\n\tdidTLSFirst                                   // Marks if this connection requested and was accepted doing the TLS handshake first (prior to INFO).\n\tisSlowConsumer                                // Marks connection as a slow consumer.\n)\n\n// set the flag (would be equivalent to set the boolean to true)\nfunc (cf *clientFlag) set(c clientFlag) {\n\t*cf |= c\n}\n\n// clear the flag (would be equivalent to set the boolean to false)\nfunc (cf *clientFlag) clear(c clientFlag) {\n\t*cf &= ^c\n}\n\n// isSet returns true if the flag is set, false otherwise\nfunc (cf clientFlag) isSet(c clientFlag) bool {\n\treturn cf&c != 0\n}\n\n// setIfNotSet will set the flag `c` only if that flag was not already\n// set and return true to indicate that the flag has been set. Returns\n// false otherwise.\nfunc (cf *clientFlag) setIfNotSet(c clientFlag) bool {\n\tif *cf&c == 0 {\n\t\t*cf |= c\n\t\treturn true\n\t}\n\treturn false\n}\n\n// ClosedState is the reason client was closed. This will\n// be passed into calls to clearConnection, but will only\n// be stored in ConnInfo for monitoring.\ntype ClosedState int\n\nconst (\n\tClientClosed = ClosedState(iota + 1)\n\tAuthenticationTimeout\n\tAuthenticationViolation\n\tTLSHandshakeError\n\tSlowConsumerPendingBytes\n\tSlowConsumerWriteDeadline\n\tWriteError\n\tReadError\n\tParseError\n\tStaleConnection\n\tProtocolViolation\n\tBadClientProtocolVersion\n\tWrongPort\n\tMaxAccountConnectionsExceeded\n\tMaxConnectionsExceeded\n\tMaxPayloadExceeded\n\tMaxControlLineExceeded\n\tMaxSubscriptionsExceeded\n\tDuplicateRoute\n\tRouteRemoved\n\tServerShutdown\n\tAuthenticationExpired\n\tWrongGateway\n\tMissingAccount\n\tRevocation\n\tInternalClient\n\tMsgHeaderViolation\n\tNoRespondersRequiresHeaders\n\tClusterNameConflict\n\tDuplicateRemoteLeafnodeConnection\n\tDuplicateClientID\n\tDuplicateServerName\n\tMinimumVersionRequired\n\tClusterNamesIdentical\n\tKicked\n)\n\n// Some flags passed to processMsgResults\nconst pmrNoFlag int = 0\nconst (\n\tpmrCollectQueueNames int = 1 << iota\n\tpmrIgnoreEmptyQueueFilter\n\tpmrAllowSendFromRouteToRoute\n\tpmrMsgImportedFromService\n)\n\ntype client struct {\n\t// Here first because of use of atomics, and memory alignment.\n\tstats\n\tgwReplyMapping\n\tkind  int\n\tsrv   *Server\n\tacc   *Account\n\tperms *permissions\n\tin    readCache\n\tparseState\n\topts       ClientOpts\n\trrTracking *rrTracking\n\tmpay       int32\n\tmsubs      int32\n\tmcl        int32\n\tmu         sync.Mutex\n\tcid        uint64\n\tstart      time.Time\n\tnonce      []byte\n\tpubKey     string\n\tnc         net.Conn\n\tncs        atomic.Value\n\tout        outbound\n\tuser       *NkeyUser\n\thost       string\n\tport       uint16\n\tsubs       map[string]*subscription\n\treplies    map[string]*resp\n\tmperms     *msgDeny\n\tdarray     []string\n\tpcd        map[*client]struct{}\n\tatmr       *time.Timer\n\texpires    time.Time\n\tping       pinfo\n\tmsgb       [msgScratchSize]byte\n\tlast       time.Time\n\tlastIn     time.Time\n\n\trepliesSincePrune uint16\n\tlastReplyPrune    time.Time\n\n\theaders bool\n\n\trtt      time.Duration\n\trttStart time.Time\n\n\troute *route\n\tgw    *gateway\n\tleaf  *leaf\n\tws    *websocket\n\tmqtt  *mqtt\n\n\tflags clientFlag // Compact booleans into a single field. Size will be increased when needed.\n\n\trref byte\n\n\ttrace bool\n\techo  bool\n\tnoIcb bool\n\tiproc bool // In-Process connection, set at creation and immutable.\n\n\ttags    jwt.TagList\n\tnameTag string\n\n\ttlsTo *time.Timer\n}\n\ntype rrTracking struct {\n\trmap map[string]*remoteLatency\n\tptmr *time.Timer\n\tlrt  time.Duration\n}\n\n// Struct for PING initiation from the server.\ntype pinfo struct {\n\ttmr *time.Timer\n\tout int\n}\n\n// outbound holds pending data for a socket.\ntype outbound struct {\n\tnb  net.Buffers   // Pending buffers for send, each has fixed capacity as per nbPool below.\n\twnb net.Buffers   // Working copy of \"nb\", reused on each flushOutbound call, partial writes may leave entries here for next iteration.\n\tpb  int64         // Total pending/queued bytes.\n\tfsp int32         // Flush signals that are pending per producer from readLoop's pcd.\n\tsg  *sync.Cond    // To signal writeLoop that there is data to flush.\n\twdl time.Duration // Snapshot of write deadline.\n\tmp  int64         // Snapshot of max pending for client.\n\tlft time.Duration // Last flush time for Write.\n\tstc chan struct{} // Stall chan we create to slow down producers on overrun, e.g. fan-in.\n\tcw  *s2.Writer\n}\n\nconst nbMaxVectorSize = 1024 // == IOV_MAX on Linux/Darwin and most other Unices (except Solaris/AIX)\n\nconst nbPoolSizeSmall = 512   // Underlying array size of small buffer\nconst nbPoolSizeMedium = 4096 // Underlying array size of medium buffer\nconst nbPoolSizeLarge = 65536 // Underlying array size of large buffer\n\nvar nbPoolSmall = &sync.Pool{\n\tNew: func() any {\n\t\tb := [nbPoolSizeSmall]byte{}\n\t\treturn &b\n\t},\n}\n\nvar nbPoolMedium = &sync.Pool{\n\tNew: func() any {\n\t\tb := [nbPoolSizeMedium]byte{}\n\t\treturn &b\n\t},\n}\n\nvar nbPoolLarge = &sync.Pool{\n\tNew: func() any {\n\t\tb := [nbPoolSizeLarge]byte{}\n\t\treturn &b\n\t},\n}\n\n// nbPoolGet returns a frame that is a best-effort match for the given size.\n// Once a pooled frame is no longer needed, it should be recycled by passing\n// it to nbPoolPut.\nfunc nbPoolGet(sz int) []byte {\n\tswitch {\n\tcase sz <= nbPoolSizeSmall:\n\t\treturn nbPoolSmall.Get().(*[nbPoolSizeSmall]byte)[:0]\n\tcase sz <= nbPoolSizeMedium:\n\t\treturn nbPoolMedium.Get().(*[nbPoolSizeMedium]byte)[:0]\n\tdefault:\n\t\treturn nbPoolLarge.Get().(*[nbPoolSizeLarge]byte)[:0]\n\t}\n}\n\n// nbPoolPut recycles a frame that was retrieved from nbPoolGet. It is not\n// safe to return multiple slices referring to chunks of the same underlying\n// array as this may create overlaps when the buffers are returned to their\n// original size, resulting in race conditions.\nfunc nbPoolPut(b []byte) {\n\tswitch cap(b) {\n\tcase nbPoolSizeSmall:\n\t\tb := (*[nbPoolSizeSmall]byte)(b[0:nbPoolSizeSmall])\n\t\tnbPoolSmall.Put(b)\n\tcase nbPoolSizeMedium:\n\t\tb := (*[nbPoolSizeMedium]byte)(b[0:nbPoolSizeMedium])\n\t\tnbPoolMedium.Put(b)\n\tcase nbPoolSizeLarge:\n\t\tb := (*[nbPoolSizeLarge]byte)(b[0:nbPoolSizeLarge])\n\t\tnbPoolLarge.Put(b)\n\tdefault:\n\t\t// Ignore frames that are the wrong size, this might happen\n\t\t// with WebSocket/MQTT messages as they are framed\n\t}\n}\n\ntype perm struct {\n\tallow *Sublist\n\tdeny  *Sublist\n}\n\ntype permissions struct {\n\t// Have these 2 first for memory alignment due to the use of atomic.\n\tpcsz   int32\n\tprun   int32\n\tsub    perm\n\tpub    perm\n\tresp   *ResponsePermission\n\tpcache sync.Map\n}\n\n// This is used to dynamically track responses and reply subjects\n// for dynamic permissioning.\ntype resp struct {\n\tt time.Time\n\tn int\n}\n\n// msgDeny is used when a user permission for subscriptions has a deny\n// clause but a subscription could be made that is of broader scope.\n// e.g. deny = \"foo\", but user subscribes to \"*\". That subscription should\n// succeed but no message sent on foo should be delivered.\ntype msgDeny struct {\n\tdeny   *Sublist\n\tdcache map[string]bool\n}\n\n// routeTarget collects information regarding routes and queue groups for\n// sending information to a remote.\ntype routeTarget struct {\n\tsub *subscription\n\tqs  []byte\n\t_qs [32]byte\n}\n\nconst (\n\tmaxResultCacheSize   = 512\n\tmaxDenyPermCacheSize = 256\n\tmaxPermCacheSize     = 128\n\tpruneSize            = 32\n\trouteTargetInit      = 8\n\treplyPermLimit       = 4096\n\treplyPruneTime       = time.Second\n)\n\n// Represent read cache booleans with a bitmask\ntype readCacheFlag uint16\n\nconst (\n\thasMappings         readCacheFlag = 1 << iota // For account subject mappings.\n\tswitchToCompression readCacheFlag = 1 << 1\n)\n\nconst sysGroup = \"_sys_\"\n\n// Used in readloop to cache hot subject lookups and group statistics.\ntype readCache struct {\n\t// These are for clients who are bound to a single account.\n\tgenid   uint64\n\tresults map[string]*SublistResult\n\n\t// This is for routes and gateways to have their own L1 as well that is account aware.\n\tpacache map[string]*perAccountCache\n\n\t// This is for when we deliver messages across a route. We use this structure\n\t// to make sure to only send one message and properly scope to queues as needed.\n\trts []routeTarget\n\n\t// These are all temporary totals for an invocation of a read in readloop.\n\tmsgs  int32\n\tbytes int32\n\tsubs  int32\n\n\trsz int32 // Read buffer size\n\tsrs int32 // Short reads, used for dynamic buffer resizing.\n\n\t// These are for readcache flags to avoid locks.\n\tflags readCacheFlag\n\n\t// Capture the time we started processing our readLoop.\n\tstart time.Time\n\n\t// Total time stalled so far for readLoop processing.\n\ttst time.Duration\n}\n\n// set the flag (would be equivalent to set the boolean to true)\nfunc (rcf *readCacheFlag) set(c readCacheFlag) {\n\t*rcf |= c\n}\n\n// clear the flag (would be equivalent to set the boolean to false)\nfunc (rcf *readCacheFlag) clear(c readCacheFlag) {\n\t*rcf &= ^c\n}\n\n// isSet returns true if the flag is set, false otherwise\nfunc (rcf readCacheFlag) isSet(c readCacheFlag) bool {\n\treturn rcf&c != 0\n}\n\nconst (\n\tdefaultMaxPerAccountCacheSize   = 8192\n\tdefaultPrunePerAccountCacheSize = 1024\n\tdefaultClosedSubsCheckInterval  = 5 * time.Minute\n)\n\nvar (\n\tmaxPerAccountCacheSize   = defaultMaxPerAccountCacheSize\n\tprunePerAccountCacheSize = defaultPrunePerAccountCacheSize\n\tclosedSubsCheckInterval  = defaultClosedSubsCheckInterval\n)\n\n// perAccountCache is for L1 semantics for inbound messages from a route or gateway to mimic the performance of clients.\ntype perAccountCache struct {\n\tacc     *Account\n\tresults *SublistResult\n\tgenid   uint64\n}\n\nfunc (c *client) String() (id string) {\n\tloaded := c.ncs.Load()\n\tif loaded != nil {\n\t\treturn loaded.(string)\n\t}\n\n\treturn _EMPTY_\n}\n\n// GetNonce returns the nonce that was presented to the user on connection\nfunc (c *client) GetNonce() []byte {\n\tc.mu.Lock()\n\tdefer c.mu.Unlock()\n\n\treturn c.nonce\n}\n\n// GetName returns the application supplied name for the connection.\nfunc (c *client) GetName() string {\n\tc.mu.Lock()\n\tname := c.opts.Name\n\tc.mu.Unlock()\n\treturn name\n}\n\n// GetOpts returns the client options provided by the application.\nfunc (c *client) GetOpts() *ClientOpts {\n\treturn &c.opts\n}\n\n// GetTLSConnectionState returns the TLS ConnectionState if TLS is enabled, nil\n// otherwise. Implements the ClientAuth interface.\nfunc (c *client) GetTLSConnectionState() *tls.ConnectionState {\n\tc.mu.Lock()\n\tdefer c.mu.Unlock()\n\tif c.nc == nil {\n\t\treturn nil\n\t}\n\ttc, ok := c.nc.(*tls.Conn)\n\tif !ok {\n\t\treturn nil\n\t}\n\tstate := tc.ConnectionState()\n\treturn &state\n}\n\n// For CLIENT connections, this function returns the client type, that is,\n// NATS (for regular clients), MQTT or WS for websocket.\n// If this is invoked for a non CLIENT connection, NON_CLIENT is returned.\n//\n// This function does not lock the client and accesses fields that are supposed\n// to be immutable and therefore it can be invoked outside of the client's lock.\nfunc (c *client) clientType() int {\n\tswitch c.kind {\n\tcase CLIENT:\n\t\tif c.isMqtt() {\n\t\t\treturn MQTT\n\t\t} else if c.isWebsocket() {\n\t\t\treturn WS\n\t\t}\n\t\treturn NATS\n\tdefault:\n\t\treturn NON_CLIENT\n\t}\n}\n\nvar clientTypeStringMap = map[int]string{\n\tNON_CLIENT: _EMPTY_,\n\tNATS:       \"nats\",\n\tWS:         \"websocket\",\n\tMQTT:       \"mqtt\",\n}\n\nfunc (c *client) clientTypeString() string {\n\tif typeStringVal, ok := clientTypeStringMap[c.clientType()]; ok {\n\t\treturn typeStringVal\n\t}\n\treturn _EMPTY_\n}\n\n// This is the main subscription struct that indicates\n// interest in published messages.\n// FIXME(dlc) - This is getting bloated for normal subs, need\n// to optionally have an opts section for non-normal stuff.\ntype subscription struct {\n\tclient  *client\n\tim      *streamImport // This is for import stream support.\n\trsi     bool\n\tsi      bool\n\tshadow  []*subscription // This is to track shadowed accounts.\n\ticb     msgHandler\n\tsubject []byte\n\tqueue   []byte\n\tsid     []byte\n\torigin  []byte\n\tnm      int64\n\tmax     int64\n\tqw      int32\n\tclosed  int32\n\tmqtt    *mqttSub\n}\n\n// Indicate that this subscription is closed.\n// This is used in pruning of route and gateway cache items.\nfunc (s *subscription) close() {\n\tatomic.StoreInt32(&s.closed, 1)\n}\n\n// Return true if this subscription was unsubscribed\n// or its connection has been closed.\nfunc (s *subscription) isClosed() bool {\n\treturn atomic.LoadInt32(&s.closed) == 1\n}\n\ntype ClientOpts struct {\n\tEcho         bool   `json:\"echo\"`\n\tVerbose      bool   `json:\"verbose\"`\n\tPedantic     bool   `json:\"pedantic\"`\n\tTLSRequired  bool   `json:\"tls_required\"`\n\tNkey         string `json:\"nkey,omitempty\"`\n\tJWT          string `json:\"jwt,omitempty\"`\n\tSig          string `json:\"sig,omitempty\"`\n\tToken        string `json:\"auth_token,omitempty\"`\n\tUsername     string `json:\"user,omitempty\"`\n\tPassword     string `json:\"pass,omitempty\"`\n\tName         string `json:\"name\"`\n\tLang         string `json:\"lang\"`\n\tVersion      string `json:\"version\"`\n\tProtocol     int    `json:\"protocol\"`\n\tAccount      string `json:\"account,omitempty\"`\n\tAccountNew   bool   `json:\"new_account,omitempty\"`\n\tHeaders      bool   `json:\"headers,omitempty\"`\n\tNoResponders bool   `json:\"no_responders,omitempty\"`\n\n\t// Routes and Leafnodes only\n\tImport *SubjectPermission `json:\"import,omitempty\"`\n\tExport *SubjectPermission `json:\"export,omitempty\"`\n\n\t// Leafnodes\n\tRemoteAccount string `json:\"remote_account,omitempty\"`\n}\n\nvar defaultOpts = ClientOpts{Verbose: true, Pedantic: true, Echo: true}\nvar internalOpts = ClientOpts{Verbose: false, Pedantic: false, Echo: false}\n\nfunc (c *client) setTraceLevel() {\n\tif c.kind == SYSTEM && !(atomic.LoadInt32(&c.srv.logging.traceSysAcc) != 0) {\n\t\tc.trace = false\n\t} else {\n\t\tc.trace = (atomic.LoadInt32(&c.srv.logging.trace) != 0)\n\t}\n}\n\n// Lock should be held\nfunc (c *client) initClient() {\n\ts := c.srv\n\tc.cid = atomic.AddUint64(&s.gcid, 1)\n\n\t// Outbound data structure setup\n\tc.out.sg = sync.NewCond(&(c.mu))\n\topts := s.getOpts()\n\t// Snapshots to avoid mutex access in fast paths.\n\tc.out.wdl = opts.WriteDeadline\n\tc.out.mp = opts.MaxPending\n\t// Snapshot max control line since currently can not be changed on reload and we\n\t// were checking it on each call to parse. If this changes and we allow MaxControlLine\n\t// to be reloaded without restart, this code will need to change.\n\tc.mcl = int32(opts.MaxControlLine)\n\tif c.mcl == 0 {\n\t\tc.mcl = MAX_CONTROL_LINE_SIZE\n\t}\n\n\tc.subs = make(map[string]*subscription)\n\tc.echo = true\n\n\tc.setTraceLevel()\n\n\t// This is a scratch buffer used for processMsg()\n\t// The msg header starts with \"RMSG \", which can be used\n\t// for both local and routes.\n\t// in bytes that is [82 77 83 71 32].\n\tc.msgb = [msgScratchSize]byte{82, 77, 83, 71, 32}\n\n\t// This is to track pending clients that have data to be flushed\n\t// after we process inbound msgs from our own connection.\n\tc.pcd = make(map[*client]struct{})\n\n\t// snapshot the string version of the connection\n\tvar conn string\n\tif c.nc != nil {\n\t\tif addr := c.nc.RemoteAddr(); addr != nil {\n\t\t\tif conn = addr.String(); conn != _EMPTY_ {\n\t\t\t\thost, port, _ := net.SplitHostPort(conn)\n\t\t\t\tiPort, _ := strconv.Atoi(port)\n\t\t\t\tc.host, c.port = host, uint16(iPort)\n\t\t\t\tif c.isWebsocket() && c.ws.clientIP != _EMPTY_ {\n\t\t\t\t\tcip := c.ws.clientIP\n\t\t\t\t\t// Surround IPv6 addresses with square brackets, as\n\t\t\t\t\t// net.JoinHostPort would do...\n\t\t\t\t\tif strings.Contains(cip, \":\") {\n\t\t\t\t\t\tcip = \"[\" + cip + \"]\"\n\t\t\t\t\t}\n\t\t\t\t\tconn = fmt.Sprintf(\"%s/%s\", cip, conn)\n\t\t\t\t}\n\t\t\t\t// Now that we have extracted host and port, escape\n\t\t\t\t// the string because it is going to be used in Sprintf\n\t\t\t\tconn = strings.ReplaceAll(conn, \"%\", \"%%\")\n\t\t\t}\n\t\t}\n\t}\n\n\tswitch c.kind {\n\tcase CLIENT:\n\t\tswitch c.clientType() {\n\t\tcase NATS:\n\t\t\tc.ncs.Store(fmt.Sprintf(\"%s - cid:%d\", conn, c.cid))\n\t\tcase WS:\n\t\t\tc.ncs.Store(fmt.Sprintf(\"%s - wid:%d\", conn, c.cid))\n\t\tcase MQTT:\n\t\t\tvar ws string\n\t\t\tif c.isWebsocket() {\n\t\t\t\tws = \"_ws\"\n\t\t\t}\n\t\t\tc.ncs.Store(fmt.Sprintf(\"%s - mid%s:%d\", conn, ws, c.cid))\n\t\t}\n\tcase ROUTER:\n\t\tc.ncs.Store(fmt.Sprintf(\"%s - rid:%d\", conn, c.cid))\n\tcase GATEWAY:\n\t\tc.ncs.Store(fmt.Sprintf(\"%s - gid:%d\", conn, c.cid))\n\tcase LEAF:\n\t\tvar ws string\n\t\tif c.isWebsocket() {\n\t\t\tws = \"_ws\"\n\t\t}\n\t\tc.ncs.Store(fmt.Sprintf(\"%s - lid%s:%d\", conn, ws, c.cid))\n\tcase SYSTEM:\n\t\tc.ncs.Store(\"SYSTEM\")\n\tcase JETSTREAM:\n\t\tc.ncs.Store(\"JETSTREAM\")\n\tcase ACCOUNT:\n\t\tc.ncs.Store(\"ACCOUNT\")\n\t}\n}\n\n// RemoteAddress expose the Address of the client connection,\n// nil when not connected or unknown\nfunc (c *client) RemoteAddress() net.Addr {\n\tc.mu.Lock()\n\tdefer c.mu.Unlock()\n\n\tif c.nc == nil {\n\t\treturn nil\n\t}\n\n\treturn c.nc.RemoteAddr()\n}\n\n// Helper function to report errors.\nfunc (c *client) reportErrRegisterAccount(acc *Account, err error) {\n\tif err == ErrTooManyAccountConnections {\n\t\tc.maxAccountConnExceeded()\n\t\treturn\n\t}\n\tc.Errorf(\"Problem registering with account %q: %s\", acc.Name, err)\n\tc.sendErr(\"Failed Account Registration\")\n}\n\n// Kind returns the client kind and will be one of the defined constants like CLIENT, ROUTER, GATEWAY, LEAF\nfunc (c *client) Kind() int {\n\tc.mu.Lock()\n\tkind := c.kind\n\tc.mu.Unlock()\n\n\treturn kind\n}\n\n// registerWithAccount will register the given user with a specific\n// account. This will change the subject namespace.\nfunc (c *client) registerWithAccount(acc *Account) error {\n\tif acc == nil {\n\t\treturn ErrBadAccount\n\t}\n\tacc.mu.RLock()\n\tbad := acc.sl == nil\n\tacc.mu.RUnlock()\n\tif bad {\n\t\treturn ErrBadAccount\n\t}\n\t// If we were previously registered, usually to $G, do accounting here to remove.\n\tif c.acc != nil {\n\t\tif prev := c.acc.removeClient(c); prev == 1 && c.srv != nil {\n\t\t\tc.srv.decActiveAccounts()\n\t\t}\n\t}\n\n\tc.mu.Lock()\n\tkind := c.kind\n\tsrv := c.srv\n\tc.acc = acc\n\tc.applyAccountLimits()\n\tc.mu.Unlock()\n\n\t// Check if we have a max connections violation\n\tif kind == CLIENT && acc.MaxTotalConnectionsReached() {\n\t\treturn ErrTooManyAccountConnections\n\t} else if kind == LEAF {\n\t\t// Check if we are already connected to this cluster.\n\t\tif rc := c.remoteCluster(); rc != _EMPTY_ && acc.hasLeafNodeCluster(rc) {\n\t\t\treturn ErrLeafNodeLoop\n\t\t}\n\t\tif acc.MaxTotalLeafNodesReached() {\n\t\t\treturn ErrTooManyAccountConnections\n\t\t}\n\t}\n\n\t// Add in new one.\n\tif prev := acc.addClient(c); prev == 0 && srv != nil {\n\t\tsrv.incActiveAccounts()\n\t}\n\n\treturn nil\n}\n\n// Helper to determine if we have met or exceeded max subs.\nfunc (c *client) subsAtLimit() bool {\n\treturn c.msubs != jwt.NoLimit && len(c.subs) >= int(c.msubs)\n}\n\nfunc minLimit(value *int32, limit int32) bool {\n\tv := atomic.LoadInt32(value)\n\tif v != jwt.NoLimit {\n\t\tif limit != jwt.NoLimit {\n\t\t\tif limit < v {\n\t\t\t\tatomic.StoreInt32(value, limit)\n\t\t\t\treturn true\n\t\t\t}\n\t\t}\n\t} else if limit != jwt.NoLimit {\n\t\tatomic.StoreInt32(value, limit)\n\t\treturn true\n\t}\n\treturn false\n}\n\n// Apply account limits\n// Lock is held on entry.\n// FIXME(dlc) - Should server be able to override here?\nfunc (c *client) applyAccountLimits() {\n\tif c.acc == nil || (c.kind != CLIENT && c.kind != LEAF) {\n\t\treturn\n\t}\n\tatomic.StoreInt32(&c.mpay, jwt.NoLimit)\n\tc.msubs = jwt.NoLimit\n\tif c.opts.JWT != _EMPTY_ { // user jwt implies account\n\t\tif uc, _ := jwt.DecodeUserClaims(c.opts.JWT); uc != nil {\n\t\t\tatomic.StoreInt32(&c.mpay, int32(uc.Limits.Payload))\n\t\t\tc.msubs = int32(uc.Limits.Subs)\n\t\t\tif uc.IssuerAccount != _EMPTY_ && uc.IssuerAccount != uc.Issuer {\n\t\t\t\tif scope, ok := c.acc.signingKeys[uc.Issuer]; ok {\n\t\t\t\t\tif userScope, ok := scope.(*jwt.UserScope); ok {\n\t\t\t\t\t\t// if signing key disappeared or changed and we don't get here, the client will be disconnected\n\t\t\t\t\t\tc.mpay = int32(userScope.Template.Limits.Payload)\n\t\t\t\t\t\tc.msubs = int32(userScope.Template.Limits.Subs)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tc.acc.mu.RLock()\n\tminLimit(&c.mpay, c.acc.mpay)\n\tminLimit(&c.msubs, c.acc.msubs)\n\tc.acc.mu.RUnlock()\n\n\ts := c.srv\n\topts := s.getOpts()\n\tmPay := opts.MaxPayload\n\t// options encode unlimited differently\n\tif mPay == 0 {\n\t\tmPay = jwt.NoLimit\n\t}\n\tmSubs := int32(opts.MaxSubs)\n\tif mSubs == 0 {\n\t\tmSubs = jwt.NoLimit\n\t}\n\twasUnlimited := c.mpay == jwt.NoLimit\n\tif minLimit(&c.mpay, mPay) && !wasUnlimited {\n\t\tc.Errorf(\"Max Payload set to %d from server overrides account or user config\", opts.MaxPayload)\n\t}\n\twasUnlimited = c.msubs == jwt.NoLimit\n\tif minLimit(&c.msubs, mSubs) && !wasUnlimited {\n\t\tc.Errorf(\"Max Subscriptions set to %d from server overrides account or user config\", opts.MaxSubs)\n\t}\n\tif c.subsAtLimit() {\n\t\tgo func() {\n\t\t\tc.maxSubsExceeded()\n\t\t\ttime.Sleep(20 * time.Millisecond)\n\t\t\tc.closeConnection(MaxSubscriptionsExceeded)\n\t\t}()\n\t}\n}\n\n// RegisterUser allows auth to call back into a new client\n// with the authenticated user. This is used to map\n// any permissions into the client and setup accounts.\nfunc (c *client) RegisterUser(user *User) {\n\t// Register with proper account and sublist.\n\tif user.Account != nil {\n\t\tif err := c.registerWithAccount(user.Account); err != nil {\n\t\t\tc.reportErrRegisterAccount(user.Account, err)\n\t\t\treturn\n\t\t}\n\t}\n\n\tc.mu.Lock()\n\n\t// Assign permissions.\n\tif user.Permissions == nil {\n\t\t// Reset perms to nil in case client previously had them.\n\t\tc.perms = nil\n\t\tc.mperms = nil\n\t} else {\n\t\tc.setPermissions(user.Permissions)\n\t}\n\n\t// allows custom authenticators to set a username to be reported in\n\t// server events and more\n\tif user.Username != _EMPTY_ {\n\t\tc.opts.Username = user.Username\n\t}\n\n\t// if a deadline time stamp is set we start a timer to disconnect the user at that time\n\tif !user.ConnectionDeadline.IsZero() {\n\t\tc.setExpirationTimerUnlocked(time.Until(user.ConnectionDeadline))\n\t}\n\n\tc.mu.Unlock()\n}\n\n// RegisterNkeyUser allows auth to call back into a new nkey\n// client with the authenticated user. This is used to map\n// any permissions into the client and setup accounts.\nfunc (c *client) RegisterNkeyUser(user *NkeyUser) error {\n\t// Register with proper account and sublist.\n\tif user.Account != nil {\n\t\tif err := c.registerWithAccount(user.Account); err != nil {\n\t\t\tc.reportErrRegisterAccount(user.Account, err)\n\t\t\treturn err\n\t\t}\n\t}\n\n\tc.mu.Lock()\n\tc.user = user\n\t// Assign permissions.\n\tif user.Permissions == nil {\n\t\t// Reset perms to nil in case client previously had them.\n\t\tc.perms = nil\n\t\tc.mperms = nil\n\t} else {\n\t\tc.setPermissions(user.Permissions)\n\t}\n\tc.mu.Unlock()\n\treturn nil\n}\n\nfunc splitSubjectQueue(sq string) ([]byte, []byte, error) {\n\tvals := strings.Fields(strings.TrimSpace(sq))\n\ts := []byte(vals[0])\n\tvar q []byte\n\tif len(vals) == 2 {\n\t\tq = []byte(vals[1])\n\t} else if len(vals) > 2 {\n\t\treturn nil, nil, fmt.Errorf(\"invalid subject-queue %q\", sq)\n\t}\n\treturn s, q, nil\n}\n\n// Initializes client.perms structure.\n// Lock is held on entry.\nfunc (c *client) setPermissions(perms *Permissions) {\n\tif perms == nil {\n\t\treturn\n\t}\n\tc.perms = &permissions{}\n\n\t// Loop over publish permissions\n\tif perms.Publish != nil {\n\t\tif perms.Publish.Allow != nil {\n\t\t\tc.perms.pub.allow = NewSublistWithCache()\n\t\t}\n\t\tfor _, pubSubject := range perms.Publish.Allow {\n\t\t\tsub := &subscription{subject: []byte(pubSubject)}\n\t\t\tc.perms.pub.allow.Insert(sub)\n\t\t}\n\t\tif len(perms.Publish.Deny) > 0 {\n\t\t\tc.perms.pub.deny = NewSublistWithCache()\n\t\t}\n\t\tfor _, pubSubject := range perms.Publish.Deny {\n\t\t\tsub := &subscription{subject: []byte(pubSubject)}\n\t\t\tc.perms.pub.deny.Insert(sub)\n\t\t}\n\t}\n\n\t// Check if we are allowed to send responses.\n\tif perms.Response != nil {\n\t\trp := *perms.Response\n\t\tc.perms.resp = &rp\n\t\tc.replies = make(map[string]*resp)\n\t}\n\n\t// Loop over subscribe permissions\n\tif perms.Subscribe != nil {\n\t\tvar err error\n\t\tif len(perms.Subscribe.Allow) > 0 {\n\t\t\tc.perms.sub.allow = NewSublistWithCache()\n\t\t}\n\t\tfor _, subSubject := range perms.Subscribe.Allow {\n\t\t\tsub := &subscription{}\n\t\t\tsub.subject, sub.queue, err = splitSubjectQueue(subSubject)\n\t\t\tif err != nil {\n\t\t\t\tc.Errorf(\"%s\", err.Error())\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tc.perms.sub.allow.Insert(sub)\n\t\t}\n\t\tif len(perms.Subscribe.Deny) > 0 {\n\t\t\tc.perms.sub.deny = NewSublistWithCache()\n\t\t\t// Also hold onto this array for later.\n\t\t\tc.darray = perms.Subscribe.Deny\n\t\t}\n\t\tfor _, subSubject := range perms.Subscribe.Deny {\n\t\t\tsub := &subscription{}\n\t\t\tsub.subject, sub.queue, err = splitSubjectQueue(subSubject)\n\t\t\tif err != nil {\n\t\t\t\tc.Errorf(\"%s\", err.Error())\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tc.perms.sub.deny.Insert(sub)\n\t\t}\n\t}\n\n\t// If we are a leafnode and we are the hub copy the extracted perms\n\t// to resend back to soliciting server. These are reversed from the\n\t// way routes interpret them since this is how the soliciting server\n\t// will receive these back in an update INFO.\n\tif c.isHubLeafNode() {\n\t\tc.opts.Import = perms.Subscribe\n\t\tc.opts.Export = perms.Publish\n\t}\n}\n\n// Build public permissions from internal ones.\n// Used for user info requests.\nfunc (c *client) publicPermissions() *Permissions {\n\tc.mu.Lock()\n\tdefer c.mu.Unlock()\n\n\tif c.perms == nil {\n\t\treturn nil\n\t}\n\tperms := &Permissions{\n\t\tPublish:   &SubjectPermission{},\n\t\tSubscribe: &SubjectPermission{},\n\t}\n\n\t_subs := [32]*subscription{}\n\n\t// Publish\n\tif c.perms.pub.allow != nil {\n\t\tsubs := _subs[:0]\n\t\tc.perms.pub.allow.All(&subs)\n\t\tfor _, sub := range subs {\n\t\t\tperms.Publish.Allow = append(perms.Publish.Allow, string(sub.subject))\n\t\t}\n\t}\n\tif c.perms.pub.deny != nil {\n\t\tsubs := _subs[:0]\n\t\tc.perms.pub.deny.All(&subs)\n\t\tfor _, sub := range subs {\n\t\t\tperms.Publish.Deny = append(perms.Publish.Deny, string(sub.subject))\n\t\t}\n\t}\n\t// Subsribe\n\tif c.perms.sub.allow != nil {\n\t\tsubs := _subs[:0]\n\t\tc.perms.sub.allow.All(&subs)\n\t\tfor _, sub := range subs {\n\t\t\tperms.Subscribe.Allow = append(perms.Subscribe.Allow, string(sub.subject))\n\t\t}\n\t}\n\tif c.perms.sub.deny != nil {\n\t\tsubs := _subs[:0]\n\t\tc.perms.sub.deny.All(&subs)\n\t\tfor _, sub := range subs {\n\t\t\tperms.Subscribe.Deny = append(perms.Subscribe.Deny, string(sub.subject))\n\t\t}\n\t}\n\t// Responses.\n\tif c.perms.resp != nil {\n\t\trp := *c.perms.resp\n\t\tperms.Response = &rp\n\t}\n\n\treturn perms\n}\n\ntype denyType int\n\nconst (\n\tpub = denyType(iota + 1)\n\tsub\n\tboth\n)\n\n// Merge client.perms structure with additional pub deny permissions\n// Lock is held on entry.\nfunc (c *client) mergeDenyPermissions(what denyType, denyPubs []string) {\n\tif len(denyPubs) == 0 {\n\t\treturn\n\t}\n\tif c.perms == nil {\n\t\tc.perms = &permissions{}\n\t}\n\tvar perms []*perm\n\tswitch what {\n\tcase pub:\n\t\tperms = []*perm{&c.perms.pub}\n\tcase sub:\n\t\tperms = []*perm{&c.perms.sub}\n\tcase both:\n\t\tperms = []*perm{&c.perms.pub, &c.perms.sub}\n\t}\n\tfor _, p := range perms {\n\t\tif p.deny == nil {\n\t\t\tp.deny = NewSublistWithCache()\n\t\t}\n\tFOR_DENY:\n\t\tfor _, subj := range denyPubs {\n\t\t\tr := p.deny.Match(subj)\n\t\t\tfor _, v := range r.qsubs {\n\t\t\t\tfor _, s := range v {\n\t\t\t\t\tif string(s.subject) == subj {\n\t\t\t\t\t\tcontinue FOR_DENY\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tfor _, s := range r.psubs {\n\t\t\t\tif string(s.subject) == subj {\n\t\t\t\t\tcontinue FOR_DENY\n\t\t\t\t}\n\t\t\t}\n\t\t\tsub := &subscription{subject: []byte(subj)}\n\t\t\tp.deny.Insert(sub)\n\t\t}\n\t}\n}\n\n// Merge client.perms structure with additional pub deny permissions\n// Client lock must not be held on entry\nfunc (c *client) mergeDenyPermissionsLocked(what denyType, denyPubs []string) {\n\tc.mu.Lock()\n\tc.mergeDenyPermissions(what, denyPubs)\n\tc.mu.Unlock()\n}\n\n// Check to see if we have an expiration for the user JWT via base claims.\n// FIXME(dlc) - Clear on connect with new JWT.\nfunc (c *client) setExpiration(claims *jwt.ClaimsData, validFor time.Duration) {\n\tif claims.Expires == 0 {\n\t\tif validFor != 0 {\n\t\t\tc.setExpirationTimer(validFor)\n\t\t}\n\t\treturn\n\t}\n\texpiresAt := time.Duration(0)\n\ttn := time.Now().Unix()\n\tif claims.Expires > tn {\n\t\texpiresAt = time.Duration(claims.Expires-tn) * time.Second\n\t}\n\tif validFor != 0 && validFor < expiresAt {\n\t\tc.setExpirationTimer(validFor)\n\t} else {\n\t\tc.setExpirationTimer(expiresAt)\n\t}\n}\n\n// This will load up the deny structure used for filtering delivered\n// messages based on a deny clause for subscriptions.\n// Lock should be held.\nfunc (c *client) loadMsgDenyFilter() {\n\tc.mperms = &msgDeny{NewSublistWithCache(), make(map[string]bool)}\n\tfor _, sub := range c.darray {\n\t\tc.mperms.deny.Insert(&subscription{subject: []byte(sub)})\n\t}\n}\n\n// writeLoop is the main socket write functionality.\n// Runs in its own Go routine.\nfunc (c *client) writeLoop() {\n\tdefer c.srv.grWG.Done()\n\tc.mu.Lock()\n\tif c.isClosed() {\n\t\tc.mu.Unlock()\n\t\treturn\n\t}\n\tc.flags.set(writeLoopStarted)\n\tc.mu.Unlock()\n\n\t// Used to check that we did flush from last wake up.\n\twaitOk := true\n\tvar closed bool\n\n\t// Main loop. Will wait to be signaled and then will use\n\t// buffered outbound structure for efficient writev to the underlying socket.\n\tfor {\n\t\tc.mu.Lock()\n\t\tif closed = c.isClosed(); !closed {\n\t\t\towtf := c.out.fsp > 0 && c.out.pb < maxBufSize && c.out.fsp < maxFlushPending\n\t\t\tif waitOk && (c.out.pb == 0 || owtf) {\n\t\t\t\tc.out.sg.Wait()\n\t\t\t\t// Check that connection has not been closed while lock was released\n\t\t\t\t// in the conditional wait.\n\t\t\t\tclosed = c.isClosed()\n\t\t\t}\n\t\t}\n\t\tif closed {\n\t\t\tc.flushAndClose(false)\n\t\t\tc.mu.Unlock()\n\n\t\t\t// We should always call closeConnection() to ensure that state is\n\t\t\t// properly cleaned-up. It will be a no-op if already done.\n\t\t\tc.closeConnection(WriteError)\n\n\t\t\t// Now explicitly call reconnect(). Thanks to ref counting, we know\n\t\t\t// that the reconnect will execute only after connection has been\n\t\t\t// removed from the server state.\n\t\t\tc.reconnect()\n\t\t\treturn\n\t\t}\n\t\t// Flush data\n\t\twaitOk = c.flushOutbound()\n\t\tc.mu.Unlock()\n\t}\n}\n\n// flushClients will make sure to flush any clients we may have\n// sent to during processing. We pass in a budget as a time.Duration\n// for how much time to spend in place flushing for this client.\nfunc (c *client) flushClients(budget time.Duration) time.Time {\n\tlast := time.Now()\n\n\t// Check pending clients for flush.\n\tfor cp := range c.pcd {\n\t\t// TODO(dlc) - Wonder if it makes more sense to create a new map?\n\t\tdelete(c.pcd, cp)\n\n\t\t// Queue up a flush for those in the set\n\t\tcp.mu.Lock()\n\t\t// Update last activity for message delivery\n\t\tcp.last = last\n\t\t// Remove ourselves from the pending list.\n\t\tcp.out.fsp--\n\n\t\t// Just ignore if this was closed.\n\t\tif cp.isClosed() {\n\t\t\tcp.mu.Unlock()\n\t\t\tcontinue\n\t\t}\n\n\t\tif budget > 0 && cp.out.lft < 2*budget && cp.flushOutbound() {\n\t\t\tbudget -= cp.out.lft\n\t\t} else {\n\t\t\tcp.flushSignal()\n\t\t}\n\n\t\tcp.mu.Unlock()\n\t}\n\treturn last\n}\n\n// readLoop is the main socket read functionality.\n// Runs in its own Go routine.\nfunc (c *client) readLoop(pre []byte) {\n\t// Grab the connection off the client, it will be cleared on a close.\n\t// We check for that after the loop, but want to avoid a nil dereference\n\tc.mu.Lock()\n\ts := c.srv\n\tdefer s.grWG.Done()\n\tif c.isClosed() {\n\t\tc.mu.Unlock()\n\t\treturn\n\t}\n\tnc := c.nc\n\tws := c.isWebsocket()\n\tif c.isMqtt() {\n\t\tc.mqtt.r = &mqttReader{reader: nc}\n\t}\n\tc.in.rsz = startBufSize\n\n\t// Check the per-account-cache for closed subscriptions\n\tcpacc := c.kind == ROUTER || c.kind == GATEWAY\n\t// Last per-account-cache check for closed subscriptions\n\tlpacc := time.Now()\n\tacc := c.acc\n\tvar masking bool\n\tif ws {\n\t\tmasking = c.ws.maskread\n\t}\n\tcheckCompress := c.kind == ROUTER || c.kind == LEAF\n\tc.mu.Unlock()\n\n\tdefer func() {\n\t\tif c.isMqtt() {\n\t\t\ts.mqttHandleClosedClient(c)\n\t\t}\n\t\t// These are used only in the readloop, so we can set them to nil\n\t\t// on exit of the readLoop.\n\t\tc.in.results, c.in.pacache = nil, nil\n\t}()\n\n\t// Start read buffer.\n\tb := make([]byte, c.in.rsz)\n\n\t// Websocket clients will return several slices if there are multiple\n\t// websocket frames in the blind read. For non WS clients though, we\n\t// will always have 1 slice per loop iteration. So we define this here\n\t// so non WS clients will use bufs[0] = b[:n].\n\tvar _bufs [1][]byte\n\tbufs := _bufs[:1]\n\n\tvar wsr *wsReadInfo\n\tif ws {\n\t\twsr = &wsReadInfo{mask: masking}\n\t\twsr.init()\n\t}\n\n\tvar decompress bool\n\tvar reader io.Reader\n\treader = nc\n\n\tfor {\n\t\tvar n int\n\t\tvar err error\n\n\t\t// If we have a pre buffer parse that first.\n\t\tif len(pre) > 0 {\n\t\t\tb = pre\n\t\t\tn = len(pre)\n\t\t\tpre = nil\n\t\t} else {\n\t\t\tn, err = reader.Read(b)\n\t\t\t// If we have any data we will try to parse and exit at the end.\n\t\t\tif n == 0 && err != nil {\n\t\t\t\tc.closeConnection(closedStateForErr(err))\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t\tif ws {\n\t\t\tbufs, err = c.wsRead(wsr, reader, b[:n])\n\t\t\tif bufs == nil && err != nil {\n\t\t\t\tif err != io.EOF {\n\t\t\t\t\tc.Errorf(\"read error: %v\", err)\n\t\t\t\t}\n\t\t\t\tc.closeConnection(closedStateForErr(err))\n\t\t\t\treturn\n\t\t\t} else if bufs == nil {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t} else {\n\t\t\tbufs[0] = b[:n]\n\t\t}\n\n\t\t// Check if the account has mappings and if so set the local readcache flag.\n\t\t// We check here to make sure any changes such as config reload are reflected here.\n\t\tif c.kind == CLIENT || c.kind == LEAF {\n\t\t\tif acc.hasMappings() {\n\t\t\t\tc.in.flags.set(hasMappings)\n\t\t\t} else {\n\t\t\t\tc.in.flags.clear(hasMappings)\n\t\t\t}\n\t\t}\n\n\t\tc.in.start = time.Now()\n\n\t\t// Clear inbound stats cache\n\t\tc.in.msgs = 0\n\t\tc.in.bytes = 0\n\t\tc.in.subs = 0\n\n\t\t// Main call into parser for inbound data. This will generate callouts\n\t\t// to process messages, etc.\n\t\tfor i := 0; i < len(bufs); i++ {\n\t\t\tif err := c.parse(bufs[i]); err != nil {\n\t\t\t\tif err == ErrMinimumVersionRequired {\n\t\t\t\t\t// Special case here, currently only for leaf node connections.\n\t\t\t\t\t// When process the CONNECT protocol, if the minimum version\n\t\t\t\t\t// required was not met, an error was printed and sent back to\n\t\t\t\t\t// the remote, and connection was closed after a certain delay\n\t\t\t\t\t// (to avoid \"rapid\" reconnection from the remote).\n\t\t\t\t\t// We don't need to do any of the things below, simply return.\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t\tif dur := time.Since(c.in.start); dur >= readLoopReportThreshold {\n\t\t\t\t\tc.Warnf(\"Readloop processing time: %v\", dur)\n\t\t\t\t}\n\t\t\t\t// Need to call flushClients because some of the clients have been\n\t\t\t\t// assigned messages and their \"fsp\" incremented, and need now to be\n\t\t\t\t// decremented and their writeLoop signaled.\n\t\t\t\tc.flushClients(0)\n\t\t\t\t// handled inline\n\t\t\t\tif err != ErrMaxPayload && err != ErrAuthentication {\n\t\t\t\t\tc.Error(err)\n\t\t\t\t\tc.closeConnection(ProtocolViolation)\n\t\t\t\t}\n\t\t\t\treturn\n\t\t\t}\n\t\t\t// Clear total stalled time here.\n\t\t\tif c.in.tst >= stallClientMaxDuration {\n\t\t\t\tc.rateLimitFormatWarnf(\"Producer was stalled for a total of %v\", c.in.tst.Round(time.Millisecond))\n\t\t\t}\n\t\t\tc.in.tst = 0\n\t\t}\n\n\t\t// If we are a ROUTER/LEAF and have processed an INFO, it is possible that\n\t\t// we are asked to switch to compression now.\n\t\tif checkCompress && c.in.flags.isSet(switchToCompression) {\n\t\t\tc.in.flags.clear(switchToCompression)\n\t\t\t// For now we support only s2 compression...\n\t\t\treader = s2.NewReader(nc)\n\t\t\tdecompress = true\n\t\t}\n\n\t\t// Updates stats for client and server that were collected\n\t\t// from parsing through the buffer.\n\t\tif c.in.msgs > 0 {\n\t\t\tinMsgs := int64(c.in.msgs)\n\t\t\tinBytes := int64(c.in.bytes)\n\n\t\t\tatomic.AddInt64(&c.inMsgs, inMsgs)\n\t\t\tatomic.AddInt64(&c.inBytes, inBytes)\n\n\t\t\tif acc != nil {\n\t\t\t\tacc.stats.Lock()\n\t\t\t\tacc.stats.inMsgs += inMsgs\n\t\t\t\tacc.stats.inBytes += inBytes\n\t\t\t\tif c.kind == LEAF {\n\t\t\t\t\tacc.stats.ln.inMsgs += int64(inMsgs)\n\t\t\t\t\tacc.stats.ln.inBytes += int64(inBytes)\n\t\t\t\t}\n\t\t\t\tacc.stats.Unlock()\n\t\t\t}\n\n\t\t\tatomic.AddInt64(&s.inMsgs, inMsgs)\n\t\t\tatomic.AddInt64(&s.inBytes, inBytes)\n\t\t}\n\n\t\t// Signal to writeLoop to flush to socket.\n\t\tlast := c.flushClients(0)\n\n\t\t// Update activity, check read buffer size.\n\t\tc.mu.Lock()\n\n\t\t// Activity based on interest changes or data/msgs.\n\t\t// Also update last receive activity for ping sender\n\t\tif c.in.msgs > 0 || c.in.subs > 0 {\n\t\t\tc.last = last\n\t\t\tc.lastIn = last\n\t\t}\n\n\t\tif n >= cap(b) {\n\t\t\tc.in.srs = 0\n\t\t} else if n < cap(b)/2 { // divide by 2 b/c we want less than what we would shrink to.\n\t\t\tc.in.srs++\n\t\t}\n\n\t\t// Update read buffer size as/if needed.\n\t\tif n >= cap(b) && cap(b) < maxBufSize {\n\t\t\t// Grow\n\t\t\tc.in.rsz = int32(cap(b) * 2)\n\t\t\tb = make([]byte, c.in.rsz)\n\t\t} else if n < cap(b) && cap(b) > minBufSize && c.in.srs > shortsToShrink {\n\t\t\t// Shrink, for now don't accelerate, ping/pong will eventually sort it out.\n\t\t\tc.in.rsz = int32(cap(b) / 2)\n\t\t\tb = make([]byte, c.in.rsz)\n\t\t}\n\t\t// re-snapshot the account since it can change during reload, etc.\n\t\tacc = c.acc\n\t\t// Refresh nc because in some cases, we have upgraded c.nc to TLS.\n\t\tif nc != c.nc {\n\t\t\tnc = c.nc\n\t\t\tif decompress && nc != nil {\n\t\t\t\t// For now we support only s2 compression...\n\t\t\t\treader.(*s2.Reader).Reset(nc)\n\t\t\t} else if !decompress {\n\t\t\t\treader = nc\n\t\t\t}\n\t\t}\n\t\tc.mu.Unlock()\n\n\t\t// Connection was closed\n\t\tif nc == nil {\n\t\t\treturn\n\t\t}\n\n\t\tif dur := time.Since(c.in.start); dur >= readLoopReportThreshold {\n\t\t\tc.Warnf(\"Readloop processing time: %v\", dur)\n\t\t}\n\n\t\t// We could have had a read error from above but still read some data.\n\t\t// If so do the close here unconditionally.\n\t\tif err != nil {\n\t\t\tc.closeConnection(closedStateForErr(err))\n\t\t\treturn\n\t\t}\n\n\t\tif cpacc && (c.in.start.Sub(lpacc)) >= closedSubsCheckInterval {\n\t\t\tc.pruneClosedSubFromPerAccountCache()\n\t\t\tlpacc = time.Now()\n\t\t}\n\t}\n}\n\n// Returns the appropriate closed state for a given read error.\nfunc closedStateForErr(err error) ClosedState {\n\tif err == io.EOF {\n\t\treturn ClientClosed\n\t}\n\treturn ReadError\n}\n\n// collapsePtoNB will either returned framed WebSocket buffers or it will\n// return a reference to c.out.nb.\nfunc (c *client) collapsePtoNB() (net.Buffers, int64) {\n\tif c.isWebsocket() {\n\t\treturn c.wsCollapsePtoNB()\n\t}\n\treturn c.out.nb, c.out.pb\n}\n\n// flushOutbound will flush outbound buffer to a client.\n// Will return true if data was attempted to be written.\n// Lock must be held\nfunc (c *client) flushOutbound() bool {\n\tif c.flags.isSet(flushOutbound) {\n\t\t// For CLIENT connections, it is possible that the readLoop calls\n\t\t// flushOutbound(). If writeLoop and readLoop compete and we are\n\t\t// here we should release the lock to reduce the risk of spinning.\n\t\tc.mu.Unlock()\n\t\truntime.Gosched()\n\t\tc.mu.Lock()\n\t\treturn false\n\t}\n\tc.flags.set(flushOutbound)\n\tdefer func() {\n\t\t// Check flushAndClose() for explanation on why we do this.\n\t\tif c.isClosed() {\n\t\t\tfor i := range c.out.wnb {\n\t\t\t\tnbPoolPut(c.out.wnb[i])\n\t\t\t}\n\t\t\tc.out.wnb = nil\n\t\t}\n\t\tc.flags.clear(flushOutbound)\n\t}()\n\n\t// Check for nothing to do.\n\tif c.nc == nil || c.srv == nil || c.out.pb == 0 {\n\t\treturn true // true because no need to queue a signal.\n\t}\n\n\t// In the case of a normal socket connection, \"collapsed\" is just a ref\n\t// to \"nb\". In the case of WebSockets, additional framing is added to\n\t// anything that is waiting in \"nb\". Also keep a note of how many bytes\n\t// were queued before we release the mutex.\n\tcollapsed, attempted := c.collapsePtoNB()\n\n\t// Frustratingly, (net.Buffers).WriteTo() modifies the receiver so we\n\t// can't work on \"nb\" directly \u2014 while the mutex is unlocked during IO,\n\t// something else might call queueOutbound and modify it. So instead we\n\t// need a working copy \u2014 we'll operate on \"wnb\" instead. Note that in\n\t// the case of a partial write, \"wnb\" may have remaining data from the\n\t// previous write, and in the case of WebSockets, that data may already\n\t// be framed, so we are careful not to re-frame \"wnb\" here. Instead we\n\t// will just frame up \"nb\" and append it onto whatever is left on \"wnb\".\n\t// \"nb\" will be set to nil so that we can manipulate \"collapsed\" outside\n\t// of the client's lock, which is interesting in case of compression.\n\tc.out.nb = nil\n\n\t// In case it goes away after releasing the lock.\n\tnc := c.nc\n\n\t// Capture this (we change the value in some tests)\n\twdl := c.out.wdl\n\n\t// Check for compression\n\tcw := c.out.cw\n\tif cw != nil {\n\t\t// We will have to adjust once we have compressed, so remove for now.\n\t\tc.out.pb -= attempted\n\t\tif c.isWebsocket() {\n\t\t\tc.ws.fs -= attempted\n\t\t}\n\t}\n\n\t// Do NOT hold lock during actual IO.\n\tc.mu.Unlock()\n\n\t// Compress outside of the lock\n\tif cw != nil {\n\t\tvar err error\n\t\tbb := bytes.Buffer{}\n\n\t\tcw.Reset(&bb)\n\t\tfor _, buf := range collapsed {\n\t\t\tif _, err = cw.Write(buf); err != nil {\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\tif err == nil {\n\t\t\terr = cw.Close()\n\t\t}\n\t\tif err != nil {\n\t\t\tc.Errorf(\"Error compressing data: %v\", err)\n\t\t\t// We need to grab the lock now before marking as closed and exiting\n\t\t\tc.mu.Lock()\n\t\t\tc.markConnAsClosed(WriteError)\n\t\t\treturn false\n\t\t}\n\t\tcollapsed = append(net.Buffers(nil), bb.Bytes())\n\t\tattempted = int64(len(collapsed[0]))\n\t}\n\n\t// This is safe to do outside of the lock since \"collapsed\" is no longer\n\t// referenced in c.out.nb (which can be modified in queueOutboud() while\n\t// the lock is released).\n\tc.out.wnb = append(c.out.wnb, collapsed...)\n\tvar _orig [nbMaxVectorSize][]byte\n\torig := append(_orig[:0], c.out.wnb...)\n\n\t// Since WriteTo is lopping things off the beginning, we need to remember\n\t// the start position of the underlying array so that we can get back to it.\n\t// Otherwise we'll always \"slide forward\" and that will result in reallocs.\n\tstartOfWnb := c.out.wnb[0:]\n\n\t// flush here\n\tstart := time.Now()\n\n\tvar n int64   // Total bytes written\n\tvar wn int64  // Bytes written per loop\n\tvar err error // Error from last write, if any\n\tfor len(c.out.wnb) > 0 {\n\t\t// Limit the number of vectors to no more than nbMaxVectorSize,\n\t\t// which if 1024, will mean a maximum of 64MB in one go.\n\t\twnb := c.out.wnb\n\t\tif len(wnb) > nbMaxVectorSize {\n\t\t\twnb = wnb[:nbMaxVectorSize]\n\t\t}\n\t\tconsumed := len(wnb)\n\n\t\t// Actual write to the socket. The deadline applies to each batch\n\t\t// rather than the total write, such that the configured deadline\n\t\t// can be tuned to a known maximum quantity (64MB).\n\t\tnc.SetWriteDeadline(time.Now().Add(wdl))\n\t\twn, err = wnb.WriteTo(nc)\n\t\tnc.SetWriteDeadline(time.Time{})\n\n\t\t// Update accounting, move wnb slice onwards if needed, or stop\n\t\t// if a write error was reported that wasn't a short write.\n\t\tn += wn\n\t\tc.out.wnb = c.out.wnb[consumed-len(wnb):]\n\t\tif err != nil && err != io.ErrShortWrite {\n\t\t\tbreak\n\t\t}\n\t}\n\n\tlft := time.Since(start)\n\n\t// Re-acquire client lock.\n\tc.mu.Lock()\n\n\t// Adjust if we were compressing.\n\tif cw != nil {\n\t\tc.out.pb += attempted\n\t\tif c.isWebsocket() {\n\t\t\tc.ws.fs += attempted\n\t\t}\n\t}\n\n\t// At this point, \"wnb\" has been mutated by WriteTo and any consumed\n\t// buffers have been lopped off the beginning, so in order to return\n\t// them to the pool, we need to look at the difference between \"orig\"\n\t// and \"wnb\".\n\tfor i := 0; i < len(orig)-len(c.out.wnb); i++ {\n\t\tnbPoolPut(orig[i])\n\t}\n\n\t// At this point it's possible that \"nb\" has been modified by another\n\t// call to queueOutbound while the lock was released, so we'll leave\n\t// those for the next iteration. Meanwhile it's possible that we only\n\t// managed a partial write of \"wnb\", so we'll shift anything that\n\t// remains up to the beginning of the array to prevent reallocating.\n\t// Anything left in \"wnb\" has already been framed for WebSocket conns\n\t// so leave them alone for the next call to flushOutbound.\n\tc.out.wnb = append(startOfWnb[:0], c.out.wnb...)\n\n\t// If we've written everything but the underlying array of our working\n\t// buffer has grown excessively then free it \u2014 the GC will tidy it up\n\t// and we can allocate a new one next time.\n\tif len(c.out.wnb) == 0 && cap(c.out.wnb) > nbPoolSizeLarge*8 {\n\t\tc.out.wnb = nil\n\t}\n\n\t// Ignore ErrShortWrite errors, they will be handled as partials.\n\tvar gotWriteTimeout bool\n\tif err != nil && err != io.ErrShortWrite {\n\t\t// Handle timeout error (slow consumer) differently\n\t\tif ne, ok := err.(net.Error); ok && ne.Timeout() {\n\t\t\tgotWriteTimeout = true\n\t\t\tif closed := c.handleWriteTimeout(n, attempted, len(orig)); closed {\n\t\t\t\treturn true\n\t\t\t}\n\t\t} else {\n\t\t\t// Other errors will cause connection to be closed.\n\t\t\t// For clients, report as debug but for others report as error.\n\t\t\treport := c.Debugf\n\t\t\tif c.kind != CLIENT {\n\t\t\t\treport = c.Errorf\n\t\t\t}\n\t\t\treport(\"Error flushing: %v\", err)\n\t\t\tc.markConnAsClosed(WriteError)\n\t\t\treturn true\n\t\t}\n\t}\n\n\t// Update flush time statistics.\n\tc.out.lft = lft\n\n\t// Subtract from pending bytes and messages.\n\tc.out.pb -= n\n\tif c.isWebsocket() {\n\t\tc.ws.fs -= n\n\t}\n\n\t// Check that if there is still data to send and writeLoop is in wait,\n\t// then we need to signal.\n\tif c.out.pb > 0 {\n\t\tc.flushSignal()\n\t}\n\n\t// Check if we have a stalled gate and if so and we are recovering release\n\t// any stalled producers. Only kind==CLIENT will stall.\n\tif c.out.stc != nil && (n == attempted || c.out.pb < c.out.mp/4*3) {\n\t\tclose(c.out.stc)\n\t\tc.out.stc = nil\n\t}\n\t// Check if the connection is recovering from being a slow consumer.\n\tif !gotWriteTimeout && c.flags.isSet(isSlowConsumer) {\n\t\tc.Noticef(\"Slow Consumer Recovered: Flush took %.3fs with %d chunks of %d total bytes.\", time.Since(start).Seconds(), len(orig), attempted)\n\t\tc.flags.clear(isSlowConsumer)\n\t}\n\n\treturn true\n}\n\n// This is invoked from flushOutbound() for io/timeout error (slow consumer).\n// Returns a boolean to indicate if the connection has been closed or not.\n// Lock is held on entry.\nfunc (c *client) handleWriteTimeout(written, attempted int64, numChunks int) bool {\n\tif tlsConn, ok := c.nc.(*tls.Conn); ok {\n\t\tif !tlsConn.ConnectionState().HandshakeComplete {\n\t\t\t// Likely a TLSTimeout error instead...\n\t\t\tc.markConnAsClosed(TLSHandshakeError)\n\t\t\t// Would need to coordinate with tlstimeout()\n\t\t\t// to avoid double logging, so skip logging\n\t\t\t// here, and don't report a slow consumer error.\n\t\t\treturn true\n\t\t}\n\t} else if c.flags.isSet(expectConnect) && !c.flags.isSet(connectReceived) {\n\t\t// Under some conditions, a connection may hit a slow consumer write deadline\n\t\t// before the authorization timeout. If that is the case, then we handle\n\t\t// as slow consumer though we do not increase the counter as that can be\n\t\t// misleading.\n\t\tc.markConnAsClosed(SlowConsumerWriteDeadline)\n\t\treturn true\n\t}\n\talreadySC := c.flags.isSet(isSlowConsumer)\n\tscState := \"Detected\"\n\tif alreadySC {\n\t\tscState = \"State\"\n\t}\n\n\t// Aggregate slow consumers.\n\tatomic.AddInt64(&c.srv.slowConsumers, 1)\n\tswitch c.kind {\n\tcase CLIENT:\n\t\tc.srv.scStats.clients.Add(1)\n\tcase ROUTER:\n\t\t// Only count each Slow Consumer event once.\n\t\tif !alreadySC {\n\t\t\tc.srv.scStats.routes.Add(1)\n\t\t}\n\tcase GATEWAY:\n\t\tc.srv.scStats.gateways.Add(1)\n\tcase LEAF:\n\t\tc.srv.scStats.leafs.Add(1)\n\t}\n\tif c.acc != nil {\n\t\tc.acc.stats.Lock()\n\t\tc.acc.stats.slowConsumers++\n\t\tc.acc.stats.Unlock()\n\t}\n\tc.Noticef(\"Slow Consumer %s: WriteDeadline of %v exceeded with %d chunks of %d total bytes.\",\n\t\tscState, c.out.wdl, numChunks, attempted)\n\n\t// We always close CLIENT connections, or when nothing was written at all...\n\tif c.kind == CLIENT || written == 0 {\n\t\tc.markConnAsClosed(SlowConsumerWriteDeadline)\n\t\treturn true\n\t} else {\n\t\tc.flags.setIfNotSet(isSlowConsumer)\n\t}\n\treturn false\n}\n\n// Marks this connection has closed with the given reason.\n// Sets the connMarkedClosed flag and skipFlushOnClose depending on the reason.\n// Depending on the kind of connection, the connection will be saved.\n// If a writeLoop has been started, the final flush will be done there, otherwise\n// flush and close of TCP connection is done here in place.\n// Returns true if closed in place, flase otherwise.\n// Lock is held on entry.\nfunc (c *client) markConnAsClosed(reason ClosedState) {\n\t// Possibly set skipFlushOnClose flag even if connection has already been\n\t// mark as closed. The rationale is that a connection may be closed with\n\t// a reason that justifies a flush (say after sending an -ERR), but then\n\t// the flushOutbound() gets a write error. If that happens, connection\n\t// being lost, there is no reason to attempt to flush again during the\n\t// teardown when the writeLoop exits.\n\tvar skipFlush bool\n\tswitch reason {\n\tcase ReadError, WriteError, SlowConsumerPendingBytes, SlowConsumerWriteDeadline, TLSHandshakeError:\n\t\tc.flags.set(skipFlushOnClose)\n\t\tskipFlush = true\n\t}\n\tif c.flags.isSet(connMarkedClosed) {\n\t\treturn\n\t}\n\tc.flags.set(connMarkedClosed)\n\t// For a websocket client, unless we are told not to flush, enqueue\n\t// a websocket CloseMessage based on the reason.\n\tif !skipFlush && c.isWebsocket() && !c.ws.closeSent {\n\t\tc.wsEnqueueCloseMessage(reason)\n\t}\n\t// Be consistent with the creation: for routes, gateways and leaf,\n\t// we use Noticef on create, so use that too for delete.\n\tif c.srv != nil {\n\t\tif c.kind == LEAF {\n\t\t\tif c.acc != nil {\n\t\t\t\tc.Noticef(\"%s connection closed: %s - Account: %s\", c.kindString(), reason, c.acc.traceLabel())\n\t\t\t} else {\n\t\t\t\tc.Noticef(\"%s connection closed: %s\", c.kindString(), reason)\n\t\t\t}\n\t\t} else if c.kind == ROUTER || c.kind == GATEWAY {\n\t\t\tc.Noticef(\"%s connection closed: %s\", c.kindString(), reason)\n\t\t} else { // Client, System, Jetstream, and Account connections.\n\t\t\tc.Debugf(\"%s connection closed: %s\", c.kindString(), reason)\n\t\t}\n\t}\n\n\t// Save off the connection if its a client or leafnode.\n\tif c.kind == CLIENT || c.kind == LEAF {\n\t\tif nc := c.nc; nc != nil && c.srv != nil {\n\t\t\t// TODO: May want to send events to single go routine instead\n\t\t\t// of creating a new go routine for each save.\n\t\t\t// Pass the c.subs as a reference. It may be set to nil in\n\t\t\t// closeConnection.\n\t\t\tgo c.srv.saveClosedClient(c, nc, c.subs, reason)\n\t\t}\n\t}\n\t// If writeLoop exists, let it do the final flush, close and teardown.\n\tif c.flags.isSet(writeLoopStarted) {\n\t\t// Since we want the writeLoop to do the final flush and tcp close,\n\t\t// we want the reconnect to be done there too. However, it should'nt\n\t\t// happen before the connection has been removed from the server\n\t\t// state (end of closeConnection()). This ref count allows us to\n\t\t// guarantee that.\n\t\tc.rref++\n\t\tc.flushSignal()\n\t\treturn\n\t}\n\t// Flush (if skipFlushOnClose is not set) and close in place. If flushing,\n\t// use a small WriteDeadline.\n\tc.flushAndClose(true)\n}\n\n// flushSignal will use server to queue the flush IO operation to a pool of flushers.\n// Lock must be held.\nfunc (c *client) flushSignal() {\n\t// Check that sg is not nil, which will happen if the connection is closed.\n\tif c.out.sg != nil {\n\t\tc.out.sg.Signal()\n\t}\n}\n\n// Traces a message.\n// Will NOT check if tracing is enabled, does NOT need the client lock.\nfunc (c *client) traceMsg(msg []byte) {\n\topts := c.srv.getOpts()\n\tmaxTrace := opts.MaxTracedMsgLen\n\theadersOnly := opts.TraceHeaders\n\tsuffix := LEN_CR_LF\n\n\t// If TraceHeaders is enabled, extract only the header portion of the msg.\n\t// If a header is present, it ends with an additional trailing CRLF.\n\tif headersOnly {\n\t\tmsg, _ = c.msgParts(msg)\n\t\tsuffix += LEN_CR_LF\n\t}\n\n\t// Do not emit a log line for zero-length payloads.\n\tl := len(msg) - suffix\n\tif l <= 0 {\n\t\treturn\n\t}\n\n\tif maxTrace > 0 && l > maxTrace {\n\t\ttm := fmt.Sprintf(\"%q\", msg[:maxTrace])\n\t\tc.Tracef(\"<<- MSG_PAYLOAD: [\\\"%s...\\\"]\", tm[1:len(tm)-1])\n\t} else {\n\t\tc.Tracef(\"<<- MSG_PAYLOAD: [%q]\", msg[:l])\n\t}\n}\n\n// Traces an incoming operation.\n// Will NOT check if tracing is enabled, does NOT need the client lock.\nfunc (c *client) traceInOp(op string, arg []byte) {\n\tc.traceOp(\"<<- %s\", op, arg)\n}\n\n// Traces an outgoing operation.\n// Will NOT check if tracing is enabled, does NOT need the client lock.\nfunc (c *client) traceOutOp(op string, arg []byte) {\n\tc.traceOp(\"->> %s\", op, arg)\n}\n\nfunc (c *client) traceOp(format, op string, arg []byte) {\n\topa := []any{}\n\tif op != _EMPTY_ {\n\t\topa = append(opa, op)\n\t}\n\tif arg != nil {\n\t\topa = append(opa, bytesToString(arg))\n\t}\n\tc.Tracef(format, opa)\n}\n\n// Process the information messages from Clients and other Routes.\nfunc (c *client) processInfo(arg []byte) error {\n\tinfo := Info{}\n\tif err := json.Unmarshal(arg, &info); err != nil {\n\t\treturn err\n\t}\n\tswitch c.kind {\n\tcase ROUTER:\n\t\tc.processRouteInfo(&info)\n\tcase GATEWAY:\n\t\tc.processGatewayInfo(&info)\n\tcase LEAF:\n\t\tc.processLeafnodeInfo(&info)\n\t}\n\treturn nil\n}\n\nfunc (c *client) processErr(errStr string) {\n\tclose := true\n\tswitch c.kind {\n\tcase CLIENT:\n\t\tc.Errorf(\"Client Error %s\", errStr)\n\tcase ROUTER:\n\t\tc.Errorf(\"Route Error %s\", errStr)\n\tcase GATEWAY:\n\t\tc.Errorf(\"Gateway Error %s\", errStr)\n\tcase LEAF:\n\t\tc.Errorf(\"Leafnode Error %s\", errStr)\n\t\tc.leafProcessErr(errStr)\n\t\tclose = false\n\tcase JETSTREAM:\n\t\tc.Errorf(\"JetStream Error %s\", errStr)\n\t}\n\tif close {\n\t\tc.closeConnection(ParseError)\n\t}\n}\n\n// Password pattern matcher.\nvar passPat = regexp.MustCompile(`\"?\\s*pass\\S*?\"?\\s*[:=]\\s*\"?(([^\",\\r\\n}])*)`)\nvar tokenPat = regexp.MustCompile(`\"?\\s*auth_token\\S*?\"?\\s*[:=]\\s*\"?(([^\",\\r\\n}])*)`)\n\n// removeSecretsFromTrace removes any notion of passwords/tokens from trace\n// messages for logging.\nfunc removeSecretsFromTrace(arg []byte) []byte {\n\tbuf := redact(\"pass\", passPat, arg)\n\treturn redact(\"auth_token\", tokenPat, buf)\n}\n\nfunc redact(name string, pat *regexp.Regexp, proto []byte) []byte {\n\tif !bytes.Contains(proto, []byte(name)) {\n\t\treturn proto\n\t}\n\t// Take a copy of the connect proto just for the trace message.\n\tvar _arg [4096]byte\n\tbuf := append(_arg[:0], proto...)\n\n\tm := pat.FindAllSubmatchIndex(buf, -1)\n\tif len(m) == 0 {\n\t\treturn proto\n\t}\n\n\tredactedPass := []byte(\"[REDACTED]\")\n\tfor _, i := range m {\n\t\tif len(i) < 4 {\n\t\t\tcontinue\n\t\t}\n\t\tstart := i[2]\n\t\tend := i[3]\n\n\t\t// Replace value substring.\n\t\tbuf = append(buf[:start], append(redactedPass, buf[end:]...)...)\n\t\tbreak\n\t}\n\treturn buf\n}\n\n// Returns the RTT by computing the elapsed time since now and `start`.\n// On Windows VM where I (IK) run tests, time.Since() will return 0\n// (I suspect some time granularity issues). So return at minimum 1ns.\nfunc computeRTT(start time.Time) time.Duration {\n\trtt := time.Since(start)\n\tif rtt <= 0 {\n\t\trtt = time.Nanosecond\n\t}\n\treturn rtt\n}\n\n// processConnect will process a client connect op.\nfunc (c *client) processConnect(arg []byte) error {\n\tsupportsHeaders := c.srv.supportsHeaders()\n\tc.mu.Lock()\n\t// If we can't stop the timer because the callback is in progress...\n\tif !c.clearAuthTimer() {\n\t\t// wait for it to finish and handle sending the failure back to\n\t\t// the client.\n\t\tfor !c.isClosed() {\n\t\t\tc.mu.Unlock()\n\t\t\ttime.Sleep(25 * time.Millisecond)\n\t\t\tc.mu.Lock()\n\t\t}\n\t\tc.mu.Unlock()\n\t\treturn nil\n\t}\n\tc.last = time.Now().UTC()\n\t// Estimate RTT to start.\n\tif c.kind == CLIENT {\n\t\tc.rtt = computeRTT(c.start)\n\t\tif c.srv != nil {\n\t\t\tc.clearPingTimer()\n\t\t\tc.setFirstPingTimer()\n\t\t}\n\t}\n\tkind := c.kind\n\tsrv := c.srv\n\n\t// Moved unmarshalling of clients' Options under the lock.\n\t// The client has already been added to the server map, so it is possible\n\t// that other routines lookup the client, and access its options under\n\t// the client's lock, so unmarshalling the options outside of the lock\n\t// would cause data RACEs.\n\tif err := json.Unmarshal(arg, &c.opts); err != nil {\n\t\tc.mu.Unlock()\n\t\treturn err\n\t}\n\t// Indicate that the CONNECT protocol has been received, and that the\n\t// server now knows which protocol this client supports.\n\tc.flags.set(connectReceived)\n\t// Capture these under lock\n\tc.echo = c.opts.Echo\n\tproto := c.opts.Protocol\n\tverbose := c.opts.Verbose\n\tlang := c.opts.Lang\n\taccount := c.opts.Account\n\taccountNew := c.opts.AccountNew\n\n\tif c.kind == CLIENT {\n\t\tvar ncs string\n\t\tif c.opts.Version != _EMPTY_ {\n\t\t\tncs = fmt.Sprintf(\"v%s\", c.opts.Version)\n\t\t}\n\t\tif c.opts.Lang != _EMPTY_ {\n\t\t\tif c.opts.Version == _EMPTY_ {\n\t\t\t\tncs = c.opts.Lang\n\t\t\t} else {\n\t\t\t\tncs = fmt.Sprintf(\"%s:%s\", ncs, c.opts.Lang)\n\t\t\t}\n\t\t}\n\t\tif c.opts.Name != _EMPTY_ {\n\t\t\tif c.opts.Version == _EMPTY_ && c.opts.Lang == _EMPTY_ {\n\t\t\t\tncs = c.opts.Name\n\t\t\t} else {\n\t\t\t\tncs = fmt.Sprintf(\"%s:%s\", ncs, c.opts.Name)\n\t\t\t}\n\t\t}\n\t\tif ncs != _EMPTY_ {\n\t\t\tc.ncs.CompareAndSwap(nil, fmt.Sprintf(\"%s - %q\", c, ncs))\n\t\t}\n\t}\n\n\t// if websocket client, maybe some options through cookies\n\tif ws := c.ws; ws != nil {\n\t\t// if JWT not in the CONNECT, use the cookie JWT (possibly empty).\n\t\tif c.opts.JWT == _EMPTY_ {\n\t\t\tc.opts.JWT = ws.cookieJwt\n\t\t}\n\t\t// if user not in the CONNECT, use the cookie user (possibly empty)\n\t\tif c.opts.Username == _EMPTY_ {\n\t\t\tc.opts.Username = ws.cookieUsername\n\t\t}\n\t\t// if pass not in the CONNECT, use the cookie password (possibly empty).\n\t\tif c.opts.Password == _EMPTY_ {\n\t\t\tc.opts.Password = ws.cookiePassword\n\t\t}\n\t\t// if token not in the CONNECT, use the cookie token (possibly empty).\n\t\tif c.opts.Token == _EMPTY_ {\n\t\t\tc.opts.Token = ws.cookieToken\n\t\t}\n\t}\n\n\t// when not in operator mode, discard the jwt\n\tif srv != nil && srv.trustedKeys == nil {\n\t\tc.opts.JWT = _EMPTY_\n\t}\n\tujwt := c.opts.JWT\n\n\t// For headers both client and server need to support.\n\tc.headers = supportsHeaders && c.opts.Headers\n\tc.mu.Unlock()\n\n\tif srv != nil {\n\t\t// Applicable to clients only:\n\t\t// As soon as c.opts is unmarshalled and if the proto is at\n\t\t// least ClientProtoInfo, we need to increment the following counter.\n\t\t// This is decremented when client is removed from the server's\n\t\t// clients map.\n\t\tif kind == CLIENT && proto >= ClientProtoInfo {\n\t\t\tsrv.mu.Lock()\n\t\t\tsrv.cproto++\n\t\t\tsrv.mu.Unlock()\n\t\t}\n\n\t\t// Check for Auth\n\t\tif ok := srv.checkAuthentication(c); !ok {\n\t\t\t// We may fail here because we reached max limits on an account.\n\t\t\tif ujwt != _EMPTY_ {\n\t\t\t\tc.mu.Lock()\n\t\t\t\tacc := c.acc\n\t\t\t\tc.mu.Unlock()\n\t\t\t\tsrv.mu.Lock()\n\t\t\t\ttooManyAccCons := acc != nil && acc != srv.gacc\n\t\t\t\tsrv.mu.Unlock()\n\t\t\t\tif tooManyAccCons {\n\t\t\t\t\treturn ErrTooManyAccountConnections\n\t\t\t\t}\n\t\t\t}\n\t\t\tc.authViolation()\n\t\t\treturn ErrAuthentication\n\t\t}\n\n\t\t// Check for Account designation, we used to have this as an optional feature for dynamic\n\t\t// sandbox environments. Now its considered an error.\n\t\tif accountNew || account != _EMPTY_ {\n\t\t\tc.authViolation()\n\t\t\treturn ErrAuthentication\n\t\t}\n\n\t\t// If no account designation.\n\t\t// Do this only for CLIENT and LEAF connections.\n\t\tif c.acc == nil && (c.kind == CLIENT || c.kind == LEAF) {\n\t\t\t// By default register with the global account.\n\t\t\tc.registerWithAccount(srv.globalAccount())\n\t\t}\n\t}\n\n\tswitch kind {\n\tcase CLIENT:\n\t\t// Check client protocol request if it exists.\n\t\tif proto < ClientProtoZero || proto > ClientProtoInfo {\n\t\t\tc.sendErr(ErrBadClientProtocol.Error())\n\t\t\tc.closeConnection(BadClientProtocolVersion)\n\t\t\treturn ErrBadClientProtocol\n\t\t}\n\t\t// Check to see that if no_responders is requested\n\t\t// they have header support on as well.\n\t\tc.mu.Lock()\n\t\tmisMatch := c.opts.NoResponders && !c.headers\n\t\tc.mu.Unlock()\n\t\tif misMatch {\n\t\t\tc.sendErr(ErrNoRespondersRequiresHeaders.Error())\n\t\t\tc.closeConnection(NoRespondersRequiresHeaders)\n\t\t\treturn ErrNoRespondersRequiresHeaders\n\t\t}\n\t\tif verbose {\n\t\t\tc.sendOK()\n\t\t}\n\tcase ROUTER:\n\t\t// Delegate the rest of processing to the route\n\t\treturn c.processRouteConnect(srv, arg, lang)\n\tcase GATEWAY:\n\t\t// Delegate the rest of processing to the gateway\n\t\treturn c.processGatewayConnect(arg)\n\tcase LEAF:\n\t\t// Delegate the rest of processing to the leaf node\n\t\treturn c.processLeafNodeConnect(srv, arg, lang)\n\t}\n\treturn nil\n}\n\nfunc (c *client) sendErrAndErr(err string) {\n\tc.sendErr(err)\n\tc.Errorf(err)\n}\n\nfunc (c *client) sendErrAndDebug(err string) {\n\tc.sendErr(err)\n\tc.Debugf(err)\n}\n\nfunc (c *client) authTimeout() {\n\tc.sendErrAndDebug(\"Authentication Timeout\")\n\tc.closeConnection(AuthenticationTimeout)\n}\n\nfunc (c *client) authExpired() {\n\tc.sendErrAndDebug(\"User Authentication Expired\")\n\tc.closeConnection(AuthenticationExpired)\n}\n\nfunc (c *client) accountAuthExpired() {\n\tc.sendErrAndDebug(\"Account Authentication Expired\")\n\tc.closeConnection(AuthenticationExpired)\n}\n\nfunc (c *client) authViolation() {\n\tvar s *Server\n\tvar hasTrustedNkeys, hasNkeys, hasUsers bool\n\tif s = c.srv; s != nil {\n\t\ts.mu.RLock()\n\t\thasTrustedNkeys = s.trustedKeys != nil\n\t\thasNkeys = s.nkeys != nil\n\t\thasUsers = s.users != nil\n\t\ts.mu.RUnlock()\n\t\tdefer s.sendAuthErrorEvent(c)\n\t}\n\n\tif hasTrustedNkeys {\n\t\tc.Errorf(\"%v\", ErrAuthentication)\n\t} else if hasNkeys {\n\t\tc.Errorf(\"%s - Nkey %q\",\n\t\t\tErrAuthentication.Error(),\n\t\t\tc.opts.Nkey)\n\t} else if hasUsers {\n\t\tc.Errorf(\"%s - User %q\",\n\t\t\tErrAuthentication.Error(),\n\t\t\tc.opts.Username)\n\t} else {\n\t\tif c.srv != nil {\n\t\t\tc.Errorf(ErrAuthentication.Error())\n\t\t}\n\t}\n\tif c.isMqtt() {\n\t\tc.mqttEnqueueConnAck(mqttConnAckRCNotAuthorized, false)\n\t} else {\n\t\tc.sendErr(\"Authorization Violation\")\n\t}\n\tc.closeConnection(AuthenticationViolation)\n}\n\nfunc (c *client) maxAccountConnExceeded() {\n\tc.sendErrAndErr(ErrTooManyAccountConnections.Error())\n\tc.closeConnection(MaxAccountConnectionsExceeded)\n}\n\nfunc (c *client) maxConnExceeded() {\n\tc.sendErrAndErr(ErrTooManyConnections.Error())\n\tc.closeConnection(MaxConnectionsExceeded)\n}\n\nfunc (c *client) maxSubsExceeded() {\n\tif c.acc.shouldLogMaxSubErr() {\n\t\tc.Errorf(ErrTooManySubs.Error())\n\t}\n\tc.sendErr(ErrTooManySubs.Error())\n}\n\nfunc (c *client) maxPayloadViolation(sz int, max int32) {\n\tc.Errorf(\"%s: %d vs %d\", ErrMaxPayload.Error(), sz, max)\n\tc.sendErr(\"Maximum Payload Violation\")\n\tc.closeConnection(MaxPayloadExceeded)\n}\n\n// queueOutbound queues data for a clientconnection.\n// Lock should be held.\nfunc (c *client) queueOutbound(data []byte) {\n\t// Do not keep going if closed\n\tif c.isClosed() {\n\t\treturn\n\t}\n\n\t// Add to pending bytes total.\n\tc.out.pb += int64(len(data))\n\n\t// Take a copy of the slice ref so that we can chop bits off the beginning\n\t// without affecting the original \"data\" slice.\n\ttoBuffer := data\n\n\t// All of the queued []byte have a fixed capacity, so if there's a []byte\n\t// at the tail of the buffer list that isn't full yet, we should top that\n\t// up first. This helps to ensure we aren't pulling more []bytes from the\n\t// pool than we need to.\n\tif len(c.out.nb) > 0 {\n\t\tlast := &c.out.nb[len(c.out.nb)-1]\n\t\tif free := cap(*last) - len(*last); free > 0 {\n\t\t\tif l := len(toBuffer); l < free {\n\t\t\t\tfree = l\n\t\t\t}\n\t\t\t*last = append(*last, toBuffer[:free]...)\n\t\t\ttoBuffer = toBuffer[free:]\n\t\t}\n\t}\n\n\t// Now we can push the rest of the data into new []bytes from the pool\n\t// in fixed size chunks. This ensures we don't go over the capacity of any\n\t// of the buffers and end up reallocating.\n\tfor len(toBuffer) > 0 {\n\t\tnew := nbPoolGet(len(toBuffer))\n\t\tn := copy(new[:cap(new)], toBuffer)\n\t\tc.out.nb = append(c.out.nb, new[:n])\n\t\ttoBuffer = toBuffer[n:]\n\t}\n\n\t// Check for slow consumer via pending bytes limit.\n\t// ok to return here, client is going away.\n\tif c.kind == CLIENT && c.out.pb > c.out.mp {\n\t\t// Perf wise, it looks like it is faster to optimistically add than\n\t\t// checking current pb+len(data) and then add to pb.\n\t\tc.out.pb -= int64(len(data))\n\n\t\t// Increment the total and client's slow consumer counters.\n\t\tatomic.AddInt64(&c.srv.slowConsumers, 1)\n\t\tc.srv.scStats.clients.Add(1)\n\t\tif c.acc != nil {\n\t\t\tc.acc.stats.Lock()\n\t\t\tc.acc.stats.slowConsumers++\n\t\t\tc.acc.stats.Unlock()\n\t\t}\n\t\tc.Noticef(\"Slow Consumer Detected: MaxPending of %d Exceeded\", c.out.mp)\n\t\tc.markConnAsClosed(SlowConsumerPendingBytes)\n\t\treturn\n\t}\n\n\t// Check here if we should create a stall channel if we are falling behind.\n\t// We do this here since if we wait for consumer's writeLoop it could be\n\t// too late with large number of fan in producers.\n\t// If the outbound connection is > 75% of maximum pending allowed, create a stall gate.\n\tif c.out.pb > c.out.mp/4*3 && c.out.stc == nil {\n\t\tc.out.stc = make(chan struct{})\n\t}\n}\n\n// Assume the lock is held upon entry.\nfunc (c *client) enqueueProtoAndFlush(proto []byte, doFlush bool) {\n\tif c.isClosed() {\n\t\treturn\n\t}\n\tc.queueOutbound(proto)\n\tif !(doFlush && c.flushOutbound()) {\n\t\tc.flushSignal()\n\t}\n}\n\n// Queues and then flushes the connection. This should only be called when\n// the writeLoop cannot be started yet. Use enqueueProto() otherwise.\n// Lock is held on entry.\nfunc (c *client) sendProtoNow(proto []byte) {\n\tc.enqueueProtoAndFlush(proto, true)\n}\n\n// Enqueues the given protocol and signal the writeLoop if necessary.\n// Lock is held on entry.\nfunc (c *client) enqueueProto(proto []byte) {\n\tc.enqueueProtoAndFlush(proto, false)\n}\n\n// Assume the lock is held upon entry.\nfunc (c *client) sendPong() {\n\tif c.trace {\n\t\tc.traceOutOp(\"PONG\", nil)\n\t}\n\tc.enqueueProto([]byte(pongProto))\n}\n\n// Used to kick off a RTT measurement for latency tracking.\nfunc (c *client) sendRTTPing() bool {\n\tc.mu.Lock()\n\tsent := c.sendRTTPingLocked()\n\tc.mu.Unlock()\n\treturn sent\n}\n\n// Used to kick off a RTT measurement for latency tracking.\n// This is normally called only when the caller has checked that\n// the c.rtt is 0 and wants to force an update by sending a PING.\n// Client lock held on entry.\nfunc (c *client) sendRTTPingLocked() bool {\n\tif c.isMqtt() {\n\t\treturn false\n\t}\n\t// Most client libs send a CONNECT+PING and wait for a PONG from the\n\t// server. So if firstPongSent flag is set, it is ok for server to\n\t// send the PING. But in case we have client libs that don't do that,\n\t// allow the send of the PING if more than 2 secs have elapsed since\n\t// the client TCP connection was accepted.\n\tif !c.isClosed() &&\n\t\t(c.flags.isSet(firstPongSent) || time.Since(c.start) > maxNoRTTPingBeforeFirstPong) {\n\t\tc.sendPing()\n\t\treturn true\n\t}\n\treturn false\n}\n\n// Assume the lock is held upon entry.\nfunc (c *client) sendPing() {\n\tc.rttStart = time.Now().UTC()\n\tc.ping.out++\n\tif c.trace {\n\t\tc.traceOutOp(\"PING\", nil)\n\t}\n\tc.enqueueProto([]byte(pingProto))\n}\n\n// Generates the INFO to be sent to the client with the client ID included.\n// info arg will be copied since passed by value.\n// Assume lock is held.\nfunc (c *client) generateClientInfoJSON(info Info) []byte {\n\tinfo.CID = c.cid\n\tinfo.ClientIP = c.host\n\tinfo.MaxPayload = c.mpay\n\tif c.isWebsocket() {\n\t\tinfo.ClientConnectURLs = info.WSConnectURLs\n\t\t// Otherwise lame duck info can panic\n\t\tif c.srv != nil {\n\t\t\tws := &c.srv.websocket\n\t\t\tinfo.TLSAvailable, info.TLSRequired = ws.tls, ws.tls\n\t\t\tinfo.Host, info.Port = ws.host, ws.port\n\t\t}\n\t}\n\tinfo.WSConnectURLs = nil\n\treturn generateInfoJSON(&info)\n}\n\nfunc (c *client) sendErr(err string) {\n\tc.mu.Lock()\n\tif c.trace {\n\t\tc.traceOutOp(\"-ERR\", []byte(err))\n\t}\n\tif !c.isMqtt() {\n\t\tc.enqueueProto([]byte(fmt.Sprintf(errProto, err)))\n\t}\n\tc.mu.Unlock()\n}\n\nfunc (c *client) sendOK() {\n\tc.mu.Lock()\n\tif c.trace {\n\t\tc.traceOutOp(\"OK\", nil)\n\t}\n\tc.enqueueProto([]byte(okProto))\n\tc.mu.Unlock()\n}\n\nfunc (c *client) processPing() {\n\tc.mu.Lock()\n\n\tif c.isClosed() {\n\t\tc.mu.Unlock()\n\t\treturn\n\t}\n\n\tc.sendPong()\n\n\t// Record this to suppress us sending one if this\n\t// is within a given time interval for activity.\n\tc.lastIn = time.Now()\n\n\t// If not a CLIENT, we are done. Also the CONNECT should\n\t// have been received, but make sure it is so before proceeding\n\tif c.kind != CLIENT || !c.flags.isSet(connectReceived) {\n\t\tc.mu.Unlock()\n\t\treturn\n\t}\n\n\t// If we are here, the CONNECT has been received so we know\n\t// if this client supports async INFO or not.\n\tvar (\n\t\tsendConnectInfo bool\n\t\tsrv             = c.srv\n\t)\n\t// For the first PING (so firstPongSet is false) and for clients\n\t// that support async INFO protocols, we will send one with ConnectInfo=true,\n\t// the name of the account the client is bound to, and if the\n\t// account is the system account.\n\tif !c.flags.isSet(firstPongSent) {\n\t\t// Flip the flag.\n\t\tc.flags.set(firstPongSent)\n\t\t// Evaluate if we should send the INFO protocol.\n\t\tsendConnectInfo = srv != nil && c.opts.Protocol >= ClientProtoInfo\n\t}\n\tc.mu.Unlock()\n\n\tif sendConnectInfo {\n\t\tsrv.mu.Lock()\n\t\tinfo := srv.copyInfo()\n\t\tc.mu.Lock()\n\t\tinfo.RemoteAccount = c.acc.Name\n\t\tinfo.IsSystemAccount = c.acc == srv.SystemAccount()\n\t\tinfo.ConnectInfo = true\n\t\tc.enqueueProto(c.generateClientInfoJSON(info))\n\t\tc.mu.Unlock()\n\t\tsrv.mu.Unlock()\n\t}\n}\n\nfunc (c *client) processPong() {\n\tc.mu.Lock()\n\tc.ping.out = 0\n\tc.rtt = computeRTT(c.rttStart)\n\tsrv := c.srv\n\treorderGWs := c.kind == GATEWAY && c.gw.outbound\n\t// If compression is currently active for a route/leaf connection, if the\n\t// compression configuration is s2_auto, check if we should change\n\t// the compression level.\n\tif c.kind == ROUTER && needsCompression(c.route.compression) {\n\t\tc.updateS2AutoCompressionLevel(&srv.getOpts().Cluster.Compression, &c.route.compression)\n\t} else if c.kind == LEAF && needsCompression(c.leaf.compression) {\n\t\tvar co *CompressionOpts\n\t\tif r := c.leaf.remote; r != nil {\n\t\t\tco = &r.Compression\n\t\t} else {\n\t\t\tco = &srv.getOpts().LeafNode.Compression\n\t\t}\n\t\tc.updateS2AutoCompressionLevel(co, &c.leaf.compression)\n\t}\n\tc.mu.Unlock()\n\tif reorderGWs {\n\t\tsrv.gateway.orderOutboundConnections()\n\t}\n}\n\n// Select the s2 compression level based on the client's current RTT and the configured\n// RTT thresholds slice. If current level is different than selected one, save the\n// new compression level string and create a new s2 writer.\n// Lock held on entry.\nfunc (c *client) updateS2AutoCompressionLevel(co *CompressionOpts, compression *string) {\n\tif co.Mode != CompressionS2Auto {\n\t\treturn\n\t}\n\tif cm := selectS2AutoModeBasedOnRTT(c.rtt, co.RTTThresholds); cm != *compression {\n\t\t*compression = cm\n\t\tc.out.cw = s2.NewWriter(nil, s2WriterOptions(cm)...)\n\t}\n}\n\n// Will return the parts from the raw wire msg.\nfunc (c *client) msgParts(data []byte) (hdr []byte, msg []byte) {\n\tif c != nil && c.pa.hdr > 0 {\n\t\treturn data[:c.pa.hdr], data[c.pa.hdr:]\n\t}\n\treturn nil, data\n}\n\n// Header pubs take form HPUB <subject> [reply] <hdr_len> <total_len>\\r\\n\nfunc (c *client) processHeaderPub(arg, remaining []byte) error {\n\tif !c.headers {\n\t\treturn ErrMsgHeadersNotSupported\n\t}\n\n\t// Unroll splitArgs to avoid runtime/heap issues\n\ta := [MAX_HPUB_ARGS][]byte{}\n\targs := a[:0]\n\tstart := -1\n\tfor i, b := range arg {\n\t\tswitch b {\n\t\tcase ' ', '\\t':\n\t\t\tif start >= 0 {\n\t\t\t\targs = append(args, arg[start:i])\n\t\t\t\tstart = -1\n\t\t\t}\n\t\tdefault:\n\t\t\tif start < 0 {\n\t\t\t\tstart = i\n\t\t\t}\n\t\t}\n\t}\n\tif start >= 0 {\n\t\targs = append(args, arg[start:])\n\t}\n\n\tc.pa.arg = arg\n\tswitch len(args) {\n\tcase 3:\n\t\tc.pa.subject = args[0]\n\t\tc.pa.reply = nil\n\t\tc.pa.hdr = parseSize(args[1])\n\t\tc.pa.size = parseSize(args[2])\n\t\tc.pa.hdb = args[1]\n\t\tc.pa.szb = args[2]\n\tcase 4:\n\t\tc.pa.subject = args[0]\n\t\tc.pa.reply = args[1]\n\t\tc.pa.hdr = parseSize(args[2])\n\t\tc.pa.size = parseSize(args[3])\n\t\tc.pa.hdb = args[2]\n\t\tc.pa.szb = args[3]\n\tdefault:\n\t\treturn fmt.Errorf(\"processHeaderPub Parse Error: %q\", arg)\n\t}\n\tif c.pa.hdr < 0 {\n\t\treturn fmt.Errorf(\"processHeaderPub Bad or Missing Header Size: %q\", arg)\n\t}\n\t// If number overruns an int64, parseSize() will have returned a negative value\n\tif c.pa.size < 0 {\n\t\treturn fmt.Errorf(\"processHeaderPub Bad or Missing Total Size: %q\", arg)\n\t}\n\tif c.pa.hdr > c.pa.size {\n\t\treturn fmt.Errorf(\"processHeaderPub Header Size larger then TotalSize: %q\", arg)\n\t}\n\tmaxPayload := atomic.LoadInt32(&c.mpay)\n\t// Use int64() to avoid int32 overrun...\n\tif maxPayload != jwt.NoLimit && int64(c.pa.size) > int64(maxPayload) {\n\t\t// If we are given the remaining read buffer (since we do blind reads\n\t\t// we may have the beginning of the message header/payload), we will\n\t\t// look for the tracing header and if found, we will generate a\n\t\t// trace event with the max payload ingress error.\n\t\t// Do this only for CLIENT connections.\n\t\tif c.kind == CLIENT && len(remaining) > 0 {\n\t\t\tif td := getHeader(MsgTraceDest, remaining); len(td) > 0 {\n\t\t\t\tc.initAndSendIngressErrEvent(remaining, string(td), ErrMaxPayload)\n\t\t\t}\n\t\t}\n\t\tc.maxPayloadViolation(c.pa.size, maxPayload)\n\t\treturn ErrMaxPayload\n\t}\n\tif c.opts.Pedantic && !IsValidLiteralSubject(bytesToString(c.pa.subject)) {\n\t\tc.sendErr(\"Invalid Publish Subject\")\n\t}\n\treturn nil\n}\n\nfunc (c *client) processPub(arg []byte) error {\n\t// Unroll splitArgs to avoid runtime/heap issues\n\ta := [MAX_PUB_ARGS][]byte{}\n\targs := a[:0]\n\tstart := -1\n\tfor i, b := range arg {\n\t\tswitch b {\n\t\tcase ' ', '\\t':\n\t\t\tif start >= 0 {\n\t\t\t\targs = append(args, arg[start:i])\n\t\t\t\tstart = -1\n\t\t\t}\n\t\tdefault:\n\t\t\tif start < 0 {\n\t\t\t\tstart = i\n\t\t\t}\n\t\t}\n\t}\n\tif start >= 0 {\n\t\targs = append(args, arg[start:])\n\t}\n\n\tc.pa.arg = arg\n\tswitch len(args) {\n\tcase 2:\n\t\tc.pa.subject = args[0]\n\t\tc.pa.reply = nil\n\t\tc.pa.size = parseSize(args[1])\n\t\tc.pa.szb = args[1]\n\tcase 3:\n\t\tc.pa.subject = args[0]\n\t\tc.pa.reply = args[1]\n\t\tc.pa.size = parseSize(args[2])\n\t\tc.pa.szb = args[2]\n\tdefault:\n\t\treturn fmt.Errorf(\"processPub Parse Error: %q\", arg)\n\t}\n\t// If number overruns an int64, parseSize() will have returned a negative value\n\tif c.pa.size < 0 {\n\t\treturn fmt.Errorf(\"processPub Bad or Missing Size: %q\", arg)\n\t}\n\tmaxPayload := atomic.LoadInt32(&c.mpay)\n\t// Use int64() to avoid int32 overrun...\n\tif maxPayload != jwt.NoLimit && int64(c.pa.size) > int64(maxPayload) {\n\t\tc.maxPayloadViolation(c.pa.size, maxPayload)\n\t\treturn ErrMaxPayload\n\t}\n\tif c.opts.Pedantic && !IsValidLiteralSubject(bytesToString(c.pa.subject)) {\n\t\tc.sendErr(\"Invalid Publish Subject\")\n\t}\n\treturn nil\n}\n\nfunc splitArg(arg []byte) [][]byte {\n\ta := [MAX_MSG_ARGS][]byte{}\n\targs := a[:0]\n\tstart := -1\n\tfor i, b := range arg {\n\t\tswitch b {\n\t\tcase ' ', '\\t', '\\r', '\\n':\n\t\t\tif start >= 0 {\n\t\t\t\targs = append(args, arg[start:i])\n\t\t\t\tstart = -1\n\t\t\t}\n\t\tdefault:\n\t\t\tif start < 0 {\n\t\t\t\tstart = i\n\t\t\t}\n\t\t}\n\t}\n\tif start >= 0 {\n\t\targs = append(args, arg[start:])\n\t}\n\treturn args\n}\n\nfunc (c *client) parseSub(argo []byte, noForward bool) error {\n\t// Copy so we do not reference a potentially large buffer\n\t// FIXME(dlc) - make more efficient.\n\targ := make([]byte, len(argo))\n\tcopy(arg, argo)\n\targs := splitArg(arg)\n\tvar (\n\t\tsubject []byte\n\t\tqueue   []byte\n\t\tsid     []byte\n\t)\n\tswitch len(args) {\n\tcase 2:\n\t\tsubject = args[0]\n\t\tqueue = nil\n\t\tsid = args[1]\n\tcase 3:\n\t\tsubject = args[0]\n\t\tqueue = args[1]\n\t\tsid = args[2]\n\tdefault:\n\t\treturn fmt.Errorf(\"processSub Parse Error: %q\", arg)\n\t}\n\t// If there was an error, it has been sent to the client. We don't return an\n\t// error here to not close the connection as a parsing error.\n\tc.processSub(subject, queue, sid, nil, noForward)\n\treturn nil\n}\n\nfunc (c *client) processSub(subject, queue, bsid []byte, cb msgHandler, noForward bool) (*subscription, error) {\n\treturn c.processSubEx(subject, queue, bsid, cb, noForward, false, false)\n}\n\nfunc (c *client) processSubEx(subject, queue, bsid []byte, cb msgHandler, noForward, si, rsi bool) (*subscription, error) {\n\t// Create the subscription\n\tsub := &subscription{client: c, subject: subject, queue: queue, sid: bsid, icb: cb, si: si, rsi: rsi}\n\n\tc.mu.Lock()\n\n\t// Indicate activity.\n\tc.in.subs++\n\n\t// Grab connection type, account and server info.\n\tkind := c.kind\n\tacc := c.acc\n\tsrv := c.srv\n\n\tsid := bytesToString(sub.sid)\n\n\t// This check does not apply to SYSTEM or JETSTREAM or ACCOUNT clients (because they don't have a `nc`...)\n\t// When a connection is closed though, we set c.subs to nil. So check for the map to not be nil.\n\tif (c.isClosed() && !isInternalClient(kind)) || (c.subs == nil) {\n\t\tc.mu.Unlock()\n\t\treturn nil, ErrConnectionClosed\n\t}\n\n\t// Check permissions if applicable.\n\tif kind == CLIENT {\n\t\t// First do a pass whether queue subscription is valid. This does not necessarily\n\t\t// mean that it will not be able to plain subscribe.\n\t\t//\n\t\t// allow = [\"foo\"]            -> can subscribe or queue subscribe to foo using any queue\n\t\t// allow = [\"foo v1\"]         -> can only queue subscribe to 'foo v1', no plain subs allowed.\n\t\t// allow = [\"foo\", \"foo v1\"]  -> can subscribe to 'foo' but can only queue subscribe to 'foo v1'\n\t\t//\n\t\tif sub.queue != nil {\n\t\t\tif !c.canSubscribe(string(sub.subject), string(sub.queue)) || string(sub.queue) == sysGroup {\n\t\t\t\tc.mu.Unlock()\n\t\t\t\tc.subPermissionViolation(sub)\n\t\t\t\treturn nil, ErrSubscribePermissionViolation\n\t\t\t}\n\t\t} else if !c.canSubscribe(string(sub.subject)) {\n\t\t\tc.mu.Unlock()\n\t\t\tc.subPermissionViolation(sub)\n\t\t\treturn nil, ErrSubscribePermissionViolation\n\t\t}\n\n\t\tif opts := srv.getOpts(); opts != nil && opts.MaxSubTokens > 0 {\n\t\t\tif len(bytes.Split(sub.subject, []byte(tsep))) > int(opts.MaxSubTokens) {\n\t\t\t\tc.mu.Unlock()\n\t\t\t\tc.maxTokensViolation(sub)\n\t\t\t\treturn nil, ErrTooManySubTokens\n\t\t\t}\n\t\t}\n\t}\n\n\t// Check if we have a maximum on the number of subscriptions.\n\tif c.subsAtLimit() {\n\t\tc.mu.Unlock()\n\t\tc.maxSubsExceeded()\n\t\treturn nil, ErrTooManySubs\n\t}\n\n\tvar updateGWs bool\n\tvar err error\n\n\t// Subscribe here.\n\tes := c.subs[sid]\n\tif es == nil {\n\t\tc.subs[sid] = sub\n\t\tif acc != nil && acc.sl != nil {\n\t\t\terr = acc.sl.Insert(sub)\n\t\t\tif err != nil {\n\t\t\t\tdelete(c.subs, sid)\n\t\t\t} else {\n\t\t\t\tupdateGWs = c.srv.gateway.enabled\n\t\t\t}\n\t\t}\n\t}\n\t// Unlocked from here onward\n\tc.mu.Unlock()\n\n\tif err != nil {\n\t\tc.sendErr(\"Invalid Subject\")\n\t\treturn nil, ErrMalformedSubject\n\t} else if c.opts.Verbose && kind != SYSTEM {\n\t\tc.sendOK()\n\t}\n\n\t// If it was already registered, return it.\n\tif es != nil {\n\t\treturn es, nil\n\t}\n\n\t// No account just return.\n\tif acc == nil {\n\t\treturn sub, nil\n\t}\n\n\tif err := c.addShadowSubscriptions(acc, sub, true); err != nil {\n\t\tc.Errorf(err.Error())\n\t}\n\n\tif noForward {\n\t\treturn sub, nil\n\t}\n\n\t// If we are routing and this is a local sub, add to the route map for the associated account.\n\tif kind == CLIENT || kind == SYSTEM || kind == JETSTREAM || kind == ACCOUNT {\n\t\tsrv.updateRouteSubscriptionMap(acc, sub, 1)\n\t\tif updateGWs {\n\t\t\tsrv.gatewayUpdateSubInterest(acc.Name, sub, 1)\n\t\t}\n\t}\n\t// Now check on leafnode updates.\n\tacc.updateLeafNodes(sub, 1)\n\treturn sub, nil\n}\n\n// Used to pass stream import matches to addShadowSub\ntype ime struct {\n\tim          *streamImport\n\toverlapSubj string\n\tdyn         bool\n}\n\n// If the client's account has stream imports and there are matches for this\n// subscription's subject, then add shadow subscriptions in the other accounts\n// that export this subject.\n//\n// enact=false allows MQTT clients to get the list of shadow subscriptions\n// without enacting them, in order to first obtain matching \"retained\" messages.\nfunc (c *client) addShadowSubscriptions(acc *Account, sub *subscription, enact bool) error {\n\tif acc == nil {\n\t\treturn ErrMissingAccount\n\t}\n\n\tvar (\n\t\t_ims           [16]ime\n\t\tims            = _ims[:0]\n\t\timTsa          [32]string\n\t\ttokens         []string\n\t\ttsa            [32]string\n\t\thasWC          bool\n\t\ttokensModified bool\n\t)\n\n\tacc.mu.RLock()\n\t// If this is from a service import, ignore.\n\tif sub.si {\n\t\tacc.mu.RUnlock()\n\t\treturn nil\n\t}\n\tsubj := bytesToString(sub.subject)\n\tif len(acc.imports.streams) > 0 {\n\t\ttokens = tokenizeSubjectIntoSlice(tsa[:0], subj)\n\t\tfor _, tk := range tokens {\n\t\t\tif tk == pwcs {\n\t\t\t\thasWC = true\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\tif !hasWC && tokens[len(tokens)-1] == fwcs {\n\t\t\thasWC = true\n\t\t}\n\t}\n\t// Loop over the import subjects. We have 4 scenarios. If we have an\n\t// exact match or a superset match we should use the from field from\n\t// the import. If we are a subset or overlap, we have to dynamically calculate\n\t// the subject. On overlap, ime requires the overlap subject.\n\tfor _, im := range acc.imports.streams {\n\t\tif im.invalid {\n\t\t\tcontinue\n\t\t}\n\t\tif subj == im.to {\n\t\t\tims = append(ims, ime{im, _EMPTY_, false})\n\t\t\tcontinue\n\t\t}\n\t\tif tokensModified {\n\t\t\t// re-tokenize subj to overwrite modifications from a previous iteration\n\t\t\ttokens = tokenizeSubjectIntoSlice(tsa[:0], subj)\n\t\t\ttokensModified = false\n\t\t}\n\t\timTokens := tokenizeSubjectIntoSlice(imTsa[:0], im.to)\n\n\t\tif isSubsetMatchTokenized(tokens, imTokens) {\n\t\t\tims = append(ims, ime{im, _EMPTY_, true})\n\t\t} else if hasWC {\n\t\t\tif isSubsetMatchTokenized(imTokens, tokens) {\n\t\t\t\tims = append(ims, ime{im, _EMPTY_, false})\n\t\t\t} else {\n\t\t\t\timTokensLen := len(imTokens)\n\t\t\t\tfor i, t := range tokens {\n\t\t\t\t\tif i >= imTokensLen {\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\t\t\t\t\tif t == pwcs && imTokens[i] != fwcs {\n\t\t\t\t\t\ttokens[i] = imTokens[i]\n\t\t\t\t\t\ttokensModified = true\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\ttokensLen := len(tokens)\n\t\t\t\tlastIdx := tokensLen - 1\n\t\t\t\tif tokens[lastIdx] == fwcs {\n\t\t\t\t\tif imTokensLen >= tokensLen {\n\t\t\t\t\t\t// rewrite \">\" in tokens to be more specific\n\t\t\t\t\t\ttokens[lastIdx] = imTokens[lastIdx]\n\t\t\t\t\t\ttokensModified = true\n\t\t\t\t\t\tif imTokensLen > tokensLen {\n\t\t\t\t\t\t\t// copy even more specific parts from import\n\t\t\t\t\t\t\ttokens = append(tokens, imTokens[tokensLen:]...)\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif isSubsetMatchTokenized(tokens, imTokens) {\n\t\t\t\t\t// As isSubsetMatchTokenized was already called with tokens and imTokens,\n\t\t\t\t\t// we wouldn't be here if it where not for tokens being modified.\n\t\t\t\t\t// Hence, Join to re compute the subject string\n\t\t\t\t\tims = append(ims, ime{im, strings.Join(tokens, tsep), true})\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tacc.mu.RUnlock()\n\n\tvar shadow []*subscription\n\n\tif len(ims) > 0 {\n\t\tshadow = make([]*subscription, 0, len(ims))\n\t}\n\n\t// Now walk through collected stream imports that matched.\n\tfor i := 0; i < len(ims); i++ {\n\t\time := &ims[i]\n\t\t// We will create a shadow subscription.\n\t\tnsub, err := c.addShadowSub(sub, ime, enact)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tshadow = append(shadow, nsub)\n\t}\n\n\tif shadow != nil {\n\t\tc.mu.Lock()\n\t\tsub.shadow = shadow\n\t\tc.mu.Unlock()\n\t}\n\n\treturn nil\n}\n\n// Add in the shadow subscription.\nfunc (c *client) addShadowSub(sub *subscription, ime *ime, enact bool) (*subscription, error) {\n\tc.mu.Lock()\n\tnsub := *sub // copy\n\tc.mu.Unlock()\n\n\tim := ime.im\n\tnsub.im = im\n\n\tif !im.usePub && ime.dyn && im.tr != nil {\n\t\tif im.rtr == nil {\n\t\t\tim.rtr = im.tr.reverse()\n\t\t}\n\t\ts := bytesToString(nsub.subject)\n\t\tif ime.overlapSubj != _EMPTY_ {\n\t\t\ts = ime.overlapSubj\n\t\t}\n\t\tsubj := im.rtr.TransformSubject(s)\n\n\t\tnsub.subject = []byte(subj)\n\t} else if !im.usePub || (im.usePub && ime.overlapSubj != _EMPTY_) || !ime.dyn {\n\t\tif ime.overlapSubj != _EMPTY_ {\n\t\t\tnsub.subject = []byte(ime.overlapSubj)\n\t\t} else {\n\t\t\tnsub.subject = []byte(im.from)\n\t\t}\n\t}\n\t// Else use original subject\n\n\tif !enact {\n\t\treturn &nsub, nil\n\t}\n\n\tc.Debugf(\"Creating import subscription on %q from account %q\", nsub.subject, im.acc.Name)\n\n\tif err := im.acc.sl.Insert(&nsub); err != nil {\n\t\terrs := fmt.Sprintf(\"Could not add shadow import subscription for account %q\", im.acc.Name)\n\t\tc.Debugf(errs)\n\t\treturn nil, errors.New(errs)\n\t}\n\n\t// Update our route map here. But only if we are not a leaf node or a hub leafnode.\n\tif c.kind != LEAF || c.isHubLeafNode() {\n\t\tc.srv.updateRemoteSubscription(im.acc, &nsub, 1)\n\t}\n\n\treturn &nsub, nil\n}\n\n// canSubscribe determines if the client is authorized to subscribe to the\n// given subject. Assumes caller is holding lock.\nfunc (c *client) canSubscribe(subject string, optQueue ...string) bool {\n\tif c.perms == nil {\n\t\treturn true\n\t}\n\n\tallowed := true\n\n\t// Optional queue group.\n\tvar queue string\n\tif len(optQueue) > 0 {\n\t\tqueue = optQueue[0]\n\t}\n\n\t// Check allow list. If no allow list that means all are allowed. Deny can overrule.\n\tif c.perms.sub.allow != nil {\n\t\tr := c.perms.sub.allow.Match(subject)\n\t\tallowed = len(r.psubs) > 0\n\t\tif queue != _EMPTY_ && len(r.qsubs) > 0 {\n\t\t\t// If the queue appears in the allow list, then DO allow.\n\t\t\tallowed = queueMatches(queue, r.qsubs)\n\t\t}\n\t\t// Leafnodes operate slightly differently in that they allow broader scoped subjects.\n\t\t// They will prune based on publish perms before sending to a leafnode client.\n\t\tif !allowed && c.kind == LEAF && subjectHasWildcard(subject) {\n\t\t\tr := c.perms.sub.allow.ReverseMatch(subject)\n\t\t\tallowed = len(r.psubs) != 0\n\t\t}\n\t}\n\t// If we have a deny list and we think we are allowed, check that as well.\n\tif allowed && c.perms.sub.deny != nil {\n\t\tr := c.perms.sub.deny.Match(subject)\n\t\tallowed = len(r.psubs) == 0\n\n\t\tif queue != _EMPTY_ && len(r.qsubs) > 0 {\n\t\t\t// If the queue appears in the deny list, then DO NOT allow.\n\t\t\tallowed = !queueMatches(queue, r.qsubs)\n\t\t}\n\n\t\t// We use the actual subscription to signal us to spin up the deny mperms\n\t\t// and cache. We check if the subject is a wildcard that contains any of\n\t\t// the deny clauses.\n\t\t// FIXME(dlc) - We could be smarter and track when these go away and remove.\n\t\tif allowed && c.mperms == nil && subjectHasWildcard(subject) {\n\t\t\t// Whip through the deny array and check if this wildcard subject is within scope.\n\t\t\tfor _, sub := range c.darray {\n\t\t\t\tif subjectIsSubsetMatch(sub, subject) {\n\t\t\t\t\tc.loadMsgDenyFilter()\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn allowed\n}\n\nfunc queueMatches(queue string, qsubs [][]*subscription) bool {\n\tif len(qsubs) == 0 {\n\t\treturn true\n\t}\n\tfor _, qsub := range qsubs {\n\t\tqs := qsub[0]\n\t\tqname := bytesToString(qs.queue)\n\n\t\t// NOTE: '*' and '>' tokens can also be valid\n\t\t// queue names so we first check against the\n\t\t// literal name.  e.g. v1.* == v1.*\n\t\tif queue == qname || (subjectHasWildcard(qname) && subjectIsSubsetMatch(queue, qname)) {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\n// Low level unsubscribe for a given client.\nfunc (c *client) unsubscribe(acc *Account, sub *subscription, force, remove bool) {\n\tif s := c.srv; s != nil && s.isShuttingDown() {\n\t\treturn\n\t}\n\n\tc.mu.Lock()\n\tif !force && sub.max > 0 && sub.nm < sub.max {\n\t\tc.Debugf(\"Deferring actual UNSUB(%s): %d max, %d received\", sub.subject, sub.max, sub.nm)\n\t\tc.mu.Unlock()\n\t\treturn\n\t}\n\n\tif c.trace {\n\t\tc.traceOp(\"<-> %s\", \"DELSUB\", sub.sid)\n\t}\n\n\t// Remove accounting if requested. This will be false when we close a connection\n\t// with open subscriptions.\n\tif remove {\n\t\tdelete(c.subs, bytesToString(sub.sid))\n\t\tif acc != nil {\n\t\t\tacc.sl.Remove(sub)\n\t\t}\n\t}\n\n\t// Check to see if we have shadow subscriptions.\n\tvar updateRoute bool\n\tvar updateGWs bool\n\tshadowSubs := sub.shadow\n\tsub.shadow = nil\n\tif len(shadowSubs) > 0 {\n\t\tupdateRoute = (c.kind == CLIENT || c.kind == SYSTEM || c.kind == LEAF) && c.srv != nil\n\t\tif updateRoute {\n\t\t\tupdateGWs = c.srv.gateway.enabled\n\t\t}\n\t}\n\tsub.close()\n\tc.mu.Unlock()\n\n\t// Process shadow subs if we have them.\n\tfor _, nsub := range shadowSubs {\n\t\tif err := nsub.im.acc.sl.Remove(nsub); err != nil {\n\t\t\tc.Debugf(\"Could not remove shadow import subscription for account %q\", nsub.im.acc.Name)\n\t\t} else {\n\t\t\tif updateRoute {\n\t\t\t\tc.srv.updateRouteSubscriptionMap(nsub.im.acc, nsub, -1)\n\t\t\t}\n\t\t\tif updateGWs {\n\t\t\t\tc.srv.gatewayUpdateSubInterest(nsub.im.acc.Name, nsub, -1)\n\t\t\t}\n\t\t}\n\t\t// Now check on leafnode updates.\n\t\tnsub.im.acc.updateLeafNodes(nsub, -1)\n\t}\n\n\t// Now check to see if this was part of a respMap entry for service imports.\n\t// We can skip subscriptions on reserved replies.\n\tif acc != nil && !isReservedReply(sub.subject) {\n\t\tacc.checkForReverseEntry(string(sub.subject), nil, true)\n\t}\n}\n\nfunc (c *client) processUnsub(arg []byte) error {\n\targs := splitArg(arg)\n\tvar sid []byte\n\tmax := int64(-1)\n\n\tswitch len(args) {\n\tcase 1:\n\t\tsid = args[0]\n\tcase 2:\n\t\tsid = args[0]\n\t\tmax = int64(parseSize(args[1]))\n\tdefault:\n\t\treturn fmt.Errorf(\"processUnsub Parse Error: %q\", arg)\n\t}\n\n\tvar sub *subscription\n\tvar ok, unsub bool\n\n\tc.mu.Lock()\n\n\t// Indicate activity.\n\tc.in.subs++\n\n\t// Grab connection type.\n\tkind := c.kind\n\tsrv := c.srv\n\tvar acc *Account\n\n\tupdateGWs := false\n\tif sub, ok = c.subs[string(sid)]; ok {\n\t\tacc = c.acc\n\t\tif max > 0 && max > sub.nm {\n\t\t\tsub.max = max\n\t\t} else {\n\t\t\t// Clear it here to override\n\t\t\tsub.max = 0\n\t\t\tunsub = true\n\t\t}\n\t\tupdateGWs = srv.gateway.enabled\n\t}\n\tc.mu.Unlock()\n\n\tif c.opts.Verbose {\n\t\tc.sendOK()\n\t}\n\n\tif unsub {\n\t\tc.unsubscribe(acc, sub, false, true)\n\t\tif acc != nil && (kind == CLIENT || kind == SYSTEM || kind == ACCOUNT || kind == JETSTREAM) {\n\t\t\tsrv.updateRouteSubscriptionMap(acc, sub, -1)\n\t\t\tif updateGWs {\n\t\t\t\tsrv.gatewayUpdateSubInterest(acc.Name, sub, -1)\n\t\t\t}\n\t\t}\n\t\t// Now check on leafnode updates.\n\t\tacc.updateLeafNodes(sub, -1)\n\t}\n\n\treturn nil\n}\n\n// checkDenySub will check if we are allowed to deliver this message in the\n// presence of deny clauses for subscriptions. Deny clauses will not prevent\n// larger scoped wildcard subscriptions, so we need to check at delivery time.\n// Lock should be held.\nfunc (c *client) checkDenySub(subject string) bool {\n\tif denied, ok := c.mperms.dcache[subject]; ok {\n\t\treturn denied\n\t} else if np, _ := c.mperms.deny.NumInterest(subject); np != 0 {\n\t\tc.mperms.dcache[subject] = true\n\t\treturn true\n\t} else {\n\t\tc.mperms.dcache[subject] = false\n\t}\n\tif len(c.mperms.dcache) > maxDenyPermCacheSize {\n\t\tc.pruneDenyCache()\n\t}\n\treturn false\n}\n\n// Create a message header for routes or leafnodes. Header and origin cluster aware.\nfunc (c *client) msgHeaderForRouteOrLeaf(subj, reply []byte, rt *routeTarget, acc *Account) []byte {\n\thasHeader := c.pa.hdr > 0\n\tsubclient := rt.sub.client\n\tcanReceiveHeader := subclient.headers\n\n\tmh := c.msgb[:msgHeadProtoLen]\n\tkind := subclient.kind\n\tvar lnoc bool\n\n\tif kind == ROUTER {\n\t\t// If we are coming from a leaf with an origin cluster we need to handle differently\n\t\t// if we can. We will send a route based LMSG which has origin cluster and headers\n\t\t// by default.\n\t\tif c.kind == LEAF && c.remoteCluster() != _EMPTY_ {\n\t\t\tsubclient.mu.Lock()\n\t\t\tlnoc = subclient.route.lnoc\n\t\t\tsubclient.mu.Unlock()\n\t\t}\n\t\tif lnoc {\n\t\t\tmh[0] = 'L'\n\t\t\tmh = append(mh, c.remoteCluster()...)\n\t\t\tmh = append(mh, ' ')\n\t\t} else {\n\t\t\t// Router (and Gateway) nodes are RMSG. Set here since leafnodes may rewrite.\n\t\t\tmh[0] = 'R'\n\t\t}\n\t\tif len(subclient.route.accName) == 0 {\n\t\t\tmh = append(mh, acc.Name...)\n\t\t\tmh = append(mh, ' ')\n\t\t}\n\t} else {\n\t\t// Leaf nodes are LMSG\n\t\tmh[0] = 'L'\n\t\t// Remap subject if its a shadow subscription, treat like a normal client.\n\t\tif rt.sub.im != nil {\n\t\t\tif rt.sub.im.tr != nil {\n\t\t\t\tto := rt.sub.im.tr.TransformSubject(bytesToString(subj))\n\t\t\t\tsubj = []byte(to)\n\t\t\t} else if !rt.sub.im.usePub {\n\t\t\t\tsubj = []byte(rt.sub.im.to)\n\t\t\t}\n\t\t}\n\t}\n\tmh = append(mh, subj...)\n\tmh = append(mh, ' ')\n\n\tif len(rt.qs) > 0 {\n\t\tif len(reply) > 0 {\n\t\t\tmh = append(mh, \"+ \"...) // Signal that there is a reply.\n\t\t\tmh = append(mh, reply...)\n\t\t\tmh = append(mh, ' ')\n\t\t} else {\n\t\t\tmh = append(mh, \"| \"...) // Only queues\n\t\t}\n\t\tmh = append(mh, rt.qs...)\n\t} else if len(reply) > 0 {\n\t\tmh = append(mh, reply...)\n\t\tmh = append(mh, ' ')\n\t}\n\n\tif lnoc {\n\t\t// leafnode origin LMSG always have a header entry even if zero.\n\t\tif c.pa.hdr <= 0 {\n\t\t\tmh = append(mh, '0')\n\t\t} else {\n\t\t\tmh = append(mh, c.pa.hdb...)\n\t\t}\n\t\tmh = append(mh, ' ')\n\t\tmh = append(mh, c.pa.szb...)\n\t} else if hasHeader {\n\t\tif canReceiveHeader {\n\t\t\tmh[0] = 'H'\n\t\t\tmh = append(mh, c.pa.hdb...)\n\t\t\tmh = append(mh, ' ')\n\t\t\tmh = append(mh, c.pa.szb...)\n\t\t} else {\n\t\t\t// If we are here we need to truncate the payload size\n\t\t\tnsz := strconv.Itoa(c.pa.size - c.pa.hdr)\n\t\t\tmh = append(mh, nsz...)\n\t\t}\n\t} else {\n\t\tmh = append(mh, c.pa.szb...)\n\t}\n\treturn append(mh, _CRLF_...)\n}\n\n// Create a message header for clients. Header aware.\nfunc (c *client) msgHeader(subj, reply []byte, sub *subscription) []byte {\n\t// See if we should do headers. We have to have a headers msg and\n\t// the client we are going to deliver to needs to support headers as well.\n\thasHeader := c.pa.hdr > 0\n\tcanReceiveHeader := sub.client != nil && sub.client.headers\n\n\tvar mh []byte\n\tif hasHeader && canReceiveHeader {\n\t\tmh = c.msgb[:msgHeadProtoLen]\n\t\tmh[0] = 'H'\n\t} else {\n\t\tmh = c.msgb[1:msgHeadProtoLen]\n\t}\n\tmh = append(mh, subj...)\n\tmh = append(mh, ' ')\n\n\tif len(sub.sid) > 0 {\n\t\tmh = append(mh, sub.sid...)\n\t\tmh = append(mh, ' ')\n\t}\n\tif reply != nil {\n\t\tmh = append(mh, reply...)\n\t\tmh = append(mh, ' ')\n\t}\n\tif hasHeader {\n\t\tif canReceiveHeader {\n\t\t\tmh = append(mh, c.pa.hdb...)\n\t\t\tmh = append(mh, ' ')\n\t\t\tmh = append(mh, c.pa.szb...)\n\t\t} else {\n\t\t\t// If we are here we need to truncate the payload size\n\t\t\tnsz := strconv.Itoa(c.pa.size - c.pa.hdr)\n\t\t\tmh = append(mh, nsz...)\n\t\t}\n\t} else {\n\t\tmh = append(mh, c.pa.szb...)\n\t}\n\tmh = append(mh, _CRLF_...)\n\treturn mh\n}\n\nfunc (c *client) stalledWait(producer *client) {\n\t// Check to see if we have exceeded our total wait time per readLoop invocation.\n\tif producer.in.tst > stallTotalAllowed {\n\t\treturn\n\t}\n\n\t// Grab stall channel which the slow consumer will close when caught up.\n\tstall := c.out.stc\n\n\t// Calculate stall time.\n\tttl := stallClientMinDuration\n\tif c.out.pb >= c.out.mp {\n\t\tttl = stallClientMaxDuration\n\t}\n\n\tc.mu.Unlock()\n\tdefer c.mu.Lock()\n\n\t// Now check if we are close to total allowed.\n\tif producer.in.tst+ttl > stallTotalAllowed {\n\t\tttl = stallTotalAllowed - producer.in.tst\n\t}\n\tdelay := time.NewTimer(ttl)\n\tdefer delay.Stop()\n\n\tstart := time.Now()\n\tselect {\n\tcase <-stall:\n\tcase <-delay.C:\n\t\tproducer.Debugf(\"Timed out of fast producer stall (%v)\", ttl)\n\t}\n\tproducer.in.tst += time.Since(start)\n}\n\n// Used to treat maps as efficient set\nvar needFlush = struct{}{}\n\n// deliverMsg will deliver a message to a matching subscription and its underlying client.\n// We process all connection/client types. mh is the part that will be protocol/client specific.\nfunc (c *client) deliverMsg(prodIsMQTT bool, sub *subscription, acc *Account, subject, reply, mh, msg []byte, gwrply bool) bool {\n\t// Check if message tracing is enabled.\n\tmt, traceOnly := c.isMsgTraceEnabled()\n\n\tclient := sub.client\n\t// Check sub client and check echo. Only do this if not a service import.\n\tif client == nil || (c == client && !client.echo && !sub.si) {\n\t\tif client != nil && mt != nil {\n\t\t\tclient.mu.Lock()\n\t\t\tmt.addEgressEvent(client, sub, errMsgTraceNoEcho)\n\t\t\tclient.mu.Unlock()\n\t\t}\n\t\treturn false\n\t}\n\n\tclient.mu.Lock()\n\n\t// Check if we have a subscribe deny clause. This will trigger us to check the subject\n\t// for a match against the denied subjects.\n\tif client.mperms != nil && client.checkDenySub(string(subject)) {\n\t\tmt.addEgressEvent(client, sub, errMsgTraceSubDeny)\n\t\tclient.mu.Unlock()\n\t\treturn false\n\t}\n\n\t// New race detector forces this now.\n\tif sub.isClosed() {\n\t\tmt.addEgressEvent(client, sub, errMsgTraceSubClosed)\n\t\tclient.mu.Unlock()\n\t\treturn false\n\t}\n\n\t// Check if we are a leafnode and have perms to check.\n\tif client.kind == LEAF && client.perms != nil {\n\t\tvar subjectToCheck []byte\n\t\tif subject[0] == '_' && bytes.HasPrefix(subject, []byte(gwReplyPrefix)) {\n\t\t\tsubjectToCheck = subject[gwSubjectOffset:]\n\t\t} else if subject[0] == '$' && bytes.HasPrefix(subject, []byte(oldGWReplyPrefix)) {\n\t\t\tsubjectToCheck = subject[oldGWReplyStart:]\n\t\t} else {\n\t\t\tsubjectToCheck = subject\n\t\t}\n\t\tif !client.pubAllowedFullCheck(string(subjectToCheck), true, true) {\n\t\t\tmt.addEgressEvent(client, sub, errMsgTracePubViolation)\n\t\t\tclient.mu.Unlock()\n\t\t\tclient.Debugf(\"Not permitted to deliver to %q\", subjectToCheck)\n\t\t\treturn false\n\t\t}\n\t}\n\n\tvar mtErr string\n\tif mt != nil {\n\t\t// For non internal subscription, and if the remote does not support\n\t\t// the tracing feature...\n\t\tif sub.icb == nil && !client.msgTraceSupport() {\n\t\t\tif traceOnly {\n\t\t\t\t// We are not sending the message at all because the user\n\t\t\t\t// expects a trace-only and the remote does not support\n\t\t\t\t// tracing, which means that it would process/deliver this\n\t\t\t\t// message, which may break applications.\n\t\t\t\t// Add the Egress with the no-support error message.\n\t\t\t\tmt.addEgressEvent(client, sub, errMsgTraceOnlyNoSupport)\n\t\t\t\tclient.mu.Unlock()\n\t\t\t\treturn false\n\t\t\t}\n\t\t\t// If we are doing delivery, we will still forward the message,\n\t\t\t// but we add an error to the Egress event to hint that one should\n\t\t\t// not expect a tracing event from that remote.\n\t\t\tmtErr = errMsgTraceNoSupport\n\t\t}\n\t\t// For ROUTER, GATEWAY and LEAF, even if we intend to do tracing only,\n\t\t// we will still deliver the message. The remote side will\n\t\t// generate an event based on what happened on that server.\n\t\tif traceOnly && (client.kind == ROUTER || client.kind == GATEWAY || client.kind == LEAF) {\n\t\t\ttraceOnly = false\n\t\t}\n\t\t// If we skip delivery and this is not for a service import, we are done.\n\t\tif traceOnly && (sub.icb == nil || c.noIcb) {\n\t\t\tmt.addEgressEvent(client, sub, _EMPTY_)\n\t\t\tclient.mu.Unlock()\n\t\t\t// Although the message is not actually delivered, for the\n\t\t\t// purpose of \"didDeliver\", we need to return \"true\" here.\n\t\t\treturn true\n\t\t}\n\t}\n\n\tsrv := client.srv\n\n\t// We don't want to bump the number of delivered messages to the subscription\n\t// if we are doing trace-only (since really we are not sending it to the sub).\n\tif !traceOnly {\n\t\tsub.nm++\n\t}\n\n\t// Check if we should auto-unsubscribe.\n\tif sub.max > 0 {\n\t\tif client.kind == ROUTER && sub.nm >= sub.max {\n\t\t\t// The only router based messages that we will see here are remoteReplies.\n\t\t\t// We handle these slightly differently.\n\t\t\tdefer client.removeReplySub(sub)\n\t\t} else {\n\t\t\t// For routing..\n\t\t\tshouldForward := client.kind == CLIENT || client.kind == SYSTEM && client.srv != nil\n\t\t\t// If we are at the exact number, unsubscribe but\n\t\t\t// still process the message in hand, otherwise\n\t\t\t// unsubscribe and drop message on the floor.\n\t\t\tif sub.nm == sub.max {\n\t\t\t\tclient.Debugf(\"Auto-unsubscribe limit of %d reached for sid '%s'\", sub.max, sub.sid)\n\t\t\t\t// Due to defer, reverse the code order so that execution\n\t\t\t\t// is consistent with other cases where we unsubscribe.\n\t\t\t\tif shouldForward {\n\t\t\t\t\tdefer srv.updateRemoteSubscription(client.acc, sub, -1)\n\t\t\t\t}\n\t\t\t\tdefer client.unsubscribe(client.acc, sub, true, true)\n\t\t\t} else if sub.nm > sub.max {\n\t\t\t\tclient.Debugf(\"Auto-unsubscribe limit [%d] exceeded\", sub.max)\n\t\t\t\tmt.addEgressEvent(client, sub, errMsgTraceAutoSubExceeded)\n\t\t\t\tclient.mu.Unlock()\n\t\t\t\tclient.unsubscribe(client.acc, sub, true, true)\n\t\t\t\tif shouldForward {\n\t\t\t\t\tsrv.updateRemoteSubscription(client.acc, sub, -1)\n\t\t\t\t}\n\t\t\t\treturn false\n\t\t\t}\n\t\t}\n\t}\n\n\t// Check here if we have a header with our message. If this client can not\n\t// support we need to strip the headers from the payload.\n\t// The actual header would have been processed correctly for us, so just\n\t// need to update payload.\n\tif c.pa.hdr > 0 && !sub.client.headers {\n\t\tmsg = msg[c.pa.hdr:]\n\t}\n\n\t// Update statistics\n\n\t// The msg includes the CR_LF, so pull back out for accounting.\n\tmsgSize := int64(len(msg))\n\t// MQTT producers send messages without CR_LF, so don't remove it for them.\n\tif !prodIsMQTT {\n\t\tmsgSize -= int64(LEN_CR_LF)\n\t}\n\n\t// We do not update the outbound stats if we are doing trace only since\n\t// this message will not be sent out.\n\t// Also do not update on internal callbacks.\n\tif !traceOnly && sub.icb == nil {\n\t\t// No atomic needed since accessed under client lock.\n\t\t// Monitor is reading those also under client's lock.\n\t\tclient.outMsgs++\n\t\tclient.outBytes += msgSize\n\t}\n\n\t// Check for internal subscriptions.\n\tif sub.icb != nil && !c.noIcb {\n\t\tif gwrply {\n\t\t\t// We will store in the account, not the client since it will likely\n\t\t\t// be a different client that will send the reply.\n\t\t\tsrv.trackGWReply(nil, client.acc, reply, c.pa.reply)\n\t\t}\n\t\tclient.mu.Unlock()\n\n\t\t// For service imports, track if we delivered.\n\t\tdidDeliver := true\n\n\t\t// Internal account clients are for service imports and need the '\\r\\n'.\n\t\tstart := time.Now()\n\t\tif client.kind == ACCOUNT {\n\t\t\tsub.icb(sub, c, acc, string(subject), string(reply), msg)\n\t\t\t// If we are a service import check to make sure we delivered the message somewhere.\n\t\t\tif sub.si {\n\t\t\t\tdidDeliver = c.pa.delivered\n\t\t\t}\n\t\t} else {\n\t\t\tsub.icb(sub, c, acc, string(subject), string(reply), msg[:msgSize])\n\t\t}\n\t\tif dur := time.Since(start); dur >= readLoopReportThreshold {\n\t\t\tsrv.Warnf(\"Internal subscription on %q took too long: %v\", subject, dur)\n\t\t}\n\n\t\treturn didDeliver\n\t}\n\n\t// If we are a client and we detect that the consumer we are\n\t// sending to is in a stalled state, go ahead and wait here\n\t// with a limit.\n\tif c.kind == CLIENT && client.out.stc != nil {\n\t\tif srv.getOpts().NoFastProducerStall {\n\t\t\tmt.addEgressEvent(client, sub, errMsgTraceFastProdNoStall)\n\t\t\tclient.mu.Unlock()\n\t\t\treturn false\n\t\t}\n\t\tclient.stalledWait(c)\n\t}\n\n\t// Check for closed connection\n\tif client.isClosed() {\n\t\tmt.addEgressEvent(client, sub, errMsgTraceClientClosed)\n\t\tclient.mu.Unlock()\n\t\treturn false\n\t}\n\n\t// We have passed cases where we could possibly fail to deliver.\n\t// Do not call for service-import.\n\tif mt != nil && sub.icb == nil {\n\t\tmt.addEgressEvent(client, sub, mtErr)\n\t}\n\n\t// Do a fast check here to see if we should be tracking this from a latency\n\t// perspective. This will be for a request being received for an exported service.\n\t// This needs to be from a non-client (otherwise tracking happens at requestor).\n\t//\n\t// Also this check captures if the original reply (c.pa.reply) is a GW routed\n\t// reply (since it is known to be > minReplyLen). If that is the case, we need to\n\t// track the binding between the routed reply and the reply set in the message\n\t// header (which is c.pa.reply without the GNR routing prefix).\n\tif client.kind == CLIENT && len(c.pa.reply) > minReplyLen {\n\t\tif gwrply {\n\t\t\t// Note that we keep track of the GW routed reply in the destination\n\t\t\t// connection (`client`). The routed reply subject is in `c.pa.reply`,\n\t\t\t// should that change, we would have to pass the GW routed reply as\n\t\t\t// a parameter of deliverMsg().\n\t\t\tsrv.trackGWReply(client, nil, reply, c.pa.reply)\n\t\t}\n\n\t\t// If we do not have a registered RTT queue that up now.\n\t\tif client.rtt == 0 {\n\t\t\tclient.sendRTTPingLocked()\n\t\t}\n\t\t// FIXME(dlc) - We may need to optimize this.\n\t\t// We will have tagged this with a suffix ('.T') if we are tracking. This is\n\t\t// needed from sampling. Not all will be tracked.\n\t\tif c.kind != CLIENT && isTrackedReply(c.pa.reply) {\n\t\t\tclient.trackRemoteReply(string(subject), string(c.pa.reply))\n\t\t}\n\t}\n\n\t// Queue to outbound buffer\n\tclient.queueOutbound(mh)\n\tclient.queueOutbound(msg)\n\tif prodIsMQTT {\n\t\t// Need to add CR_LF since MQTT producers don't send CR_LF\n\t\tclient.queueOutbound([]byte(CR_LF))\n\t}\n\n\t// If we are tracking dynamic publish permissions that track reply subjects,\n\t// do that accounting here. We only look at client.replies which will be non-nil.\n\t// Only reply subject permissions if the client is not already allowed to publish to the reply subject.\n\tif client.replies != nil && len(reply) > 0 && !client.pubAllowedFullCheck(string(reply), true, true) {\n\t\tclient.replies[string(reply)] = &resp{time.Now(), 0}\n\t\tclient.repliesSincePrune++\n\t\tif client.repliesSincePrune > replyPermLimit || time.Since(client.lastReplyPrune) > replyPruneTime {\n\t\t\tclient.pruneReplyPerms()\n\t\t}\n\t}\n\n\t// Check outbound threshold and queue IO flush if needed.\n\t// This is specifically looking at situations where we are getting behind and may want\n\t// to intervene before this producer goes back to top of readloop. We are in the producer's\n\t// readloop go routine at this point.\n\t// FIXME(dlc) - We may call this alot, maybe suppress after first call?\n\tif len(client.out.nb) != 0 {\n\t\tclient.flushSignal()\n\t}\n\n\t// Add the data size we are responsible for here. This will be processed when we\n\t// return to the top of the readLoop.\n\tc.addToPCD(client)\n\n\tif client.trace {\n\t\tclient.traceOutOp(bytesToString(mh[:len(mh)-LEN_CR_LF]), nil)\n\t}\n\n\tclient.mu.Unlock()\n\n\treturn true\n}\n\n// Add the given sub's client to the list of clients that need flushing.\n// This must be invoked from `c`'s readLoop. No lock for c is required,\n// however, `client` lock must be held on entry. This holds true even\n// if `client` is same than `c`.\nfunc (c *client) addToPCD(client *client) {\n\tif _, ok := c.pcd[client]; !ok {\n\t\tclient.out.fsp++\n\t\tc.pcd[client] = needFlush\n\t}\n}\n\n// This will track a remote reply for an exported service that has requested\n// latency tracking.\n// Lock assumed to be held.\nfunc (c *client) trackRemoteReply(subject, reply string) {\n\ta := c.acc\n\tif a == nil {\n\t\treturn\n\t}\n\n\tvar lrt time.Duration\n\tvar respThresh time.Duration\n\n\ta.mu.RLock()\n\tse := a.getServiceExport(subject)\n\tif se != nil {\n\t\tlrt = a.lowestServiceExportResponseTime()\n\t\trespThresh = se.respThresh\n\t}\n\ta.mu.RUnlock()\n\n\tif se == nil {\n\t\treturn\n\t}\n\n\tif c.rrTracking == nil {\n\t\tc.rrTracking = &rrTracking{\n\t\t\trmap: make(map[string]*remoteLatency),\n\t\t\tptmr: time.AfterFunc(lrt, c.pruneRemoteTracking),\n\t\t\tlrt:  lrt,\n\t\t}\n\t}\n\trl := remoteLatency{\n\t\tAccount:    a.Name,\n\t\tReqId:      reply,\n\t\trespThresh: respThresh,\n\t}\n\trl.M2.RequestStart = time.Now().UTC()\n\tc.rrTracking.rmap[reply] = &rl\n}\n\n// pruneRemoteTracking will prune any remote tracking objects\n// that are too old. These are orphaned when a service is not\n// sending reponses etc.\n// Lock should be held upon entry.\nfunc (c *client) pruneRemoteTracking() {\n\tc.mu.Lock()\n\tif c.rrTracking == nil {\n\t\tc.mu.Unlock()\n\t\treturn\n\t}\n\tnow := time.Now()\n\tfor subject, rl := range c.rrTracking.rmap {\n\t\tif now.After(rl.M2.RequestStart.Add(rl.respThresh)) {\n\t\t\tdelete(c.rrTracking.rmap, subject)\n\t\t}\n\t}\n\tif len(c.rrTracking.rmap) > 0 {\n\t\tt := c.rrTracking.ptmr\n\t\tt.Stop()\n\t\tt.Reset(c.rrTracking.lrt)\n\t} else {\n\t\tc.rrTracking.ptmr.Stop()\n\t\tc.rrTracking = nil\n\t}\n\tc.mu.Unlock()\n}\n\n// pruneReplyPerms will remove any stale or expired entries\n// in our reply cache. We make sure to not check too often.\nfunc (c *client) pruneReplyPerms() {\n\t// Make sure we do not check too often.\n\tif c.perms.resp == nil {\n\t\treturn\n\t}\n\n\tmm := c.perms.resp.MaxMsgs\n\tttl := c.perms.resp.Expires\n\tnow := time.Now()\n\n\tfor k, resp := range c.replies {\n\t\tif mm > 0 && resp.n >= mm {\n\t\t\tdelete(c.replies, k)\n\t\t} else if ttl > 0 && now.Sub(resp.t) > ttl {\n\t\t\tdelete(c.replies, k)\n\t\t}\n\t}\n\n\tc.repliesSincePrune = 0\n\tc.lastReplyPrune = now\n}\n\n// pruneDenyCache will prune the deny cache via randomly\n// deleting items. Doing so pruneSize items at a time.\n// Lock must be held for this one since it is shared under\n// deliverMsg.\nfunc (c *client) pruneDenyCache() {\n\tr := 0\n\tfor subject := range c.mperms.dcache {\n\t\tdelete(c.mperms.dcache, subject)\n\t\tif r++; r > pruneSize {\n\t\t\tbreak\n\t\t}\n\t}\n}\n\n// prunePubPermsCache will prune the cache via randomly\n// deleting items. Doing so pruneSize items at a time.\nfunc (c *client) prunePubPermsCache() {\n\t// With parallel additions to the cache, it is possible that this function\n\t// would not be able to reduce the cache to its max size in one go. We\n\t// will try a few times but will release/reacquire the \"lock\" at each\n\t// attempt to give a chance to another go routine to take over and not\n\t// have this go routine do too many attempts.\n\tfor i := 0; i < 5; i++ {\n\t\t// There is a case where we can invoke this from multiple go routines,\n\t\t// (in deliverMsg() if sub.client is a LEAF), so we make sure to prune\n\t\t// from only one go routine at a time.\n\t\tif !atomic.CompareAndSwapInt32(&c.perms.prun, 0, 1) {\n\t\t\treturn\n\t\t}\n\t\tconst maxPruneAtOnce = 1000\n\t\tr := 0\n\t\tc.perms.pcache.Range(func(k, _ any) bool {\n\t\t\tc.perms.pcache.Delete(k)\n\t\t\tif r++; (r > pruneSize && atomic.LoadInt32(&c.perms.pcsz) < int32(maxPermCacheSize)) ||\n\t\t\t\t(r > maxPruneAtOnce) {\n\t\t\t\treturn false\n\t\t\t}\n\t\t\treturn true\n\t\t})\n\t\tn := atomic.AddInt32(&c.perms.pcsz, -int32(r))\n\t\tatomic.StoreInt32(&c.perms.prun, 0)\n\t\tif n <= int32(maxPermCacheSize) {\n\t\t\treturn\n\t\t}\n\t}\n}\n\n// pubAllowed checks on publish permissioning.\n// Lock should not be held.\nfunc (c *client) pubAllowed(subject string) bool {\n\treturn c.pubAllowedFullCheck(subject, true, false)\n}\n\n// pubAllowedFullCheck checks on all publish permissioning depending\n// on the flag for dynamic reply permissions.\nfunc (c *client) pubAllowedFullCheck(subject string, fullCheck, hasLock bool) bool {\n\tif c.perms == nil || (c.perms.pub.allow == nil && c.perms.pub.deny == nil) {\n\t\treturn true\n\t}\n\t// Check if published subject is allowed if we have permissions in place.\n\tv, ok := c.perms.pcache.Load(subject)\n\tif ok {\n\t\treturn v.(bool)\n\t}\n\tallowed := true\n\t// Cache miss, check allow then deny as needed.\n\tif c.perms.pub.allow != nil {\n\t\tnp, _ := c.perms.pub.allow.NumInterest(subject)\n\t\tallowed = np != 0\n\t}\n\t// If we have a deny list and are currently allowed, check that as well.\n\tif allowed && c.perms.pub.deny != nil {\n\t\tnp, _ := c.perms.pub.deny.NumInterest(subject)\n\t\tallowed = np == 0\n\t}\n\n\t// If we are tracking reply subjects\n\t// dynamically, check to see if we are allowed here but avoid pcache.\n\t// We need to acquire the lock though.\n\tif !allowed && fullCheck && c.perms.resp != nil {\n\t\tif !hasLock {\n\t\t\tc.mu.Lock()\n\t\t}\n\t\tif resp := c.replies[subject]; resp != nil {\n\t\t\tresp.n++\n\t\t\t// Check if we have sent too many responses.\n\t\t\tif c.perms.resp.MaxMsgs > 0 && resp.n > c.perms.resp.MaxMsgs {\n\t\t\t\tdelete(c.replies, subject)\n\t\t\t} else if c.perms.resp.Expires > 0 && time.Since(resp.t) > c.perms.resp.Expires {\n\t\t\t\tdelete(c.replies, subject)\n\t\t\t} else {\n\t\t\t\tallowed = true\n\t\t\t}\n\t\t}\n\t\tif !hasLock {\n\t\t\tc.mu.Unlock()\n\t\t}\n\t} else {\n\t\t// Update our cache here.\n\t\tc.perms.pcache.Store(subject, allowed)\n\t\tif n := atomic.AddInt32(&c.perms.pcsz, 1); n > maxPermCacheSize {\n\t\t\tc.prunePubPermsCache()\n\t\t}\n\t}\n\treturn allowed\n}\n\n// Test whether a reply subject is a service import reply.\nfunc isServiceReply(reply []byte) bool {\n\t// This function is inlined and checking this way is actually faster\n\t// than byte-by-byte comparison.\n\treturn len(reply) > 3 && bytesToString(reply[:4]) == replyPrefix\n}\n\n// Test whether a reply subject is a service import or a gateway routed reply.\nfunc isReservedReply(reply []byte) bool {\n\tif isServiceReply(reply) {\n\t\treturn true\n\t}\n\trLen := len(reply)\n\t// Faster to check with string([:]) than byte-by-byte\n\tif rLen > jsAckPreLen && bytesToString(reply[:jsAckPreLen]) == jsAckPre {\n\t\treturn true\n\t} else if rLen > gwReplyPrefixLen && bytesToString(reply[:gwReplyPrefixLen]) == gwReplyPrefix {\n\t\treturn true\n\t}\n\treturn false\n}\n\n// This will decide to call the client code or router code.\nfunc (c *client) processInboundMsg(msg []byte) {\n\tswitch c.kind {\n\tcase CLIENT:\n\t\tc.processInboundClientMsg(msg)\n\tcase ROUTER:\n\t\tc.processInboundRoutedMsg(msg)\n\tcase GATEWAY:\n\t\tc.processInboundGatewayMsg(msg)\n\tcase LEAF:\n\t\tc.processInboundLeafMsg(msg)\n\t}\n}\n\n// selectMappedSubject will choose the mapped subject based on the client's inbound subject.\nfunc (c *client) selectMappedSubject() bool {\n\tnsubj, changed := c.acc.selectMappedSubject(bytesToString(c.pa.subject))\n\tif changed {\n\t\tc.pa.mapped = c.pa.subject\n\t\tc.pa.subject = []byte(nsubj)\n\t}\n\treturn changed\n}\n\n// clientNRGPrefix is used in processInboundClientMsg to detect if publishes\n// are being made from normal clients to NRG subjects.\nvar clientNRGPrefix = []byte(\"$NRG.\")\n\n// processInboundClientMsg is called to process an inbound msg from a client.\n// Return if the message was delivered, and if the message was not delivered\n// due to a permission issue.\nfunc (c *client) processInboundClientMsg(msg []byte) (bool, bool) {\n\t// Update statistics\n\t// The msg includes the CR_LF, so pull back out for accounting.\n\tc.in.msgs++\n\tc.in.bytes += int32(len(msg) - LEN_CR_LF)\n\n\t// Check that client (could be here with SYSTEM) is not publishing on reserved \"$GNR\" prefix.\n\tif c.kind == CLIENT && hasGWRoutedReplyPrefix(c.pa.subject) {\n\t\tc.pubPermissionViolation(c.pa.subject)\n\t\treturn false, true\n\t}\n\n\t// Mostly under testing scenarios.\n\tc.mu.Lock()\n\tif c.srv == nil || c.acc == nil {\n\t\tc.mu.Unlock()\n\t\treturn false, false\n\t}\n\tacc := c.acc\n\tgenidAddr := &acc.sl.genid\n\n\t// Check pub permissions\n\tif c.perms != nil && (c.perms.pub.allow != nil || c.perms.pub.deny != nil) && !c.pubAllowedFullCheck(string(c.pa.subject), true, true) {\n\t\tc.mu.Unlock()\n\t\tc.pubPermissionViolation(c.pa.subject)\n\t\treturn false, true\n\t}\n\tc.mu.Unlock()\n\n\t// Check if the client is trying to publish to reserved NRG subjects.\n\t// Doesn't apply to NRGs themselves as they use SYSTEM-kind clients instead.\n\tif c.kind == CLIENT && bytes.HasPrefix(c.pa.subject, clientNRGPrefix) && acc != c.srv.SystemAccount() {\n\t\tc.pubPermissionViolation(c.pa.subject)\n\t\treturn false, true\n\t}\n\n\t// Now check for reserved replies. These are used for service imports.\n\tif c.kind == CLIENT && len(c.pa.reply) > 0 && isReservedReply(c.pa.reply) {\n\t\tc.replySubjectViolation(c.pa.reply)\n\t\treturn false, true\n\t}\n\n\tif c.opts.Verbose {\n\t\tc.sendOK()\n\t}\n\n\t// If MQTT client, check for retain flag now that we have passed permissions check\n\tif c.isMqtt() {\n\t\tc.mqttHandlePubRetain()\n\t}\n\n\t// Doing this inline as opposed to create a function (which otherwise has a measured\n\t// performance impact reported in our bench)\n\tvar isGWRouted bool\n\tif c.kind != CLIENT {\n\t\tif atomic.LoadInt32(&acc.gwReplyMapping.check) > 0 {\n\t\t\tacc.mu.RLock()\n\t\t\tc.pa.subject, isGWRouted = acc.gwReplyMapping.get(c.pa.subject)\n\t\t\tacc.mu.RUnlock()\n\t\t}\n\t} else if atomic.LoadInt32(&c.gwReplyMapping.check) > 0 {\n\t\tc.mu.Lock()\n\t\tc.pa.subject, isGWRouted = c.gwReplyMapping.get(c.pa.subject)\n\t\tc.mu.Unlock()\n\t}\n\n\t// If we have an exported service and we are doing remote tracking, check this subject\n\t// to see if we need to report the latency.\n\tif c.rrTracking != nil {\n\t\tc.mu.Lock()\n\t\trl := c.rrTracking.rmap[string(c.pa.subject)]\n\t\tif rl != nil {\n\t\t\tdelete(c.rrTracking.rmap, bytesToString(c.pa.subject))\n\t\t}\n\t\tc.mu.Unlock()\n\n\t\tif rl != nil {\n\t\t\tsl := &rl.M2\n\t\t\t// Fill this in and send it off to the other side.\n\t\t\tsl.Status = 200\n\t\t\tsl.Responder = c.getClientInfo(true)\n\t\t\tsl.ServiceLatency = time.Since(sl.RequestStart) - sl.Responder.RTT\n\t\t\tsl.TotalLatency = sl.ServiceLatency + sl.Responder.RTT\n\t\t\tsanitizeLatencyMetric(sl)\n\t\t\tlsub := remoteLatencySubjectForResponse(c.pa.subject)\n\t\t\tc.srv.sendInternalAccountMsg(nil, lsub, rl) // Send to SYS account\n\t\t}\n\t}\n\n\t// If the subject was converted to the gateway routed subject, then handle it now\n\t// and be done with the rest of this function.\n\tif isGWRouted {\n\t\tc.handleGWReplyMap(msg)\n\t\treturn true, false\n\t}\n\n\t// Match the subscriptions. We will use our own L1 map if\n\t// it's still valid, avoiding contention on the shared sublist.\n\tvar r *SublistResult\n\tvar ok bool\n\n\tgenid := atomic.LoadUint64(genidAddr)\n\tif genid == c.in.genid && c.in.results != nil {\n\t\tr, ok = c.in.results[string(c.pa.subject)]\n\t} else {\n\t\t// Reset our L1 completely.\n\t\tc.in.results = make(map[string]*SublistResult)\n\t\tc.in.genid = genid\n\t}\n\n\t// Go back to the sublist data structure.\n\tif !ok {\n\t\t// Match may use the subject here to populate a cache, so can not use bytesToString here.\n\t\tr = acc.sl.Match(string(c.pa.subject))\n\t\tif len(r.psubs)+len(r.qsubs) > 0 {\n\t\t\t// Prune the results cache. Keeps us from unbounded growth. Random delete.\n\t\t\tif len(c.in.results) >= maxResultCacheSize {\n\t\t\t\tn := 0\n\t\t\t\tfor subject := range c.in.results {\n\t\t\t\t\tdelete(c.in.results, subject)\n\t\t\t\t\tif n++; n > pruneSize {\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\t// Then add the new cache entry.\n\t\t\tc.in.results[string(c.pa.subject)] = r\n\t\t}\n\t}\n\n\t// Indication if we attempted to deliver the message to anyone.\n\tvar didDeliver bool\n\tvar qnames [][]byte\n\n\t// Check for no interest, short circuit if so.\n\t// This is the fanout scale.\n\tif len(r.psubs)+len(r.qsubs) > 0 {\n\t\tflag := pmrNoFlag\n\t\t// If there are matching queue subs and we are in gateway mode,\n\t\t// we need to keep track of the queue names the messages are\n\t\t// delivered to. When sending to the GWs, the RMSG will include\n\t\t// those names so that the remote clusters do not deliver messages\n\t\t// to their queue subs of the same names.\n\t\tif len(r.qsubs) > 0 && c.srv.gateway.enabled &&\n\t\t\tatomic.LoadInt64(&c.srv.gateway.totalQSubs) > 0 {\n\t\t\tflag |= pmrCollectQueueNames\n\t\t}\n\t\tdidDeliver, qnames = c.processMsgResults(acc, r, msg, c.pa.deliver, c.pa.subject, c.pa.reply, flag)\n\t}\n\n\t// Now deal with gateways\n\tif c.srv.gateway.enabled {\n\t\treply := c.pa.reply\n\t\tif len(c.pa.deliver) > 0 && c.kind == JETSTREAM && len(c.pa.reply) > 0 {\n\t\t\treply = append(reply, '@')\n\t\t\treply = append(reply, c.pa.deliver...)\n\t\t}\n\t\tdidDeliver = c.sendMsgToGateways(acc, msg, c.pa.subject, reply, qnames, false) || didDeliver\n\t}\n\n\t// Check to see if we did not deliver to anyone and the client has a reply subject set\n\t// and wants notification of no_responders.\n\tif !didDeliver && len(c.pa.reply) > 0 {\n\t\tc.mu.Lock()\n\t\tif c.opts.NoResponders {\n\t\t\tif sub := c.subForReply(c.pa.reply); sub != nil {\n\t\t\t\tproto := fmt.Sprintf(\"HMSG %s %s 16 16\\r\\nNATS/1.0 503\\r\\n\\r\\n\\r\\n\", c.pa.reply, sub.sid)\n\t\t\t\tc.queueOutbound([]byte(proto))\n\t\t\t\tc.addToPCD(c)\n\t\t\t}\n\t\t}\n\t\tc.mu.Unlock()\n\t}\n\n\treturn didDeliver, false\n}\n\n// Return the subscription for this reply subject. Only look at normal subs for this client.\nfunc (c *client) subForReply(reply []byte) *subscription {\n\tr := c.acc.sl.Match(string(reply))\n\tfor _, sub := range r.psubs {\n\t\tif sub.client == c {\n\t\t\treturn sub\n\t\t}\n\t}\n\treturn nil\n}\n\n// This is invoked knowing that c.pa.subject has been set to the gateway routed subject.\n// This function will send the message to possibly LEAFs and directly back to the origin\n// gateway.\nfunc (c *client) handleGWReplyMap(msg []byte) bool {\n\t// Check for leaf nodes\n\tif c.srv.gwLeafSubs.Count() > 0 {\n\t\tif r := c.srv.gwLeafSubs.MatchBytes(c.pa.subject); len(r.psubs) > 0 {\n\t\t\tc.processMsgResults(c.acc, r, msg, c.pa.deliver, c.pa.subject, c.pa.reply, pmrNoFlag)\n\t\t}\n\t}\n\tif c.srv.gateway.enabled {\n\t\treply := c.pa.reply\n\t\tif len(c.pa.deliver) > 0 && c.kind == JETSTREAM && len(c.pa.reply) > 0 {\n\t\t\treply = append(reply, '@')\n\t\t\treply = append(reply, c.pa.deliver...)\n\t\t}\n\t\tc.sendMsgToGateways(c.acc, msg, c.pa.subject, reply, nil, false)\n\t}\n\treturn true\n}\n\n// Used to setup the response map for a service import request that has a reply subject.\nfunc (c *client) setupResponseServiceImport(acc *Account, si *serviceImport, tracking bool, header http.Header) *serviceImport {\n\trsi := si.acc.addRespServiceImport(acc, string(c.pa.reply), si, tracking, header)\n\tif si.latency != nil {\n\t\tif c.rtt == 0 {\n\t\t\t// We have a service import that we are tracking but have not established RTT.\n\t\t\tc.sendRTTPing()\n\t\t}\n\t\tsi.acc.mu.Lock()\n\t\trsi.rc = c\n\t\tsi.acc.mu.Unlock()\n\t}\n\treturn rsi\n}\n\n// Will remove a header if present.\nfunc removeHeaderIfPresent(hdr []byte, key string) []byte {\n\tstart := bytes.Index(hdr, []byte(key))\n\t// key can't be first and we want to check that it is preceded by a '\\n'\n\tif start < 1 || hdr[start-1] != '\\n' {\n\t\treturn hdr\n\t}\n\tindex := start + len(key)\n\tif index >= len(hdr) || hdr[index] != ':' {\n\t\treturn hdr\n\t}\n\tend := bytes.Index(hdr[start:], []byte(_CRLF_))\n\tif end < 0 {\n\t\treturn hdr\n\t}\n\thdr = append(hdr[:start], hdr[start+end+len(_CRLF_):]...)\n\tif len(hdr) <= len(emptyHdrLine) {\n\t\treturn nil\n\t}\n\treturn hdr\n}\n\nfunc removeHeaderIfPrefixPresent(hdr []byte, prefix string) []byte {\n\tvar index int\n\tfor {\n\t\tif index >= len(hdr) {\n\t\t\treturn hdr\n\t\t}\n\n\t\tstart := bytes.Index(hdr[index:], []byte(prefix))\n\t\tif start < 0 {\n\t\t\treturn hdr\n\t\t}\n\t\tindex += start\n\t\tif index < 1 || hdr[index-1] != '\\n' {\n\t\t\treturn hdr\n\t\t}\n\n\t\tend := bytes.Index(hdr[index+len(prefix):], []byte(_CRLF_))\n\t\tif end < 0 {\n\t\t\treturn hdr\n\t\t}\n\n\t\thdr = append(hdr[:index], hdr[index+end+len(prefix)+len(_CRLF_):]...)\n\t\tif len(hdr) <= len(emptyHdrLine) {\n\t\t\treturn nil\n\t\t}\n\t}\n}\n\n// Generate a new header based on optional original header and key value.\n// More used in JetStream layers.\nfunc genHeader(hdr []byte, key, value string) []byte {\n\tvar bb bytes.Buffer\n\tif len(hdr) > LEN_CR_LF {\n\t\tbb.Write(hdr[:len(hdr)-LEN_CR_LF])\n\t} else {\n\t\tbb.WriteString(hdrLine)\n\t}\n\thttp.Header{key: []string{value}}.Write(&bb)\n\tbb.WriteString(CR_LF)\n\treturn bb.Bytes()\n}\n\n// This will set a header for the message.\n// Lock does not need to be held but this should only be called\n// from the inbound go routine. We will update the pubArgs.\n// This will replace any previously set header and not add to it per normal spec.\nfunc (c *client) setHeader(key, value string, msg []byte) []byte {\n\tvar bb bytes.Buffer\n\tvar omi int\n\t// Write original header if present.\n\tif c.pa.hdr > LEN_CR_LF {\n\t\tomi = c.pa.hdr\n\t\thdr := removeHeaderIfPresent(msg[:c.pa.hdr-LEN_CR_LF], key)\n\t\tif len(hdr) == 0 {\n\t\t\tbb.WriteString(hdrLine)\n\t\t} else {\n\t\t\tbb.Write(hdr)\n\t\t}\n\t} else {\n\t\tbb.WriteString(hdrLine)\n\t}\n\thttp.Header{key: []string{value}}.Write(&bb)\n\tbb.WriteString(CR_LF)\n\tnhdr := bb.Len()\n\t// Put the original message back.\n\t// FIXME(dlc) - This is inefficient.\n\tbb.Write(msg[omi:])\n\tnsize := bb.Len() - LEN_CR_LF\n\t// MQTT producers don't have CRLF, so add it back.\n\tif c.isMqtt() {\n\t\tnsize += LEN_CR_LF\n\t}\n\t// Update pubArgs\n\t// If others will use this later we need to save and restore original.\n\tc.pa.hdr = nhdr\n\tc.pa.size = nsize\n\tc.pa.hdb = []byte(strconv.Itoa(nhdr))\n\tc.pa.szb = []byte(strconv.Itoa(nsize))\n\treturn bb.Bytes()\n}\n\n// Will return a copy of the value for the header denoted by key or nil if it does not exist.\n// If you know that it is safe to refer to the underlying hdr slice for the period that the\n// return value is used, then sliceHeader() will be faster.\nfunc getHeader(key string, hdr []byte) []byte {\n\tv := sliceHeader(key, hdr)\n\tif v == nil {\n\t\treturn nil\n\t}\n\treturn append(make([]byte, 0, len(v)), v...)\n}\n\n// Will return the sliced value for the header denoted by key or nil if it does not exists.\n// This function ignores errors and tries to achieve speed and no additional allocations.\nfunc sliceHeader(key string, hdr []byte) []byte {\n\tif len(hdr) == 0 {\n\t\treturn nil\n\t}\n\tindex := bytes.Index(hdr, stringToBytes(key))\n\thdrLen := len(hdr)\n\t// Check that we have enough characters, this will handle the -1 case of the key not\n\t// being found and will also handle not having enough characters for trailing CRLF.\n\tif index < 2 {\n\t\treturn nil\n\t}\n\t// There should be a terminating CRLF.\n\tif index >= hdrLen-1 || hdr[index-1] != '\\n' || hdr[index-2] != '\\r' {\n\t\treturn nil\n\t}\n\t// The key should be immediately followed by a : separator.\n\tindex += len(key) + 1\n\tif index >= hdrLen || hdr[index-1] != ':' {\n\t\treturn nil\n\t}\n\t// Skip over whitespace before the value.\n\tfor index < hdrLen && hdr[index] == ' ' {\n\t\tindex++\n\t}\n\t// Collect together the rest of the value until we hit a CRLF.\n\tstart := index\n\tfor index < hdrLen {\n\t\tif hdr[index] == '\\r' && index < hdrLen-1 && hdr[index+1] == '\\n' {\n\t\t\tbreak\n\t\t}\n\t\tindex++\n\t}\n\treturn hdr[start:index:index]\n}\n\n// For bytes.HasPrefix below.\nvar (\n\tjsRequestNextPreB = []byte(jsRequestNextPre)\n\tjsDirectGetPreB   = []byte(jsDirectGetPre)\n)\n\n// processServiceImport is an internal callback when a subscription matches an imported service\n// from another account. This includes response mappings as well.\nfunc (c *client) processServiceImport(si *serviceImport, acc *Account, msg []byte) bool {\n\t// If we are a GW and this is not a direct serviceImport ignore.\n\tisResponse := si.isRespServiceImport()\n\tif (c.kind == GATEWAY || c.kind == ROUTER) && !isResponse {\n\t\treturn false\n\t}\n\t// Detect cycles and ignore (return) when we detect one.\n\tif len(c.pa.psi) > 0 {\n\t\tfor i := len(c.pa.psi) - 1; i >= 0; i-- {\n\t\t\tif psi := c.pa.psi[i]; psi.se == si.se {\n\t\t\t\treturn false\n\t\t\t}\n\t\t}\n\t}\n\n\tacc.mu.RLock()\n\tvar checkJS bool\n\tshouldReturn := si.invalid || acc.sl == nil\n\tif !shouldReturn && !isResponse && si.to == jsAllAPI {\n\t\tif bytes.HasPrefix(c.pa.subject, jsDirectGetPreB) || bytes.HasPrefix(c.pa.subject, jsRequestNextPreB) {\n\t\t\tcheckJS = true\n\t\t}\n\t}\n\tsiAcc := si.acc\n\tallowTrace := si.atrc\n\tacc.mu.RUnlock()\n\n\t// We have a special case where JetStream pulls in all service imports through one export.\n\t// However the GetNext for consumers and DirectGet for streams are a no-op and causes buildups of service imports,\n\t// response service imports and rrMap entries which all will need to simply expire.\n\t// TODO(dlc) - Come up with something better.\n\tif shouldReturn || (checkJS && si.se != nil && si.se.acc == c.srv.SystemAccount()) {\n\t\treturn false\n\t}\n\n\tmt, traceOnly := c.isMsgTraceEnabled()\n\n\tvar nrr []byte\n\tvar rsi *serviceImport\n\n\t// Check if there is a reply present and set up a response.\n\ttracking, headers := shouldSample(si.latency, c)\n\tif len(c.pa.reply) > 0 {\n\t\t// Special case for now, need to formalize.\n\t\t// TODO(dlc) - Formalize as a service import option for reply rewrite.\n\t\t// For now we can't do $JS.ACK since that breaks pull consumers across accounts.\n\t\tif !bytes.HasPrefix(c.pa.reply, []byte(jsAckPre)) {\n\t\t\tif rsi = c.setupResponseServiceImport(acc, si, tracking, headers); rsi != nil {\n\t\t\t\tnrr = []byte(rsi.from)\n\t\t\t}\n\t\t} else {\n\t\t\t// This only happens when we do a pull subscriber that trampolines through another account.\n\t\t\t// Normally this code is not called.\n\t\t\tnrr = c.pa.reply\n\t\t}\n\t} else if !isResponse && si.latency != nil && tracking {\n\t\t// Check to see if this was a bad request with no reply and we were supposed to be tracking.\n\t\tsiAcc.sendBadRequestTrackingLatency(si, c, headers)\n\t}\n\n\t// Send tracking info here if we are tracking this response.\n\t// This is always a response.\n\tvar didSendTL bool\n\tif si.tracking && !si.didDeliver {\n\t\t// Stamp that we attempted delivery.\n\t\tsi.didDeliver = true\n\t\tdidSendTL = acc.sendTrackingLatency(si, c)\n\t}\n\n\t// Pick correct \"to\" subject. If we matched on a wildcard use the literal publish subject.\n\tto, subject := si.to, string(c.pa.subject)\n\n\tif si.tr != nil {\n\t\t// FIXME(dlc) - This could be slow, may want to look at adding cache to bare transforms?\n\t\tto = si.tr.TransformSubject(subject)\n\t} else if si.usePub {\n\t\tto = subject\n\t}\n\n\t// Copy our pubArg since this gets modified as we process the service import itself.\n\tpacopy := c.pa\n\n\t// Now check to see if this account has mappings that could affect the service import.\n\t// Can't use non-locked trick like in processInboundClientMsg, so just call into selectMappedSubject\n\t// so we only lock once.\n\tnsubj, changed := siAcc.selectMappedSubject(to)\n\tif changed {\n\t\tc.pa.mapped = []byte(to)\n\t\tto = nsubj\n\t}\n\n\t// Set previous service import to detect chaining.\n\tlpsi := len(c.pa.psi)\n\thadPrevSi, share := lpsi > 0, si.share\n\tif hadPrevSi {\n\t\tshare = c.pa.psi[lpsi-1].share\n\t}\n\tc.pa.psi = append(c.pa.psi, si)\n\n\t// Place our client info for the request in the original message.\n\t// This will survive going across routes, etc.\n\tif !isResponse {\n\t\tisSysImport := siAcc == c.srv.SystemAccount()\n\t\tvar ci *ClientInfo\n\t\tif hadPrevSi && c.pa.hdr >= 0 {\n\t\t\tvar cis ClientInfo\n\t\t\tif err := json.Unmarshal(sliceHeader(ClientInfoHdr, msg[:c.pa.hdr]), &cis); err == nil {\n\t\t\t\tci = &cis\n\t\t\t\tci.Service = acc.Name\n\t\t\t\t// Check if we are moving into a share details account from a non-shared\n\t\t\t\t// and add in server and cluster details.\n\t\t\t\tif !share && (si.share || isSysImport) {\n\t\t\t\t\tc.addServerAndClusterInfo(ci)\n\t\t\t\t}\n\t\t\t}\n\t\t} else if c.kind != LEAF || c.pa.hdr < 0 || len(sliceHeader(ClientInfoHdr, msg[:c.pa.hdr])) == 0 {\n\t\t\tci = c.getClientInfo(share)\n\t\t\t// If we did not share but the imports destination is the system account add in the server and cluster info.\n\t\t\tif !share && isSysImport {\n\t\t\t\tc.addServerAndClusterInfo(ci)\n\t\t\t}\n\t\t} else if c.kind == LEAF && (si.share || isSysImport) {\n\t\t\t// We have a leaf header here for ci, augment as above.\n\t\t\tci = c.getClientInfo(si.share)\n\t\t\tif !si.share && isSysImport {\n\t\t\t\tc.addServerAndClusterInfo(ci)\n\t\t\t}\n\t\t}\n\t\t// Set clientInfo if present.\n\t\tif ci != nil {\n\t\t\tif b, _ := json.Marshal(ci); b != nil {\n\t\t\t\tmsg = c.setHeader(ClientInfoHdr, bytesToString(b), msg)\n\t\t\t}\n\t\t}\n\t}\n\n\t// Set our optional subject(to) and reply.\n\tif !isResponse && to != subject {\n\t\tc.pa.subject = []byte(to)\n\t}\n\tc.pa.reply = nrr\n\n\tif changed && c.isMqtt() && c.pa.hdr > 0 {\n\t\tc.srv.mqttStoreQoSMsgForAccountOnNewSubject(c.pa.hdr, msg, siAcc.GetName(), to)\n\t}\n\n\t// FIXME(dlc) - Do L1 cache trick like normal client?\n\trr := siAcc.sl.Match(to)\n\n\t// If we are a route or gateway or leafnode and this message is flipped to a queue subscriber we\n\t// need to handle that since the processMsgResults will want a queue filter.\n\tflags := pmrMsgImportedFromService\n\tif c.kind == GATEWAY || c.kind == ROUTER || c.kind == LEAF {\n\t\tflags |= pmrIgnoreEmptyQueueFilter\n\t}\n\n\t// We will be calling back into processMsgResults since we are now being called as a normal sub.\n\t// We need to take care of the c.in.rts, so save off what is there and use a local version. We\n\t// will put back what was there after.\n\n\torts := c.in.rts\n\n\tvar lrts [routeTargetInit]routeTarget\n\tc.in.rts = lrts[:0]\n\n\tvar skipProcessing bool\n\t// If message tracing enabled, add the service import trace.\n\tif mt != nil {\n\t\tmt.addServiceImportEvent(siAcc.GetName(), string(pacopy.subject), to)\n\t\t// If we are not allowing tracing and doing trace only, we stop at this level.\n\t\tif !allowTrace {\n\t\t\tif traceOnly {\n\t\t\t\tskipProcessing = true\n\t\t\t} else {\n\t\t\t\t// We are going to do normal processing, and possibly chainning\n\t\t\t\t// with other server imports, but the rest won't be traced.\n\t\t\t\t// We do so by setting the c.pa.trace to nil (it will be restored\n\t\t\t\t// with c.pa = pacopy).\n\t\t\t\tc.pa.trace = nil\n\t\t\t\t// We also need to disable the message trace headers so that\n\t\t\t\t// if the message is routed, it does not initialize tracing in the\n\t\t\t\t// remote.\n\t\t\t\tpositions := disableTraceHeaders(c, msg)\n\t\t\t\tdefer enableTraceHeaders(msg, positions)\n\t\t\t}\n\t\t}\n\t}\n\n\tvar didDeliver bool\n\n\tif !skipProcessing {\n\t\t// If this is not a gateway connection but gateway is enabled,\n\t\t// try to send this converted message to all gateways.\n\t\tif c.srv.gateway.enabled {\n\t\t\tflags |= pmrCollectQueueNames\n\t\t\tvar queues [][]byte\n\t\t\tdidDeliver, queues = c.processMsgResults(siAcc, rr, msg, c.pa.deliver, []byte(to), nrr, flags)\n\t\t\tdidDeliver = c.sendMsgToGateways(siAcc, msg, []byte(to), nrr, queues, false) || didDeliver\n\t\t} else {\n\t\t\tdidDeliver, _ = c.processMsgResults(siAcc, rr, msg, c.pa.deliver, []byte(to), nrr, flags)\n\t\t}\n\t}\n\n\t// Restore to original values.\n\tc.in.rts = orts\n\tc.pa = pacopy\n\n\t// Before we undo didDeliver based on tracing and last mile, mark in the c.pa which informs us of no responders status.\n\t// If we override due to tracing and traceOnly we do not want to send back a no responders.\n\tc.pa.delivered = didDeliver\n\n\t// If this was a message trace but we skip last-mile delivery, we need to\n\t// do the remove, so:\n\tif mt != nil && traceOnly && didDeliver {\n\t\tdidDeliver = false\n\t}\n\n\t// Determine if we should remove this service import. This is for response service imports.\n\t// We will remove if we did not deliver, or if we are a response service import and we are\n\t// a singleton, or we have an EOF message.\n\tshouldRemove := !didDeliver || (isResponse && (si.rt == Singleton || len(msg) == LEN_CR_LF))\n\t// If we are tracking and we did not actually send the latency info we need to suppress the removal.\n\tif si.tracking && !didSendTL {\n\t\tshouldRemove = false\n\t}\n\t// If we are streamed or chunked we need to update our timestamp to avoid cleanup.\n\tif si.rt != Singleton && didDeliver {\n\t\tacc.mu.Lock()\n\t\tsi.ts = time.Now().UnixNano()\n\t\tacc.mu.Unlock()\n\t}\n\n\t// Cleanup of a response service import\n\tif shouldRemove {\n\t\treason := rsiOk\n\t\tif !didDeliver {\n\t\t\treason = rsiNoDelivery\n\t\t}\n\t\tif isResponse {\n\t\t\tacc.removeRespServiceImport(si, reason)\n\t\t} else {\n\t\t\t// This is a main import and since we could not even deliver to the exporting account\n\t\t\t// go ahead and remove the respServiceImport we created above.\n\t\t\tsiAcc.removeRespServiceImport(rsi, reason)\n\t\t}\n\t}\n\n\treturn didDeliver\n}\n\nfunc (c *client) addSubToRouteTargets(sub *subscription) {\n\tif c.in.rts == nil {\n\t\tc.in.rts = make([]routeTarget, 0, routeTargetInit)\n\t}\n\n\tfor i := range c.in.rts {\n\t\trt := &c.in.rts[i]\n\t\tif rt.sub.client == sub.client {\n\t\t\tif sub.queue != nil {\n\t\t\t\trt.qs = append(rt.qs, sub.queue...)\n\t\t\t\trt.qs = append(rt.qs, ' ')\n\t\t\t}\n\t\t\treturn\n\t\t}\n\t}\n\n\tvar rt *routeTarget\n\tlrts := len(c.in.rts)\n\n\t// If we are here we do not have the sub yet in our list\n\t// If we have to grow do so here.\n\tif lrts == cap(c.in.rts) {\n\t\tc.in.rts = append(c.in.rts, routeTarget{})\n\t}\n\n\tc.in.rts = c.in.rts[:lrts+1]\n\trt = &c.in.rts[lrts]\n\trt.sub = sub\n\trt.qs = rt._qs[:0]\n\tif sub.queue != nil {\n\t\trt.qs = append(rt.qs, sub.queue...)\n\t\trt.qs = append(rt.qs, ' ')\n\t}\n}\n\n// This processes the sublist results for a given message.\n// Returns if the message was delivered to at least target and queue filters.\nfunc (c *client) processMsgResults(acc *Account, r *SublistResult, msg, deliver, subject, reply []byte, flags int) (bool, [][]byte) {\n\t// For sending messages across routes and leafnodes.\n\t// Reset if we have one since we reuse this data structure.\n\tif c.in.rts != nil {\n\t\tc.in.rts = c.in.rts[:0]\n\t}\n\n\tvar rplyHasGWPrefix bool\n\tvar creply = reply\n\n\t// If the reply subject is a GW routed reply, we will perform some\n\t// tracking in deliverMsg(). We also want to send to the user the\n\t// reply without the prefix. `creply` will be set to that and be\n\t// used to create the message header for client connections.\n\tif rplyHasGWPrefix = isGWRoutedReply(reply); rplyHasGWPrefix {\n\t\tcreply = reply[gwSubjectOffset:]\n\t}\n\n\t// With JetStream we now have times where we want to match a subscription\n\t// on one subject, but deliver it with another. e.g. JetStream deliverables.\n\t// This only works for last mile, meaning to a client. For other types we need\n\t// to use the original subject.\n\tsubj := subject\n\tif len(deliver) > 0 {\n\t\tsubj = deliver\n\t}\n\n\t// Check for JetStream encoded reply subjects.\n\t// For now these will only be on $JS.ACK prefixed reply subjects.\n\tvar remapped bool\n\tif len(creply) > 0 && c.kind != CLIENT && !isInternalClient(c.kind) && bytes.HasPrefix(creply, []byte(jsAckPre)) {\n\t\t// We need to rewrite the subject and the reply.\n\t\t// But, we must be careful that the stream name, consumer name, and subject can contain '@' characters.\n\t\t// JS ACK contains at least 8 dots, find the first @ after this prefix.\n\t\t// - $JS.ACK.<stream>.<consumer>.<delivered>.<sseq>.<cseq>.<tm>.<pending>\n\t\tcounter := 0\n\t\tli := bytes.IndexFunc(creply, func(rn rune) bool {\n\t\t\tif rn == '.' {\n\t\t\t\tcounter++\n\t\t\t} else if rn == '@' {\n\t\t\t\treturn counter >= 8\n\t\t\t}\n\t\t\treturn false\n\t\t})\n\t\tif li != -1 && li < len(creply)-1 {\n\t\t\tremapped = true\n\t\t\tsubj, creply = creply[li+1:], creply[:li]\n\t\t}\n\t}\n\n\tvar didDeliver bool\n\n\t// delivery subject for clients\n\tvar dsubj []byte\n\t// Used as scratch if mapping\n\tvar _dsubj [128]byte\n\n\t// For stats, we will keep track of the number of messages that have been\n\t// delivered and then multiply by the size of that message and update\n\t// server and account stats in a \"single\" operation (instead of per-sub).\n\t// However, we account for situations where the message is possibly changed\n\t// by having an extra size\n\tvar dlvMsgs int64\n\tvar dlvExtraSize int64\n\tvar dlvRouteMsgs int64\n\tvar dlvLeafMsgs int64\n\n\t// We need to know if this is a MQTT producer because they send messages\n\t// without CR_LF (we otherwise remove the size of CR_LF from message size).\n\tprodIsMQTT := c.isMqtt()\n\n\tupdateStats := func() {\n\t\tif dlvMsgs == 0 {\n\t\t\treturn\n\t\t}\n\n\t\ttotalBytes := dlvMsgs*int64(len(msg)) + dlvExtraSize\n\t\trouteBytes := dlvRouteMsgs*int64(len(msg)) + dlvExtraSize\n\t\tleafBytes := dlvLeafMsgs*int64(len(msg)) + dlvExtraSize\n\n\t\t// For non MQTT producers, remove the CR_LF * number of messages\n\t\tif !prodIsMQTT {\n\t\t\ttotalBytes -= dlvMsgs * int64(LEN_CR_LF)\n\t\t\trouteBytes -= dlvRouteMsgs * int64(LEN_CR_LF)\n\t\t\tleafBytes -= dlvLeafMsgs * int64(LEN_CR_LF)\n\t\t}\n\n\t\tif acc != nil {\n\t\t\tacc.stats.Lock()\n\t\t\tacc.stats.outMsgs += dlvMsgs\n\t\t\tacc.stats.outBytes += totalBytes\n\t\t\tif dlvRouteMsgs > 0 {\n\t\t\t\tacc.stats.rt.outMsgs += dlvRouteMsgs\n\t\t\t\tacc.stats.rt.outBytes += routeBytes\n\t\t\t}\n\t\t\tif dlvLeafMsgs > 0 {\n\t\t\t\tacc.stats.ln.outMsgs += dlvLeafMsgs\n\t\t\t\tacc.stats.ln.outBytes += leafBytes\n\t\t\t}\n\t\t\tacc.stats.Unlock()\n\t\t}\n\n\t\tif srv := c.srv; srv != nil {\n\t\t\tatomic.AddInt64(&srv.outMsgs, dlvMsgs)\n\t\t\tatomic.AddInt64(&srv.outBytes, totalBytes)\n\t\t}\n\t}\n\n\tmt, traceOnly := c.isMsgTraceEnabled()\n\n\t// Loop over all normal subscriptions that match.\n\tfor _, sub := range r.psubs {\n\t\t// Check if this is a send to a ROUTER. We now process\n\t\t// these after everything else.\n\t\tswitch sub.client.kind {\n\t\tcase ROUTER:\n\t\t\tif (c.kind != ROUTER && !c.isSpokeLeafNode()) || (flags&pmrAllowSendFromRouteToRoute != 0) {\n\t\t\t\tc.addSubToRouteTargets(sub)\n\t\t\t}\n\t\t\tcontinue\n\t\tcase GATEWAY:\n\t\t\t// Never send to gateway from here.\n\t\t\tcontinue\n\t\tcase LEAF:\n\t\t\t// We handle similarly to routes and use the same data structures.\n\t\t\t// Leaf node delivery audience is different however.\n\t\t\t// Also leaf nodes are always no echo, so we make sure we are not\n\t\t\t// going to send back to ourselves here. For messages from routes we want\n\t\t\t// to suppress in general unless we know from the hub or its a service reply.\n\t\t\tif c != sub.client && (c.kind != ROUTER || sub.client.isHubLeafNode() || isServiceReply(c.pa.subject)) {\n\t\t\t\tc.addSubToRouteTargets(sub)\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\n\t\t// Assume delivery subject is the normal subject to this point.\n\t\tdsubj = subj\n\n\t\t// We may need to disable tracing, by setting c.pa.trace to `nil`\n\t\t// before the call to deliverMsg, if so, this will indicate that\n\t\t// we need to put it back.\n\t\tvar restorePaTrace bool\n\n\t\t// Check for stream import mapped subs (shadow subs). These apply to local subs only.\n\t\tif sub.im != nil {\n\t\t\t// If this message was a service import do not re-export to an exported stream.\n\t\t\tif flags&pmrMsgImportedFromService != 0 {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif sub.im.tr != nil {\n\t\t\t\tto := sub.im.tr.TransformSubject(bytesToString(subject))\n\t\t\t\tdsubj = append(_dsubj[:0], to...)\n\t\t\t} else if sub.im.usePub {\n\t\t\t\tdsubj = append(_dsubj[:0], subj...)\n\t\t\t} else {\n\t\t\t\tdsubj = append(_dsubj[:0], sub.im.to...)\n\t\t\t}\n\n\t\t\tif mt != nil {\n\t\t\t\tmt.addStreamExportEvent(sub.client, dsubj)\n\t\t\t\t// If allow_trace is false...\n\t\t\t\tif !sub.im.atrc {\n\t\t\t\t\t// If we are doing only message tracing, we can move to the\n\t\t\t\t\t//  next sub.\n\t\t\t\t\tif traceOnly {\n\t\t\t\t\t\t// Although the message was not delivered, for the purpose\n\t\t\t\t\t\t// of didDeliver, we need to set to true (to avoid possible\n\t\t\t\t\t\t// no responders).\n\t\t\t\t\t\tdidDeliver = true\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\t\t\t\t\t// If we are delivering the message, we need to disable tracing\n\t\t\t\t\t// before calling deliverMsg().\n\t\t\t\t\tc.pa.trace, restorePaTrace = nil, true\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// Make sure deliver is set if inbound from a route.\n\t\t\tif remapped && (c.kind == GATEWAY || c.kind == ROUTER || c.kind == LEAF) {\n\t\t\t\tdeliver = subj\n\t\t\t}\n\t\t\t// If we are mapping for a deliver subject we will reverse roles.\n\t\t\t// The original subj we set from above is correct for the msg header,\n\t\t\t// but we need to transform the deliver subject to properly route.\n\t\t\tif len(deliver) > 0 {\n\t\t\t\tdsubj, subj = subj, dsubj\n\t\t\t}\n\t\t}\n\n\t\t// Remap to the original subject if internal.\n\t\tif sub.icb != nil && sub.rsi {\n\t\t\tdsubj = subject\n\t\t}\n\n\t\t// Normal delivery\n\t\tmh := c.msgHeader(dsubj, creply, sub)\n\t\tif c.deliverMsg(prodIsMQTT, sub, acc, dsubj, creply, mh, msg, rplyHasGWPrefix) {\n\t\t\t// We don't count internal deliveries, so do only when sub.icb is nil.\n\t\t\tif sub.icb == nil {\n\t\t\t\tdlvMsgs++\n\t\t\t}\n\t\t\tdidDeliver = true\n\t\t}\n\t\tif restorePaTrace {\n\t\t\tc.pa.trace = mt\n\t\t}\n\t}\n\n\t// Set these up to optionally filter based on the queue lists.\n\t// This is for messages received from routes which will have directed\n\t// guidance on which queue groups we should deliver to.\n\tqf := c.pa.queues\n\n\t// Declared here because of goto.\n\tvar queues [][]byte\n\n\tvar leafOrigin string\n\tswitch c.kind {\n\tcase ROUTER:\n\t\tif len(c.pa.origin) > 0 {\n\t\t\t// Picture a message sent from a leafnode to a server that then routes\n\t\t\t// this message: CluserA -leaf-> HUB1 -route-> HUB2\n\t\t\t// Here we are in HUB2, so c.kind is a ROUTER, but the message will\n\t\t\t// contain a c.pa.origin set to \"ClusterA\" to indicate that this message\n\t\t\t// originated from that leafnode cluster.\n\t\t\tleafOrigin = bytesToString(c.pa.origin)\n\t\t}\n\tcase LEAF:\n\t\tleafOrigin = c.remoteCluster()\n\t}\n\n\t// For all routes/leaf/gateway connections, we may still want to send messages to\n\t// leaf nodes or routes even if there are no queue filters since we collect\n\t// them above and do not process inline like normal clients.\n\t// However, do select queue subs if asked to ignore empty queue filter.\n\tif (c.kind == LEAF || c.kind == ROUTER || c.kind == GATEWAY) && len(qf) == 0 && flags&pmrIgnoreEmptyQueueFilter == 0 {\n\t\tgoto sendToRoutesOrLeafs\n\t}\n\n\t// Process queue subs\n\tfor i := 0; i < len(r.qsubs); i++ {\n\t\tqsubs := r.qsubs[i]\n\t\t// If we have a filter check that here. We could make this a map or someting more\n\t\t// complex but linear search since we expect queues to be small. Should be faster\n\t\t// and more cache friendly.\n\t\tif qf != nil && len(qsubs) > 0 {\n\t\t\ttqn := qsubs[0].queue\n\t\t\tfor _, qn := range qf {\n\t\t\t\tif bytes.Equal(qn, tqn) {\n\t\t\t\t\tgoto selectQSub\n\t\t\t\t}\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\n\tselectQSub:\n\t\t// We will hold onto remote or lead qsubs when we are coming from\n\t\t// a route or a leaf node just in case we can no longer do local delivery.\n\t\tvar rsub, sub *subscription\n\t\tvar _ql [32]*subscription\n\n\t\tsrc := c.kind\n\t\t// If we just came from a route we want to prefer local subs.\n\t\t// So only select from local subs but remember the first rsub\n\t\t// in case all else fails.\n\t\tif src == ROUTER {\n\t\t\tql := _ql[:0]\n\t\t\tfor i := 0; i < len(qsubs); i++ {\n\t\t\t\tsub = qsubs[i]\n\t\t\t\tif dst := sub.client.kind; dst == LEAF || dst == ROUTER {\n\t\t\t\t\t// If the destination is a LEAF, we first need to make sure\n\t\t\t\t\t// that we would not pick one that was the origin of this\n\t\t\t\t\t// message.\n\t\t\t\t\tif dst == LEAF && leafOrigin != _EMPTY_ && leafOrigin == sub.client.remoteCluster() {\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\t\t\t\t\t// If we have assigned a ROUTER rsub already, replace if\n\t\t\t\t\t// the destination is a LEAF since we want to favor that.\n\t\t\t\t\tif rsub == nil || (rsub.client.kind == ROUTER && dst == LEAF) {\n\t\t\t\t\t\trsub = sub\n\t\t\t\t\t} else if dst == LEAF {\n\t\t\t\t\t\t// We already have a LEAF and this is another one.\n\t\t\t\t\t\t// Flip a coin to see if we swap it or not.\n\t\t\t\t\t\t// See https://github.com/nats-io/nats-server/issues/6040\n\t\t\t\t\t\tif fastrand.Uint32()%2 == 1 {\n\t\t\t\t\t\t\trsub = sub\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\tql = append(ql, sub)\n\t\t\t\t}\n\t\t\t}\n\t\t\tqsubs = ql\n\t\t}\n\n\t\tsindex := 0\n\t\tlqs := len(qsubs)\n\t\tif lqs > 1 {\n\t\t\tsindex = int(fastrand.Uint32() % uint32(lqs))\n\t\t}\n\n\t\t// Find a subscription that is able to deliver this message starting at a random index.\n\t\t// Note that if the message came from a ROUTER, we will only have CLIENT or LEAF\n\t\t// queue subs here, otherwise we can have all types.\n\t\tfor i := 0; i < lqs; i++ {\n\t\t\tif sindex+i < lqs {\n\t\t\t\tsub = qsubs[sindex+i]\n\t\t\t} else {\n\t\t\t\tsub = qsubs[(sindex+i)%lqs]\n\t\t\t}\n\t\t\tif sub == nil {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// If we are a spoke leaf node make sure to not forward across routes.\n\t\t\t// This mimics same behavior for normal subs above.\n\t\t\tif c.kind == LEAF && c.isSpokeLeafNode() && sub.client.kind == ROUTER {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// We have taken care of preferring local subs for a message from a route above.\n\t\t\t// Here we just care about a client or leaf and skipping a leaf and preferring locals.\n\t\t\tif dst := sub.client.kind; dst == ROUTER || dst == LEAF {\n\t\t\t\tif (src == LEAF || src == CLIENT) && dst == LEAF {\n\t\t\t\t\t// If we come from a LEAF and are about to pick a LEAF connection,\n\t\t\t\t\t// make sure this is not the same leaf cluster.\n\t\t\t\t\tif src == LEAF && leafOrigin != _EMPTY_ && leafOrigin == sub.client.remoteCluster() {\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\t\t\t\t\t// Remember that leaf in case we don't find any other candidate.\n\t\t\t\t\t// We already start randomly in lqs slice, so we don't need\n\t\t\t\t\t// to do a random swap if we already have an rsub like we do\n\t\t\t\t\t// when src == ROUTER above.\n\t\t\t\t\tif rsub == nil {\n\t\t\t\t\t\trsub = sub\n\t\t\t\t\t}\n\t\t\t\t\tcontinue\n\t\t\t\t} else {\n\t\t\t\t\t// We want to favor qsubs in our own cluster. If the routed\n\t\t\t\t\t// qsub has an origin, it means that is on behalf of a leaf.\n\t\t\t\t\t// We need to treat it differently.\n\t\t\t\t\tif len(sub.origin) > 0 {\n\t\t\t\t\t\t// If we already have an rsub, nothing to do. Also, do\n\t\t\t\t\t\t// not pick a routed qsub for a LEAF origin cluster\n\t\t\t\t\t\t// that is the same than where the message comes from.\n\t\t\t\t\t\tif rsub == nil && (leafOrigin == _EMPTY_ || leafOrigin != bytesToString(sub.origin)) {\n\t\t\t\t\t\t\trsub = sub\n\t\t\t\t\t\t}\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\t\t\t\t\t// This is a qsub that is local on the remote server (or\n\t\t\t\t\t// we are connected to an older server and we don't know).\n\t\t\t\t\t// Pick this one and be done.\n\t\t\t\t\trsub = sub\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// Assume delivery subject is normal subject to this point.\n\t\t\tdsubj = subj\n\n\t\t\t// We may need to disable tracing, by setting c.pa.trace to `nil`\n\t\t\t// before the call to deliverMsg, if so, this will indicate that\n\t\t\t// we need to put it back.\n\t\t\tvar restorePaTrace bool\n\t\t\tvar skipDelivery bool\n\n\t\t\t// Check for stream import mapped subs. These apply to local subs only.\n\t\t\tif sub.im != nil {\n\t\t\t\t// If this message was a service import do not re-export to an exported stream.\n\t\t\t\tif flags&pmrMsgImportedFromService != 0 {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tif sub.im.tr != nil {\n\t\t\t\t\tto := sub.im.tr.TransformSubject(bytesToString(subject))\n\t\t\t\t\tdsubj = append(_dsubj[:0], to...)\n\t\t\t\t} else if sub.im.usePub {\n\t\t\t\t\tdsubj = append(_dsubj[:0], subj...)\n\t\t\t\t} else {\n\t\t\t\t\tdsubj = append(_dsubj[:0], sub.im.to...)\n\t\t\t\t}\n\n\t\t\t\tif mt != nil {\n\t\t\t\t\tmt.addStreamExportEvent(sub.client, dsubj)\n\t\t\t\t\t// If allow_trace is false...\n\t\t\t\t\tif !sub.im.atrc {\n\t\t\t\t\t\t// If we are doing only message tracing, we are done\n\t\t\t\t\t\t// with this queue group.\n\t\t\t\t\t\tif traceOnly {\n\t\t\t\t\t\t\tskipDelivery = true\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\t// If we are delivering, we need to disable tracing\n\t\t\t\t\t\t\t// before the call to deliverMsg()\n\t\t\t\t\t\t\tc.pa.trace, restorePaTrace = nil, true\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\t// Make sure deliver is set if inbound from a route.\n\t\t\t\tif remapped && (c.kind == GATEWAY || c.kind == ROUTER || c.kind == LEAF) {\n\t\t\t\t\tdeliver = subj\n\t\t\t\t}\n\t\t\t\t// If we are mapping for a deliver subject we will reverse roles.\n\t\t\t\t// The original subj we set from above is correct for the msg header,\n\t\t\t\t// but we need to transform the deliver subject to properly route.\n\t\t\t\tif len(deliver) > 0 {\n\t\t\t\t\tdsubj, subj = subj, dsubj\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tvar delivered bool\n\t\t\tif !skipDelivery {\n\t\t\t\tmh := c.msgHeader(dsubj, creply, sub)\n\t\t\t\tdelivered = c.deliverMsg(prodIsMQTT, sub, acc, subject, creply, mh, msg, rplyHasGWPrefix)\n\t\t\t\tif restorePaTrace {\n\t\t\t\t\tc.pa.trace = mt\n\t\t\t\t}\n\t\t\t}\n\t\t\tif skipDelivery || delivered {\n\t\t\t\t// Update only if not skipped.\n\t\t\t\tif !skipDelivery && sub.icb == nil {\n\t\t\t\t\tdlvMsgs++\n\t\t\t\t\tswitch sub.client.kind {\n\t\t\t\t\tcase ROUTER:\n\t\t\t\t\t\tdlvRouteMsgs++\n\t\t\t\t\tcase LEAF:\n\t\t\t\t\t\tdlvLeafMsgs++\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\t// Do the rest even when message delivery was skipped.\n\t\t\t\tdidDeliver = true\n\t\t\t\t// Clear rsub\n\t\t\t\trsub = nil\n\t\t\t\tif flags&pmrCollectQueueNames != 0 {\n\t\t\t\t\tqueues = append(queues, sub.queue)\n\t\t\t\t}\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\n\t\tif rsub != nil {\n\t\t\t// We are here if we have selected a leaf or route as the destination,\n\t\t\t// or if we tried to deliver to a local qsub but failed.\n\t\t\tc.addSubToRouteTargets(rsub)\n\t\t\tif flags&pmrCollectQueueNames != 0 {\n\t\t\t\tqueues = append(queues, rsub.queue)\n\t\t\t}\n\t\t}\n\t}\n\nsendToRoutesOrLeafs:\n\n\t// If no messages for routes or leafnodes return here.\n\tif len(c.in.rts) == 0 {\n\t\tupdateStats()\n\t\treturn didDeliver, queues\n\t}\n\n\t// If we do have a deliver subject we need to do something with it.\n\t// Again this is when JetStream (but possibly others) wants the system\n\t// to rewrite the delivered subject. The way we will do that is place it\n\t// at the end of the reply subject if it exists.\n\tif len(deliver) > 0 && len(reply) > 0 {\n\t\treply = append(reply, '@')\n\t\treply = append(reply, deliver...)\n\t}\n\n\t// Copy off original pa in case it changes.\n\tpa := c.pa\n\n\tif mt != nil {\n\t\t// We are going to replace \"pa\" with our copy of c.pa, but to restore\n\t\t// to the original copy of c.pa, we need to save it again.\n\t\tcpa := pa\n\t\tmsg = mt.setOriginAccountHeaderIfNeeded(c, acc, msg)\n\t\tdefer func() { c.pa = cpa }()\n\t\t// Update pa with our current c.pa state.\n\t\tpa = c.pa\n\t}\n\n\t// We address by index to avoid struct copy.\n\t// We have inline structs for memory layout and cache coherency.\n\tfor i := range c.in.rts {\n\t\trt := &c.in.rts[i]\n\t\tdc := rt.sub.client\n\t\tdmsg, hset := msg, false\n\n\t\t// Check if we have an origin cluster set from a leafnode message.\n\t\t// If so make sure we do not send it back to the same cluster for a different\n\t\t// leafnode. Cluster wide no echo.\n\t\tif dc.kind == LEAF {\n\t\t\t// Check two scenarios. One is inbound from a route (c.pa.origin),\n\t\t\t// and the other is leaf to leaf. In both case, leafOrigin is the one\n\t\t\t// to use for the comparison.\n\t\t\tif leafOrigin != _EMPTY_ && leafOrigin == dc.remoteCluster() {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// We need to check if this is a request that has a stamped client information header.\n\t\t\t// This will contain an account but will represent the account from the leafnode. If\n\t\t\t// they are not named the same this would cause an account lookup failure trying to\n\t\t\t// process the request for something like JetStream or other system services that rely\n\t\t\t// on the client info header. We can just check for reply and the presence of a header\n\t\t\t// to avoid slow downs for all traffic.\n\t\t\tif len(c.pa.reply) > 0 && c.pa.hdr >= 0 {\n\t\t\t\tdmsg, hset = c.checkLeafClientInfoHeader(msg)\n\t\t\t}\n\t\t}\n\n\t\tif mt != nil {\n\t\t\tdmsg = mt.setHopHeader(c, dmsg)\n\t\t\thset = true\n\t\t}\n\n\t\tmh := c.msgHeaderForRouteOrLeaf(subject, reply, rt, acc)\n\t\tif c.deliverMsg(prodIsMQTT, rt.sub, acc, subject, reply, mh, dmsg, false) {\n\t\t\tif rt.sub.icb == nil {\n\t\t\t\tdlvMsgs++\n\t\t\t\tswitch dc.kind {\n\t\t\t\tcase ROUTER:\n\t\t\t\t\tdlvRouteMsgs++\n\t\t\t\tcase LEAF:\n\t\t\t\t\tdlvLeafMsgs++\n\t\t\t\t}\n\t\t\t\tdlvExtraSize += int64(len(dmsg) - len(msg))\n\t\t\t}\n\t\t\tdidDeliver = true\n\t\t}\n\n\t\t// If we set the header reset the origin pub args.\n\t\tif hset {\n\t\t\tc.pa = pa\n\t\t}\n\t}\n\tupdateStats()\n\treturn didDeliver, queues\n}\n\n// Check and swap accounts on a client info header destined across a leafnode.\nfunc (c *client) checkLeafClientInfoHeader(msg []byte) (dmsg []byte, setHdr bool) {\n\tif c.pa.hdr < 0 || len(msg) < c.pa.hdr {\n\t\treturn msg, false\n\t}\n\tcir := sliceHeader(ClientInfoHdr, msg[:c.pa.hdr])\n\tif len(cir) == 0 {\n\t\treturn msg, false\n\t}\n\n\tdmsg = msg\n\n\tvar ci ClientInfo\n\tif err := json.Unmarshal(cir, &ci); err == nil {\n\t\tif v, _ := c.srv.leafRemoteAccounts.Load(ci.Account); v != nil {\n\t\t\tremoteAcc := v.(string)\n\t\t\tif ci.Account != remoteAcc {\n\t\t\t\tci.Account = remoteAcc\n\t\t\t\tif b, _ := json.Marshal(ci); b != nil {\n\t\t\t\t\tdmsg, setHdr = c.setHeader(ClientInfoHdr, bytesToString(b), msg), true\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn dmsg, setHdr\n}\n\nfunc (c *client) pubPermissionViolation(subject []byte) {\n\terrTxt := fmt.Sprintf(\"Permissions Violation for Publish to %q\", subject)\n\tif mt, _ := c.isMsgTraceEnabled(); mt != nil {\n\t\tmt.setIngressError(errTxt)\n\t}\n\tc.sendErr(errTxt)\n\tc.Errorf(\"Publish Violation - %s, Subject %q\", c.getAuthUser(), subject)\n}\n\nfunc (c *client) subPermissionViolation(sub *subscription) {\n\terrTxt := fmt.Sprintf(\"Permissions Violation for Subscription to %q\", sub.subject)\n\tlogTxt := fmt.Sprintf(\"Subscription Violation - %s, Subject %q, SID %s\",\n\t\tc.getAuthUser(), sub.subject, sub.sid)\n\n\tif sub.queue != nil {\n\t\terrTxt = fmt.Sprintf(\"Permissions Violation for Subscription to %q using queue %q\", sub.subject, sub.queue)\n\t\tlogTxt = fmt.Sprintf(\"Subscription Violation - %s, Subject %q, Queue: %q, SID %s\",\n\t\t\tc.getAuthUser(), sub.subject, sub.queue, sub.sid)\n\t}\n\n\tc.sendErr(errTxt)\n\tc.Errorf(logTxt)\n}\n\nfunc (c *client) replySubjectViolation(reply []byte) {\n\terrTxt := fmt.Sprintf(\"Permissions Violation for Publish with Reply of %q\", reply)\n\tif mt, _ := c.isMsgTraceEnabled(); mt != nil {\n\t\tmt.setIngressError(errTxt)\n\t}\n\tc.sendErr(errTxt)\n\tc.Errorf(\"Publish Violation - %s, Reply %q\", c.getAuthUser(), reply)\n}\n\nfunc (c *client) maxTokensViolation(sub *subscription) {\n\terrTxt := fmt.Sprintf(\"Permissions Violation for Subscription to %q, too many tokens\", sub.subject)\n\tlogTxt := fmt.Sprintf(\"Subscription Violation Too Many Tokens - %s, Subject %q, SID %s\",\n\t\tc.getAuthUser(), sub.subject, sub.sid)\n\tc.sendErr(errTxt)\n\tc.Errorf(logTxt)\n}\n\nfunc (c *client) processPingTimer() {\n\tc.mu.Lock()\n\tc.ping.tmr = nil\n\t// Check if connection is still opened\n\tif c.isClosed() {\n\t\tc.mu.Unlock()\n\t\treturn\n\t}\n\n\tc.Debugf(\"%s Ping Timer\", c.kindString())\n\n\tvar sendPing bool\n\n\topts := c.srv.getOpts()\n\tpingInterval := opts.PingInterval\n\tif c.kind == ROUTER && opts.Cluster.PingInterval > 0 {\n\t\tpingInterval = opts.Cluster.PingInterval\n\t}\n\tpingInterval = adjustPingInterval(c.kind, pingInterval)\n\tnow := time.Now()\n\tneedRTT := c.rtt == 0 || now.Sub(c.rttStart) > DEFAULT_RTT_MEASUREMENT_INTERVAL\n\n\t// Do not delay PINGs for ROUTER, GATEWAY or spoke LEAF connections.\n\tif c.kind == ROUTER || c.kind == GATEWAY || c.isSpokeLeafNode() {\n\t\tsendPing = true\n\t} else {\n\t\t// If we received client data or a ping from the other side within the PingInterval,\n\t\t// then there is no need to send a ping.\n\t\tif delta := now.Sub(c.lastIn); delta < pingInterval && !needRTT {\n\t\t\tc.Debugf(\"Delaying PING due to remote client data or ping %v ago\", delta.Round(time.Second))\n\t\t} else {\n\t\t\tsendPing = true\n\t\t}\n\t}\n\n\tif sendPing {\n\t\t// Check for violation\n\t\tmaxPingsOut := opts.MaxPingsOut\n\t\tif c.kind == ROUTER && opts.Cluster.MaxPingsOut > 0 {\n\t\t\tmaxPingsOut = opts.Cluster.MaxPingsOut\n\t\t}\n\t\tif c.ping.out+1 > maxPingsOut {\n\t\t\tc.Debugf(\"Stale Client Connection - Closing\")\n\t\t\tc.enqueueProto([]byte(fmt.Sprintf(errProto, \"Stale Connection\")))\n\t\t\tc.mu.Unlock()\n\t\t\tc.closeConnection(StaleConnection)\n\t\t\treturn\n\t\t}\n\t\t// Send PING\n\t\tc.sendPing()\n\t}\n\n\t// Reset to fire again.\n\tc.setPingTimer()\n\tc.mu.Unlock()\n}\n\n// Returns the smallest value between the given `d` and some max value\n// based on the connection kind.\nfunc adjustPingInterval(kind int, d time.Duration) time.Duration {\n\tswitch kind {\n\tcase ROUTER:\n\t\tif d > routeMaxPingInterval {\n\t\t\treturn routeMaxPingInterval\n\t\t}\n\tcase GATEWAY:\n\t\tif d > gatewayMaxPingInterval {\n\t\t\treturn gatewayMaxPingInterval\n\t\t}\n\t}\n\treturn d\n}\n\n// This is used when a connection cannot yet start to send PINGs because\n// the remote would not be able to handle them (case of compression,\n// or outbound gateway, etc...), but we still want to close the connection\n// if the timer has not been reset by the time we reach the time equivalent\n// to have sent the max number of pings.\n//\n// Lock should be held\nfunc (c *client) watchForStaleConnection(pingInterval time.Duration, pingMax int) {\n\tc.ping.tmr = time.AfterFunc(pingInterval*time.Duration(pingMax+1), func() {\n\t\tc.mu.Lock()\n\t\tc.Debugf(\"Stale Client Connection - Closing\")\n\t\tc.enqueueProto([]byte(fmt.Sprintf(errProto, \"Stale Connection\")))\n\t\tc.mu.Unlock()\n\t\tc.closeConnection(StaleConnection)\n\t})\n}\n\n// Lock should be held\nfunc (c *client) setPingTimer() {\n\tif c.srv == nil {\n\t\treturn\n\t}\n\topts := c.srv.getOpts()\n\td := opts.PingInterval\n\tif c.kind == ROUTER && opts.Cluster.PingInterval > 0 {\n\t\td = opts.Cluster.PingInterval\n\t}\n\td = adjustPingInterval(c.kind, d)\n\tc.ping.tmr = time.AfterFunc(d, c.processPingTimer)\n}\n\n// Lock should be held\nfunc (c *client) clearPingTimer() {\n\tif c.ping.tmr == nil {\n\t\treturn\n\t}\n\tc.ping.tmr.Stop()\n\tc.ping.tmr = nil\n}\n\nfunc (c *client) clearTlsToTimer() {\n\tif c.tlsTo == nil {\n\t\treturn\n\t}\n\tc.tlsTo.Stop()\n\tc.tlsTo = nil\n}\n\n// Lock should be held\nfunc (c *client) setAuthTimer(d time.Duration) {\n\tc.atmr = time.AfterFunc(d, c.authTimeout)\n}\n\n// Lock should be held\nfunc (c *client) clearAuthTimer() bool {\n\tif c.atmr == nil {\n\t\treturn true\n\t}\n\tstopped := c.atmr.Stop()\n\tc.atmr = nil\n\treturn stopped\n}\n\n// We may reuse atmr for expiring user jwts,\n// so check connectReceived.\n// Lock assume held on entry.\nfunc (c *client) awaitingAuth() bool {\n\treturn !c.flags.isSet(connectReceived) && c.atmr != nil\n}\n\n// This will set the atmr for the JWT expiration time.\n// We will lock on entry.\nfunc (c *client) setExpirationTimer(d time.Duration) {\n\tc.mu.Lock()\n\tc.setExpirationTimerUnlocked(d)\n\tc.mu.Unlock()\n}\n\n// This will set the atmr for the JWT expiration time. client lock should be held before call\nfunc (c *client) setExpirationTimerUnlocked(d time.Duration) {\n\tc.atmr = time.AfterFunc(d, c.authExpired)\n\t// This is an JWT expiration.\n\tif c.flags.isSet(connectReceived) {\n\t\tc.expires = time.Now().Add(d).Truncate(time.Second)\n\t}\n}\n\n// Return when this client expires via a claim, or 0 if not set.\nfunc (c *client) claimExpiration() time.Duration {\n\tc.mu.Lock()\n\tdefer c.mu.Unlock()\n\tif c.expires.IsZero() {\n\t\treturn 0\n\t}\n\treturn time.Until(c.expires).Truncate(time.Second)\n}\n\n// Possibly flush the connection and then close the low level connection.\n// The boolean `minimalFlush` indicates if the flush operation should have a\n// minimal write deadline.\n// Lock is held on entry.\nfunc (c *client) flushAndClose(minimalFlush bool) {\n\tif !c.flags.isSet(skipFlushOnClose) && c.out.pb > 0 {\n\t\tif minimalFlush {\n\t\t\tconst lowWriteDeadline = 100 * time.Millisecond\n\n\t\t\t// Reduce the write deadline if needed.\n\t\t\tif c.out.wdl > lowWriteDeadline {\n\t\t\t\tc.out.wdl = lowWriteDeadline\n\t\t\t}\n\t\t}\n\t\tc.flushOutbound()\n\t}\n\tfor i := range c.out.nb {\n\t\tnbPoolPut(c.out.nb[i])\n\t}\n\tc.out.nb = nil\n\t// We can't touch c.out.wnb when a flushOutbound is in progress since it\n\t// is accessed outside the lock there. If in progress, the cleanup will be\n\t// done in flushOutbound when detecting that connection is closed.\n\tif !c.flags.isSet(flushOutbound) {\n\t\tfor i := range c.out.wnb {\n\t\t\tnbPoolPut(c.out.wnb[i])\n\t\t}\n\t\tc.out.wnb = nil\n\t}\n\t// This seem to be important (from experimentation) for the GC to release\n\t// the connection.\n\tc.out.sg = nil\n\n\t// Close the low level connection.\n\tif c.nc != nil {\n\t\t// Starting with Go 1.16, the low level close will set its own deadline\n\t\t// of 5 seconds, so setting our own deadline does not work. Instead,\n\t\t// we will close the TLS connection in separate go routine.\n\t\tnc := c.nc\n\t\tc.nc = nil\n\t\tif _, ok := nc.(*tls.Conn); ok {\n\t\t\tgo func() { nc.Close() }()\n\t\t} else {\n\t\t\tnc.Close()\n\t\t}\n\t}\n}\n\nvar kindStringMap = map[int]string{\n\tCLIENT:    \"Client\",\n\tROUTER:    \"Router\",\n\tGATEWAY:   \"Gateway\",\n\tLEAF:      \"Leafnode\",\n\tJETSTREAM: \"JetStream\",\n\tACCOUNT:   \"Account\",\n\tSYSTEM:    \"System\",\n}\n\nfunc (c *client) kindString() string {\n\tif kindStringVal, ok := kindStringMap[c.kind]; ok {\n\t\treturn kindStringVal\n\t}\n\treturn \"Unknown Type\"\n}\n\n// swapAccountAfterReload will check to make sure the bound account for this client\n// is current. Under certain circumstances after a reload we could be pointing to\n// an older one.\nfunc (c *client) swapAccountAfterReload() {\n\tc.mu.Lock()\n\tsrv := c.srv\n\tan := c.acc.GetName()\n\tc.mu.Unlock()\n\tif srv == nil {\n\t\treturn\n\t}\n\tif acc, _ := srv.LookupAccount(an); acc != nil {\n\t\tc.mu.Lock()\n\t\tif c.acc != acc {\n\t\t\tc.acc = acc\n\t\t}\n\t\tc.mu.Unlock()\n\t}\n}\n\n// processSubsOnConfigReload removes any subscriptions the client has that are no\n// longer authorized, and checks for imports (accounts) due to a config reload.\nfunc (c *client) processSubsOnConfigReload(awcsti map[string]struct{}) {\n\tc.mu.Lock()\n\tvar (\n\t\tcheckPerms = c.perms != nil\n\t\tcheckAcc   = c.acc != nil\n\t\tacc        = c.acc\n\t)\n\tif !checkPerms && !checkAcc {\n\t\tc.mu.Unlock()\n\t\treturn\n\t}\n\tvar (\n\t\t_subs    [32]*subscription\n\t\tsubs     = _subs[:0]\n\t\t_removed [32]*subscription\n\t\tremoved  = _removed[:0]\n\t\tsrv      = c.srv\n\t)\n\tif checkAcc {\n\t\t// We actually only want to check if stream imports have changed.\n\t\tif _, ok := awcsti[acc.Name]; !ok {\n\t\t\tcheckAcc = false\n\t\t}\n\t}\n\t// We will clear any mperms we have here. It will rebuild on the fly with canSubscribe,\n\t// so we do that here as we collect them. We will check result down below.\n\tc.mperms = nil\n\t// Collect client's subs under the lock\n\tfor _, sub := range c.subs {\n\t\t// Just checking to rebuild mperms under the lock, will collect removed though here.\n\t\t// Only collect under subs array of canSubscribe and checkAcc true.\n\t\tcanSub := c.canSubscribe(string(sub.subject))\n\t\tcanQSub := sub.queue != nil && c.canSubscribe(string(sub.subject), string(sub.queue))\n\n\t\tif !canSub && !canQSub {\n\t\t\tremoved = append(removed, sub)\n\t\t} else if checkAcc {\n\t\t\tsubs = append(subs, sub)\n\t\t}\n\t}\n\tc.mu.Unlock()\n\n\t// This list is all subs who are allowed and we need to check accounts.\n\tfor _, sub := range subs {\n\t\tc.mu.Lock()\n\t\toldShadows := sub.shadow\n\t\tsub.shadow = nil\n\t\tc.mu.Unlock()\n\t\tc.addShadowSubscriptions(acc, sub, true)\n\t\tfor _, nsub := range oldShadows {\n\t\t\tnsub.im.acc.sl.Remove(nsub)\n\t\t}\n\t}\n\n\t// Unsubscribe all that need to be removed and report back to client and logs.\n\tfor _, sub := range removed {\n\t\tc.unsubscribe(acc, sub, true, true)\n\t\tc.sendErr(fmt.Sprintf(\"Permissions Violation for Subscription to %q (sid %q)\",\n\t\t\tsub.subject, sub.sid))\n\t\tsrv.Noticef(\"Removed sub %q (sid %q) for %s - not authorized\",\n\t\t\tsub.subject, sub.sid, c.getAuthUser())\n\t}\n}\n\n// Allows us to count up all the queue subscribers during close.\ntype qsub struct {\n\tsub *subscription\n\tn   int32\n}\n\nfunc (c *client) closeConnection(reason ClosedState) {\n\tc.mu.Lock()\n\tif c.flags.isSet(closeConnection) {\n\t\tc.mu.Unlock()\n\t\treturn\n\t}\n\t// Note that we may have markConnAsClosed() invoked before closeConnection(),\n\t// so don't set this to 1, instead bump the count.\n\tc.rref++\n\tc.flags.set(closeConnection)\n\tc.clearAuthTimer()\n\tc.clearPingTimer()\n\tc.clearTlsToTimer()\n\tc.markConnAsClosed(reason)\n\n\t// Unblock anyone who is potentially stalled waiting on us.\n\tif c.out.stc != nil {\n\t\tclose(c.out.stc)\n\t\tc.out.stc = nil\n\t}\n\n\t// If we have remote latency tracking running shut that down.\n\tif c.rrTracking != nil {\n\t\tc.rrTracking.ptmr.Stop()\n\t\tc.rrTracking = nil\n\t}\n\n\t// If we are shutting down, no need to do all the accounting on subs, etc.\n\tif reason == ServerShutdown {\n\t\ts := c.srv\n\t\tc.mu.Unlock()\n\t\tif s != nil {\n\t\t\t// Unregister\n\t\t\ts.removeClient(c)\n\t\t}\n\t\treturn\n\t}\n\n\tvar (\n\t\tkind        = c.kind\n\t\tsrv         = c.srv\n\t\tnoReconnect = c.flags.isSet(noReconnect)\n\t\tacc         = c.acc\n\t\tspoke       bool\n\t)\n\n\t// Snapshot for use if we are a client connection.\n\t// FIXME(dlc) - we can just stub in a new one for client\n\t// and reference existing one.\n\tvar subs []*subscription\n\tif kind == CLIENT || kind == LEAF || kind == JETSTREAM {\n\t\tvar _subs [32]*subscription\n\t\tsubs = _subs[:0]\n\t\t// Do not set c.subs to nil or delete the sub from c.subs here because\n\t\t// it will be needed in saveClosedClient (which has been started as a\n\t\t// go routine in markConnAsClosed). Cleanup will be done there.\n\t\tfor _, sub := range c.subs {\n\t\t\t// Auto-unsubscribe subscriptions must be unsubscribed forcibly.\n\t\t\tsub.max = 0\n\t\t\tsub.close()\n\t\t\tsubs = append(subs, sub)\n\t\t}\n\t\tspoke = c.isSpokeLeafNode()\n\t}\n\n\tc.mu.Unlock()\n\n\t// Remove client's or leaf node or jetstream subscriptions.\n\tif acc != nil && (kind == CLIENT || kind == LEAF || kind == JETSTREAM) {\n\t\tacc.sl.RemoveBatch(subs)\n\t} else if kind == ROUTER {\n\t\tc.removeRemoteSubs()\n\t}\n\n\tif srv != nil {\n\t\t// Unregister\n\t\tsrv.removeClient(c)\n\n\t\tif acc != nil {\n\t\t\t// Update remote subscriptions.\n\t\t\tif kind == CLIENT || kind == LEAF || kind == JETSTREAM {\n\t\t\t\tqsubs := map[string]*qsub{}\n\t\t\t\tfor _, sub := range subs {\n\t\t\t\t\t// Call unsubscribe here to cleanup shadow subscriptions and such.\n\t\t\t\t\tc.unsubscribe(acc, sub, true, false)\n\t\t\t\t\t// Update route as normal for a normal subscriber.\n\t\t\t\t\tif sub.queue == nil {\n\t\t\t\t\t\tif !spoke {\n\t\t\t\t\t\t\tsrv.updateRouteSubscriptionMap(acc, sub, -1)\n\t\t\t\t\t\t\tif srv.gateway.enabled {\n\t\t\t\t\t\t\t\tsrv.gatewayUpdateSubInterest(acc.Name, sub, -1)\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t\tacc.updateLeafNodes(sub, -1)\n\t\t\t\t\t} else {\n\t\t\t\t\t\t// We handle queue subscribers special in case we\n\t\t\t\t\t\t// have a bunch we can just send one update to the\n\t\t\t\t\t\t// connected routes.\n\t\t\t\t\t\tnum := int32(1)\n\t\t\t\t\t\tif kind == LEAF {\n\t\t\t\t\t\t\tnum = sub.qw\n\t\t\t\t\t\t}\n\t\t\t\t\t\tkey := keyFromSub(sub)\n\t\t\t\t\t\tif esub, ok := qsubs[key]; ok {\n\t\t\t\t\t\t\tesub.n += num\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\tqsubs[key] = &qsub{sub, num}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\t// Process any qsubs here.\n\t\t\t\tfor _, esub := range qsubs {\n\t\t\t\t\tif !spoke {\n\t\t\t\t\t\tsrv.updateRouteSubscriptionMap(acc, esub.sub, -(esub.n))\n\t\t\t\t\t\tif srv.gateway.enabled {\n\t\t\t\t\t\t\tsrv.gatewayUpdateSubInterest(acc.Name, esub.sub, -(esub.n))\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tacc.updateLeafNodes(esub.sub, -(esub.n))\n\t\t\t\t}\n\t\t\t}\n\t\t\t// Always remove from the account, otherwise we can leak clients.\n\t\t\t// Note that SYSTEM and ACCOUNT types from above cleanup their own subs.\n\t\t\tif prev := acc.removeClient(c); prev == 1 {\n\t\t\t\tsrv.decActiveAccounts()\n\t\t\t}\n\t\t}\n\t}\n\n\t// Now that we are done with subscriptions, clear the field so that the\n\t// connection can be released and gc'ed.\n\tif kind == CLIENT || kind == LEAF {\n\t\tc.mu.Lock()\n\t\tc.subs = nil\n\t\tc.mu.Unlock()\n\t}\n\n\t// Don't reconnect connections that have been marked with\n\t// the no reconnect flag.\n\tif noReconnect {\n\t\treturn\n\t}\n\n\tc.reconnect()\n}\n\n// Depending on the kind of connections, this may attempt to recreate a connection.\n// The actual reconnect attempt will be started in a go routine.\nfunc (c *client) reconnect() {\n\tvar (\n\t\tretryImplicit bool\n\t\tgwName        string\n\t\tgwIsOutbound  bool\n\t\tgwCfg         *gatewayCfg\n\t\tleafCfg       *leafNodeCfg\n\t)\n\n\tc.mu.Lock()\n\t// Decrease the ref count and perform the reconnect only if == 0.\n\tc.rref--\n\tif c.flags.isSet(noReconnect) || c.rref > 0 {\n\t\tc.mu.Unlock()\n\t\treturn\n\t}\n\tif c.route != nil {\n\t\t// A route is marked as solicited if it was given an URL to connect to,\n\t\t// which would be the case even with implicit (due to gossip), so mark this\n\t\t// as a retry for a route that is solicited and not explicit.\n\t\tretryImplicit = c.route.retry || (c.route.didSolicit && c.route.routeType == Implicit)\n\t}\n\tkind := c.kind\n\tswitch kind {\n\tcase GATEWAY:\n\t\tgwName = c.gw.name\n\t\tgwIsOutbound = c.gw.outbound\n\t\tgwCfg = c.gw.cfg\n\tcase LEAF:\n\t\tif c.isSolicitedLeafNode() {\n\t\t\tleafCfg = c.leaf.remote\n\t\t}\n\t}\n\tsrv := c.srv\n\tc.mu.Unlock()\n\n\t// Check for a solicited route. If it was, start up a reconnect unless\n\t// we are already connected to the other end.\n\tif didSolicit := c.isSolicitedRoute(); didSolicit || retryImplicit {\n\t\tsrv.mu.Lock()\n\t\tdefer srv.mu.Unlock()\n\n\t\t// Capture these under lock\n\t\tc.mu.Lock()\n\t\trid := c.route.remoteID\n\t\trtype := c.route.routeType\n\t\trurl := c.route.url\n\t\taccName := string(c.route.accName)\n\t\tcheckRID := accName == _EMPTY_ && srv.getOpts().Cluster.PoolSize < 1 && rid != _EMPTY_\n\t\tc.mu.Unlock()\n\n\t\t// It is possible that the server is being shutdown.\n\t\t// If so, don't try to reconnect\n\t\tif !srv.isRunning() {\n\t\t\treturn\n\t\t}\n\n\t\tif checkRID && srv.routes[rid] != nil {\n\t\t\t// This is the case of \"no pool\". Make sure that the registered one\n\t\t\t// is upgraded to solicited if the connection trying to reconnect\n\t\t\t// was a solicited one.\n\t\t\tif didSolicit {\n\t\t\t\tif remote := srv.routes[rid][0]; remote != nil {\n\t\t\t\t\tupgradeRouteToSolicited(remote, rurl, rtype)\n\t\t\t\t}\n\t\t\t}\n\t\t\tsrv.Debugf(\"Not attempting reconnect for solicited route, already connected to %q\", rid)\n\t\t\treturn\n\t\t} else if rid == srv.info.ID {\n\t\t\tsrv.Debugf(\"Detected route to self, ignoring %q\", rurl.Redacted())\n\t\t\treturn\n\t\t} else if rtype != Implicit || retryImplicit {\n\t\t\tsrv.Debugf(\"Attempting reconnect for solicited route %q\", rurl.Redacted())\n\t\t\t// Keep track of this go-routine so we can wait for it on\n\t\t\t// server shutdown.\n\t\t\tsrv.startGoRoutine(func() { srv.reConnectToRoute(rurl, rtype, accName) })\n\t\t}\n\t} else if srv != nil && kind == GATEWAY && gwIsOutbound {\n\t\tif gwCfg != nil {\n\t\t\tsrv.Debugf(\"Attempting reconnect for gateway %q\", gwName)\n\t\t\t// Run this as a go routine since we may be called within\n\t\t\t// the solicitGateway itself if there was an error during\n\t\t\t// the creation of the gateway connection.\n\t\t\tsrv.startGoRoutine(func() { srv.reconnectGateway(gwCfg) })\n\t\t} else {\n\t\t\tsrv.Debugf(\"Gateway %q not in configuration, not attempting reconnect\", gwName)\n\t\t}\n\t} else if leafCfg != nil {\n\t\t// Check if this is a solicited leaf node. Start up a reconnect.\n\t\tsrv.startGoRoutine(func() { srv.reConnectToRemoteLeafNode(leafCfg) })\n\t}\n}\n\n// Set the noReconnect flag. This is used before a call to closeConnection()\n// to prevent the connection to reconnect (routes, gateways).\nfunc (c *client) setNoReconnect() {\n\tc.mu.Lock()\n\tc.flags.set(noReconnect)\n\tc.mu.Unlock()\n}\n\n// Returns the client's RTT value with the protection of the client's lock.\nfunc (c *client) getRTTValue() time.Duration {\n\tc.mu.Lock()\n\trtt := c.rtt\n\tc.mu.Unlock()\n\treturn rtt\n}\n\n// This function is used by ROUTER and GATEWAY connections to\n// look for a subject on a given account (since these type of\n// connections are not bound to a specific account).\n// If the c.pa.subject is found in the cache, the cached result\n// is returned, otherwse, we match the account's sublist and update\n// the cache. The cache is pruned if reaching a certain size.\nfunc (c *client) getAccAndResultFromCache() (*Account, *SublistResult) {\n\tvar (\n\t\tacc *Account\n\t\tpac *perAccountCache\n\t\tr   *SublistResult\n\t\tok  bool\n\t)\n\t// Check our cache.\n\tif pac, ok = c.in.pacache[string(c.pa.pacache)]; ok {\n\t\t// Check the genid to see if it's still valid.\n\t\t// Since v2.10.0, the config reload of accounts has been fixed\n\t\t// and an account's sublist pointer should not change, so no need to\n\t\t// lock to access it.\n\t\tsl := pac.acc.sl\n\n\t\tif genid := atomic.LoadUint64(&sl.genid); genid != pac.genid {\n\t\t\tok = false\n\t\t\tc.in.pacache = make(map[string]*perAccountCache)\n\t\t} else {\n\t\t\tacc = pac.acc\n\t\t\tr = pac.results\n\t\t}\n\t}\n\n\tif !ok {\n\t\tif c.kind == ROUTER && len(c.route.accName) > 0 {\n\t\t\tif acc = c.acc; acc == nil {\n\t\t\t\treturn nil, nil\n\t\t\t}\n\t\t} else {\n\t\t\t// Match correct account and sublist.\n\t\t\tif acc, _ = c.srv.LookupAccount(bytesToString(c.pa.account)); acc == nil {\n\t\t\t\treturn nil, nil\n\t\t\t}\n\t\t}\n\t\tsl := acc.sl\n\n\t\t// Match against the account sublist.\n\t\tr = sl.MatchBytes(c.pa.subject)\n\n\t\t// Check if we need to prune.\n\t\tif len(c.in.pacache) >= maxPerAccountCacheSize {\n\t\t\tc.prunePerAccountCache()\n\t\t}\n\n\t\t// Store in our cache,make sure to do so after we prune.\n\t\tc.in.pacache[string(c.pa.pacache)] = &perAccountCache{acc, r, atomic.LoadUint64(&sl.genid)}\n\t}\n\treturn acc, r\n}\n\n// Account will return the associated account for this client.\nfunc (c *client) Account() *Account {\n\tif c == nil {\n\t\treturn nil\n\t}\n\tc.mu.Lock()\n\tacc := c.acc\n\tc.mu.Unlock()\n\treturn acc\n}\n\n// prunePerAccountCache will prune off a random number of cache entries.\nfunc (c *client) prunePerAccountCache() {\n\tn := 0\n\tfor cacheKey := range c.in.pacache {\n\t\tdelete(c.in.pacache, cacheKey)\n\t\tif n++; n > prunePerAccountCacheSize {\n\t\t\tbreak\n\t\t}\n\t}\n}\n\n// pruneClosedSubFromPerAccountCache remove entries that contain subscriptions\n// that have been closed.\nfunc (c *client) pruneClosedSubFromPerAccountCache() {\n\tfor cacheKey, pac := range c.in.pacache {\n\t\tfor _, sub := range pac.results.psubs {\n\t\t\tif sub.isClosed() {\n\t\t\t\tgoto REMOVE\n\t\t\t}\n\t\t}\n\t\tfor _, qsub := range pac.results.qsubs {\n\t\t\tfor _, sub := range qsub {\n\t\t\t\tif sub.isClosed() {\n\t\t\t\t\tgoto REMOVE\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tcontinue\n\tREMOVE:\n\t\tdelete(c.in.pacache, cacheKey)\n\t}\n}\n\n// Returns our service account for this request.\nfunc (ci *ClientInfo) serviceAccount() string {\n\tif ci == nil {\n\t\treturn _EMPTY_\n\t}\n\tif ci.Service != _EMPTY_ {\n\t\treturn ci.Service\n\t}\n\treturn ci.Account\n}\n\n// Add in our server and cluster information to this client info.\nfunc (c *client) addServerAndClusterInfo(ci *ClientInfo) {\n\tif ci == nil {\n\t\treturn\n\t}\n\t// Server\n\tif c.kind != LEAF {\n\t\tci.Server = c.srv.Name()\n\t} else if c.kind == LEAF {\n\t\tci.Server = c.leaf.remoteServer\n\t}\n\t// Cluster\n\tci.Cluster = c.srv.cachedClusterName()\n\t// If we have gateways fill in cluster alternates.\n\t// These will be in RTT asc order.\n\tif c.srv.gateway.enabled {\n\t\tvar gws []*client\n\t\tc.srv.getOutboundGatewayConnections(&gws)\n\t\tfor _, c := range gws {\n\t\t\tc.mu.Lock()\n\t\t\tcn := c.gw.name\n\t\t\tc.mu.Unlock()\n\t\t\tci.Alternates = append(ci.Alternates, cn)\n\t\t}\n\t}\n}\n\n// Grabs the information for this client.\nfunc (c *client) getClientInfo(detailed bool) *ClientInfo {\n\tif c == nil || (c.kind != CLIENT && c.kind != LEAF && c.kind != JETSTREAM && c.kind != ACCOUNT) {\n\t\treturn nil\n\t}\n\n\t// Result\n\tvar ci ClientInfo\n\n\tif detailed {\n\t\tc.addServerAndClusterInfo(&ci)\n\t}\n\n\tc.mu.Lock()\n\t// RTT and Account are always added.\n\tci.Account = accForClient(c)\n\tci.RTT = c.rtt\n\t// Detailed signals additional opt in.\n\tif detailed {\n\t\tci.Start = &c.start\n\t\tci.Host = c.host\n\t\tci.ID = c.cid\n\t\tci.Name = c.opts.Name\n\t\tci.User = c.getRawAuthUser()\n\t\tci.Lang = c.opts.Lang\n\t\tci.Version = c.opts.Version\n\t\tci.Jwt = c.opts.JWT\n\t\tci.IssuerKey = issuerForClient(c)\n\t\tci.NameTag = c.nameTag\n\t\tci.Tags = c.tags\n\t\tci.Kind = c.kindString()\n\t\tci.ClientType = c.clientTypeString()\n\t}\n\tc.mu.Unlock()\n\treturn &ci\n}\n\nfunc (c *client) doTLSServerHandshake(typ string, tlsConfig *tls.Config, timeout float64, pCerts PinnedCertSet) error {\n\t_, err := c.doTLSHandshake(typ, false, nil, tlsConfig, _EMPTY_, timeout, pCerts)\n\treturn err\n}\n\nfunc (c *client) doTLSClientHandshake(typ string, url *url.URL, tlsConfig *tls.Config, tlsName string, timeout float64, pCerts PinnedCertSet) (bool, error) {\n\treturn c.doTLSHandshake(typ, true, url, tlsConfig, tlsName, timeout, pCerts)\n}\n\n// Performs either server or client side (if solicit is true) TLS Handshake.\n// On error, the TLS handshake error has been logged and the connection\n// has been closed.\n//\n// Lock is held on entry.\nfunc (c *client) doTLSHandshake(typ string, solicit bool, url *url.URL, tlsConfig *tls.Config, tlsName string, timeout float64, pCerts PinnedCertSet) (bool, error) {\n\tvar host string\n\tvar resetTLSName bool\n\tvar err error\n\n\t// Capture kind for some debug/error statements.\n\tkind := c.kind\n\n\t// If we solicited, we will act like the client, otherwise the server.\n\tif solicit {\n\t\tc.Debugf(\"Starting TLS %s client handshake\", typ)\n\t\tif tlsConfig.ServerName == _EMPTY_ {\n\t\t\t// If the given url is a hostname, use this hostname for the\n\t\t\t// ServerName. If it is an IP, use the cfg's tlsName. If none\n\t\t\t// is available, resort to current IP.\n\t\t\thost = url.Hostname()\n\t\t\tif tlsName != _EMPTY_ && net.ParseIP(host) != nil {\n\t\t\t\thost = tlsName\n\t\t\t}\n\t\t\ttlsConfig.ServerName = host\n\t\t}\n\t\tc.nc = tls.Client(c.nc, tlsConfig)\n\t} else {\n\t\tif kind == CLIENT {\n\t\t\tc.Debugf(\"Starting TLS client connection handshake\")\n\t\t} else {\n\t\t\tc.Debugf(\"Starting TLS %s server handshake\", typ)\n\t\t}\n\t\tc.nc = tls.Server(c.nc, tlsConfig)\n\t}\n\n\tconn := c.nc.(*tls.Conn)\n\n\t// Setup the timeout\n\tttl := secondsToDuration(timeout)\n\tc.tlsTo = time.AfterFunc(ttl, func() { tlsTimeout(c, conn) })\n\tconn.SetReadDeadline(time.Now().Add(ttl))\n\n\tc.mu.Unlock()\n\tif err = conn.Handshake(); err != nil {\n\t\tif solicit {\n\t\t\t// Based on type of error, possibly clear the saved tlsName\n\t\t\t// See: https://github.com/nats-io/nats-server/issues/1256\n\t\t\t// NOTE: As of Go 1.20, the HostnameError is wrapped so cannot\n\t\t\t// type assert to check directly.\n\t\t\tvar hostnameErr x509.HostnameError\n\t\t\tif errors.As(err, &hostnameErr) {\n\t\t\t\tif host == tlsName {\n\t\t\t\t\tresetTLSName = true\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t} else if !c.matchesPinnedCert(pCerts) {\n\t\terr = ErrCertNotPinned\n\t}\n\n\tif err != nil {\n\t\tvar detail string\n\t\tvar subjs []string\n\t\tif ve, ok := err.(*tls.CertificateVerificationError); ok {\n\t\t\tfor _, cert := range ve.UnverifiedCertificates {\n\t\t\t\tfp := sha256.Sum256(cert.Raw)\n\t\t\t\tfph := hex.EncodeToString(fp[:])\n\t\t\t\tsubjs = append(subjs, fmt.Sprintf(\"%s SHA-256: %s\", cert.Subject.String(), fph))\n\t\t\t}\n\t\t}\n\t\tif len(subjs) > 0 {\n\t\t\tdetail = fmt.Sprintf(\" (%s)\", strings.Join(subjs, \"; \"))\n\t\t}\n\t\tif kind == CLIENT {\n\t\t\tc.Errorf(\"TLS handshake error: %v%s\", err, detail)\n\t\t} else {\n\t\t\tc.Errorf(\"TLS %s handshake error: %v%s\", typ, err, detail)\n\t\t}\n\t\tc.closeConnection(TLSHandshakeError)\n\n\t\t// Grab the lock before returning since the caller was holding the lock on entry\n\t\tc.mu.Lock()\n\t\t// Returning any error is fine. Since the connection is closed ErrConnectionClosed\n\t\t// is appropriate.\n\t\treturn resetTLSName, ErrConnectionClosed\n\t}\n\n\t// Reset the read deadline\n\tconn.SetReadDeadline(time.Time{})\n\n\t// Re-Grab lock\n\tc.mu.Lock()\n\n\t// To be consistent with client, set this flag to indicate that handshake is done\n\tc.flags.set(handshakeComplete)\n\n\t// The connection still may have been closed on success handshake due\n\t// to a race with tls timeout. If that the case, return error indicating\n\t// that the connection is closed.\n\tif c.isClosed() {\n\t\terr = ErrConnectionClosed\n\t}\n\n\treturn false, err\n}\n\n// getRawAuthUserLock returns the raw auth user for the client.\n// Will acquire the client lock.\nfunc (c *client) getRawAuthUserLock() string {\n\tc.mu.Lock()\n\tdefer c.mu.Unlock()\n\treturn c.getRawAuthUser()\n}\n\n// getRawAuthUser returns the raw auth user for the client.\n// Lock should be held.\nfunc (c *client) getRawAuthUser() string {\n\tswitch {\n\tcase c.opts.Nkey != _EMPTY_:\n\t\treturn c.opts.Nkey\n\tcase c.opts.Username != _EMPTY_:\n\t\treturn c.opts.Username\n\tcase c.opts.JWT != _EMPTY_:\n\t\treturn c.pubKey\n\tcase c.opts.Token != _EMPTY_:\n\t\treturn \"[REDACTED]\"\n\tdefault:\n\t\treturn _EMPTY_\n\t}\n}\n\n// getAuthUser returns the auth user for the client.\n// Lock should be held.\nfunc (c *client) getAuthUser() string {\n\tswitch {\n\tcase c.opts.Nkey != _EMPTY_:\n\t\treturn fmt.Sprintf(\"Nkey %q\", c.opts.Nkey)\n\tcase c.opts.Username != _EMPTY_:\n\t\treturn fmt.Sprintf(\"User %q\", c.opts.Username)\n\tcase c.opts.JWT != _EMPTY_:\n\t\treturn fmt.Sprintf(\"JWT User %q\", c.pubKey)\n\tcase c.opts.Token != _EMPTY_:\n\t\treturn fmt.Sprintf(\"Token %q\", \"[REDACTED]\")\n\tdefault:\n\t\treturn `User \"N/A\"`\n\t}\n}\n\n// Given an array of strings, this function converts it to a map as long\n// as all the content (converted to upper-case) matches some constants.\n\n// Converts the given array of strings to a map of string.\n// The strings are converted to upper-case and added to the map only\n// if the server recognize them as valid connection types.\n// If there are unknown connection types, the map of valid ones is returned\n// along with an error that contains the name of the unknown.\nfunc convertAllowedConnectionTypes(cts []string) (map[string]struct{}, error) {\n\tvar unknown []string\n\tm := make(map[string]struct{}, len(cts))\n\tfor _, i := range cts {\n\t\ti = strings.ToUpper(i)\n\t\tswitch i {\n\t\tcase jwt.ConnectionTypeStandard, jwt.ConnectionTypeWebsocket,\n\t\t\tjwt.ConnectionTypeLeafnode, jwt.ConnectionTypeLeafnodeWS,\n\t\t\tjwt.ConnectionTypeMqtt, jwt.ConnectionTypeMqttWS,\n\t\t\tjwt.ConnectionTypeInProcess:\n\t\t\tm[i] = struct{}{}\n\t\tdefault:\n\t\t\tunknown = append(unknown, i)\n\t\t}\n\t}\n\tvar err error\n\t// We will still return the map of valid ones.\n\tif len(unknown) != 0 {\n\t\terr = fmt.Errorf(\"invalid connection types %q\", unknown)\n\t}\n\treturn m, err\n}\n\n// This will return true if the connection is of a type present in the given `acts` map.\n// Note that so far this is used only for CLIENT or LEAF connections.\n// But a CLIENT can be standard or websocket (and other types in the future).\nfunc (c *client) connectionTypeAllowed(acts map[string]struct{}) bool {\n\t// Empty means all type of clients are allowed\n\tif len(acts) == 0 {\n\t\treturn true\n\t}\n\tvar want string\n\tswitch c.kind {\n\tcase CLIENT:\n\t\tswitch c.clientType() {\n\t\tcase NATS:\n\t\t\tif c.iproc {\n\t\t\t\twant = jwt.ConnectionTypeInProcess\n\t\t\t} else {\n\t\t\t\twant = jwt.ConnectionTypeStandard\n\t\t\t}\n\t\tcase WS:\n\t\t\twant = jwt.ConnectionTypeWebsocket\n\t\tcase MQTT:\n\t\t\tif c.isWebsocket() {\n\t\t\t\twant = jwt.ConnectionTypeMqttWS\n\t\t\t} else {\n\t\t\t\twant = jwt.ConnectionTypeMqtt\n\t\t\t}\n\t\t}\n\tcase LEAF:\n\t\tif c.isWebsocket() {\n\t\t\twant = jwt.ConnectionTypeLeafnodeWS\n\t\t} else {\n\t\t\twant = jwt.ConnectionTypeLeafnode\n\t\t}\n\t}\n\t_, ok := acts[want]\n\treturn ok\n}\n\n// isClosed returns true if either closeConnection or connMarkedClosed\n// flag have been set, or if `nc` is nil, which may happen in tests.\nfunc (c *client) isClosed() bool {\n\treturn c.flags.isSet(closeConnection) || c.flags.isSet(connMarkedClosed) || c.nc == nil\n}\n\n// Logging functionality scoped to a client or route.\nfunc (c *client) Error(err error) {\n\tc.srv.Errors(c, err)\n}\n\nfunc (c *client) Errorf(format string, v ...any) {\n\tformat = fmt.Sprintf(\"%s - %s\", c, format)\n\tc.srv.Errorf(format, v...)\n}\n\nfunc (c *client) Debugf(format string, v ...any) {\n\tformat = fmt.Sprintf(\"%s - %s\", c, format)\n\tc.srv.Debugf(format, v...)\n}\n\nfunc (c *client) Noticef(format string, v ...any) {\n\tformat = fmt.Sprintf(\"%s - %s\", c, format)\n\tc.srv.Noticef(format, v...)\n}\n\nfunc (c *client) Tracef(format string, v ...any) {\n\tformat = fmt.Sprintf(\"%s - %s\", c, format)\n\tc.srv.Tracef(format, v...)\n}\n\nfunc (c *client) Warnf(format string, v ...any) {\n\tformat = fmt.Sprintf(\"%s - %s\", c, format)\n\tc.srv.Warnf(format, v...)\n}\n\nfunc (c *client) rateLimitFormatWarnf(format string, v ...any) {\n\tif _, loaded := c.srv.rateLimitLogging.LoadOrStore(format, time.Now()); loaded {\n\t\treturn\n\t}\n\tstatement := fmt.Sprintf(format, v...)\n\tc.Warnf(\"%s\", statement)\n}\n\nfunc (c *client) RateLimitWarnf(format string, v ...any) {\n\t// Do the check before adding the client info to the format...\n\tstatement := fmt.Sprintf(format, v...)\n\tif _, loaded := c.srv.rateLimitLogging.LoadOrStore(statement, time.Now()); loaded {\n\t\treturn\n\t}\n\tc.Warnf(\"%s\", statement)\n}\n\n// Set the very first PING to a lower interval to capture the initial RTT.\n// After that the PING interval will be set to the user defined value.\n// Client lock should be held.\nfunc (c *client) setFirstPingTimer() {\n\ts := c.srv\n\tif s == nil {\n\t\treturn\n\t}\n\topts := s.getOpts()\n\td := opts.PingInterval\n\n\tif c.kind == ROUTER && opts.Cluster.PingInterval > 0 {\n\t\td = opts.Cluster.PingInterval\n\t}\n\tif !opts.DisableShortFirstPing {\n\t\tif c.kind != CLIENT {\n\t\t\tif d > firstPingInterval {\n\t\t\t\td = firstPingInterval\n\t\t\t}\n\t\t\td = adjustPingInterval(c.kind, d)\n\t\t} else if d > firstClientPingInterval {\n\t\t\td = firstClientPingInterval\n\t\t}\n\t}\n\t// We randomize the first one by an offset up to 20%, e.g. 2m ~= max 24s.\n\taddDelay := rand.Int63n(int64(d / 5))\n\td += time.Duration(addDelay)\n\t// In the case of ROUTER/LEAF and when compression is configured, it is possible\n\t// that this timer was already set, but just to detect a stale connection\n\t// since we have to delay the first PING after compression negotiation\n\t// occurred.\n\tif c.ping.tmr != nil {\n\t\tc.ping.tmr.Stop()\n\t}\n\tc.ping.tmr = time.AfterFunc(d, c.processPingTimer)\n}\n",
    "source_file": "server/client.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2018-2025 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage server\n\nimport (\n\t\"bytes\"\n\t\"compress/gzip\"\n\t\"crypto/sha256\"\n\t\"crypto/x509\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"fmt\"\n\t\"math\"\n\t\"math/rand\"\n\t\"net/http\"\n\t\"runtime\"\n\t\"runtime/debug\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t\"github.com/klauspost/compress/s2\"\n\t\"github.com/nats-io/jwt/v2\"\n\t\"github.com/nats-io/nats-server/v2/server/certidp\"\n\t\"github.com/nats-io/nats-server/v2/server/pse\"\n)\n\nconst (\n\taccLookupReqTokens = 6\n\taccLookupReqSubj   = \"$SYS.REQ.ACCOUNT.%s.CLAIMS.LOOKUP\"\n\taccPackReqSubj     = \"$SYS.REQ.CLAIMS.PACK\"\n\taccListReqSubj     = \"$SYS.REQ.CLAIMS.LIST\"\n\taccClaimsReqSubj   = \"$SYS.REQ.CLAIMS.UPDATE\"\n\taccDeleteReqSubj   = \"$SYS.REQ.CLAIMS.DELETE\"\n\n\tconnectEventSubj    = \"$SYS.ACCOUNT.%s.CONNECT\"\n\tdisconnectEventSubj = \"$SYS.ACCOUNT.%s.DISCONNECT\"\n\taccDirectReqSubj    = \"$SYS.REQ.ACCOUNT.%s.%s\"\n\taccPingReqSubj      = \"$SYS.REQ.ACCOUNT.PING.%s\" // atm. only used for STATZ and CONNZ import from system account\n\t// kept for backward compatibility when using http resolver\n\t// this overlaps with the names for events but you'd have to have the operator private key in order to succeed.\n\taccUpdateEventSubjOld     = \"$SYS.ACCOUNT.%s.CLAIMS.UPDATE\"\n\taccUpdateEventSubjNew     = \"$SYS.REQ.ACCOUNT.%s.CLAIMS.UPDATE\"\n\tconnsRespSubj             = \"$SYS._INBOX_.%s\"\n\taccConnsEventSubjNew      = \"$SYS.ACCOUNT.%s.SERVER.CONNS\"\n\taccConnsEventSubjOld      = \"$SYS.SERVER.ACCOUNT.%s.CONNS\" // kept for backward compatibility\n\tlameDuckEventSubj         = \"$SYS.SERVER.%s.LAMEDUCK\"\n\tshutdownEventSubj         = \"$SYS.SERVER.%s.SHUTDOWN\"\n\tclientKickReqSubj         = \"$SYS.REQ.SERVER.%s.KICK\"\n\tclientLDMReqSubj          = \"$SYS.REQ.SERVER.%s.LDM\"\n\tauthErrorEventSubj        = \"$SYS.SERVER.%s.CLIENT.AUTH.ERR\"\n\tauthErrorAccountEventSubj = \"$SYS.ACCOUNT.CLIENT.AUTH.ERR\"\n\tserverStatsSubj           = \"$SYS.SERVER.%s.STATSZ\"\n\tserverDirectReqSubj       = \"$SYS.REQ.SERVER.%s.%s\"\n\tserverPingReqSubj         = \"$SYS.REQ.SERVER.PING.%s\"\n\tserverStatsPingReqSubj    = \"$SYS.REQ.SERVER.PING\"             // use $SYS.REQ.SERVER.PING.STATSZ instead\n\tserverReloadReqSubj       = \"$SYS.REQ.SERVER.%s.RELOAD\"        // with server ID\n\tleafNodeConnectEventSubj  = \"$SYS.ACCOUNT.%s.LEAFNODE.CONNECT\" // for internal use only\n\tremoteLatencyEventSubj    = \"$SYS.LATENCY.M2.%s\"\n\tinboxRespSubj             = \"$SYS._INBOX.%s.%s\"\n\n\t// Used to return information to a user on bound account and user permissions.\n\tuserDirectInfoSubj = \"$SYS.REQ.USER.INFO\"\n\tuserDirectReqSubj  = \"$SYS.REQ.USER.%s.INFO\"\n\n\t// FIXME(dlc) - Should account scope, even with wc for now, but later on\n\t// we can then shard as needed.\n\taccNumSubsReqSubj = \"$SYS.REQ.ACCOUNT.NSUBS\"\n\n\t// These are for exported debug services. These are local to this server only.\n\taccSubsSubj = \"$SYS.DEBUG.SUBSCRIBERS\"\n\n\tshutdownEventTokens = 4\n\tserverSubjectIndex  = 2\n\taccUpdateTokensNew  = 6\n\taccUpdateTokensOld  = 5\n\taccUpdateAccIdxOld  = 2\n\n\taccReqTokens   = 5\n\taccReqAccIndex = 3\n\n\tocspPeerRejectEventSubj           = \"$SYS.SERVER.%s.OCSP.PEER.CONN.REJECT\"\n\tocspPeerChainlinkInvalidEventSubj = \"$SYS.SERVER.%s.OCSP.PEER.LINK.INVALID\"\n)\n\n// FIXME(dlc) - make configurable.\nvar eventsHBInterval = 30 * time.Second\nvar statsHBInterval = 10 * time.Second\n\n// Default minimum wait time for sending statsz\nconst defaultStatszRateLimit = 1 * time.Second\n\n// Variable version so we can set in tests.\nvar statszRateLimit = defaultStatszRateLimit\n\ntype sysMsgHandler func(sub *subscription, client *client, acc *Account, subject, reply string, hdr, msg []byte)\n\n// Used if we have to queue things internally to avoid the route/gw path.\ntype inSysMsg struct {\n\tsub  *subscription\n\tc    *client\n\tacc  *Account\n\tsubj string\n\trply string\n\thdr  []byte\n\tmsg  []byte\n\tcb   sysMsgHandler\n}\n\n// Used to send and receive messages from inside the server.\ntype internal struct {\n\taccount        *Account\n\tclient         *client\n\tseq            uint64\n\tsid            int\n\tservers        map[string]*serverUpdate\n\tsweeper        *time.Timer\n\tstmr           *time.Timer\n\treplies        map[string]msgHandler\n\tsendq          *ipQueue[*pubMsg]\n\trecvq          *ipQueue[*inSysMsg]\n\trecvqp         *ipQueue[*inSysMsg] // For STATSZ/Pings\n\tresetCh        chan struct{}\n\twg             sync.WaitGroup\n\tsq             *sendq\n\torphMax        time.Duration\n\tchkOrph        time.Duration\n\tstatsz         time.Duration\n\tcstatsz        time.Duration\n\tshash          string\n\tinboxPre       string\n\tremoteStatsSub *subscription\n\tlastStatsz     time.Time\n}\n\n// ServerStatsMsg is sent periodically with stats updates.\ntype ServerStatsMsg struct {\n\tServer ServerInfo  `json:\"server\"`\n\tStats  ServerStats `json:\"statsz\"`\n}\n\n// ConnectEventMsg is sent when a new connection is made that is part of an account.\ntype ConnectEventMsg struct {\n\tTypedEvent\n\tServer ServerInfo `json:\"server\"`\n\tClient ClientInfo `json:\"client\"`\n}\n\n// ConnectEventMsgType is the schema type for ConnectEventMsg\nconst ConnectEventMsgType = \"io.nats.server.advisory.v1.client_connect\"\n\n// DisconnectEventMsg is sent when a new connection previously defined from a\n// ConnectEventMsg is closed.\ntype DisconnectEventMsg struct {\n\tTypedEvent\n\tServer   ServerInfo `json:\"server\"`\n\tClient   ClientInfo `json:\"client\"`\n\tSent     DataStats  `json:\"sent\"`\n\tReceived DataStats  `json:\"received\"`\n\tReason   string     `json:\"reason\"`\n}\n\n// DisconnectEventMsgType is the schema type for DisconnectEventMsg\nconst DisconnectEventMsgType = \"io.nats.server.advisory.v1.client_disconnect\"\n\n// OCSPPeerRejectEventMsg is sent when a peer TLS handshake is ultimately rejected due to OCSP invalidation.\n// A \"peer\" can be an inbound client connection or a leaf connection to a remote server. Peer in event payload\n// is always the peer's (TLS) leaf cert, which may or may be the invalid cert (See also OCSPPeerChainlinkInvalidEventMsg)\ntype OCSPPeerRejectEventMsg struct {\n\tTypedEvent\n\tKind   string           `json:\"kind\"`\n\tPeer   certidp.CertInfo `json:\"peer\"`\n\tServer ServerInfo       `json:\"server\"`\n\tReason string           `json:\"reason\"`\n}\n\n// OCSPPeerRejectEventMsgType is the schema type for OCSPPeerRejectEventMsg\nconst OCSPPeerRejectEventMsgType = \"io.nats.server.advisory.v1.ocsp_peer_reject\"\n\n// OCSPPeerChainlinkInvalidEventMsg is sent when a certificate (link) in a valid TLS chain is found to be OCSP invalid\n// during a peer TLS handshake. A \"peer\" can be an inbound client connection or a leaf connection to a remote server.\n// Peer and Link may be the same if the invalid cert was the peer's leaf cert\ntype OCSPPeerChainlinkInvalidEventMsg struct {\n\tTypedEvent\n\tLink   certidp.CertInfo `json:\"link\"`\n\tPeer   certidp.CertInfo `json:\"peer\"`\n\tServer ServerInfo       `json:\"server\"`\n\tReason string           `json:\"reason\"`\n}\n\n// OCSPPeerChainlinkInvalidEventMsgType is the schema type for OCSPPeerChainlinkInvalidEventMsg\nconst OCSPPeerChainlinkInvalidEventMsgType = \"io.nats.server.advisory.v1.ocsp_peer_link_invalid\"\n\n// AccountNumConns is an event that will be sent from a server that is tracking\n// a given account when the number of connections changes. It will also HB\n// updates in the absence of any changes.\ntype AccountNumConns struct {\n\tTypedEvent\n\tServer ServerInfo `json:\"server\"`\n\tAccountStat\n}\n\n// AccountStat contains the data common between AccountNumConns and AccountStatz\ntype AccountStat struct {\n\tAccount       string    `json:\"acc\"`\n\tName          string    `json:\"name\"`\n\tConns         int       `json:\"conns\"`\n\tLeafNodes     int       `json:\"leafnodes\"`\n\tTotalConns    int       `json:\"total_conns\"`\n\tNumSubs       uint32    `json:\"num_subscriptions\"`\n\tSent          DataStats `json:\"sent\"`\n\tReceived      DataStats `json:\"received\"`\n\tSlowConsumers int64     `json:\"slow_consumers\"`\n}\n\nconst AccountNumConnsMsgType = \"io.nats.server.advisory.v1.account_connections\"\n\n// accNumConnsReq is sent when we are starting to track an account for the first\n// time. We will request others send info to us about their local state.\ntype accNumConnsReq struct {\n\tServer  ServerInfo `json:\"server\"`\n\tAccount string     `json:\"acc\"`\n}\n\n// ServerID is basic static info for a server.\ntype ServerID struct {\n\tName string `json:\"name\"`\n\tHost string `json:\"host\"`\n\tID   string `json:\"id\"`\n}\n\n// Type for our server capabilities.\ntype ServerCapability uint64\n\n// ServerInfo identifies remote servers.\ntype ServerInfo struct {\n\tName    string   `json:\"name\"`\n\tHost    string   `json:\"host\"`\n\tID      string   `json:\"id\"`\n\tCluster string   `json:\"cluster,omitempty\"`\n\tDomain  string   `json:\"domain,omitempty\"`\n\tVersion string   `json:\"ver\"`\n\tTags    []string `json:\"tags,omitempty\"`\n\t// Whether JetStream is enabled (deprecated in favor of the `ServerCapability`).\n\tJetStream bool `json:\"jetstream\"`\n\t// Generic capability flags\n\tFlags ServerCapability `json:\"flags\"`\n\t// Sequence and Time from the remote server for this message.\n\tSeq  uint64    `json:\"seq\"`\n\tTime time.Time `json:\"time\"`\n}\n\nconst (\n\tJetStreamEnabled     ServerCapability = 1 << iota // Server had JetStream enabled.\n\tBinaryStreamSnapshot                              // New stream snapshot capability.\n\tAccountNRG                                        // Move NRG traffic out of system account.\n)\n\n// Set JetStream capability.\nfunc (si *ServerInfo) SetJetStreamEnabled() {\n\tsi.Flags |= JetStreamEnabled\n\t// Still set old version.\n\tsi.JetStream = true\n}\n\n// JetStreamEnabled indicates whether or not we have JetStream enabled.\nfunc (si *ServerInfo) JetStreamEnabled() bool {\n\t// Take into account old version.\n\treturn si.Flags&JetStreamEnabled != 0 || si.JetStream\n}\n\n// Set binary stream snapshot capability.\nfunc (si *ServerInfo) SetBinaryStreamSnapshot() {\n\tsi.Flags |= BinaryStreamSnapshot\n}\n\n// JetStreamEnabled indicates whether or not we have binary stream snapshot capbilities.\nfunc (si *ServerInfo) BinaryStreamSnapshot() bool {\n\treturn si.Flags&BinaryStreamSnapshot != 0\n}\n\n// Set account NRG capability.\nfunc (si *ServerInfo) SetAccountNRG() {\n\tsi.Flags |= AccountNRG\n}\n\n// AccountNRG indicates whether or not we support moving the NRG traffic out of the\n// system account and into the asset account.\nfunc (si *ServerInfo) AccountNRG() bool {\n\treturn si.Flags&AccountNRG != 0\n}\n\n// ClientInfo is detailed information about the client forming a connection.\ntype ClientInfo struct {\n\tStart      *time.Time    `json:\"start,omitempty\"`\n\tHost       string        `json:\"host,omitempty\"`\n\tID         uint64        `json:\"id,omitempty\"`\n\tAccount    string        `json:\"acc,omitempty\"`\n\tService    string        `json:\"svc,omitempty\"`\n\tUser       string        `json:\"user,omitempty\"`\n\tName       string        `json:\"name,omitempty\"`\n\tLang       string        `json:\"lang,omitempty\"`\n\tVersion    string        `json:\"ver,omitempty\"`\n\tRTT        time.Duration `json:\"rtt,omitempty\"`\n\tServer     string        `json:\"server,omitempty\"`\n\tCluster    string        `json:\"cluster,omitempty\"`\n\tAlternates []string      `json:\"alts,omitempty\"`\n\tStop       *time.Time    `json:\"stop,omitempty\"`\n\tJwt        string        `json:\"jwt,omitempty\"`\n\tIssuerKey  string        `json:\"issuer_key,omitempty\"`\n\tNameTag    string        `json:\"name_tag,omitempty\"`\n\tTags       jwt.TagList   `json:\"tags,omitempty\"`\n\tKind       string        `json:\"kind,omitempty\"`\n\tClientType string        `json:\"client_type,omitempty\"`\n\tMQTTClient string        `json:\"client_id,omitempty\"` // This is the MQTT client ID\n\tNonce      string        `json:\"nonce,omitempty\"`\n}\n\n// forAssignmentSnap returns the minimum amount of ClientInfo we need for assignment snapshots.\nfunc (ci *ClientInfo) forAssignmentSnap() *ClientInfo {\n\treturn &ClientInfo{\n\t\tAccount: ci.Account,\n\t\tService: ci.Service,\n\t\tCluster: ci.Cluster,\n\t}\n}\n\n// forProposal returns the minimum amount of ClientInfo we need for assignment proposals.\nfunc (ci *ClientInfo) forProposal() *ClientInfo {\n\tif ci == nil {\n\t\treturn nil\n\t}\n\tcci := *ci\n\tcci.Jwt = _EMPTY_\n\tcci.IssuerKey = _EMPTY_\n\treturn &cci\n}\n\n// forAdvisory returns the minimum amount of ClientInfo we need for JS advisory events.\nfunc (ci *ClientInfo) forAdvisory() *ClientInfo {\n\tif ci == nil {\n\t\treturn nil\n\t}\n\tcci := *ci\n\tcci.Jwt = _EMPTY_\n\tcci.Alternates = nil\n\treturn &cci\n}\n\n// ServerStats hold various statistics that we will periodically send out.\ntype ServerStats struct {\n\tStart              time.Time           `json:\"start\"`\n\tMem                int64               `json:\"mem\"`\n\tCores              int                 `json:\"cores\"`\n\tCPU                float64             `json:\"cpu\"`\n\tConnections        int                 `json:\"connections\"`\n\tTotalConnections   uint64              `json:\"total_connections\"`\n\tActiveAccounts     int                 `json:\"active_accounts\"`\n\tNumSubs            uint32              `json:\"subscriptions\"`\n\tSent               DataStats           `json:\"sent\"`\n\tReceived           DataStats           `json:\"received\"`\n\tSlowConsumers      int64               `json:\"slow_consumers\"`\n\tSlowConsumersStats *SlowConsumersStats `json:\"slow_consumer_stats,omitempty\"`\n\tRoutes             []*RouteStat        `json:\"routes,omitempty\"`\n\tGateways           []*GatewayStat      `json:\"gateways,omitempty\"`\n\tActiveServers      int                 `json:\"active_servers,omitempty\"`\n\tJetStream          *JetStreamVarz      `json:\"jetstream,omitempty\"`\n\tMemLimit           int64               `json:\"gomemlimit,omitempty\"`\n\tMaxProcs           int                 `json:\"gomaxprocs,omitempty\"`\n}\n\n// RouteStat holds route statistics.\ntype RouteStat struct {\n\tID       uint64    `json:\"rid\"`\n\tName     string    `json:\"name,omitempty\"`\n\tSent     DataStats `json:\"sent\"`\n\tReceived DataStats `json:\"received\"`\n\tPending  int       `json:\"pending\"`\n}\n\n// GatewayStat holds gateway statistics.\ntype GatewayStat struct {\n\tID         uint64    `json:\"gwid\"`\n\tName       string    `json:\"name\"`\n\tSent       DataStats `json:\"sent\"`\n\tReceived   DataStats `json:\"received\"`\n\tNumInbound int       `json:\"inbound_connections\"`\n}\n\ntype dataStats struct {\n\tMsgs  int64 `json:\"msgs\"`\n\tBytes int64 `json:\"bytes\"`\n}\n\n// DataStats reports how may msg and bytes. Applicable for both sent and received.\ntype DataStats struct {\n\tdataStats\n\tGateways dataStats `json:\"gateways,omitempty\"`\n\tRoutes   dataStats `json:\"routes,omitempty\"`\n\tLeafs    dataStats `json:\"leafs,omitempty\"`\n}\n\n// Used for internally queueing up messages that the server wants to send.\ntype pubMsg struct {\n\tc    *client\n\tsub  string\n\trply string\n\tsi   *ServerInfo\n\thdr  map[string]string\n\tmsg  any\n\toct  compressionType\n\techo bool\n\tlast bool\n}\n\nvar pubMsgPool sync.Pool\n\nfunc newPubMsg(c *client, sub, rply string, si *ServerInfo, hdr map[string]string,\n\tmsg any, oct compressionType, echo, last bool) *pubMsg {\n\n\tvar m *pubMsg\n\tpm := pubMsgPool.Get()\n\tif pm != nil {\n\t\tm = pm.(*pubMsg)\n\t} else {\n\t\tm = &pubMsg{}\n\t}\n\t// When getting something from a pool it is critical that all fields are\n\t// initialized. Doing this way guarantees that if someone adds a field to\n\t// the structure, the compiler will fail the build if this line is not updated.\n\t(*m) = pubMsg{c, sub, rply, si, hdr, msg, oct, echo, last}\n\treturn m\n}\n\nfunc (pm *pubMsg) returnToPool() {\n\tif pm == nil {\n\t\treturn\n\t}\n\tpm.c, pm.sub, pm.rply, pm.si, pm.hdr, pm.msg = nil, _EMPTY_, _EMPTY_, nil, nil, nil\n\tpubMsgPool.Put(pm)\n}\n\n// Used to track server updates.\ntype serverUpdate struct {\n\tseq   uint64\n\tltime time.Time\n}\n\n// TypedEvent is a event or advisory sent by the server that has nats type hints\n// typically used for events that might be consumed by 3rd party event systems\ntype TypedEvent struct {\n\tType string    `json:\"type\"`\n\tID   string    `json:\"id\"`\n\tTime time.Time `json:\"timestamp\"`\n}\n\n// internalReceiveLoop will be responsible for dispatching all messages that\n// a server receives and needs to internally process, e.g. internal subs.\nfunc (s *Server) internalReceiveLoop(recvq *ipQueue[*inSysMsg]) {\n\tfor s.eventsRunning() {\n\t\tselect {\n\t\tcase <-recvq.ch:\n\t\t\tmsgs := recvq.pop()\n\t\t\tfor _, m := range msgs {\n\t\t\t\tif m.cb != nil {\n\t\t\t\t\tm.cb(m.sub, m.c, m.acc, m.subj, m.rply, m.hdr, m.msg)\n\t\t\t\t}\n\t\t\t}\n\t\t\trecvq.recycle(&msgs)\n\t\tcase <-s.quitCh:\n\t\t\treturn\n\t\t}\n\t}\n}\n\n// internalSendLoop will be responsible for serializing all messages that\n// a server wants to send.\nfunc (s *Server) internalSendLoop(wg *sync.WaitGroup) {\n\tdefer wg.Done()\n\nRESET:\n\ts.mu.RLock()\n\tif s.sys == nil || s.sys.sendq == nil {\n\t\ts.mu.RUnlock()\n\t\treturn\n\t}\n\tsysc := s.sys.client\n\tresetCh := s.sys.resetCh\n\tsendq := s.sys.sendq\n\tid := s.info.ID\n\thost := s.info.Host\n\tservername := s.info.Name\n\tdomain := s.info.Domain\n\tseqp := &s.sys.seq\n\tjs := s.info.JetStream\n\tcluster := s.info.Cluster\n\tif s.gateway.enabled {\n\t\tcluster = s.getGatewayName()\n\t}\n\ts.mu.RUnlock()\n\n\t// Grab tags.\n\ttags := s.getOpts().Tags\n\n\tfor s.eventsRunning() {\n\t\tselect {\n\t\tcase <-sendq.ch:\n\t\t\tmsgs := sendq.pop()\n\t\t\tfor _, pm := range msgs {\n\t\t\t\tif si := pm.si; si != nil {\n\t\t\t\t\tsi.Name = servername\n\t\t\t\t\tsi.Domain = domain\n\t\t\t\t\tsi.Host = host\n\t\t\t\t\tsi.Cluster = cluster\n\t\t\t\t\tsi.ID = id\n\t\t\t\t\tsi.Seq = atomic.AddUint64(seqp, 1)\n\t\t\t\t\tsi.Version = VERSION\n\t\t\t\t\tsi.Time = time.Now().UTC()\n\t\t\t\t\tsi.Tags = tags\n\t\t\t\t\tsi.Flags = 0\n\t\t\t\t\tif js {\n\t\t\t\t\t\t// New capability based flags.\n\t\t\t\t\t\tsi.SetJetStreamEnabled()\n\t\t\t\t\t\tsi.SetBinaryStreamSnapshot()\n\t\t\t\t\t\tif s.accountNRGAllowed.Load() {\n\t\t\t\t\t\t\tsi.SetAccountNRG()\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tvar b []byte\n\t\t\t\tif pm.msg != nil {\n\t\t\t\t\tswitch v := pm.msg.(type) {\n\t\t\t\t\tcase string:\n\t\t\t\t\t\tb = []byte(v)\n\t\t\t\t\tcase []byte:\n\t\t\t\t\t\tb = v\n\t\t\t\t\tdefault:\n\t\t\t\t\t\tb, _ = json.Marshal(pm.msg)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\t// Setup our client. If the user wants to use a non-system account use our internal\n\t\t\t\t// account scoped here so that we are not changing out accounts for the system client.\n\t\t\t\tvar c *client\n\t\t\t\tif pm.c != nil {\n\t\t\t\t\tc = pm.c\n\t\t\t\t} else {\n\t\t\t\t\tc = sysc\n\t\t\t\t}\n\n\t\t\t\t// Grab client lock.\n\t\t\t\tc.mu.Lock()\n\n\t\t\t\t// Prep internal structures needed to send message.\n\t\t\t\tc.pa.subject, c.pa.reply = []byte(pm.sub), []byte(pm.rply)\n\t\t\t\tc.pa.size, c.pa.szb = len(b), []byte(strconv.FormatInt(int64(len(b)), 10))\n\t\t\t\tc.pa.hdr, c.pa.hdb = -1, nil\n\t\t\t\ttrace := c.trace\n\n\t\t\t\t// Now check for optional compression.\n\t\t\t\tvar contentHeader string\n\t\t\t\tvar bb bytes.Buffer\n\n\t\t\t\tif len(b) > 0 {\n\t\t\t\t\tswitch pm.oct {\n\t\t\t\t\tcase gzipCompression:\n\t\t\t\t\t\tzw := gzip.NewWriter(&bb)\n\t\t\t\t\t\tzw.Write(b)\n\t\t\t\t\t\tzw.Close()\n\t\t\t\t\t\tb = bb.Bytes()\n\t\t\t\t\t\tcontentHeader = \"gzip\"\n\t\t\t\t\tcase snappyCompression:\n\t\t\t\t\t\tsw := s2.NewWriter(&bb, s2.WriterSnappyCompat())\n\t\t\t\t\t\tsw.Write(b)\n\t\t\t\t\t\tsw.Close()\n\t\t\t\t\t\tb = bb.Bytes()\n\t\t\t\t\t\tcontentHeader = \"snappy\"\n\t\t\t\t\tcase unsupportedCompression:\n\t\t\t\t\t\tcontentHeader = \"identity\"\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\t// Optional Echo\n\t\t\t\treplaceEcho := c.echo != pm.echo\n\t\t\t\tif replaceEcho {\n\t\t\t\t\tc.echo = !c.echo\n\t\t\t\t}\n\t\t\t\tc.mu.Unlock()\n\n\t\t\t\t// Add in NL\n\t\t\t\tb = append(b, _CRLF_...)\n\n\t\t\t\t// Check if we should set content-encoding\n\t\t\t\tif contentHeader != _EMPTY_ {\n\t\t\t\t\tb = c.setHeader(contentEncodingHeader, contentHeader, b)\n\t\t\t\t}\n\n\t\t\t\t// Optional header processing.\n\t\t\t\tif pm.hdr != nil {\n\t\t\t\t\tfor k, v := range pm.hdr {\n\t\t\t\t\t\tb = c.setHeader(k, v, b)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\t// Tracing\n\t\t\t\tif trace {\n\t\t\t\t\tc.traceInOp(fmt.Sprintf(\"PUB %s %s %d\", c.pa.subject, c.pa.reply, c.pa.size), nil)\n\t\t\t\t\tc.traceMsg(b)\n\t\t\t\t}\n\n\t\t\t\t// Process like a normal inbound msg.\n\t\t\t\tc.processInboundClientMsg(b)\n\n\t\t\t\t// Put echo back if needed.\n\t\t\t\tif replaceEcho {\n\t\t\t\t\tc.mu.Lock()\n\t\t\t\t\tc.echo = !c.echo\n\t\t\t\t\tc.mu.Unlock()\n\t\t\t\t}\n\n\t\t\t\t// See if we are doing graceful shutdown.\n\t\t\t\tif !pm.last {\n\t\t\t\t\tc.flushClients(0) // Never spend time in place.\n\t\t\t\t} else {\n\t\t\t\t\t// For the Shutdown event, we need to send in place otherwise\n\t\t\t\t\t// there is a chance that the process will exit before the\n\t\t\t\t\t// writeLoop has a chance to send it.\n\t\t\t\t\tc.flushClients(time.Second)\n\t\t\t\t\tsendq.recycle(&msgs)\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t\tpm.returnToPool()\n\t\t\t}\n\t\t\tsendq.recycle(&msgs)\n\t\tcase <-resetCh:\n\t\t\tgoto RESET\n\t\tcase <-s.quitCh:\n\t\t\treturn\n\t\t}\n\t}\n}\n\n// Will send a shutdown message for lame-duck. Unlike sendShutdownEvent, this will\n// not close off the send queue or reply handler, as we may still have a workload\n// that needs migrating off.\n// Lock should be held.\nfunc (s *Server) sendLDMShutdownEventLocked() {\n\tif s.sys == nil || s.sys.sendq == nil {\n\t\treturn\n\t}\n\tsubj := fmt.Sprintf(lameDuckEventSubj, s.info.ID)\n\tsi := &ServerInfo{}\n\ts.sys.sendq.push(newPubMsg(nil, subj, _EMPTY_, si, nil, si, noCompression, false, true))\n}\n\n// Will send a shutdown message.\nfunc (s *Server) sendShutdownEvent() {\n\ts.mu.Lock()\n\tif s.sys == nil || s.sys.sendq == nil {\n\t\ts.mu.Unlock()\n\t\treturn\n\t}\n\tsubj := fmt.Sprintf(shutdownEventSubj, s.info.ID)\n\tsendq := s.sys.sendq\n\t// Stop any more messages from queueing up.\n\ts.sys.sendq = nil\n\t// Unhook all msgHandlers. Normal client cleanup will deal with subs, etc.\n\ts.sys.replies = nil\n\t// Send to the internal queue and mark as last.\n\tsi := &ServerInfo{}\n\tsendq.push(newPubMsg(nil, subj, _EMPTY_, si, nil, si, noCompression, false, true))\n\ts.mu.Unlock()\n}\n\n// Used to send an internal message to an arbitrary account.\nfunc (s *Server) sendInternalAccountMsg(a *Account, subject string, msg any) error {\n\treturn s.sendInternalAccountMsgWithReply(a, subject, _EMPTY_, nil, msg, false)\n}\n\n// Used to send an internal message with an optional reply to an arbitrary account.\nfunc (s *Server) sendInternalAccountMsgWithReply(a *Account, subject, reply string, hdr map[string]string, msg any, echo bool) error {\n\ts.mu.RLock()\n\tif s.sys == nil || s.sys.sendq == nil {\n\t\ts.mu.RUnlock()\n\t\tif s.isShuttingDown() {\n\t\t\t// Skip in case this was called at the end phase during shut down\n\t\t\t// to avoid too many entries in the logs.\n\t\t\treturn nil\n\t\t}\n\t\treturn ErrNoSysAccount\n\t}\n\tc := s.sys.client\n\t// Replace our client with the account's internal client.\n\tif a != nil {\n\t\ta.mu.Lock()\n\t\tc = a.internalClient()\n\t\ta.mu.Unlock()\n\t}\n\ts.sys.sendq.push(newPubMsg(c, subject, reply, nil, hdr, msg, noCompression, echo, false))\n\ts.mu.RUnlock()\n\treturn nil\n}\n\n// Send system style message to an account scope.\nfunc (s *Server) sendInternalAccountSysMsg(a *Account, subj string, si *ServerInfo, msg any, ct compressionType) {\n\ts.mu.RLock()\n\tif s.sys == nil || s.sys.sendq == nil || a == nil {\n\t\ts.mu.RUnlock()\n\t\treturn\n\t}\n\tsendq := s.sys.sendq\n\ts.mu.RUnlock()\n\n\ta.mu.Lock()\n\tc := a.internalClient()\n\ta.mu.Unlock()\n\n\tsendq.push(newPubMsg(c, subj, _EMPTY_, si, nil, msg, ct, false, false))\n}\n\n// This will queue up a message to be sent.\n// Lock should not be held.\nfunc (s *Server) sendInternalMsgLocked(subj, rply string, si *ServerInfo, msg any) {\n\ts.mu.RLock()\n\ts.sendInternalMsg(subj, rply, si, msg)\n\ts.mu.RUnlock()\n}\n\n// This will queue up a message to be sent.\n// Assumes lock is held on entry.\nfunc (s *Server) sendInternalMsg(subj, rply string, si *ServerInfo, msg any) {\n\tif s.sys == nil || s.sys.sendq == nil {\n\t\treturn\n\t}\n\ts.sys.sendq.push(newPubMsg(nil, subj, rply, si, nil, msg, noCompression, false, false))\n}\n\n// Will send an api response.\nfunc (s *Server) sendInternalResponse(subj string, response *ServerAPIResponse) {\n\ts.mu.RLock()\n\tif s.sys == nil || s.sys.sendq == nil {\n\t\ts.mu.RUnlock()\n\t\treturn\n\t}\n\ts.sys.sendq.push(newPubMsg(nil, subj, _EMPTY_, response.Server, nil, response, response.compress, false, false))\n\ts.mu.RUnlock()\n}\n\n// Used to send internal messages from other system clients to avoid no echo issues.\nfunc (c *client) sendInternalMsg(subj, rply string, si *ServerInfo, msg any) {\n\tif c == nil {\n\t\treturn\n\t}\n\ts := c.srv\n\tif s == nil {\n\t\treturn\n\t}\n\ts.mu.RLock()\n\tif s.sys == nil || s.sys.sendq == nil {\n\t\ts.mu.RUnlock()\n\t\treturn\n\t}\n\ts.sys.sendq.push(newPubMsg(c, subj, rply, si, nil, msg, noCompression, false, false))\n\ts.mu.RUnlock()\n}\n\n// Locked version of checking if events system running. Also checks server.\nfunc (s *Server) eventsRunning() bool {\n\tif s == nil {\n\t\treturn false\n\t}\n\ts.mu.RLock()\n\ter := s.isRunning() && s.eventsEnabled()\n\ts.mu.RUnlock()\n\treturn er\n}\n\n// EventsEnabled will report if the server has internal events enabled via\n// a defined system account.\nfunc (s *Server) EventsEnabled() bool {\n\ts.mu.RLock()\n\tdefer s.mu.RUnlock()\n\treturn s.eventsEnabled()\n}\n\n// eventsEnabled will report if events are enabled.\n// Lock should be held.\nfunc (s *Server) eventsEnabled() bool {\n\treturn s.sys != nil && s.sys.client != nil && s.sys.account != nil\n}\n\n// TrackedRemoteServers returns how many remote servers we are tracking\n// from a system events perspective.\nfunc (s *Server) TrackedRemoteServers() int {\n\ts.mu.RLock()\n\tdefer s.mu.RUnlock()\n\tif !s.isRunning() || !s.eventsEnabled() {\n\t\treturn -1\n\t}\n\treturn len(s.sys.servers)\n}\n\n// Check for orphan servers who may have gone away without notification.\n// This should be wrapChk() to setup common locking.\nfunc (s *Server) checkRemoteServers() {\n\tnow := time.Now()\n\tfor sid, su := range s.sys.servers {\n\t\tif now.Sub(su.ltime) > s.sys.orphMax {\n\t\t\ts.Debugf(\"Detected orphan remote server: %q\", sid)\n\t\t\t// Simulate it going away.\n\t\t\ts.processRemoteServerShutdown(sid)\n\t\t}\n\t}\n\tif s.sys.sweeper != nil {\n\t\ts.sys.sweeper.Reset(s.sys.chkOrph)\n\t}\n}\n\n// Grab RSS and PCPU\n// Server lock will be held but released.\nfunc (s *Server) updateServerUsage(v *ServerStats) {\n\tvar vss int64\n\tpse.ProcUsage(&v.CPU, &v.Mem, &vss)\n\tv.Cores = runtime.NumCPU()\n\tv.MaxProcs = runtime.GOMAXPROCS(-1)\n\tif mm := debug.SetMemoryLimit(-1); mm < math.MaxInt64 {\n\t\tv.MemLimit = mm\n\t}\n}\n\n// Generate a route stat for our statz update.\nfunc routeStat(r *client) *RouteStat {\n\tif r == nil {\n\t\treturn nil\n\t}\n\tr.mu.Lock()\n\t// Note: *client.out[Msgs|Bytes] are not set using atomics,\n\t// unlike in[Msgs|Bytes].\n\trs := &RouteStat{\n\t\tID: r.cid,\n\t\tSent: DataStats{\n\t\t\tdataStats: dataStats{\n\t\t\t\tMsgs:  r.outMsgs,\n\t\t\t\tBytes: r.outBytes,\n\t\t\t},\n\t\t},\n\t\tReceived: DataStats{\n\t\t\tdataStats: dataStats{\n\t\t\t\tMsgs:  atomic.LoadInt64(&r.inMsgs),\n\t\t\t\tBytes: atomic.LoadInt64(&r.inBytes),\n\t\t\t},\n\t\t},\n\t\tPending: int(r.out.pb),\n\t}\n\tif r.route != nil {\n\t\trs.Name = r.route.remoteName\n\t}\n\tr.mu.Unlock()\n\treturn rs\n}\n\n// Actual send method for statz updates.\n// Lock should be held.\nfunc (s *Server) sendStatsz(subj string) {\n\tvar m ServerStatsMsg\n\ts.updateServerUsage(&m.Stats)\n\n\tif s.limitStatsz(subj) {\n\t\treturn\n\t}\n\n\ts.mu.RLock()\n\tdefer s.mu.RUnlock()\n\n\t// Check that we have a system account, etc.\n\tif s.sys == nil || s.sys.account == nil {\n\t\treturn\n\t}\n\n\tshouldCheckInterest := func() bool {\n\t\topts := s.getOpts()\n\t\tif opts.Cluster.Port != 0 || opts.Gateway.Port != 0 || opts.LeafNode.Port != 0 {\n\t\t\treturn false\n\t\t}\n\t\t// If we are here we have no clustering or gateways and are not a leafnode hub.\n\t\t// Check for leafnode remotes that connect the system account.\n\t\tif len(opts.LeafNode.Remotes) > 0 {\n\t\t\tsysAcc := s.sys.account.GetName()\n\t\t\tfor _, r := range opts.LeafNode.Remotes {\n\t\t\t\tif r.LocalAccount == sysAcc {\n\t\t\t\t\treturn false\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\treturn true\n\t}\n\n\t// if we are running standalone, check for interest.\n\tif shouldCheckInterest() {\n\t\t// Check if we even have interest in this subject.\n\t\tsacc := s.sys.account\n\t\trr := sacc.sl.Match(subj)\n\t\ttotalSubs := len(rr.psubs) + len(rr.qsubs)\n\t\tif totalSubs == 0 {\n\t\t\treturn\n\t\t} else if totalSubs == 1 && len(rr.psubs) == 1 {\n\t\t\t// For the broadcast subject we listen to that ourselves with no echo for remote updates.\n\t\t\t// If we are the only ones listening do not send either.\n\t\t\tif rr.psubs[0] == s.sys.remoteStatsSub {\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}\n\n\tm.Stats.Start = s.start\n\tm.Stats.Connections = len(s.clients)\n\tm.Stats.TotalConnections = s.totalClients\n\tm.Stats.ActiveAccounts = int(atomic.LoadInt32(&s.activeAccounts))\n\tm.Stats.Received.Msgs = atomic.LoadInt64(&s.inMsgs)\n\tm.Stats.Received.Bytes = atomic.LoadInt64(&s.inBytes)\n\tm.Stats.Sent.Msgs = atomic.LoadInt64(&s.outMsgs)\n\tm.Stats.Sent.Bytes = atomic.LoadInt64(&s.outBytes)\n\tm.Stats.SlowConsumers = atomic.LoadInt64(&s.slowConsumers)\n\t// Evaluate the slow consumer stats, but set it only if one of the value is not 0.\n\tscs := &SlowConsumersStats{\n\t\tClients:  s.NumSlowConsumersClients(),\n\t\tRoutes:   s.NumSlowConsumersRoutes(),\n\t\tGateways: s.NumSlowConsumersGateways(),\n\t\tLeafs:    s.NumSlowConsumersLeafs(),\n\t}\n\tif scs.Clients != 0 || scs.Routes != 0 || scs.Gateways != 0 || scs.Leafs != 0 {\n\t\tm.Stats.SlowConsumersStats = scs\n\t}\n\tm.Stats.NumSubs = s.numSubscriptions()\n\t// Routes\n\ts.forEachRoute(func(r *client) {\n\t\tm.Stats.Routes = append(m.Stats.Routes, routeStat(r))\n\t})\n\t// Gateways\n\tif s.gateway.enabled {\n\t\tgw := s.gateway\n\t\tgw.RLock()\n\t\tfor name, c := range gw.out {\n\t\t\tgs := &GatewayStat{Name: name}\n\t\t\tc.mu.Lock()\n\t\t\tgs.ID = c.cid\n\t\t\t// Note that *client.out[Msgs|Bytes] are not set using atomic,\n\t\t\t// unlike the in[Msgs|bytes].\n\t\t\tgs.Sent = DataStats{\n\t\t\t\tdataStats: dataStats{\n\t\t\t\t\tMsgs:  c.outMsgs,\n\t\t\t\t\tBytes: c.outBytes,\n\t\t\t\t},\n\t\t\t}\n\t\t\tc.mu.Unlock()\n\t\t\t// Gather matching inbound connections\n\t\t\tgs.Received = DataStats{}\n\t\t\tfor _, c := range gw.in {\n\t\t\t\tc.mu.Lock()\n\t\t\t\tif c.gw.name == name {\n\t\t\t\t\tgs.Received.Msgs += atomic.LoadInt64(&c.inMsgs)\n\t\t\t\t\tgs.Received.Bytes += atomic.LoadInt64(&c.inBytes)\n\t\t\t\t\tgs.NumInbound++\n\t\t\t\t}\n\t\t\t\tc.mu.Unlock()\n\t\t\t}\n\t\t\tm.Stats.Gateways = append(m.Stats.Gateways, gs)\n\t\t}\n\t\tgw.RUnlock()\n\t}\n\t// Active Servers\n\tm.Stats.ActiveServers = len(s.sys.servers) + 1\n\n\t// JetStream\n\tif js := s.js.Load(); js != nil {\n\t\tjStat := &JetStreamVarz{}\n\t\ts.mu.RUnlock()\n\t\tjs.mu.RLock()\n\t\tc := js.config\n\t\tc.StoreDir = _EMPTY_\n\t\tjStat.Config = &c\n\t\tjs.mu.RUnlock()\n\t\tjStat.Stats = js.usageStats()\n\t\t// Update our own usage since we do not echo so we will not hear ourselves.\n\t\tourNode := getHash(s.serverName())\n\t\tif v, ok := s.nodeToInfo.Load(ourNode); ok && v != nil {\n\t\t\tni := v.(nodeInfo)\n\t\t\tni.stats = jStat.Stats\n\t\t\tni.cfg = jStat.Config\n\t\t\ts.optsMu.RLock()\n\t\t\tni.tags = copyStrings(s.opts.Tags)\n\t\t\ts.optsMu.RUnlock()\n\t\t\ts.nodeToInfo.Store(ourNode, ni)\n\t\t}\n\t\t// Metagroup info.\n\t\tif mg := js.getMetaGroup(); mg != nil {\n\t\t\tif mg.Leader() {\n\t\t\t\tif ci := s.raftNodeToClusterInfo(mg); ci != nil {\n\t\t\t\t\tjStat.Meta = &MetaClusterInfo{\n\t\t\t\t\t\tName:     ci.Name,\n\t\t\t\t\t\tLeader:   ci.Leader,\n\t\t\t\t\t\tPeer:     getHash(ci.Leader),\n\t\t\t\t\t\tReplicas: ci.Replicas,\n\t\t\t\t\t\tSize:     mg.ClusterSize(),\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\t// non leader only include a shortened version without peers\n\t\t\t\tleader := s.serverNameForNode(mg.GroupLeader())\n\t\t\t\tjStat.Meta = &MetaClusterInfo{\n\t\t\t\t\tName:   mg.Group(),\n\t\t\t\t\tLeader: leader,\n\t\t\t\t\tPeer:   getHash(leader),\n\t\t\t\t\tSize:   mg.ClusterSize(),\n\t\t\t\t}\n\t\t\t}\n\t\t\tif ipq := s.jsAPIRoutedReqs; ipq != nil && jStat.Meta != nil {\n\t\t\t\tjStat.Meta.Pending = ipq.len()\n\t\t\t}\n\t\t}\n\t\tjStat.Limits = &s.getOpts().JetStreamLimits\n\t\tm.Stats.JetStream = jStat\n\t\ts.mu.RLock()\n\t}\n\t// Send message.\n\ts.sendInternalMsg(subj, _EMPTY_, &m.Server, &m)\n}\n\n// Limit updates to the heartbeat interval, max one second by default.\nfunc (s *Server) limitStatsz(subj string) bool {\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\n\tif s.sys == nil {\n\t\treturn true\n\t}\n\n\t// Only limit the normal broadcast subject.\n\tif subj != fmt.Sprintf(serverStatsSubj, s.ID()) {\n\t\treturn false\n\t}\n\n\tinterval := statszRateLimit\n\tif s.sys.cstatsz < interval {\n\t\tinterval = s.sys.cstatsz\n\t}\n\tif time.Since(s.sys.lastStatsz) < interval {\n\t\t// Reschedule heartbeat for the next interval.\n\t\tif s.sys.stmr != nil {\n\t\t\ts.sys.stmr.Reset(time.Until(s.sys.lastStatsz.Add(interval)))\n\t\t}\n\t\treturn true\n\t}\n\ts.sys.lastStatsz = time.Now()\n\treturn false\n}\n\n// Send out our statz update.\n// This should be wrapChk() to setup common locking.\nfunc (s *Server) heartbeatStatsz() {\n\tif s.sys.stmr != nil {\n\t\t// Increase after startup to our max.\n\t\tif s.sys.cstatsz < s.sys.statsz {\n\t\t\ts.sys.cstatsz *= 2\n\t\t\tif s.sys.cstatsz > s.sys.statsz {\n\t\t\t\ts.sys.cstatsz = s.sys.statsz\n\t\t\t}\n\t\t}\n\t\ts.sys.stmr.Reset(s.sys.cstatsz)\n\t}\n\t// Do in separate Go routine.\n\tgo s.sendStatszUpdate()\n}\n\n// Reset statsz rate limit for the next broadcast.\n// This should be wrapChk() to setup common locking.\nfunc (s *Server) resetLastStatsz() {\n\ts.sys.lastStatsz = time.Time{}\n}\n\nfunc (s *Server) sendStatszUpdate() {\n\ts.sendStatsz(fmt.Sprintf(serverStatsSubj, s.ID()))\n}\n\n// This should be wrapChk() to setup common locking.\nfunc (s *Server) startStatszTimer() {\n\t// We will start by sending out more of these and trail off to the statsz being the max.\n\ts.sys.cstatsz = 250 * time.Millisecond\n\t// Send out the first one quickly, we will slowly back off.\n\ts.sys.stmr = time.AfterFunc(s.sys.cstatsz, s.wrapChk(s.heartbeatStatsz))\n}\n\n// Start a ticker that will fire periodically and check for orphaned servers.\n// This should be wrapChk() to setup common locking.\nfunc (s *Server) startRemoteServerSweepTimer() {\n\ts.sys.sweeper = time.AfterFunc(s.sys.chkOrph, s.wrapChk(s.checkRemoteServers))\n}\n\n// Length of our system hash used for server targeted messages.\nconst sysHashLen = 8\n\n// Computes a hash of 8 characters for the name.\nfunc getHash(name string) string {\n\treturn getHashSize(name, sysHashLen)\n}\n\n// Computes a hash for the given `name`. The result will be `size` characters long.\nfunc getHashSize(name string, size int) string {\n\tsha := sha256.New()\n\tsha.Write([]byte(name))\n\tb := sha.Sum(nil)\n\tfor i := 0; i < size; i++ {\n\t\tb[i] = digits[int(b[i]%base)]\n\t}\n\treturn string(b[:size])\n}\n\n// Returns the node name for this server which is a hash of the server name.\nfunc (s *Server) Node() string {\n\ts.mu.RLock()\n\tdefer s.mu.RUnlock()\n\tif s.sys != nil {\n\t\treturn s.sys.shash\n\t}\n\treturn _EMPTY_\n}\n\n// This will setup our system wide tracking subs.\n// For now we will setup one wildcard subscription to\n// monitor all accounts for changes in number of connections.\n// We can make this on a per account tracking basis if needed.\n// Tradeoff is subscription and interest graph events vs connect and\n// disconnect events, etc.\nfunc (s *Server) initEventTracking() {\n\t// Capture sys in case we are shutdown while setting up.\n\ts.mu.RLock()\n\tsys := s.sys\n\ts.mu.RUnlock()\n\n\tif sys == nil || sys.client == nil || sys.account == nil {\n\t\treturn\n\t}\n\t// Create a system hash which we use for other servers to target us specifically.\n\tsys.shash = getHash(s.info.Name)\n\n\t// This will be for all inbox responses.\n\tsubject := fmt.Sprintf(inboxRespSubj, sys.shash, \"*\")\n\tif _, err := s.sysSubscribe(subject, s.inboxReply); err != nil {\n\t\ts.Errorf(\"Error setting up internal tracking: %v\", err)\n\t\treturn\n\t}\n\tsys.inboxPre = subject\n\t// This is for remote updates for connection accounting.\n\tsubject = fmt.Sprintf(accConnsEventSubjOld, \"*\")\n\tif _, err := s.sysSubscribe(subject, s.noInlineCallback(s.remoteConnsUpdate)); err != nil {\n\t\ts.Errorf(\"Error setting up internal tracking for %s: %v\", subject, err)\n\t\treturn\n\t}\n\t// This will be for responses for account info that we send out.\n\tsubject = fmt.Sprintf(connsRespSubj, s.info.ID)\n\tif _, err := s.sysSubscribe(subject, s.noInlineCallback(s.remoteConnsUpdate)); err != nil {\n\t\ts.Errorf(\"Error setting up internal tracking: %v\", err)\n\t\treturn\n\t}\n\t// Listen for broad requests to respond with number of subscriptions for a given subject.\n\tif _, err := s.sysSubscribe(accNumSubsReqSubj, s.noInlineCallback(s.nsubsRequest)); err != nil {\n\t\ts.Errorf(\"Error setting up internal tracking: %v\", err)\n\t\treturn\n\t}\n\t// Listen for statsz from others.\n\tsubject = fmt.Sprintf(serverStatsSubj, \"*\")\n\tif sub, err := s.sysSubscribe(subject, s.noInlineCallback(s.remoteServerUpdate)); err != nil {\n\t\ts.Errorf(\"Error setting up internal tracking: %v\", err)\n\t\treturn\n\t} else {\n\t\t// Keep track of this one.\n\t\tsys.remoteStatsSub = sub\n\t}\n\n\t// Listen for all server shutdowns.\n\tsubject = fmt.Sprintf(shutdownEventSubj, \"*\")\n\tif _, err := s.sysSubscribe(subject, s.noInlineCallback(s.remoteServerShutdown)); err != nil {\n\t\ts.Errorf(\"Error setting up internal tracking: %v\", err)\n\t\treturn\n\t}\n\t// Listen for servers entering lame-duck mode.\n\t// NOTE: This currently is handled in the same way as a server shutdown, but has\n\t// a different subject in case we need to handle differently in future.\n\tsubject = fmt.Sprintf(lameDuckEventSubj, \"*\")\n\tif _, err := s.sysSubscribe(subject, s.noInlineCallback(s.remoteServerShutdown)); err != nil {\n\t\ts.Errorf(\"Error setting up internal tracking: %v\", err)\n\t\treturn\n\t}\n\t// Listen for account claims updates.\n\tsubscribeToUpdate := true\n\tif s.accResolver != nil {\n\t\tsubscribeToUpdate = !s.accResolver.IsTrackingUpdate()\n\t}\n\tif subscribeToUpdate {\n\t\tfor _, sub := range []string{accUpdateEventSubjOld, accUpdateEventSubjNew} {\n\t\t\tif _, err := s.sysSubscribe(fmt.Sprintf(sub, \"*\"), s.noInlineCallback(s.accountClaimUpdate)); err != nil {\n\t\t\t\ts.Errorf(\"Error setting up internal tracking: %v\", err)\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}\n\t// Listen for ping messages that will be sent to all servers for statsz.\n\t// This subscription is kept for backwards compatibility. Got replaced by ...PING.STATZ from below\n\tif _, err := s.sysSubscribe(serverStatsPingReqSubj, s.noInlineCallbackStatsz(s.statszReq)); err != nil {\n\t\ts.Errorf(\"Error setting up internal tracking: %v\", err)\n\t\treturn\n\t}\n\tmonSrvc := map[string]sysMsgHandler{\n\t\t\"IDZ\":    s.idzReq,\n\t\t\"STATSZ\": s.statszReq,\n\t\t\"VARZ\": func(sub *subscription, c *client, _ *Account, subject, reply string, hdr, msg []byte) {\n\t\t\toptz := &VarzEventOptions{}\n\t\t\ts.zReq(c, reply, hdr, msg, &optz.EventFilterOptions, optz, func() (any, error) { return s.Varz(&optz.VarzOptions) })\n\t\t},\n\t\t\"SUBSZ\": func(sub *subscription, c *client, _ *Account, subject, reply string, hdr, msg []byte) {\n\t\t\toptz := &SubszEventOptions{}\n\t\t\ts.zReq(c, reply, hdr, msg, &optz.EventFilterOptions, optz, func() (any, error) { return s.Subsz(&optz.SubszOptions) })\n\t\t},\n\t\t\"CONNZ\": func(sub *subscription, c *client, _ *Account, subject, reply string, hdr, msg []byte) {\n\t\t\toptz := &ConnzEventOptions{}\n\t\t\ts.zReq(c, reply, hdr, msg, &optz.EventFilterOptions, optz, func() (any, error) { return s.Connz(&optz.ConnzOptions) })\n\t\t},\n\t\t\"ROUTEZ\": func(sub *subscription, c *client, _ *Account, subject, reply string, hdr, msg []byte) {\n\t\t\toptz := &RoutezEventOptions{}\n\t\t\ts.zReq(c, reply, hdr, msg, &optz.EventFilterOptions, optz, func() (any, error) { return s.Routez(&optz.RoutezOptions) })\n\t\t},\n\t\t\"GATEWAYZ\": func(sub *subscription, c *client, _ *Account, subject, reply string, hdr, msg []byte) {\n\t\t\toptz := &GatewayzEventOptions{}\n\t\t\ts.zReq(c, reply, hdr, msg, &optz.EventFilterOptions, optz, func() (any, error) { return s.Gatewayz(&optz.GatewayzOptions) })\n\t\t},\n\t\t\"LEAFZ\": func(sub *subscription, c *client, _ *Account, subject, reply string, hdr, msg []byte) {\n\t\t\toptz := &LeafzEventOptions{}\n\t\t\ts.zReq(c, reply, hdr, msg, &optz.EventFilterOptions, optz, func() (any, error) { return s.Leafz(&optz.LeafzOptions) })\n\t\t},\n\t\t\"ACCOUNTZ\": func(sub *subscription, c *client, _ *Account, subject, reply string, hdr, msg []byte) {\n\t\t\toptz := &AccountzEventOptions{}\n\t\t\ts.zReq(c, reply, hdr, msg, &optz.EventFilterOptions, optz, func() (any, error) { return s.Accountz(&optz.AccountzOptions) })\n\t\t},\n\t\t\"JSZ\": func(sub *subscription, c *client, _ *Account, subject, reply string, hdr, msg []byte) {\n\t\t\toptz := &JszEventOptions{}\n\t\t\ts.zReq(c, reply, hdr, msg, &optz.EventFilterOptions, optz, func() (any, error) { return s.Jsz(&optz.JSzOptions) })\n\t\t},\n\t\t\"HEALTHZ\": func(sub *subscription, c *client, _ *Account, subject, reply string, hdr, msg []byte) {\n\t\t\toptz := &HealthzEventOptions{}\n\t\t\ts.zReq(c, reply, hdr, msg, &optz.EventFilterOptions, optz, func() (any, error) { return s.healthz(&optz.HealthzOptions), nil })\n\t\t},\n\t\t\"PROFILEZ\": nil, // Special case, see below\n\t\t\"EXPVARZ\": func(sub *subscription, c *client, _ *Account, subject, reply string, hdr, msg []byte) {\n\t\t\toptz := &ExpvarzEventOptions{}\n\t\t\ts.zReq(c, reply, hdr, msg, &optz.EventFilterOptions, optz, func() (any, error) { return s.expvarz(optz), nil })\n\t\t},\n\t\t\"IPQUEUESZ\": func(sub *subscription, c *client, _ *Account, subject, reply string, hdr, msg []byte) {\n\t\t\toptz := &IpqueueszEventOptions{}\n\t\t\ts.zReq(c, reply, hdr, msg, &optz.EventFilterOptions, optz, func() (any, error) { return s.Ipqueuesz(&optz.IpqueueszOptions), nil })\n\t\t},\n\t\t\"RAFTZ\": func(sub *subscription, c *client, _ *Account, subject, reply string, hdr, msg []byte) {\n\t\t\toptz := &RaftzEventOptions{}\n\t\t\ts.zReq(c, reply, hdr, msg, &optz.EventFilterOptions, optz, func() (any, error) { return s.Raftz(&optz.RaftzOptions), nil })\n\t\t},\n\t}\n\tprofilez := func(_ *subscription, c *client, _ *Account, _, rply string, rmsg []byte) {\n\t\thdr, msg := c.msgParts(rmsg)\n\t\t// Need to copy since we are passing those to the go routine below.\n\t\thdr, msg = copyBytes(hdr), copyBytes(msg)\n\t\t// Execute in its own go routine because CPU profiling, for instance,\n\t\t// could take several seconds to complete.\n\t\tgo func() {\n\t\t\toptz := &ProfilezEventOptions{}\n\t\t\ts.zReq(c, rply, hdr, msg, &optz.EventFilterOptions, optz, func() (any, error) {\n\t\t\t\treturn s.profilez(&optz.ProfilezOptions), nil\n\t\t\t})\n\t\t}()\n\t}\n\tfor name, req := range monSrvc {\n\t\tvar h msgHandler\n\t\tswitch name {\n\t\tcase \"PROFILEZ\":\n\t\t\th = profilez\n\t\tcase \"STATSZ\":\n\t\t\th = s.noInlineCallbackStatsz(req)\n\t\tdefault:\n\t\t\th = s.noInlineCallback(req)\n\t\t}\n\t\tsubject = fmt.Sprintf(serverDirectReqSubj, s.info.ID, name)\n\t\tif _, err := s.sysSubscribe(subject, h); err != nil {\n\t\t\ts.Errorf(\"Error setting up internal tracking: %v\", err)\n\t\t\treturn\n\t\t}\n\t\tsubject = fmt.Sprintf(serverPingReqSubj, name)\n\t\tif _, err := s.sysSubscribe(subject, h); err != nil {\n\t\t\ts.Errorf(\"Error setting up internal tracking: %v\", err)\n\t\t\treturn\n\t\t}\n\t}\n\textractAccount := func(subject string) (string, error) {\n\t\tif tk := strings.Split(subject, tsep); len(tk) != accReqTokens {\n\t\t\treturn _EMPTY_, fmt.Errorf(\"subject %q is malformed\", subject)\n\t\t} else {\n\t\t\treturn tk[accReqAccIndex], nil\n\t\t}\n\t}\n\tmonAccSrvc := map[string]sysMsgHandler{\n\t\t\"SUBSZ\": func(sub *subscription, c *client, _ *Account, subject, reply string, hdr, msg []byte) {\n\t\t\toptz := &SubszEventOptions{}\n\t\t\ts.zReq(c, reply, hdr, msg, &optz.EventFilterOptions, optz, func() (any, error) {\n\t\t\t\tif acc, err := extractAccount(subject); err != nil {\n\t\t\t\t\treturn nil, err\n\t\t\t\t} else {\n\t\t\t\t\toptz.SubszOptions.Subscriptions = true\n\t\t\t\t\toptz.SubszOptions.Account = acc\n\t\t\t\t\treturn s.Subsz(&optz.SubszOptions)\n\t\t\t\t}\n\t\t\t})\n\t\t},\n\t\t\"CONNZ\": func(sub *subscription, c *client, _ *Account, subject, reply string, hdr, msg []byte) {\n\t\t\toptz := &ConnzEventOptions{}\n\t\t\ts.zReq(c, reply, hdr, msg, &optz.EventFilterOptions, optz, func() (any, error) {\n\t\t\t\tif acc, err := extractAccount(subject); err != nil {\n\t\t\t\t\treturn nil, err\n\t\t\t\t} else {\n\t\t\t\t\toptz.ConnzOptions.Account = acc\n\t\t\t\t\treturn s.Connz(&optz.ConnzOptions)\n\t\t\t\t}\n\t\t\t})\n\t\t},\n\t\t\"LEAFZ\": func(sub *subscription, c *client, _ *Account, subject, reply string, hdr, msg []byte) {\n\t\t\toptz := &LeafzEventOptions{}\n\t\t\ts.zReq(c, reply, hdr, msg, &optz.EventFilterOptions, optz, func() (any, error) {\n\t\t\t\tif acc, err := extractAccount(subject); err != nil {\n\t\t\t\t\treturn nil, err\n\t\t\t\t} else {\n\t\t\t\t\toptz.LeafzOptions.Account = acc\n\t\t\t\t\treturn s.Leafz(&optz.LeafzOptions)\n\t\t\t\t}\n\t\t\t})\n\t\t},\n\t\t\"JSZ\": func(sub *subscription, c *client, _ *Account, subject, reply string, hdr, msg []byte) {\n\t\t\toptz := &JszEventOptions{}\n\t\t\ts.zReq(c, reply, hdr, msg, &optz.EventFilterOptions, optz, func() (any, error) {\n\t\t\t\tif acc, err := extractAccount(subject); err != nil {\n\t\t\t\t\treturn nil, err\n\t\t\t\t} else {\n\t\t\t\t\toptz.Account = acc\n\t\t\t\t\treturn s.JszAccount(&optz.JSzOptions)\n\t\t\t\t}\n\t\t\t})\n\t\t},\n\t\t\"INFO\": func(sub *subscription, c *client, _ *Account, subject, reply string, hdr, msg []byte) {\n\t\t\toptz := &AccInfoEventOptions{}\n\t\t\ts.zReq(c, reply, hdr, msg, &optz.EventFilterOptions, optz, func() (any, error) {\n\t\t\t\tif acc, err := extractAccount(subject); err != nil {\n\t\t\t\t\treturn nil, err\n\t\t\t\t} else {\n\t\t\t\t\treturn s.accountInfo(acc)\n\t\t\t\t}\n\t\t\t})\n\t\t},\n\t\t// STATZ is essentially a duplicate of CONNS with an envelope identical to the others.\n\t\t// For historical reasons CONNS is the odd one out.\n\t\t// STATZ is also less heavy weight than INFO\n\t\t\"STATZ\": func(sub *subscription, c *client, _ *Account, subject, reply string, hdr, msg []byte) {\n\t\t\toptz := &AccountStatzEventOptions{}\n\t\t\ts.zReq(c, reply, hdr, msg, &optz.EventFilterOptions, optz, func() (any, error) {\n\t\t\t\tif acc, err := extractAccount(subject); err != nil {\n\t\t\t\t\treturn nil, err\n\t\t\t\t} else if acc == \"PING\" { // Filter PING subject. Happens for server as well. But wildcards are not used\n\t\t\t\t\treturn nil, errSkipZreq\n\t\t\t\t} else {\n\t\t\t\t\toptz.Accounts = []string{acc}\n\t\t\t\t\tif stz, err := s.AccountStatz(&optz.AccountStatzOptions); err != nil {\n\t\t\t\t\t\treturn nil, err\n\t\t\t\t\t} else if len(stz.Accounts) == 0 && !optz.IncludeUnused {\n\t\t\t\t\t\treturn nil, errSkipZreq\n\t\t\t\t\t} else {\n\t\t\t\t\t\treturn stz, nil\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t})\n\t\t},\n\t\t\"CONNS\": s.connsRequest,\n\t}\n\tfor name, req := range monAccSrvc {\n\t\tif _, err := s.sysSubscribe(fmt.Sprintf(accDirectReqSubj, \"*\", name), s.noInlineCallback(req)); err != nil {\n\t\t\ts.Errorf(\"Error setting up internal tracking: %v\", err)\n\t\t\treturn\n\t\t}\n\t}\n\n\t// User info.\n\t// TODO(dlc) - Can be internal and not forwarded since bound server for the client connection\n\t// is only one that will answer. This breaks tests since we still forward on remote server connect.\n\tif _, err := s.sysSubscribe(fmt.Sprintf(userDirectReqSubj, \"*\"), s.userInfoReq); err != nil {\n\t\ts.Errorf(\"Error setting up internal tracking: %v\", err)\n\t\treturn\n\t}\n\n\t// For now only the STATZ subject has an account specific ping equivalent.\n\tif _, err := s.sysSubscribe(fmt.Sprintf(accPingReqSubj, \"STATZ\"),\n\t\ts.noInlineCallback(func(sub *subscription, c *client, _ *Account, subject, reply string, hdr, msg []byte) {\n\t\t\toptz := &AccountStatzEventOptions{}\n\t\t\ts.zReq(c, reply, hdr, msg, &optz.EventFilterOptions, optz, func() (any, error) {\n\t\t\t\tif stz, err := s.AccountStatz(&optz.AccountStatzOptions); err != nil {\n\t\t\t\t\treturn nil, err\n\t\t\t\t} else if len(stz.Accounts) == 0 && !optz.IncludeUnused {\n\t\t\t\t\treturn nil, errSkipZreq\n\t\t\t\t} else {\n\t\t\t\t\treturn stz, nil\n\t\t\t\t}\n\t\t\t})\n\t\t})); err != nil {\n\t\ts.Errorf(\"Error setting up internal tracking: %v\", err)\n\t\treturn\n\t}\n\n\t// Listen for updates when leaf nodes connect for a given account. This will\n\t// force any gateway connections to move to `modeInterestOnly`\n\tsubject = fmt.Sprintf(leafNodeConnectEventSubj, \"*\")\n\tif _, err := s.sysSubscribe(subject, s.noInlineCallback(s.leafNodeConnected)); err != nil {\n\t\ts.Errorf(\"Error setting up internal tracking: %v\", err)\n\t\treturn\n\t}\n\t// For tracking remote latency measurements.\n\tsubject = fmt.Sprintf(remoteLatencyEventSubj, sys.shash)\n\tif _, err := s.sysSubscribe(subject, s.noInlineCallback(s.remoteLatencyUpdate)); err != nil {\n\t\ts.Errorf(\"Error setting up internal latency tracking: %v\", err)\n\t\treturn\n\t}\n\t// This is for simple debugging of number of subscribers that exist in the system.\n\tif _, err := s.sysSubscribeInternal(accSubsSubj, s.noInlineCallback(s.debugSubscribers)); err != nil {\n\t\ts.Errorf(\"Error setting up internal debug service for subscribers: %v\", err)\n\t\treturn\n\t}\n\n\t// Listen for requests to reload the server configuration.\n\tsubject = fmt.Sprintf(serverReloadReqSubj, s.info.ID)\n\tif _, err := s.sysSubscribe(subject, s.noInlineCallback(s.reloadConfig)); err != nil {\n\t\ts.Errorf(\"Error setting up server reload handler: %v\", err)\n\t\treturn\n\t}\n\n\t// Client connection kick\n\tsubject = fmt.Sprintf(clientKickReqSubj, s.info.ID)\n\tif _, err := s.sysSubscribe(subject, s.noInlineCallback(s.kickClient)); err != nil {\n\t\ts.Errorf(\"Error setting up client kick service: %v\", err)\n\t\treturn\n\t}\n\t// Client connection LDM\n\tsubject = fmt.Sprintf(clientLDMReqSubj, s.info.ID)\n\tif _, err := s.sysSubscribe(subject, s.noInlineCallback(s.ldmClient)); err != nil {\n\t\ts.Errorf(\"Error setting up client LDM service: %v\", err)\n\t\treturn\n\t}\n}\n\n// UserInfo returns basic information to a user about bound account and user permissions.\n// For account information they will need to ping that separately, and this allows security\n// controls on each subsystem if desired, e.g. account info, jetstream account info, etc.\ntype UserInfo struct {\n\tUserID      string        `json:\"user\"`\n\tAccount     string        `json:\"account\"`\n\tPermissions *Permissions  `json:\"permissions,omitempty\"`\n\tExpires     time.Duration `json:\"expires,omitempty\"`\n}\n\n// Process a user info request.\nfunc (s *Server) userInfoReq(sub *subscription, c *client, _ *Account, subject, reply string, msg []byte) {\n\tif !s.EventsEnabled() || reply == _EMPTY_ {\n\t\treturn\n\t}\n\n\tresponse := &ServerAPIResponse{Server: &ServerInfo{}}\n\n\tci, _, _, _, err := s.getRequestInfo(c, msg)\n\tif err != nil {\n\t\tresponse.Error = &ApiError{Code: http.StatusBadRequest}\n\t\ts.sendInternalResponse(reply, response)\n\t\treturn\n\t}\n\n\tresponse.Data = &UserInfo{\n\t\tUserID:      ci.User,\n\t\tAccount:     ci.Account,\n\t\tPermissions: c.publicPermissions(),\n\t\tExpires:     c.claimExpiration(),\n\t}\n\ts.sendInternalResponse(reply, response)\n}\n\n// register existing accounts with any system exports.\nfunc (s *Server) registerSystemImportsForExisting() {\n\tvar accounts []*Account\n\n\ts.mu.RLock()\n\tif s.sys == nil {\n\t\ts.mu.RUnlock()\n\t\treturn\n\t}\n\tsacc := s.sys.account\n\ts.accounts.Range(func(k, v any) bool {\n\t\ta := v.(*Account)\n\t\tif a != sacc {\n\t\t\taccounts = append(accounts, a)\n\t\t}\n\t\treturn true\n\t})\n\ts.mu.RUnlock()\n\n\tfor _, a := range accounts {\n\t\ts.registerSystemImports(a)\n\t}\n}\n\n// add all exports a system account will need\nfunc (s *Server) addSystemAccountExports(sacc *Account) {\n\tif !s.EventsEnabled() {\n\t\treturn\n\t}\n\taccConnzSubj := fmt.Sprintf(accDirectReqSubj, \"*\", \"CONNZ\")\n\t// prioritize not automatically added exports\n\tif !sacc.hasServiceExportMatching(accConnzSubj) {\n\t\t// pick export type that clamps importing account id into subject\n\t\tif err := sacc.addServiceExportWithResponseAndAccountPos(accConnzSubj, Streamed, nil, 4); err != nil {\n\t\t\t//if err := sacc.AddServiceExportWithResponse(accConnzSubj, Streamed, nil); err != nil {\n\t\t\ts.Errorf(\"Error adding system service export for %q: %v\", accConnzSubj, err)\n\t\t}\n\t}\n\t// prioritize not automatically added exports\n\taccStatzSubj := fmt.Sprintf(accDirectReqSubj, \"*\", \"STATZ\")\n\tif !sacc.hasServiceExportMatching(accStatzSubj) {\n\t\t// pick export type that clamps importing account id into subject\n\t\tif err := sacc.addServiceExportWithResponseAndAccountPos(accStatzSubj, Streamed, nil, 4); err != nil {\n\t\t\ts.Errorf(\"Error adding system service export for %q: %v\", accStatzSubj, err)\n\t\t}\n\t}\n\t// FIXME(dlc) - Old experiment, Remove?\n\tif !sacc.hasServiceExportMatching(accSubsSubj) {\n\t\tif err := sacc.AddServiceExport(accSubsSubj, nil); err != nil {\n\t\t\ts.Errorf(\"Error adding system service export for %q: %v\", accSubsSubj, err)\n\t\t}\n\t}\n\n\t// User info export.\n\tuserInfoSubj := fmt.Sprintf(userDirectReqSubj, \"*\")\n\tif !sacc.hasServiceExportMatching(userInfoSubj) {\n\t\tif err := sacc.AddServiceExport(userInfoSubj, nil); err != nil {\n\t\t\ts.Errorf(\"Error adding system service export for %q: %v\", userInfoSubj, err)\n\t\t}\n\t\tmappedSubj := fmt.Sprintf(userDirectReqSubj, sacc.GetName())\n\t\tif err := sacc.AddServiceImport(sacc, userDirectInfoSubj, mappedSubj); err != nil {\n\t\t\ts.Errorf(\"Error setting up system service import %s: %v\", mappedSubj, err)\n\t\t}\n\t\t// Make sure to share details.\n\t\tsacc.setServiceImportSharing(sacc, mappedSubj, false, true)\n\t}\n\n\t// Register any accounts that existed prior.\n\ts.registerSystemImportsForExisting()\n\n\t// in case of a mixed mode setup, enable js exports anyway\n\tif s.JetStreamEnabled() || !s.standAloneMode() {\n\t\ts.checkJetStreamExports()\n\t}\n}\n\n// accountClaimUpdate will receive claim updates for accounts.\nfunc (s *Server) accountClaimUpdate(sub *subscription, c *client, _ *Account, subject, resp string, hdr, msg []byte) {\n\tif !s.EventsEnabled() {\n\t\treturn\n\t}\n\tvar pubKey string\n\ttoks := strings.Split(subject, tsep)\n\tif len(toks) == accUpdateTokensNew {\n\t\tpubKey = toks[accReqAccIndex]\n\t} else if len(toks) == accUpdateTokensOld {\n\t\tpubKey = toks[accUpdateAccIdxOld]\n\t} else {\n\t\ts.Debugf(\"Received account claims update on bad subject %q\", subject)\n\t\treturn\n\t}\n\tif len(msg) == 0 {\n\t\terr := errors.New(\"request body is empty\")\n\t\trespondToUpdate(s, resp, pubKey, \"jwt update error\", err)\n\t} else if claim, err := jwt.DecodeAccountClaims(string(msg)); err != nil {\n\t\trespondToUpdate(s, resp, pubKey, \"jwt update resulted in error\", err)\n\t} else if claim.Subject != pubKey {\n\t\terr := errors.New(\"subject does not match jwt content\")\n\t\trespondToUpdate(s, resp, pubKey, \"jwt update resulted in error\", err)\n\t} else if v, ok := s.accounts.Load(pubKey); !ok {\n\t\trespondToUpdate(s, resp, pubKey, \"jwt update skipped\", nil)\n\t} else if err := s.updateAccountWithClaimJWT(v.(*Account), string(msg)); err != nil {\n\t\trespondToUpdate(s, resp, pubKey, \"jwt update resulted in error\", err)\n\t} else {\n\t\trespondToUpdate(s, resp, pubKey, \"jwt updated\", nil)\n\t}\n}\n\n// processRemoteServerShutdown will update any affected accounts.\n// Will update the remote count for clients.\n// Lock assume held.\nfunc (s *Server) processRemoteServerShutdown(sid string) {\n\ts.accounts.Range(func(k, v any) bool {\n\t\tv.(*Account).removeRemoteServer(sid)\n\t\treturn true\n\t})\n\t// Update any state in nodeInfo.\n\ts.nodeToInfo.Range(func(k, v any) bool {\n\t\tni := v.(nodeInfo)\n\t\tif ni.id == sid {\n\t\t\tni.offline = true\n\t\t\ts.nodeToInfo.Store(k, ni)\n\t\t\treturn false\n\t\t}\n\t\treturn true\n\t})\n\tdelete(s.sys.servers, sid)\n}\n\nfunc (s *Server) sameDomain(domain string) bool {\n\treturn domain == _EMPTY_ || s.info.Domain == _EMPTY_ || domain == s.info.Domain\n}\n\n// remoteServerShutdown is called when we get an event from another server shutting down.\nfunc (s *Server) remoteServerShutdown(sub *subscription, c *client, _ *Account, subject, reply string, hdr, msg []byte) {\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\tif !s.eventsEnabled() {\n\t\treturn\n\t}\n\ttoks := strings.Split(subject, tsep)\n\tif len(toks) < shutdownEventTokens {\n\t\ts.Debugf(\"Received remote server shutdown on bad subject %q\", subject)\n\t\treturn\n\t}\n\n\tif len(msg) == 0 {\n\t\ts.Errorf(\"Remote server sent invalid (empty) shutdown message to %q\", subject)\n\t\treturn\n\t}\n\n\t// We have an optional serverInfo here, remove from nodeToX lookups.\n\tvar si ServerInfo\n\tif err := json.Unmarshal(msg, &si); err != nil {\n\t\ts.Debugf(\"Received bad server info for remote server shutdown\")\n\t\treturn\n\t}\n\n\t// JetStream node updates if applicable.\n\tnode := getHash(si.Name)\n\tif v, ok := s.nodeToInfo.Load(node); ok && v != nil {\n\t\tni := v.(nodeInfo)\n\t\tni.offline = true\n\t\ts.nodeToInfo.Store(node, ni)\n\t}\n\n\tsid := toks[serverSubjectIndex]\n\tif su := s.sys.servers[sid]; su != nil {\n\t\ts.processRemoteServerShutdown(sid)\n\t}\n}\n\n// remoteServerUpdate listens for statsz updates from other servers.\nfunc (s *Server) remoteServerUpdate(sub *subscription, c *client, _ *Account, subject, reply string, hdr, msg []byte) {\n\tvar ssm ServerStatsMsg\n\tif len(msg) == 0 {\n\t\ts.Debugf(\"Received empty server info for remote server update\")\n\t\treturn\n\t} else if err := json.Unmarshal(msg, &ssm); err != nil {\n\t\ts.Debugf(\"Received bad server info for remote server update\")\n\t\treturn\n\t}\n\tsi := ssm.Server\n\n\t// Should do normal updates before bailing if wrong domain.\n\ts.mu.Lock()\n\tif s.isRunning() && s.eventsEnabled() && ssm.Server.ID != s.info.ID {\n\t\ts.updateRemoteServer(&si)\n\t}\n\ts.mu.Unlock()\n\n\t// JetStream node updates.\n\tif !s.sameDomain(si.Domain) {\n\t\treturn\n\t}\n\n\tvar cfg *JetStreamConfig\n\tvar stats *JetStreamStats\n\n\tif ssm.Stats.JetStream != nil {\n\t\tcfg = ssm.Stats.JetStream.Config\n\t\tstats = ssm.Stats.JetStream.Stats\n\t}\n\n\tnode := getHash(si.Name)\n\taccountNRG := si.AccountNRG()\n\toldInfo, _ := s.nodeToInfo.Swap(node, nodeInfo{\n\t\tsi.Name,\n\t\tsi.Version,\n\t\tsi.Cluster,\n\t\tsi.Domain,\n\t\tsi.ID,\n\t\tsi.Tags,\n\t\tcfg,\n\t\tstats,\n\t\tfalse,\n\t\tsi.JetStreamEnabled(),\n\t\tsi.BinaryStreamSnapshot(),\n\t\taccountNRG,\n\t})\n\tif oldInfo == nil || accountNRG != oldInfo.(nodeInfo).accountNRG {\n\t\t// One of the servers we received statsz from changed its mind about\n\t\t// whether or not it supports in-account NRG, so update the groups\n\t\t// with this information.\n\t\ts.updateNRGAccountStatus()\n\t}\n}\n\n// updateRemoteServer is called when we have an update from a remote server.\n// This allows us to track remote servers, respond to shutdown messages properly,\n// make sure that messages are ordered, and allow us to prune dead servers.\n// Lock should be held upon entry.\nfunc (s *Server) updateRemoteServer(si *ServerInfo) {\n\tsu := s.sys.servers[si.ID]\n\tif su == nil {\n\t\ts.sys.servers[si.ID] = &serverUpdate{si.Seq, time.Now()}\n\t\ts.processNewServer(si)\n\t} else {\n\t\t// Should always be going up.\n\t\tif si.Seq <= su.seq {\n\t\t\ts.Errorf(\"Received out of order remote server update from: %q\", si.ID)\n\t\t\treturn\n\t\t}\n\t\tsu.seq = si.Seq\n\t\tsu.ltime = time.Now()\n\t}\n}\n\n// processNewServer will hold any logic we want to use when we discover a new server.\n// Lock should be held upon entry.\nfunc (s *Server) processNewServer(si *ServerInfo) {\n\t// Right now we only check if we have leafnode servers and if so send another\n\t// connect update to make sure they switch this account to interest only mode.\n\ts.ensureGWsInterestOnlyForLeafNodes()\n\n\t// Add to our nodeToName\n\tif s.sameDomain(si.Domain) {\n\t\tnode := getHash(si.Name)\n\t\t// Only update if non-existent\n\t\tif _, ok := s.nodeToInfo.Load(node); !ok {\n\t\t\ts.nodeToInfo.Store(node, nodeInfo{\n\t\t\t\tsi.Name,\n\t\t\t\tsi.Version,\n\t\t\t\tsi.Cluster,\n\t\t\t\tsi.Domain,\n\t\t\t\tsi.ID,\n\t\t\t\tsi.Tags,\n\t\t\t\tnil,\n\t\t\t\tnil,\n\t\t\t\tfalse,\n\t\t\t\tsi.JetStreamEnabled(),\n\t\t\t\tsi.BinaryStreamSnapshot(),\n\t\t\t\tsi.AccountNRG(),\n\t\t\t})\n\t\t}\n\t}\n\tgo s.updateNRGAccountStatus()\n\t// Announce ourselves..\n\t// Do this in a separate Go routine.\n\tgo s.sendStatszUpdate()\n}\n\n// Works out whether all nodes support moving the NRG traffic into\n// the account and moves it appropriately.\n// Server lock MUST NOT be held on entry.\nfunc (s *Server) updateNRGAccountStatus() {\n\ts.rnMu.RLock()\n\traftNodes := make([]RaftNode, 0, len(s.raftNodes))\n\tfor _, n := range s.raftNodes {\n\t\traftNodes = append(raftNodes, n)\n\t}\n\ts.rnMu.RUnlock()\n\tfor _, n := range raftNodes {\n\t\t// In the event that the node is happy that all nodes that\n\t\t// it cares about haven't changed, this will be a no-op.\n\t\tif err := n.RecreateInternalSubs(); err != nil {\n\t\t\tn.Stop()\n\t\t}\n\t}\n}\n\n// If GW is enabled on this server and there are any leaf node connections,\n// this function will send a LeafNode connect system event to the super cluster\n// to ensure that the GWs are in interest-only mode for this account.\n// Lock should be held upon entry.\n// TODO(dlc) - this will cause this account to be loaded on all servers. Need a better\n// way with GW2.\nfunc (s *Server) ensureGWsInterestOnlyForLeafNodes() {\n\tif !s.gateway.enabled || len(s.leafs) == 0 {\n\t\treturn\n\t}\n\tsent := make(map[*Account]bool, len(s.leafs))\n\tfor _, c := range s.leafs {\n\t\tif !sent[c.acc] {\n\t\t\ts.sendLeafNodeConnectMsg(c.acc.Name)\n\t\t\tsent[c.acc] = true\n\t\t}\n\t}\n}\n\n// shutdownEventing will clean up all eventing state.\nfunc (s *Server) shutdownEventing() {\n\tif !s.eventsRunning() {\n\t\treturn\n\t}\n\n\ts.mu.Lock()\n\tclearTimer(&s.sys.sweeper)\n\tclearTimer(&s.sys.stmr)\n\trc := s.sys.resetCh\n\ts.sys.resetCh = nil\n\twg := &s.sys.wg\n\ts.mu.Unlock()\n\n\t// We will queue up a shutdown event and wait for the\n\t// internal send loop to exit.\n\ts.sendShutdownEvent()\n\twg.Wait()\n\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\n\t// Whip through all accounts.\n\ts.accounts.Range(func(k, v any) bool {\n\t\tv.(*Account).clearEventing()\n\t\treturn true\n\t})\n\t// Turn everything off here.\n\ts.sys = nil\n\t// Make sure this is done after s.sys = nil, so that we don't\n\t// get sends to closed channels on badly-timed config reloads.\n\tclose(rc)\n}\n\n// Request for our local connection count.\nfunc (s *Server) connsRequest(sub *subscription, c *client, _ *Account, subject, reply string, hdr, msg []byte) {\n\tif !s.eventsRunning() {\n\t\treturn\n\t}\n\ttk := strings.Split(subject, tsep)\n\tif len(tk) != accReqTokens {\n\t\ts.sys.client.Errorf(\"Bad subject account connections request message\")\n\t\treturn\n\t}\n\ta := tk[accReqAccIndex]\n\tm := accNumConnsReq{Account: a}\n\tif len(msg) > 0 {\n\t\tif err := json.Unmarshal(msg, &m); err != nil {\n\t\t\ts.sys.client.Errorf(\"Error unmarshalling account connections request message: %v\", err)\n\t\t\treturn\n\t\t}\n\t}\n\tif m.Account != a {\n\t\ts.sys.client.Errorf(\"Error unmarshalled account does not match subject\")\n\t\treturn\n\t}\n\t// Here we really only want to lookup the account if its local. We do not want to fetch this\n\t// account if we have no interest in it.\n\tvar acc *Account\n\tif v, ok := s.accounts.Load(m.Account); ok {\n\t\tacc = v.(*Account)\n\t}\n\tif acc == nil {\n\t\treturn\n\t}\n\t// We know this is a local connection.\n\tif nlc := acc.NumLocalConnections(); nlc > 0 {\n\t\ts.mu.Lock()\n\t\ts.sendAccConnsUpdate(acc, reply)\n\t\ts.mu.Unlock()\n\t}\n}\n\n// leafNodeConnected is an event we will receive when a leaf node for a given account connects.\nfunc (s *Server) leafNodeConnected(sub *subscription, _ *client, _ *Account, subject, reply string, hdr, msg []byte) {\n\tm := accNumConnsReq{}\n\tif err := json.Unmarshal(msg, &m); err != nil {\n\t\ts.sys.client.Errorf(\"Error unmarshalling account connections request message: %v\", err)\n\t\treturn\n\t}\n\n\ts.mu.RLock()\n\tna := m.Account == _EMPTY_ || !s.eventsEnabled() || !s.gateway.enabled\n\ts.mu.RUnlock()\n\n\tif na {\n\t\treturn\n\t}\n\n\tif acc, _ := s.lookupAccount(m.Account); acc != nil {\n\t\ts.switchAccountToInterestMode(acc.Name)\n\t}\n}\n\n// Common filter options for system requests STATSZ VARZ SUBSZ CONNZ ROUTEZ GATEWAYZ LEAFZ\ntype EventFilterOptions struct {\n\tName    string   `json:\"server_name,omitempty\"` // filter by server name\n\tCluster string   `json:\"cluster,omitempty\"`     // filter by cluster name\n\tHost    string   `json:\"host,omitempty\"`        // filter by host name\n\tTags    []string `json:\"tags,omitempty\"`        // filter by tags (must match all tags)\n\tDomain  string   `json:\"domain,omitempty\"`      // filter by JS domain\n}\n\n// StatszEventOptions are options passed to Statsz\ntype StatszEventOptions struct {\n\t// No actual options yet\n\tEventFilterOptions\n}\n\n// Options for account Info\ntype AccInfoEventOptions struct {\n\t// No actual options yet\n\tEventFilterOptions\n}\n\n// In the context of system events, ConnzEventOptions are options passed to Connz\ntype ConnzEventOptions struct {\n\tConnzOptions\n\tEventFilterOptions\n}\n\n// In the context of system events, RoutezEventOptions are options passed to Routez\ntype RoutezEventOptions struct {\n\tRoutezOptions\n\tEventFilterOptions\n}\n\n// In the context of system events, SubzEventOptions are options passed to Subz\ntype SubszEventOptions struct {\n\tSubszOptions\n\tEventFilterOptions\n}\n\n// In the context of system events, VarzEventOptions are options passed to Varz\ntype VarzEventOptions struct {\n\tVarzOptions\n\tEventFilterOptions\n}\n\n// In the context of system events, GatewayzEventOptions are options passed to Gatewayz\ntype GatewayzEventOptions struct {\n\tGatewayzOptions\n\tEventFilterOptions\n}\n\n// In the context of system events, LeafzEventOptions are options passed to Leafz\ntype LeafzEventOptions struct {\n\tLeafzOptions\n\tEventFilterOptions\n}\n\n// In the context of system events, AccountzEventOptions are options passed to Accountz\ntype AccountzEventOptions struct {\n\tAccountzOptions\n\tEventFilterOptions\n}\n\n// In the context of system events, AccountzEventOptions are options passed to Accountz\ntype AccountStatzEventOptions struct {\n\tAccountStatzOptions\n\tEventFilterOptions\n}\n\n// In the context of system events, JszEventOptions are options passed to Jsz\ntype JszEventOptions struct {\n\tJSzOptions\n\tEventFilterOptions\n}\n\n// In the context of system events, HealthzEventOptions are options passed to Healthz\ntype HealthzEventOptions struct {\n\tHealthzOptions\n\tEventFilterOptions\n}\n\n// In the context of system events, ProfilezEventOptions are options passed to Profilez\ntype ProfilezEventOptions struct {\n\tProfilezOptions\n\tEventFilterOptions\n}\n\n// In the context of system events, ExpvarzEventOptions are options passed to Expvarz\ntype ExpvarzEventOptions struct {\n\tEventFilterOptions\n}\n\n// In the context of system events, IpqueueszEventOptions are options passed to Ipqueuesz\ntype IpqueueszEventOptions struct {\n\tEventFilterOptions\n\tIpqueueszOptions\n}\n\n// In the context of system events, RaftzEventOptions are options passed to Raftz\ntype RaftzEventOptions struct {\n\tEventFilterOptions\n\tRaftzOptions\n}\n\n// returns true if the request does NOT apply to this server and can be ignored.\n// DO NOT hold the server lock when\nfunc (s *Server) filterRequest(fOpts *EventFilterOptions) bool {\n\tif fOpts.Name != _EMPTY_ && !strings.Contains(s.info.Name, fOpts.Name) {\n\t\treturn true\n\t}\n\tif fOpts.Host != _EMPTY_ && !strings.Contains(s.info.Host, fOpts.Host) {\n\t\treturn true\n\t}\n\tif fOpts.Cluster != _EMPTY_ {\n\t\tif !strings.Contains(s.ClusterName(), fOpts.Cluster) {\n\t\t\treturn true\n\t\t}\n\t}\n\tif len(fOpts.Tags) > 0 {\n\t\topts := s.getOpts()\n\t\tfor _, t := range fOpts.Tags {\n\t\t\tif !opts.Tags.Contains(t) {\n\t\t\t\treturn true\n\t\t\t}\n\t\t}\n\t}\n\tif fOpts.Domain != _EMPTY_ && s.getOpts().JetStreamDomain != fOpts.Domain {\n\t\treturn true\n\t}\n\treturn false\n}\n\n// Encoding support (compression)\ntype compressionType int8\n\nconst (\n\tnoCompression = compressionType(iota)\n\tgzipCompression\n\tsnappyCompression\n\tunsupportedCompression\n)\n\n// ServerAPIResponse is the response type for the server API like varz, connz etc.\ntype ServerAPIResponse struct {\n\tServer *ServerInfo `json:\"server\"`\n\tData   any         `json:\"data,omitempty\"`\n\tError  *ApiError   `json:\"error,omitempty\"`\n\n\t// Private to indicate compression if any.\n\tcompress compressionType\n}\n\n// Specialized response types for unmarshalling. These structures are not\n// used in the server code and only there for users of the Z endpoints to\n// unmarshal the data without having to create these structs in their code\n\n// ServerAPIConnzResponse is the response type connz\ntype ServerAPIConnzResponse struct {\n\tServer *ServerInfo `json:\"server\"`\n\tData   *Connz      `json:\"data,omitempty\"`\n\tError  *ApiError   `json:\"error,omitempty\"`\n}\n\n// ServerAPIRoutezResponse is the response type for routez\ntype ServerAPIRoutezResponse struct {\n\tServer *ServerInfo `json:\"server\"`\n\tData   *Routez     `json:\"data,omitempty\"`\n\tError  *ApiError   `json:\"error,omitempty\"`\n}\n\n// ServerAPIGatewayzResponse is the response type for gatewayz\ntype ServerAPIGatewayzResponse struct {\n\tServer *ServerInfo `json:\"server\"`\n\tData   *Gatewayz   `json:\"data,omitempty\"`\n\tError  *ApiError   `json:\"error,omitempty\"`\n}\n\n// ServerAPIJszResponse is the response type for jsz\ntype ServerAPIJszResponse struct {\n\tServer *ServerInfo `json:\"server\"`\n\tData   *JSInfo     `json:\"data,omitempty\"`\n\tError  *ApiError   `json:\"error,omitempty\"`\n}\n\n// ServerAPIHealthzResponse is the response type for healthz\ntype ServerAPIHealthzResponse struct {\n\tServer *ServerInfo   `json:\"server\"`\n\tData   *HealthStatus `json:\"data,omitempty\"`\n\tError  *ApiError     `json:\"error,omitempty\"`\n}\n\n// ServerAPIVarzResponse is the response type for varz\ntype ServerAPIVarzResponse struct {\n\tServer *ServerInfo `json:\"server\"`\n\tData   *Varz       `json:\"data,omitempty\"`\n\tError  *ApiError   `json:\"error,omitempty\"`\n}\n\n// ServerAPISubszResponse is the response type for subsz\ntype ServerAPISubszResponse struct {\n\tServer *ServerInfo `json:\"server\"`\n\tData   *Subsz      `json:\"data,omitempty\"`\n\tError  *ApiError   `json:\"error,omitempty\"`\n}\n\n// ServerAPILeafzResponse is the response type for leafz\ntype ServerAPILeafzResponse struct {\n\tServer *ServerInfo `json:\"server\"`\n\tData   *Leafz      `json:\"data,omitempty\"`\n\tError  *ApiError   `json:\"error,omitempty\"`\n}\n\n// ServerAPIAccountzResponse is the response type for accountz\ntype ServerAPIAccountzResponse struct {\n\tServer *ServerInfo `json:\"server\"`\n\tData   *Accountz   `json:\"data,omitempty\"`\n\tError  *ApiError   `json:\"error,omitempty\"`\n}\n\n// ServerAPIExpvarzResponse is the response type for expvarz\ntype ServerAPIExpvarzResponse struct {\n\tServer *ServerInfo    `json:\"server\"`\n\tData   *ExpvarzStatus `json:\"data,omitempty\"`\n\tError  *ApiError      `json:\"error,omitempty\"`\n}\n\n// ServerAPIpqueueszResponse is the response type for ipqueuesz\ntype ServerAPIpqueueszResponse struct {\n\tServer *ServerInfo      `json:\"server\"`\n\tData   *IpqueueszStatus `json:\"data,omitempty\"`\n\tError  *ApiError        `json:\"error,omitempty\"`\n}\n\n// ServerAPIRaftzResponse is the response type for raftz\ntype ServerAPIRaftzResponse struct {\n\tServer *ServerInfo  `json:\"server\"`\n\tData   *RaftzStatus `json:\"data,omitempty\"`\n\tError  *ApiError    `json:\"error,omitempty\"`\n}\n\n// statszReq is a request for us to respond with current statsz.\nfunc (s *Server) statszReq(sub *subscription, c *client, _ *Account, subject, reply string, hdr, msg []byte) {\n\tif !s.EventsEnabled() {\n\t\treturn\n\t}\n\n\t// No reply is a signal that we should use our normal broadcast subject.\n\tif reply == _EMPTY_ {\n\t\treply = fmt.Sprintf(serverStatsSubj, s.info.ID)\n\t\ts.wrapChk(s.resetLastStatsz)\n\t}\n\n\topts := StatszEventOptions{}\n\tif len(msg) != 0 {\n\t\tif err := json.Unmarshal(msg, &opts); err != nil {\n\t\t\tresponse := &ServerAPIResponse{\n\t\t\t\tServer: &ServerInfo{},\n\t\t\t\tError:  &ApiError{Code: http.StatusBadRequest, Description: err.Error()},\n\t\t\t}\n\t\t\ts.sendInternalMsgLocked(reply, _EMPTY_, response.Server, response)\n\t\t\treturn\n\t\t} else if ignore := s.filterRequest(&opts.EventFilterOptions); ignore {\n\t\t\treturn\n\t\t}\n\t}\n\ts.sendStatsz(reply)\n}\n\n// idzReq is for a request for basic static server info.\n// Try to not hold the write lock or dynamically create data.\nfunc (s *Server) idzReq(sub *subscription, c *client, _ *Account, subject, reply string, hdr, msg []byte) {\n\ts.mu.RLock()\n\tdefer s.mu.RUnlock()\n\tid := &ServerID{\n\t\tName: s.info.Name,\n\t\tHost: s.info.Host,\n\t\tID:   s.info.ID,\n\t}\n\ts.sendInternalMsg(reply, _EMPTY_, nil, &id)\n}\n\nvar errSkipZreq = errors.New(\"filtered response\")\n\nconst (\n\tacceptEncodingHeader  = \"Accept-Encoding\"\n\tcontentEncodingHeader = \"Content-Encoding\"\n)\n\n// This is not as formal as it could be. We see if anything has s2 or snappy first, then gzip.\nfunc getAcceptEncoding(hdr []byte) compressionType {\n\tae := strings.ToLower(string(getHeader(acceptEncodingHeader, hdr)))\n\tif ae == _EMPTY_ {\n\t\treturn noCompression\n\t}\n\tif strings.Contains(ae, \"snappy\") || strings.Contains(ae, \"s2\") {\n\t\treturn snappyCompression\n\t}\n\tif strings.Contains(ae, \"gzip\") {\n\t\treturn gzipCompression\n\t}\n\treturn unsupportedCompression\n}\n\nfunc (s *Server) zReq(_ *client, reply string, hdr, msg []byte, fOpts *EventFilterOptions, optz any, respf func() (any, error)) {\n\tif !s.EventsEnabled() || reply == _EMPTY_ {\n\t\treturn\n\t}\n\tresponse := &ServerAPIResponse{Server: &ServerInfo{}}\n\tvar err error\n\tstatus := 0\n\tif len(msg) != 0 {\n\t\tif err = json.Unmarshal(msg, optz); err != nil {\n\t\t\tstatus = http.StatusBadRequest // status is only included on error, so record how far execution got\n\t\t} else if s.filterRequest(fOpts) {\n\t\t\treturn\n\t\t}\n\t}\n\tif err == nil {\n\t\tresponse.Data, err = respf()\n\t\tif errors.Is(err, errSkipZreq) {\n\t\t\treturn\n\t\t} else if err != nil {\n\t\t\tstatus = http.StatusInternalServerError\n\t\t}\n\t}\n\tif err != nil {\n\t\tresponse.Error = &ApiError{Code: status, Description: err.Error()}\n\t} else if len(hdr) > 0 {\n\t\tresponse.compress = getAcceptEncoding(hdr)\n\t}\n\ts.sendInternalResponse(reply, response)\n}\n\n// remoteConnsUpdate gets called when we receive a remote update from another server.\nfunc (s *Server) remoteConnsUpdate(sub *subscription, c *client, _ *Account, subject, reply string, hdr, msg []byte) {\n\tif !s.eventsRunning() {\n\t\treturn\n\t}\n\tvar m AccountNumConns\n\tif len(msg) == 0 {\n\t\ts.sys.client.Errorf(\"No message body provided\")\n\t\treturn\n\t} else if err := json.Unmarshal(msg, &m); err != nil {\n\t\ts.sys.client.Errorf(\"Error unmarshalling account connection event message: %v\", err)\n\t\treturn\n\t}\n\n\t// See if we have the account registered, if not drop it.\n\t// Make sure this does not force us to load this account here.\n\tvar acc *Account\n\tif v, ok := s.accounts.Load(m.Account); ok {\n\t\tacc = v.(*Account)\n\t}\n\t// Silently ignore these if we do not have local interest in the account.\n\tif acc == nil {\n\t\treturn\n\t}\n\n\ts.mu.Lock()\n\n\t// check again here if we have been shutdown.\n\tif !s.isRunning() || !s.eventsEnabled() {\n\t\ts.mu.Unlock()\n\t\treturn\n\t}\n\t// Double check that this is not us, should never happen, so error if it does.\n\tif m.Server.ID == s.info.ID {\n\t\ts.sys.client.Errorf(\"Processing our own account connection event message: ignored\")\n\t\ts.mu.Unlock()\n\t\treturn\n\t}\n\t// If we are here we have interest in tracking this account. Update our accounting.\n\tclients := acc.updateRemoteServer(&m)\n\ts.updateRemoteServer(&m.Server)\n\ts.mu.Unlock()\n\t// Need to close clients outside of server lock\n\tfor _, c := range clients {\n\t\tc.maxAccountConnExceeded()\n\t}\n}\n\n// This will import any system level exports.\nfunc (s *Server) registerSystemImports(a *Account) {\n\tif a == nil || !s.EventsEnabled() {\n\t\treturn\n\t}\n\tsacc := s.SystemAccount()\n\tif sacc == nil || sacc == a {\n\t\treturn\n\t}\n\tdstAccName := sacc.Name\n\t// FIXME(dlc) - make a shared list between sys exports etc.\n\n\timportSrvc := func(subj, mappedSubj string) {\n\t\tif !a.serviceImportExists(dstAccName, subj) {\n\t\t\tif err := a.addServiceImportWithClaim(sacc, subj, mappedSubj, nil, true); err != nil {\n\t\t\t\ts.Errorf(\"Error setting up system service import %s -> %s for account: %v\",\n\t\t\t\t\tsubj, mappedSubj, err)\n\t\t\t}\n\t\t}\n\t}\n\t// Add in this to the account in 2 places.\n\t// \"$SYS.REQ.SERVER.PING.CONNZ\" and \"$SYS.REQ.ACCOUNT.PING.CONNZ\"\n\tmappedConnzSubj := fmt.Sprintf(accDirectReqSubj, a.Name, \"CONNZ\")\n\timportSrvc(fmt.Sprintf(accPingReqSubj, \"CONNZ\"), mappedConnzSubj)\n\timportSrvc(fmt.Sprintf(serverPingReqSubj, \"CONNZ\"), mappedConnzSubj)\n\timportSrvc(fmt.Sprintf(accPingReqSubj, \"STATZ\"), fmt.Sprintf(accDirectReqSubj, a.Name, \"STATZ\"))\n\n\t// This is for user's looking up their own info.\n\tmappedSubject := fmt.Sprintf(userDirectReqSubj, a.Name)\n\timportSrvc(userDirectInfoSubj, mappedSubject)\n\t// Make sure to share details.\n\ta.setServiceImportSharing(sacc, mappedSubject, false, true)\n}\n\n// Setup tracking for this account. This allows us to track global account activity.\n// Lock should be held on entry.\nfunc (s *Server) enableAccountTracking(a *Account) {\n\tif a == nil || !s.eventsEnabled() {\n\t\treturn\n\t}\n\n\t// TODO(ik): Generate payload although message may not be sent.\n\t// May need to ensure we do so only if there is a known interest.\n\t// This can get complicated with gateways.\n\n\tsubj := fmt.Sprintf(accDirectReqSubj, a.Name, \"CONNS\")\n\treply := fmt.Sprintf(connsRespSubj, s.info.ID)\n\tm := accNumConnsReq{Account: a.Name}\n\ts.sendInternalMsg(subj, reply, &m.Server, &m)\n}\n\n// Event on leaf node connect.\n// Lock should NOT be held on entry.\nfunc (s *Server) sendLeafNodeConnect(a *Account) {\n\ts.mu.Lock()\n\t// If we are not in operator mode, or do not have any gateways defined, this should also be a no-op.\n\tif a == nil || !s.eventsEnabled() || !s.gateway.enabled {\n\t\ts.mu.Unlock()\n\t\treturn\n\t}\n\ts.sendLeafNodeConnectMsg(a.Name)\n\ts.mu.Unlock()\n\n\ts.switchAccountToInterestMode(a.Name)\n}\n\n// Send the leafnode connect message.\n// Lock should be held.\nfunc (s *Server) sendLeafNodeConnectMsg(accName string) {\n\tsubj := fmt.Sprintf(leafNodeConnectEventSubj, accName)\n\tm := accNumConnsReq{Account: accName}\n\ts.sendInternalMsg(subj, _EMPTY_, &m.Server, &m)\n}\n\n// sendAccConnsUpdate is called to send out our information on the\n// account's local connections.\n// Lock should be held on entry.\nfunc (s *Server) sendAccConnsUpdate(a *Account, subj ...string) {\n\tif !s.eventsEnabled() || a == nil {\n\t\treturn\n\t}\n\tsendQ := s.sys.sendq\n\tif sendQ == nil {\n\t\treturn\n\t}\n\t// Build event with account name and number of local clients and leafnodes.\n\teid := s.nextEventID()\n\ta.mu.Lock()\n\tstat := a.statz()\n\tm := AccountNumConns{\n\t\tTypedEvent: TypedEvent{\n\t\t\tType: AccountNumConnsMsgType,\n\t\t\tID:   eid,\n\t\t\tTime: time.Now().UTC(),\n\t\t},\n\t\tAccountStat: *stat,\n\t}\n\t// Set timer to fire again unless we are at zero.\n\tif m.TotalConns == 0 {\n\t\tclearTimer(&a.ctmr)\n\t} else {\n\t\t// Check to see if we have an HB running and update.\n\t\tif a.ctmr == nil {\n\t\t\ta.ctmr = time.AfterFunc(eventsHBInterval, func() { s.accConnsUpdate(a) })\n\t\t} else {\n\t\t\ta.ctmr.Reset(eventsHBInterval)\n\t\t}\n\t}\n\tfor _, sub := range subj {\n\t\tmsg := newPubMsg(nil, sub, _EMPTY_, &m.Server, nil, &m, noCompression, false, false)\n\t\tsendQ.push(msg)\n\t}\n\ta.mu.Unlock()\n}\n\n// Lock should be held on entry.\nfunc (a *Account) statz() *AccountStat {\n\tlocalConns := a.numLocalConnections()\n\tleafConns := a.numLocalLeafNodes()\n\n\ta.stats.Lock()\n\treceived := DataStats{\n\t\tdataStats: dataStats{\n\t\t\tMsgs:  a.stats.inMsgs,\n\t\t\tBytes: a.stats.inBytes,\n\t\t},\n\t\tGateways: dataStats{\n\t\t\tMsgs:  a.stats.gw.inMsgs,\n\t\t\tBytes: a.stats.gw.inBytes,\n\t\t},\n\t\tRoutes: dataStats{\n\t\t\tMsgs:  a.stats.rt.inMsgs,\n\t\t\tBytes: a.stats.rt.inBytes,\n\t\t},\n\t\tLeafs: dataStats{\n\t\t\tMsgs:  a.stats.ln.inMsgs,\n\t\t\tBytes: a.stats.ln.inBytes,\n\t\t},\n\t}\n\tsent := DataStats{\n\t\tdataStats: dataStats{\n\t\t\tMsgs:  a.stats.outMsgs,\n\t\t\tBytes: a.stats.outBytes,\n\t\t},\n\t\tGateways: dataStats{\n\t\t\tMsgs:  a.stats.gw.outMsgs,\n\t\t\tBytes: a.stats.gw.outBytes,\n\t\t},\n\t\tRoutes: dataStats{\n\t\t\tMsgs:  a.stats.rt.outMsgs,\n\t\t\tBytes: a.stats.rt.outBytes,\n\t\t},\n\t\tLeafs: dataStats{\n\t\t\tMsgs:  a.stats.ln.outMsgs,\n\t\t\tBytes: a.stats.ln.outBytes,\n\t\t},\n\t}\n\tslowConsumers := a.stats.slowConsumers\n\ta.stats.Unlock()\n\n\treturn &AccountStat{\n\t\tAccount:       a.Name,\n\t\tName:          a.getNameTagLocked(),\n\t\tConns:         localConns,\n\t\tLeafNodes:     leafConns,\n\t\tTotalConns:    localConns + leafConns,\n\t\tNumSubs:       a.sl.Count(),\n\t\tReceived:      received,\n\t\tSent:          sent,\n\t\tSlowConsumers: slowConsumers,\n\t}\n}\n\n// accConnsUpdate is called whenever there is a change to the account's\n// number of active connections, or during a heartbeat.\n// We will not send for $G.\nfunc (s *Server) accConnsUpdate(a *Account) {\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\tif !s.eventsEnabled() || a == nil || a == s.gacc {\n\t\treturn\n\t}\n\ts.sendAccConnsUpdate(a, fmt.Sprintf(accConnsEventSubjOld, a.Name), fmt.Sprintf(accConnsEventSubjNew, a.Name))\n}\n\n// server lock should be held\nfunc (s *Server) nextEventID() string {\n\treturn s.eventIds.Next()\n}\n\n// accountConnectEvent will send an account client connect event if there is interest.\n// This is a billing event.\nfunc (s *Server) accountConnectEvent(c *client) {\n\ts.mu.Lock()\n\tif !s.eventsEnabled() {\n\t\ts.mu.Unlock()\n\t\treturn\n\t}\n\tgacc := s.gacc\n\teid := s.nextEventID()\n\ts.mu.Unlock()\n\n\tc.mu.Lock()\n\t// Ignore global account activity\n\tif c.acc == nil || c.acc == gacc {\n\t\tc.mu.Unlock()\n\t\treturn\n\t}\n\n\tm := ConnectEventMsg{\n\t\tTypedEvent: TypedEvent{\n\t\t\tType: ConnectEventMsgType,\n\t\t\tID:   eid,\n\t\t\tTime: time.Now().UTC(),\n\t\t},\n\t\tClient: ClientInfo{\n\t\t\tStart:      &c.start,\n\t\t\tHost:       c.host,\n\t\t\tID:         c.cid,\n\t\t\tAccount:    accForClient(c),\n\t\t\tUser:       c.getRawAuthUser(),\n\t\t\tName:       c.opts.Name,\n\t\t\tLang:       c.opts.Lang,\n\t\t\tVersion:    c.opts.Version,\n\t\t\tJwt:        c.opts.JWT,\n\t\t\tIssuerKey:  issuerForClient(c),\n\t\t\tTags:       c.tags,\n\t\t\tNameTag:    c.acc.getNameTag(),\n\t\t\tKind:       c.kindString(),\n\t\t\tClientType: c.clientTypeString(),\n\t\t\tMQTTClient: c.getMQTTClientID(),\n\t\t},\n\t}\n\tsubj := fmt.Sprintf(connectEventSubj, c.acc.Name)\n\tc.mu.Unlock()\n\n\ts.sendInternalMsgLocked(subj, _EMPTY_, &m.Server, &m)\n}\n\n// accountDisconnectEvent will send an account client disconnect event if there is interest.\n// This is a billing event.\nfunc (s *Server) accountDisconnectEvent(c *client, now time.Time, reason string) {\n\ts.mu.Lock()\n\tif !s.eventsEnabled() {\n\t\ts.mu.Unlock()\n\t\treturn\n\t}\n\tgacc := s.gacc\n\teid := s.nextEventID()\n\ts.mu.Unlock()\n\n\tc.mu.Lock()\n\n\t// Ignore global account activity\n\tif c.acc == nil || c.acc == gacc {\n\t\tc.mu.Unlock()\n\t\treturn\n\t}\n\n\tm := DisconnectEventMsg{\n\t\tTypedEvent: TypedEvent{\n\t\t\tType: DisconnectEventMsgType,\n\t\t\tID:   eid,\n\t\t\tTime: now,\n\t\t},\n\t\tClient: ClientInfo{\n\t\t\tStart:      &c.start,\n\t\t\tStop:       &now,\n\t\t\tHost:       c.host,\n\t\t\tID:         c.cid,\n\t\t\tAccount:    accForClient(c),\n\t\t\tUser:       c.getRawAuthUser(),\n\t\t\tName:       c.opts.Name,\n\t\t\tLang:       c.opts.Lang,\n\t\t\tVersion:    c.opts.Version,\n\t\t\tRTT:        c.getRTT(),\n\t\t\tJwt:        c.opts.JWT,\n\t\t\tIssuerKey:  issuerForClient(c),\n\t\t\tTags:       c.tags,\n\t\t\tNameTag:    c.acc.getNameTag(),\n\t\t\tKind:       c.kindString(),\n\t\t\tClientType: c.clientTypeString(),\n\t\t\tMQTTClient: c.getMQTTClientID(),\n\t\t},\n\t\tSent: DataStats{\n\t\t\tdataStats: dataStats{\n\t\t\t\tMsgs:  atomic.LoadInt64(&c.inMsgs),\n\t\t\t\tBytes: atomic.LoadInt64(&c.inBytes),\n\t\t\t},\n\t\t},\n\t\tReceived: DataStats{\n\t\t\tdataStats: dataStats{\n\t\t\t\tMsgs:  c.outMsgs,\n\t\t\t\tBytes: c.outBytes,\n\t\t\t},\n\t\t},\n\t\tReason: reason,\n\t}\n\taccName := c.acc.Name\n\tc.mu.Unlock()\n\n\tsubj := fmt.Sprintf(disconnectEventSubj, accName)\n\ts.sendInternalMsgLocked(subj, _EMPTY_, &m.Server, &m)\n}\n\n// This is the system level event sent to the system account for operators.\nfunc (s *Server) sendAuthErrorEvent(c *client) {\n\ts.mu.Lock()\n\tif !s.eventsEnabled() {\n\t\ts.mu.Unlock()\n\t\treturn\n\t}\n\teid := s.nextEventID()\n\ts.mu.Unlock()\n\n\tnow := time.Now().UTC()\n\tc.mu.Lock()\n\tm := DisconnectEventMsg{\n\t\tTypedEvent: TypedEvent{\n\t\t\tType: DisconnectEventMsgType,\n\t\t\tID:   eid,\n\t\t\tTime: now,\n\t\t},\n\t\tClient: ClientInfo{\n\t\t\tStart:      &c.start,\n\t\t\tStop:       &now,\n\t\t\tHost:       c.host,\n\t\t\tID:         c.cid,\n\t\t\tAccount:    accForClient(c),\n\t\t\tUser:       c.getRawAuthUser(),\n\t\t\tName:       c.opts.Name,\n\t\t\tLang:       c.opts.Lang,\n\t\t\tVersion:    c.opts.Version,\n\t\t\tRTT:        c.getRTT(),\n\t\t\tJwt:        c.opts.JWT,\n\t\t\tIssuerKey:  issuerForClient(c),\n\t\t\tTags:       c.tags,\n\t\t\tNameTag:    c.acc.getNameTag(),\n\t\t\tKind:       c.kindString(),\n\t\t\tClientType: c.clientTypeString(),\n\t\t\tMQTTClient: c.getMQTTClientID(),\n\t\t},\n\t\tSent: DataStats{\n\t\t\tdataStats: dataStats{\n\t\t\t\tMsgs:  c.inMsgs,\n\t\t\t\tBytes: c.inBytes,\n\t\t\t},\n\t\t},\n\t\tReceived: DataStats{\n\t\t\tdataStats: dataStats{\n\t\t\t\tMsgs:  c.outMsgs,\n\t\t\t\tBytes: c.outBytes,\n\t\t\t},\n\t\t},\n\t\tReason: AuthenticationViolation.String(),\n\t}\n\tc.mu.Unlock()\n\n\ts.mu.Lock()\n\tsubj := fmt.Sprintf(authErrorEventSubj, s.info.ID)\n\ts.sendInternalMsg(subj, _EMPTY_, &m.Server, &m)\n\ts.mu.Unlock()\n}\n\n// This is the account level event sent to the origin account for account owners.\nfunc (s *Server) sendAccountAuthErrorEvent(c *client, acc *Account, reason string) {\n\tif acc == nil {\n\t\treturn\n\t}\n\ts.mu.Lock()\n\tif !s.eventsEnabled() {\n\t\ts.mu.Unlock()\n\t\treturn\n\t}\n\teid := s.nextEventID()\n\ts.mu.Unlock()\n\n\tnow := time.Now().UTC()\n\tc.mu.Lock()\n\tm := DisconnectEventMsg{\n\t\tTypedEvent: TypedEvent{\n\t\t\tType: DisconnectEventMsgType,\n\t\t\tID:   eid,\n\t\t\tTime: now,\n\t\t},\n\t\tClient: ClientInfo{\n\t\t\tStart:      &c.start,\n\t\t\tStop:       &now,\n\t\t\tHost:       c.host,\n\t\t\tID:         c.cid,\n\t\t\tAccount:    acc.Name,\n\t\t\tUser:       c.getRawAuthUser(),\n\t\t\tName:       c.opts.Name,\n\t\t\tLang:       c.opts.Lang,\n\t\t\tVersion:    c.opts.Version,\n\t\t\tRTT:        c.getRTT(),\n\t\t\tJwt:        c.opts.JWT,\n\t\t\tIssuerKey:  issuerForClient(c),\n\t\t\tTags:       c.tags,\n\t\t\tNameTag:    c.acc.getNameTag(),\n\t\t\tKind:       c.kindString(),\n\t\t\tClientType: c.clientTypeString(),\n\t\t\tMQTTClient: c.getMQTTClientID(),\n\t\t},\n\t\tSent: DataStats{\n\t\t\tdataStats: dataStats{\n\t\t\t\tMsgs:  c.inMsgs,\n\t\t\t\tBytes: c.inBytes,\n\t\t\t},\n\t\t},\n\t\tReceived: DataStats{\n\t\t\tdataStats: dataStats{\n\t\t\t\tMsgs:  c.outMsgs,\n\t\t\t\tBytes: c.outBytes,\n\t\t\t},\n\t\t},\n\t\tReason: reason,\n\t}\n\tc.mu.Unlock()\n\n\ts.sendInternalAccountSysMsg(acc, authErrorAccountEventSubj, &m.Server, &m, noCompression)\n}\n\n// Internal message callback.\n// If the msg is needed past the callback it is required to be copied.\n// rmsg contains header and the message. use client.msgParts(rmsg) to split them apart\ntype msgHandler func(sub *subscription, client *client, acc *Account, subject, reply string, rmsg []byte)\n\nconst (\n\trecvQMuxed  = 1\n\trecvQStatsz = 2\n)\n\n// Create a wrapped callback handler for the subscription that will move it to an\n// internal recvQ for processing not inline with routes etc.\nfunc (s *Server) noInlineCallback(cb sysMsgHandler) msgHandler {\n\treturn s.noInlineCallbackRecvQSelect(cb, recvQMuxed)\n}\n\n// Create a wrapped callback handler for the subscription that will move it to an\n// internal recvQ for Statsz/Pings for processing not inline with routes etc.\nfunc (s *Server) noInlineCallbackStatsz(cb sysMsgHandler) msgHandler {\n\treturn s.noInlineCallbackRecvQSelect(cb, recvQStatsz)\n}\n\n// Create a wrapped callback handler for the subscription that will move it to an\n// internal IPQueue for processing not inline with routes etc.\nfunc (s *Server) noInlineCallbackRecvQSelect(cb sysMsgHandler, recvQSelect int) msgHandler {\n\ts.mu.RLock()\n\tif !s.eventsEnabled() {\n\t\ts.mu.RUnlock()\n\t\treturn nil\n\t}\n\t// Capture here for direct reference to avoid any unnecessary blocking inline with routes, gateways etc.\n\tvar recvq *ipQueue[*inSysMsg]\n\tswitch recvQSelect {\n\tcase recvQStatsz:\n\t\trecvq = s.sys.recvqp\n\tdefault:\n\t\trecvq = s.sys.recvq\n\t}\n\ts.mu.RUnlock()\n\n\treturn func(sub *subscription, c *client, acc *Account, subj, rply string, rmsg []byte) {\n\t\t// Need to copy and split here.\n\t\thdr, msg := c.msgParts(rmsg)\n\t\trecvq.push(&inSysMsg{sub, c, acc, subj, rply, copyBytes(hdr), copyBytes(msg), cb})\n\t}\n}\n\n// Create an internal subscription. sysSubscribeQ for queue groups.\nfunc (s *Server) sysSubscribe(subject string, cb msgHandler) (*subscription, error) {\n\treturn s.systemSubscribe(subject, _EMPTY_, false, nil, cb)\n}\n\n// Create an internal subscription with queue\nfunc (s *Server) sysSubscribeQ(subject, queue string, cb msgHandler) (*subscription, error) {\n\treturn s.systemSubscribe(subject, queue, false, nil, cb)\n}\n\n// Create an internal subscription but do not forward interest.\nfunc (s *Server) sysSubscribeInternal(subject string, cb msgHandler) (*subscription, error) {\n\treturn s.systemSubscribe(subject, _EMPTY_, true, nil, cb)\n}\n\nfunc (s *Server) systemSubscribe(subject, queue string, internalOnly bool, c *client, cb msgHandler) (*subscription, error) {\n\ts.mu.Lock()\n\tif !s.eventsEnabled() {\n\t\ts.mu.Unlock()\n\t\treturn nil, ErrNoSysAccount\n\t}\n\tif cb == nil {\n\t\ts.mu.Unlock()\n\t\treturn nil, fmt.Errorf(\"undefined message handler\")\n\t}\n\tif c == nil {\n\t\tc = s.sys.client\n\t}\n\ttrace := c.trace\n\ts.sys.sid++\n\tsid := strconv.Itoa(s.sys.sid)\n\ts.mu.Unlock()\n\n\t// Now create the subscription\n\tif trace {\n\t\tc.traceInOp(\"SUB\", []byte(subject+\" \"+queue+\" \"+sid))\n\t}\n\n\tvar q []byte\n\tif queue != _EMPTY_ {\n\t\tq = []byte(queue)\n\t}\n\n\t// Now create the subscription\n\treturn c.processSub([]byte(subject), q, []byte(sid), cb, internalOnly)\n}\n\nfunc (s *Server) sysUnsubscribe(sub *subscription) {\n\tif sub == nil {\n\t\treturn\n\t}\n\ts.mu.RLock()\n\tif !s.eventsEnabled() {\n\t\ts.mu.RUnlock()\n\t\treturn\n\t}\n\tc := sub.client\n\ts.mu.RUnlock()\n\n\tif c != nil {\n\t\tc.processUnsub(sub.sid)\n\t}\n}\n\n// This will generate the tracking subject for remote latency from the response subject.\nfunc remoteLatencySubjectForResponse(subject []byte) string {\n\tif !isTrackedReply(subject) {\n\t\treturn \"\"\n\t}\n\ttoks := bytes.Split(subject, []byte(tsep))\n\t// FIXME(dlc) - Sprintf may become a performance concern at some point.\n\treturn fmt.Sprintf(remoteLatencyEventSubj, toks[len(toks)-2])\n}\n\n// remoteLatencyUpdate is used to track remote latency measurements for tracking on exported services.\nfunc (s *Server) remoteLatencyUpdate(sub *subscription, _ *client, _ *Account, subject, _ string, hdr, msg []byte) {\n\tif !s.eventsRunning() {\n\t\treturn\n\t}\n\tvar rl remoteLatency\n\tif err := json.Unmarshal(msg, &rl); err != nil {\n\t\ts.Errorf(\"Error unmarshalling remote latency measurement: %v\", err)\n\t\treturn\n\t}\n\t// Now we need to look up the responseServiceImport associated with this measurement.\n\tacc, err := s.LookupAccount(rl.Account)\n\tif err != nil {\n\t\ts.Warnf(\"Could not lookup account %q for latency measurement\", rl.Account)\n\t\treturn\n\t}\n\t// Now get the request id / reply. We need to see if we have a GW prefix and if so strip that off.\n\treply := rl.ReqId\n\tif gwPrefix, old := isGWRoutedSubjectAndIsOldPrefix([]byte(reply)); gwPrefix {\n\t\treply = string(getSubjectFromGWRoutedReply([]byte(reply), old))\n\t}\n\tacc.mu.RLock()\n\tsi := acc.exports.responses[reply]\n\tif si == nil {\n\t\tacc.mu.RUnlock()\n\t\treturn\n\t}\n\tlsub := si.latency.subject\n\tacc.mu.RUnlock()\n\n\tsi.acc.mu.Lock()\n\tm1 := si.m1\n\tm2 := rl.M2\n\n\t// So we have not processed the response tracking measurement yet.\n\tif m1 == nil {\n\t\t// Store our value there for them to pick up.\n\t\tsi.m1 = &m2\n\t}\n\tsi.acc.mu.Unlock()\n\n\tif m1 == nil {\n\t\treturn\n\t}\n\n\t// Calculate the correct latencies given M1 and M2.\n\tm1.merge(&m2)\n\n\t// Clear the requesting client since we send the result here.\n\tacc.mu.Lock()\n\tsi.rc = nil\n\tacc.mu.Unlock()\n\n\t// Send the metrics\n\ts.sendInternalAccountMsg(acc, lsub, m1)\n}\n\n// This is used for all inbox replies so that we do not send supercluster wide interest\n// updates for every request. Same trick used in modern NATS clients.\nfunc (s *Server) inboxReply(sub *subscription, c *client, acc *Account, subject, reply string, msg []byte) {\n\ts.mu.RLock()\n\tif !s.eventsEnabled() || s.sys.replies == nil {\n\t\ts.mu.RUnlock()\n\t\treturn\n\t}\n\tcb, ok := s.sys.replies[subject]\n\ts.mu.RUnlock()\n\n\tif ok && cb != nil {\n\t\tcb(sub, c, acc, subject, reply, msg)\n\t}\n}\n\n// Copied from go client.\n// We could use serviceReply here instead to save some code.\n// I prefer these semantics for the moment, when tracing you know what this is.\nconst (\n\tInboxPrefix        = \"$SYS._INBOX.\"\n\tinboxPrefixLen     = len(InboxPrefix)\n\trespInboxPrefixLen = inboxPrefixLen + sysHashLen + 1\n\treplySuffixLen     = 8 // Gives us 62^8\n)\n\n// Creates an internal inbox used for replies that will be processed by the global wc handler.\nfunc (s *Server) newRespInbox() string {\n\tvar b [respInboxPrefixLen + replySuffixLen]byte\n\tpres := b[:respInboxPrefixLen]\n\tcopy(pres, s.sys.inboxPre)\n\trn := rand.Int63()\n\tfor i, l := respInboxPrefixLen, rn; i < len(b); i++ {\n\t\tb[i] = digits[l%base]\n\t\tl /= base\n\t}\n\treturn string(b[:])\n}\n\n// accNumSubsReq is sent when we need to gather remote info on subs.\ntype accNumSubsReq struct {\n\tAccount string `json:\"acc\"`\n\tSubject string `json:\"subject\"`\n\tQueue   []byte `json:\"queue,omitempty\"`\n}\n\n// helper function to total information from results to count subs.\nfunc totalSubs(rr *SublistResult, qg []byte) (nsubs int32) {\n\tif rr == nil {\n\t\treturn\n\t}\n\tcheckSub := func(sub *subscription) {\n\t\t// TODO(dlc) - This could be smarter.\n\t\tif qg != nil && !bytes.Equal(qg, sub.queue) {\n\t\t\treturn\n\t\t}\n\t\tif sub.client.kind == CLIENT || sub.client.isHubLeafNode() {\n\t\t\tnsubs++\n\t\t}\n\t}\n\tif qg == nil {\n\t\tfor _, sub := range rr.psubs {\n\t\t\tcheckSub(sub)\n\t\t}\n\t}\n\tfor _, qsub := range rr.qsubs {\n\t\tfor _, sub := range qsub {\n\t\t\tcheckSub(sub)\n\t\t}\n\t}\n\treturn\n}\n\n// Allows users of large systems to debug active subscribers for a given subject.\n// Payload should be the subject of interest.\nfunc (s *Server) debugSubscribers(sub *subscription, c *client, _ *Account, subject, reply string, hdr, msg []byte) {\n\t// Even though this is an internal only subscription, meaning interest was not forwarded, we could\n\t// get one here from a GW in optimistic mode. Ignore for now.\n\t// FIXME(dlc) - Should we send no interest here back to the GW?\n\tif c.kind != CLIENT {\n\t\treturn\n\t}\n\n\tvar ci ClientInfo\n\tif len(hdr) > 0 {\n\t\tif err := json.Unmarshal(getHeader(ClientInfoHdr, hdr), &ci); err != nil {\n\t\t\treturn\n\t\t}\n\t}\n\n\tvar acc *Account\n\tif ci.Service != _EMPTY_ {\n\t\tacc, _ = s.LookupAccount(ci.Service)\n\t} else if ci.Account != _EMPTY_ {\n\t\tacc, _ = s.LookupAccount(ci.Account)\n\t} else {\n\t\t// Direct $SYS access.\n\t\tacc = c.acc\n\t\tif acc == nil {\n\t\t\tacc = s.SystemAccount()\n\t\t}\n\t}\n\tif acc == nil {\n\t\treturn\n\t}\n\n\t// We could have a single subject or we could have a subject and a wildcard separated by whitespace.\n\targs := strings.Split(strings.TrimSpace(string(msg)), \" \")\n\tif len(args) == 0 {\n\t\ts.sendInternalAccountMsg(acc, reply, 0)\n\t\treturn\n\t}\n\n\ttsubj := args[0]\n\tvar qgroup []byte\n\tif len(args) > 1 {\n\t\tqgroup = []byte(args[1])\n\t}\n\n\tvar nsubs int32\n\n\tif subjectIsLiteral(tsubj) {\n\t\t// We will look up subscribers locally first then determine if we need to solicit other servers.\n\t\trr := acc.sl.Match(tsubj)\n\t\tnsubs = totalSubs(rr, qgroup)\n\t} else {\n\t\t// We have a wildcard, so this is a bit slower path.\n\t\tvar _subs [32]*subscription\n\t\tsubs := _subs[:0]\n\t\tacc.sl.All(&subs)\n\t\tfor _, sub := range subs {\n\t\t\tif subjectIsSubsetMatch(string(sub.subject), tsubj) {\n\t\t\t\tif qgroup != nil && !bytes.Equal(qgroup, sub.queue) {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tif sub.client.kind == CLIENT || sub.client.isHubLeafNode() {\n\t\t\t\t\tnsubs++\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\t// We should have an idea of how many responses to expect from remote servers.\n\tvar expected = acc.expectedRemoteResponses()\n\n\t// If we are only local, go ahead and return.\n\tif expected == 0 {\n\t\ts.sendInternalAccountMsg(nil, reply, nsubs)\n\t\treturn\n\t}\n\n\t// We need to solicit from others.\n\t// To track status.\n\tresponses := int32(0)\n\tdone := make(chan (bool))\n\n\ts.mu.Lock()\n\t// Create direct reply inbox that we multiplex under the WC replies.\n\treplySubj := s.newRespInbox()\n\t// Store our handler.\n\ts.sys.replies[replySubj] = func(sub *subscription, _ *client, _ *Account, subject, _ string, msg []byte) {\n\t\tif n, err := strconv.Atoi(string(msg)); err == nil {\n\t\t\tatomic.AddInt32(&nsubs, int32(n))\n\t\t}\n\t\tif atomic.AddInt32(&responses, 1) >= expected {\n\t\t\tselect {\n\t\t\tcase done <- true:\n\t\t\tdefault:\n\t\t\t}\n\t\t}\n\t}\n\t// Send the request to the other servers.\n\trequest := &accNumSubsReq{\n\t\tAccount: acc.Name,\n\t\tSubject: tsubj,\n\t\tQueue:   qgroup,\n\t}\n\ts.sendInternalMsg(accNumSubsReqSubj, replySubj, nil, request)\n\ts.mu.Unlock()\n\n\t// FIXME(dlc) - We should rate limit here instead of blind Go routine.\n\tgo func() {\n\t\tselect {\n\t\tcase <-done:\n\t\tcase <-time.After(500 * time.Millisecond):\n\t\t}\n\t\t// Cleanup the WC entry.\n\t\tvar sendResponse bool\n\t\ts.mu.Lock()\n\t\tif s.sys != nil && s.sys.replies != nil {\n\t\t\tdelete(s.sys.replies, replySubj)\n\t\t\tsendResponse = true\n\t\t}\n\t\ts.mu.Unlock()\n\t\tif sendResponse {\n\t\t\t// Send the response.\n\t\t\ts.sendInternalAccountMsg(nil, reply, atomic.LoadInt32(&nsubs))\n\t\t}\n\t}()\n}\n\n// Request for our local subscription count. This will come from a remote origin server\n// that received the initial request.\nfunc (s *Server) nsubsRequest(sub *subscription, c *client, _ *Account, subject, reply string, hdr, msg []byte) {\n\tif !s.eventsRunning() {\n\t\treturn\n\t}\n\tm := accNumSubsReq{}\n\tif len(msg) == 0 {\n\t\ts.sys.client.Errorf(\"request requires a body\")\n\t\treturn\n\t} else if err := json.Unmarshal(msg, &m); err != nil {\n\t\ts.sys.client.Errorf(\"Error unmarshalling account nsubs request message: %v\", err)\n\t\treturn\n\t}\n\t// Grab account.\n\tacc, _ := s.lookupAccount(m.Account)\n\tif acc == nil || acc.numLocalAndLeafConnections() == 0 {\n\t\treturn\n\t}\n\t// We will look up subscribers locally first then determine if we need to solicit other servers.\n\tvar nsubs int32\n\tif subjectIsLiteral(m.Subject) {\n\t\trr := acc.sl.Match(m.Subject)\n\t\tnsubs = totalSubs(rr, m.Queue)\n\t} else {\n\t\t// We have a wildcard, so this is a bit slower path.\n\t\tvar _subs [32]*subscription\n\t\tsubs := _subs[:0]\n\t\tacc.sl.All(&subs)\n\t\tfor _, sub := range subs {\n\t\t\tif (sub.client.kind == CLIENT || sub.client.isHubLeafNode()) && subjectIsSubsetMatch(string(sub.subject), m.Subject) {\n\t\t\t\tif m.Queue != nil && !bytes.Equal(m.Queue, sub.queue) {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tnsubs++\n\t\t\t}\n\t\t}\n\t}\n\ts.sendInternalMsgLocked(reply, _EMPTY_, nil, nsubs)\n}\n\nfunc (s *Server) reloadConfig(sub *subscription, c *client, _ *Account, subject, reply string, hdr, msg []byte) {\n\tif !s.eventsRunning() {\n\t\treturn\n\t}\n\n\toptz := &EventFilterOptions{}\n\ts.zReq(c, reply, hdr, msg, optz, optz, func() (any, error) {\n\t\t// Reload the server config, as requested.\n\t\treturn nil, s.Reload()\n\t})\n}\n\ntype KickClientReq struct {\n\tCID uint64 `json:\"cid\"`\n}\n\ntype LDMClientReq struct {\n\tCID uint64 `json:\"cid\"`\n}\n\nfunc (s *Server) kickClient(_ *subscription, c *client, _ *Account, subject, reply string, hdr, msg []byte) {\n\tif !s.eventsRunning() {\n\t\treturn\n\t}\n\n\tvar req KickClientReq\n\tif err := json.Unmarshal(msg, &req); err != nil {\n\t\ts.sys.client.Errorf(\"Error unmarshalling kick client request: %v\", err)\n\t\treturn\n\t}\n\n\toptz := &EventFilterOptions{}\n\ts.zReq(c, reply, hdr, msg, optz, optz, func() (any, error) {\n\t\treturn nil, s.DisconnectClientByID(req.CID)\n\t})\n\n}\n\nfunc (s *Server) ldmClient(_ *subscription, c *client, _ *Account, subject, reply string, hdr, msg []byte) {\n\tif !s.eventsRunning() {\n\t\treturn\n\t}\n\n\tvar req LDMClientReq\n\tif err := json.Unmarshal(msg, &req); err != nil {\n\t\ts.sys.client.Errorf(\"Error unmarshalling kick client request: %v\", err)\n\t\treturn\n\t}\n\n\toptz := &EventFilterOptions{}\n\ts.zReq(c, reply, hdr, msg, optz, optz, func() (any, error) {\n\t\treturn nil, s.LDMClientByID(req.CID)\n\t})\n}\n\n// Helper to grab account name for a client.\nfunc accForClient(c *client) string {\n\tif c.acc != nil {\n\t\treturn c.acc.Name\n\t}\n\treturn \"N/A\"\n}\n\n// Helper to grab issuer for a client.\nfunc issuerForClient(c *client) (issuerKey string) {\n\tif c == nil || c.user == nil {\n\t\treturn\n\t}\n\tissuerKey = c.user.SigningKey\n\tif issuerKey == _EMPTY_ && c.user.Account != nil {\n\t\tissuerKey = c.user.Account.Name\n\t}\n\treturn\n}\n\n// Helper to clear timers.\nfunc clearTimer(tp **time.Timer) {\n\tif t := *tp; t != nil {\n\t\tt.Stop()\n\t\t*tp = nil\n\t}\n}\n\n// Helper function to wrap functions with common test\n// to lock server and return if events not enabled.\nfunc (s *Server) wrapChk(f func()) func() {\n\treturn func() {\n\t\ts.mu.Lock()\n\t\tif !s.eventsEnabled() {\n\t\t\ts.mu.Unlock()\n\t\t\treturn\n\t\t}\n\t\tf()\n\t\ts.mu.Unlock()\n\t}\n}\n\n// sendOCSPPeerRejectEvent sends a system level event to system account when a peer connection is\n// rejected due to OCSP invalid status of its trust chain(s).\nfunc (s *Server) sendOCSPPeerRejectEvent(kind string, peer *x509.Certificate, reason string) {\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\tif !s.eventsEnabled() {\n\t\treturn\n\t}\n\tif peer == nil {\n\t\ts.Errorf(certidp.ErrPeerEmptyNoEvent)\n\t\treturn\n\t}\n\teid := s.nextEventID()\n\tnow := time.Now().UTC()\n\tm := OCSPPeerRejectEventMsg{\n\t\tTypedEvent: TypedEvent{\n\t\t\tType: OCSPPeerRejectEventMsgType,\n\t\t\tID:   eid,\n\t\t\tTime: now,\n\t\t},\n\t\tKind: kind,\n\t\tPeer: certidp.CertInfo{\n\t\t\tSubject:     certidp.GetSubjectDNForm(peer),\n\t\t\tIssuer:      certidp.GetIssuerDNForm(peer),\n\t\t\tFingerprint: certidp.GenerateFingerprint(peer),\n\t\t\tRaw:         peer.Raw,\n\t\t},\n\t\tReason: reason,\n\t}\n\tsubj := fmt.Sprintf(ocspPeerRejectEventSubj, s.info.ID)\n\ts.sendInternalMsg(subj, _EMPTY_, &m.Server, &m)\n}\n\n// sendOCSPPeerChainlinkInvalidEvent sends a system level event to system account when a link in a peer's trust chain\n// is OCSP invalid.\nfunc (s *Server) sendOCSPPeerChainlinkInvalidEvent(peer *x509.Certificate, link *x509.Certificate, reason string) {\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\tif !s.eventsEnabled() {\n\t\treturn\n\t}\n\tif peer == nil || link == nil {\n\t\ts.Errorf(certidp.ErrPeerEmptyNoEvent)\n\t\treturn\n\t}\n\teid := s.nextEventID()\n\tnow := time.Now().UTC()\n\tm := OCSPPeerChainlinkInvalidEventMsg{\n\t\tTypedEvent: TypedEvent{\n\t\t\tType: OCSPPeerChainlinkInvalidEventMsgType,\n\t\t\tID:   eid,\n\t\t\tTime: now,\n\t\t},\n\t\tLink: certidp.CertInfo{\n\t\t\tSubject:     certidp.GetSubjectDNForm(link),\n\t\t\tIssuer:      certidp.GetIssuerDNForm(link),\n\t\t\tFingerprint: certidp.GenerateFingerprint(link),\n\t\t\tRaw:         link.Raw,\n\t\t},\n\t\tPeer: certidp.CertInfo{\n\t\t\tSubject:     certidp.GetSubjectDNForm(peer),\n\t\t\tIssuer:      certidp.GetIssuerDNForm(peer),\n\t\t\tFingerprint: certidp.GenerateFingerprint(peer),\n\t\t\tRaw:         peer.Raw,\n\t\t},\n\t\tReason: reason,\n\t}\n\tsubj := fmt.Sprintf(ocspPeerChainlinkInvalidEventSubj, s.info.ID)\n\ts.sendInternalMsg(subj, _EMPTY_, &m.Server, &m)\n}\n",
    "source_file": "server/events.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2020-2025 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage server\n\nimport (\n\t\"bytes\"\n\t\"cmp\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"runtime\"\n\t\"slices\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync/atomic\"\n\t\"time\"\n\t\"unicode\"\n\n\t\"github.com/nats-io/nuid\"\n)\n\n// Request API subjects for JetStream.\nconst (\n\t// All API endpoints.\n\tjsAllAPI = \"$JS.API.>\"\n\n\t// For constructing JetStream domain prefixes.\n\tjsDomainAPI = \"$JS.%s.API.>\"\n\n\tJSApiPrefix = \"$JS.API\"\n\n\t// JSApiAccountInfo is for obtaining general information about JetStream for this account.\n\t// Will return JSON response.\n\tJSApiAccountInfo = \"$JS.API.INFO\"\n\n\t// JSApiTemplateCreate is the endpoint to create new stream templates.\n\t// Will return JSON response.\n\tJSApiTemplateCreate  = \"$JS.API.STREAM.TEMPLATE.CREATE.*\"\n\tJSApiTemplateCreateT = \"$JS.API.STREAM.TEMPLATE.CREATE.%s\"\n\n\t// JSApiTemplates is the endpoint to list all stream template names for this account.\n\t// Will return JSON response.\n\tJSApiTemplates = \"$JS.API.STREAM.TEMPLATE.NAMES\"\n\n\t// JSApiTemplateInfo is for obtaining general information about a named stream template.\n\t// Will return JSON response.\n\tJSApiTemplateInfo  = \"$JS.API.STREAM.TEMPLATE.INFO.*\"\n\tJSApiTemplateInfoT = \"$JS.API.STREAM.TEMPLATE.INFO.%s\"\n\n\t// JSApiTemplateDelete is the endpoint to delete stream templates.\n\t// Will return JSON response.\n\tJSApiTemplateDelete  = \"$JS.API.STREAM.TEMPLATE.DELETE.*\"\n\tJSApiTemplateDeleteT = \"$JS.API.STREAM.TEMPLATE.DELETE.%s\"\n\n\t// JSApiStreamCreate is the endpoint to create new streams.\n\t// Will return JSON response.\n\tJSApiStreamCreate  = \"$JS.API.STREAM.CREATE.*\"\n\tJSApiStreamCreateT = \"$JS.API.STREAM.CREATE.%s\"\n\n\t// JSApiStreamUpdate is the endpoint to update existing streams.\n\t// Will return JSON response.\n\tJSApiStreamUpdate  = \"$JS.API.STREAM.UPDATE.*\"\n\tJSApiStreamUpdateT = \"$JS.API.STREAM.UPDATE.%s\"\n\n\t// JSApiStreams is the endpoint to list all stream names for this account.\n\t// Will return JSON response.\n\tJSApiStreams = \"$JS.API.STREAM.NAMES\"\n\t// JSApiStreamList is the endpoint that will return all detailed stream information\n\tJSApiStreamList = \"$JS.API.STREAM.LIST\"\n\n\t// JSApiStreamInfo is for obtaining general information about a named stream.\n\t// Will return JSON response.\n\tJSApiStreamInfo  = \"$JS.API.STREAM.INFO.*\"\n\tJSApiStreamInfoT = \"$JS.API.STREAM.INFO.%s\"\n\n\t// JSApiStreamDelete is the endpoint to delete streams.\n\t// Will return JSON response.\n\tJSApiStreamDelete  = \"$JS.API.STREAM.DELETE.*\"\n\tJSApiStreamDeleteT = \"$JS.API.STREAM.DELETE.%s\"\n\n\t// JSApiStreamPurge is the endpoint to purge streams.\n\t// Will return JSON response.\n\tJSApiStreamPurge  = \"$JS.API.STREAM.PURGE.*\"\n\tJSApiStreamPurgeT = \"$JS.API.STREAM.PURGE.%s\"\n\n\t// JSApiStreamSnapshot is the endpoint to snapshot streams.\n\t// Will return a stream of chunks with a nil chunk as EOF to\n\t// the deliver subject. Caller should respond to each chunk\n\t// with a nil body response for ack flow.\n\tJSApiStreamSnapshot  = \"$JS.API.STREAM.SNAPSHOT.*\"\n\tJSApiStreamSnapshotT = \"$JS.API.STREAM.SNAPSHOT.%s\"\n\n\t// JSApiStreamRestore is the endpoint to restore a stream from a snapshot.\n\t// Caller should respond to each chunk with a nil body response.\n\tJSApiStreamRestore  = \"$JS.API.STREAM.RESTORE.*\"\n\tJSApiStreamRestoreT = \"$JS.API.STREAM.RESTORE.%s\"\n\n\t// JSApiMsgDelete is the endpoint to delete messages from a stream.\n\t// Will return JSON response.\n\tJSApiMsgDelete  = \"$JS.API.STREAM.MSG.DELETE.*\"\n\tJSApiMsgDeleteT = \"$JS.API.STREAM.MSG.DELETE.%s\"\n\n\t// JSApiMsgGet is the template for direct requests for a message by its stream sequence number.\n\t// Will return JSON response.\n\tJSApiMsgGet  = \"$JS.API.STREAM.MSG.GET.*\"\n\tJSApiMsgGetT = \"$JS.API.STREAM.MSG.GET.%s\"\n\n\t// JSDirectMsgGet is the template for non-api layer direct requests for a message by its stream sequence number or last by subject.\n\t// Will return the message similar to how a consumer receives the message, no JSON processing.\n\t// If the message can not be found we will use a status header of 404. If the stream does not exist the client will get a no-responders or timeout.\n\tJSDirectMsgGet  = \"$JS.API.DIRECT.GET.*\"\n\tJSDirectMsgGetT = \"$JS.API.DIRECT.GET.%s\"\n\n\t// This is a direct version of get last by subject, which will be the dominant pattern for KV access once 2.9 is released.\n\t// The stream and the key will be part of the subject to allow for no-marshal payloads and subject based security permissions.\n\tJSDirectGetLastBySubject  = \"$JS.API.DIRECT.GET.*.>\"\n\tJSDirectGetLastBySubjectT = \"$JS.API.DIRECT.GET.%s.%s\"\n\n\t// jsDirectGetPre\n\tjsDirectGetPre = \"$JS.API.DIRECT.GET\"\n\n\t// JSApiConsumerCreate is the endpoint to create consumers for streams.\n\t// This was also the legacy endpoint for ephemeral consumers.\n\t// It now can take consumer name and optional filter subject, which when part of the subject controls access.\n\t// Will return JSON response.\n\tJSApiConsumerCreate    = \"$JS.API.CONSUMER.CREATE.*\"\n\tJSApiConsumerCreateT   = \"$JS.API.CONSUMER.CREATE.%s\"\n\tJSApiConsumerCreateEx  = \"$JS.API.CONSUMER.CREATE.*.>\"\n\tJSApiConsumerCreateExT = \"$JS.API.CONSUMER.CREATE.%s.%s.%s\"\n\n\t// JSApiDurableCreate is the endpoint to create durable consumers for streams.\n\t// You need to include the stream and consumer name in the subject.\n\tJSApiDurableCreate  = \"$JS.API.CONSUMER.DURABLE.CREATE.*.*\"\n\tJSApiDurableCreateT = \"$JS.API.CONSUMER.DURABLE.CREATE.%s.%s\"\n\n\t// JSApiConsumers is the endpoint to list all consumer names for the stream.\n\t// Will return JSON response.\n\tJSApiConsumers  = \"$JS.API.CONSUMER.NAMES.*\"\n\tJSApiConsumersT = \"$JS.API.CONSUMER.NAMES.%s\"\n\n\t// JSApiConsumerList is the endpoint that will return all detailed consumer information\n\tJSApiConsumerList  = \"$JS.API.CONSUMER.LIST.*\"\n\tJSApiConsumerListT = \"$JS.API.CONSUMER.LIST.%s\"\n\n\t// JSApiConsumerInfo is for obtaining general information about a consumer.\n\t// Will return JSON response.\n\tJSApiConsumerInfo  = \"$JS.API.CONSUMER.INFO.*.*\"\n\tJSApiConsumerInfoT = \"$JS.API.CONSUMER.INFO.%s.%s\"\n\n\t// JSApiConsumerDelete is the endpoint to delete consumers.\n\t// Will return JSON response.\n\tJSApiConsumerDelete  = \"$JS.API.CONSUMER.DELETE.*.*\"\n\tJSApiConsumerDeleteT = \"$JS.API.CONSUMER.DELETE.%s.%s\"\n\n\t// JSApiConsumerPause is the endpoint to pause or unpause consumers.\n\t// Will return JSON response.\n\tJSApiConsumerPause  = \"$JS.API.CONSUMER.PAUSE.*.*\"\n\tJSApiConsumerPauseT = \"$JS.API.CONSUMER.PAUSE.%s.%s\"\n\n\t// JSApiRequestNextT is the prefix for the request next message(s) for a consumer in worker/pull mode.\n\tJSApiRequestNextT = \"$JS.API.CONSUMER.MSG.NEXT.%s.%s\"\n\n\t// JSApiConsumerUnpinT is the prefix for unpinning subscription for a given consumer.\n\tJSApiConsumerUnpin  = \"$JS.API.CONSUMER.UNPIN.*.*\"\n\tJSApiConsumerUnpinT = \"$JS.API.CONSUMER.UNPIN.%s.%s\"\n\n\t// jsRequestNextPre\n\tjsRequestNextPre = \"$JS.API.CONSUMER.MSG.NEXT.\"\n\n\t// For snapshots and restores. The ack will have additional tokens.\n\tjsSnapshotAckT    = \"$JS.SNAPSHOT.ACK.%s.%s\"\n\tjsRestoreDeliverT = \"$JS.SNAPSHOT.RESTORE.%s.%s\"\n\n\t// JSApiStreamRemovePeer is the endpoint to remove a peer from a clustered stream and its consumers.\n\t// Will return JSON response.\n\tJSApiStreamRemovePeer  = \"$JS.API.STREAM.PEER.REMOVE.*\"\n\tJSApiStreamRemovePeerT = \"$JS.API.STREAM.PEER.REMOVE.%s\"\n\n\t// JSApiStreamLeaderStepDown is the endpoint to have stream leader stepdown.\n\t// Will return JSON response.\n\tJSApiStreamLeaderStepDown  = \"$JS.API.STREAM.LEADER.STEPDOWN.*\"\n\tJSApiStreamLeaderStepDownT = \"$JS.API.STREAM.LEADER.STEPDOWN.%s\"\n\n\t// JSApiConsumerLeaderStepDown is the endpoint to have consumer leader stepdown.\n\t// Will return JSON response.\n\tJSApiConsumerLeaderStepDown  = \"$JS.API.CONSUMER.LEADER.STEPDOWN.*.*\"\n\tJSApiConsumerLeaderStepDownT = \"$JS.API.CONSUMER.LEADER.STEPDOWN.%s.%s\"\n\n\t// JSApiLeaderStepDown is the endpoint to have our metaleader stepdown.\n\t// Only works from system account.\n\t// Will return JSON response.\n\tJSApiLeaderStepDown = \"$JS.API.META.LEADER.STEPDOWN\"\n\n\t// JSApiRemoveServer is the endpoint to remove a peer server from the cluster.\n\t// Only works from system account.\n\t// Will return JSON response.\n\tJSApiRemoveServer = \"$JS.API.SERVER.REMOVE\"\n\n\t// JSApiAccountPurge is the endpoint to purge the js content of an account\n\t// Only works from system account.\n\t// Will return JSON response.\n\tJSApiAccountPurge  = \"$JS.API.ACCOUNT.PURGE.*\"\n\tJSApiAccountPurgeT = \"$JS.API.ACCOUNT.PURGE.%s\"\n\n\t// JSApiServerStreamMove is the endpoint to move streams off a server\n\t// Only works from system account.\n\t// Will return JSON response.\n\tJSApiServerStreamMove  = \"$JS.API.ACCOUNT.STREAM.MOVE.*.*\"\n\tJSApiServerStreamMoveT = \"$JS.API.ACCOUNT.STREAM.MOVE.%s.%s\"\n\n\t// JSApiServerStreamCancelMove is the endpoint to cancel a stream move\n\t// Only works from system account.\n\t// Will return JSON response.\n\tJSApiServerStreamCancelMove  = \"$JS.API.ACCOUNT.STREAM.CANCEL_MOVE.*.*\"\n\tJSApiServerStreamCancelMoveT = \"$JS.API.ACCOUNT.STREAM.CANCEL_MOVE.%s.%s\"\n\n\t// The prefix for system level account API.\n\tjsAPIAccountPre = \"$JS.API.ACCOUNT.\"\n\n\t// jsAckT is the template for the ack message stream coming back from a consumer\n\t// when they ACK/NAK, etc a message.\n\tjsAckT      = \"$JS.ACK.%s.%s\"\n\tjsAckPre    = \"$JS.ACK.\"\n\tjsAckPreLen = len(jsAckPre)\n\n\t// jsFlowControl is for flow control subjects.\n\tjsFlowControlPre = \"$JS.FC.\"\n\t// jsFlowControl is for FC responses.\n\tjsFlowControl = \"$JS.FC.%s.%s.*\"\n\n\t// JSAdvisoryPrefix is a prefix for all JetStream advisories.\n\tJSAdvisoryPrefix = \"$JS.EVENT.ADVISORY\"\n\n\t// JSMetricPrefix is a prefix for all JetStream metrics.\n\tJSMetricPrefix = \"$JS.EVENT.METRIC\"\n\n\t// JSMetricConsumerAckPre is a metric containing ack latency.\n\tJSMetricConsumerAckPre = \"$JS.EVENT.METRIC.CONSUMER.ACK\"\n\n\t// JSAdvisoryConsumerMaxDeliveryExceedPre is a notification published when a message exceeds its delivery threshold.\n\tJSAdvisoryConsumerMaxDeliveryExceedPre = \"$JS.EVENT.ADVISORY.CONSUMER.MAX_DELIVERIES\"\n\n\t// JSAdvisoryConsumerMsgNakPre is a notification published when a message has been naked\n\tJSAdvisoryConsumerMsgNakPre = \"$JS.EVENT.ADVISORY.CONSUMER.MSG_NAKED\"\n\n\t// JSAdvisoryConsumerMsgTerminatedPre is a notification published when a message has been terminated.\n\tJSAdvisoryConsumerMsgTerminatedPre = \"$JS.EVENT.ADVISORY.CONSUMER.MSG_TERMINATED\"\n\n\t// JSAdvisoryStreamCreatedPre notification that a stream was created.\n\tJSAdvisoryStreamCreatedPre = \"$JS.EVENT.ADVISORY.STREAM.CREATED\"\n\n\t// JSAdvisoryStreamDeletedPre notification that a stream was deleted.\n\tJSAdvisoryStreamDeletedPre = \"$JS.EVENT.ADVISORY.STREAM.DELETED\"\n\n\t// JSAdvisoryStreamUpdatedPre notification that a stream was updated.\n\tJSAdvisoryStreamUpdatedPre = \"$JS.EVENT.ADVISORY.STREAM.UPDATED\"\n\n\t// JSAdvisoryConsumerCreatedPre notification that a consumer was created.\n\tJSAdvisoryConsumerCreatedPre = \"$JS.EVENT.ADVISORY.CONSUMER.CREATED\"\n\n\t// JSAdvisoryConsumerDeletedPre notification that a consumer was deleted.\n\tJSAdvisoryConsumerDeletedPre = \"$JS.EVENT.ADVISORY.CONSUMER.DELETED\"\n\n\t// JSAdvisoryConsumerPausePre notification that a consumer paused/unpaused.\n\tJSAdvisoryConsumerPausePre = \"$JS.EVENT.ADVISORY.CONSUMER.PAUSE\"\n\n\t// JSAdvisoryConsumerPinnedPre notification that a consumer was pinned.\n\tJSAdvisoryConsumerPinnedPre = \"$JS.EVENT.ADVISORY.CONSUMER.PINNED\"\n\n\t// JSAdvisoryConsumerUnpinnedPre notification that a consumer was unpinned.\n\tJSAdvisoryConsumerUnpinnedPre = \"$JS.EVENT.ADVISORY.CONSUMER.UNPINNED\"\n\n\t// JSAdvisoryStreamSnapshotCreatePre notification that a snapshot was created.\n\tJSAdvisoryStreamSnapshotCreatePre = \"$JS.EVENT.ADVISORY.STREAM.SNAPSHOT_CREATE\"\n\n\t// JSAdvisoryStreamSnapshotCompletePre notification that a snapshot was completed.\n\tJSAdvisoryStreamSnapshotCompletePre = \"$JS.EVENT.ADVISORY.STREAM.SNAPSHOT_COMPLETE\"\n\n\t// JSAdvisoryStreamRestoreCreatePre notification that a restore was start.\n\tJSAdvisoryStreamRestoreCreatePre = \"$JS.EVENT.ADVISORY.STREAM.RESTORE_CREATE\"\n\n\t// JSAdvisoryStreamRestoreCompletePre notification that a restore was completed.\n\tJSAdvisoryStreamRestoreCompletePre = \"$JS.EVENT.ADVISORY.STREAM.RESTORE_COMPLETE\"\n\n\t// JSAdvisoryDomainLeaderElectedPre notification that a jetstream domain has elected a leader.\n\tJSAdvisoryDomainLeaderElected = \"$JS.EVENT.ADVISORY.DOMAIN.LEADER_ELECTED\"\n\n\t// JSAdvisoryStreamLeaderElectedPre notification that a replicated stream has elected a leader.\n\tJSAdvisoryStreamLeaderElectedPre = \"$JS.EVENT.ADVISORY.STREAM.LEADER_ELECTED\"\n\n\t// JSAdvisoryStreamQuorumLostPre notification that a stream and its consumers are stalled.\n\tJSAdvisoryStreamQuorumLostPre = \"$JS.EVENT.ADVISORY.STREAM.QUORUM_LOST\"\n\n\t// JSAdvisoryConsumerLeaderElectedPre notification that a replicated consumer has elected a leader.\n\tJSAdvisoryConsumerLeaderElectedPre = \"$JS.EVENT.ADVISORY.CONSUMER.LEADER_ELECTED\"\n\n\t// JSAdvisoryConsumerQuorumLostPre notification that a consumer is stalled.\n\tJSAdvisoryConsumerQuorumLostPre = \"$JS.EVENT.ADVISORY.CONSUMER.QUORUM_LOST\"\n\n\t// JSAdvisoryServerOutOfStorage notification that a server has no more storage.\n\tJSAdvisoryServerOutOfStorage = \"$JS.EVENT.ADVISORY.SERVER.OUT_OF_STORAGE\"\n\n\t// JSAdvisoryServerRemoved notification that a server has been removed from the system.\n\tJSAdvisoryServerRemoved = \"$JS.EVENT.ADVISORY.SERVER.REMOVED\"\n\n\t// JSAdvisoryAPILimitReached notification that a server has reached the JS API hard limit.\n\tJSAdvisoryAPILimitReached = \"$JS.EVENT.ADVISORY.API.LIMIT_REACHED\"\n\n\t// JSAuditAdvisory is a notification about JetStream API access.\n\t// FIXME - Add in details about who..\n\tJSAuditAdvisory = \"$JS.EVENT.ADVISORY.API\"\n)\n\nvar denyAllClientJs = []string{jsAllAPI, \"$KV.>\", \"$OBJ.>\"}\nvar denyAllJs = []string{jscAllSubj, raftAllSubj, jsAllAPI, \"$KV.>\", \"$OBJ.>\"}\n\nfunc generateJSMappingTable(domain string) map[string]string {\n\tmappings := map[string]string{}\n\t// This set of mappings is very very very ugly.\n\t// It is a consequence of what we defined the domain prefix to be \"$JS.domain.API\" and it's mapping to \"$JS.API\"\n\t// For optics $KV and $OBJ where made to be independent subject spaces.\n\t// As materialized views of JS, they did not simply extend that subject space to say \"$JS.API.KV\" \"$JS.API.OBJ\"\n\t// This is very unfortunate!!!\n\t// Furthermore, it seemed bad to require different domain prefixes for JS/KV/OBJ.\n\t// Especially since the actual API for say KV, does use stream create from JS.\n\t// To avoid overlaps KV and OBJ views append the prefix to their API.\n\t// (Replacing $KV with the prefix allows users to create collisions with say the bucket name)\n\t// This mapping therefore needs to have extra token so that the mapping can properly discern between $JS, $KV, $OBJ\n\tfor srcMappingSuffix, to := range map[string]string{\n\t\t\"INFO\":       JSApiAccountInfo,\n\t\t\"STREAM.>\":   \"$JS.API.STREAM.>\",\n\t\t\"CONSUMER.>\": \"$JS.API.CONSUMER.>\",\n\t\t\"DIRECT.>\":   \"$JS.API.DIRECT.>\",\n\t\t\"META.>\":     \"$JS.API.META.>\",\n\t\t\"SERVER.>\":   \"$JS.API.SERVER.>\",\n\t\t\"ACCOUNT.>\":  \"$JS.API.ACCOUNT.>\",\n\t\t\"$KV.>\":      \"$KV.>\",\n\t\t\"$OBJ.>\":     \"$OBJ.>\",\n\t} {\n\t\tmappings[fmt.Sprintf(\"$JS.%s.API.%s\", domain, srcMappingSuffix)] = to\n\t}\n\treturn mappings\n}\n\n// JSMaxDescription is the maximum description length for streams and consumers.\nconst JSMaxDescriptionLen = 4 * 1024\n\n// JSMaxMetadataLen is the maximum length for streams and consumers metadata map.\n// It's calculated by summing length of all keys and values.\nconst JSMaxMetadataLen = 128 * 1024\n\n// JSMaxNameLen is the maximum name lengths for streams, consumers and templates.\n// Picked 255 as it seems to be a widely used file name limit\nconst JSMaxNameLen = 255\n\n// JSDefaultRequestQueueLimit is the default number of entries that we will\n// put on the global request queue before we react.\nconst JSDefaultRequestQueueLimit = 10_000\n\n// Responses for API calls.\n\n// ApiResponse is a standard response from the JetStream JSON API\ntype ApiResponse struct {\n\tType  string    `json:\"type\"`\n\tError *ApiError `json:\"error,omitempty\"`\n}\n\nconst JSApiSystemResponseType = \"io.nats.jetstream.api.v1.system_response\"\n\n// When passing back to the clients generalize store failures.\nvar (\n\terrStreamStoreFailed   = errors.New(\"error creating store for stream\")\n\terrConsumerStoreFailed = errors.New(\"error creating store for consumer\")\n)\n\n// ToError checks if the response has a error and if it does converts it to an error avoiding\n// the pitfalls described by https://yourbasic.org/golang/gotcha-why-nil-error-not-equal-nil/\nfunc (r *ApiResponse) ToError() error {\n\tif r.Error == nil {\n\t\treturn nil\n\t}\n\n\treturn r.Error\n}\n\nconst JSApiOverloadedType = \"io.nats.jetstream.api.v1.system_overloaded\"\n\n// ApiPaged includes variables used to create paged responses from the JSON API\ntype ApiPaged struct {\n\tTotal  int `json:\"total\"`\n\tOffset int `json:\"offset\"`\n\tLimit  int `json:\"limit\"`\n}\n\n// ApiPagedRequest includes parameters allowing specific pages to be requests from APIs responding with ApiPaged\ntype ApiPagedRequest struct {\n\tOffset int `json:\"offset\"`\n}\n\n// JSApiAccountInfoResponse reports back information on jetstream for this account.\ntype JSApiAccountInfoResponse struct {\n\tApiResponse\n\t*JetStreamAccountStats\n}\n\nconst JSApiAccountInfoResponseType = \"io.nats.jetstream.api.v1.account_info_response\"\n\n// JSApiStreamCreateResponse stream creation.\ntype JSApiStreamCreateResponse struct {\n\tApiResponse\n\t*StreamInfo\n\tDidCreate bool `json:\"did_create,omitempty\"`\n}\n\nconst JSApiStreamCreateResponseType = \"io.nats.jetstream.api.v1.stream_create_response\"\n\n// JSApiStreamDeleteResponse stream removal.\ntype JSApiStreamDeleteResponse struct {\n\tApiResponse\n\tSuccess bool `json:\"success,omitempty\"`\n}\n\nconst JSApiStreamDeleteResponseType = \"io.nats.jetstream.api.v1.stream_delete_response\"\n\n// JSMaxSubjectDetails The limit of the number of subject details we will send in a stream info response.\nconst JSMaxSubjectDetails = 100_000\n\ntype JSApiStreamInfoRequest struct {\n\tApiPagedRequest\n\tDeletedDetails bool   `json:\"deleted_details,omitempty\"`\n\tSubjectsFilter string `json:\"subjects_filter,omitempty\"`\n}\n\ntype JSApiStreamInfoResponse struct {\n\tApiResponse\n\tApiPaged\n\t*StreamInfo\n}\n\nconst JSApiStreamInfoResponseType = \"io.nats.jetstream.api.v1.stream_info_response\"\n\n// JSApiNamesLimit is the maximum entries we will return for streams or consumers lists.\n// TODO(dlc) - with header or request support could request chunked response.\nconst JSApiNamesLimit = 1024\nconst JSApiListLimit = 256\n\ntype JSApiStreamNamesRequest struct {\n\tApiPagedRequest\n\t// These are filters that can be applied to the list.\n\tSubject string `json:\"subject,omitempty\"`\n}\n\n// JSApiStreamNamesResponse list of streams.\n// A nil request is valid and means all streams.\ntype JSApiStreamNamesResponse struct {\n\tApiResponse\n\tApiPaged\n\tStreams []string `json:\"streams\"`\n}\n\nconst JSApiStreamNamesResponseType = \"io.nats.jetstream.api.v1.stream_names_response\"\n\ntype JSApiStreamListRequest struct {\n\tApiPagedRequest\n\t// These are filters that can be applied to the list.\n\tSubject string `json:\"subject,omitempty\"`\n}\n\n// JSApiStreamListResponse list of detailed stream information.\n// A nil request is valid and means all streams.\ntype JSApiStreamListResponse struct {\n\tApiResponse\n\tApiPaged\n\tStreams []*StreamInfo `json:\"streams\"`\n\tMissing []string      `json:\"missing,omitempty\"`\n}\n\nconst JSApiStreamListResponseType = \"io.nats.jetstream.api.v1.stream_list_response\"\n\n// JSApiStreamPurgeRequest is optional request information to the purge API.\n// Subject will filter the purge request to only messages that match the subject, which can have wildcards.\n// Sequence will purge up to but not including this sequence and can be combined with subject filtering.\n// Keep will specify how many messages to keep. This can also be combined with subject filtering.\n// Note that Sequence and Keep are mutually exclusive, so both can not be set at the same time.\ntype JSApiStreamPurgeRequest struct {\n\t// Purge up to but not including sequence.\n\tSequence uint64 `json:\"seq,omitempty\"`\n\t// Subject to match against messages for the purge command.\n\tSubject string `json:\"filter,omitempty\"`\n\t// Number of messages to keep.\n\tKeep uint64 `json:\"keep,omitempty\"`\n}\n\ntype JSApiStreamPurgeResponse struct {\n\tApiResponse\n\tSuccess bool   `json:\"success,omitempty\"`\n\tPurged  uint64 `json:\"purged\"`\n}\n\nconst JSApiStreamPurgeResponseType = \"io.nats.jetstream.api.v1.stream_purge_response\"\n\ntype JSApiConsumerUnpinRequest struct {\n\tGroup string `json:\"group\"`\n}\n\ntype JSApiConsumerUnpinResponse struct {\n\tApiResponse\n}\n\nconst JSApiConsumerUnpinResponseType = \"io.nats.jetstream.api.v1.consumer_unpin_response\"\n\n// JSApiStreamUpdateResponse for updating a stream.\ntype JSApiStreamUpdateResponse struct {\n\tApiResponse\n\t*StreamInfo\n}\n\nconst JSApiStreamUpdateResponseType = \"io.nats.jetstream.api.v1.stream_update_response\"\n\n// JSApiMsgDeleteRequest delete message request.\ntype JSApiMsgDeleteRequest struct {\n\tSeq     uint64 `json:\"seq\"`\n\tNoErase bool   `json:\"no_erase,omitempty\"`\n}\n\ntype JSApiMsgDeleteResponse struct {\n\tApiResponse\n\tSuccess bool `json:\"success,omitempty\"`\n}\n\nconst JSApiMsgDeleteResponseType = \"io.nats.jetstream.api.v1.stream_msg_delete_response\"\n\ntype JSApiStreamSnapshotRequest struct {\n\t// Subject to deliver the chunks to for the snapshot.\n\tDeliverSubject string `json:\"deliver_subject\"`\n\t// Do not include consumers in the snapshot.\n\tNoConsumers bool `json:\"no_consumers,omitempty\"`\n\t// Optional chunk size preference.\n\t// Best to just let server select.\n\tChunkSize int `json:\"chunk_size,omitempty\"`\n\t// Check all message's checksums prior to snapshot.\n\tCheckMsgs bool `json:\"jsck,omitempty\"`\n}\n\n// JSApiStreamSnapshotResponse is the direct response to the snapshot request.\ntype JSApiStreamSnapshotResponse struct {\n\tApiResponse\n\t// Configuration of the given stream.\n\tConfig *StreamConfig `json:\"config,omitempty\"`\n\t// Current State for the given stream.\n\tState *StreamState `json:\"state,omitempty\"`\n}\n\nconst JSApiStreamSnapshotResponseType = \"io.nats.jetstream.api.v1.stream_snapshot_response\"\n\n// JSApiStreamRestoreRequest is the required restore request.\ntype JSApiStreamRestoreRequest struct {\n\t// Configuration of the given stream.\n\tConfig StreamConfig `json:\"config\"`\n\t// Current State for the given stream.\n\tState StreamState `json:\"state\"`\n}\n\n// JSApiStreamRestoreResponse is the direct response to the restore request.\ntype JSApiStreamRestoreResponse struct {\n\tApiResponse\n\t// Subject to deliver the chunks to for the snapshot restore.\n\tDeliverSubject string `json:\"deliver_subject\"`\n}\n\nconst JSApiStreamRestoreResponseType = \"io.nats.jetstream.api.v1.stream_restore_response\"\n\n// JSApiStreamRemovePeerRequest is the required remove peer request.\ntype JSApiStreamRemovePeerRequest struct {\n\t// Server name of the peer to be removed.\n\tPeer string `json:\"peer\"`\n}\n\n// JSApiStreamRemovePeerResponse is the response to a remove peer request.\ntype JSApiStreamRemovePeerResponse struct {\n\tApiResponse\n\tSuccess bool `json:\"success,omitempty\"`\n}\n\nconst JSApiStreamRemovePeerResponseType = \"io.nats.jetstream.api.v1.stream_remove_peer_response\"\n\n// JSApiStreamLeaderStepDownResponse is the response to a leader stepdown request.\ntype JSApiStreamLeaderStepDownResponse struct {\n\tApiResponse\n\tSuccess bool `json:\"success,omitempty\"`\n}\n\nconst JSApiStreamLeaderStepDownResponseType = \"io.nats.jetstream.api.v1.stream_leader_stepdown_response\"\n\n// JSApiConsumerLeaderStepDownResponse is the response to a consumer leader stepdown request.\ntype JSApiConsumerLeaderStepDownResponse struct {\n\tApiResponse\n\tSuccess bool `json:\"success,omitempty\"`\n}\n\nconst JSApiConsumerLeaderStepDownResponseType = \"io.nats.jetstream.api.v1.consumer_leader_stepdown_response\"\n\n// JSApiLeaderStepdownRequest allows placement control over the meta leader placement.\ntype JSApiLeaderStepdownRequest struct {\n\tPlacement *Placement `json:\"placement,omitempty\"`\n}\n\n// JSApiLeaderStepDownResponse is the response to a meta leader stepdown request.\ntype JSApiLeaderStepDownResponse struct {\n\tApiResponse\n\tSuccess bool `json:\"success,omitempty\"`\n}\n\nconst JSApiLeaderStepDownResponseType = \"io.nats.jetstream.api.v1.meta_leader_stepdown_response\"\n\n// JSApiMetaServerRemoveRequest will remove a peer from the meta group.\ntype JSApiMetaServerRemoveRequest struct {\n\t// Server name of the peer to be removed.\n\tServer string `json:\"peer\"`\n\t// Peer ID of the peer to be removed. If specified this is used\n\t// instead of the server name.\n\tPeer string `json:\"peer_id,omitempty\"`\n}\n\n// JSApiMetaServerRemoveResponse is the response to a peer removal request in the meta group.\ntype JSApiMetaServerRemoveResponse struct {\n\tApiResponse\n\tSuccess bool `json:\"success,omitempty\"`\n}\n\nconst JSApiMetaServerRemoveResponseType = \"io.nats.jetstream.api.v1.meta_server_remove_response\"\n\n// JSApiMetaServerStreamMoveRequest will move a stream on a server to another\n// response to this will come as JSApiStreamUpdateResponse/JSApiStreamUpdateResponseType\ntype JSApiMetaServerStreamMoveRequest struct {\n\t// Server name of the peer to be evacuated.\n\tServer string `json:\"server,omitempty\"`\n\t// Cluster the server is in\n\tCluster string `json:\"cluster,omitempty\"`\n\t// Domain the sever is in\n\tDomain string `json:\"domain,omitempty\"`\n\t// Ephemeral placement tags for the move\n\tTags []string `json:\"tags,omitempty\"`\n}\n\nconst JSApiAccountPurgeResponseType = \"io.nats.jetstream.api.v1.account_purge_response\"\n\n// JSApiAccountPurgeResponse is the response to a purge request in the meta group.\ntype JSApiAccountPurgeResponse struct {\n\tApiResponse\n\tInitiated bool `json:\"initiated,omitempty\"`\n}\n\n// JSApiMsgGetRequest get a message request.\ntype JSApiMsgGetRequest struct {\n\tSeq     uint64 `json:\"seq,omitempty\"`\n\tLastFor string `json:\"last_by_subj,omitempty\"`\n\tNextFor string `json:\"next_by_subj,omitempty\"`\n\n\t// Batch support. Used to request more then one msg at a time.\n\t// Can be used with simple starting seq, but also NextFor with wildcards.\n\tBatch int `json:\"batch,omitempty\"`\n\t// This will make sure we limit how much data we blast out. If not set we will\n\t// inherit the slow consumer default max setting of the server. Default is MAX_PENDING_SIZE.\n\tMaxBytes int `json:\"max_bytes,omitempty\"`\n\t// Return messages as of this start time.\n\tStartTime *time.Time `json:\"start_time,omitempty\"`\n\n\t// Multiple response support. Will get the last msgs matching the subjects. These can include wildcards.\n\tMultiLastFor []string `json:\"multi_last,omitempty\"`\n\t// Only return messages up to this sequence. If not set, will be last sequence for the stream.\n\tUpToSeq uint64 `json:\"up_to_seq,omitempty\"`\n\t// Only return messages up to this time.\n\tUpToTime *time.Time `json:\"up_to_time,omitempty\"`\n}\n\ntype JSApiMsgGetResponse struct {\n\tApiResponse\n\tMessage *StoredMsg `json:\"message,omitempty\"`\n}\n\nconst JSApiMsgGetResponseType = \"io.nats.jetstream.api.v1.stream_msg_get_response\"\n\n// JSWaitQueueDefaultMax is the default max number of outstanding requests for pull consumers.\nconst JSWaitQueueDefaultMax = 512\n\ntype JSApiConsumerCreateResponse struct {\n\tApiResponse\n\t*ConsumerInfo\n}\n\nconst JSApiConsumerCreateResponseType = \"io.nats.jetstream.api.v1.consumer_create_response\"\n\ntype JSApiConsumerDeleteResponse struct {\n\tApiResponse\n\tSuccess bool `json:\"success,omitempty\"`\n}\n\nconst JSApiConsumerDeleteResponseType = \"io.nats.jetstream.api.v1.consumer_delete_response\"\n\ntype JSApiConsumerPauseRequest struct {\n\tPauseUntil time.Time `json:\"pause_until,omitempty\"`\n}\n\nconst JSApiConsumerPauseResponseType = \"io.nats.jetstream.api.v1.consumer_pause_response\"\n\ntype JSApiConsumerPauseResponse struct {\n\tApiResponse\n\tPaused         bool          `json:\"paused\"`\n\tPauseUntil     time.Time     `json:\"pause_until\"`\n\tPauseRemaining time.Duration `json:\"pause_remaining,omitempty\"`\n}\n\ntype JSApiConsumerInfoResponse struct {\n\tApiResponse\n\t*ConsumerInfo\n}\n\nconst JSApiConsumerInfoResponseType = \"io.nats.jetstream.api.v1.consumer_info_response\"\n\ntype JSApiConsumersRequest struct {\n\tApiPagedRequest\n}\n\ntype JSApiConsumerNamesResponse struct {\n\tApiResponse\n\tApiPaged\n\tConsumers []string `json:\"consumers\"`\n}\n\nconst JSApiConsumerNamesResponseType = \"io.nats.jetstream.api.v1.consumer_names_response\"\n\ntype JSApiConsumerListResponse struct {\n\tApiResponse\n\tApiPaged\n\tConsumers []*ConsumerInfo `json:\"consumers\"`\n\tMissing   []string        `json:\"missing,omitempty\"`\n}\n\nconst JSApiConsumerListResponseType = \"io.nats.jetstream.api.v1.consumer_list_response\"\n\n// JSApiConsumerGetNextRequest is for getting next messages for pull based consumers.\ntype JSApiConsumerGetNextRequest struct {\n\tExpires   time.Duration `json:\"expires,omitempty\"`\n\tBatch     int           `json:\"batch,omitempty\"`\n\tMaxBytes  int           `json:\"max_bytes,omitempty\"`\n\tNoWait    bool          `json:\"no_wait,omitempty\"`\n\tHeartbeat time.Duration `json:\"idle_heartbeat,omitempty\"`\n\tPriorityGroup\n}\n\n// JSApiStreamTemplateCreateResponse for creating templates.\ntype JSApiStreamTemplateCreateResponse struct {\n\tApiResponse\n\t*StreamTemplateInfo\n}\n\nconst JSApiStreamTemplateCreateResponseType = \"io.nats.jetstream.api.v1.stream_template_create_response\"\n\ntype JSApiStreamTemplateDeleteResponse struct {\n\tApiResponse\n\tSuccess bool `json:\"success,omitempty\"`\n}\n\nconst JSApiStreamTemplateDeleteResponseType = \"io.nats.jetstream.api.v1.stream_template_delete_response\"\n\n// JSApiStreamTemplateInfoResponse for information about stream templates.\ntype JSApiStreamTemplateInfoResponse struct {\n\tApiResponse\n\t*StreamTemplateInfo\n}\n\nconst JSApiStreamTemplateInfoResponseType = \"io.nats.jetstream.api.v1.stream_template_info_response\"\n\ntype JSApiStreamTemplatesRequest struct {\n\tApiPagedRequest\n}\n\n// JSApiStreamTemplateNamesResponse list of templates\ntype JSApiStreamTemplateNamesResponse struct {\n\tApiResponse\n\tApiPaged\n\tTemplates []string `json:\"streams\"`\n}\n\nconst JSApiStreamTemplateNamesResponseType = \"io.nats.jetstream.api.v1.stream_template_names_response\"\n\n// Structure that holds state for a JetStream API request that is processed\n// in a separate long-lived go routine. This is to avoid possibly blocking\n// ROUTE and GATEWAY connections.\ntype jsAPIRoutedReq struct {\n\tjsub    *subscription\n\tsub     *subscription\n\tacc     *Account\n\tsubject string\n\treply   string\n\tmsg     []byte\n\tpa      pubArg\n}\n\nfunc (js *jetStream) apiDispatch(sub *subscription, c *client, acc *Account, subject, reply string, rmsg []byte) {\n\t// Ignore system level directives meta stepdown and peer remove requests here.\n\tif subject == JSApiLeaderStepDown ||\n\t\tsubject == JSApiRemoveServer ||\n\t\tstrings.HasPrefix(subject, jsAPIAccountPre) {\n\t\treturn\n\t}\n\t// No lock needed, those are immutable.\n\ts, rr := js.srv, js.apiSubs.Match(subject)\n\n\thdr, msg := c.msgParts(rmsg)\n\tif len(sliceHeader(ClientInfoHdr, hdr)) == 0 {\n\t\t// Check if this is the system account. We will let these through for the account info only.\n\t\tsacc := s.SystemAccount()\n\t\tif sacc != acc {\n\t\t\treturn\n\t\t}\n\t\tif subject != JSApiAccountInfo {\n\t\t\t// Only respond from the initial server entry to the NATS system.\n\t\t\tif c.kind == CLIENT || c.kind == LEAF {\n\t\t\t\tvar resp = ApiResponse{\n\t\t\t\t\tType:  JSApiSystemResponseType,\n\t\t\t\t\tError: NewJSNotEnabledForAccountError(),\n\t\t\t\t}\n\t\t\t\ts.sendAPIErrResponse(nil, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\t}\n\t\t\treturn\n\t\t}\n\t}\n\n\t// Short circuit for no interest.\n\tif len(rr.psubs)+len(rr.qsubs) == 0 {\n\t\tif (c.kind == CLIENT || c.kind == LEAF) && acc != s.SystemAccount() {\n\t\t\tci, acc, _, _, _ := s.getRequestInfo(c, rmsg)\n\t\t\tvar resp = ApiResponse{\n\t\t\t\tType:  JSApiSystemResponseType,\n\t\t\t\tError: NewJSBadRequestError(),\n\t\t\t}\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t}\n\t\treturn\n\t}\n\n\t// We should only have psubs and only 1 per result.\n\tif len(rr.psubs) != 1 {\n\t\ts.Warnf(\"Malformed JetStream API Request: [%s] %q\", subject, rmsg)\n\t\tif c.kind == CLIENT || c.kind == LEAF {\n\t\t\tci, acc, _, _, _ := s.getRequestInfo(c, rmsg)\n\t\t\tvar resp = ApiResponse{\n\t\t\t\tType:  JSApiSystemResponseType,\n\t\t\t\tError: NewJSBadRequestError(),\n\t\t\t}\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t}\n\t\treturn\n\t}\n\tjsub := rr.psubs[0]\n\n\t// If this is directly from a client connection ok to do in place.\n\tif c.kind != ROUTER && c.kind != GATEWAY && c.kind != LEAF {\n\t\tstart := time.Now()\n\t\tjsub.icb(sub, c, acc, subject, reply, rmsg)\n\t\tif dur := time.Since(start); dur >= readLoopReportThreshold {\n\t\t\ts.Warnf(\"Internal subscription on %q took too long: %v\", subject, dur)\n\t\t}\n\t\treturn\n\t}\n\n\t// If we are here we have received this request over a non-client connection.\n\t// We need to make sure not to block. We will send the request to a long-lived\n\t// pool of go routines.\n\n\t// Increment inflight. Do this before queueing.\n\tatomic.AddInt64(&js.apiInflight, 1)\n\n\t// Copy the state. Note the JSAPI only uses the hdr index to piece apart the\n\t// header from the msg body. No other references are needed.\n\t// Check pending and warn if getting backed up.\n\tpending, _ := s.jsAPIRoutedReqs.push(&jsAPIRoutedReq{jsub, sub, acc, subject, reply, copyBytes(rmsg), c.pa})\n\tlimit := atomic.LoadInt64(&js.queueLimit)\n\tif pending >= int(limit) {\n\t\ts.rateLimitFormatWarnf(\"JetStream API queue limit reached, dropping %d requests\", pending)\n\t\tdrained := int64(s.jsAPIRoutedReqs.drain())\n\t\tatomic.AddInt64(&js.apiInflight, -drained)\n\n\t\ts.publishAdvisory(nil, JSAdvisoryAPILimitReached, JSAPILimitReachedAdvisory{\n\t\t\tTypedEvent: TypedEvent{\n\t\t\t\tType: JSAPILimitReachedAdvisoryType,\n\t\t\t\tID:   nuid.Next(),\n\t\t\t\tTime: time.Now().UTC(),\n\t\t\t},\n\t\t\tServer:  s.Name(),\n\t\t\tDomain:  js.config.Domain,\n\t\t\tDropped: drained,\n\t\t})\n\t}\n}\n\nfunc (s *Server) processJSAPIRoutedRequests() {\n\tdefer s.grWG.Done()\n\n\ts.mu.RLock()\n\tqueue := s.jsAPIRoutedReqs\n\tclient := &client{srv: s, kind: JETSTREAM}\n\ts.mu.RUnlock()\n\n\tjs := s.getJetStream()\n\n\tfor {\n\t\tselect {\n\t\tcase <-queue.ch:\n\t\t\t// Only pop one item at a time here, otherwise if the system is recovering\n\t\t\t// from queue buildup, then one worker will pull off all the tasks and the\n\t\t\t// others will be starved of work.\n\t\t\tfor r, ok := queue.popOne(); ok && r != nil; r, ok = queue.popOne() {\n\t\t\t\tclient.pa = r.pa\n\t\t\t\tstart := time.Now()\n\t\t\t\tr.jsub.icb(r.sub, client, r.acc, r.subject, r.reply, r.msg)\n\t\t\t\tif dur := time.Since(start); dur >= readLoopReportThreshold {\n\t\t\t\t\ts.Warnf(\"Internal subscription on %q took too long: %v\", r.subject, dur)\n\t\t\t\t}\n\t\t\t\tatomic.AddInt64(&js.apiInflight, -1)\n\t\t\t}\n\t\tcase <-s.quitCh:\n\t\t\treturn\n\t\t}\n\t}\n}\n\nfunc (s *Server) setJetStreamExportSubs() error {\n\tjs := s.getJetStream()\n\tif js == nil {\n\t\treturn NewJSNotEnabledError()\n\t}\n\n\t// Start the go routine that will process API requests received by the\n\t// subscription below when they are coming from routes, etc..\n\tconst maxProcs = 16\n\tmp := runtime.GOMAXPROCS(0)\n\t// Cap at 16 max for now on larger core setups.\n\tif mp > maxProcs {\n\t\tmp = maxProcs\n\t}\n\ts.jsAPIRoutedReqs = newIPQueue[*jsAPIRoutedReq](s, \"Routed JS API Requests\")\n\tfor i := 0; i < mp; i++ {\n\t\ts.startGoRoutine(s.processJSAPIRoutedRequests)\n\t}\n\n\t// This is the catch all now for all JetStream API calls.\n\tif _, err := s.sysSubscribe(jsAllAPI, js.apiDispatch); err != nil {\n\t\treturn err\n\t}\n\n\tif err := s.SystemAccount().AddServiceExport(jsAllAPI, nil); err != nil {\n\t\ts.Warnf(\"Error setting up jetstream service exports: %v\", err)\n\t\treturn err\n\t}\n\n\t// API handles themselves.\n\tpairs := []struct {\n\t\tsubject string\n\t\thandler msgHandler\n\t}{\n\t\t{JSApiAccountInfo, s.jsAccountInfoRequest},\n\t\t{JSApiTemplateCreate, s.jsTemplateCreateRequest},\n\t\t{JSApiTemplates, s.jsTemplateNamesRequest},\n\t\t{JSApiTemplateInfo, s.jsTemplateInfoRequest},\n\t\t{JSApiTemplateDelete, s.jsTemplateDeleteRequest},\n\t\t{JSApiStreamCreate, s.jsStreamCreateRequest},\n\t\t{JSApiStreamUpdate, s.jsStreamUpdateRequest},\n\t\t{JSApiStreams, s.jsStreamNamesRequest},\n\t\t{JSApiStreamList, s.jsStreamListRequest},\n\t\t{JSApiStreamInfo, s.jsStreamInfoRequest},\n\t\t{JSApiStreamDelete, s.jsStreamDeleteRequest},\n\t\t{JSApiStreamPurge, s.jsStreamPurgeRequest},\n\t\t{JSApiStreamSnapshot, s.jsStreamSnapshotRequest},\n\t\t{JSApiStreamRestore, s.jsStreamRestoreRequest},\n\t\t{JSApiStreamRemovePeer, s.jsStreamRemovePeerRequest},\n\t\t{JSApiStreamLeaderStepDown, s.jsStreamLeaderStepDownRequest},\n\t\t{JSApiConsumerLeaderStepDown, s.jsConsumerLeaderStepDownRequest},\n\t\t{JSApiMsgDelete, s.jsMsgDeleteRequest},\n\t\t{JSApiMsgGet, s.jsMsgGetRequest},\n\t\t{JSApiConsumerCreateEx, s.jsConsumerCreateRequest},\n\t\t{JSApiConsumerCreate, s.jsConsumerCreateRequest},\n\t\t{JSApiDurableCreate, s.jsConsumerCreateRequest},\n\t\t{JSApiConsumers, s.jsConsumerNamesRequest},\n\t\t{JSApiConsumerList, s.jsConsumerListRequest},\n\t\t{JSApiConsumerInfo, s.jsConsumerInfoRequest},\n\t\t{JSApiConsumerDelete, s.jsConsumerDeleteRequest},\n\t\t{JSApiConsumerPause, s.jsConsumerPauseRequest},\n\t\t{JSApiConsumerUnpin, s.jsConsumerUnpinRequest},\n\t}\n\n\tjs.mu.Lock()\n\tdefer js.mu.Unlock()\n\n\tfor _, p := range pairs {\n\t\tsub := &subscription{subject: []byte(p.subject), icb: p.handler}\n\t\tif err := js.apiSubs.Insert(sub); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc (s *Server) sendAPIResponse(ci *ClientInfo, acc *Account, subject, reply, request, response string) {\n\tacc.trackAPI()\n\tif reply != _EMPTY_ {\n\t\ts.sendInternalAccountMsg(nil, reply, response)\n\t}\n\ts.sendJetStreamAPIAuditAdvisory(ci, acc, subject, request, response)\n}\n\nfunc (s *Server) sendAPIErrResponse(ci *ClientInfo, acc *Account, subject, reply, request, response string) {\n\tacc.trackAPIErr()\n\tif reply != _EMPTY_ {\n\t\ts.sendInternalAccountMsg(nil, reply, response)\n\t}\n\ts.sendJetStreamAPIAuditAdvisory(ci, acc, subject, request, response)\n}\n\nconst errRespDelay = 500 * time.Millisecond\n\ntype delayedAPIResponse struct {\n\tci       *ClientInfo\n\tacc      *Account\n\tsubject  string\n\treply    string\n\trequest  string\n\tresponse string\n\trg       *raftGroup\n\tdeadline time.Time\n\tnext     *delayedAPIResponse\n}\n\n// Add `r` in the list that is maintained ordered by the `delayedAPIResponse.deadline` time.\nfunc addDelayedResponse(head, tail **delayedAPIResponse, r *delayedAPIResponse) {\n\t// Check if list empty.\n\tif *head == nil {\n\t\t*head, *tail = r, r\n\t\treturn\n\t}\n\t// Check if it should be added at the end, which is if after or equal to the tail.\n\tif r.deadline.After((*tail).deadline) || r.deadline.Equal((*tail).deadline) {\n\t\t(*tail).next, *tail = r, r\n\t\treturn\n\t}\n\t// Find its spot in the list.\n\tvar prev *delayedAPIResponse\n\tfor c := *head; c != nil; c = c.next {\n\t\t// We insert only if we are stricly before the current `c`.\n\t\tif r.deadline.Before(c.deadline) {\n\t\t\tr.next = c\n\t\t\tif prev != nil {\n\t\t\t\tprev.next = r\n\t\t\t} else {\n\t\t\t\t*head = r\n\t\t\t}\n\t\t\treturn\n\t\t}\n\t\tprev = c\n\t}\n}\n\nfunc (s *Server) delayedAPIResponder() {\n\tdefer s.grWG.Done()\n\tvar (\n\t\thead, tail *delayedAPIResponse // Linked list.\n\t\tr          *delayedAPIResponse // Updated by calling next().\n\t\trqch       <-chan struct{}     // Quit channel of the Raft group (if present).\n\t\ttm         = time.NewTimer(time.Hour)\n\t)\n\tnext := func() {\n\t\tr, rqch = nil, nil\n\t\t// Check that JetStream is still on. Do not exit the go routine\n\t\t// since JS can be enabled/disabled. The go routine will exit\n\t\t// only if server is shutdown.\n\t\tjs := s.getJetStream()\n\t\tif js == nil {\n\t\t\t// Reset head and tail here. Also drain the ipQueue.\n\t\t\thead, tail = nil, nil\n\t\t\ts.delayedAPIResponses.drain()\n\t\t\t// Fall back into next \"if\" that resets timer.\n\t\t}\n\t\t// If there are no delayed messages then delay the timer for\n\t\t// a while.\n\t\tif head == nil {\n\t\t\ttm.Reset(time.Hour)\n\t\t\treturn\n\t\t}\n\t\t// Get the first expected message and then reset the timer.\n\t\tr = head\n\t\tjs.mu.RLock()\n\t\tif r.rg != nil && r.rg.node != nil {\n\t\t\t// If there's an attached Raft group to the delayed response\n\t\t\t// then pull out the quit channel, so that we don't bother\n\t\t\t// sending responses for entities which are now no longer\n\t\t\t// running.\n\t\t\trqch = r.rg.node.QuitC()\n\t\t}\n\t\tjs.mu.RUnlock()\n\t\ttm.Reset(time.Until(r.deadline))\n\t}\n\tpop := func() {\n\t\tif head == nil {\n\t\t\treturn\n\t\t}\n\t\thead = head.next\n\t\tif head == nil {\n\t\t\ttail = nil\n\t\t}\n\t}\n\tfor {\n\t\tselect {\n\t\tcase <-s.delayedAPIResponses.ch:\n\t\t\tv, ok := s.delayedAPIResponses.popOne()\n\t\t\tif !ok {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\t// Add it to the list, and if ends up being the head, set things up.\n\t\t\taddDelayedResponse(&head, &tail, v)\n\t\t\tif v == head {\n\t\t\t\tnext()\n\t\t\t}\n\t\tcase <-s.quitCh:\n\t\t\treturn\n\t\tcase <-rqch:\n\t\t\t// If we were the head, drop and setup things for next.\n\t\t\tif r != nil && r == head {\n\t\t\t\tpop()\n\t\t\t}\n\t\t\tnext()\n\t\tcase <-tm.C:\n\t\t\tif r != nil {\n\t\t\t\ts.sendAPIErrResponse(r.ci, r.acc, r.subject, r.reply, r.request, r.response)\n\t\t\t\tpop()\n\t\t\t}\n\t\t\tnext()\n\t\t}\n\t}\n}\n\nfunc (s *Server) sendDelayedAPIErrResponse(ci *ClientInfo, acc *Account, subject, reply, request, response string, rg *raftGroup, duration time.Duration) {\n\ts.delayedAPIResponses.push(&delayedAPIResponse{\n\t\tci, acc, subject, reply, request, response, rg, time.Now().Add(duration), nil,\n\t})\n}\n\nfunc (s *Server) getRequestInfo(c *client, raw []byte) (pci *ClientInfo, acc *Account, hdr, msg []byte, err error) {\n\thdr, msg = c.msgParts(raw)\n\tvar ci ClientInfo\n\n\tif len(hdr) > 0 {\n\t\tif err := json.Unmarshal(sliceHeader(ClientInfoHdr, hdr), &ci); err != nil {\n\t\t\treturn nil, nil, nil, nil, err\n\t\t}\n\t}\n\n\tif ci.Service != _EMPTY_ {\n\t\tacc, _ = s.LookupAccount(ci.Service)\n\t} else if ci.Account != _EMPTY_ {\n\t\tacc, _ = s.LookupAccount(ci.Account)\n\t} else {\n\t\t// Direct $SYS access.\n\t\tacc = c.acc\n\t\tif acc == nil {\n\t\t\tacc = s.SystemAccount()\n\t\t}\n\t}\n\tif acc == nil {\n\t\treturn nil, nil, nil, nil, ErrMissingAccount\n\t}\n\treturn &ci, acc, hdr, msg, nil\n}\n\nfunc (s *Server) unmarshalRequest(c *client, acc *Account, subject string, msg []byte, v any) error {\n\tdecoder := json.NewDecoder(bytes.NewReader(msg))\n\tdecoder.DisallowUnknownFields()\n\n\tfor {\n\t\tif err := decoder.Decode(v); err != nil {\n\t\t\tif err == io.EOF {\n\t\t\t\treturn nil\n\t\t\t}\n\n\t\t\tvar syntaxErr *json.SyntaxError\n\t\t\tif errors.As(err, &syntaxErr) {\n\t\t\t\terr = fmt.Errorf(\"%w at offset %d\", err, syntaxErr.Offset)\n\t\t\t}\n\n\t\t\tc.RateLimitWarnf(\"Invalid JetStream request '%s > %s': %s\", acc, subject, err)\n\n\t\t\tif s.JetStreamConfig().Strict {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t\treturn json.Unmarshal(msg, v)\n\t\t}\n\t}\n}\n\nfunc (a *Account) trackAPI() {\n\ta.mu.RLock()\n\tjsa := a.js\n\ta.mu.RUnlock()\n\tif jsa != nil {\n\t\tjsa.usageMu.Lock()\n\t\tjsa.usageApi++\n\t\tjsa.apiTotal++\n\t\tjsa.sendClusterUsageUpdate()\n\t\tatomic.AddInt64(&jsa.js.apiTotal, 1)\n\t\tjsa.usageMu.Unlock()\n\t}\n}\n\nfunc (a *Account) trackAPIErr() {\n\ta.mu.RLock()\n\tjsa := a.js\n\ta.mu.RUnlock()\n\tif jsa != nil {\n\t\tjsa.usageMu.Lock()\n\t\tjsa.usageApi++\n\t\tjsa.apiTotal++\n\t\tjsa.usageErr++\n\t\tjsa.apiErrors++\n\t\tjsa.sendClusterUsageUpdate()\n\t\tatomic.AddInt64(&jsa.js.apiTotal, 1)\n\t\tatomic.AddInt64(&jsa.js.apiErrors, 1)\n\t\tjsa.usageMu.Unlock()\n\t}\n}\n\nconst badAPIRequestT = \"Malformed JetStream API Request: %q\"\n\n// Helper function to check on JetStream being enabled but also on status of leafnodes\n// If the local account is not enabled but does have leafnode connectivity we will not\n// want to error immediately and let the other side decide.\nfunc (a *Account) checkJetStream() (enabled, shouldError bool) {\n\ta.mu.RLock()\n\tdefer a.mu.RUnlock()\n\treturn a.js != nil, a.nleafs+a.nrleafs == 0\n}\n\n// Request for current usage and limits for this account.\nfunc (s *Server) jsAccountInfoRequest(sub *subscription, c *client, _ *Account, subject, reply string, rmsg []byte) {\n\tif c == nil || !s.JetStreamEnabled() {\n\t\treturn\n\t}\n\n\tci, acc, _, msg, err := s.getRequestInfo(c, rmsg)\n\tif err != nil {\n\t\ts.Warnf(badAPIRequestT, msg)\n\t\treturn\n\t}\n\n\tvar resp = JSApiAccountInfoResponse{ApiResponse: ApiResponse{Type: JSApiAccountInfoResponseType}}\n\n\t// Determine if we should proceed here when we are in clustered mode.\n\tif s.JetStreamIsClustered() {\n\t\tjs, cc := s.getJetStreamCluster()\n\t\tif js == nil || cc == nil {\n\t\t\treturn\n\t\t}\n\t\tif js.isLeaderless() {\n\t\t\tresp.Error = NewJSClusterNotAvailError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\t// Make sure we are meta leader.\n\t\tif !s.JetStreamIsLeader() {\n\t\t\treturn\n\t\t}\n\t}\n\n\tif hasJS, doErr := acc.checkJetStream(); !hasJS {\n\t\tif !doErr {\n\t\t\treturn\n\t\t}\n\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t} else {\n\t\tstats := acc.JetStreamUsage()\n\t\tresp.JetStreamAccountStats = &stats\n\t}\n\tb, err := json.Marshal(resp)\n\tif err != nil {\n\t\treturn\n\t}\n\n\ts.sendAPIResponse(ci, acc, subject, reply, string(msg), string(b))\n}\n\n// Helpers for token extraction.\nfunc templateNameFromSubject(subject string) string {\n\treturn tokenAt(subject, 6)\n}\n\nfunc streamNameFromSubject(subject string) string {\n\treturn tokenAt(subject, 5)\n}\n\nfunc consumerNameFromSubject(subject string) string {\n\treturn tokenAt(subject, 6)\n}\n\n// Request to create a new template.\nfunc (s *Server) jsTemplateCreateRequest(sub *subscription, c *client, _ *Account, subject, reply string, rmsg []byte) {\n\tif c == nil {\n\t\treturn\n\t}\n\tci, acc, _, msg, err := s.getRequestInfo(c, rmsg)\n\tif err != nil {\n\t\ts.Warnf(badAPIRequestT, msg)\n\t\treturn\n\t}\n\n\tvar resp = JSApiStreamTemplateCreateResponse{ApiResponse: ApiResponse{Type: JSApiStreamTemplateCreateResponseType}}\n\tif !acc.JetStreamEnabled() {\n\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\t// Not supported for now.\n\tif s.JetStreamIsClustered() {\n\t\tresp.Error = NewJSClusterUnSupportFeatureError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tvar cfg StreamTemplateConfig\n\tif err := s.unmarshalRequest(c, acc, subject, msg, &cfg); err != nil {\n\t\tresp.Error = NewJSInvalidJSONError(err)\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\ttemplateName := templateNameFromSubject(subject)\n\tif templateName != cfg.Name {\n\t\tresp.Error = NewJSTemplateNameNotMatchSubjectError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tt, err := acc.addStreamTemplate(&cfg)\n\tif err != nil {\n\t\tresp.Error = NewJSStreamTemplateCreateError(err, Unless(err))\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\tt.mu.Lock()\n\ttcfg := t.StreamTemplateConfig.deepCopy()\n\tstreams := t.streams\n\tif streams == nil {\n\t\tstreams = []string{}\n\t}\n\tt.mu.Unlock()\n\tresp.StreamTemplateInfo = &StreamTemplateInfo{Config: tcfg, Streams: streams}\n\ts.sendAPIResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(resp))\n}\n\n// Request for the list of all template names.\nfunc (s *Server) jsTemplateNamesRequest(sub *subscription, c *client, _ *Account, subject, reply string, rmsg []byte) {\n\tif c == nil {\n\t\treturn\n\t}\n\tci, acc, _, msg, err := s.getRequestInfo(c, rmsg)\n\tif err != nil {\n\t\ts.Warnf(badAPIRequestT, msg)\n\t\treturn\n\t}\n\n\tvar resp = JSApiStreamTemplateNamesResponse{ApiResponse: ApiResponse{Type: JSApiStreamTemplateNamesResponseType}}\n\tif !acc.JetStreamEnabled() {\n\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\t// Not supported for now.\n\tif s.JetStreamIsClustered() {\n\t\tresp.Error = NewJSClusterUnSupportFeatureError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tvar offset int\n\tif isJSONObjectOrArray(msg) {\n\t\tvar req JSApiStreamTemplatesRequest\n\t\tif err := s.unmarshalRequest(c, acc, subject, msg, &req); err != nil {\n\t\t\tresp.Error = NewJSInvalidJSONError(err)\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\toffset = req.Offset\n\t}\n\n\tts := acc.templates()\n\tslices.SortFunc(ts, func(i, j *streamTemplate) int {\n\t\treturn cmp.Compare(i.StreamTemplateConfig.Name, j.StreamTemplateConfig.Name)\n\t})\n\n\ttcnt := len(ts)\n\tif offset > tcnt {\n\t\toffset = tcnt\n\t}\n\n\tfor _, t := range ts[offset:] {\n\t\tt.mu.Lock()\n\t\tname := t.Name\n\t\tt.mu.Unlock()\n\t\tresp.Templates = append(resp.Templates, name)\n\t\tif len(resp.Templates) >= JSApiNamesLimit {\n\t\t\tbreak\n\t\t}\n\t}\n\tresp.Total = tcnt\n\tresp.Limit = JSApiNamesLimit\n\tresp.Offset = offset\n\tif resp.Templates == nil {\n\t\tresp.Templates = []string{}\n\t}\n\ts.sendAPIResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(resp))\n}\n\n// Request for information about a stream template.\nfunc (s *Server) jsTemplateInfoRequest(sub *subscription, c *client, _ *Account, subject, reply string, rmsg []byte) {\n\tif c == nil {\n\t\treturn\n\t}\n\tci, acc, _, msg, err := s.getRequestInfo(c, rmsg)\n\tif err != nil {\n\t\ts.Warnf(badAPIRequestT, msg)\n\t\treturn\n\t}\n\n\tvar resp = JSApiStreamTemplateInfoResponse{ApiResponse: ApiResponse{Type: JSApiStreamTemplateInfoResponseType}}\n\tif !acc.JetStreamEnabled() {\n\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\tif !isEmptyRequest(msg) {\n\t\tresp.Error = NewJSNotEmptyRequestError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\tname := templateNameFromSubject(subject)\n\tt, err := acc.lookupStreamTemplate(name)\n\tif err != nil {\n\t\tresp.Error = NewJSStreamTemplateNotFoundError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\tt.mu.Lock()\n\tcfg := t.StreamTemplateConfig.deepCopy()\n\tstreams := t.streams\n\tif streams == nil {\n\t\tstreams = []string{}\n\t}\n\tt.mu.Unlock()\n\n\tresp.StreamTemplateInfo = &StreamTemplateInfo{Config: cfg, Streams: streams}\n\ts.sendAPIResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(resp))\n}\n\n// Request to delete a stream template.\nfunc (s *Server) jsTemplateDeleteRequest(sub *subscription, c *client, _ *Account, subject, reply string, rmsg []byte) {\n\tif c == nil {\n\t\treturn\n\t}\n\tci, acc, _, msg, err := s.getRequestInfo(c, rmsg)\n\tif err != nil {\n\t\ts.Warnf(badAPIRequestT, msg)\n\t\treturn\n\t}\n\n\tvar resp = JSApiStreamTemplateDeleteResponse{ApiResponse: ApiResponse{Type: JSApiStreamTemplateDeleteResponseType}}\n\tif !acc.JetStreamEnabled() {\n\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\tif !isEmptyRequest(msg) {\n\t\tresp.Error = NewJSNotEmptyRequestError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\tname := templateNameFromSubject(subject)\n\terr = acc.deleteStreamTemplate(name)\n\tif err != nil {\n\t\tresp.Error = NewJSStreamTemplateDeleteError(err, Unless(err))\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\tresp.Success = true\n\ts.sendAPIResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(resp))\n}\n\nfunc (s *Server) jsonResponse(v any) string {\n\tb, err := json.Marshal(v)\n\tif err != nil {\n\t\ts.Warnf(\"Problem marshaling JSON for JetStream API:\", err)\n\t\treturn \"\"\n\t}\n\treturn string(b)\n}\n\n// Read lock must be held\nfunc (jsa *jsAccount) tieredReservation(tier string, cfg *StreamConfig) int64 {\n\treservation := int64(0)\n\tif tier == _EMPTY_ {\n\t\tfor _, sa := range jsa.streams {\n\t\t\tif sa.cfg.MaxBytes > 0 {\n\t\t\t\tif sa.cfg.Storage == cfg.Storage && sa.cfg.Name != cfg.Name {\n\t\t\t\t\treservation += (int64(sa.cfg.Replicas) * sa.cfg.MaxBytes)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t} else {\n\t\tfor _, sa := range jsa.streams {\n\t\t\tif sa.cfg.Replicas == cfg.Replicas {\n\t\t\t\tif sa.cfg.MaxBytes > 0 {\n\t\t\t\t\tif isSameTier(&sa.cfg, cfg) && sa.cfg.Name != cfg.Name {\n\t\t\t\t\t\treservation += (int64(sa.cfg.Replicas) * sa.cfg.MaxBytes)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn reservation\n}\n\n// Request to create a stream.\nfunc (s *Server) jsStreamCreateRequest(sub *subscription, c *client, _ *Account, subject, reply string, rmsg []byte) {\n\tif c == nil || !s.JetStreamEnabled() {\n\t\treturn\n\t}\n\tci, acc, _, msg, err := s.getRequestInfo(c, rmsg)\n\tif err != nil {\n\t\ts.Warnf(badAPIRequestT, msg)\n\t\treturn\n\t}\n\n\tvar resp = JSApiStreamCreateResponse{ApiResponse: ApiResponse{Type: JSApiStreamCreateResponseType}}\n\n\t// Determine if we should proceed here when we are in clustered mode.\n\tif s.JetStreamIsClustered() {\n\t\tjs, cc := s.getJetStreamCluster()\n\t\tif js == nil || cc == nil {\n\t\t\treturn\n\t\t}\n\t\tif js.isLeaderless() {\n\t\t\tresp.Error = NewJSClusterNotAvailError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\t// Make sure we are meta leader.\n\t\tif !s.JetStreamIsLeader() {\n\t\t\treturn\n\t\t}\n\t}\n\n\tif hasJS, doErr := acc.checkJetStream(); !hasJS {\n\t\tif doErr {\n\t\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t}\n\t\treturn\n\t}\n\n\tvar cfg StreamConfigRequest\n\tif err := s.unmarshalRequest(c, acc, subject, msg, &cfg); err != nil {\n\t\tresp.Error = NewJSInvalidJSONError(err)\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\t// Initialize asset version metadata.\n\tsetStaticStreamMetadata(&cfg.StreamConfig)\n\n\tstreamName := streamNameFromSubject(subject)\n\tif streamName != cfg.Name {\n\t\tresp.Error = NewJSStreamMismatchError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\t// Check for path like separators in the name.\n\tif strings.ContainsAny(streamName, `\\/`) {\n\t\tresp.Error = NewJSStreamNameContainsPathSeparatorsError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\t// Can't create a stream with a sealed state.\n\tif cfg.Sealed {\n\t\tresp.Error = NewJSStreamInvalidConfigError(fmt.Errorf(\"stream configuration for create can not be sealed\"))\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\t// If we are told to do mirror direct but are not mirroring, error.\n\tif cfg.MirrorDirect && cfg.Mirror == nil {\n\t\tresp.Error = NewJSStreamInvalidConfigError(fmt.Errorf(\"stream has no mirror but does have mirror direct\"))\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\t// Hand off to cluster for processing.\n\tif s.JetStreamIsClustered() {\n\t\ts.jsClusteredStreamRequest(ci, acc, subject, reply, rmsg, &cfg)\n\t\treturn\n\t}\n\n\tif err := acc.jsNonClusteredStreamLimitsCheck(&cfg.StreamConfig); err != nil {\n\t\tresp.Error = err\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tmset, err := acc.addStreamPedantic(&cfg.StreamConfig, cfg.Pedantic)\n\tif err != nil {\n\t\tif IsNatsErr(err, JSStreamStoreFailedF) {\n\t\t\ts.Warnf(\"Stream create failed for '%s > %s': %v\", acc, streamName, err)\n\t\t\terr = errStreamStoreFailed\n\t\t}\n\t\tresp.Error = NewJSStreamCreateError(err, Unless(err))\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\tmsetCfg := mset.config()\n\tresp.StreamInfo = &StreamInfo{\n\t\tCreated:   mset.createdTime(),\n\t\tState:     mset.state(),\n\t\tConfig:    *setDynamicStreamMetadata(&msetCfg),\n\t\tTimeStamp: time.Now().UTC(),\n\t\tMirror:    mset.mirrorInfo(),\n\t\tSources:   mset.sourcesInfo(),\n\t}\n\tresp.DidCreate = true\n\ts.sendAPIResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(resp))\n}\n\n// Request to update a stream.\nfunc (s *Server) jsStreamUpdateRequest(sub *subscription, c *client, _ *Account, subject, reply string, rmsg []byte) {\n\tif c == nil || !s.JetStreamEnabled() {\n\t\treturn\n\t}\n\n\tci, acc, _, msg, err := s.getRequestInfo(c, rmsg)\n\tif err != nil {\n\t\ts.Warnf(badAPIRequestT, msg)\n\t\treturn\n\t}\n\n\tvar resp = JSApiStreamUpdateResponse{ApiResponse: ApiResponse{Type: JSApiStreamUpdateResponseType}}\n\n\t// Determine if we should proceed here when we are in clustered mode.\n\tif s.JetStreamIsClustered() {\n\t\tjs, cc := s.getJetStreamCluster()\n\t\tif js == nil || cc == nil {\n\t\t\treturn\n\t\t}\n\t\tif js.isLeaderless() {\n\t\t\tresp.Error = NewJSClusterNotAvailError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\t// Make sure we are meta leader.\n\t\tif !s.JetStreamIsLeader() {\n\t\t\treturn\n\t\t}\n\t}\n\n\tif hasJS, doErr := acc.checkJetStream(); !hasJS {\n\t\tif doErr {\n\t\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t}\n\t\treturn\n\t}\n\tvar ncfg StreamConfigRequest\n\tif err := s.unmarshalRequest(c, acc, subject, msg, &ncfg); err != nil {\n\t\tresp.Error = NewJSInvalidJSONError(err)\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tcfg, apiErr := s.checkStreamCfg(&ncfg.StreamConfig, acc, ncfg.Pedantic)\n\tif apiErr != nil {\n\t\tresp.Error = apiErr\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tstreamName := streamNameFromSubject(subject)\n\tif streamName != cfg.Name {\n\t\tresp.Error = NewJSStreamMismatchError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\t// Handle clustered version here.\n\tif s.JetStreamIsClustered() {\n\t\t// Always do in separate Go routine.\n\t\tgo s.jsClusteredStreamUpdateRequest(ci, acc, subject, reply, copyBytes(rmsg), &cfg, nil, ncfg.Pedantic)\n\t\treturn\n\t}\n\n\tmset, err := acc.lookupStream(streamName)\n\tif err != nil {\n\t\tresp.Error = NewJSStreamNotFoundError(Unless(err))\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\t// Update asset version metadata.\n\tsetStaticStreamMetadata(&cfg)\n\n\tif err := mset.updatePedantic(&cfg, ncfg.Pedantic); err != nil {\n\t\tresp.Error = NewJSStreamUpdateError(err, Unless(err))\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tmsetCfg := mset.config()\n\tresp.StreamInfo = &StreamInfo{\n\t\tCreated:   mset.createdTime(),\n\t\tState:     mset.state(),\n\t\tConfig:    *setDynamicStreamMetadata(&msetCfg),\n\t\tDomain:    s.getOpts().JetStreamDomain,\n\t\tMirror:    mset.mirrorInfo(),\n\t\tSources:   mset.sourcesInfo(),\n\t\tTimeStamp: time.Now().UTC(),\n\t}\n\ts.sendAPIResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(resp))\n}\n\n// Request for the list of all stream names.\nfunc (s *Server) jsStreamNamesRequest(sub *subscription, c *client, _ *Account, subject, reply string, rmsg []byte) {\n\tif c == nil || !s.JetStreamEnabled() {\n\t\treturn\n\t}\n\tci, acc, _, msg, err := s.getRequestInfo(c, rmsg)\n\tif err != nil {\n\t\ts.Warnf(badAPIRequestT, msg)\n\t\treturn\n\t}\n\n\tvar resp = JSApiStreamNamesResponse{ApiResponse: ApiResponse{Type: JSApiStreamNamesResponseType}}\n\n\t// Determine if we should proceed here when we are in clustered mode.\n\tif s.JetStreamIsClustered() {\n\t\tjs, cc := s.getJetStreamCluster()\n\t\tif js == nil || cc == nil {\n\t\t\treturn\n\t\t}\n\t\tif js.isLeaderless() {\n\t\t\tresp.Error = NewJSClusterNotAvailError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\t// Make sure we are meta leader.\n\t\tif !s.JetStreamIsLeader() {\n\t\t\treturn\n\t\t}\n\t}\n\n\tif hasJS, doErr := acc.checkJetStream(); !hasJS {\n\t\tif doErr {\n\t\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t}\n\t\treturn\n\t}\n\n\tvar offset int\n\tvar filter string\n\n\tif isJSONObjectOrArray(msg) {\n\t\tvar req JSApiStreamNamesRequest\n\t\tif err := s.unmarshalRequest(c, acc, subject, msg, &req); err != nil {\n\t\t\tresp.Error = NewJSInvalidJSONError(err)\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\toffset = req.Offset\n\t\tif req.Subject != _EMPTY_ {\n\t\t\tfilter = req.Subject\n\t\t}\n\t}\n\n\t// TODO(dlc) - Maybe hold these results for large results that we expect to be paged.\n\t// TODO(dlc) - If this list is long maybe do this in a Go routine?\n\tvar numStreams int\n\tif s.JetStreamIsClustered() {\n\t\tjs, cc := s.getJetStreamCluster()\n\t\tif js == nil || cc == nil {\n\t\t\t// TODO(dlc) - Debug or Warn?\n\t\t\treturn\n\t\t}\n\t\tjs.mu.RLock()\n\t\tfor stream, sa := range cc.streams[acc.Name] {\n\t\t\tif IsNatsErr(sa.err, JSClusterNotAssignedErr) {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif filter != _EMPTY_ {\n\t\t\t\t// These could not have subjects auto-filled in since they are raw and unprocessed.\n\t\t\t\tif len(sa.Config.Subjects) == 0 {\n\t\t\t\t\tif SubjectsCollide(filter, sa.Config.Name) {\n\t\t\t\t\t\tresp.Streams = append(resp.Streams, stream)\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\tfor _, subj := range sa.Config.Subjects {\n\t\t\t\t\t\tif SubjectsCollide(filter, subj) {\n\t\t\t\t\t\t\tresp.Streams = append(resp.Streams, stream)\n\t\t\t\t\t\t\tbreak\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tresp.Streams = append(resp.Streams, stream)\n\t\t\t}\n\t\t}\n\t\tjs.mu.RUnlock()\n\t\tif len(resp.Streams) > 1 {\n\t\t\tslices.Sort(resp.Streams)\n\t\t}\n\t\tnumStreams = len(resp.Streams)\n\t\tif offset > numStreams {\n\t\t\toffset = numStreams\n\t\t}\n\t\tif offset > 0 {\n\t\t\tresp.Streams = resp.Streams[offset:]\n\t\t}\n\t\tif len(resp.Streams) > JSApiNamesLimit {\n\t\t\tresp.Streams = resp.Streams[:JSApiNamesLimit]\n\t\t}\n\t} else {\n\t\tmsets := acc.filteredStreams(filter)\n\t\t// Since we page results order matters.\n\t\tif len(msets) > 1 {\n\t\t\tslices.SortFunc(msets, func(i, j *stream) int { return cmp.Compare(i.cfg.Name, j.cfg.Name) })\n\t\t}\n\n\t\tnumStreams = len(msets)\n\t\tif offset > numStreams {\n\t\t\toffset = numStreams\n\t\t}\n\n\t\tfor _, mset := range msets[offset:] {\n\t\t\tresp.Streams = append(resp.Streams, mset.cfg.Name)\n\t\t\tif len(resp.Streams) >= JSApiNamesLimit {\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t}\n\tresp.Total = numStreams\n\tresp.Limit = JSApiNamesLimit\n\tresp.Offset = offset\n\n\ts.sendAPIResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(resp))\n}\n\n// Request for the list of all detailed stream info.\n// TODO(dlc) - combine with above long term\nfunc (s *Server) jsStreamListRequest(sub *subscription, c *client, _ *Account, subject, reply string, rmsg []byte) {\n\tif c == nil || !s.JetStreamEnabled() {\n\t\treturn\n\t}\n\tci, acc, _, msg, err := s.getRequestInfo(c, rmsg)\n\tif err != nil {\n\t\ts.Warnf(badAPIRequestT, msg)\n\t\treturn\n\t}\n\n\tvar resp = JSApiStreamListResponse{\n\t\tApiResponse: ApiResponse{Type: JSApiStreamListResponseType},\n\t\tStreams:     []*StreamInfo{},\n\t}\n\n\t// Determine if we should proceed here when we are in clustered mode.\n\tif s.JetStreamIsClustered() {\n\t\tjs, cc := s.getJetStreamCluster()\n\t\tif js == nil || cc == nil {\n\t\t\treturn\n\t\t}\n\t\tif js.isLeaderless() {\n\t\t\tresp.Error = NewJSClusterNotAvailError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\t// Make sure we are meta leader.\n\t\tif !s.JetStreamIsLeader() {\n\t\t\treturn\n\t\t}\n\t}\n\n\tif hasJS, doErr := acc.checkJetStream(); !hasJS {\n\t\tif doErr {\n\t\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t}\n\t\treturn\n\t}\n\n\tvar offset int\n\tvar filter string\n\n\tif isJSONObjectOrArray(msg) {\n\t\tvar req JSApiStreamListRequest\n\t\tif err := s.unmarshalRequest(c, acc, subject, msg, &req); err != nil {\n\t\t\tresp.Error = NewJSInvalidJSONError(err)\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\toffset = req.Offset\n\t\tif req.Subject != _EMPTY_ {\n\t\t\tfilter = req.Subject\n\t\t}\n\t}\n\n\t// Clustered mode will invoke a scatter and gather.\n\tif s.JetStreamIsClustered() {\n\t\t// Need to copy these off before sending.. don't move this inside startGoRoutine!!!\n\t\tmsg = copyBytes(msg)\n\t\ts.startGoRoutine(func() { s.jsClusteredStreamListRequest(acc, ci, filter, offset, subject, reply, msg) })\n\t\treturn\n\t}\n\n\t// TODO(dlc) - Maybe hold these results for large results that we expect to be paged.\n\t// TODO(dlc) - If this list is long maybe do this in a Go routine?\n\tvar msets []*stream\n\tif filter == _EMPTY_ {\n\t\tmsets = acc.streams()\n\t} else {\n\t\tmsets = acc.filteredStreams(filter)\n\t}\n\n\tslices.SortFunc(msets, func(i, j *stream) int { return cmp.Compare(i.cfg.Name, j.cfg.Name) })\n\n\tscnt := len(msets)\n\tif offset > scnt {\n\t\toffset = scnt\n\t}\n\n\tfor _, mset := range msets[offset:] {\n\t\tconfig := mset.config()\n\t\tresp.Streams = append(resp.Streams, &StreamInfo{\n\t\t\tCreated:   mset.createdTime(),\n\t\t\tState:     mset.state(),\n\t\t\tConfig:    config,\n\t\t\tDomain:    s.getOpts().JetStreamDomain,\n\t\t\tMirror:    mset.mirrorInfo(),\n\t\t\tSources:   mset.sourcesInfo(),\n\t\t\tTimeStamp: time.Now().UTC(),\n\t\t})\n\t\tif len(resp.Streams) >= JSApiListLimit {\n\t\t\tbreak\n\t\t}\n\t}\n\tresp.Total = scnt\n\tresp.Limit = JSApiListLimit\n\tresp.Offset = offset\n\ts.sendAPIResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(resp))\n}\n\n// Request for information about a stream.\nfunc (s *Server) jsStreamInfoRequest(sub *subscription, c *client, a *Account, subject, reply string, rmsg []byte) {\n\tif c == nil || !s.JetStreamEnabled() {\n\t\treturn\n\t}\n\tci, acc, hdr, msg, err := s.getRequestInfo(c, rmsg)\n\tif err != nil {\n\t\ts.Warnf(badAPIRequestT, msg)\n\t\treturn\n\t}\n\n\tstreamName := streamNameFromSubject(subject)\n\n\tvar resp = JSApiStreamInfoResponse{ApiResponse: ApiResponse{Type: JSApiStreamInfoResponseType}}\n\n\t// If someone creates a duplicate stream that is identical we will get this request forwarded to us.\n\t// Make sure the response type is for a create call.\n\tif rt := getHeader(JSResponseType, hdr); len(rt) > 0 && string(rt) == jsCreateResponse {\n\t\tresp.ApiResponse.Type = JSApiStreamCreateResponseType\n\t}\n\n\tvar clusterWideConsCount int\n\n\tjs, cc := s.getJetStreamCluster()\n\tif js == nil {\n\t\treturn\n\t}\n\t// If we are in clustered mode we need to be the stream leader to proceed.\n\tif cc != nil {\n\t\t// Check to make sure the stream is assigned.\n\t\tjs.mu.RLock()\n\t\tisLeader, sa := cc.isLeader(), js.streamAssignment(acc.Name, streamName)\n\t\tvar offline bool\n\t\tif sa != nil {\n\t\t\tclusterWideConsCount = len(sa.consumers)\n\t\t\toffline = s.allPeersOffline(sa.Group)\n\t\t}\n\t\tjs.mu.RUnlock()\n\n\t\tif isLeader && sa == nil {\n\t\t\t// We can't find the stream, so mimic what would be the errors below.\n\t\t\tif hasJS, doErr := acc.checkJetStream(); !hasJS {\n\t\t\t\tif doErr {\n\t\t\t\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\t\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\t\t}\n\t\t\t\treturn\n\t\t\t}\n\t\t\t// No stream present.\n\t\t\tresp.Error = NewJSStreamNotFoundError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t} else if sa == nil {\n\t\t\tif js.isLeaderless() {\n\t\t\t\tresp.Error = NewJSClusterNotAvailError()\n\t\t\t\t// Delaying an error response gives the leader a chance to respond before us\n\t\t\t\ts.sendDelayedAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp), nil, errRespDelay)\n\t\t\t}\n\t\t\treturn\n\t\t} else if isLeader && offline {\n\t\t\tresp.Error = NewJSStreamOfflineError()\n\t\t\ts.sendDelayedAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp), nil, errRespDelay)\n\t\t\treturn\n\t\t}\n\n\t\t// Check to see if we are a member of the group and if the group has no leader.\n\t\tisLeaderless := js.isGroupLeaderless(sa.Group)\n\n\t\t// We have the stream assigned and a leader, so only the stream leader should answer.\n\t\tif !acc.JetStreamIsStreamLeader(streamName) && !isLeaderless {\n\t\t\tif js.isLeaderless() {\n\t\t\t\tresp.Error = NewJSClusterNotAvailError()\n\t\t\t\t// Delaying an error response gives the leader a chance to respond before us\n\t\t\t\ts.sendDelayedAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp), sa.Group, errRespDelay)\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\t// We may be in process of electing a leader, but if this is a scale up from 1 we will still be the state leader\n\t\t\t// while the new members work through the election and catchup process.\n\t\t\t// Double check for that instead of exiting here and being silent. e.g. nats stream update test --replicas=3\n\t\t\tjs.mu.RLock()\n\t\t\trg := sa.Group\n\t\t\tvar ourID string\n\t\t\tif cc.meta != nil {\n\t\t\t\tourID = cc.meta.ID()\n\t\t\t}\n\t\t\t// We have seen cases where rg is nil at this point,\n\t\t\t// so check explicitly and bail if that is the case.\n\t\t\tbail := rg == nil || !rg.isMember(ourID)\n\t\t\tif !bail {\n\t\t\t\t// We know we are a member here, if this group is new and we are preferred allow us to answer.\n\t\t\t\t// Also, we have seen cases where rg.node is nil at this point,\n\t\t\t\t// so check explicitly and bail if that is the case.\n\t\t\t\tbail = rg.Preferred != ourID || (rg.node != nil && time.Since(rg.node.Created()) > lostQuorumIntervalDefault)\n\t\t\t}\n\t\t\tjs.mu.RUnlock()\n\t\t\tif bail {\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}\n\n\tif hasJS, doErr := acc.checkJetStream(); !hasJS {\n\t\tif doErr {\n\t\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t}\n\t\treturn\n\t}\n\n\tvar details bool\n\tvar subjects string\n\tvar offset int\n\tif isJSONObjectOrArray(msg) {\n\t\tvar req JSApiStreamInfoRequest\n\t\tif err := s.unmarshalRequest(c, acc, subject, msg, &req); err != nil {\n\t\t\tresp.Error = NewJSInvalidJSONError(err)\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\tdetails, subjects = req.DeletedDetails, req.SubjectsFilter\n\t\toffset = req.Offset\n\t}\n\n\tmset, err := acc.lookupStream(streamName)\n\t// Error is not to be expected at this point, but could happen if same stream trying to be created.\n\tif err != nil {\n\t\tif cc != nil {\n\t\t\t// This could be inflight, pause for a short bit and try again.\n\t\t\t// This will not be inline, so ok.\n\t\t\ttime.Sleep(10 * time.Millisecond)\n\t\t\tmset, err = acc.lookupStream(streamName)\n\t\t}\n\t\t// Check again.\n\t\tif err != nil {\n\t\t\tresp.Error = NewJSStreamNotFoundError(Unless(err))\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t}\n\n\tconfig := mset.config()\n\tresp.StreamInfo = &StreamInfo{\n\t\tCreated:    mset.createdTime(),\n\t\tState:      mset.stateWithDetail(details),\n\t\tConfig:     *setDynamicStreamMetadata(&config),\n\t\tDomain:     s.getOpts().JetStreamDomain,\n\t\tCluster:    js.clusterInfo(mset.raftGroup()),\n\t\tMirror:     mset.mirrorInfo(),\n\t\tSources:    mset.sourcesInfo(),\n\t\tAlternates: js.streamAlternates(ci, config.Name),\n\t\tTimeStamp:  time.Now().UTC(),\n\t}\n\tif clusterWideConsCount > 0 {\n\t\tresp.StreamInfo.State.Consumers = clusterWideConsCount\n\t}\n\n\t// Check if they have asked for subject details.\n\tif subjects != _EMPTY_ {\n\t\tst := mset.store.SubjectsTotals(subjects)\n\t\tif lst := len(st); lst > 0 {\n\t\t\t// Common for both cases.\n\t\t\tresp.Offset = offset\n\t\t\tresp.Limit = JSMaxSubjectDetails\n\t\t\tresp.Total = lst\n\n\t\t\tif offset == 0 && lst <= JSMaxSubjectDetails {\n\t\t\t\tresp.StreamInfo.State.Subjects = st\n\t\t\t} else {\n\t\t\t\t// Here we have to filter list due to offset or maximum constraints.\n\t\t\t\tsubjs := make([]string, 0, len(st))\n\t\t\t\tfor subj := range st {\n\t\t\t\t\tsubjs = append(subjs, subj)\n\t\t\t\t}\n\t\t\t\t// Sort it\n\t\t\t\tslices.Sort(subjs)\n\n\t\t\t\tif offset > len(subjs) {\n\t\t\t\t\toffset = len(subjs)\n\t\t\t\t}\n\n\t\t\t\tend := offset + JSMaxSubjectDetails\n\t\t\t\tif end > len(subjs) {\n\t\t\t\t\tend = len(subjs)\n\t\t\t\t}\n\t\t\t\tactualSize := end - offset\n\t\t\t\tvar sd map[string]uint64\n\n\t\t\t\tif actualSize > 0 {\n\t\t\t\t\tsd = make(map[string]uint64, actualSize)\n\t\t\t\t\tfor _, ss := range subjs[offset:end] {\n\t\t\t\t\t\tsd[ss] = st[ss]\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tresp.StreamInfo.State.Subjects = sd\n\t\t\t}\n\t\t}\n\t}\n\t// Check for out of band catchups.\n\tif mset.hasCatchupPeers() {\n\t\tmset.checkClusterInfo(resp.StreamInfo.Cluster)\n\t}\n\n\ts.sendAPIResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(resp))\n}\n\n// Request to have a stream leader stepdown.\nfunc (s *Server) jsStreamLeaderStepDownRequest(sub *subscription, c *client, _ *Account, subject, reply string, rmsg []byte) {\n\tif c == nil || !s.JetStreamEnabled() {\n\t\treturn\n\t}\n\tci, acc, _, msg, err := s.getRequestInfo(c, rmsg)\n\tif err != nil {\n\t\ts.Warnf(badAPIRequestT, msg)\n\t\treturn\n\t}\n\n\t// Have extra token for this one.\n\tname := tokenAt(subject, 6)\n\n\tvar resp = JSApiStreamLeaderStepDownResponse{ApiResponse: ApiResponse{Type: JSApiStreamLeaderStepDownResponseType}}\n\n\t// If we are not in clustered mode this is a failed request.\n\tif !s.JetStreamIsClustered() {\n\t\tresp.Error = NewJSClusterRequiredError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\t// If we are here we are clustered. See if we are the stream leader in order to proceed.\n\tjs, cc := s.getJetStreamCluster()\n\tif js == nil || cc == nil {\n\t\treturn\n\t}\n\tif js.isLeaderless() {\n\t\tresp.Error = NewJSClusterNotAvailError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tjs.mu.RLock()\n\tisLeader, sa := cc.isLeader(), js.streamAssignment(acc.Name, name)\n\tjs.mu.RUnlock()\n\n\tif isLeader && sa == nil {\n\t\tresp.Error = NewJSStreamNotFoundError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t} else if sa == nil {\n\t\treturn\n\t}\n\n\tif hasJS, doErr := acc.checkJetStream(); !hasJS {\n\t\tif doErr {\n\t\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t}\n\t\treturn\n\t}\n\n\t// Check to see if we are a member of the group and if the group has no leader.\n\tif js.isGroupLeaderless(sa.Group) {\n\t\tresp.Error = NewJSClusterNotAvailError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\t// We have the stream assigned and a leader, so only the stream leader should answer.\n\tif !acc.JetStreamIsStreamLeader(name) {\n\t\treturn\n\t}\n\n\tmset, err := acc.lookupStream(name)\n\tif err != nil {\n\t\tresp.Error = NewJSStreamNotFoundError(Unless(err))\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tif mset == nil {\n\t\tresp.Success = true\n\t\ts.sendAPIResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(resp))\n\t\treturn\n\t}\n\n\tnode := mset.raftNode()\n\tif node == nil {\n\t\tresp.Success = true\n\t\ts.sendAPIResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(resp))\n\t\treturn\n\t}\n\n\tvar preferredLeader string\n\tif isJSONObjectOrArray(msg) {\n\t\tvar req JSApiLeaderStepdownRequest\n\t\tif err := s.unmarshalRequest(c, acc, subject, msg, &req); err != nil {\n\t\t\tresp.Error = NewJSInvalidJSONError(err)\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\tif preferredLeader, resp.Error = s.getStepDownPreferredPlacement(node, req.Placement); resp.Error != nil {\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t}\n\n\t// Call actual stepdown.\n\terr = node.StepDown(preferredLeader)\n\tif err != nil {\n\t\tresp.Error = NewJSRaftGeneralError(err, Unless(err))\n\t} else {\n\t\tresp.Success = true\n\t}\n\ts.sendAPIResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(resp))\n}\n\n// Request to have a consumer leader stepdown.\nfunc (s *Server) jsConsumerLeaderStepDownRequest(sub *subscription, c *client, _ *Account, subject, reply string, rmsg []byte) {\n\tif c == nil || !s.JetStreamEnabled() {\n\t\treturn\n\t}\n\tci, acc, _, msg, err := s.getRequestInfo(c, rmsg)\n\tif err != nil {\n\t\ts.Warnf(badAPIRequestT, msg)\n\t\treturn\n\t}\n\n\tvar resp = JSApiConsumerLeaderStepDownResponse{ApiResponse: ApiResponse{Type: JSApiConsumerLeaderStepDownResponseType}}\n\n\t// If we are not in clustered mode this is a failed request.\n\tif !s.JetStreamIsClustered() {\n\t\tresp.Error = NewJSClusterRequiredError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\t// If we are here we are clustered. See if we are the stream leader in order to proceed.\n\tjs, cc := s.getJetStreamCluster()\n\tif js == nil || cc == nil {\n\t\treturn\n\t}\n\tif js.isLeaderless() {\n\t\tresp.Error = NewJSClusterNotAvailError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\t// Have extra token for this one.\n\tstream := tokenAt(subject, 6)\n\tconsumer := tokenAt(subject, 7)\n\n\tjs.mu.RLock()\n\tisLeader, sa := cc.isLeader(), js.streamAssignment(acc.Name, stream)\n\tjs.mu.RUnlock()\n\n\tif isLeader && sa == nil {\n\t\tresp.Error = NewJSStreamNotFoundError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t} else if sa == nil {\n\t\treturn\n\t}\n\tvar ca *consumerAssignment\n\tif sa.consumers != nil {\n\t\tca = sa.consumers[consumer]\n\t}\n\tif ca == nil {\n\t\tresp.Error = NewJSConsumerNotFoundError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\t// Check to see if we are a member of the group and if the group has no leader.\n\tif js.isGroupLeaderless(ca.Group) {\n\t\tresp.Error = NewJSClusterNotAvailError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tif !acc.JetStreamIsConsumerLeader(stream, consumer) {\n\t\treturn\n\t}\n\n\tif hasJS, doErr := acc.checkJetStream(); !hasJS {\n\t\tif doErr {\n\t\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t}\n\t\treturn\n\t}\n\n\tmset, err := acc.lookupStream(stream)\n\tif err != nil {\n\t\tresp.Error = NewJSStreamNotFoundError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\to := mset.lookupConsumer(consumer)\n\tif o == nil {\n\t\tresp.Error = NewJSConsumerNotFoundError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tn := o.raftNode()\n\tif n == nil {\n\t\tresp.Success = true\n\t\ts.sendAPIResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(resp))\n\t\treturn\n\t}\n\n\tvar preferredLeader string\n\tif isJSONObjectOrArray(msg) {\n\t\tvar req JSApiLeaderStepdownRequest\n\t\tif err := s.unmarshalRequest(c, acc, subject, msg, &req); err != nil {\n\t\t\tresp.Error = NewJSInvalidJSONError(err)\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\tif preferredLeader, resp.Error = s.getStepDownPreferredPlacement(n, req.Placement); resp.Error != nil {\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t}\n\n\t// Call actual stepdown.\n\terr = n.StepDown(preferredLeader)\n\tif err != nil {\n\t\tresp.Error = NewJSRaftGeneralError(err, Unless(err))\n\t} else {\n\t\tresp.Success = true\n\t}\n\ts.sendAPIResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(resp))\n}\n\n// Request to remove a peer from a clustered stream.\nfunc (s *Server) jsStreamRemovePeerRequest(sub *subscription, c *client, _ *Account, subject, reply string, rmsg []byte) {\n\tif c == nil || !s.JetStreamEnabled() {\n\t\treturn\n\t}\n\tci, acc, _, msg, err := s.getRequestInfo(c, rmsg)\n\tif err != nil {\n\t\ts.Warnf(badAPIRequestT, msg)\n\t\treturn\n\t}\n\n\t// Have extra token for this one.\n\tname := tokenAt(subject, 6)\n\n\tvar resp = JSApiStreamRemovePeerResponse{ApiResponse: ApiResponse{Type: JSApiStreamRemovePeerResponseType}}\n\n\t// If we are not in clustered mode this is a failed request.\n\tif !s.JetStreamIsClustered() {\n\t\tresp.Error = NewJSClusterRequiredError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\t// If we are here we are clustered. See if we are the stream leader in order to proceed.\n\tjs, cc := s.getJetStreamCluster()\n\tif js == nil || cc == nil {\n\t\treturn\n\t}\n\tif js.isLeaderless() {\n\t\tresp.Error = NewJSClusterNotAvailError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tjs.mu.RLock()\n\tisLeader, sa := cc.isLeader(), js.streamAssignment(acc.Name, name)\n\tjs.mu.RUnlock()\n\n\t// Make sure we are meta leader.\n\tif !isLeader {\n\t\treturn\n\t}\n\n\tif hasJS, doErr := acc.checkJetStream(); !hasJS {\n\t\tif doErr {\n\t\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t}\n\t\treturn\n\t}\n\tif isEmptyRequest(msg) {\n\t\tresp.Error = NewJSBadRequestError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tvar req JSApiStreamRemovePeerRequest\n\tif err := s.unmarshalRequest(c, acc, subject, msg, &req); err != nil {\n\t\tresp.Error = NewJSInvalidJSONError(err)\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\tif req.Peer == _EMPTY_ {\n\t\tresp.Error = NewJSBadRequestError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tif sa == nil {\n\t\t// No stream present.\n\t\tresp.Error = NewJSStreamNotFoundError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\t// Check to see if we are a member of the group and if the group has no leader.\n\t// Peers here is a server name, convert to node name.\n\tnodeName := getHash(req.Peer)\n\n\tjs.mu.RLock()\n\trg := sa.Group\n\tisMember := rg.isMember(nodeName)\n\tjs.mu.RUnlock()\n\n\t// Make sure we are a member.\n\tif !isMember {\n\t\tresp.Error = NewJSClusterPeerNotMemberError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\t// If we are here we have a valid peer member set for removal.\n\tif !js.removePeerFromStream(sa, nodeName) {\n\t\tresp.Error = NewJSPeerRemapError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tresp.Success = true\n\ts.sendAPIResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(resp))\n}\n\n// Request to have the metaleader remove a peer from the system.\nfunc (s *Server) jsLeaderServerRemoveRequest(sub *subscription, c *client, _ *Account, subject, reply string, rmsg []byte) {\n\tif c == nil || !s.JetStreamEnabled() {\n\t\treturn\n\t}\n\n\tci, acc, _, msg, err := s.getRequestInfo(c, rmsg)\n\tif err != nil {\n\t\ts.Warnf(badAPIRequestT, msg)\n\t\treturn\n\t}\n\tif acc != s.SystemAccount() {\n\t\treturn\n\t}\n\n\tjs, cc := s.getJetStreamCluster()\n\tif js == nil || cc == nil || cc.meta == nil {\n\t\treturn\n\t}\n\n\t// Extra checks here but only leader is listening.\n\tjs.mu.RLock()\n\tisLeader := cc.isLeader()\n\tjs.mu.RUnlock()\n\n\tif !isLeader {\n\t\treturn\n\t}\n\n\tvar resp = JSApiMetaServerRemoveResponse{ApiResponse: ApiResponse{Type: JSApiMetaServerRemoveResponseType}}\n\n\tif isEmptyRequest(msg) {\n\t\tresp.Error = NewJSBadRequestError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tvar req JSApiMetaServerRemoveRequest\n\tif err := s.unmarshalRequest(c, acc, subject, msg, &req); err != nil {\n\t\tresp.Error = NewJSInvalidJSONError(err)\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tvar found string\n\tjs.mu.RLock()\n\tfor _, p := range cc.meta.Peers() {\n\t\t// If Peer is specified, it takes precedence\n\t\tif req.Peer != _EMPTY_ {\n\t\t\tif p.ID == req.Peer {\n\t\t\t\tfound = req.Peer\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\t\tsi, ok := s.nodeToInfo.Load(p.ID)\n\t\tif ok && si.(nodeInfo).name == req.Server {\n\t\t\tfound = p.ID\n\t\t\tbreak\n\t\t}\n\t}\n\tjs.mu.RUnlock()\n\n\tif found == _EMPTY_ {\n\t\tresp.Error = NewJSClusterServerNotMemberError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\t// So we have a valid peer.\n\tjs.mu.Lock()\n\tcc.meta.ProposeRemovePeer(found)\n\tjs.mu.Unlock()\n\n\tresp.Success = true\n\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n}\n\nfunc (s *Server) peerSetToNames(ps []string) []string {\n\tnames := make([]string, len(ps))\n\tfor i := 0; i < len(ps); i++ {\n\t\tif si, ok := s.nodeToInfo.Load(ps[i]); !ok {\n\t\t\tnames[i] = ps[i]\n\t\t} else {\n\t\t\tnames[i] = si.(nodeInfo).name\n\t\t}\n\t}\n\treturn names\n}\n\n// looks up the peer id for a given server name. Cluster and domain name are optional filter criteria\nfunc (s *Server) nameToPeer(js *jetStream, serverName, clusterName, domainName string) string {\n\tjs.mu.RLock()\n\tdefer js.mu.RUnlock()\n\tif cc := js.cluster; cc != nil {\n\t\tfor _, p := range cc.meta.Peers() {\n\t\t\tsi, ok := s.nodeToInfo.Load(p.ID)\n\t\t\tif ok && si.(nodeInfo).name == serverName {\n\t\t\t\tif clusterName == _EMPTY_ || clusterName == si.(nodeInfo).cluster {\n\t\t\t\t\tif domainName == _EMPTY_ || domainName == si.(nodeInfo).domain {\n\t\t\t\t\t\treturn p.ID\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn _EMPTY_\n}\n\n// Request to have the metaleader move a stream on a peer to another\nfunc (s *Server) jsLeaderServerStreamMoveRequest(sub *subscription, c *client, _ *Account, subject, reply string, rmsg []byte) {\n\tif c == nil || !s.JetStreamEnabled() {\n\t\treturn\n\t}\n\n\tci, acc, _, msg, err := s.getRequestInfo(c, rmsg)\n\tif err != nil {\n\t\ts.Warnf(badAPIRequestT, msg)\n\t\treturn\n\t}\n\n\tjs, cc := s.getJetStreamCluster()\n\tif js == nil || cc == nil || cc.meta == nil {\n\t\treturn\n\t}\n\n\t// Extra checks here but only leader is listening.\n\tjs.mu.RLock()\n\tisLeader := cc.isLeader()\n\tjs.mu.RUnlock()\n\n\tif !isLeader {\n\t\treturn\n\t}\n\n\taccName := tokenAt(subject, 6)\n\tstreamName := tokenAt(subject, 7)\n\n\tif acc.GetName() != accName && acc != s.SystemAccount() {\n\t\treturn\n\t}\n\n\tvar resp = JSApiStreamUpdateResponse{ApiResponse: ApiResponse{Type: JSApiStreamUpdateResponseType}}\n\n\tvar req JSApiMetaServerStreamMoveRequest\n\tif err := s.unmarshalRequest(c, acc, subject, msg, &req); err != nil {\n\t\tresp.Error = NewJSInvalidJSONError(err)\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tsrcPeer := _EMPTY_\n\tif req.Server != _EMPTY_ {\n\t\tsrcPeer = s.nameToPeer(js, req.Server, req.Cluster, req.Domain)\n\t}\n\n\ttargetAcc, ok := s.accounts.Load(accName)\n\tif !ok {\n\t\tresp.Error = NewJSNoAccountError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tvar streamFound bool\n\tcfg := StreamConfig{}\n\tcurrPeers := []string{}\n\tcurrCluster := _EMPTY_\n\tjs.mu.Lock()\n\tstreams, ok := cc.streams[accName]\n\tif ok {\n\t\tsa, ok := streams[streamName]\n\t\tif ok {\n\t\t\tcfg = *sa.Config.clone()\n\t\t\tstreamFound = true\n\t\t\tcurrPeers = sa.Group.Peers\n\t\t\tcurrCluster = sa.Group.Cluster\n\t\t}\n\t}\n\tjs.mu.Unlock()\n\n\tif !streamFound {\n\t\tresp.Error = NewJSStreamNotFoundError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\t// if server was picked, make sure src peer exists and move it to first position.\n\t// removal will drop peers from the left\n\tif req.Server != _EMPTY_ {\n\t\tif srcPeer == _EMPTY_ {\n\t\t\tresp.Error = NewJSClusterServerNotMemberError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\tvar peerFound bool\n\t\tfor i := 0; i < len(currPeers); i++ {\n\t\t\tif currPeers[i] == srcPeer {\n\t\t\t\tcopy(currPeers[1:], currPeers[:i])\n\t\t\t\tcurrPeers[0] = srcPeer\n\t\t\t\tpeerFound = true\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\tif !peerFound {\n\t\t\tresp.Error = NewJSClusterPeerNotMemberError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t}\n\n\t// make sure client is scoped to requested account\n\tciNew := *(ci)\n\tciNew.Account = accName\n\n\t// backup placement such that peers can be looked up with modified tag list\n\tvar origPlacement *Placement\n\tif cfg.Placement != nil {\n\t\ttmp := *cfg.Placement\n\t\torigPlacement = &tmp\n\t}\n\n\tif len(req.Tags) > 0 {\n\t\tif cfg.Placement == nil {\n\t\t\tcfg.Placement = &Placement{}\n\t\t}\n\t\tcfg.Placement.Tags = append(cfg.Placement.Tags, req.Tags...)\n\t}\n\n\tpeers, e := cc.selectPeerGroup(cfg.Replicas+1, currCluster, &cfg, currPeers, 1, nil)\n\tif len(peers) <= cfg.Replicas {\n\t\t// since expanding in the same cluster did not yield a result, try in different cluster\n\t\tpeers = nil\n\n\t\tclusters := map[string]struct{}{}\n\t\ts.nodeToInfo.Range(func(_, ni any) bool {\n\t\t\tif currCluster != ni.(nodeInfo).cluster {\n\t\t\t\tclusters[ni.(nodeInfo).cluster] = struct{}{}\n\t\t\t}\n\t\t\treturn true\n\t\t})\n\t\terrs := &selectPeerError{}\n\t\terrs.accumulate(e)\n\t\tfor cluster := range clusters {\n\t\t\tnewPeers, e := cc.selectPeerGroup(cfg.Replicas, cluster, &cfg, nil, 0, nil)\n\t\t\tif len(newPeers) >= cfg.Replicas {\n\t\t\t\tpeers = append([]string{}, currPeers...)\n\t\t\t\tpeers = append(peers, newPeers[:cfg.Replicas]...)\n\t\t\t\tbreak\n\t\t\t}\n\t\t\terrs.accumulate(e)\n\t\t}\n\t\tif peers == nil {\n\t\t\tresp.Error = NewJSClusterNoPeersError(errs)\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t}\n\n\tcfg.Placement = origPlacement\n\n\ts.Noticef(\"Requested move for stream '%s > %s' R=%d from %+v to %+v\",\n\t\taccName, streamName, cfg.Replicas, s.peerSetToNames(currPeers), s.peerSetToNames(peers))\n\n\t// We will always have peers and therefore never do a callout, therefore it is safe to call inline\n\t// We should be fine ignoring pedantic mode here. as we do not touch configuration.\n\ts.jsClusteredStreamUpdateRequest(&ciNew, targetAcc.(*Account), subject, reply, rmsg, &cfg, peers, false)\n}\n\n// Request to have the metaleader move a stream on a peer to another\nfunc (s *Server) jsLeaderServerStreamCancelMoveRequest(sub *subscription, c *client, _ *Account, subject, reply string, rmsg []byte) {\n\tif c == nil || !s.JetStreamEnabled() {\n\t\treturn\n\t}\n\n\tci, acc, _, msg, err := s.getRequestInfo(c, rmsg)\n\tif err != nil {\n\t\ts.Warnf(badAPIRequestT, msg)\n\t\treturn\n\t}\n\n\tjs, cc := s.getJetStreamCluster()\n\tif js == nil || cc == nil || cc.meta == nil {\n\t\treturn\n\t}\n\n\t// Extra checks here but only leader is listening.\n\tjs.mu.RLock()\n\tisLeader := cc.isLeader()\n\tjs.mu.RUnlock()\n\n\tif !isLeader {\n\t\treturn\n\t}\n\n\tvar resp = JSApiStreamUpdateResponse{ApiResponse: ApiResponse{Type: JSApiStreamUpdateResponseType}}\n\n\taccName := tokenAt(subject, 6)\n\tstreamName := tokenAt(subject, 7)\n\n\tif acc.GetName() != accName && acc != s.SystemAccount() {\n\t\treturn\n\t}\n\n\ttargetAcc, ok := s.accounts.Load(accName)\n\tif !ok {\n\t\tresp.Error = NewJSNoAccountError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tstreamFound := false\n\tcfg := StreamConfig{}\n\tcurrPeers := []string{}\n\tjs.mu.Lock()\n\tstreams, ok := cc.streams[accName]\n\tif ok {\n\t\tsa, ok := streams[streamName]\n\t\tif ok {\n\t\t\tcfg = *sa.Config.clone()\n\t\t\tstreamFound = true\n\t\t\tcurrPeers = sa.Group.Peers\n\t\t}\n\t}\n\tjs.mu.Unlock()\n\n\tif !streamFound {\n\t\tresp.Error = NewJSStreamNotFoundError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tif len(currPeers) <= cfg.Replicas {\n\t\tresp.Error = NewJSStreamMoveNotInProgressError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\t// make sure client is scoped to requested account\n\tciNew := *(ci)\n\tciNew.Account = accName\n\n\tpeers := currPeers[:cfg.Replicas]\n\n\t// Remove placement in case tags don't match\n\t// This can happen if the move was initiated by modifying the tags.\n\t// This is an account operation.\n\t// This can NOT happen when the move was initiated by the system account.\n\t// There move honors the original tag list.\n\tif cfg.Placement != nil && len(cfg.Placement.Tags) != 0 {\n\tFOR_TAGCHECK:\n\t\tfor _, peer := range peers {\n\t\t\tsi, ok := s.nodeToInfo.Load(peer)\n\t\t\tif !ok {\n\t\t\t\t// can't verify tags, do the safe thing and error\n\t\t\t\tresp.Error = NewJSStreamGeneralError(\n\t\t\t\t\tfmt.Errorf(\"peer %s not present for tag validation\", peer))\n\t\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\t\treturn\n\t\t\t}\n\t\t\tnodeTags := si.(nodeInfo).tags\n\t\t\tfor _, tag := range cfg.Placement.Tags {\n\t\t\t\tif !nodeTags.Contains(tag) {\n\t\t\t\t\t// clear placement as tags don't match\n\t\t\t\t\tcfg.Placement = nil\n\t\t\t\t\tbreak FOR_TAGCHECK\n\t\t\t\t}\n\t\t\t}\n\n\t\t}\n\t}\n\n\ts.Noticef(\"Requested cancel of move: R=%d '%s > %s' to peer set %+v and restore previous peer set %+v\",\n\t\tcfg.Replicas, accName, streamName, s.peerSetToNames(currPeers), s.peerSetToNames(peers))\n\n\t// We will always have peers and therefore never do a callout, therefore it is safe to call inline\n\ts.jsClusteredStreamUpdateRequest(&ciNew, targetAcc.(*Account), subject, reply, rmsg, &cfg, peers, false)\n}\n\n// Request to have an account purged\nfunc (s *Server) jsLeaderAccountPurgeRequest(sub *subscription, c *client, _ *Account, subject, reply string, rmsg []byte) {\n\tif c == nil || !s.JetStreamEnabled() {\n\t\treturn\n\t}\n\n\tci, acc, _, msg, err := s.getRequestInfo(c, rmsg)\n\tif err != nil {\n\t\ts.Warnf(badAPIRequestT, msg)\n\t\treturn\n\t}\n\tif acc != s.SystemAccount() {\n\t\treturn\n\t}\n\n\tjs := s.getJetStream()\n\tif js == nil {\n\t\treturn\n\t}\n\n\taccName := tokenAt(subject, 5)\n\n\tvar resp = JSApiAccountPurgeResponse{ApiResponse: ApiResponse{Type: JSApiAccountPurgeResponseType}}\n\n\tif !s.JetStreamIsClustered() {\n\t\tvar streams []*stream\n\t\tvar ac *Account\n\t\tif ac, err = s.lookupAccount(accName); err == nil && ac != nil {\n\t\t\tstreams = ac.streams()\n\t\t}\n\n\t\ts.Noticef(\"Purge request for account %s (streams: %d, hasAccount: %t)\",\n\t\t\taccName, len(streams), ac != nil)\n\n\t\tfor _, mset := range streams {\n\t\t\terr := mset.delete()\n\t\t\tif err != nil {\n\t\t\t\tresp.Error = NewJSStreamDeleteError(err)\n\t\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t\tif err := os.RemoveAll(filepath.Join(js.config.StoreDir, accName)); err != nil {\n\t\t\tresp.Error = NewJSStreamGeneralError(err)\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\tresp.Initiated = true\n\t\ts.sendAPIResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\t_, cc := s.getJetStreamCluster()\n\tif cc == nil || cc.meta == nil || !cc.isLeader() {\n\t\treturn\n\t}\n\n\tif js.isMetaRecovering() {\n\t\t// While in recovery mode, the data structures are not fully initialized\n\t\tresp.Error = NewJSClusterNotAvailError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tjs.mu.RLock()\n\tns, nc := 0, 0\n\tstreams, hasAccount := cc.streams[accName]\n\tfor _, osa := range streams {\n\t\tfor _, oca := range osa.consumers {\n\t\t\toca.deleted = true\n\t\t\tca := &consumerAssignment{Group: oca.Group, Stream: oca.Stream, Name: oca.Name, Config: oca.Config, Subject: subject, Client: oca.Client}\n\t\t\tcc.meta.Propose(encodeDeleteConsumerAssignment(ca))\n\t\t\tnc++\n\t\t}\n\t\tsa := &streamAssignment{Group: osa.Group, Config: osa.Config, Subject: subject, Client: osa.Client}\n\t\tcc.meta.Propose(encodeDeleteStreamAssignment(sa))\n\t\tns++\n\t}\n\tjs.mu.RUnlock()\n\n\ts.Noticef(\"Purge request for account %s (streams: %d, consumer: %d, hasAccount: %t)\", accName, ns, nc, hasAccount)\n\n\tresp.Initiated = true\n\ts.sendAPIResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n}\n\n// Request to have the meta leader stepdown.\n// These will only be received by the meta leader, so less checking needed.\nfunc (s *Server) jsLeaderStepDownRequest(sub *subscription, c *client, _ *Account, subject, reply string, rmsg []byte) {\n\tif c == nil || !s.JetStreamEnabled() {\n\t\treturn\n\t}\n\n\tci, acc, _, msg, err := s.getRequestInfo(c, rmsg)\n\tif err != nil {\n\t\ts.Warnf(badAPIRequestT, msg)\n\t\treturn\n\t}\n\n\t// This should only be coming from the System Account.\n\tif acc != s.SystemAccount() {\n\t\ts.RateLimitWarnf(\"JetStream API stepdown request from non-system account: %q user: %q\", ci.serviceAccount(), ci.User)\n\t\treturn\n\t}\n\n\tjs, cc := s.getJetStreamCluster()\n\tif js == nil || cc == nil || cc.meta == nil {\n\t\treturn\n\t}\n\n\t// Extra checks here but only leader is listening.\n\tjs.mu.RLock()\n\tisLeader := cc.isLeader()\n\tjs.mu.RUnlock()\n\n\tif !isLeader {\n\t\treturn\n\t}\n\n\tvar preferredLeader string\n\tvar resp = JSApiLeaderStepDownResponse{ApiResponse: ApiResponse{Type: JSApiLeaderStepDownResponseType}}\n\n\tif isJSONObjectOrArray(msg) {\n\t\tvar req JSApiLeaderStepdownRequest\n\t\tif err := s.unmarshalRequest(c, acc, subject, msg, &req); err != nil {\n\t\t\tresp.Error = NewJSInvalidJSONError(err)\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\tif preferredLeader, resp.Error = s.getStepDownPreferredPlacement(cc.meta, req.Placement); resp.Error != nil {\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t}\n\n\t// Call actual stepdown.\n\terr = cc.meta.StepDown(preferredLeader)\n\tif err != nil {\n\t\tresp.Error = NewJSRaftGeneralError(err, Unless(err))\n\t} else {\n\t\tresp.Success = true\n\t}\n\ts.sendAPIResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(resp))\n}\n\n// Check if given []bytes is a JSON Object or Array.\n// Technically, valid JSON can also be a plain string or number, but for our use case,\n// we care only for JSON objects or arrays which starts with `[` or `{`.\n// This function does not have to ensure valid JSON in its entirety. It is used merely\n// to hint the codepath if it should attempt to parse the request as JSON or not.\nfunc isJSONObjectOrArray(req []byte) bool {\n\t// Skip leading JSON whitespace (space, tab, newline, carriage return)\n\ti := 0\n\tfor i < len(req) && (req[i] == ' ' || req[i] == '\\t' || req[i] == '\\n' || req[i] == '\\r') {\n\t\ti++\n\t}\n\t// Check for empty input after trimming\n\tif i >= len(req) {\n\t\treturn false\n\t}\n\t// Check if the first non-whitespace character is '{' or '['\n\treturn req[i] == '{' || req[i] == '['\n}\n\nfunc isEmptyRequest(req []byte) bool {\n\tif len(req) == 0 {\n\t\treturn true\n\t}\n\tif bytes.Equal(req, []byte(\"{}\")) {\n\t\treturn true\n\t}\n\t// If we are here we didn't get our simple match, but still could be valid.\n\tvar v any\n\tif err := json.Unmarshal(req, &v); err != nil {\n\t\treturn false\n\t}\n\tvm, ok := v.(map[string]any)\n\tif !ok {\n\t\treturn false\n\t}\n\treturn len(vm) == 0\n}\n\n// getStepDownPreferredPlacement attempts to work out what the best placement is\n// for a stepdown request. The preferred server name always takes precedence, but\n// if not specified, the placement will be used to filter by cluster. The caller\n// should check for return API errors and return those to the requestor if needed.\nfunc (s *Server) getStepDownPreferredPlacement(group RaftNode, placement *Placement) (string, *ApiError) {\n\tif placement == nil {\n\t\treturn _EMPTY_, nil\n\t}\n\tvar preferredLeader string\n\tif placement.Preferred != _EMPTY_ {\n\t\tfor _, p := range group.Peers() {\n\t\t\tsi, ok := s.nodeToInfo.Load(p.ID)\n\t\t\tif !ok || si == nil {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif si.(nodeInfo).name == placement.Preferred {\n\t\t\t\tpreferredLeader = p.ID\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\tif preferredLeader == group.ID() {\n\t\t\treturn _EMPTY_, NewJSClusterNoPeersError(fmt.Errorf(\"preferred server %q is already leader\", placement.Preferred))\n\t\t}\n\t\tif preferredLeader == _EMPTY_ {\n\t\t\treturn _EMPTY_, NewJSClusterNoPeersError(fmt.Errorf(\"preferred server %q not known\", placement.Preferred))\n\t\t}\n\t} else {\n\t\tpossiblePeers := make(map[*Peer]nodeInfo, len(group.Peers()))\n\t\tourID := group.ID()\n\t\tfor _, p := range group.Peers() {\n\t\t\tif p == nil {\n\t\t\t\tcontinue // ... shouldn't happen.\n\t\t\t}\n\t\t\tsi, ok := s.nodeToInfo.Load(p.ID)\n\t\t\tif !ok || si == nil {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tni := si.(nodeInfo)\n\t\t\tif ni.offline || p.ID == ourID {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tpossiblePeers[p] = ni\n\t\t}\n\t\t// If cluster is specified, filter out anything not matching the cluster name.\n\t\tif placement.Cluster != _EMPTY_ {\n\t\t\tfor p, si := range possiblePeers {\n\t\t\t\tif si.cluster != placement.Cluster {\n\t\t\t\t\tdelete(possiblePeers, p)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t// If tags are specified, filter out anything not matching all supplied tags.\n\t\tif len(placement.Tags) > 0 {\n\t\t\tfor p, si := range possiblePeers {\n\t\t\t\tmatchesAll := true\n\t\t\t\tfor _, tag := range placement.Tags {\n\t\t\t\t\tif matchesAll = matchesAll && si.tags.Contains(tag); !matchesAll {\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif !matchesAll {\n\t\t\t\t\tdelete(possiblePeers, p)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t// If there are no possible peers, return an error.\n\t\tif len(possiblePeers) == 0 {\n\t\t\treturn _EMPTY_, NewJSClusterNoPeersError(fmt.Errorf(\"no replacement peer connected\"))\n\t\t}\n\t\t// Take advantage of random map iteration order to select the preferred.\n\t\tfor p := range possiblePeers {\n\t\t\tpreferredLeader = p.ID\n\t\t\tbreak\n\t\t}\n\t}\n\treturn preferredLeader, nil\n}\n\n// Request to delete a stream.\nfunc (s *Server) jsStreamDeleteRequest(sub *subscription, c *client, _ *Account, subject, reply string, rmsg []byte) {\n\tif c == nil || !s.JetStreamEnabled() {\n\t\treturn\n\t}\n\tci, acc, _, msg, err := s.getRequestInfo(c, rmsg)\n\tif err != nil {\n\t\ts.Warnf(badAPIRequestT, msg)\n\t\treturn\n\t}\n\n\tvar resp = JSApiStreamDeleteResponse{ApiResponse: ApiResponse{Type: JSApiStreamDeleteResponseType}}\n\n\t// Determine if we should proceed here when we are in clustered mode.\n\tif s.JetStreamIsClustered() {\n\t\tjs, cc := s.getJetStreamCluster()\n\t\tif js == nil || cc == nil {\n\t\t\treturn\n\t\t}\n\t\tif js.isLeaderless() {\n\t\t\tresp.Error = NewJSClusterNotAvailError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\t// Make sure we are meta leader.\n\t\tif !s.JetStreamIsLeader() {\n\t\t\treturn\n\t\t}\n\t}\n\n\tif hasJS, doErr := acc.checkJetStream(); !hasJS {\n\t\tif doErr {\n\t\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t}\n\t\treturn\n\t}\n\n\tif !isEmptyRequest(msg) {\n\t\tresp.Error = NewJSNotEmptyRequestError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\tstream := streamNameFromSubject(subject)\n\n\t// Clustered.\n\tif s.JetStreamIsClustered() {\n\t\ts.jsClusteredStreamDeleteRequest(ci, acc, stream, subject, reply, msg)\n\t\treturn\n\t}\n\n\tmset, err := acc.lookupStream(stream)\n\tif err != nil {\n\t\tresp.Error = NewJSStreamNotFoundError(Unless(err))\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tif err := mset.delete(); err != nil {\n\t\tresp.Error = NewJSStreamDeleteError(err, Unless(err))\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\tresp.Success = true\n\ts.sendAPIResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(resp))\n}\n\n// Request to delete a message.\n// This expects a stream sequence number as the msg body.\nfunc (s *Server) jsMsgDeleteRequest(sub *subscription, c *client, _ *Account, subject, reply string, rmsg []byte) {\n\tif c == nil || !s.JetStreamEnabled() {\n\t\treturn\n\t}\n\tci, acc, _, msg, err := s.getRequestInfo(c, rmsg)\n\tif err != nil {\n\t\ts.Warnf(badAPIRequestT, msg)\n\t\treturn\n\t}\n\n\tstream := tokenAt(subject, 6)\n\n\tvar resp = JSApiMsgDeleteResponse{ApiResponse: ApiResponse{Type: JSApiMsgDeleteResponseType}}\n\n\t// If we are in clustered mode we need to be the stream leader to proceed.\n\tif s.JetStreamIsClustered() {\n\t\t// Check to make sure the stream is assigned.\n\t\tjs, cc := s.getJetStreamCluster()\n\t\tif js == nil || cc == nil {\n\t\t\treturn\n\t\t}\n\t\tif js.isLeaderless() {\n\t\t\tresp.Error = NewJSClusterNotAvailError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\n\t\tjs.mu.RLock()\n\t\tisLeader, sa := cc.isLeader(), js.streamAssignment(acc.Name, stream)\n\t\tjs.mu.RUnlock()\n\n\t\tif isLeader && sa == nil {\n\t\t\t// We can't find the stream, so mimic what would be the errors below.\n\t\t\tif hasJS, doErr := acc.checkJetStream(); !hasJS {\n\t\t\t\tif doErr {\n\t\t\t\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\t\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\t\t}\n\t\t\t\treturn\n\t\t\t}\n\t\t\t// No stream present.\n\t\t\tresp.Error = NewJSStreamNotFoundError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t} else if sa == nil {\n\t\t\treturn\n\t\t}\n\n\t\t// Check to see if we are a member of the group and if the group has no leader.\n\t\tif js.isGroupLeaderless(sa.Group) {\n\t\t\tresp.Error = NewJSClusterNotAvailError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\n\t\t// We have the stream assigned and a leader, so only the stream leader should answer.\n\t\tif !acc.JetStreamIsStreamLeader(stream) {\n\t\t\treturn\n\t\t}\n\t}\n\n\tif hasJS, doErr := acc.checkJetStream(); !hasJS {\n\t\tif doErr {\n\t\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t}\n\t\treturn\n\t}\n\tif isEmptyRequest(msg) {\n\t\tresp.Error = NewJSBadRequestError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\tvar req JSApiMsgDeleteRequest\n\tif err := s.unmarshalRequest(c, acc, subject, msg, &req); err != nil {\n\t\tresp.Error = NewJSInvalidJSONError(err)\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tmset, err := acc.lookupStream(stream)\n\tif err != nil {\n\t\tresp.Error = NewJSStreamNotFoundError(Unless(err))\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\tif mset.cfg.Sealed {\n\t\tresp.Error = NewJSStreamSealedError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\tif mset.cfg.DenyDelete {\n\t\tresp.Error = NewJSStreamMsgDeleteFailedError(errors.New(\"message delete not permitted\"))\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tif s.JetStreamIsClustered() {\n\t\ts.jsClusteredMsgDeleteRequest(ci, acc, mset, stream, subject, reply, &req, rmsg)\n\t\treturn\n\t}\n\n\tvar removed bool\n\tif req.NoErase {\n\t\tremoved, err = mset.removeMsg(req.Seq)\n\t} else {\n\t\tremoved, err = mset.eraseMsg(req.Seq)\n\t}\n\tif err != nil {\n\t\tresp.Error = NewJSStreamMsgDeleteFailedError(err, Unless(err))\n\t} else if !removed {\n\t\tresp.Error = NewJSSequenceNotFoundError(req.Seq)\n\t} else {\n\t\tresp.Success = true\n\t}\n\ts.sendAPIResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(resp))\n}\n\n// Request to get a raw stream message.\nfunc (s *Server) jsMsgGetRequest(sub *subscription, c *client, _ *Account, subject, reply string, rmsg []byte) {\n\tif c == nil || !s.JetStreamEnabled() {\n\t\treturn\n\t}\n\tci, acc, _, msg, err := s.getRequestInfo(c, rmsg)\n\tif err != nil {\n\t\ts.Warnf(badAPIRequestT, msg)\n\t\treturn\n\t}\n\n\tstream := tokenAt(subject, 6)\n\n\tvar resp = JSApiMsgGetResponse{ApiResponse: ApiResponse{Type: JSApiMsgGetResponseType}}\n\n\t// If we are in clustered mode we need to be the stream leader to proceed.\n\tif s.JetStreamIsClustered() {\n\t\t// Check to make sure the stream is assigned.\n\t\tjs, cc := s.getJetStreamCluster()\n\t\tif js == nil || cc == nil {\n\t\t\treturn\n\t\t}\n\t\tif js.isLeaderless() {\n\t\t\tresp.Error = NewJSClusterNotAvailError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\n\t\tjs.mu.RLock()\n\t\tisLeader, sa := cc.isLeader(), js.streamAssignment(acc.Name, stream)\n\t\tjs.mu.RUnlock()\n\n\t\tif isLeader && sa == nil {\n\t\t\t// We can't find the stream, so mimic what would be the errors below.\n\t\t\tif hasJS, doErr := acc.checkJetStream(); !hasJS {\n\t\t\t\tif doErr {\n\t\t\t\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\t\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\t\t}\n\t\t\t\treturn\n\t\t\t}\n\t\t\t// No stream present.\n\t\t\tresp.Error = NewJSStreamNotFoundError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t} else if sa == nil {\n\t\t\treturn\n\t\t}\n\n\t\t// Check to see if we are a member of the group and if the group has no leader.\n\t\tif js.isGroupLeaderless(sa.Group) {\n\t\t\tresp.Error = NewJSClusterNotAvailError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\n\t\t// We have the stream assigned and a leader, so only the stream leader should answer.\n\t\tif !acc.JetStreamIsStreamLeader(stream) {\n\t\t\treturn\n\t\t}\n\t}\n\n\tif hasJS, doErr := acc.checkJetStream(); !hasJS {\n\t\tif doErr {\n\t\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t}\n\t\treturn\n\t}\n\tif isEmptyRequest(msg) {\n\t\tresp.Error = NewJSBadRequestError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\tvar req JSApiMsgGetRequest\n\tif err := s.unmarshalRequest(c, acc, subject, msg, &req); err != nil {\n\t\tresp.Error = NewJSInvalidJSONError(err)\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\t// This version does not support batch.\n\tif req.Batch > 0 || req.MaxBytes > 0 {\n\t\tresp.Error = NewJSBadRequestError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\t// Validate non-conflicting options. Seq, LastFor, and AsOfTime are mutually exclusive.\n\t// NextFor can be paired with Seq or AsOfTime indicating a filter subject.\n\tif (req.Seq > 0 && req.LastFor != _EMPTY_) ||\n\t\t(req.Seq == 0 && req.LastFor == _EMPTY_ && req.NextFor == _EMPTY_ && req.StartTime == nil) ||\n\t\t(req.Seq > 0 && req.StartTime != nil) ||\n\t\t(req.StartTime != nil && req.LastFor != _EMPTY_) ||\n\t\t(req.LastFor != _EMPTY_ && req.NextFor != _EMPTY_) {\n\t\tresp.Error = NewJSBadRequestError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tmset, err := acc.lookupStream(stream)\n\tif err != nil {\n\t\tresp.Error = NewJSStreamNotFoundError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tvar svp StoreMsg\n\tvar sm *StoreMsg\n\n\t// If AsOfTime is set, perform this first to get the sequence.\n\tvar seq uint64\n\tif req.StartTime != nil {\n\t\tseq = mset.store.GetSeqFromTime(*req.StartTime)\n\t} else {\n\t\tseq = req.Seq\n\t}\n\n\tif seq > 0 && req.NextFor == _EMPTY_ {\n\t\tsm, err = mset.store.LoadMsg(seq, &svp)\n\t} else if req.NextFor != _EMPTY_ {\n\t\tsm, _, err = mset.store.LoadNextMsg(req.NextFor, subjectHasWildcard(req.NextFor), seq, &svp)\n\t} else {\n\t\tsm, err = mset.store.LoadLastMsg(req.LastFor, &svp)\n\t}\n\tif err != nil {\n\t\tresp.Error = NewJSNoMessageFoundError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\tresp.Message = &StoredMsg{\n\t\tSubject:  sm.subj,\n\t\tSequence: sm.seq,\n\t\tHeader:   sm.hdr,\n\t\tData:     sm.msg,\n\t\tTime:     time.Unix(0, sm.ts).UTC(),\n\t}\n\n\t// Don't send response through API layer for this call.\n\ts.sendInternalAccountMsg(nil, reply, s.jsonResponse(resp))\n}\n\nfunc (s *Server) jsConsumerUnpinRequest(sub *subscription, c *client, _ *Account, subject, reply string, rmsg []byte) {\n\tif c == nil || !s.JetStreamEnabled() {\n\t\treturn\n\t}\n\n\tci, acc, _, msg, err := s.getRequestInfo(c, rmsg)\n\tif err != nil {\n\t\ts.Warnf(badAPIRequestT, msg)\n\t\treturn\n\t}\n\n\tstream := streamNameFromSubject(subject)\n\tconsumer := consumerNameFromSubject(subject)\n\n\tvar req JSApiConsumerUnpinRequest\n\tvar resp = JSApiConsumerUnpinResponse{ApiResponse: ApiResponse{Type: JSApiConsumerUnpinResponseType}}\n\n\tif err := json.Unmarshal(msg, &req); err != nil {\n\t\tresp.Error = NewJSInvalidJSONError(err)\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tif req.Group == _EMPTY_ {\n\t\tresp.Error = NewJSInvalidJSONError(errors.New(\"consumer group not specified\"))\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tif !validGroupName.MatchString(req.Group) {\n\t\tresp.Error = NewJSConsumerInvalidGroupNameError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\tif s.JetStreamIsClustered() {\n\t\t// Check to make sure the stream is assigned.\n\t\tjs, cc := s.getJetStreamCluster()\n\t\tif js == nil || cc == nil {\n\t\t\treturn\n\t\t}\n\n\t\t// First check if the stream and consumer is there.\n\t\tjs.mu.RLock()\n\t\tsa := js.streamAssignment(acc.Name, stream)\n\t\tif sa == nil {\n\t\t\tjs.mu.RUnlock()\n\t\t\tresp.Error = NewJSStreamNotFoundError(Unless(err))\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\n\t\tca, ok := sa.consumers[consumer]\n\t\tif !ok || ca == nil {\n\t\t\tjs.mu.RUnlock()\n\t\t\tresp.Error = NewJSConsumerNotFoundError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\tjs.mu.RUnlock()\n\n\t\t// Then check if we are the leader.\n\t\tmset, err := acc.lookupStream(stream)\n\t\tif err != nil {\n\t\t\treturn\n\t\t}\n\n\t\to := mset.lookupConsumer(consumer)\n\t\tif o == nil {\n\t\t\treturn\n\t\t}\n\t\tif !o.isLeader() {\n\t\t\treturn\n\t\t}\n\t}\n\n\tif hasJS, doErr := acc.checkJetStream(); !hasJS {\n\t\tif doErr {\n\t\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t}\n\t\treturn\n\t}\n\n\tmset, err := acc.lookupStream(stream)\n\tif err != nil {\n\t\tresp.Error = NewJSStreamNotFoundError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\to := mset.lookupConsumer(consumer)\n\tif o == nil {\n\t\tresp.Error = NewJSConsumerNotFoundError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tvar foundPriority bool\n\tfor _, group := range o.config().PriorityGroups {\n\t\tif group == req.Group {\n\t\t\tfoundPriority = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif !foundPriority {\n\t\tresp.Error = NewJSConsumerInvalidPriorityGroupError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\to.mu.Lock()\n\to.currentPinId = _EMPTY_\n\to.sendUnpinnedAdvisoryLocked(req.Group, \"admin\")\n\to.mu.Unlock()\n\ts.sendAPIResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(resp))\n}\n\n// Request to purge a stream.\nfunc (s *Server) jsStreamPurgeRequest(sub *subscription, c *client, _ *Account, subject, reply string, rmsg []byte) {\n\tif c == nil || !s.JetStreamEnabled() {\n\t\treturn\n\t}\n\tci, acc, _, msg, err := s.getRequestInfo(c, rmsg)\n\tif err != nil {\n\t\ts.Warnf(badAPIRequestT, msg)\n\t\treturn\n\t}\n\n\tstream := streamNameFromSubject(subject)\n\n\tvar resp = JSApiStreamPurgeResponse{ApiResponse: ApiResponse{Type: JSApiStreamPurgeResponseType}}\n\n\t// If we are in clustered mode we need to be the stream leader to proceed.\n\tif s.JetStreamIsClustered() {\n\t\t// Check to make sure the stream is assigned.\n\t\tjs, cc := s.getJetStreamCluster()\n\t\tif js == nil || cc == nil {\n\t\t\treturn\n\t\t}\n\n\t\tjs.mu.RLock()\n\t\tisLeader, sa := cc.isLeader(), js.streamAssignment(acc.Name, stream)\n\t\tjs.mu.RUnlock()\n\n\t\tif isLeader && sa == nil {\n\t\t\t// We can't find the stream, so mimic what would be the errors below.\n\t\t\tif hasJS, doErr := acc.checkJetStream(); !hasJS {\n\t\t\t\tif doErr {\n\t\t\t\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\t\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\t\t}\n\t\t\t\treturn\n\t\t\t}\n\t\t\t// No stream present.\n\t\t\tresp.Error = NewJSStreamNotFoundError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t} else if sa == nil {\n\t\t\tif js.isLeaderless() {\n\t\t\t\tresp.Error = NewJSClusterNotAvailError()\n\t\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\t}\n\t\t\treturn\n\t\t}\n\n\t\t// Check to see if we are a member of the group and if the group has no leader.\n\t\tif js.isGroupLeaderless(sa.Group) {\n\t\t\tresp.Error = NewJSClusterNotAvailError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\n\t\t// We have the stream assigned and a leader, so only the stream leader should answer.\n\t\tif !acc.JetStreamIsStreamLeader(stream) {\n\t\t\tif js.isLeaderless() {\n\t\t\t\tresp.Error = NewJSClusterNotAvailError()\n\t\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\t}\n\t\t\treturn\n\t\t}\n\t}\n\n\tif hasJS, doErr := acc.checkJetStream(); !hasJS {\n\t\tif doErr {\n\t\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t}\n\t\treturn\n\t}\n\n\tvar purgeRequest *JSApiStreamPurgeRequest\n\tif isJSONObjectOrArray(msg) {\n\t\tvar req JSApiStreamPurgeRequest\n\t\tif err := s.unmarshalRequest(c, acc, subject, msg, &req); err != nil {\n\t\t\tresp.Error = NewJSInvalidJSONError(err)\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\tif req.Sequence > 0 && req.Keep > 0 {\n\t\t\tresp.Error = NewJSBadRequestError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\tpurgeRequest = &req\n\t}\n\n\tmset, err := acc.lookupStream(stream)\n\tif err != nil {\n\t\tresp.Error = NewJSStreamNotFoundError(Unless(err))\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\tif mset.cfg.Sealed {\n\t\tresp.Error = NewJSStreamSealedError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\tif mset.cfg.DenyPurge {\n\t\tresp.Error = NewJSStreamPurgeFailedError(errors.New(\"stream purge not permitted\"))\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tif s.JetStreamIsClustered() {\n\t\ts.jsClusteredStreamPurgeRequest(ci, acc, mset, stream, subject, reply, rmsg, purgeRequest)\n\t\treturn\n\t}\n\n\tpurged, err := mset.purge(purgeRequest)\n\tif err != nil {\n\t\tresp.Error = NewJSStreamGeneralError(err, Unless(err))\n\t} else {\n\t\tresp.Purged = purged\n\t\tresp.Success = true\n\t}\n\ts.sendAPIResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(resp))\n}\n\nfunc (acc *Account) jsNonClusteredStreamLimitsCheck(cfg *StreamConfig) *ApiError {\n\tvar replicas int\n\tif cfg != nil {\n\t\treplicas = cfg.Replicas\n\t}\n\tselectedLimits, tier, jsa, apiErr := acc.selectLimits(replicas)\n\tif apiErr != nil {\n\t\treturn apiErr\n\t}\n\tjsa.mu.RLock()\n\tdefer jsa.mu.RUnlock()\n\tif selectedLimits.MaxStreams > 0 && jsa.countStreams(tier, cfg) >= selectedLimits.MaxStreams {\n\t\treturn NewJSMaximumStreamsLimitError()\n\t}\n\treserved := jsa.tieredReservation(tier, cfg)\n\tif err := jsa.js.checkAllLimits(selectedLimits, cfg, reserved, 0); err != nil {\n\t\treturn NewJSStreamLimitsError(err, Unless(err))\n\t}\n\treturn nil\n}\n\n// Request to restore a stream.\nfunc (s *Server) jsStreamRestoreRequest(sub *subscription, c *client, _ *Account, subject, reply string, rmsg []byte) {\n\tif c == nil || !s.JetStreamIsLeader() {\n\t\treturn\n\t}\n\tci, acc, _, msg, err := s.getRequestInfo(c, rmsg)\n\tif err != nil {\n\t\ts.Warnf(badAPIRequestT, msg)\n\t\treturn\n\t}\n\n\tvar resp = JSApiStreamRestoreResponse{ApiResponse: ApiResponse{Type: JSApiStreamRestoreResponseType}}\n\tif !acc.JetStreamEnabled() {\n\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\tif isEmptyRequest(msg) {\n\t\tresp.Error = NewJSBadRequestError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tvar req JSApiStreamRestoreRequest\n\tif err := s.unmarshalRequest(c, acc, subject, msg, &req); err != nil {\n\t\tresp.Error = NewJSInvalidJSONError(err)\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tstream := streamNameFromSubject(subject)\n\n\tif stream != req.Config.Name && req.Config.Name == _EMPTY_ {\n\t\treq.Config.Name = stream\n\t}\n\n\t// check stream config at the start of the restore process, not at the end\n\tcfg, apiErr := s.checkStreamCfg(&req.Config, acc, false)\n\tif apiErr != nil {\n\t\tresp.Error = apiErr\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tif s.JetStreamIsClustered() {\n\t\ts.jsClusteredStreamRestoreRequest(ci, acc, &req, subject, reply, rmsg)\n\t\treturn\n\t}\n\n\tif err := acc.jsNonClusteredStreamLimitsCheck(&cfg); err != nil {\n\t\tresp.Error = err\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tif _, err := acc.lookupStream(stream); err == nil {\n\t\tresp.Error = NewJSStreamNameExistRestoreFailedError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tif hasJS, doErr := acc.checkJetStream(); !hasJS {\n\t\tif doErr {\n\t\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t}\n\t\treturn\n\t}\n\n\ts.processStreamRestore(ci, acc, &req.Config, subject, reply, string(msg))\n}\n\nfunc (s *Server) processStreamRestore(ci *ClientInfo, acc *Account, cfg *StreamConfig, subject, reply, msg string) <-chan error {\n\tjs := s.getJetStream()\n\n\tvar resp = JSApiStreamRestoreResponse{ApiResponse: ApiResponse{Type: JSApiStreamRestoreResponseType}}\n\n\tsnapDir := filepath.Join(js.config.StoreDir, snapStagingDir)\n\tif _, err := os.Stat(snapDir); os.IsNotExist(err) {\n\t\tif err := os.MkdirAll(snapDir, defaultDirPerms); err != nil {\n\t\t\tresp.Error = &ApiError{Code: 503, Description: \"JetStream unable to create temp storage for restore\"}\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn nil\n\t\t}\n\t}\n\n\ttfile, err := os.CreateTemp(snapDir, \"js-restore-\")\n\tif err != nil {\n\t\tresp.Error = NewJSTempStorageFailedError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, msg, s.jsonResponse(&resp))\n\t\treturn nil\n\t}\n\n\tstreamName := cfg.Name\n\ts.Noticef(\"Starting restore for stream '%s > %s'\", acc.Name, streamName)\n\n\tstart := time.Now().UTC()\n\tdomain := s.getOpts().JetStreamDomain\n\ts.publishAdvisory(acc, JSAdvisoryStreamRestoreCreatePre+\".\"+streamName, &JSRestoreCreateAdvisory{\n\t\tTypedEvent: TypedEvent{\n\t\t\tType: JSRestoreCreateAdvisoryType,\n\t\t\tID:   nuid.Next(),\n\t\t\tTime: start,\n\t\t},\n\t\tStream: streamName,\n\t\tClient: ci.forAdvisory(),\n\t\tDomain: domain,\n\t})\n\n\t// Create our internal subscription to accept the snapshot.\n\trestoreSubj := fmt.Sprintf(jsRestoreDeliverT, streamName, nuid.Next())\n\n\ttype result struct {\n\t\terr   error\n\t\treply string\n\t}\n\n\t// For signaling to upper layers.\n\tresultCh := make(chan result, 1)\n\tactiveQ := newIPQueue[int](s, fmt.Sprintf(\"[ACC:%s] stream '%s' restore\", acc.Name, streamName)) // of int\n\n\tvar total int\n\n\t// FIXME(dlc) - Probably take out of network path eventually due to disk I/O?\n\tprocessChunk := func(sub *subscription, c *client, _ *Account, subject, reply string, msg []byte) {\n\t\t// We require reply subjects to communicate back failures, flow etc. If they do not have one log and cancel.\n\t\tif reply == _EMPTY_ {\n\t\t\tsub.client.processUnsub(sub.sid)\n\t\t\tresultCh <- result{\n\t\t\t\tfmt.Errorf(\"restore for stream '%s > %s' requires reply subject for each chunk\", acc.Name, streamName),\n\t\t\t\treply,\n\t\t\t}\n\t\t\treturn\n\t\t}\n\t\t// Account client messages have \\r\\n on end. This is an error.\n\t\tif len(msg) < LEN_CR_LF {\n\t\t\tsub.client.processUnsub(sub.sid)\n\t\t\tresultCh <- result{\n\t\t\t\tfmt.Errorf(\"restore for stream '%s > %s' received short chunk\", acc.Name, streamName),\n\t\t\t\treply,\n\t\t\t}\n\t\t\treturn\n\t\t}\n\t\t// Adjust.\n\t\tmsg = msg[:len(msg)-LEN_CR_LF]\n\n\t\t// This means we are complete with our transfer from the client.\n\t\tif len(msg) == 0 {\n\t\t\ts.Debugf(\"Finished staging restore for stream '%s > %s'\", acc.Name, streamName)\n\t\t\tresultCh <- result{err, reply}\n\t\t\treturn\n\t\t}\n\n\t\t// We track total and check on server limits.\n\t\t// TODO(dlc) - We could check apriori and cancel initial request if we know it won't fit.\n\t\ttotal += len(msg)\n\t\tif js.wouldExceedLimits(FileStorage, total) {\n\t\t\ts.resourcesExceededError()\n\t\t\tresultCh <- result{NewJSInsufficientResourcesError(), reply}\n\t\t\treturn\n\t\t}\n\n\t\t// Append chunk to temp file. Mark as issue if we encounter an error.\n\t\tif n, err := tfile.Write(msg); n != len(msg) || err != nil {\n\t\t\tresultCh <- result{err, reply}\n\t\t\tif reply != _EMPTY_ {\n\t\t\t\ts.sendInternalAccountMsg(acc, reply, \"-ERR 'storage failure during restore'\")\n\t\t\t}\n\t\t\treturn\n\t\t}\n\n\t\tactiveQ.push(len(msg))\n\n\t\ts.sendInternalAccountMsg(acc, reply, nil)\n\t}\n\n\tsub, err := acc.subscribeInternal(restoreSubj, processChunk)\n\tif err != nil {\n\t\ttfile.Close()\n\t\tos.Remove(tfile.Name())\n\t\tresp.Error = NewJSRestoreSubscribeFailedError(err, restoreSubj)\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, msg, s.jsonResponse(&resp))\n\t\treturn nil\n\t}\n\n\t// Mark the subject so the end user knows where to send the snapshot chunks.\n\tresp.DeliverSubject = restoreSubj\n\ts.sendAPIResponse(ci, acc, subject, reply, msg, s.jsonResponse(resp))\n\n\tdoneCh := make(chan error, 1)\n\n\t// Monitor the progress from another Go routine.\n\ts.startGoRoutine(func() {\n\t\tdefer s.grWG.Done()\n\t\tdefer func() {\n\t\t\ttfile.Close()\n\t\t\tos.Remove(tfile.Name())\n\t\t\tsub.client.processUnsub(sub.sid)\n\t\t\tactiveQ.unregister()\n\t\t}()\n\n\t\tconst activityInterval = 5 * time.Second\n\t\tnotActive := time.NewTimer(activityInterval)\n\t\tdefer notActive.Stop()\n\n\t\ttotal := 0\n\t\tfor {\n\t\t\tselect {\n\t\t\tcase result := <-resultCh:\n\t\t\t\terr := result.err\n\t\t\t\tvar mset *stream\n\n\t\t\t\t// If we staged properly go ahead and do restore now.\n\t\t\t\tif err == nil {\n\t\t\t\t\ts.Debugf(\"Finalizing restore for stream '%s > %s'\", acc.Name, streamName)\n\t\t\t\t\ttfile.Seek(0, 0)\n\t\t\t\t\tmset, err = acc.RestoreStream(cfg, tfile)\n\t\t\t\t} else {\n\t\t\t\t\terrStr := err.Error()\n\t\t\t\t\ttmp := []rune(errStr)\n\t\t\t\t\ttmp[0] = unicode.ToUpper(tmp[0])\n\t\t\t\t\ts.Warnf(errStr)\n\t\t\t\t}\n\n\t\t\t\tend := time.Now().UTC()\n\n\t\t\t\t// TODO(rip) - Should this have the error code in it??\n\t\t\t\ts.publishAdvisory(acc, JSAdvisoryStreamRestoreCompletePre+\".\"+streamName, &JSRestoreCompleteAdvisory{\n\t\t\t\t\tTypedEvent: TypedEvent{\n\t\t\t\t\t\tType: JSRestoreCompleteAdvisoryType,\n\t\t\t\t\t\tID:   nuid.Next(),\n\t\t\t\t\t\tTime: end,\n\t\t\t\t\t},\n\t\t\t\t\tStream: streamName,\n\t\t\t\t\tStart:  start,\n\t\t\t\t\tEnd:    end,\n\t\t\t\t\tBytes:  int64(total),\n\t\t\t\t\tClient: ci.forAdvisory(),\n\t\t\t\t\tDomain: domain,\n\t\t\t\t})\n\n\t\t\t\tvar resp = JSApiStreamCreateResponse{ApiResponse: ApiResponse{Type: JSApiStreamCreateResponseType}}\n\n\t\t\t\tif err != nil {\n\t\t\t\t\tresp.Error = NewJSStreamRestoreError(err, Unless(err))\n\t\t\t\t\ts.Warnf(\"Restore failed for %s for stream '%s > %s' in %v\",\n\t\t\t\t\t\tfriendlyBytes(int64(total)), acc.Name, streamName, end.Sub(start))\n\t\t\t\t} else {\n\t\t\t\t\tmsetCfg := mset.config()\n\t\t\t\t\tresp.StreamInfo = &StreamInfo{\n\t\t\t\t\t\tCreated:   mset.createdTime(),\n\t\t\t\t\t\tState:     mset.state(),\n\t\t\t\t\t\tConfig:    *setDynamicStreamMetadata(&msetCfg),\n\t\t\t\t\t\tTimeStamp: time.Now().UTC(),\n\t\t\t\t\t}\n\t\t\t\t\ts.Noticef(\"Completed restore of %s for stream '%s > %s' in %v\",\n\t\t\t\t\t\tfriendlyBytes(int64(total)), acc.Name, streamName, end.Sub(start).Round(time.Millisecond))\n\t\t\t\t}\n\n\t\t\t\t// On the last EOF, send back the stream info or error status.\n\t\t\t\ts.sendInternalAccountMsg(acc, result.reply, s.jsonResponse(&resp))\n\t\t\t\t// Signal to the upper layers.\n\t\t\t\tdoneCh <- err\n\t\t\t\treturn\n\t\t\tcase <-activeQ.ch:\n\t\t\t\tif n, ok := activeQ.popOne(); ok {\n\t\t\t\t\ttotal += n\n\t\t\t\t\tnotActive.Reset(activityInterval)\n\t\t\t\t}\n\t\t\tcase <-notActive.C:\n\t\t\t\terr := fmt.Errorf(\"restore for stream '%s > %s' is stalled\", acc, streamName)\n\t\t\t\tdoneCh <- err\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t})\n\n\treturn doneCh\n}\n\n// Process a snapshot request.\nfunc (s *Server) jsStreamSnapshotRequest(sub *subscription, c *client, _ *Account, subject, reply string, rmsg []byte) {\n\tif c == nil || !s.JetStreamEnabled() {\n\t\treturn\n\t}\n\tci, acc, _, msg, err := s.getRequestInfo(c, rmsg)\n\tif err != nil {\n\t\ts.Warnf(badAPIRequestT, msg)\n\t\treturn\n\t}\n\n\tsmsg := string(msg)\n\tstream := streamNameFromSubject(subject)\n\n\t// If we are in clustered mode we need to be the stream leader to proceed.\n\tif s.JetStreamIsClustered() && !acc.JetStreamIsStreamLeader(stream) {\n\t\treturn\n\t}\n\n\tvar resp = JSApiStreamSnapshotResponse{ApiResponse: ApiResponse{Type: JSApiStreamSnapshotResponseType}}\n\tif !acc.JetStreamEnabled() {\n\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, smsg, s.jsonResponse(&resp))\n\t\treturn\n\t}\n\tif isEmptyRequest(msg) {\n\t\tresp.Error = NewJSBadRequestError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, smsg, s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tmset, err := acc.lookupStream(stream)\n\tif err != nil {\n\t\tresp.Error = NewJSStreamNotFoundError(Unless(err))\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, smsg, s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tif hasJS, doErr := acc.checkJetStream(); !hasJS {\n\t\tif doErr {\n\t\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, smsg, s.jsonResponse(&resp))\n\t\t}\n\t\treturn\n\t}\n\n\tvar req JSApiStreamSnapshotRequest\n\tif err := s.unmarshalRequest(c, acc, subject, msg, &req); err != nil {\n\t\tresp.Error = NewJSInvalidJSONError(err)\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, smsg, s.jsonResponse(&resp))\n\t\treturn\n\t}\n\tif !IsValidSubject(req.DeliverSubject) {\n\t\tresp.Error = NewJSSnapshotDeliverSubjectInvalidError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, smsg, s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\t// We will do the snapshot in a go routine as well since check msgs may\n\t// stall this go routine.\n\tgo func() {\n\t\tif req.CheckMsgs {\n\t\t\ts.Noticef(\"Starting health check and snapshot for stream '%s > %s'\", mset.jsa.account.Name, mset.name())\n\t\t} else {\n\t\t\ts.Noticef(\"Starting snapshot for stream '%s > %s'\", mset.jsa.account.Name, mset.name())\n\t\t}\n\n\t\tstart := time.Now().UTC()\n\n\t\tsr, err := mset.snapshot(0, req.CheckMsgs, !req.NoConsumers)\n\t\tif err != nil {\n\t\t\ts.Warnf(\"Snapshot of stream '%s > %s' failed: %v\", mset.jsa.account.Name, mset.name(), err)\n\t\t\tresp.Error = NewJSStreamSnapshotError(err, Unless(err))\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, smsg, s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\n\t\tconfig := mset.config()\n\t\tresp.State = &sr.State\n\t\tresp.Config = &config\n\n\t\ts.sendAPIResponse(ci, acc, subject, reply, smsg, s.jsonResponse(resp))\n\n\t\ts.publishAdvisory(acc, JSAdvisoryStreamSnapshotCreatePre+\".\"+mset.name(), &JSSnapshotCreateAdvisory{\n\t\t\tTypedEvent: TypedEvent{\n\t\t\t\tType: JSSnapshotCreatedAdvisoryType,\n\t\t\t\tID:   nuid.Next(),\n\t\t\t\tTime: time.Now().UTC(),\n\t\t\t},\n\t\t\tStream: mset.name(),\n\t\t\tState:  sr.State,\n\t\t\tClient: ci.forAdvisory(),\n\t\t\tDomain: s.getOpts().JetStreamDomain,\n\t\t})\n\n\t\t// Now do the real streaming.\n\t\ts.streamSnapshot(acc, mset, sr, &req)\n\n\t\tend := time.Now().UTC()\n\n\t\ts.publishAdvisory(acc, JSAdvisoryStreamSnapshotCompletePre+\".\"+mset.name(), &JSSnapshotCompleteAdvisory{\n\t\t\tTypedEvent: TypedEvent{\n\t\t\t\tType: JSSnapshotCompleteAdvisoryType,\n\t\t\t\tID:   nuid.Next(),\n\t\t\t\tTime: end,\n\t\t\t},\n\t\t\tStream: mset.name(),\n\t\t\tStart:  start,\n\t\t\tEnd:    end,\n\t\t\tClient: ci.forAdvisory(),\n\t\t\tDomain: s.getOpts().JetStreamDomain,\n\t\t})\n\n\t\ts.Noticef(\"Completed snapshot of %s for stream '%s > %s' in %v\",\n\t\t\tfriendlyBytes(int64(sr.State.Bytes)),\n\t\t\tmset.jsa.account.Name,\n\t\t\tmset.name(),\n\t\t\tend.Sub(start))\n\t}()\n}\n\n// Default chunk size for now.\nconst defaultSnapshotChunkSize = 128 * 1024\nconst defaultSnapshotWindowSize = 8 * 1024 * 1024 // 8MB\n\n// streamSnapshot will stream out our snapshot to the reply subject.\nfunc (s *Server) streamSnapshot(acc *Account, mset *stream, sr *SnapshotResult, req *JSApiStreamSnapshotRequest) {\n\tchunkSize := req.ChunkSize\n\tif chunkSize == 0 {\n\t\tchunkSize = defaultSnapshotChunkSize\n\t}\n\t// Setup for the chunk stream.\n\treply := req.DeliverSubject\n\tr := sr.Reader\n\tdefer r.Close()\n\n\t// Check interest for the snapshot deliver subject.\n\tinch := make(chan bool, 1)\n\tacc.sl.RegisterNotification(req.DeliverSubject, inch)\n\tdefer acc.sl.ClearNotification(req.DeliverSubject, inch)\n\thasInterest := <-inch\n\tif !hasInterest {\n\t\t// Allow 2 seconds or so for interest to show up.\n\t\tselect {\n\t\tcase <-inch:\n\t\tcase <-time.After(2 * time.Second):\n\t\t}\n\t}\n\n\t// Create our ack flow handler.\n\t// This is very simple for now.\n\tackSize := defaultSnapshotWindowSize / chunkSize\n\tif ackSize < 8 {\n\t\tackSize = 8\n\t} else if ackSize > 8*1024 {\n\t\tackSize = 8 * 1024\n\t}\n\tacks := make(chan struct{}, ackSize)\n\tacks <- struct{}{}\n\n\t// Track bytes outstanding.\n\tvar out int32\n\n\t// We will place sequence number and size of chunk sent in the reply.\n\tackSubj := fmt.Sprintf(jsSnapshotAckT, mset.name(), nuid.Next())\n\tackSub, _ := mset.subscribeInternal(ackSubj+\".>\", func(_ *subscription, _ *client, _ *Account, subject, _ string, _ []byte) {\n\t\tcs, _ := strconv.Atoi(tokenAt(subject, 6))\n\t\t// This is very crude and simple, but ok for now.\n\t\t// This only matters when sending multiple chunks.\n\t\tif atomic.AddInt32(&out, int32(-cs)) < defaultSnapshotWindowSize {\n\t\t\tselect {\n\t\t\tcase acks <- struct{}{}:\n\t\t\tdefault:\n\t\t\t}\n\t\t}\n\t})\n\tdefer mset.unsubscribe(ackSub)\n\n\t// TODO(dlc) - Add in NATS-Chunked-Sequence header\n\tvar hdr []byte\n\tfor index := 1; ; index++ {\n\t\tchunk := make([]byte, chunkSize)\n\t\tn, err := r.Read(chunk)\n\t\tchunk = chunk[:n]\n\t\tif err != nil {\n\t\t\tif n > 0 {\n\t\t\t\tmset.outq.send(newJSPubMsg(reply, _EMPTY_, _EMPTY_, nil, chunk, nil, 0))\n\t\t\t}\n\t\t\tbreak\n\t\t}\n\n\t\t// Wait on acks for flow control if past our window size.\n\t\t// Wait up to 10ms for now if no acks received.\n\t\tif atomic.LoadInt32(&out) > defaultSnapshotWindowSize {\n\t\t\tselect {\n\t\t\tcase <-acks:\n\t\t\t\t// ok to proceed.\n\t\t\tcase <-inch:\n\t\t\t\t// Lost interest\n\t\t\t\thdr = []byte(\"NATS/1.0 408 No Interest\\r\\n\\r\\n\")\n\t\t\t\tgoto done\n\t\t\tcase <-time.After(2 * time.Second):\n\t\t\t\thdr = []byte(\"NATS/1.0 408 No Flow Response\\r\\n\\r\\n\")\n\t\t\t\tgoto done\n\t\t\t}\n\t\t}\n\t\tackReply := fmt.Sprintf(\"%s.%d.%d\", ackSubj, len(chunk), index)\n\t\tif hdr == nil {\n\t\t\thdr = []byte(\"NATS/1.0 204\\r\\n\\r\\n\")\n\t\t}\n\t\tmset.outq.send(newJSPubMsg(reply, _EMPTY_, ackReply, nil, chunk, nil, 0))\n\t\tatomic.AddInt32(&out, int32(len(chunk)))\n\t}\n\n\tif err := <-sr.errCh; err != _EMPTY_ {\n\t\thdr = []byte(fmt.Sprintf(\"NATS/1.0 500 %s\\r\\n\\r\\n\", err))\n\t}\n\ndone:\n\t// Send last EOF\n\t// TODO(dlc) - place hash in header\n\tmset.outq.send(newJSPubMsg(reply, _EMPTY_, _EMPTY_, hdr, nil, nil, 0))\n}\n\n// For determining consumer request type.\ntype ccReqType uint8\n\nconst (\n\tccNew = iota\n\tccLegacyEphemeral\n\tccLegacyDurable\n)\n\n// Request to create a consumer where stream and optional consumer name are part of the subject, and optional\n// filtered subjects can be at the tail end.\n// Assumes stream and consumer names are single tokens.\nfunc (s *Server) jsConsumerCreateRequest(sub *subscription, c *client, a *Account, subject, reply string, rmsg []byte) {\n\tif c == nil || !s.JetStreamEnabled() {\n\t\treturn\n\t}\n\n\tci, acc, _, msg, err := s.getRequestInfo(c, rmsg)\n\tif err != nil {\n\t\ts.Warnf(badAPIRequestT, msg)\n\t\treturn\n\t}\n\n\tvar resp = JSApiConsumerCreateResponse{ApiResponse: ApiResponse{Type: JSApiConsumerCreateResponseType}}\n\n\tvar req CreateConsumerRequest\n\tif err := s.unmarshalRequest(c, acc, subject, msg, &req); err != nil {\n\t\tresp.Error = NewJSInvalidJSONError(err)\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tvar js *jetStream\n\tisClustered := s.JetStreamIsClustered()\n\n\t// Determine if we should proceed here when we are in clustered mode.\n\tif isClustered {\n\t\tif req.Config.Direct {\n\t\t\t// Check to see if we have this stream and are the stream leader.\n\t\t\tif !acc.JetStreamIsStreamLeader(streamNameFromSubject(subject)) {\n\t\t\t\treturn\n\t\t\t}\n\t\t} else {\n\t\t\tvar cc *jetStreamCluster\n\t\t\tjs, cc = s.getJetStreamCluster()\n\t\t\tif js == nil || cc == nil {\n\t\t\t\treturn\n\t\t\t}\n\t\t\tif js.isLeaderless() {\n\t\t\t\tresp.Error = NewJSClusterNotAvailError()\n\t\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\t\treturn\n\t\t\t}\n\t\t\t// Make sure we are meta leader.\n\t\t\tif !s.JetStreamIsLeader() {\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}\n\n\tvar streamName, consumerName, filteredSubject string\n\tvar rt ccReqType\n\n\tif n := numTokens(subject); n < 5 {\n\t\ts.Warnf(badAPIRequestT, msg)\n\t\treturn\n\t} else if n == 5 {\n\t\t// Legacy ephemeral.\n\t\trt = ccLegacyEphemeral\n\t\tstreamName = streamNameFromSubject(subject)\n\t} else {\n\t\t// New style and durable legacy.\n\t\tif tokenAt(subject, 4) == \"DURABLE\" {\n\t\t\trt = ccLegacyDurable\n\t\t\tif n != 7 {\n\t\t\t\tresp.Error = NewJSConsumerDurableNameNotInSubjectError()\n\t\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\t\treturn\n\t\t\t}\n\t\t\tstreamName = tokenAt(subject, 6)\n\t\t\tconsumerName = tokenAt(subject, 7)\n\t\t} else {\n\t\t\tstreamName = streamNameFromSubject(subject)\n\t\t\tconsumerName = consumerNameFromSubject(subject)\n\t\t\t// New has optional filtered subject as part of main subject..\n\t\t\tif n > 6 {\n\t\t\t\ttokens := strings.Split(subject, tsep)\n\t\t\t\tfilteredSubject = strings.Join(tokens[6:], tsep)\n\t\t\t}\n\t\t}\n\t}\n\n\tif hasJS, doErr := acc.checkJetStream(); !hasJS {\n\t\tif doErr {\n\t\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t}\n\t\treturn\n\t}\n\n\tif streamName != req.Stream {\n\t\tresp.Error = NewJSStreamMismatchError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tif consumerName != _EMPTY_ {\n\t\t// Check for path like separators in the name.\n\t\tif strings.ContainsAny(consumerName, `\\/`) {\n\t\t\tresp.Error = NewJSConsumerNameContainsPathSeparatorsError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t}\n\n\t// Should we expect a durable name\n\tif rt == ccLegacyDurable {\n\t\tif numTokens(subject) < 7 {\n\t\t\tresp.Error = NewJSConsumerDurableNameNotInSubjectError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\t// Now check on requirements for durable request.\n\t\tif req.Config.Durable == _EMPTY_ {\n\t\t\tresp.Error = NewJSConsumerDurableNameNotSetError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\tif consumerName != req.Config.Durable {\n\t\t\tresp.Error = NewJSConsumerDurableNameNotMatchSubjectError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t}\n\t// If new style and durable set make sure they match.\n\tif rt == ccNew {\n\t\tif req.Config.Durable != _EMPTY_ {\n\t\t\tif consumerName != req.Config.Durable {\n\t\t\t\tresp.Error = NewJSConsumerDurableNameNotMatchSubjectError()\n\t\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t\t// New style ephemeral so we need to honor the name.\n\t\treq.Config.Name = consumerName\n\t}\n\t// Check for legacy ephemeral mis-configuration.\n\tif rt == ccLegacyEphemeral && req.Config.Durable != _EMPTY_ {\n\t\tresp.Error = NewJSConsumerEphemeralWithDurableNameError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\t// in case of multiple filters provided, error if new API is used.\n\tif filteredSubject != _EMPTY_ && len(req.Config.FilterSubjects) != 0 {\n\t\tresp.Error = NewJSConsumerMultipleFiltersNotAllowedError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\t// Check for a filter subject.\n\tif filteredSubject != _EMPTY_ && req.Config.FilterSubject != filteredSubject {\n\t\tresp.Error = NewJSConsumerCreateFilterSubjectMismatchError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tif isClustered && !req.Config.Direct {\n\t\t// If we are inline with client, we still may need to do a callout for consumer info\n\t\t// during this call, so place in Go routine to not block client.\n\t\t// Router and Gateway API calls already in separate context.\n\t\tif c.kind != ROUTER && c.kind != GATEWAY {\n\t\t\tgo s.jsClusteredConsumerRequest(ci, acc, subject, reply, rmsg, req.Stream, &req.Config, req.Action, req.Pedantic)\n\t\t} else {\n\t\t\ts.jsClusteredConsumerRequest(ci, acc, subject, reply, rmsg, req.Stream, &req.Config, req.Action, req.Pedantic)\n\t\t}\n\t\treturn\n\t}\n\n\t// If we are here we are single server mode.\n\tif req.Config.Replicas > 1 {\n\t\tresp.Error = NewJSStreamReplicasNotSupportedError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tstream, err := acc.lookupStream(req.Stream)\n\tif err != nil {\n\t\tresp.Error = NewJSStreamNotFoundError(Unless(err))\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tif o := stream.lookupConsumer(consumerName); o != nil {\n\t\t// If the consumer already exists then don't allow updating the PauseUntil, just set\n\t\t// it back to whatever the current configured value is.\n\t\treq.Config.PauseUntil = o.cfg.PauseUntil\n\t}\n\n\t// Initialize/update asset version metadata.\n\tsetStaticConsumerMetadata(&req.Config)\n\n\to, err := stream.addConsumerWithAction(&req.Config, req.Action, req.Pedantic)\n\n\tif err != nil {\n\t\tif IsNatsErr(err, JSConsumerStoreFailedErrF) {\n\t\t\tcname := req.Config.Durable // Will be empty if ephemeral.\n\t\t\ts.Warnf(\"Consumer create failed for '%s > %s > %s': %v\", acc, req.Stream, cname, err)\n\t\t\terr = errConsumerStoreFailed\n\t\t}\n\t\tresp.Error = NewJSConsumerCreateError(err, Unless(err))\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\tresp.ConsumerInfo = setDynamicConsumerInfoMetadata(o.initialInfo())\n\ts.sendAPIResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(resp))\n\n\tif o.cfg.PauseUntil != nil && !o.cfg.PauseUntil.IsZero() && time.Now().Before(*o.cfg.PauseUntil) {\n\t\to.sendPauseAdvisoryLocked(&o.cfg)\n\t}\n}\n\n// Request for the list of all consumer names.\nfunc (s *Server) jsConsumerNamesRequest(sub *subscription, c *client, _ *Account, subject, reply string, rmsg []byte) {\n\tif c == nil || !s.JetStreamEnabled() {\n\t\treturn\n\t}\n\tci, acc, _, msg, err := s.getRequestInfo(c, rmsg)\n\tif err != nil {\n\t\ts.Warnf(badAPIRequestT, msg)\n\t\treturn\n\t}\n\n\tvar resp = JSApiConsumerNamesResponse{\n\t\tApiResponse: ApiResponse{Type: JSApiConsumerNamesResponseType},\n\t\tConsumers:   []string{},\n\t}\n\n\t// Determine if we should proceed here when we are in clustered mode.\n\tif s.JetStreamIsClustered() {\n\t\tjs, cc := s.getJetStreamCluster()\n\t\tif js == nil || cc == nil {\n\t\t\treturn\n\t\t}\n\t\tif js.isLeaderless() {\n\t\t\tresp.Error = NewJSClusterNotAvailError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\t// Make sure we are meta leader.\n\t\tif !s.JetStreamIsLeader() {\n\t\t\treturn\n\t\t}\n\t}\n\n\tif hasJS, doErr := acc.checkJetStream(); !hasJS {\n\t\tif doErr {\n\t\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t}\n\t\treturn\n\t}\n\n\tvar offset int\n\tif isJSONObjectOrArray(msg) {\n\t\tvar req JSApiConsumersRequest\n\t\tif err := s.unmarshalRequest(c, acc, subject, msg, &req); err != nil {\n\t\t\tresp.Error = NewJSInvalidJSONError(err)\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\toffset = req.Offset\n\t}\n\n\tstreamName := streamNameFromSubject(subject)\n\tvar numConsumers int\n\n\tif s.JetStreamIsClustered() {\n\t\tjs, cc := s.getJetStreamCluster()\n\t\tif js == nil || cc == nil {\n\t\t\t// TODO(dlc) - Debug or Warn?\n\t\t\treturn\n\t\t}\n\t\tjs.mu.RLock()\n\t\tsas := cc.streams[acc.Name]\n\t\tif sas == nil {\n\t\t\tjs.mu.RUnlock()\n\t\t\tresp.Error = NewJSStreamNotFoundError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\tsa := sas[streamName]\n\t\tif sa == nil || sa.err != nil {\n\t\t\tjs.mu.RUnlock()\n\t\t\tresp.Error = NewJSStreamNotFoundError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\tfor consumer := range sa.consumers {\n\t\t\tresp.Consumers = append(resp.Consumers, consumer)\n\t\t}\n\t\tif len(resp.Consumers) > 1 {\n\t\t\tslices.Sort(resp.Consumers)\n\t\t}\n\t\tnumConsumers = len(resp.Consumers)\n\t\tif offset > numConsumers {\n\t\t\toffset = numConsumers\n\t\t}\n\t\tresp.Consumers = resp.Consumers[offset:]\n\t\tif len(resp.Consumers) > JSApiNamesLimit {\n\t\t\tresp.Consumers = resp.Consumers[:JSApiNamesLimit]\n\t\t}\n\t\tjs.mu.RUnlock()\n\n\t} else {\n\t\tmset, err := acc.lookupStream(streamName)\n\t\tif err != nil {\n\t\t\tresp.Error = NewJSStreamNotFoundError(Unless(err))\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\n\t\tobs := mset.getPublicConsumers()\n\t\tslices.SortFunc(obs, func(i, j *consumer) int { return cmp.Compare(i.name, j.name) })\n\n\t\tnumConsumers = len(obs)\n\t\tif offset > numConsumers {\n\t\t\toffset = numConsumers\n\t\t}\n\n\t\tfor _, o := range obs[offset:] {\n\t\t\tresp.Consumers = append(resp.Consumers, o.String())\n\t\t\tif len(resp.Consumers) >= JSApiNamesLimit {\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t}\n\tresp.Total = numConsumers\n\tresp.Limit = JSApiNamesLimit\n\tresp.Offset = offset\n\ts.sendAPIResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(resp))\n}\n\n// Request for the list of all detailed consumer information.\nfunc (s *Server) jsConsumerListRequest(sub *subscription, c *client, _ *Account, subject, reply string, rmsg []byte) {\n\tif c == nil || !s.JetStreamEnabled() {\n\t\treturn\n\t}\n\n\tci, acc, _, msg, err := s.getRequestInfo(c, rmsg)\n\tif err != nil {\n\t\ts.Warnf(badAPIRequestT, msg)\n\t\treturn\n\t}\n\n\tvar resp = JSApiConsumerListResponse{\n\t\tApiResponse: ApiResponse{Type: JSApiConsumerListResponseType},\n\t\tConsumers:   []*ConsumerInfo{},\n\t}\n\n\t// Determine if we should proceed here when we are in clustered mode.\n\tif s.JetStreamIsClustered() {\n\t\tjs, cc := s.getJetStreamCluster()\n\t\tif js == nil || cc == nil {\n\t\t\treturn\n\t\t}\n\t\tif js.isLeaderless() {\n\t\t\tresp.Error = NewJSClusterNotAvailError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\t// Make sure we are meta leader.\n\t\tif !s.JetStreamIsLeader() {\n\t\t\treturn\n\t\t}\n\t}\n\n\tif hasJS, doErr := acc.checkJetStream(); !hasJS {\n\t\tif doErr {\n\t\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t}\n\t\treturn\n\t}\n\n\tvar offset int\n\tif isJSONObjectOrArray(msg) {\n\t\tvar req JSApiConsumersRequest\n\t\tif err := s.unmarshalRequest(c, acc, subject, msg, &req); err != nil {\n\t\t\tresp.Error = NewJSInvalidJSONError(err)\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\toffset = req.Offset\n\t}\n\n\tstreamName := streamNameFromSubject(subject)\n\n\t// Clustered mode will invoke a scatter and gather.\n\tif s.JetStreamIsClustered() {\n\t\t// Need to copy these off before sending.. don't move this inside startGoRoutine!!!\n\t\tmsg = copyBytes(msg)\n\t\ts.startGoRoutine(func() {\n\t\t\ts.jsClusteredConsumerListRequest(acc, ci, offset, streamName, subject, reply, msg)\n\t\t})\n\t\treturn\n\t}\n\n\tmset, err := acc.lookupStream(streamName)\n\tif err != nil {\n\t\tresp.Error = NewJSStreamNotFoundError(Unless(err))\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tobs := mset.getPublicConsumers()\n\tslices.SortFunc(obs, func(i, j *consumer) int { return cmp.Compare(i.name, j.name) })\n\n\tocnt := len(obs)\n\tif offset > ocnt {\n\t\toffset = ocnt\n\t}\n\n\tfor _, o := range obs[offset:] {\n\t\tif cinfo := o.info(); cinfo != nil {\n\t\t\tresp.Consumers = append(resp.Consumers, cinfo)\n\t\t}\n\t\tif len(resp.Consumers) >= JSApiListLimit {\n\t\t\tbreak\n\t\t}\n\t}\n\tresp.Total = ocnt\n\tresp.Limit = JSApiListLimit\n\tresp.Offset = offset\n\ts.sendAPIResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(resp))\n}\n\n// Request for information about an consumer.\nfunc (s *Server) jsConsumerInfoRequest(sub *subscription, c *client, _ *Account, subject, reply string, rmsg []byte) {\n\tif c == nil || !s.JetStreamEnabled() {\n\t\treturn\n\t}\n\tci, acc, _, msg, err := s.getRequestInfo(c, rmsg)\n\tif err != nil {\n\t\ts.Warnf(badAPIRequestT, msg)\n\t\treturn\n\t}\n\n\tstreamName := streamNameFromSubject(subject)\n\tconsumerName := consumerNameFromSubject(subject)\n\n\tvar resp = JSApiConsumerInfoResponse{ApiResponse: ApiResponse{Type: JSApiConsumerInfoResponseType}}\n\n\tif !isEmptyRequest(msg) {\n\t\tresp.Error = NewJSNotEmptyRequestError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\t// If we are in clustered mode we need to be the consumer leader to proceed.\n\tif s.JetStreamIsClustered() {\n\t\t// Check to make sure the consumer is assigned.\n\t\tjs, cc := s.getJetStreamCluster()\n\t\tif js == nil || cc == nil {\n\t\t\treturn\n\t\t}\n\n\t\tjs.mu.RLock()\n\t\tmeta := cc.meta\n\t\tjs.mu.RUnlock()\n\n\t\t// Since these could wait on the Raft group lock, don't do so under the JS lock.\n\t\tourID := meta.ID()\n\t\tgroupLeaderless := meta.Leaderless()\n\t\tgroupCreated := meta.Created()\n\n\t\tjs.mu.RLock()\n\t\tisLeader, sa, ca := cc.isLeader(), js.streamAssignment(acc.Name, streamName), js.consumerAssignment(acc.Name, streamName, consumerName)\n\t\tvar rg *raftGroup\n\t\tvar offline, isMember bool\n\t\tif ca != nil {\n\t\t\tif rg = ca.Group; rg != nil {\n\t\t\t\toffline = s.allPeersOffline(rg)\n\t\t\t\tisMember = rg.isMember(ourID)\n\t\t\t}\n\t\t}\n\t\t// Capture consumer leader here.\n\t\tisConsumerLeader := cc.isConsumerLeader(acc.Name, streamName, consumerName)\n\t\t// Also capture if we think there is no meta leader.\n\t\tvar isLeaderLess bool\n\t\tif !isLeader {\n\t\t\tisLeaderLess = groupLeaderless && time.Since(groupCreated) > lostQuorumIntervalDefault\n\t\t}\n\t\tjs.mu.RUnlock()\n\n\t\tif isLeader && ca == nil {\n\t\t\t// We can't find the consumer, so mimic what would be the errors below.\n\t\t\tif hasJS, doErr := acc.checkJetStream(); !hasJS {\n\t\t\t\tif doErr {\n\t\t\t\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\t\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\t\t}\n\t\t\t\treturn\n\t\t\t}\n\t\t\tif sa == nil {\n\t\t\t\tresp.Error = NewJSStreamNotFoundError()\n\t\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\t\treturn\n\t\t\t}\n\t\t\t// If we are here the consumer is not present.\n\t\t\tresp.Error = NewJSConsumerNotFoundError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t} else if ca == nil {\n\t\t\tif isLeaderLess {\n\t\t\t\tresp.Error = NewJSClusterNotAvailError()\n\t\t\t\t// Delaying an error response gives the leader a chance to respond before us\n\t\t\t\ts.sendDelayedAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp), nil, errRespDelay)\n\t\t\t}\n\t\t\treturn\n\t\t} else if isLeader && offline {\n\t\t\tresp.Error = NewJSConsumerOfflineError()\n\t\t\ts.sendDelayedAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp), nil, errRespDelay)\n\t\t\treturn\n\t\t}\n\n\t\t// Check to see if we are a member of the group and if the group has no leader.\n\t\tif isMember && js.isGroupLeaderless(ca.Group) {\n\t\t\tresp.Error = NewJSClusterNotAvailError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\n\t\t// We have the consumer assigned and a leader, so only the consumer leader should answer.\n\t\tif !isConsumerLeader {\n\t\t\tif isLeaderLess {\n\t\t\t\tresp.Error = NewJSClusterNotAvailError()\n\t\t\t\t// Delaying an error response gives the leader a chance to respond before us\n\t\t\t\ts.sendDelayedAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp), ca.Group, errRespDelay)\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\tvar node RaftNode\n\t\t\tvar leaderNotPartOfGroup bool\n\n\t\t\t// We have a consumer assignment.\n\t\t\tif isMember {\n\t\t\t\tjs.mu.RLock()\n\t\t\t\tif rg != nil && rg.node != nil {\n\t\t\t\t\tnode = rg.node\n\t\t\t\t\tif gl := node.GroupLeader(); gl != _EMPTY_ && !rg.isMember(gl) {\n\t\t\t\t\t\tleaderNotPartOfGroup = true\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tjs.mu.RUnlock()\n\t\t\t}\n\n\t\t\t// Check if we should ignore all together.\n\t\t\tif node == nil {\n\t\t\t\t// We have been assigned but have not created a node yet. If we are a member return\n\t\t\t\t// our config and defaults for state and no cluster info.\n\t\t\t\tif isMember {\n\t\t\t\t\t// Since we access consumerAssignment, need js lock.\n\t\t\t\t\tjs.mu.RLock()\n\t\t\t\t\tresp.ConsumerInfo = &ConsumerInfo{\n\t\t\t\t\t\tStream:    ca.Stream,\n\t\t\t\t\t\tName:      ca.Name,\n\t\t\t\t\t\tCreated:   ca.Created,\n\t\t\t\t\t\tConfig:    setDynamicConsumerMetadata(ca.Config),\n\t\t\t\t\t\tTimeStamp: time.Now().UTC(),\n\t\t\t\t\t}\n\t\t\t\t\tb := s.jsonResponse(resp)\n\t\t\t\t\tjs.mu.RUnlock()\n\t\t\t\t\ts.sendAPIResponse(ci, acc, subject, reply, string(msg), b)\n\t\t\t\t}\n\t\t\t\treturn\n\t\t\t}\n\t\t\t// If we are a member and we have a group leader or we had a previous leader consider bailing out.\n\t\t\tif !node.Leaderless() || node.HadPreviousLeader() || (rg != nil && rg.Preferred != _EMPTY_ && rg.Preferred != ourID) {\n\t\t\t\tif leaderNotPartOfGroup {\n\t\t\t\t\tresp.Error = NewJSConsumerOfflineError()\n\t\t\t\t\ts.sendDelayedAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp), nil, errRespDelay)\n\t\t\t\t}\n\t\t\t\treturn\n\t\t\t}\n\t\t\t// If we are here we are a member and this is just a new consumer that does not have a (preferred) leader yet.\n\t\t\t// Will fall through and return what we have. All consumers can respond but this should be very rare\n\t\t\t// but makes more sense to clients when they try to create, get a consumer exists, and then do consumer info.\n\t\t}\n\t}\n\n\tif !acc.JetStreamEnabled() {\n\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tmset, err := acc.lookupStream(streamName)\n\tif err != nil {\n\t\tresp.Error = NewJSStreamNotFoundError(Unless(err))\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tobs := mset.lookupConsumer(consumerName)\n\tif obs == nil {\n\t\tresp.Error = NewJSConsumerNotFoundError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tif resp.ConsumerInfo = setDynamicConsumerInfoMetadata(obs.info()); resp.ConsumerInfo == nil {\n\t\t// This consumer returned nil which means it's closed. Respond with not found.\n\t\tresp.Error = NewJSConsumerNotFoundError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\ts.sendAPIResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(resp))\n}\n\n// Request to delete an Consumer.\nfunc (s *Server) jsConsumerDeleteRequest(sub *subscription, c *client, _ *Account, subject, reply string, rmsg []byte) {\n\tif c == nil || !s.JetStreamEnabled() {\n\t\treturn\n\t}\n\tci, acc, _, msg, err := s.getRequestInfo(c, rmsg)\n\tif err != nil {\n\t\ts.Warnf(badAPIRequestT, msg)\n\t\treturn\n\t}\n\n\tvar resp = JSApiConsumerDeleteResponse{ApiResponse: ApiResponse{Type: JSApiConsumerDeleteResponseType}}\n\n\t// Determine if we should proceed here when we are in clustered mode.\n\tif s.JetStreamIsClustered() {\n\t\tjs, cc := s.getJetStreamCluster()\n\t\tif js == nil || cc == nil {\n\t\t\treturn\n\t\t}\n\t\tif js.isLeaderless() {\n\t\t\tresp.Error = NewJSClusterNotAvailError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\t// Make sure we are meta leader.\n\t\tif !s.JetStreamIsLeader() {\n\t\t\treturn\n\t\t}\n\t}\n\n\tif hasJS, doErr := acc.checkJetStream(); !hasJS {\n\t\tif doErr {\n\t\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t}\n\t\treturn\n\t}\n\tif !isEmptyRequest(msg) {\n\t\tresp.Error = NewJSNotEmptyRequestError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\tstream := streamNameFromSubject(subject)\n\tconsumer := consumerNameFromSubject(subject)\n\n\tif s.JetStreamIsClustered() {\n\t\ts.jsClusteredConsumerDeleteRequest(ci, acc, stream, consumer, subject, reply, rmsg)\n\t\treturn\n\t}\n\n\tmset, err := acc.lookupStream(stream)\n\tif err != nil {\n\t\tresp.Error = NewJSStreamNotFoundError(Unless(err))\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tobs := mset.lookupConsumer(consumer)\n\tif obs == nil {\n\t\tresp.Error = NewJSConsumerNotFoundError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\tif err := obs.delete(); err != nil {\n\t\tresp.Error = NewJSStreamGeneralError(err, Unless(err))\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\tresp.Success = true\n\ts.sendAPIResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(resp))\n}\n\n// Request to pause or unpause a Consumer.\nfunc (s *Server) jsConsumerPauseRequest(sub *subscription, c *client, _ *Account, subject, reply string, rmsg []byte) {\n\tif c == nil || !s.JetStreamEnabled() {\n\t\treturn\n\t}\n\tci, acc, _, msg, err := s.getRequestInfo(c, rmsg)\n\tif err != nil {\n\t\ts.Warnf(badAPIRequestT, msg)\n\t\treturn\n\t}\n\n\tvar req JSApiConsumerPauseRequest\n\tvar resp = JSApiConsumerPauseResponse{ApiResponse: ApiResponse{Type: JSApiConsumerPauseResponseType}}\n\n\tif isJSONObjectOrArray(msg) {\n\t\tif err := s.unmarshalRequest(c, acc, subject, msg, &req); err != nil {\n\t\t\tresp.Error = NewJSInvalidJSONError(err)\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t}\n\n\t// Determine if we should proceed here when we are in clustered mode.\n\tisClustered := s.JetStreamIsClustered()\n\tjs, cc := s.getJetStreamCluster()\n\tif isClustered {\n\t\tif js == nil || cc == nil {\n\t\t\treturn\n\t\t}\n\t\tif js.isLeaderless() {\n\t\t\tresp.Error = NewJSClusterNotAvailError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\t// Make sure we are meta leader.\n\t\tif !s.JetStreamIsLeader() {\n\t\t\treturn\n\t\t}\n\t}\n\n\tif hasJS, doErr := acc.checkJetStream(); !hasJS {\n\t\tif doErr {\n\t\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t}\n\t\treturn\n\t}\n\n\tstream := streamNameFromSubject(subject)\n\tconsumer := consumerNameFromSubject(subject)\n\n\tif isClustered {\n\t\tjs.mu.RLock()\n\t\tsa := js.streamAssignment(acc.Name, stream)\n\t\tif sa == nil {\n\t\t\tjs.mu.RUnlock()\n\t\t\tresp.Error = NewJSStreamNotFoundError(Unless(err))\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\n\t\tca, ok := sa.consumers[consumer]\n\t\tif !ok || ca == nil {\n\t\t\tjs.mu.RUnlock()\n\t\t\tresp.Error = NewJSConsumerNotFoundError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\n\t\tnca := *ca\n\t\tncfg := *ca.Config\n\t\tnca.Config = &ncfg\n\t\tjs.mu.RUnlock()\n\t\tpauseUTC := req.PauseUntil.UTC()\n\t\tif !pauseUTC.IsZero() {\n\t\t\tnca.Config.PauseUntil = &pauseUTC\n\t\t} else {\n\t\t\tnca.Config.PauseUntil = nil\n\t\t}\n\n\t\t// Update asset version metadata due to updating pause/resume.\n\t\t// Only PauseUntil is updated above, so reuse config for both.\n\t\tsetStaticConsumerMetadata(nca.Config)\n\n\t\teca := encodeAddConsumerAssignment(&nca)\n\t\tcc.meta.Propose(eca)\n\n\t\tresp.PauseUntil = pauseUTC\n\t\tif resp.Paused = time.Now().Before(pauseUTC); resp.Paused {\n\t\t\tresp.PauseRemaining = time.Until(pauseUTC)\n\t\t}\n\t\ts.sendAPIResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(resp))\n\t\treturn\n\t}\n\n\tmset, err := acc.lookupStream(stream)\n\tif err != nil {\n\t\tresp.Error = NewJSStreamNotFoundError(Unless(err))\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tobs := mset.lookupConsumer(consumer)\n\tif obs == nil {\n\t\tresp.Error = NewJSConsumerNotFoundError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tncfg := obs.cfg\n\tpauseUTC := req.PauseUntil.UTC()\n\tif !pauseUTC.IsZero() {\n\t\tncfg.PauseUntil = &pauseUTC\n\t} else {\n\t\tncfg.PauseUntil = nil\n\t}\n\n\t// Update asset version metadata due to updating pause/resume.\n\tsetStaticConsumerMetadata(&ncfg)\n\n\tif err := obs.updateConfig(&ncfg); err != nil {\n\t\t// The only type of error that should be returned here is from o.store,\n\t\t// so use a store failed error type.\n\t\tresp.Error = NewJSConsumerStoreFailedError(err)\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tresp.PauseUntil = pauseUTC\n\tif resp.Paused = time.Now().Before(pauseUTC); resp.Paused {\n\t\tresp.PauseRemaining = time.Until(pauseUTC)\n\t}\n\ts.sendAPIResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(resp))\n}\n\n// sendJetStreamAPIAuditAdvisory will send the audit event for a given event.\nfunc (s *Server) sendJetStreamAPIAuditAdvisory(ci *ClientInfo, acc *Account, subject, request, response string) {\n\ts.publishAdvisory(acc, JSAuditAdvisory, JSAPIAudit{\n\t\tTypedEvent: TypedEvent{\n\t\t\tType: JSAPIAuditType,\n\t\t\tID:   nuid.Next(),\n\t\t\tTime: time.Now().UTC(),\n\t\t},\n\t\tServer:   s.Name(),\n\t\tClient:   ci.forAdvisory(),\n\t\tSubject:  subject,\n\t\tRequest:  request,\n\t\tResponse: response,\n\t\tDomain:   s.getOpts().JetStreamDomain,\n\t})\n}\n",
    "source_file": "server/jetstream_api.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2012-2024 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage server\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"fmt\"\n\t\"math\"\n\t\"net\"\n\t\"net/url\"\n\t\"reflect\"\n\t\"strconv\"\n\t\"strings\"\n\t\"time\"\n)\n\n// This map is used to store URLs string as the key with a reference count as\n// the value. This is used to handle gossiped URLs such as connect_urls, etc..\ntype refCountedUrlSet map[string]int\n\n// Ascii numbers 0-9\nconst (\n\tasciiZero = 48\n\tasciiNine = 57\n)\n\nfunc versionComponents(version string) (major, minor, patch int, err error) {\n\tm := semVerRe.FindStringSubmatch(version)\n\tif len(m) == 0 {\n\t\treturn 0, 0, 0, errors.New(\"invalid semver\")\n\t}\n\tmajor, err = strconv.Atoi(m[1])\n\tif err != nil {\n\t\treturn -1, -1, -1, err\n\t}\n\tminor, err = strconv.Atoi(m[2])\n\tif err != nil {\n\t\treturn -1, -1, -1, err\n\t}\n\tpatch, err = strconv.Atoi(m[3])\n\tif err != nil {\n\t\treturn -1, -1, -1, err\n\t}\n\treturn major, minor, patch, err\n}\n\nfunc versionAtLeastCheckError(version string, emajor, eminor, epatch int) (bool, error) {\n\tmajor, minor, patch, err := versionComponents(version)\n\tif err != nil {\n\t\treturn false, err\n\t}\n\tif major > emajor ||\n\t\t(major == emajor && minor > eminor) ||\n\t\t(major == emajor && minor == eminor && patch >= epatch) {\n\t\treturn true, nil\n\t}\n\treturn false, err\n}\n\nfunc versionAtLeast(version string, emajor, eminor, epatch int) bool {\n\tres, _ := versionAtLeastCheckError(version, emajor, eminor, epatch)\n\treturn res\n}\n\n// parseSize expects decimal positive numbers. We\n// return -1 to signal error.\nfunc parseSize(d []byte) (n int) {\n\tconst maxParseSizeLen = 9 //999M\n\n\tl := len(d)\n\tif l == 0 || l > maxParseSizeLen {\n\t\treturn -1\n\t}\n\tvar (\n\t\ti   int\n\t\tdec byte\n\t)\n\n\t// Note: Use `goto` here to avoid for loop in order\n\t// to have the function be inlined.\n\t// See: https://github.com/golang/go/issues/14768\nloop:\n\tdec = d[i]\n\tif dec < asciiZero || dec > asciiNine {\n\t\treturn -1\n\t}\n\tn = n*10 + (int(dec) - asciiZero)\n\n\ti++\n\tif i < l {\n\t\tgoto loop\n\t}\n\treturn n\n}\n\n// parseInt64 expects decimal positive numbers. We\n// return -1 to signal error\nfunc parseInt64(d []byte) (n int64) {\n\tif len(d) == 0 {\n\t\treturn -1\n\t}\n\tfor _, dec := range d {\n\t\tif dec < asciiZero || dec > asciiNine {\n\t\t\treturn -1\n\t\t}\n\t\tn = n*10 + (int64(dec) - asciiZero)\n\t}\n\treturn n\n}\n\n// Helper to move from float seconds to time.Duration\nfunc secondsToDuration(seconds float64) time.Duration {\n\tttl := seconds * float64(time.Second)\n\treturn time.Duration(ttl)\n}\n\n// Parse a host/port string with a default port to use\n// if none (or 0 or -1) is specified in `hostPort` string.\nfunc parseHostPort(hostPort string, defaultPort int) (host string, port int, err error) {\n\tif hostPort != \"\" {\n\t\thost, sPort, err := net.SplitHostPort(hostPort)\n\t\tif ae, ok := err.(*net.AddrError); ok && strings.Contains(ae.Err, \"missing port\") {\n\t\t\t// try appending the current port\n\t\t\thost, sPort, err = net.SplitHostPort(fmt.Sprintf(\"%s:%d\", hostPort, defaultPort))\n\t\t}\n\t\tif err != nil {\n\t\t\treturn \"\", -1, err\n\t\t}\n\t\tport, err = strconv.Atoi(strings.TrimSpace(sPort))\n\t\tif err != nil {\n\t\t\treturn \"\", -1, err\n\t\t}\n\t\tif port == 0 || port == -1 {\n\t\t\tport = defaultPort\n\t\t}\n\t\treturn strings.TrimSpace(host), port, nil\n\t}\n\treturn \"\", -1, errors.New(\"no hostport specified\")\n}\n\n// Returns true if URL u1 represents the same URL than u2,\n// false otherwise.\nfunc urlsAreEqual(u1, u2 *url.URL) bool {\n\treturn reflect.DeepEqual(u1, u2)\n}\n\n// comma produces a string form of the given number in base 10 with\n// commas after every three orders of magnitude.\n//\n// e.g. comma(834142) -> 834,142\n//\n// This function was copied from the github.com/dustin/go-humanize\n// package and is Copyright Dustin Sallings <dustin@spy.net>\nfunc comma(v int64) string {\n\tsign := \"\"\n\n\t// Min int64 can't be negated to a usable value, so it has to be special cased.\n\tif v == math.MinInt64 {\n\t\treturn \"-9,223,372,036,854,775,808\"\n\t}\n\n\tif v < 0 {\n\t\tsign = \"-\"\n\t\tv = 0 - v\n\t}\n\n\tparts := []string{\"\", \"\", \"\", \"\", \"\", \"\", \"\"}\n\tj := len(parts) - 1\n\n\tfor v > 999 {\n\t\tparts[j] = strconv.FormatInt(v%1000, 10)\n\t\tswitch len(parts[j]) {\n\t\tcase 2:\n\t\t\tparts[j] = \"0\" + parts[j]\n\t\tcase 1:\n\t\t\tparts[j] = \"00\" + parts[j]\n\t\t}\n\t\tv = v / 1000\n\t\tj--\n\t}\n\tparts[j] = strconv.Itoa(int(v))\n\treturn sign + strings.Join(parts[j:], \",\")\n}\n\n// Adds urlStr to the given map. If the string was already present, simply\n// bumps the reference count.\n// Returns true only if it was added for the first time.\nfunc (m refCountedUrlSet) addUrl(urlStr string) bool {\n\tm[urlStr]++\n\treturn m[urlStr] == 1\n}\n\n// Removes urlStr from the given map. If the string is not present, nothing\n// is done and false is returned.\n// If the string was present, its reference count is decreased. Returns true\n// if this was the last reference, false otherwise.\nfunc (m refCountedUrlSet) removeUrl(urlStr string) bool {\n\tremoved := false\n\tif ref, ok := m[urlStr]; ok {\n\t\tif ref == 1 {\n\t\t\tremoved = true\n\t\t\tdelete(m, urlStr)\n\t\t} else {\n\t\t\tm[urlStr]--\n\t\t}\n\t}\n\treturn removed\n}\n\n// Returns the unique URLs in this map as a slice\nfunc (m refCountedUrlSet) getAsStringSlice() []string {\n\ta := make([]string, 0, len(m))\n\tfor u := range m {\n\t\ta = append(a, u)\n\t}\n\treturn a\n}\n\n// natsListenConfig provides a common configuration to match the one used by\n// net.Listen() but with our own defaults.\n// Go 1.13 introduced default-on TCP keepalives with aggressive timings and\n// there's no sane portable way in Go with stdlib to split the initial timer\n// from the retry timer.  Linux/BSD defaults are 2hrs/75s and Go sets both\n// to 15s; the issue re making them indepedently tunable has been open since\n// 2014 and this code here is being written in 2020.\n// The NATS protocol has its own L7 PING/PONG keepalive system and the Go\n// defaults are inappropriate for IoT deployment scenarios.\n// Replace any NATS-protocol calls to net.Listen(...) with\n// natsListenConfig.Listen(ctx,...) or use natsListen(); leave calls for HTTP\n// monitoring, etc, on the default.\nvar natsListenConfig = &net.ListenConfig{\n\tKeepAlive: -1,\n}\n\n// natsListen() is the same as net.Listen() except that TCP keepalives are\n// disabled (to match Go's behavior before Go 1.13).\nfunc natsListen(network, address string) (net.Listener, error) {\n\treturn natsListenConfig.Listen(context.Background(), network, address)\n}\n\n// natsDialTimeout is the same as net.DialTimeout() except the TCP keepalives\n// are disabled (to match Go's behavior before Go 1.13).\nfunc natsDialTimeout(network, address string, timeout time.Duration) (net.Conn, error) {\n\td := net.Dialer{\n\t\tTimeout:   timeout,\n\t\tKeepAlive: -1,\n\t}\n\treturn d.Dial(network, address)\n}\n\n// redactURLList() returns a copy of a list of URL pointers where each item\n// in the list will either be the same pointer if the URL does not contain a\n// password, or to a new object if there is a password.\n// The intended use-case is for logging lists of URLs safely.\nfunc redactURLList(unredacted []*url.URL) []*url.URL {\n\tr := make([]*url.URL, len(unredacted))\n\t// In the common case of no passwords, if we don't let the new object leave\n\t// this function then GC should be easier.\n\tneedCopy := false\n\tfor i := range unredacted {\n\t\tif unredacted[i] == nil {\n\t\t\tr[i] = nil\n\t\t\tcontinue\n\t\t}\n\t\tif _, has := unredacted[i].User.Password(); !has {\n\t\t\tr[i] = unredacted[i]\n\t\t\tcontinue\n\t\t}\n\t\tneedCopy = true\n\t\tru := *unredacted[i]\n\t\tru.User = url.UserPassword(ru.User.Username(), \"xxxxx\")\n\t\tr[i] = &ru\n\t}\n\tif needCopy {\n\t\treturn r\n\t}\n\treturn unredacted\n}\n\n// redactURLString() attempts to redact a URL string.\nfunc redactURLString(raw string) string {\n\tif !strings.ContainsRune(raw, '@') {\n\t\treturn raw\n\t}\n\tu, err := url.Parse(raw)\n\tif err != nil {\n\t\treturn raw\n\t}\n\treturn u.Redacted()\n}\n\n// getURLsAsString returns a slice of u.Host from the given slice of url.URL's\nfunc getURLsAsString(urls []*url.URL) []string {\n\ta := make([]string, 0, len(urls))\n\tfor _, u := range urls {\n\t\ta = append(a, u.Host)\n\t}\n\treturn a\n}\n\n// copyBytes make a new slice of the same size than `src` and copy its content.\n// If `src` is nil or its length is 0, then this returns `nil`\nfunc copyBytes(src []byte) []byte {\n\tif len(src) == 0 {\n\t\treturn nil\n\t}\n\tdst := make([]byte, len(src))\n\tcopy(dst, src)\n\treturn dst\n}\n\n// copyStrings make a new slice of the same size than `src` and copy its content.\n// If `src` is nil, then this returns `nil`\nfunc copyStrings(src []string) []string {\n\tif src == nil {\n\t\treturn nil\n\t}\n\tdst := make([]string, len(src))\n\tcopy(dst, src)\n\treturn dst\n}\n\n// Returns a byte slice for the INFO protocol.\nfunc generateInfoJSON(info *Info) []byte {\n\tb, _ := json.Marshal(info)\n\tpcs := [][]byte{[]byte(\"INFO\"), b, []byte(CR_LF)}\n\treturn bytes.Join(pcs, []byte(\" \"))\n}\n",
    "source_file": "server/util.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2018-2025 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage server\n\nimport (\n\tcrand \"crypto/rand\"\n\t\"encoding/base64\"\n)\n\n// Raw length of the nonce challenge\nconst (\n\tnonceRawLen = 11\n\tnonceLen    = 15 // base64.RawURLEncoding.EncodedLen(nonceRawLen)\n)\n\n// NonceRequired tells us if we should send a nonce.\nfunc (s *Server) NonceRequired() bool {\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\treturn s.nonceRequired()\n}\n\n// nonceRequired tells us if we should send a nonce.\n// Lock should be held on entry.\nfunc (s *Server) nonceRequired() bool {\n\treturn s.getOpts().AlwaysEnableNonce || len(s.nkeys) > 0 || s.trustedKeys != nil\n}\n\n// Generate a nonce for INFO challenge.\n// Assumes server lock is held\nfunc (s *Server) generateNonce(n []byte) {\n\tvar raw [nonceRawLen]byte\n\tdata := raw[:]\n\tcrand.Read(data)\n\tbase64.RawURLEncoding.Encode(n, data)\n}\n",
    "source_file": "server/nkey.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2023-2025 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage server\n\nimport (\n\t\"fmt\"\n\t\"hash/fnv\"\n\t\"regexp\"\n\t\"strconv\"\n\t\"strings\"\n)\n\n// Subject mapping and transform setups.\nvar (\n\tcommaSeparatorRegEx                = regexp.MustCompile(`,\\s*`)\n\tpartitionMappingFunctionRegEx      = regexp.MustCompile(`{{\\s*[pP]artition\\s*\\((.*)\\)\\s*}}`)\n\twildcardMappingFunctionRegEx       = regexp.MustCompile(`{{\\s*[wW]ildcard\\s*\\((.*)\\)\\s*}}`)\n\tsplitFromLeftMappingFunctionRegEx  = regexp.MustCompile(`{{\\s*[sS]plit[fF]rom[lL]eft\\s*\\((.*)\\)\\s*}}`)\n\tsplitFromRightMappingFunctionRegEx = regexp.MustCompile(`{{\\s*[sS]plit[fF]rom[rR]ight\\s*\\((.*)\\)\\s*}}`)\n\tsliceFromLeftMappingFunctionRegEx  = regexp.MustCompile(`{{\\s*[sS]lice[fF]rom[lL]eft\\s*\\((.*)\\)\\s*}}`)\n\tsliceFromRightMappingFunctionRegEx = regexp.MustCompile(`{{\\s*[sS]lice[fF]rom[rR]ight\\s*\\((.*)\\)\\s*}}`)\n\tsplitMappingFunctionRegEx          = regexp.MustCompile(`{{\\s*[sS]plit\\s*\\((.*)\\)\\s*}}`)\n\tleftMappingFunctionRegEx           = regexp.MustCompile(`{{\\s*[lL]eft\\s*\\((.*)\\)\\s*}}`)\n\trightMappingFunctionRegEx          = regexp.MustCompile(`{{\\s*[rR]ight\\s*\\((.*)\\)\\s*}}`)\n)\n\n// Enum for the subject mapping subjectTransform function types\nconst (\n\tNoTransform int16 = iota\n\tBadTransform\n\tPartition\n\tWildcard\n\tSplitFromLeft\n\tSplitFromRight\n\tSliceFromLeft\n\tSliceFromRight\n\tSplit\n\tLeft\n\tRight\n)\n\n// Transforms for arbitrarily mapping subjects from one to another for maps, tees and filters.\n// These can also be used for proper mapping on wildcard exports/imports.\n// These will be grouped and caching and locking are assumed to be in the upper layers.\ntype subjectTransform struct {\n\tsrc, dest            string\n\tdtoks                []string // destination tokens\n\tstoks                []string // source tokens\n\tdtokmftypes          []int16  // destination token mapping function types\n\tdtokmftokindexesargs [][]int  // destination token mapping function array of source token index arguments\n\tdtokmfintargs        []int32  // destination token mapping function int32 arguments\n\tdtokmfstringargs     []string // destination token mapping function string arguments\n}\n\n// SubjectTransformer transforms subjects using mappings\n//\n// This API is not part of the public API and not subject to SemVer protections\ntype SubjectTransformer interface {\n\t// TODO(dlc) - We could add in client here to allow for things like foo -> foo.$ACCOUNT\n\tMatch(string) (string, error)\n\tTransformSubject(subject string) string\n\tTransformTokenizedSubject(tokens []string) string\n}\n\nfunc NewSubjectTransformWithStrict(src, dest string, strict bool) (*subjectTransform, error) {\n\t// strict = true for import subject mappings that need to be reversible\n\t// (meaning can only use the Wildcard function and must use all the pwcs that are present in the source)\n\t// No source given is equivalent to the source being \">\"\n\n\tif dest == _EMPTY_ {\n\t\treturn nil, nil\n\t}\n\n\tif src == _EMPTY_ {\n\t\tsrc = fwcs\n\t}\n\n\t// Both entries need to be valid subjects.\n\tsv, stokens, npwcs, hasFwc := subjectInfo(src)\n\tdv, dtokens, dnpwcs, dHasFwc := subjectInfo(dest)\n\n\t// Make sure both are valid, match fwc if present and there are no pwcs in the dest subject.\n\tif !sv || !dv || dnpwcs > 0 || hasFwc != dHasFwc {\n\t\treturn nil, ErrBadSubject\n\t}\n\n\tvar dtokMappingFunctionTypes []int16\n\tvar dtokMappingFunctionTokenIndexes [][]int\n\tvar dtokMappingFunctionIntArgs []int32\n\tvar dtokMappingFunctionStringArgs []string\n\n\t// If the src has partial wildcards then the dest needs to have the token place markers.\n\tif npwcs > 0 || hasFwc {\n\t\t// We need to count to make sure that the dest has token holders for the pwcs.\n\t\tsti := make(map[int]int)\n\t\tfor i, token := range stokens {\n\t\t\tif len(token) == 1 && token[0] == pwc {\n\t\t\t\tsti[len(sti)+1] = i\n\t\t\t}\n\t\t}\n\n\t\tnphs := 0\n\t\tfor _, token := range dtokens {\n\t\t\ttranformType, transformArgWildcardIndexes, transfomArgInt, transformArgString, err := indexPlaceHolders(token)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\n\t\t\tif strict {\n\t\t\t\tif tranformType != NoTransform && tranformType != Wildcard {\n\t\t\t\t\treturn nil, &mappingDestinationErr{token, ErrMappingDestinationNotSupportedForImport}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif npwcs == 0 {\n\t\t\t\tif tranformType != NoTransform {\n\t\t\t\t\treturn nil, &mappingDestinationErr{token, ErrMappingDestinationIndexOutOfRange}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif tranformType == NoTransform {\n\t\t\t\tdtokMappingFunctionTypes = append(dtokMappingFunctionTypes, NoTransform)\n\t\t\t\tdtokMappingFunctionTokenIndexes = append(dtokMappingFunctionTokenIndexes, []int{-1})\n\t\t\t\tdtokMappingFunctionIntArgs = append(dtokMappingFunctionIntArgs, -1)\n\t\t\t\tdtokMappingFunctionStringArgs = append(dtokMappingFunctionStringArgs, _EMPTY_)\n\t\t\t} else {\n\t\t\t\tnphs += len(transformArgWildcardIndexes)\n\t\t\t\t// Now build up our runtime mapping from dest to source tokens.\n\t\t\t\tvar stis []int\n\t\t\t\tfor _, wildcardIndex := range transformArgWildcardIndexes {\n\t\t\t\t\tif wildcardIndex > npwcs {\n\t\t\t\t\t\treturn nil, &mappingDestinationErr{fmt.Sprintf(\"%s: [%d]\", token, wildcardIndex), ErrMappingDestinationIndexOutOfRange}\n\t\t\t\t\t}\n\t\t\t\t\tstis = append(stis, sti[wildcardIndex])\n\t\t\t\t}\n\t\t\t\tdtokMappingFunctionTypes = append(dtokMappingFunctionTypes, tranformType)\n\t\t\t\tdtokMappingFunctionTokenIndexes = append(dtokMappingFunctionTokenIndexes, stis)\n\t\t\t\tdtokMappingFunctionIntArgs = append(dtokMappingFunctionIntArgs, transfomArgInt)\n\t\t\t\tdtokMappingFunctionStringArgs = append(dtokMappingFunctionStringArgs, transformArgString)\n\n\t\t\t}\n\t\t}\n\t\tif strict && nphs < npwcs {\n\t\t\t// not all wildcards are being used in the destination\n\t\t\treturn nil, &mappingDestinationErr{dest, ErrMappingDestinationNotUsingAllWildcards}\n\t\t}\n\t} else {\n\t\t// no wildcards used in the source: check that no transform functions are used in the destination\n\t\tfor _, token := range dtokens {\n\t\t\ttranformType, _, _, _, err := indexPlaceHolders(token)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\n\t\t\tif tranformType != NoTransform {\n\t\t\t\treturn nil, &mappingDestinationErr{token, ErrMappingDestinationIndexOutOfRange}\n\t\t\t}\n\t\t}\n\t}\n\n\treturn &subjectTransform{\n\t\tsrc:                  src,\n\t\tdest:                 dest,\n\t\tdtoks:                dtokens,\n\t\tstoks:                stokens,\n\t\tdtokmftypes:          dtokMappingFunctionTypes,\n\t\tdtokmftokindexesargs: dtokMappingFunctionTokenIndexes,\n\t\tdtokmfintargs:        dtokMappingFunctionIntArgs,\n\t\tdtokmfstringargs:     dtokMappingFunctionStringArgs,\n\t}, nil\n}\n\nfunc NewSubjectTransform(src, dest string) (*subjectTransform, error) {\n\treturn NewSubjectTransformWithStrict(src, dest, false)\n}\n\nfunc NewSubjectTransformStrict(src, dest string) (*subjectTransform, error) {\n\treturn NewSubjectTransformWithStrict(src, dest, true)\n}\n\nfunc getMappingFunctionArgs(functionRegEx *regexp.Regexp, token string) []string {\n\tcommandStrings := functionRegEx.FindStringSubmatch(token)\n\tif len(commandStrings) > 1 {\n\t\treturn commaSeparatorRegEx.Split(commandStrings[1], -1)\n\t}\n\treturn nil\n}\n\n// Helper for mapping functions that take a wildcard index and an integer as arguments\nfunc transformIndexIntArgsHelper(token string, args []string, transformType int16) (int16, []int, int32, string, error) {\n\tif len(args) < 2 {\n\t\treturn BadTransform, []int{}, -1, _EMPTY_, &mappingDestinationErr{token, ErrMappingDestinationNotEnoughArgs}\n\t}\n\tif len(args) > 2 {\n\t\treturn BadTransform, []int{}, -1, _EMPTY_, &mappingDestinationErr{token, ErrMappingDestinationTooManyArgs}\n\t}\n\ti, err := strconv.Atoi(strings.Trim(args[0], \" \"))\n\tif err != nil {\n\t\treturn BadTransform, []int{}, -1, _EMPTY_, &mappingDestinationErr{token, ErrMappingDestinationInvalidArg}\n\t}\n\tmappingFunctionIntArg, err := strconv.Atoi(strings.Trim(args[1], \" \"))\n\tif err != nil {\n\t\treturn BadTransform, []int{}, -1, _EMPTY_, &mappingDestinationErr{token, ErrMappingDestinationInvalidArg}\n\t}\n\n\treturn transformType, []int{i}, int32(mappingFunctionIntArg), _EMPTY_, nil\n}\n\n// Helper to ingest and index the subjectTransform destination token (e.g. $x or {{}}) in the token\n// returns a transformation type, and three function arguments: an array of source subject token indexes,\n// and a single number (e.g. number of partitions, or a slice size), and a string (e.g.a split delimiter)\nfunc indexPlaceHolders(token string) (int16, []int, int32, string, error) {\n\tlength := len(token)\n\tif length > 1 {\n\t\t// old $1, $2, etc... mapping format still supported to maintain backwards compatibility\n\t\tif token[0] == '$' { // simple non-partition mapping\n\t\t\ttp, err := strconv.Atoi(token[1:])\n\t\t\tif err != nil {\n\t\t\t\t// other things rely on tokens starting with $ so not an error just leave it as is\n\t\t\t\treturn NoTransform, []int{-1}, -1, _EMPTY_, nil\n\t\t\t}\n\t\t\treturn Wildcard, []int{tp}, -1, _EMPTY_, nil\n\t\t}\n\n\t\t// New 'mustache' style mapping\n\t\tif length > 4 && token[0] == '{' && token[1] == '{' && token[length-2] == '}' && token[length-1] == '}' {\n\t\t\t// wildcard(wildcard token index) (equivalent to $)\n\t\t\targs := getMappingFunctionArgs(wildcardMappingFunctionRegEx, token)\n\t\t\tif args != nil {\n\t\t\t\tif len(args) == 1 && args[0] == _EMPTY_ {\n\t\t\t\t\treturn BadTransform, []int{}, -1, _EMPTY_, &mappingDestinationErr{token, ErrMappingDestinationNotEnoughArgs}\n\t\t\t\t}\n\t\t\t\tif len(args) == 1 {\n\t\t\t\t\ttokenIndex, err := strconv.Atoi(strings.Trim(args[0], \" \"))\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn BadTransform, []int{}, -1, _EMPTY_, &mappingDestinationErr{token, ErrMappingDestinationInvalidArg}\n\t\t\t\t\t}\n\t\t\t\t\treturn Wildcard, []int{tokenIndex}, -1, _EMPTY_, nil\n\t\t\t\t} else {\n\t\t\t\t\treturn BadTransform, []int{}, -1, _EMPTY_, &mappingDestinationErr{token, ErrMappingDestinationTooManyArgs}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// partition(number of partitions, token1, token2, ...)\n\t\t\targs = getMappingFunctionArgs(partitionMappingFunctionRegEx, token)\n\t\t\tif args != nil {\n\t\t\t\tif len(args) < 2 {\n\t\t\t\t\treturn BadTransform, []int{}, -1, _EMPTY_, &mappingDestinationErr{token, ErrMappingDestinationNotEnoughArgs}\n\t\t\t\t}\n\t\t\t\tif len(args) >= 2 {\n\t\t\t\t\tmappingFunctionIntArg, err := strconv.Atoi(strings.Trim(args[0], \" \"))\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn BadTransform, []int{}, -1, _EMPTY_, &mappingDestinationErr{token, ErrMappingDestinationInvalidArg}\n\t\t\t\t\t}\n\t\t\t\t\tvar numPositions = len(args[1:])\n\t\t\t\t\ttokenIndexes := make([]int, numPositions)\n\t\t\t\t\tfor ti, t := range args[1:] {\n\t\t\t\t\t\ti, err := strconv.Atoi(strings.Trim(t, \" \"))\n\t\t\t\t\t\tif err != nil {\n\t\t\t\t\t\t\treturn BadTransform, []int{}, -1, _EMPTY_, &mappingDestinationErr{token, ErrMappingDestinationInvalidArg}\n\t\t\t\t\t\t}\n\t\t\t\t\t\ttokenIndexes[ti] = i\n\t\t\t\t\t}\n\n\t\t\t\t\treturn Partition, tokenIndexes, int32(mappingFunctionIntArg), _EMPTY_, nil\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// SplitFromLeft(token, position)\n\t\t\targs = getMappingFunctionArgs(splitFromLeftMappingFunctionRegEx, token)\n\t\t\tif args != nil {\n\t\t\t\treturn transformIndexIntArgsHelper(token, args, SplitFromLeft)\n\t\t\t}\n\n\t\t\t// SplitFromRight(token, position)\n\t\t\targs = getMappingFunctionArgs(splitFromRightMappingFunctionRegEx, token)\n\t\t\tif args != nil {\n\t\t\t\treturn transformIndexIntArgsHelper(token, args, SplitFromRight)\n\t\t\t}\n\n\t\t\t// SliceFromLeft(token, position)\n\t\t\targs = getMappingFunctionArgs(sliceFromLeftMappingFunctionRegEx, token)\n\t\t\tif args != nil {\n\t\t\t\treturn transformIndexIntArgsHelper(token, args, SliceFromLeft)\n\t\t\t}\n\n\t\t\t// SliceFromRight(token, position)\n\t\t\targs = getMappingFunctionArgs(sliceFromRightMappingFunctionRegEx, token)\n\t\t\tif args != nil {\n\t\t\t\treturn transformIndexIntArgsHelper(token, args, SliceFromRight)\n\t\t\t}\n\n\t\t\t// Right(token, length)\n\t\t\targs = getMappingFunctionArgs(rightMappingFunctionRegEx, token)\n\t\t\tif args != nil {\n\t\t\t\treturn transformIndexIntArgsHelper(token, args, Right)\n\t\t\t}\n\n\t\t\t// Left(token, length)\n\t\t\targs = getMappingFunctionArgs(leftMappingFunctionRegEx, token)\n\t\t\tif args != nil {\n\t\t\t\treturn transformIndexIntArgsHelper(token, args, Left)\n\t\t\t}\n\n\t\t\t// split(token, deliminator)\n\t\t\targs = getMappingFunctionArgs(splitMappingFunctionRegEx, token)\n\t\t\tif args != nil {\n\t\t\t\tif len(args) < 2 {\n\t\t\t\t\treturn BadTransform, []int{}, -1, _EMPTY_, &mappingDestinationErr{token, ErrMappingDestinationNotEnoughArgs}\n\t\t\t\t}\n\t\t\t\tif len(args) > 2 {\n\t\t\t\t\treturn BadTransform, []int{}, -1, _EMPTY_, &mappingDestinationErr{token, ErrMappingDestinationTooManyArgs}\n\t\t\t\t}\n\t\t\t\ti, err := strconv.Atoi(strings.Trim(args[0], \" \"))\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn BadTransform, []int{}, -1, _EMPTY_, &mappingDestinationErr{token, ErrMappingDestinationInvalidArg}\n\t\t\t\t}\n\t\t\t\tif strings.Contains(args[1], \" \") || strings.Contains(args[1], tsep) {\n\t\t\t\t\treturn BadTransform, []int{}, -1, _EMPTY_, &mappingDestinationErr{token: token, err: ErrMappingDestinationInvalidArg}\n\t\t\t\t}\n\n\t\t\t\treturn Split, []int{i}, -1, args[1], nil\n\t\t\t}\n\n\t\t\treturn BadTransform, []int{}, -1, _EMPTY_, &mappingDestinationErr{token, ErrUnknownMappingDestinationFunction}\n\t\t}\n\t}\n\treturn NoTransform, []int{-1}, -1, _EMPTY_, nil\n}\n\n// Helper function to tokenize subjects with partial wildcards into formal transform destinations.\n// e.g. \"foo.*.*\" -> \"foo.$1.$2\"\nfunc transformTokenize(subject string) string {\n\t// We need to make the appropriate markers for the wildcards etc.\n\ti := 1\n\tvar nda []string\n\tfor _, token := range strings.Split(subject, tsep) {\n\t\tif token == pwcs {\n\t\t\tnda = append(nda, fmt.Sprintf(\"$%d\", i))\n\t\t\ti++\n\t\t} else {\n\t\t\tnda = append(nda, token)\n\t\t}\n\t}\n\treturn strings.Join(nda, tsep)\n}\n\n// Helper function to go from transform destination to a subject with partial wildcards and ordered list of placeholders\n// E.g.:\n//\n//\t\t\"bar\" -> \"bar\", []\n//\t\t\"foo.$2.$1\" -> \"foo.*.*\", [\"$2\",\"$1\"]\n//\t    \"foo.{{wildcard(2)}}.{{wildcard(1)}}\" -> \"foo.*.*\", [\"{{wildcard(2)}}\",\"{{wildcard(1)}}\"]\nfunc transformUntokenize(subject string) (string, []string) {\n\tvar phs []string\n\tvar nda []string\n\n\tfor _, token := range strings.Split(subject, tsep) {\n\t\tif args := getMappingFunctionArgs(wildcardMappingFunctionRegEx, token); (len(token) > 1 && token[0] == '$' && token[1] >= '1' && token[1] <= '9') || (len(args) == 1 && args[0] != _EMPTY_) {\n\t\t\tphs = append(phs, token)\n\t\t\tnda = append(nda, pwcs)\n\t\t} else {\n\t\t\tnda = append(nda, token)\n\t\t}\n\t}\n\treturn strings.Join(nda, tsep), phs\n}\n\nfunc tokenizeSubject(subject string) []string {\n\t// Tokenize the subject.\n\ttsa := [32]string{}\n\ttts := tsa[:0]\n\tstart := 0\n\tfor i := 0; i < len(subject); i++ {\n\t\tif subject[i] == btsep {\n\t\t\ttts = append(tts, subject[start:i])\n\t\t\tstart = i + 1\n\t\t}\n\t}\n\ttts = append(tts, subject[start:])\n\treturn tts\n}\n\n// Match will take a literal published subject that is associated with a client and will match and subjectTransform\n// the subject if possible.\n//\n// This API is not part of the public API and not subject to SemVer protections\nfunc (tr *subjectTransform) Match(subject string) (string, error) {\n\t// Special case: matches any and no no-op subjectTransform. May not be legal config for some features\n\t// but specific validations made at subjectTransform create time\n\tif (tr.src == fwcs || tr.src == _EMPTY_) && (tr.dest == fwcs || tr.dest == _EMPTY_) {\n\t\treturn subject, nil\n\t}\n\n\ttts := tokenizeSubject(subject)\n\n\t// TODO(jnm): optimization -> not sure this is actually needed but was there in initial code\n\tif !isValidLiteralSubject(tts) {\n\t\treturn _EMPTY_, ErrBadSubject\n\t}\n\n\tif (tr.src == _EMPTY_ || tr.src == fwcs) || isSubsetMatch(tts, tr.src) {\n\t\treturn tr.TransformTokenizedSubject(tts), nil\n\t}\n\treturn _EMPTY_, ErrNoTransforms\n}\n\n// TransformSubject transforms a subject\n//\n// This API is not part of the public API and not subject to SemVer protection\nfunc (tr *subjectTransform) TransformSubject(subject string) string {\n\treturn tr.TransformTokenizedSubject(tokenizeSubject(subject))\n}\n\nfunc (tr *subjectTransform) getHashPartition(key []byte, numBuckets int) string {\n\th := fnv.New32a()\n\t_, _ = h.Write(key)\n\n\treturn strconv.Itoa(int(h.Sum32() % uint32(numBuckets)))\n}\n\n// Do a subjectTransform on the subject to the dest subject.\nfunc (tr *subjectTransform) TransformTokenizedSubject(tokens []string) string {\n\tif len(tr.dtokmftypes) == 0 {\n\t\treturn tr.dest\n\t}\n\n\tvar b strings.Builder\n\n\t// We need to walk destination tokens and create the mapped subject pulling tokens or mapping functions\n\tli := len(tr.dtokmftypes) - 1\n\tfor i, mfType := range tr.dtokmftypes {\n\t\tif mfType == NoTransform {\n\t\t\t// Break if fwc\n\t\t\tif len(tr.dtoks[i]) == 1 && tr.dtoks[i][0] == fwc {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tb.WriteString(tr.dtoks[i])\n\t\t} else {\n\t\t\tswitch mfType {\n\t\t\tcase Partition:\n\t\t\t\tvar (\n\t\t\t\t\t_buffer       [64]byte\n\t\t\t\t\tkeyForHashing = _buffer[:0]\n\t\t\t\t)\n\t\t\t\tfor _, sourceToken := range tr.dtokmftokindexesargs[i] {\n\t\t\t\t\tkeyForHashing = append(keyForHashing, []byte(tokens[sourceToken])...)\n\t\t\t\t}\n\t\t\t\tb.WriteString(tr.getHashPartition(keyForHashing, int(tr.dtokmfintargs[i])))\n\t\t\tcase Wildcard: // simple substitution\n\t\t\t\tswitch {\n\t\t\t\tcase len(tr.dtokmftokindexesargs) < i:\n\t\t\t\t\tbreak\n\t\t\t\tcase len(tr.dtokmftokindexesargs[i]) < 1:\n\t\t\t\t\tbreak\n\t\t\t\tcase len(tokens) <= tr.dtokmftokindexesargs[i][0]:\n\t\t\t\t\tbreak\n\t\t\t\tdefault:\n\t\t\t\t\tb.WriteString(tokens[tr.dtokmftokindexesargs[i][0]])\n\t\t\t\t}\n\t\t\tcase SplitFromLeft:\n\t\t\t\tsourceToken := tokens[tr.dtokmftokindexesargs[i][0]]\n\t\t\t\tsourceTokenLen := len(sourceToken)\n\t\t\t\tposition := int(tr.dtokmfintargs[i])\n\t\t\t\tif position > 0 && position < sourceTokenLen {\n\t\t\t\t\tb.WriteString(sourceToken[:position])\n\t\t\t\t\tb.WriteString(tsep)\n\t\t\t\t\tb.WriteString(sourceToken[position:])\n\t\t\t\t} else { // too small to split at the requested position: don't split\n\t\t\t\t\tb.WriteString(sourceToken)\n\t\t\t\t}\n\t\t\tcase SplitFromRight:\n\t\t\t\tsourceToken := tokens[tr.dtokmftokindexesargs[i][0]]\n\t\t\t\tsourceTokenLen := len(sourceToken)\n\t\t\t\tposition := int(tr.dtokmfintargs[i])\n\t\t\t\tif position > 0 && position < sourceTokenLen {\n\t\t\t\t\tb.WriteString(sourceToken[:sourceTokenLen-position])\n\t\t\t\t\tb.WriteString(tsep)\n\t\t\t\t\tb.WriteString(sourceToken[sourceTokenLen-position:])\n\t\t\t\t} else { // too small to split at the requested position: don't split\n\t\t\t\t\tb.WriteString(sourceToken)\n\t\t\t\t}\n\t\t\tcase SliceFromLeft:\n\t\t\t\tsourceToken := tokens[tr.dtokmftokindexesargs[i][0]]\n\t\t\t\tsourceTokenLen := len(sourceToken)\n\t\t\t\tsliceSize := int(tr.dtokmfintargs[i])\n\t\t\t\tif sliceSize > 0 && sliceSize < sourceTokenLen {\n\t\t\t\t\tfor i := 0; i+sliceSize <= sourceTokenLen; i += sliceSize {\n\t\t\t\t\t\tif i != 0 {\n\t\t\t\t\t\t\tb.WriteString(tsep)\n\t\t\t\t\t\t}\n\t\t\t\t\t\tb.WriteString(sourceToken[i : i+sliceSize])\n\t\t\t\t\t\tif i+sliceSize != sourceTokenLen && i+sliceSize+sliceSize > sourceTokenLen {\n\t\t\t\t\t\t\tb.WriteString(tsep)\n\t\t\t\t\t\t\tb.WriteString(sourceToken[i+sliceSize:])\n\t\t\t\t\t\t\tbreak\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t} else { // too small to slice at the requested size: don't slice\n\t\t\t\t\tb.WriteString(sourceToken)\n\t\t\t\t}\n\t\t\tcase SliceFromRight:\n\t\t\t\tsourceToken := tokens[tr.dtokmftokindexesargs[i][0]]\n\t\t\t\tsourceTokenLen := len(sourceToken)\n\t\t\t\tsliceSize := int(tr.dtokmfintargs[i])\n\t\t\t\tif sliceSize > 0 && sliceSize < sourceTokenLen {\n\t\t\t\t\tremainder := sourceTokenLen % sliceSize\n\t\t\t\t\tif remainder > 0 {\n\t\t\t\t\t\tb.WriteString(sourceToken[:remainder])\n\t\t\t\t\t\tb.WriteString(tsep)\n\t\t\t\t\t}\n\t\t\t\t\tfor i := remainder; i+sliceSize <= sourceTokenLen; i += sliceSize {\n\t\t\t\t\t\tb.WriteString(sourceToken[i : i+sliceSize])\n\t\t\t\t\t\tif i+sliceSize < sourceTokenLen {\n\t\t\t\t\t\t\tb.WriteString(tsep)\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t} else { // too small to slice at the requested size: don't slice\n\t\t\t\t\tb.WriteString(sourceToken)\n\t\t\t\t}\n\t\t\tcase Split:\n\t\t\t\tsourceToken := tokens[tr.dtokmftokindexesargs[i][0]]\n\t\t\t\tsplits := strings.Split(sourceToken, tr.dtokmfstringargs[i])\n\t\t\t\tfor j, split := range splits {\n\t\t\t\t\tif split != _EMPTY_ {\n\t\t\t\t\t\tb.WriteString(split)\n\t\t\t\t\t}\n\t\t\t\t\tif j < len(splits)-1 && splits[j+1] != _EMPTY_ && !(j == 0 && split == _EMPTY_) {\n\t\t\t\t\t\tb.WriteString(tsep)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\tcase Left:\n\t\t\t\tsourceToken := tokens[tr.dtokmftokindexesargs[i][0]]\n\t\t\t\tsourceTokenLen := len(sourceToken)\n\t\t\t\tsliceSize := int(tr.dtokmfintargs[i])\n\t\t\t\tif sliceSize > 0 && sliceSize < sourceTokenLen {\n\t\t\t\t\tb.WriteString(sourceToken[0:sliceSize])\n\t\t\t\t} else { // too small to slice at the requested size: don't slice\n\t\t\t\t\tb.WriteString(sourceToken)\n\t\t\t\t}\n\t\t\tcase Right:\n\t\t\t\tsourceToken := tokens[tr.dtokmftokindexesargs[i][0]]\n\t\t\t\tsourceTokenLen := len(sourceToken)\n\t\t\t\tsliceSize := int(tr.dtokmfintargs[i])\n\t\t\t\tif sliceSize > 0 && sliceSize < sourceTokenLen {\n\t\t\t\t\tb.WriteString(sourceToken[sourceTokenLen-sliceSize : sourceTokenLen])\n\t\t\t\t} else { // too small to slice at the requested size: don't slice\n\t\t\t\t\tb.WriteString(sourceToken)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tif i < li {\n\t\t\tb.WriteByte(btsep)\n\t\t}\n\t}\n\n\t// We may have more source tokens available. This happens with \">\".\n\tif tr.dtoks[len(tr.dtoks)-1] == fwcs {\n\t\tfor sli, i := len(tokens)-1, len(tr.stoks)-1; i < len(tokens); i++ {\n\t\t\tb.WriteString(tokens[i])\n\t\t\tif i < sli {\n\t\t\t\tb.WriteByte(btsep)\n\t\t\t}\n\t\t}\n\t}\n\treturn b.String()\n}\n\n// Reverse a subjectTransform.\nfunc (tr *subjectTransform) reverse() *subjectTransform {\n\tif len(tr.dtokmftokindexesargs) == 0 {\n\t\trtr, _ := NewSubjectTransformStrict(tr.dest, tr.src)\n\t\treturn rtr\n\t}\n\t// If we are here we need to dynamically get the correct reverse\n\t// of this subjectTransform.\n\tnsrc, phs := transformUntokenize(tr.dest)\n\tvar nda []string\n\tfor _, token := range tr.stoks {\n\t\tif token == pwcs {\n\t\t\tif len(phs) == 0 {\n\t\t\t\t// TODO(dlc) - Should not happen\n\t\t\t\treturn nil\n\t\t\t}\n\t\t\tnda = append(nda, phs[0])\n\t\t\tphs = phs[1:]\n\t\t} else {\n\t\t\tnda = append(nda, token)\n\t\t}\n\t}\n\tndest := strings.Join(nda, tsep)\n\trtr, _ := NewSubjectTransformStrict(nsrc, ndest)\n\treturn rtr\n}\n\n// Will share relevant info regarding the subject.\n// Returns valid, tokens, num pwcs, has fwc.\nfunc subjectInfo(subject string) (bool, []string, int, bool) {\n\tif subject == \"\" {\n\t\treturn false, nil, 0, false\n\t}\n\tnpwcs := 0\n\tsfwc := false\n\ttokens := strings.Split(subject, tsep)\n\tfor _, t := range tokens {\n\t\tif len(t) == 0 || sfwc {\n\t\t\treturn false, nil, 0, false\n\t\t}\n\t\tif len(t) > 1 {\n\t\t\tcontinue\n\t\t}\n\t\tswitch t[0] {\n\t\tcase fwc:\n\t\t\tsfwc = true\n\t\tcase pwc:\n\t\t\tnpwcs++\n\t\t}\n\t}\n\treturn true, tokens, npwcs, sfwc\n}\n",
    "source_file": "server/subject_transform.go",
    "chunk_type": "code"
  },
  {
    "content": "# Tests\n\nTests that run on Travis have been split into jobs that run in their own VM in parallel. This reduces the overall running time but also is allowing recycling of a job when we get a flapper as opposed to have to recycle the whole test suite.\n",
    "source_file": "server/README.md",
    "chunk_type": "doc"
  },
  {
    "content": "## JetStream Tests\n\nFor JetStream tests, we need to observe a naming convention so that no tests are omitted when running on Travis.\n\nThe script `runTestsOnTravis.sh` will run a given job based on the definition found in \"`.travis.yml`\".\n\nAs for the naming convention:\n\n- All JetStream test name should start with `TestJetStream`\n- Cluster tests should go into `jetstream_cluster_test.go` and start with `TestJetStreamCluster`\n- Super-cluster tests should go into `jetstream_super_cluster_test.go` and start with `TestJetStreamSuperCluster`\n\nNot following this convention means that some tests may not be executed on Travis.",
    "source_file": "server/README.md",
    "chunk_type": "doc"
  },
  {
    "content": "// Copyright 2020-2025 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n//go:build windows\n\npackage server\n\n// TODO(dlc) - See if there is a version of this for windows.\nfunc diskAvailable(storeDir string) int64 {\n\treturn JetStreamMaxStoreDefault\n}\n",
    "source_file": "server/disk_avail_windows.go",
    "chunk_type": "code"
  },
  {
    "content": "**MQTT Implementation Overview**\n\nRevision 1.1\n\nAuthors: Ivan Kozlovic, Lev Brouk\n\nNATS Server currently supports most of MQTT 3.1.1. This document describes how\nit is implemented.\n\nIt is strongly recommended to review the [MQTT v3.1.1\nspecifications](https://docs.oasis-open.org/mqtt/mqtt/v3.1.1/os/mqtt-v3.1.1-os.html)\nand get a detailed understanding before proceeding with this document.\n",
    "source_file": "server/README-MQTT.md",
    "chunk_type": "doc"
  },
  {
    "content": "# Contents\n\n1. [Concepts](#1-concepts)\n   - [Server, client](#server-client)\n   - [Connection, client ID, session](#connection-client-id-session)\n   - [Packets, messages, and subscriptions](#packets-messages-and-subscriptions)\n   - [Quality of Service (QoS), publish identifier (PI)](#quality-of-service-qos-publish-identifier-pi)\n   - [Retained message](#retained-message)\n   - [Will message](#will-message)\n2. [Use of JetStream](#2-use-of-jetstream)\n   - [JetStream API](#jetstream-api)\n   - [Streams](#streams)\n   - [Consumers and Internal NATS Subscriptions](#consumers-and-internal-nats-subscriptions)\n3. [Lifecycles](#3-lifecycles)\n   - [Connection, Session](#connection-session)\n   - [Subscription](#subscription)\n   - [Message](#message)\n   - [Retained messages](#retained-messages)\n4. [Implementation Notes](#4-implementation-notes)\n   - [Hooking into NATS I/O](#hooking-into-nats-io)\n   - [Session Management](#session-management)\n   - [Processing QoS acks: PUBACK, PUBREC, PUBCOMP](#processing-qos-acks-puback-pubrec-pubcomp)\n   - [Subject Wildcards](#subject-wildcards)\n5. [Known issues](#5-known-issues)\n",
    "source_file": "server/README-MQTT.md",
    "chunk_type": "doc"
  },
  {
    "content": "# 1. Concepts\n",
    "source_file": "server/README-MQTT.md",
    "chunk_type": "doc"
  },
  {
    "content": "## Server, client\n\nIn the MQTT specification there are concepts of **Client** and **Server**, used\nsomewhat interchangeably with those of **Sender** and **Receiver**. A **Server**\nacts as a **Receiver** when it gets `PUBLISH` messages from a **Sender**\n**Client**, and acts as a **Sender** when it delivers them to subscribed\n**Clients**.\n\nIn the NATS server implementation there are also concepts (types) `server` and\n`client`. `client` is an internal representation of a (connected) client and\nruns its own read and write loops. Both of these have an `mqtt` field that if\nset makes them behave as MQTT-compliant.\n\nThe code and comments may sometimes be confusing as they refer to `server` and\n`client` sometimes ambiguously between MQTT and NATS.\n",
    "source_file": "server/README-MQTT.md",
    "chunk_type": "doc"
  },
  {
    "content": "## Connection, client ID, session\n\nWhen an MQTT client connects to a server, it must send a `CONNECT` packet to\ncreate an **MQTT Connection**. The packet must include a **Client Identifier**.\nThe server will then create or load a previously saved **Session** for the (hash\nof) the client ID.\n",
    "source_file": "server/README-MQTT.md",
    "chunk_type": "doc"
  },
  {
    "content": "## Packets, messages, and subscriptions\n\nThe low level unit of transmission in MQTT is a **Packet**. Examples of packets\nare: `CONNECT`, `SUBSCRIBE`, `SUBACK`, `PUBLISH`, `PUBCOMP`, etc.\n\nAn **MQTT Message** starts with a `PUBLISH` packet that a client sends to the\nserver. It is then matched against the current **MQTT Subscriptions** and is\ndelivered to them as appropriate. During the message delivery the server acts as\nan MQTT client, and the receiver acts as an MQTT server.\n\nInternally we use **NATS Messages** and **NATS Subscriptions** to facilitate\nmessage delivery. This may be somewhat confusing as the code refers to `msg` and\n`sub`. What may be even more confusing is that some MQTT packets (specifically,\n`PUBREL`) are represented as NATS messages, and that the original MQTT packet\n\"metadata\" may be encoded as NATS message headers.\n",
    "source_file": "server/README-MQTT.md",
    "chunk_type": "doc"
  },
  {
    "content": "## Quality of Service (QoS), publish identifier (PI)\n\nMQTT specifies 3 levels of quality of service (**QoS**):\n\n- `0` for at most once. A single delivery attempt.\n- `1` for at least once. Will try to redeliver until acknowledged by the\n  receiver.\n- `2` for exactly once. See the [SPEC REF] for the acknowledgement flow.\n\nQoS 1 and 2 messages need to be identified with publish identifiers (**PI**s). A\nPI is a 16-bit integer that must uniquely identify a message for the duration of\nthe required exchange of acknowledgment packets.\n\nNote that the QoS applies separately to the transmission of a message from a\nsender client to the server, and from the server to the receiving client. There\nis no protocol-level acknowledgements between the receiver and the original\nsender. The sender passes the ownership of messages to the server, and the\nserver then delivers them at maximum possible QoS to the receivers\n(subscribers). The PIs for in-flight outgoing messages are issued and stored per\nsession.\n",
    "source_file": "server/README-MQTT.md",
    "chunk_type": "doc"
  },
  {
    "content": "## Retained message\n\nA **Retained Message** is not part of any MQTT session and is not removed when the\nsession that produced it goes away. Instead, the server needs to persist a\n_single_ retained message per topic. When a subscription is started, the server\nneeds to send the \u201cmatching\u201d retained messages, that is, messages that would\nhave been delivered to the new subscription should that subscription had been\nrunning prior to the publication of this message.\n\nRetained messages are removed when the server receives a retained message with\nan empty body. Still, this retained message that serves as a \u201cdelete\u201d of a\nretained message will be processed as a normal published message.\n\nRetained messages can have QoS.\n",
    "source_file": "server/README-MQTT.md",
    "chunk_type": "doc"
  },
  {
    "content": "## Will message\n\nThe `CONNECT` packet can contain information about a **Will Message** that needs to\nbe sent to any client subscribing on the Will topic/subject in the event that\nthe client is disconnected implicitly, that is, not as a result as the client\nsending the `DISCONNECT` packet.\n\nWill messages can have the retain flag and QoS.\n",
    "source_file": "server/README-MQTT.md",
    "chunk_type": "doc"
  },
  {
    "content": "# 2. Use of JetStream\n\nThe MQTT implementation relies heavily on JetStream. We use it to:\n\n- Persist (and restore) the [Session](#connection-client-id-session) state.\n- Store and retrieve [Retained messages](#retained-message).\n- Persist incoming [QoS 1 and\n  2](#quality-of-service-qos-publish-identifier-pi) messages, and\n  re-deliver if needed.\n- Store and de-duplicate incoming [QoS\n  2](#quality-of-service-qos-publish-identifier-pi) messages.\n- Persist and re-deliver outgoing [QoS\n  2](#quality-of-service-qos-publish-identifier-pi) `PUBREL` packets.\n\nHere is the overview of how we set up and use JetStream **streams**,\n**consumers**, and **internal NATS subscriptions**.\n",
    "source_file": "server/README-MQTT.md",
    "chunk_type": "doc"
  },
  {
    "content": "## JetStream API\n\nAll interactions with JetStream are performed via `mqttJSA` that sends NATS\nrequests to JetStream. Most are processed synchronously and await a response,\nsome (e.g. `jsa.sendAck()`) are sent asynchronously. JetStream API is usually\nreferred to as `jsa` in the code. No special locking is required to use `jsa`,\nhowever the asynchronous use of JetStream may create race conditions with\ndelivery callbacks.\n",
    "source_file": "server/README-MQTT.md",
    "chunk_type": "doc"
  },
  {
    "content": "## Streams\n\nWe create the following streams unless they already exist. Failing to ensure the\nstreams would prevent the client from connecting.\n\nEach stream is created with a replica value that is determined by the size of\nthe cluster but limited to 3. It can also be overwritten by the stream_replicas\noption in the MQTT configuration block.\n\nThe streams are created the first time an Account Session Manager is initialized\nand are used by all sessions in it. Note that to avoid race conditions, some\nsubscriptions are created first. The streams are never deleted. See\n`mqttCreateAccountSessionManager()` for details.\n\n1. `$MQTT_sess` stores persisted **Session** records. It filters on\n   `\"$MQTT.sess.>` subject and has a \u201climits\u201d policy with `MaxMsgsPer` setting\n   of 1.\n2. `$MQTT_msgs` is used for **QoS 1 and 2 message delivery**.\n   It filters on `$MQTT.msgs.>` subject and has an \u201cinterest\u201d policy.\n3. `$MQTT_rmsgs` stores **Retained Messages**. They are all\n   stored (and filtered) on a single subject `$MQTT.rmsg`. This stream has a\n   limits policy.\n4. `$MQTT_qos2in` stores and deduplicates **Incoming QoS 2 Messages**. It\n   filters on `$MQTT.qos2.in.>` and has a \"limits\" policy with `MaxMsgsPer` of\n   1.\n5. `$MQTT_out` stores **Outgoing QoS 2** `PUBREL` packets. It filters on\n   `$MQTT.out.>` and has a \"interest\" retention policy.\n",
    "source_file": "server/README-MQTT.md",
    "chunk_type": "doc"
  },
  {
    "content": "## Consumers and Internal NATS Subscriptions\n",
    "source_file": "server/README-MQTT.md",
    "chunk_type": "doc"
  },
  {
    "content": "### Account Scope\n\n- A durable consumer for [Retained Messages](#retained-message) -\n  `$MQTT_rmsgs_<server name hash>`\n- A subscription to handle all [jsa](#jetstream-api) replies for the account.\n- A subscription to replies to \"session persist\" requests, so that we can detect\n  the use of a session with the same client ID anywhere in the cluster.\n- 2 subscriptions to support [retained messages](#retained-message):\n  `$MQTT.sub.<nuid>` for the messages themselves, and one to receive replies to\n  \"delete retained message\" JS API (on the JS reply subject var).\n",
    "source_file": "server/README-MQTT.md",
    "chunk_type": "doc"
  },
  {
    "content": "### Session Scope\n\nWhen a new QoS 2 MQTT subscription is detected in a session, we ensure that\nthere is a durable consumer for [QoS\n2](#quality-of-service-qos-publish-identifier-pi) `PUBREL`s out for delivery -\n`$MQTT_PUBREL_<session id hash>`\n",
    "source_file": "server/README-MQTT.md",
    "chunk_type": "doc"
  },
  {
    "content": "### Subscription Scope\n\nFor all MQTT subscriptions, regardless of their QoS, we create internal NATS subscriptions to\n\n- `subject` (directly encoded from `topic`). This subscription is used to\n  deliver QoS 0 messages, and messages originating from NATS.\n- if needed, `subject fwc` complements `subject` for topics like `topic.#` to\n  include `topic` itself, see [top-level wildcards](#subject-wildcards)\n\nFor QoS 1 or 2 MQTT subscriptions we ensure:\n\n- A durable consumer for messages out for delivery - `<session ID hash>_<nuid>`\n- An internal subscription to `$MQTT.sub.<nuid>` to deliver the messages to the\n  receiving client.\n",
    "source_file": "server/README-MQTT.md",
    "chunk_type": "doc"
  },
  {
    "content": "### (Old) Notes\n\nAs indicated before, for a QoS1 or QoS2 subscription, the server will create a\nJetStream consumer with the appropriate subject filter. If the subscription\nalready existed, then only the NATS subscription is created for the JetStream\nconsumer\u2019s delivery subject.\n\nNote that JS consumers can be created with an \u201cReplicas\u201d override, which from\nrecent discussion is problematic with \u201cInterest\u201d policy streams, which\n\u201c$MQTT_msgs\u201d is.\n\nWe do handle situations where a subscription on the same subject filter is sent\nwith a different QoS as per MQTT specifications. If the existing was on QoS 1 or\n2, and the \u201cnew\u201d is for QoS 0, then we delete the existing JS consumer.\n\nSubscriptions that are QoS 0 have a NATS subscription with the callback function\nbeing `mqttDeliverMsgCbQos0()`; while QoS 1 and 2 have a NATS subscription with\ncallback `mqttDeliverMsgCbQos12()`. Both those functions have comments that\ndescribe the reason for their existence and what they are doing. For instance\nthe `mqttDeliverMsgCbQos0()` callback will reject any producing client that is\nof type JETSTREAM, so that it handles only non JetStream (QoS 1 and 2) messages.\n\nBoth these functions end-up calling mqttDeliver() which will first enqueue the\npossible retained messages buffer before delivering any new message. The message\nitself being delivered is serialized in MQTT format and enqueued to the client\u2019s\noutbound buffer and call to addToPCD is made so that it is flushed out of the\nreadloop.\n",
    "source_file": "server/README-MQTT.md",
    "chunk_type": "doc"
  },
  {
    "content": "# 3. Lifecycles\n",
    "source_file": "server/README-MQTT.md",
    "chunk_type": "doc"
  },
  {
    "content": "## Connection, Session\n\nAn MQTT connection is created when a listening MQTT server receives a `CONNECT`\npacket. See `mqttProcessConnect()`. A connection is associated with a session.\nSteps:\n\n1. Ensure that we have an `AccountSessionManager` so we can have an\n   `mqttSession`. Lazily initialize JetStream streams, and internal consumers\n   and subscriptions. See `getOrCreateMQTTAccountSessionManager()`.\n2. Find and disconnect any previous session/client for the same ID. See\n   `mqttProcessConnect()`.\n3. Ensure we have an `mqttSession` - create a new or load a previously persisted\n   one. If the clean flag is set in `CONNECT`, clean the session. see\n   `mqttSession.clear()`\n4. Initialize session's subscriptions, if any.\n5. Always send back a `CONNACK` packet. If there were errors in previous steps,\n   include the error.\n\nAn MQTT connection can be closed for a number of reasons, including receiving a\n`DISCONNECT` from the client, explicit internal errors processing MQTT packets,\nor the server receiving another `CONNECT` packet with the same client ID. See\n`mqttHandleClosedClient()` and `mqttHandleWill()`. Steps:\n\n1. Send out the Will Message if applicable (if not caused by a `DISCONNECT` packet)\n2. Delete the JetStream consumers for to QoS 1 and 2 packet delivery through\n   JS API calls (if \"clean\" session flag is set)\n3. Delete the session record from the \u201c$MQTT_sess\u201d stream, based on recorded\n   stream sequence. (if \"clean\" session flag is set)\n4. Close the client connection.\n\nOn an explicit disconnect, that is, the client sends the DISCONNECT packet, the\nserver will NOT send the Will, as per specifications.\n\nFor sessions that had the \u201cclean\u201d flag, the JS consumers corresponding to QoS 1\nsubscriptions are deleted through JS API calls, the session record is then\ndeleted (based on recorded stream sequence) from the \u201c$MQTT_sess\u201d stream.\n\nFinally, the client connection is closed\n\nSessions are persisted on disconnect, and on subscriptions changes.\n",
    "source_file": "server/README-MQTT.md",
    "chunk_type": "doc"
  },
  {
    "content": "## Subscription\n\nReceiving an MQTT `SUBSCRIBE` packet creates new subscriptions, or updates\nexisting subscriptions in a session. Each `SUBSCRIBE` packet may contain several\nspecific subscriptions (`topic` + QoS in each). We always respond with a\n`SUBACK`, which may indicate which subscriptions errored out.\n\nFor each subscription in the packet, we:\n\n1. Ignore it if `topic` starts with `$MQTT.sub.`.\n2. Set up QoS 0 message delivery - an internal NATS subscription on `topic`.\n3. Replay any retained messages for `topic`, once as QoS 0.\n4. If we already have a subscription on `topic`, update its QoS\n5. If this is a QoS 2 subscription in the session, ensure we have the [PUBREL\n   consumer](#session-scope) for the session.\n6. If this is a QoS 1 or 2 subscription, ensure we have the [Message\n   consumer](#subscription-scope) for this subscription (or delete one if it\n   exists and this is now a QoS 0 sub).\n7. Add an extra subscription for the [top-level wildcard](#subject-wildcards) case.\n8. Update the session, persist it if changed.\n\nWhen a session is restored (no clean flag), we go through the same steps to\nre-subscribe to its stored subscription, except step #8 which would have been\nredundant.\n\nWhen we get an `UNSUBSCRIBE` packet, it can contain multiple subscriptions to\nunsubscribe. The parsing will generate a slice of mqttFilter objects that\ncontain the \u201cfilter\u201d (the topic with possibly wildcard of the subscription) and\nthe QoS value. The server goes through the list and deletes the JS consumer (if\nQoS 1 or 2) and unsubscribes the NATS subscription for the delivery subject (if\nit was a QoS 1 or 2) or on the actual topic/subject. In case of the \u201c#\u201d\nwildcard, the server will handle the \u201clevel up\u201d subscriptions that NATS had to\ncreate.\n\nAgain, we update the session and persist it as needed in the `$MQTT_sess`\nstream.\n",
    "source_file": "server/README-MQTT.md",
    "chunk_type": "doc"
  },
  {
    "content": "## Message\n\n1. Detect an incoming PUBLISH packet, parse and check the message QoS. Fill out\n   the session's `mqttPublish` struct that contains information about the\n   published message. (see `mqttParse()`, `mqttParsePub()`)\n2. Process the message according to its QoS (see `mqttProcessPub()`)\n\n   - QoS 0:\n     - Initiate message delivery\n   - QoS 1:\n     - Initiate message delivery\n     - Send back a `PUBACK`\n   - QoS 2:\n     - Store the message in `$MQTT_qos2in` stream, using a PI-specific subject.\n       Since `MaxMsgsPer` is set to 1, we will ignore duplicates on the PI.\n     - Send back a `PUBREC`\n     - \"Wait\" for a `PUBREL`, then initiate message delivery\n     - Remove the previously stored QoS2 message\n     - Send back a `PUBCOMP`\n\n3. Initiate message delivery (see `mqttInitiateMsgDelivery()`)\n\n   - Convert the MQTT `topic` into a NATS `subject` using\n     `mqttTopicToNATSPubSubject()` function. If there is a known subject\n     mapping, then we select the new subject using `selectMappedSubject()`\n     function and then convert back this subject into an MQTT topic using\n     `natsSubjectToMQTTTopic()` function.\n   - Re-serialize the `PUBLISH` packet received as a NATS message. Use NATS\n     headers for the metadata, and the deliverable MQTT `PUBLISH` packet as the\n     contents.\n   - Publish the messages as `subject` (and `subject fwc` if applicable, see\n     [subject wildcards](#subject-wildcards)). Use the \"standard\" NATS\n     `c.processInboundClientMsg()` to do that. `processInboundClientMsg()` will\n     distribute the message to any NATS subscriptions (including routes,\n     gateways, leafnodes) and the relevant MQTT subscriptions.\n   - Check for retained messages, process as needed. See\n     `c.processInboundClientMsg()` calling `c.mqttHandlePubRetain()` For MQTT\n     clients.\n   - If the message QoS is 1 or 2, store it in `$MQTT_msgs` stream as\n     `$MQTT.msgs.<subject>` for \"at least once\" delivery with retries.\n\n4. Let NATS and JetStream deliver to the internal subscriptions, and to the\n   receiving clients. See `mqttDeliverMsgCb...()`\n\n   - The NATS message posted to `subject` (and `subject fwc`) will be delivered\n     to each relevant internal subscription by calling `mqttDeliverMsgCbQoS0()`.\n     The function has access to both the publishing and the receiving clients.\n\n     - Ignore all irrelevant invocations. Specifically, do nothing if the\n       message needs to be delivered with a higher QoS - that will be handled by\n       the other, `...QoS12` callback. Note that if the original message was\n       publuished with a QoS 1 or 2, but the subscription has its maximum QoS\n       set to 0, the message will be delivered by this callback.\n     - Ignore \"reserved\" subscriptions, as per MQTT spec.\n     - Decode delivery `topic` from the NATS `subject`.\n     - Write (enqueue) outgoing `PUBLISH` packet.\n     - **DONE for QoS 0**\n\n   - The NATS message posted to JetStream as `$MQTT.msgs.subject` will be\n     consumed by subscription-specific consumers. Note that MQTT subscriptions\n     with max QoS 0 do not have JetStream consumers. They are handled by the\n     QoS0 callback.\n\n     The consumers will deliver it to the `$MQTT.sub.<nuid>`\n     subject for their respective NATS subscriptions by calling\n     `mqttDeliverMsgCbQoS12()`. This callback too has access to both the\n     publishing and the receiving clients.\n\n     - Ignore \"reserved\" subscriptions, as per MQTT spec.\n     - See if this is a re-delivery from JetStream by checking `sess.cpending`\n       for the JS reply subject. If so, use the existing PI and treat this as a\n       duplicate redelivery.\n     - Otherwise, assign the message a new PI (see `trackPublish()` and\n       `bumpPI()`) and store it in `sess.cpending` and `sess.pendingPublish`,\n       along with the JS reply subject that can be used to remove this pending\n       message from the consumer once it's delivered to the receipient.\n     - Decode delivery `topic` from the NATS `subject`.\n     - Write (enqueue) outgoing `PUBLISH` packet.\n\n5. QoS 1: \"Wait\" for a `PUBACK`. See `mqttProcessPubAck()`.\n\n   - When received, remove the PI from the tracking maps, send an ACK to\n     consumer to remove the message.\n   - **DONE for QoS 1**\n\n6. QoS 2: \"Wait\" for a `PUBREC`. When received, we need to do all the same\n   things as in the QoS 1 `PUBACK` case, but we need to send out a `PUBREL`, and\n   continue using the same PI until the delivery flow is complete and we get\n   back a `PUBCOMP`. For that, we add the PI to `sess.pendingPubRel`, and to\n   `sess.cpending` with the PubRel consumer durable name.\n\n   We also compose and store a headers-only NATS message signifying a `PUBREL`\n   out for delivery, and store it in the `$MQTT_qos2out` stream, as\n   `$MQTT.qos2.out.<session-id>`.\n\n7. QoS 2: Deliver `PUBREL`. The PubRel session-specific consumer will publish to\n   internal subscription on `$MQTT.qos2.delivery`, calling\n   `mqttDeliverPubRelCb()`. We store the ACK reply subject in `cpending` to\n   remove the JS message on `PUBCOMP`, compose and send out a `PUBREL` packet.\n\n8. QoS 2: \"Wait\" for a `PUBCOMP`. See `mqttProcessPubComp()`.\n   - When received, remove the PI from the tracking maps, send an ACK to\n     consumer to remove the `PUBREL` message.\n   - **DONE for QoS 2**\n",
    "source_file": "server/README-MQTT.md",
    "chunk_type": "doc"
  },
  {
    "content": "## Retained messages\n\nWhen we process an inbound `PUBLISH` and submit it to\n`processInboundClientMsg()` function, for MQTT clients it will invoke\n`mqttHandlePubRetain()` which checks if the published message is \u201cretained\u201d or\nnot.\n\nIf it is, then we construct a record representing the retained message and store\nit in the `$MQTT_rmsg` stream, under the single `$MQTT.rmsg` subject. The stored\nrecord (in JSON) contains information about the subject, topic, MQTT flags, user\nthat produced this message and the message content itself. It is stored and the\nstream sequence is remembered in the memory structure that contains retained\nmessages.\n\nNote that when creating an account session manager, the retained messages stream\nis read from scratch to load all the messages through the use of a JS consumer.\nThe associated subscription will process the recovered retained messages or any\nnew that comes from the network.\n\nA retained message is added to a map and a subscription is created and inserted\ninto a sublist that will be used to perform a ReverseMatch() when a subscription\nis started and we want to find all retained messages that the subscription would\nhave received if it had been running prior to the message being published.\n\nIf a retained message on topic \u201cfoo\u201d already exists, then the server has to\ndelete the old message at the stream sequence we saved when storing it.\n\nThis could have been done with having retained messages stored under\n`$MQTT.rmsg.<subject>` as opposed to all under a single subject, and make use of\nthe `MaxMsgsPer` field set to 1. The `MaxMsgsPer` option was introduced well into\nthe availability of MQTT and changes to the sessions was made in [PR",
    "source_file": "server/README-MQTT.md",
    "chunk_type": "doc"
  },
  {
    "content": "#2501](https://github.com/nats-io/nats-server/pull/2501), with a conversion of\nexisting streams such as `$MQTT*sess*<sess ID>` into a single stream with unique\nsubjects, but the changes were not made to the retained messages stream.\n\nThere are also subscriptions for the handling of retained messages which are\nmessages that are asked by the publisher to be retained by the MQTT server to be\ndelivered to matching subscriptions when they start. There is a single message\nper topic. Retained messages are deleted when the user sends a retained message\n(there is a flag in the PUBLISH protocol) on a given topic with an empty body.\nThe difficulty with retained messages is to handle them in a cluster since all\nservers need to be aware of their presence so that they can deliver them to\nsubscriptions that those servers may become the leader for.\n\n- `$MQTT_rmsgs` which has a \u201climits\u201d policy and holds retained messages, all\n  under `$MQTT.rmsg` single subject. Not sure why I did not use MaxMsgsPer for\n  this stream and not filter `$MQTT.rmsg.>`.\n\nThe first step when processing a new subscription is to gather the retained\nmessages that would be a match for this subscription. To do so, the server will\nserialize into a buffer all messages for the account session manager\u2019s sublist\u2019s\nReverseMatch result. We use the returned subscriptions\u2019 subject to find from a\nmap appropriate retained message (see `serializeRetainedMsgsForSub()` for\ndetails).\n",
    "source_file": "server/README-MQTT.md",
    "chunk_type": "doc"
  },
  {
    "content": "# 4. Implementation Notes\n",
    "source_file": "server/README-MQTT.md",
    "chunk_type": "doc"
  },
  {
    "content": "## Hooking into NATS I/O\n",
    "source_file": "server/README-MQTT.md",
    "chunk_type": "doc"
  },
  {
    "content": "### Starting the accept loop\n\nThe MQTT accept loop is started when the server detects that an MQTT port has\nbeen defined in the configuration file. It works similarly to all other accept\nloops. Note that for MQTT over websocket, the websocket port has to be defined\nand MQTT clients will connect to that port instead of the MQTT port and need to\nprovide `/mqtt` as part of the URL to redirect the creation of the client to an\nMQTT client (with websocket support) instead of a regular NATS with websocket.\nSee the branching done in `startWebsocketServer()`. See `startMQTT()`.\n",
    "source_file": "server/README-MQTT.md",
    "chunk_type": "doc"
  },
  {
    "content": "### Starting the read/write loops\n\nWhen a TCP connection is accepted, the internal go routine will invoke\n`createMQTTClient()`. This function will set a `c.mqtt` object that will make it\nbecome an MQTT client (through the `isMqtt()` helper function). The `readLoop()`\nand `writeLoop()` are started similarly to other clients. However, the read loop\nwill branch out to `mqttParse()` instead when detecting that this is an MQTT\nclient.\n",
    "source_file": "server/README-MQTT.md",
    "chunk_type": "doc"
  },
  {
    "content": "## Session Management\n",
    "source_file": "server/README-MQTT.md",
    "chunk_type": "doc"
  },
  {
    "content": "### Account Session Manager\n\n`mqttAccountSessionManager` is an object that holds the state of all sessions in\nan account. It also manages the lifecycle of JetStream streams and internal\nsubscriptions for processing JS API replies, session updates, etc. See\n`mqttCreateAccountSessionManager()`. It is lazily initialized upon the first\nMQTT `CONNECT` packet received. Account session manager is referred to as `asm`\nin the code.\n\nNote that creating the account session manager (and attempting to create the\nstreams) is done only once per account on a given server, since once created the\naccount session manager for a given account would be found in the sessions map\nof the mqttSessionManager object.\n",
    "source_file": "server/README-MQTT.md",
    "chunk_type": "doc"
  },
  {
    "content": "### Find and disconnect previous session/client\n\nOnce all that is done, we now go to the creation of the session object itself.\nFor that, we first need to make sure that it does not already exist, meaning\nthat it is registered on the server - or anywhere in the cluster. Note that MQTT\ndictates that if a session with the same ID connects, the OLD session needs to\nbe closed, not the new one being created. NATS Server complies with this\nrequirement.\n\nOnce a session is detected to already exists, the old one (as described above)\nis closed and the new one accepted, however, the session ID is maintained in a\nflappers map so that we detect situations where sessions with the same ID are\nstarted multiple times causing the previous one to be closed. When that\ndetection occurs, the newly created session is put in \u201cjail\u201d for a second to\navoid a very rapid succession of connect/disconnect. This has already been seen\nby users since there was some issue there where we would schedule the connection\nclosed instead of waiting in place which was causing a panic.\n\nWe also protect from multiple clients on a given server trying to connect with\nthe same ID at the \u201csame time\u201d while the processing of a CONNECT of a session is\nnot yet finished. This is done with the use of a sessLocked map, keyed by the\nsession ID.\n",
    "source_file": "server/README-MQTT.md",
    "chunk_type": "doc"
  },
  {
    "content": "### Create or restore the session\n\nIf everything is good up to that point, the server will either create or restore\na session from the stream. This is done in the `createOrRestoreSession()`\nfunction. The client/session ID is hashed and added to the session\u2019s stream\nsubject along with the JS domain to prevent clients connecting from different\ndomains to \u201cpollute\u201d the session stream of a given domain.\n\nSince each session constitutes a subject and the stream has a maximum of 1\nmessage per subject, we attempt to load the last message on the formed subject.\nIf we don\u2019t find it, then the session object is created \u201cempty\u201d, while if we\nfind a record, we create the session object based on the record persisted on the\nstream.\n\nIf the session was restored from the JS stream, we keep track of the stream\nsequence where the record was located. When we save the session (even if it\nalready exists) we will use this sequence number to set the\n`JSExpectedLastSubjSeq` header so that we handle possibly different servers in a\n(super)cluster to detect the race of clients trying to use the same session ID,\nsince only one of the write should succeed. On success, the session\u2019s new\nsequence is remembered by the server that did the write.\n\nWhen created or restored, the CONNACK can now be sent back to the client, and if\nthere were any recovered subscriptions, they are now processed.\n",
    "source_file": "server/README-MQTT.md",
    "chunk_type": "doc"
  },
  {
    "content": "## Processing QoS acks: PUBACK, PUBREC, PUBCOMP\n\nWhen the server delivers a message with QoS 1 or 2 (also a `PUBREL` for QoS 2) to a subscribed client, the client will send back an acknowledgement. See `mqttProcessPubAck()`, `mqttProcessPubRec()`, and `mqttProcessPubComp()`\n\nWhile the specific logic for each packet differs, these handlers all update the\nsession's PI mappings (`cpending`, `pendingPublish`, `pendingPubRel`), and if\nneeded send an ACK to JetStream to remove the message from its consumer and stop\nthe re-delivery attempts.\n",
    "source_file": "server/README-MQTT.md",
    "chunk_type": "doc"
  },
  {
    "content": "## Subject Wildcards\n\nNote that MQTT subscriptions have wildcards too, the `\u201c+\u201d` wildcard is equivalent\nto NATS\u2019s `\u201c*\u201d` wildcard, however, MQTT\u2019s wildcard `\u201c#\u201d` is similar to `\u201c>\u201d`, except\nthat it also includes the level above. That is, a subscription on `\u201cfoo/#\u201d` would\nreceive messages on `\u201cfoo/bar/baz\u201d`, but also on `\u201cfoo\u201d`.\n\nSo, for MQTT subscriptions enging with a `'#'` we are forced to create 2\ninternal NATS subscriptions, one on `\u201cfoo\u201d` and one on `\u201cfoo.>\u201d`.\n",
    "source_file": "server/README-MQTT.md",
    "chunk_type": "doc"
  },
  {
    "content": "# 5. Known issues\n- \"active\" redelivery for QoS from JetStream (compliant, just a note)\n- JetStream QoS redelivery happens out of (original) order\n- finish delivery of in-flight messages after UNSUB\n- finish delivery of in-flight messages after a reconnect\n- consider replacing `$MQTT_msgs` with `$MQTT_out`.\n- consider using unique `$MQTT.rmsg.>` and `MaxMsgsPer` for retained messages.\n- add a cli command to list/clean old sessions",
    "source_file": "server/README-MQTT.md",
    "chunk_type": "doc"
  },
  {
    "content": "// Copyright 2013-2025 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage server\n\nimport (\n\t\"time\"\n)\n\n// ConnInfos represents a connection info list. We use pointers since it will be sorted.\ntype ConnInfos []*ConnInfo\n\n// For sorting\n// Len returns length for sorting.\nfunc (cl ConnInfos) Len() int { return len(cl) }\n\n// Swap will sawap the elements.\nfunc (cl ConnInfos) Swap(i, j int) { cl[i], cl[j] = cl[j], cl[i] }\n\n// SortOpt is a helper type to sort clients\ntype SortOpt string\n\n// Possible sort options\nconst (\n\tByCid      SortOpt = \"cid\"        // By connection ID\n\tByStart    SortOpt = \"start\"      // By connection start time, same as CID\n\tBySubs     SortOpt = \"subs\"       // By number of subscriptions\n\tByPending  SortOpt = \"pending\"    // By amount of data in bytes waiting to be sent to client\n\tByOutMsgs  SortOpt = \"msgs_to\"    // By number of messages sent\n\tByInMsgs   SortOpt = \"msgs_from\"  // By number of messages received\n\tByOutBytes SortOpt = \"bytes_to\"   // By amount of bytes sent\n\tByInBytes  SortOpt = \"bytes_from\" // By amount of bytes received\n\tByLast     SortOpt = \"last\"       // By the last activity\n\tByIdle     SortOpt = \"idle\"       // By the amount of inactivity\n\tByUptime   SortOpt = \"uptime\"     // By the amount of time connections exist\n\tByStop     SortOpt = \"stop\"       // By the stop time for a closed connection\n\tByReason   SortOpt = \"reason\"     // By the reason for a closed connection\n\tByRTT      SortOpt = \"rtt\"        // By the round trip time\n)\n\n// Individual sort options provide the Less for sort.Interface. Len and Swap are on cList.\n// CID\ntype byCid struct{ ConnInfos }\n\nfunc (l byCid) Less(i, j int) bool { return l.ConnInfos[i].Cid < l.ConnInfos[j].Cid }\n\n// Number of Subscriptions\ntype bySubs struct{ ConnInfos }\n\nfunc (l bySubs) Less(i, j int) bool { return l.ConnInfos[i].NumSubs < l.ConnInfos[j].NumSubs }\n\n// Pending Bytes\ntype byPending struct{ ConnInfos }\n\nfunc (l byPending) Less(i, j int) bool { return l.ConnInfos[i].Pending < l.ConnInfos[j].Pending }\n\n// Outbound Msgs\ntype byOutMsgs struct{ ConnInfos }\n\nfunc (l byOutMsgs) Less(i, j int) bool { return l.ConnInfos[i].OutMsgs < l.ConnInfos[j].OutMsgs }\n\n// Inbound Msgs\ntype byInMsgs struct{ ConnInfos }\n\nfunc (l byInMsgs) Less(i, j int) bool { return l.ConnInfos[i].InMsgs < l.ConnInfos[j].InMsgs }\n\n// Outbound Bytes\ntype byOutBytes struct{ ConnInfos }\n\nfunc (l byOutBytes) Less(i, j int) bool { return l.ConnInfos[i].OutBytes < l.ConnInfos[j].OutBytes }\n\n// Inbound Bytes\ntype byInBytes struct{ ConnInfos }\n\nfunc (l byInBytes) Less(i, j int) bool { return l.ConnInfos[i].InBytes < l.ConnInfos[j].InBytes }\n\n// Last Activity\ntype byLast struct{ ConnInfos }\n\nfunc (l byLast) Less(i, j int) bool {\n\treturn l.ConnInfos[i].LastActivity.UnixNano() < l.ConnInfos[j].LastActivity.UnixNano()\n}\n\n// Idle time\ntype byIdle struct {\n\tConnInfos\n\tnow time.Time\n}\n\nfunc (l byIdle) Less(i, j int) bool {\n\treturn l.now.Sub(l.ConnInfos[i].LastActivity) < l.now.Sub(l.ConnInfos[j].LastActivity)\n}\n\n// Uptime\ntype byUptime struct {\n\tConnInfos\n\tnow time.Time\n}\n\nfunc (l byUptime) Less(i, j int) bool {\n\tci := l.ConnInfos[i]\n\tcj := l.ConnInfos[j]\n\tvar upi, upj time.Duration\n\tif ci.Stop == nil || ci.Stop.IsZero() {\n\t\tupi = l.now.Sub(ci.Start)\n\t} else {\n\t\tupi = ci.Stop.Sub(ci.Start)\n\t}\n\tif cj.Stop == nil || cj.Stop.IsZero() {\n\t\tupj = l.now.Sub(cj.Start)\n\t} else {\n\t\tupj = cj.Stop.Sub(cj.Start)\n\t}\n\treturn upi < upj\n}\n\n// Stop\ntype byStop struct{ ConnInfos }\n\nfunc (l byStop) Less(i, j int) bool {\n\tciStop := l.ConnInfos[i].Stop\n\tcjStop := l.ConnInfos[j].Stop\n\treturn ciStop.Before(*cjStop)\n}\n\n// Reason\ntype byReason struct{ ConnInfos }\n\nfunc (l byReason) Less(i, j int) bool {\n\treturn l.ConnInfos[i].Reason < l.ConnInfos[j].Reason\n}\n\n// RTT - Default is descending\ntype byRTT struct{ ConnInfos }\n\nfunc (l byRTT) Less(i, j int) bool { return l.ConnInfos[i].rtt < l.ConnInfos[j].rtt }\n\n// IsValid determines if a sort option is valid\nfunc (s SortOpt) IsValid() bool {\n\tswitch s {\n\tcase _EMPTY_, ByCid, ByStart, BySubs, ByPending, ByOutMsgs, ByInMsgs, ByOutBytes, ByInBytes, ByLast, ByIdle, ByUptime, ByStop, ByReason, ByRTT:\n\t\treturn true\n\tdefault:\n\t\treturn false\n\t}\n}\n",
    "source_file": "server/monitor_sort_opts.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2012-2025 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage server\n\nimport (\n\t\"context\"\n\t\"crypto/tls\"\n\t\"crypto/x509\"\n\t\"errors\"\n\t\"flag\"\n\t\"fmt\"\n\t\"math\"\n\t\"net\"\n\t\"net/url\"\n\t\"os\"\n\t\"path\"\n\t\"path/filepath\"\n\t\"regexp\"\n\t\"runtime\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t\"github.com/nats-io/jwt/v2\"\n\t\"github.com/nats-io/nats-server/v2/conf\"\n\t\"github.com/nats-io/nats-server/v2/server/certidp\"\n\t\"github.com/nats-io/nats-server/v2/server/certstore\"\n\t\"github.com/nats-io/nkeys\"\n)\n\nvar allowUnknownTopLevelField = int32(0)\n\n// NoErrOnUnknownFields can be used to change the behavior the processing\n// of a configuration file. By default, an error is reported if unknown\n// fields are found. If `noError` is set to true, no error will be reported\n// if top-level unknown fields are found.\nfunc NoErrOnUnknownFields(noError bool) {\n\tvar val int32\n\tif noError {\n\t\tval = int32(1)\n\t}\n\tatomic.StoreInt32(&allowUnknownTopLevelField, val)\n}\n\n// PinnedCertSet is a set of lower case hex-encoded sha256 of DER encoded SubjectPublicKeyInfo\ntype PinnedCertSet map[string]struct{}\n\n// ClusterOpts are options for clusters.\n// NOTE: This structure is no longer used for monitoring endpoints\n// and json tags are deprecated and may be removed in the future.\ntype ClusterOpts struct {\n\tName              string            `json:\"-\"`\n\tHost              string            `json:\"addr,omitempty\"`\n\tPort              int               `json:\"cluster_port,omitempty\"`\n\tUsername          string            `json:\"-\"`\n\tPassword          string            `json:\"-\"`\n\tAuthTimeout       float64           `json:\"auth_timeout,omitempty\"`\n\tPermissions       *RoutePermissions `json:\"-\"`\n\tTLSTimeout        float64           `json:\"-\"`\n\tTLSConfig         *tls.Config       `json:\"-\"`\n\tTLSMap            bool              `json:\"-\"`\n\tTLSCheckKnownURLs bool              `json:\"-\"`\n\tTLSPinnedCerts    PinnedCertSet     `json:\"-\"`\n\tListenStr         string            `json:\"-\"`\n\tAdvertise         string            `json:\"-\"`\n\tNoAdvertise       bool              `json:\"-\"`\n\tConnectRetries    int               `json:\"-\"`\n\tPoolSize          int               `json:\"-\"`\n\tPinnedAccounts    []string          `json:\"-\"`\n\tCompression       CompressionOpts   `json:\"-\"`\n\tPingInterval      time.Duration     `json:\"-\"`\n\tMaxPingsOut       int               `json:\"-\"`\n\n\t// Not exported (used in tests)\n\tresolver netResolver\n\t// Snapshot of configured TLS options.\n\ttlsConfigOpts *TLSConfigOpts\n}\n\n// CompressionOpts defines the compression mode and optional configuration.\ntype CompressionOpts struct {\n\tMode string\n\t// If `Mode` is set to CompressionS2Auto, RTTThresholds provides the\n\t// thresholds at which the compression level will go from\n\t// CompressionS2Uncompressed to CompressionS2Fast, CompressionS2Better\n\t// or CompressionS2Best. If a given level is not desired, specify 0\n\t// for this slot. For instance, the slice []{0, 10ms, 20ms} means that\n\t// for any RTT up to 10ms included the compression level will be\n\t// CompressionS2Fast, then from ]10ms..20ms], the level will be selected\n\t// as CompressionS2Better. Anything above 20ms will result in picking\n\t// the CompressionS2Best compression level.\n\tRTTThresholds []time.Duration\n}\n\n// GatewayOpts are options for gateways.\n// NOTE: This structure is no longer used for monitoring endpoints\n// and json tags are deprecated and may be removed in the future.\ntype GatewayOpts struct {\n\tName              string               `json:\"name\"`\n\tHost              string               `json:\"addr,omitempty\"`\n\tPort              int                  `json:\"port,omitempty\"`\n\tUsername          string               `json:\"-\"`\n\tPassword          string               `json:\"-\"`\n\tAuthTimeout       float64              `json:\"auth_timeout,omitempty\"`\n\tTLSConfig         *tls.Config          `json:\"-\"`\n\tTLSTimeout        float64              `json:\"tls_timeout,omitempty\"`\n\tTLSMap            bool                 `json:\"-\"`\n\tTLSCheckKnownURLs bool                 `json:\"-\"`\n\tTLSPinnedCerts    PinnedCertSet        `json:\"-\"`\n\tAdvertise         string               `json:\"advertise,omitempty\"`\n\tConnectRetries    int                  `json:\"connect_retries,omitempty\"`\n\tGateways          []*RemoteGatewayOpts `json:\"gateways,omitempty\"`\n\tRejectUnknown     bool                 `json:\"reject_unknown,omitempty\"` // config got renamed to reject_unknown_cluster\n\n\t// Not exported, for tests.\n\tresolver         netResolver\n\tsendQSubsBufSize int\n\n\t// Snapshot of configured TLS options.\n\ttlsConfigOpts *TLSConfigOpts\n}\n\n// RemoteGatewayOpts are options for connecting to a remote gateway\n// NOTE: This structure is no longer used for monitoring endpoints\n// and json tags are deprecated and may be removed in the future.\ntype RemoteGatewayOpts struct {\n\tName          string      `json:\"name\"`\n\tTLSConfig     *tls.Config `json:\"-\"`\n\tTLSTimeout    float64     `json:\"tls_timeout,omitempty\"`\n\tURLs          []*url.URL  `json:\"urls,omitempty\"`\n\ttlsConfigOpts *TLSConfigOpts\n}\n\n// LeafNodeOpts are options for a given server to accept leaf node connections and/or connect to a remote cluster.\ntype LeafNodeOpts struct {\n\tHost           string        `json:\"addr,omitempty\"`\n\tPort           int           `json:\"port,omitempty\"`\n\tUsername       string        `json:\"-\"`\n\tPassword       string        `json:\"-\"`\n\tNkey           string        `json:\"-\"`\n\tAccount        string        `json:\"-\"`\n\tUsers          []*User       `json:\"-\"`\n\tAuthTimeout    float64       `json:\"auth_timeout,omitempty\"`\n\tTLSConfig      *tls.Config   `json:\"-\"`\n\tTLSTimeout     float64       `json:\"tls_timeout,omitempty\"`\n\tTLSMap         bool          `json:\"-\"`\n\tTLSPinnedCerts PinnedCertSet `json:\"-\"`\n\t// When set to true, the server will perform the TLS handshake before\n\t// sending the INFO protocol. For remote leafnodes that are not configured\n\t// with a similar option, their connection will fail with some sort\n\t// of timeout or EOF error since they are expecting to receive an\n\t// INFO protocol first.\n\tTLSHandshakeFirst bool `json:\"-\"`\n\t// If TLSHandshakeFirst is true and this value is strictly positive,\n\t// the server will wait for that amount of time for the TLS handshake\n\t// to start before falling back to previous behavior of sending the\n\t// INFO protocol first. It allows for a mix of newer remote leafnodes\n\t// that can require a TLS handshake first, and older that can't.\n\tTLSHandshakeFirstFallback time.Duration `json:\"-\"`\n\tAdvertise                 string        `json:\"-\"`\n\tNoAdvertise               bool          `json:\"-\"`\n\tReconnectInterval         time.Duration `json:\"-\"`\n\n\t// Compression options\n\tCompression CompressionOpts `json:\"-\"`\n\n\t// For solicited connections to other clusters/superclusters.\n\tRemotes []*RemoteLeafOpts `json:\"remotes,omitempty\"`\n\n\t// This is the minimum version that is accepted for remote connections.\n\t// Note that since the server version in the CONNECT protocol was added\n\t// only starting at v2.8.0, any version below that will be rejected\n\t// (since empty version string in CONNECT would fail the \"version at\n\t// least\" test).\n\tMinVersion string\n\n\t// Not exported, for tests.\n\tresolver    netResolver\n\tdialTimeout time.Duration\n\tconnDelay   time.Duration\n\n\t// Snapshot of configured TLS options.\n\ttlsConfigOpts *TLSConfigOpts\n}\n\n// SignatureHandler is used to sign a nonce from the server while\n// authenticating with Nkeys. The callback should sign the nonce and\n// return the JWT and the raw signature.\ntype SignatureHandler func([]byte) (string, []byte, error)\n\n// RemoteLeafOpts are options for connecting to a remote server as a leaf node.\ntype RemoteLeafOpts struct {\n\tLocalAccount      string           `json:\"local_account,omitempty\"`\n\tNoRandomize       bool             `json:\"-\"`\n\tURLs              []*url.URL       `json:\"urls,omitempty\"`\n\tCredentials       string           `json:\"-\"`\n\tNkey              string           `json:\"-\"`\n\tSignatureCB       SignatureHandler `json:\"-\"`\n\tTLS               bool             `json:\"-\"`\n\tTLSConfig         *tls.Config      `json:\"-\"`\n\tTLSTimeout        float64          `json:\"tls_timeout,omitempty\"`\n\tTLSHandshakeFirst bool             `json:\"-\"`\n\tHub               bool             `json:\"hub,omitempty\"`\n\tDenyImports       []string         `json:\"-\"`\n\tDenyExports       []string         `json:\"-\"`\n\n\t// FirstInfoTimeout is the amount of time the server will wait for the\n\t// initial INFO protocol from the remote server before closing the\n\t// connection.\n\tFirstInfoTimeout time.Duration `json:\"-\"`\n\n\t// Compression options for this remote. Each remote could have a different\n\t// setting and also be different from the LeafNode options.\n\tCompression CompressionOpts `json:\"-\"`\n\n\t// When an URL has the \"ws\" (or \"wss\") scheme, then the server will initiate the\n\t// connection as a websocket connection. By default, the websocket frames will be\n\t// masked (as if this server was a websocket client to the remote server). The\n\t// NoMasking option will change this behavior and will send umasked frames.\n\tWebsocket struct {\n\t\tCompression bool `json:\"-\"`\n\t\tNoMasking   bool `json:\"-\"`\n\t}\n\n\ttlsConfigOpts *TLSConfigOpts\n\n\t// If we are clustered and our local account has JetStream, if apps are accessing\n\t// a stream or consumer leader through this LN and it gets dropped, the apps will\n\t// not be able to work. This tells the system to migrate the leaders away from this server.\n\t// This only changes leader for R>1 assets.\n\tJetStreamClusterMigrate bool `json:\"jetstream_cluster_migrate,omitempty\"`\n\n\t// If JetStreamClusterMigrate is set to true, this is the time after which the leader\n\t// will be migrated away from this server if still disconnected.\n\tJetStreamClusterMigrateDelay time.Duration `json:\"jetstream_cluster_migrate_delay,omitempty\"`\n}\n\ntype JSLimitOpts struct {\n\tMaxRequestBatch int           `json:\"max_request_batch,omitempty\"`\n\tMaxAckPending   int           `json:\"max_ack_pending,omitempty\"`\n\tMaxHAAssets     int           `json:\"max_ha_assets,omitempty\"`\n\tDuplicates      time.Duration `json:\"max_duplicate_window,omitempty\"`\n}\n\ntype JSTpmOpts struct {\n\tKeysFile    string\n\tKeyPassword string\n\tSrkPassword string\n\tPcr         int\n}\n\n// AuthCallout option used to map external AuthN to NATS based AuthZ.\ntype AuthCallout struct {\n\t// Must be a public account Nkey.\n\tIssuer string\n\t// Account to be used for sending requests.\n\tAccount string\n\t// Users that will bypass auth_callout and be used for the auth service itself.\n\tAuthUsers []string\n\t// XKey is a public xkey for the authorization service.\n\t// This will enable encryption for server requests and the authorization service responses.\n\tXKey string\n\t// AllowedAccounts that will be delegated to the auth service.\n\t// If empty then all accounts will be delegated.\n\tAllowedAccounts []string\n}\n\n// Options block for nats-server.\n// NOTE: This structure is no longer used for monitoring endpoints\n// and json tags are deprecated and may be removed in the future.\ntype Options struct {\n\tConfigFile      string `json:\"-\"`\n\tServerName      string `json:\"server_name\"`\n\tHost            string `json:\"addr\"`\n\tPort            int    `json:\"port\"`\n\tDontListen      bool   `json:\"dont_listen\"`\n\tClientAdvertise string `json:\"-\"`\n\tTrace           bool   `json:\"-\"`\n\tDebug           bool   `json:\"-\"`\n\tTraceVerbose    bool   `json:\"-\"`\n\t// TraceHeaders if true will only trace message headers, not the payload\n\tTraceHeaders               bool          `json:\"-\"`\n\tNoLog                      bool          `json:\"-\"`\n\tNoSigs                     bool          `json:\"-\"`\n\tNoSublistCache             bool          `json:\"-\"`\n\tNoHeaderSupport            bool          `json:\"-\"`\n\tDisableShortFirstPing      bool          `json:\"-\"`\n\tLogtime                    bool          `json:\"-\"`\n\tLogtimeUTC                 bool          `json:\"-\"`\n\tMaxConn                    int           `json:\"max_connections\"`\n\tMaxSubs                    int           `json:\"max_subscriptions,omitempty\"`\n\tMaxSubTokens               uint8         `json:\"-\"`\n\tNkeys                      []*NkeyUser   `json:\"-\"`\n\tUsers                      []*User       `json:\"-\"`\n\tAccounts                   []*Account    `json:\"-\"`\n\tNoAuthUser                 string        `json:\"-\"`\n\tDefaultSentinel            string        `json:\"-\"`\n\tSystemAccount              string        `json:\"-\"`\n\tNoSystemAccount            bool          `json:\"-\"`\n\tUsername                   string        `json:\"-\"`\n\tPassword                   string        `json:\"-\"`\n\tAuthorization              string        `json:\"-\"`\n\tAuthCallout                *AuthCallout  `json:\"-\"`\n\tPingInterval               time.Duration `json:\"ping_interval\"`\n\tMaxPingsOut                int           `json:\"ping_max\"`\n\tHTTPHost                   string        `json:\"http_host\"`\n\tHTTPPort                   int           `json:\"http_port\"`\n\tHTTPBasePath               string        `json:\"http_base_path\"`\n\tHTTPSPort                  int           `json:\"https_port\"`\n\tAuthTimeout                float64       `json:\"auth_timeout\"`\n\tMaxControlLine             int32         `json:\"max_control_line\"`\n\tMaxPayload                 int32         `json:\"max_payload\"`\n\tMaxPending                 int64         `json:\"max_pending\"`\n\tNoFastProducerStall        bool          `json:\"-\"`\n\tCluster                    ClusterOpts   `json:\"cluster,omitempty\"`\n\tGateway                    GatewayOpts   `json:\"gateway,omitempty\"`\n\tLeafNode                   LeafNodeOpts  `json:\"leaf,omitempty\"`\n\tJetStream                  bool          `json:\"jetstream\"`\n\tJetStreamStrict            bool          `json:\"-\"`\n\tJetStreamMaxMemory         int64         `json:\"-\"`\n\tJetStreamMaxStore          int64         `json:\"-\"`\n\tJetStreamDomain            string        `json:\"-\"`\n\tJetStreamExtHint           string        `json:\"-\"`\n\tJetStreamKey               string        `json:\"-\"`\n\tJetStreamOldKey            string        `json:\"-\"`\n\tJetStreamCipher            StoreCipher   `json:\"-\"`\n\tJetStreamUniqueTag         string\n\tJetStreamLimits            JSLimitOpts\n\tJetStreamTpm               JSTpmOpts\n\tJetStreamMaxCatchup        int64\n\tJetStreamRequestQueueLimit int64\n\tStreamMaxBufferedMsgs      int               `json:\"-\"`\n\tStreamMaxBufferedSize      int64             `json:\"-\"`\n\tStoreDir                   string            `json:\"-\"`\n\tSyncInterval               time.Duration     `json:\"-\"`\n\tSyncAlways                 bool              `json:\"-\"`\n\tJsAccDefaultDomain         map[string]string `json:\"-\"` // account to domain name mapping\n\tWebsocket                  WebsocketOpts     `json:\"-\"`\n\tMQTT                       MQTTOpts          `json:\"-\"`\n\tProfPort                   int               `json:\"-\"`\n\tProfBlockRate              int               `json:\"-\"`\n\tPidFile                    string            `json:\"-\"`\n\tPortsFileDir               string            `json:\"-\"`\n\tLogFile                    string            `json:\"-\"`\n\tLogSizeLimit               int64             `json:\"-\"`\n\tLogMaxFiles                int64             `json:\"-\"`\n\tSyslog                     bool              `json:\"-\"`\n\tRemoteSyslog               string            `json:\"-\"`\n\tRoutes                     []*url.URL        `json:\"-\"`\n\tRoutesStr                  string            `json:\"-\"`\n\tTLSTimeout                 float64           `json:\"tls_timeout\"`\n\tTLS                        bool              `json:\"-\"`\n\tTLSVerify                  bool              `json:\"-\"`\n\tTLSMap                     bool              `json:\"-\"`\n\tTLSCert                    string            `json:\"-\"`\n\tTLSKey                     string            `json:\"-\"`\n\tTLSCaCert                  string            `json:\"-\"`\n\tTLSConfig                  *tls.Config       `json:\"-\"`\n\tTLSPinnedCerts             PinnedCertSet     `json:\"-\"`\n\tTLSRateLimit               int64             `json:\"-\"`\n\t// When set to true, the server will perform the TLS handshake before\n\t// sending the INFO protocol. For clients that are not configured\n\t// with a similar option, their connection will fail with some sort\n\t// of timeout or EOF error since they are expecting to receive an\n\t// INFO protocol first.\n\tTLSHandshakeFirst bool `json:\"-\"`\n\t// If TLSHandshakeFirst is true and this value is strictly positive,\n\t// the server will wait for that amount of time for the TLS handshake\n\t// to start before falling back to previous behavior of sending the\n\t// INFO protocol first. It allows for a mix of newer clients that can\n\t// require a TLS handshake first, and older clients that can't.\n\tTLSHandshakeFirstFallback time.Duration `json:\"-\"`\n\tAllowNonTLS               bool          `json:\"-\"`\n\tWriteDeadline             time.Duration `json:\"-\"`\n\tMaxClosedClients          int           `json:\"-\"`\n\tLameDuckDuration          time.Duration `json:\"-\"`\n\tLameDuckGracePeriod       time.Duration `json:\"-\"`\n\n\t// MaxTracedMsgLen is the maximum printable length for traced messages.\n\tMaxTracedMsgLen int `json:\"-\"`\n\n\t// Operating a trusted NATS server\n\tTrustedKeys              []string              `json:\"-\"`\n\tTrustedOperators         []*jwt.OperatorClaims `json:\"-\"`\n\tAccountResolver          AccountResolver       `json:\"-\"`\n\tAccountResolverTLSConfig *tls.Config           `json:\"-\"`\n\n\t// AlwaysEnableNonce will always present a nonce to new connections\n\t// typically used by custom Authentication implementations who embeds\n\t// the server and so not presented as a configuration option\n\tAlwaysEnableNonce bool\n\n\tCustomClientAuthentication Authentication `json:\"-\"`\n\tCustomRouterAuthentication Authentication `json:\"-\"`\n\n\t// CheckConfig configuration file syntax test was successful and exit.\n\tCheckConfig bool `json:\"-\"`\n\n\t// DisableJetStreamBanner will not print the ascii art on startup for JetStream enabled servers\n\tDisableJetStreamBanner bool `json:\"-\"`\n\n\t// ConnectErrorReports specifies the number of failed attempts\n\t// at which point server should report the failure of an initial\n\t// connection to a route, gateway or leaf node.\n\t// See DEFAULT_CONNECT_ERROR_REPORTS for default value.\n\tConnectErrorReports int\n\n\t// ReconnectErrorReports is similar to ConnectErrorReports except\n\t// that this applies to reconnect events.\n\tReconnectErrorReports int\n\n\t// Tags describing the server. They will be included in varz\n\t// and used as a filter criteria for some system requests.\n\tTags jwt.TagList `json:\"-\"`\n\n\t// OCSPConfig enables OCSP Stapling in the server.\n\tOCSPConfig    *OCSPConfig\n\ttlsConfigOpts *TLSConfigOpts\n\n\t// private fields, used to know if bool options are explicitly\n\t// defined in config and/or command line params.\n\tinConfig  map[string]bool\n\tinCmdLine map[string]bool\n\n\t// private fields for operator mode\n\toperatorJWT            []string\n\tresolverPreloads       map[string]string\n\tresolverPinnedAccounts map[string]struct{}\n\n\t// private fields, used for testing\n\tgatewaysSolicitDelay time.Duration\n\toverrideProto        int\n\n\t// JetStream\n\tmaxMemSet   bool\n\tmaxStoreSet bool\n\tsyncSet     bool\n\n\t// OCSP Cache config enables next-gen cache for OCSP features\n\tOCSPCacheConfig *OCSPResponseCacheConfig\n\n\t// Used to mark that we had a top level authorization block.\n\tauthBlockDefined bool\n\n\t// configDigest represents the state of configuration.\n\tconfigDigest string\n}\n\n// WebsocketOpts are options for websocket\ntype WebsocketOpts struct {\n\t// The server will accept websocket client connections on this hostname/IP.\n\tHost string\n\t// The server will accept websocket client connections on this port.\n\tPort int\n\t// The host:port to advertise to websocket clients in the cluster.\n\tAdvertise string\n\n\t// If no user name is provided when a client connects, will default to the\n\t// matching user from the global list of users in `Options.Users`.\n\tNoAuthUser string\n\n\t// Name of the cookie, which if present in WebSocket upgrade headers,\n\t// will be treated as JWT during CONNECT phase as long as\n\t// \"jwt\" specified in the CONNECT options is missing or empty.\n\tJWTCookie string\n\n\t// Name of the cookie, which if present in WebSocket upgrade headers,\n\t// will be treated as Username during CONNECT phase as long as\n\t// \"user\" specified in the CONNECT options is missing or empty.\n\tUsernameCookie string\n\n\t// Name of the cookie, which if present in WebSocket upgrade headers,\n\t// will be treated as Password during CONNECT phase as long as\n\t// \"pass\" specified in the CONNECT options is missing or empty.\n\tPasswordCookie string\n\n\t// Name of the cookie, which if present in WebSocket upgrade headers,\n\t// will be treated as Token during CONNECT phase as long as\n\t// \"auth_token\" specified in the CONNECT options is missing or empty.\n\t// Note that when this is useful for passing a JWT to an cuth callout\n\t// when the server uses delegated authentication (\"operator mode\") or\n\t// when using delegated authentication, but the auth callout validates some\n\t// other JWT or string. Note that this does map to an actual server-wide\n\t// \"auth_token\", note that using it for that purpose is greatly discouraged.\n\tTokenCookie string\n\n\t// Authentication section. If anything is configured in this section,\n\t// it will override the authorization configuration of regular clients.\n\tUsername string\n\tPassword string\n\tToken    string\n\n\t// Timeout for the authentication process.\n\tAuthTimeout float64\n\n\t// By default the server will enforce the use of TLS. If no TLS configuration\n\t// is provided, you need to explicitly set NoTLS to true to allow the server\n\t// to start without TLS configuration. Note that if a TLS configuration is\n\t// present, this boolean is ignored and the server will run the Websocket\n\t// server with that TLS configuration.\n\t// Running without TLS is less secure since Websocket clients that use bearer\n\t// tokens will send them in clear. So this should not be used in production.\n\tNoTLS bool\n\n\t// TLS configuration is required.\n\tTLSConfig *tls.Config\n\t// If true, map certificate values for authentication purposes.\n\tTLSMap bool\n\n\t// When present, accepted client certificates (verify/verify_and_map) must be in this list\n\tTLSPinnedCerts PinnedCertSet\n\n\t// If true, the Origin header must match the request's host.\n\tSameOrigin bool\n\n\t// Only origins in this list will be accepted. If empty and\n\t// SameOrigin is false, any origin is accepted.\n\tAllowedOrigins []string\n\n\t// If set to true, the server will negotiate with clients\n\t// if compression can be used. If this is false, no compression\n\t// will be used (both in server and clients) since it has to\n\t// be negotiated between both endpoints\n\tCompression bool\n\n\t// Total time allowed for the server to read the client request\n\t// and write the response back to the client. This include the\n\t// time needed for the TLS Handshake.\n\tHandshakeTimeout time.Duration\n\n\t// Headers to be added to the upgrade response.\n\t// Useful for adding custom headers like Strict-Transport-Security.\n\tHeaders map[string]string\n\n\t// Snapshot of configured TLS options.\n\ttlsConfigOpts *TLSConfigOpts\n}\n\n// MQTTOpts are options for MQTT\ntype MQTTOpts struct {\n\t// The server will accept MQTT client connections on this hostname/IP.\n\tHost string\n\t// The server will accept MQTT client connections on this port.\n\tPort int\n\n\t// If no user name is provided when a client connects, will default to the\n\t// matching user from the global list of users in `Options.Users`.\n\tNoAuthUser string\n\n\t// Authentication section. If anything is configured in this section,\n\t// it will override the authorization configuration of regular clients.\n\tUsername string\n\tPassword string\n\tToken    string\n\n\t// JetStream domain mqtt is supposed to pick up\n\tJsDomain string\n\n\t// Number of replicas for MQTT streams.\n\t// Negative or 0 value means that the server(s) will pick a replica\n\t// number based on the known size of the cluster (but capped at 3).\n\t// Note that if an account was already connected, the stream's replica\n\t// count is not modified. Use the NATS CLI to update the count if desired.\n\tStreamReplicas int\n\n\t// Number of replicas for MQTT consumers.\n\t// Negative or 0 value means that there is no override and the consumer\n\t// will have the same replica factor that the stream it belongs to.\n\t// If a value is specified, it will require to be lower than the stream\n\t// replicas count (lower than StreamReplicas if specified, but also lower\n\t// than the automatic value determined by cluster size).\n\t// Note that existing consumers are not modified.\n\t//\n\t// UPDATE: This is no longer used while messages stream has interest policy retention\n\t// which requires consumer replica count to match the parent stream.\n\tConsumerReplicas int\n\n\t// Indicate if the consumers should be created with memory storage.\n\t// Note that existing consumers are not modified.\n\tConsumerMemoryStorage bool\n\n\t// If specified will have the system auto-cleanup the consumers after being\n\t// inactive for the specified amount of time.\n\tConsumerInactiveThreshold time.Duration\n\n\t// Timeout for the authentication process.\n\tAuthTimeout float64\n\n\t// TLS configuration is required.\n\tTLSConfig *tls.Config\n\t// If true, map certificate values for authentication purposes.\n\tTLSMap bool\n\t// Timeout for the TLS handshake\n\tTLSTimeout float64\n\t// Set of allowable certificates\n\tTLSPinnedCerts PinnedCertSet\n\n\t// AckWait is the amount of time after which a QoS 1 or 2 message sent to a\n\t// client is redelivered as a DUPLICATE if the server has not received the\n\t// PUBACK on the original Packet Identifier. The same value applies to\n\t// PubRel redelivery. The value has to be positive. Zero will cause the\n\t// server to use the default value (30 seconds). Note that changes to this\n\t// option is applied only to new MQTT subscriptions (or sessions for\n\t// PubRels).\n\tAckWait time.Duration\n\n\t// JSAPITimeout defines timeout for JetStream api calls (default is 5 seconds)\n\tJSAPITimeout time.Duration\n\n\t// MaxAckPending is the amount of QoS 1 and 2 messages (combined) the server\n\t// can send to a subscription without receiving any PUBACK for those\n\t// messages. The valid range is [0..65535].\n\t//\n\t// The total of subscriptions' MaxAckPending on a given session cannot\n\t// exceed 65535. Attempting to create a subscription that would bring the\n\t// total above the limit would result in the server returning 0x80 in the\n\t// SUBACK for this subscription.\n\t//\n\t// Due to how the NATS Server handles the MQTT \"#\" wildcard, each\n\t// subscription ending with \"#\" will use 2 times the MaxAckPending value.\n\t// Note that changes to this option is applied only to new subscriptions.\n\tMaxAckPending uint16\n\n\t// Snapshot of configured TLS options.\n\ttlsConfigOpts *TLSConfigOpts\n\n\t// rejectQoS2Pub tells the MQTT client to not accept QoS2 PUBLISH, instead\n\t// error and terminate the connection.\n\trejectQoS2Pub bool\n\n\t// downgradeQOS2Sub tells the MQTT client to downgrade QoS2 SUBSCRIBE\n\t// requests to QoS1.\n\tdowngradeQoS2Sub bool\n}\n\ntype netResolver interface {\n\tLookupHost(ctx context.Context, host string) ([]string, error)\n}\n\n// Clone performs a deep copy of the Options struct, returning a new clone\n// with all values copied.\nfunc (o *Options) Clone() *Options {\n\tif o == nil {\n\t\treturn nil\n\t}\n\tclone := &Options{}\n\t*clone = *o\n\tif o.Users != nil {\n\t\tclone.Users = make([]*User, len(o.Users))\n\t\tfor i, user := range o.Users {\n\t\t\tclone.Users[i] = user.clone()\n\t\t}\n\t}\n\tif o.Nkeys != nil {\n\t\tclone.Nkeys = make([]*NkeyUser, len(o.Nkeys))\n\t\tfor i, nkey := range o.Nkeys {\n\t\t\tclone.Nkeys[i] = nkey.clone()\n\t\t}\n\t}\n\n\tif o.Routes != nil {\n\t\tclone.Routes = deepCopyURLs(o.Routes)\n\t}\n\tif o.TLSConfig != nil {\n\t\tclone.TLSConfig = o.TLSConfig.Clone()\n\t}\n\tif o.Cluster.TLSConfig != nil {\n\t\tclone.Cluster.TLSConfig = o.Cluster.TLSConfig.Clone()\n\t}\n\tif o.Gateway.TLSConfig != nil {\n\t\tclone.Gateway.TLSConfig = o.Gateway.TLSConfig.Clone()\n\t}\n\tif len(o.Gateway.Gateways) > 0 {\n\t\tclone.Gateway.Gateways = make([]*RemoteGatewayOpts, len(o.Gateway.Gateways))\n\t\tfor i, g := range o.Gateway.Gateways {\n\t\t\tclone.Gateway.Gateways[i] = g.clone()\n\t\t}\n\t}\n\t// FIXME(dlc) - clone leaf node stuff.\n\treturn clone\n}\n\nfunc deepCopyURLs(urls []*url.URL) []*url.URL {\n\tif urls == nil {\n\t\treturn nil\n\t}\n\tcurls := make([]*url.URL, len(urls))\n\tfor i, u := range urls {\n\t\tcu := &url.URL{}\n\t\t*cu = *u\n\t\tcurls[i] = cu\n\t}\n\treturn curls\n}\n\n// Configuration file authorization section.\ntype authorization struct {\n\t// Singles\n\tuser  string\n\tpass  string\n\ttoken string\n\tnkey  string\n\tacc   string\n\t// Multiple Nkeys/Users\n\tnkeys              []*NkeyUser\n\tusers              []*User\n\ttimeout            float64\n\tdefaultPermissions *Permissions\n\t// Auth Callouts\n\tcallout *AuthCallout\n}\n\n// TLSConfigOpts holds the parsed tls config information,\n// used with flag parsing\ntype TLSConfigOpts struct {\n\tCertFile             string\n\tKeyFile              string\n\tCaFile               string\n\tVerify               bool\n\tInsecure             bool\n\tMap                  bool\n\tTLSCheckKnownURLs    bool\n\tHandshakeFirst       bool          // Indicate that the TLS handshake should occur first, before sending the INFO protocol.\n\tFallbackDelay        time.Duration // Where supported, indicates how long to wait for the handshake before falling back to sending the INFO protocol first.\n\tTimeout              float64\n\tRateLimit            int64\n\tCiphers              []uint16\n\tCurvePreferences     []tls.CurveID\n\tPinnedCerts          PinnedCertSet\n\tCertStore            certstore.StoreType\n\tCertMatchBy          certstore.MatchByType\n\tCertMatch            string\n\tCertMatchSkipInvalid bool\n\tCaCertsMatch         []string\n\tOCSPPeerConfig       *certidp.OCSPPeerConfig\n\tCertificates         []*TLSCertPairOpt\n\tMinVersion           uint16\n}\n\n// TLSCertPairOpt are the paths to a certificate and private key.\ntype TLSCertPairOpt struct {\n\tCertFile string\n\tKeyFile  string\n}\n\n// OCSPConfig represents the options of OCSP stapling options.\ntype OCSPConfig struct {\n\t// Mode defines the policy for OCSP stapling.\n\tMode OCSPMode\n\n\t// OverrideURLs is the http URL endpoint used to get OCSP staples.\n\tOverrideURLs []string\n}\n\nvar tlsUsage = `\nTLS configuration is specified in the tls section of a configuration file:\n\ne.g.\n\n    tls {\n        cert_file:      \"./certs/server-cert.pem\"\n        key_file:       \"./certs/server-key.pem\"\n        ca_file:        \"./certs/ca.pem\"\n        verify:         true\n        verify_and_map: true\n\n        cipher_suites: [\n            \"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256\",\n            \"TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\"\n        ]\n        curve_preferences: [\n            \"CurveP256\",\n            \"CurveP384\",\n            \"CurveP521\"\n        ]\n    }\n\nAvailable cipher suites include:\n`\n\n// ProcessConfigFile processes a configuration file.\n// FIXME(dlc): A bit hacky\nfunc ProcessConfigFile(configFile string) (*Options, error) {\n\topts := &Options{}\n\tif err := opts.ProcessConfigFile(configFile); err != nil {\n\t\t// If only warnings then continue and return the options.\n\t\tif cerr, ok := err.(*processConfigErr); ok && len(cerr.Errors()) == 0 {\n\t\t\treturn opts, nil\n\t\t}\n\n\t\treturn nil, err\n\t}\n\treturn opts, nil\n}\n\n// token is an item parsed from the configuration.\ntype token interface {\n\tValue() any\n\tLine() int\n\tIsUsedVariable() bool\n\tSourceFile() string\n\tPosition() int\n}\n\n// unwrapValue can be used to get the token and value from an item\n// to be able to report the line number in case of an incorrect\n// configuration.\n// also stores the token in lastToken for use in convertPanicToError\nfunc unwrapValue(v any, lastToken *token) (token, any) {\n\tswitch tk := v.(type) {\n\tcase token:\n\t\tif lastToken != nil {\n\t\t\t*lastToken = tk\n\t\t}\n\t\treturn tk, tk.Value()\n\tdefault:\n\t\treturn nil, v\n\t}\n}\n\n// use in defer to recover from panic and turn it into an error associated with last token\nfunc convertPanicToErrorList(lastToken *token, errors *[]error) {\n\t// only recover if an error can be stored\n\tif errors == nil {\n\t\treturn\n\t} else if err := recover(); err == nil {\n\t\treturn\n\t} else if lastToken != nil && *lastToken != nil {\n\t\t*errors = append(*errors, &configErr{*lastToken, fmt.Sprint(err)})\n\t} else {\n\t\t*errors = append(*errors, fmt.Errorf(\"encountered panic without a token %v\", err))\n\t}\n}\n\n// use in defer to recover from panic and turn it into an error associated with last token\nfunc convertPanicToError(lastToken *token, e *error) {\n\t// only recover if an error can be stored\n\tif e == nil || *e != nil {\n\t\treturn\n\t} else if err := recover(); err == nil {\n\t\treturn\n\t} else if lastToken != nil && *lastToken != nil {\n\t\t*e = &configErr{*lastToken, fmt.Sprint(err)}\n\t} else {\n\t\t*e = fmt.Errorf(\"%v\", err)\n\t}\n}\n\n// configureSystemAccount configures a system account\n// if present in the configuration.\nfunc configureSystemAccount(o *Options, m map[string]any) (retErr error) {\n\tvar lt token\n\tdefer convertPanicToError(&lt, &retErr)\n\tconfigure := func(v any) error {\n\t\ttk, v := unwrapValue(v, &lt)\n\t\tsa, ok := v.(string)\n\t\tif !ok {\n\t\t\treturn &configErr{tk, \"system account name must be a string\"}\n\t\t}\n\t\to.SystemAccount = sa\n\t\treturn nil\n\t}\n\n\tif v, ok := m[\"system_account\"]; ok {\n\t\treturn configure(v)\n\t} else if v, ok := m[\"system\"]; ok {\n\t\treturn configure(v)\n\t}\n\n\treturn nil\n}\n\n// ProcessConfigFile updates the Options structure with options\n// present in the given configuration file.\n// This version is convenient if one wants to set some default\n// options and then override them with what is in the config file.\n// For instance, this version allows you to do something such as:\n//\n// opts := &Options{Debug: true}\n// opts.ProcessConfigFile(myConfigFile)\n//\n// If the config file contains \"debug: false\", after this call,\n// opts.Debug would really be false. It would be impossible to\n// achieve that with the non receiver ProcessConfigFile() version,\n// since one would not know after the call if \"debug\" was not present\n// or was present but set to false.\nfunc (o *Options) ProcessConfigFile(configFile string) error {\n\to.ConfigFile = configFile\n\tif configFile == _EMPTY_ {\n\t\treturn nil\n\t}\n\tm, digest, err := conf.ParseFileWithChecksDigest(configFile)\n\tif err != nil {\n\t\treturn err\n\t}\n\to.configDigest = digest\n\n\treturn o.processConfigFile(configFile, m)\n}\n\n// ProcessConfigString is the same as ProcessConfigFile, but expects the\n// contents of the config file to be passed in rather than the file name.\nfunc (o *Options) ProcessConfigString(data string) error {\n\tm, err := conf.ParseWithChecks(data)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\treturn o.processConfigFile(_EMPTY_, m)\n}\n\n// ConfigDigest returns the digest representing the configuration.\nfunc (o *Options) ConfigDigest() string {\n\treturn o.configDigest\n}\n\nfunc (o *Options) processConfigFile(configFile string, m map[string]any) error {\n\t// Collect all errors and warnings and report them all together.\n\terrors := make([]error, 0)\n\twarnings := make([]error, 0)\n\tif len(m) == 0 {\n\t\twarnings = append(warnings, fmt.Errorf(\"%s: config has no values or is empty\", configFile))\n\t}\n\n\t// First check whether a system account has been defined,\n\t// as that is a condition for other features to be enabled.\n\tif err := configureSystemAccount(o, m); err != nil {\n\t\terrors = append(errors, err)\n\t}\n\n\tfor k, v := range m {\n\t\to.processConfigFileLine(k, v, &errors, &warnings)\n\t}\n\n\t// Post-process: check auth callout allowed accounts against configured accounts.\n\tif o.AuthCallout != nil {\n\t\taccounts := make(map[string]struct{})\n\t\tfor _, acc := range o.Accounts {\n\t\t\taccounts[acc.Name] = struct{}{}\n\t\t}\n\n\t\tfor _, acc := range o.AuthCallout.AllowedAccounts {\n\t\t\tif _, ok := accounts[acc]; !ok {\n\t\t\t\terr := &configErr{nil, fmt.Sprintf(\"auth_callout allowed account %q not found in configured accounts\", acc)}\n\t\t\t\terrors = append(errors, err)\n\t\t\t}\n\t\t}\n\t}\n\n\tif len(errors) > 0 || len(warnings) > 0 {\n\t\treturn &processConfigErr{\n\t\t\terrors:   errors,\n\t\t\twarnings: warnings,\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc (o *Options) processConfigFileLine(k string, v any, errors *[]error, warnings *[]error) {\n\tvar lt token\n\tdefer convertPanicToErrorList(&lt, errors)\n\n\ttk, v := unwrapValue(v, &lt)\n\tswitch strings.ToLower(k) {\n\tcase \"listen\":\n\t\thp, err := parseListen(v)\n\t\tif err != nil {\n\t\t\t*errors = append(*errors, &configErr{tk, err.Error()})\n\t\t\treturn\n\t\t}\n\t\to.Host = hp.host\n\t\to.Port = hp.port\n\tcase \"client_advertise\":\n\t\to.ClientAdvertise = v.(string)\n\tcase \"port\":\n\t\to.Port = int(v.(int64))\n\tcase \"server_name\":\n\t\tsn := v.(string)\n\t\tif strings.Contains(sn, \" \") {\n\t\t\terr := &configErr{tk, ErrServerNameHasSpaces.Error()}\n\t\t\t*errors = append(*errors, err)\n\t\t\treturn\n\t\t}\n\t\to.ServerName = sn\n\tcase \"host\", \"net\":\n\t\to.Host = v.(string)\n\tcase \"debug\":\n\t\to.Debug = v.(bool)\n\t\ttrackExplicitVal(&o.inConfig, \"Debug\", o.Debug)\n\tcase \"trace\":\n\t\to.Trace = v.(bool)\n\t\ttrackExplicitVal(&o.inConfig, \"Trace\", o.Trace)\n\tcase \"trace_verbose\":\n\t\to.TraceVerbose = v.(bool)\n\t\to.Trace = v.(bool)\n\t\ttrackExplicitVal(&o.inConfig, \"TraceVerbose\", o.TraceVerbose)\n\t\ttrackExplicitVal(&o.inConfig, \"Trace\", o.Trace)\n\tcase \"trace_headers\":\n\t\to.TraceHeaders = v.(bool)\n\t\to.Trace = v.(bool)\n\t\ttrackExplicitVal(&o.inConfig, \"TraceHeaders\", o.TraceHeaders)\n\t\ttrackExplicitVal(&o.inConfig, \"Trace\", o.Trace)\n\tcase \"logtime\":\n\t\to.Logtime = v.(bool)\n\t\ttrackExplicitVal(&o.inConfig, \"Logtime\", o.Logtime)\n\tcase \"logtime_utc\":\n\t\to.LogtimeUTC = v.(bool)\n\t\ttrackExplicitVal(&o.inConfig, \"LogtimeUTC\", o.LogtimeUTC)\n\tcase \"mappings\", \"maps\":\n\t\tgacc := NewAccount(globalAccountName)\n\t\to.Accounts = append(o.Accounts, gacc)\n\t\terr := parseAccountMappings(tk, gacc, errors)\n\t\tif err != nil {\n\t\t\t*errors = append(*errors, err)\n\t\t\treturn\n\t\t}\n\tcase \"disable_sublist_cache\", \"no_sublist_cache\":\n\t\to.NoSublistCache = v.(bool)\n\tcase \"accounts\":\n\t\terr := parseAccounts(tk, o, errors, warnings)\n\t\tif err != nil {\n\t\t\t*errors = append(*errors, err)\n\t\t\treturn\n\t\t}\n\tcase \"default_sentinel\":\n\t\to.DefaultSentinel = v.(string)\n\tcase \"authorization\":\n\t\tauth, err := parseAuthorization(tk, errors, warnings)\n\t\tif err != nil {\n\t\t\t*errors = append(*errors, err)\n\t\t\treturn\n\t\t}\n\t\to.authBlockDefined = true\n\t\to.Username = auth.user\n\t\to.Password = auth.pass\n\t\to.Authorization = auth.token\n\t\to.AuthTimeout = auth.timeout\n\t\to.AuthCallout = auth.callout\n\n\t\tif (auth.user != _EMPTY_ || auth.pass != _EMPTY_) && auth.token != _EMPTY_ {\n\t\t\terr := &configErr{tk, \"Cannot have a user/pass and token\"}\n\t\t\t*errors = append(*errors, err)\n\t\t\treturn\n\t\t}\n\t\t// In case parseAccounts() was done first, we need to check for duplicates.\n\t\tunames := setupUsersAndNKeysDuplicateCheckMap(o)\n\t\t// Check for multiple users defined.\n\t\t// Note: auth.users will be != nil as long as `users: []` is present\n\t\t// in the authorization block, even if empty, and will also account for\n\t\t// nkey users. We also check for users/nkeys that may have been already\n\t\t// added in parseAccounts() (which means they will be in unames)\n\t\tif auth.users != nil || len(unames) > 0 {\n\t\t\tif auth.user != _EMPTY_ {\n\t\t\t\terr := &configErr{tk, \"Can not have a single user/pass and a users array\"}\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t\treturn\n\t\t\t}\n\t\t\tif auth.token != _EMPTY_ {\n\t\t\t\terr := &configErr{tk, \"Can not have a token and a users array\"}\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t\treturn\n\t\t\t}\n\t\t\t// Now check that if we have users, there is no duplicate, including\n\t\t\t// users that may have been configured in parseAccounts().\n\t\t\tif len(auth.users) > 0 {\n\t\t\t\tfor _, u := range auth.users {\n\t\t\t\t\tif _, ok := unames[u.Username]; ok {\n\t\t\t\t\t\terr := &configErr{tk, fmt.Sprintf(\"Duplicate user %q detected\", u.Username)}\n\t\t\t\t\t\t*errors = append(*errors, err)\n\t\t\t\t\t\treturn\n\t\t\t\t\t}\n\t\t\t\t\tunames[u.Username] = struct{}{}\n\t\t\t\t}\n\t\t\t\t// Users may have been added from Accounts parsing, so do an append here\n\t\t\t\to.Users = append(o.Users, auth.users...)\n\t\t\t}\n\t\t}\n\t\t// Check for nkeys\n\t\tif len(auth.nkeys) > 0 {\n\t\t\tfor _, u := range auth.nkeys {\n\t\t\t\tif _, ok := unames[u.Nkey]; ok {\n\t\t\t\t\terr := &configErr{tk, fmt.Sprintf(\"Duplicate nkey %q detected\", u.Nkey)}\n\t\t\t\t\t*errors = append(*errors, err)\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t\tunames[u.Nkey] = struct{}{}\n\t\t\t}\n\t\t\t// NKeys may have been added from Accounts parsing, so do an append here\n\t\t\to.Nkeys = append(o.Nkeys, auth.nkeys...)\n\t\t}\n\tcase \"http\":\n\t\thp, err := parseListen(v)\n\t\tif err != nil {\n\t\t\terr := &configErr{tk, err.Error()}\n\t\t\t*errors = append(*errors, err)\n\t\t\treturn\n\t\t}\n\t\to.HTTPHost = hp.host\n\t\to.HTTPPort = hp.port\n\tcase \"https\":\n\t\thp, err := parseListen(v)\n\t\tif err != nil {\n\t\t\terr := &configErr{tk, err.Error()}\n\t\t\t*errors = append(*errors, err)\n\t\t\treturn\n\t\t}\n\t\to.HTTPHost = hp.host\n\t\to.HTTPSPort = hp.port\n\tcase \"http_port\", \"monitor_port\":\n\t\to.HTTPPort = int(v.(int64))\n\tcase \"https_port\":\n\t\to.HTTPSPort = int(v.(int64))\n\tcase \"http_base_path\":\n\t\to.HTTPBasePath = v.(string)\n\tcase \"cluster\":\n\t\terr := parseCluster(tk, o, errors, warnings)\n\t\tif err != nil {\n\t\t\t*errors = append(*errors, err)\n\t\t\treturn\n\t\t}\n\tcase \"gateway\":\n\t\tif err := parseGateway(tk, o, errors, warnings); err != nil {\n\t\t\t*errors = append(*errors, err)\n\t\t\treturn\n\t\t}\n\tcase \"leaf\", \"leafnodes\":\n\t\terr := parseLeafNodes(tk, o, errors, warnings)\n\t\tif err != nil {\n\t\t\t*errors = append(*errors, err)\n\t\t\treturn\n\t\t}\n\tcase \"store_dir\", \"storedir\":\n\t\t// Check if JetStream configuration is also setting the storage directory.\n\t\tif o.StoreDir != _EMPTY_ {\n\t\t\t*errors = append(*errors, &configErr{tk, \"Duplicate 'store_dir' configuration\"})\n\t\t\treturn\n\t\t}\n\t\to.StoreDir = v.(string)\n\tcase \"jetstream\":\n\t\terr := parseJetStream(tk, o, errors, warnings)\n\t\tif err != nil {\n\t\t\t*errors = append(*errors, err)\n\t\t\treturn\n\t\t}\n\tcase \"logfile\", \"log_file\":\n\t\to.LogFile = v.(string)\n\tcase \"logfile_size_limit\", \"log_size_limit\":\n\t\to.LogSizeLimit = v.(int64)\n\tcase \"logfile_max_num\", \"log_max_num\":\n\t\to.LogMaxFiles = v.(int64)\n\tcase \"syslog\":\n\t\to.Syslog = v.(bool)\n\t\ttrackExplicitVal(&o.inConfig, \"Syslog\", o.Syslog)\n\tcase \"remote_syslog\":\n\t\to.RemoteSyslog = v.(string)\n\tcase \"pidfile\", \"pid_file\":\n\t\to.PidFile = v.(string)\n\tcase \"ports_file_dir\":\n\t\to.PortsFileDir = v.(string)\n\tcase \"prof_port\":\n\t\to.ProfPort = int(v.(int64))\n\tcase \"prof_block_rate\":\n\t\to.ProfBlockRate = int(v.(int64))\n\tcase \"max_control_line\":\n\t\tif v.(int64) > 1<<31-1 {\n\t\t\terr := &configErr{tk, fmt.Sprintf(\"%s value is too big\", k)}\n\t\t\t*errors = append(*errors, err)\n\t\t\treturn\n\t\t}\n\t\to.MaxControlLine = int32(v.(int64))\n\tcase \"max_payload\":\n\t\tif v.(int64) > 1<<31-1 {\n\t\t\terr := &configErr{tk, fmt.Sprintf(\"%s value is too big\", k)}\n\t\t\t*errors = append(*errors, err)\n\t\t\treturn\n\t\t}\n\t\to.MaxPayload = int32(v.(int64))\n\tcase \"max_pending\":\n\t\to.MaxPending = v.(int64)\n\tcase \"max_connections\", \"max_conn\":\n\t\to.MaxConn = int(v.(int64))\n\tcase \"max_traced_msg_len\":\n\t\to.MaxTracedMsgLen = int(v.(int64))\n\tcase \"max_subscriptions\", \"max_subs\":\n\t\to.MaxSubs = int(v.(int64))\n\tcase \"max_sub_tokens\", \"max_subscription_tokens\":\n\t\tif n := v.(int64); n > math.MaxUint8 {\n\t\t\terr := &configErr{tk, fmt.Sprintf(\"%s value is too big\", k)}\n\t\t\t*errors = append(*errors, err)\n\t\t\treturn\n\t\t} else if n <= 0 {\n\t\t\terr := &configErr{tk, fmt.Sprintf(\"%s value can not be negative\", k)}\n\t\t\t*errors = append(*errors, err)\n\t\t\treturn\n\t\t} else {\n\t\t\to.MaxSubTokens = uint8(n)\n\t\t}\n\tcase \"ping_interval\":\n\t\to.PingInterval = parseDuration(\"ping_interval\", tk, v, errors, warnings)\n\tcase \"ping_max\":\n\t\to.MaxPingsOut = int(v.(int64))\n\tcase \"tls\":\n\t\ttc, err := parseTLS(tk, true)\n\t\tif err != nil {\n\t\t\t*errors = append(*errors, err)\n\t\t\treturn\n\t\t}\n\t\tif o.TLSConfig, err = GenTLSConfig(tc); err != nil {\n\t\t\terr := &configErr{tk, err.Error()}\n\t\t\t*errors = append(*errors, err)\n\t\t\treturn\n\t\t}\n\t\to.TLSTimeout = tc.Timeout\n\t\to.TLSMap = tc.Map\n\t\to.TLSPinnedCerts = tc.PinnedCerts\n\t\to.TLSRateLimit = tc.RateLimit\n\t\to.TLSHandshakeFirst = tc.HandshakeFirst\n\t\to.TLSHandshakeFirstFallback = tc.FallbackDelay\n\n\t\t// Need to keep track of path of the original TLS config\n\t\t// and certs path for OCSP Stapling monitoring.\n\t\to.tlsConfigOpts = tc\n\tcase \"ocsp\":\n\t\tswitch vv := v.(type) {\n\t\tcase bool:\n\t\t\tif vv {\n\t\t\t\t// Default is Auto which honors Must Staple status request\n\t\t\t\t// but does not shutdown the server in case it is revoked,\n\t\t\t\t// letting the client choose whether to trust or not the server.\n\t\t\t\to.OCSPConfig = &OCSPConfig{Mode: OCSPModeAuto}\n\t\t\t} else {\n\t\t\t\to.OCSPConfig = &OCSPConfig{Mode: OCSPModeNever}\n\t\t\t}\n\t\tcase map[string]any:\n\t\t\tocsp := &OCSPConfig{Mode: OCSPModeAuto}\n\n\t\t\tfor kk, kv := range vv {\n\t\t\t\t_, v = unwrapValue(kv, &tk)\n\t\t\t\tswitch kk {\n\t\t\t\tcase \"mode\":\n\t\t\t\t\tmode := v.(string)\n\t\t\t\t\tswitch {\n\t\t\t\t\tcase strings.EqualFold(mode, \"always\"):\n\t\t\t\t\t\tocsp.Mode = OCSPModeAlways\n\t\t\t\t\tcase strings.EqualFold(mode, \"must\"):\n\t\t\t\t\t\tocsp.Mode = OCSPModeMust\n\t\t\t\t\tcase strings.EqualFold(mode, \"never\"):\n\t\t\t\t\t\tocsp.Mode = OCSPModeNever\n\t\t\t\t\tcase strings.EqualFold(mode, \"auto\"):\n\t\t\t\t\t\tocsp.Mode = OCSPModeAuto\n\t\t\t\t\tdefault:\n\t\t\t\t\t\t*errors = append(*errors, &configErr{tk, fmt.Sprintf(\"error parsing ocsp config: unsupported ocsp mode %T\", mode)})\n\t\t\t\t\t}\n\t\t\t\tcase \"urls\":\n\t\t\t\t\turls := v.([]string)\n\t\t\t\t\tocsp.OverrideURLs = urls\n\t\t\t\tcase \"url\":\n\t\t\t\t\turl := v.(string)\n\t\t\t\t\tocsp.OverrideURLs = []string{url}\n\t\t\t\tdefault:\n\t\t\t\t\t*errors = append(*errors, &configErr{tk, fmt.Sprintf(\"error parsing ocsp config: unsupported field %T\", kk)})\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}\n\t\t\to.OCSPConfig = ocsp\n\t\tdefault:\n\t\t\t*errors = append(*errors, &configErr{tk, fmt.Sprintf(\"error parsing ocsp config: unsupported type %T\", v)})\n\t\t\treturn\n\t\t}\n\tcase \"allow_non_tls\":\n\t\to.AllowNonTLS = v.(bool)\n\tcase \"write_deadline\":\n\t\to.WriteDeadline = parseDuration(\"write_deadline\", tk, v, errors, warnings)\n\tcase \"lame_duck_duration\":\n\t\tdur, err := time.ParseDuration(v.(string))\n\t\tif err != nil {\n\t\t\terr := &configErr{tk, fmt.Sprintf(\"error parsing lame_duck_duration: %v\", err)}\n\t\t\t*errors = append(*errors, err)\n\t\t\treturn\n\t\t}\n\t\tif dur < 30*time.Second {\n\t\t\terr := &configErr{tk, fmt.Sprintf(\"invalid lame_duck_duration of %v, minimum is 30 seconds\", dur)}\n\t\t\t*errors = append(*errors, err)\n\t\t\treturn\n\t\t}\n\t\to.LameDuckDuration = dur\n\tcase \"lame_duck_grace_period\":\n\t\tdur, err := time.ParseDuration(v.(string))\n\t\tif err != nil {\n\t\t\terr := &configErr{tk, fmt.Sprintf(\"error parsing lame_duck_grace_period: %v\", err)}\n\t\t\t*errors = append(*errors, err)\n\t\t\treturn\n\t\t}\n\t\tif dur < 0 {\n\t\t\terr := &configErr{tk, \"invalid lame_duck_grace_period, needs to be positive\"}\n\t\t\t*errors = append(*errors, err)\n\t\t\treturn\n\t\t}\n\t\to.LameDuckGracePeriod = dur\n\tcase \"operator\", \"operators\", \"roots\", \"root\", \"root_operators\", \"root_operator\":\n\t\topFiles := []string{}\n\t\tswitch v := v.(type) {\n\t\tcase string:\n\t\t\topFiles = append(opFiles, v)\n\t\tcase []string:\n\t\t\topFiles = append(opFiles, v...)\n\t\tcase []any:\n\t\t\tfor _, t := range v {\n\t\t\t\tif token, ok := t.(token); ok {\n\t\t\t\t\tif v, ok := token.Value().(string); ok {\n\t\t\t\t\t\topFiles = append(opFiles, v)\n\t\t\t\t\t} else {\n\t\t\t\t\t\terr := &configErr{tk, fmt.Sprintf(\"error parsing operators: unsupported type %T where string is expected\", token)}\n\t\t\t\t\t\t*errors = append(*errors, err)\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\terr := &configErr{tk, fmt.Sprintf(\"error parsing operators: unsupported type %T\", t)}\n\t\t\t\t\t*errors = append(*errors, err)\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\tdefault:\n\t\t\terr := &configErr{tk, fmt.Sprintf(\"error parsing operators: unsupported type %T\", v)}\n\t\t\t*errors = append(*errors, err)\n\t\t}\n\t\t// Assume for now these are file names, but they can also be the JWT itself inline.\n\t\to.TrustedOperators = make([]*jwt.OperatorClaims, 0, len(opFiles))\n\t\tfor _, fname := range opFiles {\n\t\t\ttheJWT, opc, err := readOperatorJWT(fname)\n\t\t\tif err != nil {\n\t\t\t\terr := &configErr{tk, fmt.Sprintf(\"error parsing operator JWT: %v\", err)}\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\to.operatorJWT = append(o.operatorJWT, theJWT)\n\t\t\to.TrustedOperators = append(o.TrustedOperators, opc)\n\t\t}\n\t\tif len(o.TrustedOperators) == 1 {\n\t\t\t// In case \"resolver\" is defined as well, it takes precedence\n\t\t\tif o.AccountResolver == nil {\n\t\t\t\tif accUrl, err := parseURL(o.TrustedOperators[0].AccountServerURL, \"account resolver\"); err == nil {\n\t\t\t\t\t// nsc automatically appends \"/accounts\" during nsc push\n\t\t\t\t\to.AccountResolver, _ = NewURLAccResolver(accUrl.String() + \"/accounts\")\n\t\t\t\t}\n\t\t\t}\n\t\t\t// In case \"system_account\" is defined as well, it takes precedence\n\t\t\tif o.SystemAccount == _EMPTY_ {\n\t\t\t\to.SystemAccount = o.TrustedOperators[0].SystemAccount\n\t\t\t}\n\t\t}\n\tcase \"resolver\", \"account_resolver\", \"accounts_resolver\":\n\t\tswitch v := v.(type) {\n\t\tcase string:\n\t\t\t// \"resolver\" takes precedence over value obtained from \"operator\".\n\t\t\t// Clear so that parsing errors are not silently ignored.\n\t\t\to.AccountResolver = nil\n\t\t\tmemResolverRe := regexp.MustCompile(`(?i)(MEM|MEMORY)\\s*`)\n\t\t\tresolverRe := regexp.MustCompile(`(?i)(?:URL){1}(?:\\({1}\\s*\"?([^\\s\"]*)\"?\\s*\\){1})?\\s*`)\n\t\t\tif memResolverRe.MatchString(v) {\n\t\t\t\to.AccountResolver = &MemAccResolver{}\n\t\t\t} else if items := resolverRe.FindStringSubmatch(v); len(items) == 2 {\n\t\t\t\turl := items[1]\n\t\t\t\t_, err := parseURL(url, \"account resolver\")\n\t\t\t\tif err != nil {\n\t\t\t\t\t*errors = append(*errors, &configErr{tk, err.Error()})\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t\tif ur, err := NewURLAccResolver(url); err != nil {\n\t\t\t\t\terr := &configErr{tk, err.Error()}\n\t\t\t\t\t*errors = append(*errors, err)\n\t\t\t\t\treturn\n\t\t\t\t} else {\n\t\t\t\t\to.AccountResolver = ur\n\t\t\t\t}\n\t\t\t}\n\t\tcase map[string]any:\n\t\t\tdel := false\n\t\t\thdel := false\n\t\t\thdel_set := false\n\t\t\tdir := _EMPTY_\n\t\t\tdirType := _EMPTY_\n\t\t\tlimit := int64(0)\n\t\t\tttl := time.Duration(0)\n\t\t\tsync := time.Duration(0)\n\t\t\topts := []DirResOption{}\n\t\t\tvar err error\n\t\t\tif v, ok := v[\"dir\"]; ok {\n\t\t\t\t_, v := unwrapValue(v, &lt)\n\t\t\t\tdir = v.(string)\n\t\t\t}\n\t\t\tif v, ok := v[\"type\"]; ok {\n\t\t\t\t_, v := unwrapValue(v, &lt)\n\t\t\t\tdirType = v.(string)\n\t\t\t}\n\t\t\tif v, ok := v[\"allow_delete\"]; ok {\n\t\t\t\t_, v := unwrapValue(v, &lt)\n\t\t\t\tdel = v.(bool)\n\t\t\t}\n\t\t\tif v, ok := v[\"hard_delete\"]; ok {\n\t\t\t\t_, v := unwrapValue(v, &lt)\n\t\t\t\thdel_set = true\n\t\t\t\thdel = v.(bool)\n\t\t\t}\n\t\t\tif v, ok := v[\"limit\"]; ok {\n\t\t\t\t_, v := unwrapValue(v, &lt)\n\t\t\t\tlimit = v.(int64)\n\t\t\t}\n\t\t\tif v, ok := v[\"ttl\"]; ok {\n\t\t\t\t_, v := unwrapValue(v, &lt)\n\t\t\t\tttl, err = time.ParseDuration(v.(string))\n\t\t\t}\n\t\t\tif v, ok := v[\"interval\"]; err == nil && ok {\n\t\t\t\t_, v := unwrapValue(v, &lt)\n\t\t\t\tsync, err = time.ParseDuration(v.(string))\n\t\t\t}\n\t\t\tif v, ok := v[\"timeout\"]; err == nil && ok {\n\t\t\t\t_, v := unwrapValue(v, &lt)\n\t\t\t\tvar to time.Duration\n\t\t\t\tif to, err = time.ParseDuration(v.(string)); err == nil {\n\t\t\t\t\topts = append(opts, FetchTimeout(to))\n\t\t\t\t}\n\t\t\t}\n\t\t\tif err != nil {\n\t\t\t\t*errors = append(*errors, &configErr{tk, err.Error()})\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\tcheckDir := func() {\n\t\t\t\tif dir == _EMPTY_ {\n\t\t\t\t\t*errors = append(*errors, &configErr{tk, \"dir has no value and needs to point to a directory\"})\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t\tif info, _ := os.Stat(dir); info != nil && (!info.IsDir() || info.Mode().Perm()&(1<<(uint(7))) == 0) {\n\t\t\t\t\t*errors = append(*errors, &configErr{tk, \"dir needs to point to an accessible directory\"})\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tvar res AccountResolver\n\t\t\tswitch strings.ToUpper(dirType) {\n\t\t\tcase \"CACHE\":\n\t\t\t\tcheckDir()\n\t\t\t\tif sync != 0 {\n\t\t\t\t\t*errors = append(*errors, &configErr{tk, \"CACHE does not accept sync\"})\n\t\t\t\t}\n\t\t\t\tif del {\n\t\t\t\t\t*errors = append(*errors, &configErr{tk, \"CACHE does not accept allow_delete\"})\n\t\t\t\t}\n\t\t\t\tif hdel_set {\n\t\t\t\t\t*errors = append(*errors, &configErr{tk, \"CACHE does not accept hard_delete\"})\n\t\t\t\t}\n\t\t\t\tres, err = NewCacheDirAccResolver(dir, limit, ttl, opts...)\n\t\t\tcase \"FULL\":\n\t\t\t\tcheckDir()\n\t\t\t\tif ttl != 0 {\n\t\t\t\t\t*errors = append(*errors, &configErr{tk, \"FULL does not accept ttl\"})\n\t\t\t\t}\n\t\t\t\tif hdel_set && !del {\n\t\t\t\t\t*errors = append(*errors, &configErr{tk, \"hard_delete has no effect without delete\"})\n\t\t\t\t}\n\t\t\t\tdelete := NoDelete\n\t\t\t\tif del {\n\t\t\t\t\tif hdel {\n\t\t\t\t\t\tdelete = HardDelete\n\t\t\t\t\t} else {\n\t\t\t\t\t\tdelete = RenameDeleted\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tres, err = NewDirAccResolver(dir, limit, sync, delete, opts...)\n\t\t\tcase \"MEM\", \"MEMORY\":\n\t\t\t\tres = &MemAccResolver{}\n\t\t\t}\n\t\t\tif err != nil {\n\t\t\t\t*errors = append(*errors, &configErr{tk, err.Error()})\n\t\t\t\treturn\n\t\t\t}\n\t\t\to.AccountResolver = res\n\t\tdefault:\n\t\t\terr := &configErr{tk, fmt.Sprintf(\"error parsing operator resolver, wrong type %T\", v)}\n\t\t\t*errors = append(*errors, err)\n\t\t\treturn\n\t\t}\n\t\tif o.AccountResolver == nil {\n\t\t\terr := &configErr{tk, \"error parsing account resolver, should be MEM or \" +\n\t\t\t\t\" URL(\\\"url\\\") or a map containing dir and type state=[FULL|CACHE])\"}\n\t\t\t*errors = append(*errors, err)\n\t\t}\n\tcase \"resolver_tls\":\n\t\ttc, err := parseTLS(tk, true)\n\t\tif err != nil {\n\t\t\t*errors = append(*errors, err)\n\t\t\treturn\n\t\t}\n\t\ttlsConfig, err := GenTLSConfig(tc)\n\t\tif err != nil {\n\t\t\terr := &configErr{tk, err.Error()}\n\t\t\t*errors = append(*errors, err)\n\t\t\treturn\n\t\t}\n\t\to.AccountResolverTLSConfig = tlsConfig\n\t\t// GenTLSConfig loads the CA file into ClientCAs, but since this will\n\t\t// be used as a client connection, we need to set RootCAs.\n\t\to.AccountResolverTLSConfig.RootCAs = tlsConfig.ClientCAs\n\tcase \"resolver_preload\":\n\t\tmp, ok := v.(map[string]any)\n\t\tif !ok {\n\t\t\terr := &configErr{tk, \"preload should be a map of account_public_key:account_jwt\"}\n\t\t\t*errors = append(*errors, err)\n\t\t\treturn\n\t\t}\n\t\to.resolverPreloads = make(map[string]string)\n\t\tfor key, val := range mp {\n\t\t\ttk, val = unwrapValue(val, &lt)\n\t\t\tif jwtstr, ok := val.(string); !ok {\n\t\t\t\t*errors = append(*errors, &configErr{tk, \"preload map value should be a string JWT\"})\n\t\t\t\tcontinue\n\t\t\t} else {\n\t\t\t\t// Make sure this is a valid account JWT, that is a config error.\n\t\t\t\t// We will warn of expirations, etc later.\n\t\t\t\tif _, err := jwt.DecodeAccountClaims(jwtstr); err != nil {\n\t\t\t\t\terr := &configErr{tk, \"invalid account JWT\"}\n\t\t\t\t\t*errors = append(*errors, err)\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\to.resolverPreloads[key] = jwtstr\n\t\t\t}\n\t\t}\n\tcase \"resolver_pinned_accounts\":\n\t\tswitch v := v.(type) {\n\t\tcase string:\n\t\t\to.resolverPinnedAccounts = map[string]struct{}{v: {}}\n\t\tcase []string:\n\t\t\to.resolverPinnedAccounts = make(map[string]struct{})\n\t\t\tfor _, mv := range v {\n\t\t\t\to.resolverPinnedAccounts[mv] = struct{}{}\n\t\t\t}\n\t\tcase []any:\n\t\t\to.resolverPinnedAccounts = make(map[string]struct{})\n\t\t\tfor _, mv := range v {\n\t\t\t\ttk, mv = unwrapValue(mv, &lt)\n\t\t\t\tif key, ok := mv.(string); ok {\n\t\t\t\t\to.resolverPinnedAccounts[key] = struct{}{}\n\t\t\t\t} else {\n\t\t\t\t\terr := &configErr{tk,\n\t\t\t\t\t\tfmt.Sprintf(\"error parsing resolver_pinned_accounts: unsupported type in array %T\", mv)}\n\t\t\t\t\t*errors = append(*errors, err)\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t}\n\t\tdefault:\n\t\t\terr := &configErr{tk, fmt.Sprintf(\"error parsing resolver_pinned_accounts: unsupported type %T\", v)}\n\t\t\t*errors = append(*errors, err)\n\t\t\treturn\n\t\t}\n\tcase \"no_auth_user\":\n\t\to.NoAuthUser = v.(string)\n\tcase \"system_account\", \"system\":\n\t\t// Already processed at the beginning so we just skip them\n\t\t// to not treat them as unknown values.\n\t\treturn\n\tcase \"no_system_account\", \"no_system\", \"no_sys_acc\":\n\t\to.NoSystemAccount = v.(bool)\n\tcase \"no_header_support\":\n\t\to.NoHeaderSupport = v.(bool)\n\tcase \"trusted\", \"trusted_keys\":\n\t\tswitch v := v.(type) {\n\t\tcase string:\n\t\t\to.TrustedKeys = []string{v}\n\t\tcase []string:\n\t\t\to.TrustedKeys = v\n\t\tcase []any:\n\t\t\tkeys := make([]string, 0, len(v))\n\t\t\tfor _, mv := range v {\n\t\t\t\ttk, mv = unwrapValue(mv, &lt)\n\t\t\t\tif key, ok := mv.(string); ok {\n\t\t\t\t\tkeys = append(keys, key)\n\t\t\t\t} else {\n\t\t\t\t\terr := &configErr{tk, fmt.Sprintf(\"error parsing trusted: unsupported type in array %T\", mv)}\n\t\t\t\t\t*errors = append(*errors, err)\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t}\n\t\t\to.TrustedKeys = keys\n\t\tdefault:\n\t\t\terr := &configErr{tk, fmt.Sprintf(\"error parsing trusted: unsupported type %T\", v)}\n\t\t\t*errors = append(*errors, err)\n\t\t}\n\t\t// Do a quick sanity check on keys\n\t\tfor _, key := range o.TrustedKeys {\n\t\t\tif !nkeys.IsValidPublicOperatorKey(key) {\n\t\t\t\terr := &configErr{tk, fmt.Sprintf(\"trust key %q required to be a valid public operator nkey\", key)}\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t}\n\t\t}\n\tcase \"connect_error_reports\":\n\t\to.ConnectErrorReports = int(v.(int64))\n\tcase \"reconnect_error_reports\":\n\t\to.ReconnectErrorReports = int(v.(int64))\n\tcase \"websocket\", \"ws\":\n\t\tif err := parseWebsocket(tk, o, errors); err != nil {\n\t\t\t*errors = append(*errors, err)\n\t\t\treturn\n\t\t}\n\tcase \"mqtt\":\n\t\tif err := parseMQTT(tk, o, errors, warnings); err != nil {\n\t\t\t*errors = append(*errors, err)\n\t\t\treturn\n\t\t}\n\tcase \"server_tags\":\n\t\tvar err error\n\t\tswitch v := v.(type) {\n\t\tcase string:\n\t\t\to.Tags.Add(v)\n\t\tcase []string:\n\t\t\to.Tags.Add(v...)\n\t\tcase []any:\n\t\t\tfor _, t := range v {\n\t\t\t\tif token, ok := t.(token); ok {\n\t\t\t\t\tif ts, ok := token.Value().(string); ok {\n\t\t\t\t\t\to.Tags.Add(ts)\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t} else {\n\t\t\t\t\t\terr = &configErr{tk, fmt.Sprintf(\"error parsing tags: unsupported type %T where string is expected\", token)}\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\terr = &configErr{tk, fmt.Sprintf(\"error parsing tags: unsupported type %T\", t)}\n\t\t\t\t}\n\t\t\t\tbreak\n\t\t\t}\n\t\tdefault:\n\t\t\terr = &configErr{tk, fmt.Sprintf(\"error parsing tags: unsupported type %T\", v)}\n\t\t}\n\t\tif err != nil {\n\t\t\t*errors = append(*errors, err)\n\t\t\treturn\n\t\t}\n\tcase \"default_js_domain\":\n\t\tvv, ok := v.(map[string]any)\n\t\tif !ok {\n\t\t\t*errors = append(*errors, &configErr{tk, fmt.Sprintf(\"error default_js_domain config: unsupported type %T\", v)})\n\t\t\treturn\n\t\t}\n\t\tm := make(map[string]string)\n\t\tfor kk, kv := range vv {\n\t\t\t_, v = unwrapValue(kv, &tk)\n\t\t\tm[kk] = v.(string)\n\t\t}\n\t\to.JsAccDefaultDomain = m\n\tcase \"ocsp_cache\":\n\t\tvar err error\n\t\tswitch vv := v.(type) {\n\t\tcase bool:\n\t\t\tpc := NewOCSPResponseCacheConfig()\n\t\t\tif vv {\n\t\t\t\t// Set enabled\n\t\t\t\tpc.Type = LOCAL\n\t\t\t\to.OCSPCacheConfig = pc\n\t\t\t} else {\n\t\t\t\t// Set disabled (none cache)\n\t\t\t\tpc.Type = NONE\n\t\t\t\to.OCSPCacheConfig = pc\n\t\t\t}\n\t\tcase map[string]any:\n\t\t\tpc, err := parseOCSPResponseCache(v)\n\t\t\tif err != nil {\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t\treturn\n\t\t\t}\n\t\t\to.OCSPCacheConfig = pc\n\t\tdefault:\n\t\t\terr = &configErr{tk, fmt.Sprintf(\"error parsing tags: unsupported type %T\", v)}\n\t\t}\n\t\tif err != nil {\n\t\t\t*errors = append(*errors, err)\n\t\t\treturn\n\t\t}\n\tcase \"no_fast_producer_stall\":\n\t\to.NoFastProducerStall = v.(bool)\n\tcase \"max_closed_clients\":\n\t\to.MaxClosedClients = int(v.(int64))\n\tdefault:\n\t\tif au := atomic.LoadInt32(&allowUnknownTopLevelField); au == 0 && !tk.IsUsedVariable() {\n\t\t\terr := &unknownConfigFieldErr{\n\t\t\t\tfield: k,\n\t\t\t\tconfigErr: configErr{\n\t\t\t\t\ttoken: tk,\n\t\t\t\t},\n\t\t\t}\n\t\t\t*errors = append(*errors, err)\n\t\t}\n\t}\n}\n\nfunc setupUsersAndNKeysDuplicateCheckMap(o *Options) map[string]struct{} {\n\tunames := make(map[string]struct{}, len(o.Users)+len(o.Nkeys))\n\tfor _, u := range o.Users {\n\t\tunames[u.Username] = struct{}{}\n\t}\n\tfor _, u := range o.Nkeys {\n\t\tunames[u.Nkey] = struct{}{}\n\t}\n\treturn unames\n}\n\nfunc parseDuration(field string, tk token, v any, errors *[]error, warnings *[]error) time.Duration {\n\tif wd, ok := v.(string); ok {\n\t\tif dur, err := time.ParseDuration(wd); err != nil {\n\t\t\terr := &configErr{tk, fmt.Sprintf(\"error parsing %s: %v\", field, err)}\n\t\t\t*errors = append(*errors, err)\n\t\t\treturn 0\n\t\t} else {\n\t\t\treturn dur\n\t\t}\n\t} else {\n\t\t// Backward compatible with old type, assume this is the\n\t\t// number of seconds.\n\t\terr := &configWarningErr{\n\t\t\tfield: field,\n\t\t\tconfigErr: configErr{\n\t\t\t\ttoken:  tk,\n\t\t\t\treason: field + \" should be converted to a duration\",\n\t\t\t},\n\t\t}\n\t\t*warnings = append(*warnings, err)\n\t\treturn time.Duration(v.(int64)) * time.Second\n\t}\n}\n\nfunc trackExplicitVal(pm *map[string]bool, name string, val bool) {\n\tm := *pm\n\tif m == nil {\n\t\tm = make(map[string]bool)\n\t\t*pm = m\n\t}\n\tm[name] = val\n}\n\n// hostPort is simple struct to hold parsed listen/addr strings.\ntype hostPort struct {\n\thost string\n\tport int\n}\n\n// parseListen will parse listen option which is replacing host/net and port\nfunc parseListen(v any) (*hostPort, error) {\n\thp := &hostPort{}\n\tswitch vv := v.(type) {\n\t// Only a port\n\tcase int64:\n\t\thp.port = int(vv)\n\tcase string:\n\t\thost, port, err := net.SplitHostPort(vv)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"could not parse address string %q\", vv)\n\t\t}\n\t\thp.port, err = strconv.Atoi(port)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"could not parse port %q\", port)\n\t\t}\n\t\thp.host = host\n\tdefault:\n\t\treturn nil, fmt.Errorf(\"expected port or host:port, got %T\", vv)\n\t}\n\treturn hp, nil\n}\n\n// parseCluster will parse the cluster config.\nfunc parseCluster(v any, opts *Options, errors *[]error, warnings *[]error) error {\n\tvar lt token\n\tdefer convertPanicToErrorList(&lt, errors)\n\n\ttk, v := unwrapValue(v, &lt)\n\tcm, ok := v.(map[string]any)\n\tif !ok {\n\t\treturn &configErr{tk, fmt.Sprintf(\"Expected map to define cluster, got %T\", v)}\n\t}\n\n\tfor mk, mv := range cm {\n\t\t// Again, unwrap token value if line check is required.\n\t\ttk, mv = unwrapValue(mv, &lt)\n\t\tswitch strings.ToLower(mk) {\n\t\tcase \"name\":\n\t\t\tcn := mv.(string)\n\t\t\tif strings.Contains(cn, \" \") {\n\t\t\t\terr := &configErr{tk, ErrClusterNameHasSpaces.Error()}\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\topts.Cluster.Name = cn\n\t\tcase \"listen\":\n\t\t\thp, err := parseListen(mv)\n\t\t\tif err != nil {\n\t\t\t\terr := &configErr{tk, err.Error()}\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\topts.Cluster.Host = hp.host\n\t\t\topts.Cluster.Port = hp.port\n\t\tcase \"port\":\n\t\t\topts.Cluster.Port = int(mv.(int64))\n\t\tcase \"host\", \"net\":\n\t\t\topts.Cluster.Host = mv.(string)\n\t\tcase \"authorization\":\n\t\t\tauth, err := parseAuthorization(tk, errors, warnings)\n\t\t\tif err != nil {\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif auth.users != nil {\n\t\t\t\terr := &configErr{tk, \"Cluster authorization does not allow multiple users\"}\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif auth.token != _EMPTY_ {\n\t\t\t\terr := &configErr{tk, \"Cluster authorization does not support tokens\"}\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif auth.callout != nil {\n\t\t\t\terr := &configErr{tk, \"Cluster authorization does not support callouts\"}\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\topts.Cluster.Username = auth.user\n\t\t\topts.Cluster.Password = auth.pass\n\t\t\topts.Cluster.AuthTimeout = auth.timeout\n\n\t\t\tif auth.defaultPermissions != nil {\n\t\t\t\terr := &configWarningErr{\n\t\t\t\t\tfield: mk,\n\t\t\t\t\tconfigErr: configErr{\n\t\t\t\t\t\ttoken:  tk,\n\t\t\t\t\t\treason: `setting \"permissions\" within cluster authorization block is deprecated`,\n\t\t\t\t\t},\n\t\t\t\t}\n\t\t\t\t*warnings = append(*warnings, err)\n\n\t\t\t\t// Do not set permissions if they were specified in top-level cluster block.\n\t\t\t\tif opts.Cluster.Permissions == nil {\n\t\t\t\t\tsetClusterPermissions(&opts.Cluster, auth.defaultPermissions)\n\t\t\t\t}\n\t\t\t}\n\t\tcase \"routes\":\n\t\t\tra := mv.([]any)\n\t\t\troutes, errs := parseURLs(ra, \"route\", warnings)\n\t\t\tif errs != nil {\n\t\t\t\t*errors = append(*errors, errs...)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\topts.Routes = routes\n\t\tcase \"tls\":\n\t\t\tconfig, tlsopts, err := getTLSConfig(tk)\n\t\t\tif err != nil {\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\topts.Cluster.TLSConfig = config\n\t\t\topts.Cluster.TLSTimeout = tlsopts.Timeout\n\t\t\topts.Cluster.TLSMap = tlsopts.Map\n\t\t\topts.Cluster.TLSPinnedCerts = tlsopts.PinnedCerts\n\t\t\topts.Cluster.TLSCheckKnownURLs = tlsopts.TLSCheckKnownURLs\n\t\t\topts.Cluster.tlsConfigOpts = tlsopts\n\t\tcase \"cluster_advertise\", \"advertise\":\n\t\t\topts.Cluster.Advertise = mv.(string)\n\t\tcase \"no_advertise\":\n\t\t\topts.Cluster.NoAdvertise = mv.(bool)\n\t\t\ttrackExplicitVal(&opts.inConfig, \"Cluster.NoAdvertise\", opts.Cluster.NoAdvertise)\n\t\tcase \"connect_retries\":\n\t\t\topts.Cluster.ConnectRetries = int(mv.(int64))\n\t\tcase \"permissions\":\n\t\t\tperms, err := parseUserPermissions(mv, errors)\n\t\t\tif err != nil {\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\t// Dynamic response permissions do not make sense here.\n\t\t\tif perms.Response != nil {\n\t\t\t\terr := &configErr{tk, \"Cluster permissions do not support dynamic responses\"}\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\t// This will possibly override permissions that were define in auth block\n\t\t\tsetClusterPermissions(&opts.Cluster, perms)\n\t\tcase \"pool_size\":\n\t\t\topts.Cluster.PoolSize = int(mv.(int64))\n\t\tcase \"accounts\":\n\t\t\topts.Cluster.PinnedAccounts, _ = parseStringArray(\"accounts\", tk, &lt, mv, errors)\n\t\tcase \"compression\":\n\t\t\tif err := parseCompression(&opts.Cluster.Compression, CompressionS2Fast, tk, mk, mv); err != nil {\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\tcase \"ping_interval\":\n\t\t\topts.Cluster.PingInterval = parseDuration(\"ping_interval\", tk, mv, errors, warnings)\n\t\t\tif opts.Cluster.PingInterval > routeMaxPingInterval {\n\t\t\t\t*warnings = append(*warnings, &configErr{tk, fmt.Sprintf(\"Cluster 'ping_interval' will reset to %v which is the max for routes\", routeMaxPingInterval)})\n\t\t\t}\n\t\tcase \"ping_max\":\n\t\t\topts.Cluster.MaxPingsOut = int(mv.(int64))\n\t\tdefault:\n\t\t\tif !tk.IsUsedVariable() {\n\t\t\t\terr := &unknownConfigFieldErr{\n\t\t\t\t\tfield: mk,\n\t\t\t\t\tconfigErr: configErr{\n\t\t\t\t\t\ttoken: tk,\n\t\t\t\t\t},\n\t\t\t\t}\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t}\n\t}\n\treturn nil\n}\n\n// The parameter `chosenModeForOn` indicates which compression mode to use\n// when the user selects \"on\" (or enabled, true, etc..). This is because\n// we may have different defaults depending on where the compression is used.\nfunc parseCompression(c *CompressionOpts, chosenModeForOn string, tk token, mk string, mv any) (retErr error) {\n\tvar lt token\n\tdefer convertPanicToError(&lt, &retErr)\n\n\tswitch mv := mv.(type) {\n\tcase string:\n\t\t// Do not validate here, it will be done in NewServer.\n\t\tc.Mode = mv\n\tcase bool:\n\t\tif mv {\n\t\t\tc.Mode = chosenModeForOn\n\t\t} else {\n\t\t\tc.Mode = CompressionOff\n\t\t}\n\tcase map[string]any:\n\t\tfor mk, mv := range mv {\n\t\t\ttk, mv = unwrapValue(mv, &lt)\n\t\t\tswitch strings.ToLower(mk) {\n\t\t\tcase \"mode\":\n\t\t\t\tc.Mode = mv.(string)\n\t\t\tcase \"rtt_thresholds\", \"thresholds\", \"rtts\", \"rtt\":\n\t\t\t\tfor _, iv := range mv.([]any) {\n\t\t\t\t\t_, mv := unwrapValue(iv, &lt)\n\t\t\t\t\tdur, err := time.ParseDuration(mv.(string))\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn &configErr{tk, err.Error()}\n\t\t\t\t\t}\n\t\t\t\t\tc.RTTThresholds = append(c.RTTThresholds, dur)\n\t\t\t\t}\n\t\t\tdefault:\n\t\t\t\tif !tk.IsUsedVariable() {\n\t\t\t\t\treturn &configErr{tk, fmt.Sprintf(\"unknown field %q\", mk)}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\tdefault:\n\t\treturn &configErr{tk, fmt.Sprintf(\"field %q should be a boolean or a structure, got %T\", mk, mv)}\n\t}\n\treturn nil\n}\n\nfunc parseURLs(a []any, typ string, warnings *[]error) (urls []*url.URL, errors []error) {\n\turls = make([]*url.URL, 0, len(a))\n\tvar lt token\n\tdefer convertPanicToErrorList(&lt, &errors)\n\n\tdd := make(map[string]bool)\n\n\tfor _, u := range a {\n\t\ttk, u := unwrapValue(u, &lt)\n\t\tsURL := u.(string)\n\t\tif dd[sURL] {\n\t\t\terr := &configWarningErr{\n\t\t\t\tfield: sURL,\n\t\t\t\tconfigErr: configErr{\n\t\t\t\t\ttoken:  tk,\n\t\t\t\t\treason: fmt.Sprintf(\"Duplicate %s entry detected\", typ),\n\t\t\t\t},\n\t\t\t}\n\t\t\t*warnings = append(*warnings, err)\n\t\t\tcontinue\n\t\t}\n\t\tdd[sURL] = true\n\t\turl, err := parseURL(sURL, typ)\n\t\tif err != nil {\n\t\t\terr := &configErr{tk, err.Error()}\n\t\t\terrors = append(errors, err)\n\t\t\tcontinue\n\t\t}\n\t\turls = append(urls, url)\n\t}\n\treturn urls, errors\n}\n\nfunc parseURL(u string, typ string) (*url.URL, error) {\n\turlStr := strings.TrimSpace(u)\n\turl, err := url.Parse(urlStr)\n\tif err != nil {\n\t\t// Security note: if it's not well-formed but still reached us, then we're going to log as-is which might include password information here.\n\t\t// If the URL parses, we don't log the credentials ever, but if it doesn't even parse we don't have a sane way to redact.\n\t\treturn nil, fmt.Errorf(\"error parsing %s url [%q]\", typ, urlStr)\n\t}\n\treturn url, nil\n}\n\nfunc parseGateway(v any, o *Options, errors *[]error, warnings *[]error) error {\n\tvar lt token\n\tdefer convertPanicToErrorList(&lt, errors)\n\n\ttk, v := unwrapValue(v, &lt)\n\tgm, ok := v.(map[string]any)\n\tif !ok {\n\t\treturn &configErr{tk, fmt.Sprintf(\"Expected gateway to be a map, got %T\", v)}\n\t}\n\tfor mk, mv := range gm {\n\t\t// Again, unwrap token value if line check is required.\n\t\ttk, mv = unwrapValue(mv, &lt)\n\t\tswitch strings.ToLower(mk) {\n\t\tcase \"name\":\n\t\t\tgn := mv.(string)\n\t\t\tif strings.Contains(gn, \" \") {\n\t\t\t\terr := &configErr{tk, ErrGatewayNameHasSpaces.Error()}\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\to.Gateway.Name = gn\n\t\tcase \"listen\":\n\t\t\thp, err := parseListen(mv)\n\t\t\tif err != nil {\n\t\t\t\terr := &configErr{tk, err.Error()}\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\to.Gateway.Host = hp.host\n\t\t\to.Gateway.Port = hp.port\n\t\tcase \"port\":\n\t\t\to.Gateway.Port = int(mv.(int64))\n\t\tcase \"host\", \"net\":\n\t\t\to.Gateway.Host = mv.(string)\n\t\tcase \"authorization\":\n\t\t\tauth, err := parseAuthorization(tk, errors, warnings)\n\t\t\tif err != nil {\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif auth.users != nil {\n\t\t\t\t*errors = append(*errors, &configErr{tk, \"Gateway authorization does not allow multiple users\"})\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif auth.token != _EMPTY_ {\n\t\t\t\terr := &configErr{tk, \"Gateway authorization does not support tokens\"}\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif auth.callout != nil {\n\t\t\t\terr := &configErr{tk, \"Gateway authorization does not support callouts\"}\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\to.Gateway.Username = auth.user\n\t\t\to.Gateway.Password = auth.pass\n\t\t\to.Gateway.AuthTimeout = auth.timeout\n\t\tcase \"tls\":\n\t\t\tconfig, tlsopts, err := getTLSConfig(tk)\n\t\t\tif err != nil {\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\to.Gateway.TLSConfig = config\n\t\t\to.Gateway.TLSTimeout = tlsopts.Timeout\n\t\t\to.Gateway.TLSMap = tlsopts.Map\n\t\t\to.Gateway.TLSCheckKnownURLs = tlsopts.TLSCheckKnownURLs\n\t\t\to.Gateway.TLSPinnedCerts = tlsopts.PinnedCerts\n\t\t\to.Gateway.tlsConfigOpts = tlsopts\n\t\tcase \"advertise\":\n\t\t\to.Gateway.Advertise = mv.(string)\n\t\tcase \"connect_retries\":\n\t\t\to.Gateway.ConnectRetries = int(mv.(int64))\n\t\tcase \"gateways\":\n\t\t\tgateways, err := parseGateways(mv, errors, warnings)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\to.Gateway.Gateways = gateways\n\t\tcase \"reject_unknown\", \"reject_unknown_cluster\":\n\t\t\to.Gateway.RejectUnknown = mv.(bool)\n\t\tdefault:\n\t\t\tif !tk.IsUsedVariable() {\n\t\t\t\terr := &unknownConfigFieldErr{\n\t\t\t\t\tfield: mk,\n\t\t\t\t\tconfigErr: configErr{\n\t\t\t\t\t\ttoken: tk,\n\t\t\t\t\t},\n\t\t\t\t}\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t}\n\t}\n\treturn nil\n}\n\nvar dynamicJSAccountLimits = JetStreamAccountLimits{-1, -1, -1, -1, -1, -1, -1, false}\nvar defaultJSAccountTiers = map[string]JetStreamAccountLimits{_EMPTY_: dynamicJSAccountLimits}\n\n// Parses jetstream account limits for an account. Simple setup with boolen is allowed, and we will\n// use dynamic account limits.\nfunc parseJetStreamForAccount(v any, acc *Account, errors *[]error) error {\n\tvar lt token\n\n\ttk, v := unwrapValue(v, &lt)\n\n\t// Value here can be bool, or string \"enabled\" or a map.\n\tswitch vv := v.(type) {\n\tcase bool:\n\t\tif vv {\n\t\t\tacc.jsLimits = defaultJSAccountTiers\n\t\t}\n\tcase string:\n\t\tswitch strings.ToLower(vv) {\n\t\tcase \"enabled\", \"enable\":\n\t\t\tacc.jsLimits = defaultJSAccountTiers\n\t\tcase \"disabled\", \"disable\":\n\t\t\tacc.jsLimits = nil\n\t\tdefault:\n\t\t\treturn &configErr{tk, fmt.Sprintf(\"Expected 'enabled' or 'disabled' for string value, got '%s'\", vv)}\n\t\t}\n\tcase map[string]any:\n\t\tjsLimits := JetStreamAccountLimits{-1, -1, -1, -1, -1, -1, -1, false}\n\t\tfor mk, mv := range vv {\n\t\t\ttk, mv = unwrapValue(mv, &lt)\n\t\t\tswitch strings.ToLower(mk) {\n\t\t\tcase \"max_memory\", \"max_mem\", \"mem\", \"memory\":\n\t\t\t\tvv, ok := mv.(int64)\n\t\t\t\tif !ok {\n\t\t\t\t\treturn &configErr{tk, fmt.Sprintf(\"Expected a parseable size for %q, got %v\", mk, mv)}\n\t\t\t\t}\n\t\t\t\tjsLimits.MaxMemory = vv\n\t\t\tcase \"max_store\", \"max_file\", \"max_disk\", \"store\", \"disk\":\n\t\t\t\tvv, ok := mv.(int64)\n\t\t\t\tif !ok {\n\t\t\t\t\treturn &configErr{tk, fmt.Sprintf(\"Expected a parseable size for %q, got %v\", mk, mv)}\n\t\t\t\t}\n\t\t\t\tjsLimits.MaxStore = vv\n\t\t\tcase \"max_streams\", \"streams\":\n\t\t\t\tvv, ok := mv.(int64)\n\t\t\t\tif !ok {\n\t\t\t\t\treturn &configErr{tk, fmt.Sprintf(\"Expected a parseable size for %q, got %v\", mk, mv)}\n\t\t\t\t}\n\t\t\t\tjsLimits.MaxStreams = int(vv)\n\t\t\tcase \"max_consumers\", \"consumers\":\n\t\t\t\tvv, ok := mv.(int64)\n\t\t\t\tif !ok {\n\t\t\t\t\treturn &configErr{tk, fmt.Sprintf(\"Expected a parseable size for %q, got %v\", mk, mv)}\n\t\t\t\t}\n\t\t\t\tjsLimits.MaxConsumers = int(vv)\n\t\t\tcase \"max_bytes_required\", \"max_stream_bytes\", \"max_bytes\":\n\t\t\t\tvv, ok := mv.(bool)\n\t\t\t\tif !ok {\n\t\t\t\t\treturn &configErr{tk, fmt.Sprintf(\"Expected a parseable bool for %q, got %v\", mk, mv)}\n\t\t\t\t}\n\t\t\t\tjsLimits.MaxBytesRequired = vv\n\t\t\tcase \"mem_max_stream_bytes\", \"memory_max_stream_bytes\":\n\t\t\t\tvv, ok := mv.(int64)\n\t\t\t\tif !ok {\n\t\t\t\t\treturn &configErr{tk, fmt.Sprintf(\"Expected a parseable size for %q, got %v\", mk, mv)}\n\t\t\t\t}\n\t\t\t\tjsLimits.MemoryMaxStreamBytes = vv\n\t\t\tcase \"disk_max_stream_bytes\", \"store_max_stream_bytes\":\n\t\t\t\tvv, ok := mv.(int64)\n\t\t\t\tif !ok {\n\t\t\t\t\treturn &configErr{tk, fmt.Sprintf(\"Expected a parseable size for %q, got %v\", mk, mv)}\n\t\t\t\t}\n\t\t\t\tjsLimits.StoreMaxStreamBytes = vv\n\t\t\tcase \"max_ack_pending\":\n\t\t\t\tvv, ok := mv.(int64)\n\t\t\t\tif !ok {\n\t\t\t\t\treturn &configErr{tk, fmt.Sprintf(\"Expected a parseable size for %q, got %v\", mk, mv)}\n\t\t\t\t}\n\t\t\t\tjsLimits.MaxAckPending = int(vv)\n\t\t\tcase \"cluster_traffic\":\n\t\t\t\tvv, ok := mv.(string)\n\t\t\t\tif !ok {\n\t\t\t\t\treturn &configErr{tk, fmt.Sprintf(\"Expected either 'system' or 'account' string value for %q, got %v\", mk, mv)}\n\t\t\t\t}\n\t\t\t\tswitch vv {\n\t\t\t\tcase \"system\", _EMPTY_:\n\t\t\t\t\tacc.nrgAccount = _EMPTY_\n\t\t\t\tcase \"owner\":\n\t\t\t\t\tacc.nrgAccount = acc.Name\n\t\t\t\tdefault:\n\t\t\t\t\treturn &configErr{tk, fmt.Sprintf(\"Expected 'system' or 'owner' string value for %q, got %v\", mk, mv)}\n\t\t\t\t}\n\t\t\tdefault:\n\t\t\t\tif !tk.IsUsedVariable() {\n\t\t\t\t\terr := &unknownConfigFieldErr{\n\t\t\t\t\t\tfield: mk,\n\t\t\t\t\t\tconfigErr: configErr{\n\t\t\t\t\t\t\ttoken: tk,\n\t\t\t\t\t\t},\n\t\t\t\t\t}\n\t\t\t\t\t*errors = append(*errors, err)\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tacc.jsLimits = map[string]JetStreamAccountLimits{_EMPTY_: jsLimits}\n\tdefault:\n\t\treturn &configErr{tk, fmt.Sprintf(\"Expected map, bool or string to define JetStream, got %T\", v)}\n\t}\n\treturn nil\n}\n\n// takes in a storage size as either an int or a string and returns an int64 value based on the input.\nfunc getStorageSize(v any) (int64, error) {\n\t_, ok := v.(int64)\n\tif ok {\n\t\treturn v.(int64), nil\n\t}\n\n\ts, ok := v.(string)\n\tif !ok {\n\t\treturn 0, fmt.Errorf(\"must be int64 or string\")\n\t}\n\n\tif s == _EMPTY_ {\n\t\treturn 0, nil\n\t}\n\n\tsuffix := s[len(s)-1:]\n\tprefix := s[:len(s)-1]\n\tnum, err := strconv.ParseInt(prefix, 10, 64)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\n\tsuffixMap := map[string]int64{\"K\": 10, \"M\": 20, \"G\": 30, \"T\": 40}\n\n\tmult, ok := suffixMap[suffix]\n\tif !ok {\n\t\treturn 0, fmt.Errorf(\"sizes defined as strings must end in K, M, G, T\")\n\t}\n\tnum *= 1 << mult\n\n\treturn num, nil\n}\n\n// Parse enablement of jetstream for a server.\nfunc parseJetStreamLimits(v any, opts *Options, errors *[]error) error {\n\tvar lt token\n\ttk, v := unwrapValue(v, &lt)\n\n\tlim := JSLimitOpts{}\n\n\tvv, ok := v.(map[string]any)\n\tif !ok {\n\t\treturn &configErr{tk, fmt.Sprintf(\"Expected a map to define JetStreamLimits, got %T\", v)}\n\t}\n\tfor mk, mv := range vv {\n\t\ttk, mv = unwrapValue(mv, &lt)\n\t\tswitch strings.ToLower(mk) {\n\t\tcase \"max_ack_pending\":\n\t\t\tlim.MaxAckPending = int(mv.(int64))\n\t\tcase \"max_ha_assets\":\n\t\t\tlim.MaxHAAssets = int(mv.(int64))\n\t\tcase \"max_request_batch\":\n\t\t\tlim.MaxRequestBatch = int(mv.(int64))\n\t\tcase \"duplicate_window\":\n\t\t\tvar err error\n\t\t\tlim.Duplicates, err = time.ParseDuration(mv.(string))\n\t\t\tif err != nil {\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t}\n\t\tdefault:\n\t\t\tif !tk.IsUsedVariable() {\n\t\t\t\terr := &unknownConfigFieldErr{\n\t\t\t\t\tfield: mk,\n\t\t\t\t\tconfigErr: configErr{\n\t\t\t\t\t\ttoken: tk,\n\t\t\t\t\t},\n\t\t\t\t}\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t}\n\t}\n\topts.JetStreamLimits = lim\n\treturn nil\n}\n\n// Parse the JetStream TPM options.\nfunc parseJetStreamTPM(v interface{}, opts *Options, errors *[]error) error {\n\tvar lt token\n\ttk, v := unwrapValue(v, &lt)\n\n\ttpm := JSTpmOpts{}\n\n\tvv, ok := v.(map[string]interface{})\n\tif !ok {\n\t\treturn &configErr{tk, fmt.Sprintf(\"Expected a map to define JetStreamLimits, got %T\", v)}\n\t}\n\tfor mk, mv := range vv {\n\t\ttk, mv = unwrapValue(mv, &lt)\n\t\tswitch strings.ToLower(mk) {\n\t\tcase \"keys_file\":\n\t\t\ttpm.KeysFile = mv.(string)\n\t\tcase \"encryption_password\":\n\t\t\ttpm.KeyPassword = mv.(string)\n\t\tcase \"srk_password\":\n\t\t\ttpm.SrkPassword = mv.(string)\n\t\tcase \"pcr\":\n\t\t\ttpm.Pcr = int(mv.(int64))\n\t\tcase \"cipher\":\n\t\t\tif err := setJetStreamEkCipher(opts, mv, tk); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\tdefault:\n\t\t\tif !tk.IsUsedVariable() {\n\t\t\t\terr := &unknownConfigFieldErr{\n\t\t\t\t\tfield: mk,\n\t\t\t\t\tconfigErr: configErr{\n\t\t\t\t\t\ttoken: tk,\n\t\t\t\t\t},\n\t\t\t\t}\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t}\n\t}\n\topts.JetStreamTpm = tpm\n\treturn nil\n}\n\nfunc setJetStreamEkCipher(opts *Options, mv interface{}, tk token) error {\n\tswitch strings.ToLower(mv.(string)) {\n\tcase \"chacha\", \"chachapoly\":\n\t\topts.JetStreamCipher = ChaCha\n\tcase \"aes\":\n\t\topts.JetStreamCipher = AES\n\tdefault:\n\t\treturn &configErr{tk, fmt.Sprintf(\"Unknown cipher type: %q\", mv)}\n\t}\n\treturn nil\n}\n\n// Parse enablement of jetstream for a server.\nfunc parseJetStream(v any, opts *Options, errors *[]error, warnings *[]error) error {\n\tvar lt token\n\n\ttk, v := unwrapValue(v, &lt)\n\n\t// Value here can be bool, or string \"enabled\" or a map.\n\tswitch vv := v.(type) {\n\tcase bool:\n\t\topts.JetStream = v.(bool)\n\tcase string:\n\t\tswitch strings.ToLower(vv) {\n\t\tcase \"enabled\", \"enable\":\n\t\t\topts.JetStream = true\n\t\tcase \"disabled\", \"disable\":\n\t\t\topts.JetStream = false\n\t\tdefault:\n\t\t\treturn &configErr{tk, fmt.Sprintf(\"Expected 'enabled' or 'disabled' for string value, got '%s'\", vv)}\n\t\t}\n\tcase map[string]any:\n\t\tdoEnable := true\n\t\tfor mk, mv := range vv {\n\t\t\ttk, mv = unwrapValue(mv, &lt)\n\t\t\tswitch strings.ToLower(mk) {\n\t\t\tcase \"strict\":\n\t\t\t\tif v, ok := mv.(bool); ok {\n\t\t\t\t\topts.JetStreamStrict = v\n\t\t\t\t} else {\n\t\t\t\t\treturn &configErr{tk, fmt.Sprintf(\"Expected 'true' or 'false' for bool value, got '%s'\", mv)}\n\t\t\t\t}\n\t\t\tcase \"store\", \"store_dir\", \"storedir\":\n\t\t\t\t// StoreDir can be set at the top level as well so have to prevent ambiguous declarations.\n\t\t\t\tif opts.StoreDir != _EMPTY_ {\n\t\t\t\t\treturn &configErr{tk, \"Duplicate 'store_dir' configuration\"}\n\t\t\t\t}\n\t\t\t\topts.StoreDir = mv.(string)\n\t\t\tcase \"sync\", \"sync_interval\":\n\t\t\t\tif v, ok := mv.(string); ok && strings.ToLower(v) == \"always\" {\n\t\t\t\t\topts.SyncInterval = defaultSyncInterval\n\t\t\t\t\topts.SyncAlways = true\n\t\t\t\t} else {\n\t\t\t\t\topts.SyncInterval = parseDuration(mk, tk, mv, errors, warnings)\n\t\t\t\t}\n\t\t\t\topts.syncSet = true\n\t\t\tcase \"max_memory_store\", \"max_mem_store\", \"max_mem\":\n\t\t\t\ts, err := getStorageSize(mv)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn &configErr{tk, fmt.Sprintf(\"max_mem_store %s\", err)}\n\t\t\t\t}\n\t\t\t\topts.JetStreamMaxMemory = s\n\t\t\t\topts.maxMemSet = true\n\t\t\tcase \"max_file_store\", \"max_file\":\n\t\t\t\ts, err := getStorageSize(mv)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn &configErr{tk, fmt.Sprintf(\"max_file_store %s\", err)}\n\t\t\t\t}\n\t\t\t\topts.JetStreamMaxStore = s\n\t\t\t\topts.maxStoreSet = true\n\t\t\tcase \"domain\":\n\t\t\t\topts.JetStreamDomain = mv.(string)\n\t\t\tcase \"enable\", \"enabled\":\n\t\t\t\tdoEnable = mv.(bool)\n\t\t\tcase \"key\", \"ek\", \"encryption_key\":\n\t\t\t\topts.JetStreamKey = mv.(string)\n\t\t\tcase \"prev_key\", \"prev_ek\", \"prev_encryption_key\":\n\t\t\t\topts.JetStreamOldKey = mv.(string)\n\t\t\tcase \"cipher\":\n\t\t\t\tif err := setJetStreamEkCipher(opts, mv, tk); err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\tcase \"extension_hint\":\n\t\t\t\topts.JetStreamExtHint = mv.(string)\n\t\t\tcase \"limits\":\n\t\t\t\tif err := parseJetStreamLimits(tk, opts, errors); err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\tcase \"tpm\":\n\t\t\t\tif err := parseJetStreamTPM(tk, opts, errors); err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\tcase \"unique_tag\":\n\t\t\t\topts.JetStreamUniqueTag = strings.ToLower(strings.TrimSpace(mv.(string)))\n\t\t\tcase \"max_outstanding_catchup\":\n\t\t\t\ts, err := getStorageSize(mv)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn &configErr{tk, fmt.Sprintf(\"%s %s\", strings.ToLower(mk), err)}\n\t\t\t\t}\n\t\t\t\topts.JetStreamMaxCatchup = s\n\t\t\tcase \"max_buffered_size\":\n\t\t\t\ts, err := getStorageSize(mv)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn &configErr{tk, fmt.Sprintf(\"%s %s\", strings.ToLower(mk), err)}\n\t\t\t\t}\n\t\t\t\topts.StreamMaxBufferedSize = s\n\t\t\tcase \"max_buffered_msgs\":\n\t\t\t\tmlen, ok := mv.(int64)\n\t\t\t\tif !ok {\n\t\t\t\t\treturn &configErr{tk, fmt.Sprintf(\"Expected a parseable size for %q, got %v\", mk, mv)}\n\t\t\t\t}\n\t\t\t\topts.StreamMaxBufferedMsgs = int(mlen)\n\t\t\tcase \"request_queue_limit\":\n\t\t\t\tlim, ok := mv.(int64)\n\t\t\t\tif !ok {\n\t\t\t\t\treturn &configErr{tk, fmt.Sprintf(\"Expected a parseable size for %q, got %v\", mk, mv)}\n\t\t\t\t}\n\t\t\t\topts.JetStreamRequestQueueLimit = lim\n\t\t\tdefault:\n\t\t\t\tif !tk.IsUsedVariable() {\n\t\t\t\t\terr := &unknownConfigFieldErr{\n\t\t\t\t\t\tfield: mk,\n\t\t\t\t\t\tconfigErr: configErr{\n\t\t\t\t\t\t\ttoken: tk,\n\t\t\t\t\t\t},\n\t\t\t\t\t}\n\t\t\t\t\t*errors = append(*errors, err)\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\topts.JetStream = doEnable\n\tdefault:\n\t\treturn &configErr{tk, fmt.Sprintf(\"Expected map, bool or string to define JetStream, got %T\", v)}\n\t}\n\n\treturn nil\n}\n\n// parseLeafNodes will parse the leaf node config.\nfunc parseLeafNodes(v any, opts *Options, errors *[]error, warnings *[]error) error {\n\tvar lt token\n\tdefer convertPanicToErrorList(&lt, errors)\n\n\ttk, v := unwrapValue(v, &lt)\n\tcm, ok := v.(map[string]any)\n\tif !ok {\n\t\treturn &configErr{tk, fmt.Sprintf(\"Expected map to define a leafnode, got %T\", v)}\n\t}\n\n\tfor mk, mv := range cm {\n\t\t// Again, unwrap token value if line check is required.\n\t\ttk, mv = unwrapValue(mv, &lt)\n\t\tswitch strings.ToLower(mk) {\n\t\tcase \"listen\":\n\t\t\thp, err := parseListen(mv)\n\t\t\tif err != nil {\n\t\t\t\terr := &configErr{tk, err.Error()}\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\topts.LeafNode.Host = hp.host\n\t\t\topts.LeafNode.Port = hp.port\n\t\tcase \"port\":\n\t\t\topts.LeafNode.Port = int(mv.(int64))\n\t\tcase \"host\", \"net\":\n\t\t\topts.LeafNode.Host = mv.(string)\n\t\tcase \"authorization\":\n\t\t\tauth, err := parseLeafAuthorization(tk, errors, warnings)\n\t\t\tif err != nil {\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\topts.LeafNode.Username = auth.user\n\t\t\topts.LeafNode.Password = auth.pass\n\t\t\topts.LeafNode.AuthTimeout = auth.timeout\n\t\t\topts.LeafNode.Account = auth.acc\n\t\t\topts.LeafNode.Users = auth.users\n\t\t\topts.LeafNode.Nkey = auth.nkey\n\t\t\t// Validate user info config for leafnode authorization\n\t\t\tif err := validateLeafNodeAuthOptions(opts); err != nil {\n\t\t\t\t*errors = append(*errors, &configErr{tk, err.Error()})\n\t\t\t\tcontinue\n\t\t\t}\n\t\tcase \"remotes\":\n\t\t\t// Parse the remote options here.\n\t\t\tremotes, err := parseRemoteLeafNodes(tk, errors, warnings)\n\t\t\tif err != nil {\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\topts.LeafNode.Remotes = remotes\n\t\tcase \"reconnect\", \"reconnect_delay\", \"reconnect_interval\":\n\t\t\topts.LeafNode.ReconnectInterval = parseDuration(\"reconnect\", tk, mv, errors, warnings)\n\t\tcase \"tls\":\n\t\t\ttc, err := parseTLS(tk, true)\n\t\t\tif err != nil {\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif opts.LeafNode.TLSConfig, err = GenTLSConfig(tc); err != nil {\n\t\t\t\terr := &configErr{tk, err.Error()}\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\topts.LeafNode.TLSTimeout = tc.Timeout\n\t\t\topts.LeafNode.TLSMap = tc.Map\n\t\t\topts.LeafNode.TLSPinnedCerts = tc.PinnedCerts\n\t\t\topts.LeafNode.TLSHandshakeFirst = tc.HandshakeFirst\n\t\t\topts.LeafNode.TLSHandshakeFirstFallback = tc.FallbackDelay\n\t\t\topts.LeafNode.tlsConfigOpts = tc\n\t\tcase \"leafnode_advertise\", \"advertise\":\n\t\t\topts.LeafNode.Advertise = mv.(string)\n\t\tcase \"no_advertise\":\n\t\t\topts.LeafNode.NoAdvertise = mv.(bool)\n\t\t\ttrackExplicitVal(&opts.inConfig, \"LeafNode.NoAdvertise\", opts.LeafNode.NoAdvertise)\n\t\tcase \"min_version\", \"minimum_version\":\n\t\t\tversion := mv.(string)\n\t\t\tif err := checkLeafMinVersionConfig(version); err != nil {\n\t\t\t\terr = &configErr{tk, err.Error()}\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\topts.LeafNode.MinVersion = version\n\t\tcase \"compression\":\n\t\t\tif err := parseCompression(&opts.LeafNode.Compression, CompressionS2Auto, tk, mk, mv); err != nil {\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\tdefault:\n\t\t\tif !tk.IsUsedVariable() {\n\t\t\t\terr := &unknownConfigFieldErr{\n\t\t\t\t\tfield: mk,\n\t\t\t\t\tconfigErr: configErr{\n\t\t\t\t\t\ttoken: tk,\n\t\t\t\t\t},\n\t\t\t\t}\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t}\n\t}\n\treturn nil\n}\n\n// This is the authorization parser adapter for the leafnode's\n// authorization config.\nfunc parseLeafAuthorization(v any, errors, warnings *[]error) (*authorization, error) {\n\tvar (\n\t\tam   map[string]any\n\t\ttk   token\n\t\tlt   token\n\t\tauth = &authorization{}\n\t)\n\tdefer convertPanicToErrorList(&lt, errors)\n\n\t_, v = unwrapValue(v, &lt)\n\tam = v.(map[string]any)\n\tfor mk, mv := range am {\n\t\ttk, mv = unwrapValue(mv, &lt)\n\t\tswitch strings.ToLower(mk) {\n\t\tcase \"user\", \"username\":\n\t\t\tauth.user = mv.(string)\n\t\tcase \"pass\", \"password\":\n\t\t\tauth.pass = mv.(string)\n\t\tcase \"nkey\":\n\t\t\tnk := mv.(string)\n\t\t\tif !nkeys.IsValidPublicUserKey(nk) {\n\t\t\t\t*errors = append(*errors, &configErr{tk, \"Not a valid public nkey for leafnode authorization\"})\n\t\t\t}\n\t\t\tauth.nkey = nk\n\t\tcase \"timeout\":\n\t\t\tat := float64(0)\n\t\t\tswitch mv := mv.(type) {\n\t\t\tcase int64:\n\t\t\t\tat = float64(mv)\n\t\t\tcase float64:\n\t\t\t\tat = mv\n\t\t\tcase string:\n\t\t\t\td, err := time.ParseDuration(mv)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn nil, &configErr{tk, fmt.Sprintf(\"error parsing leafnode authorization config, 'timeout' %s\", err)}\n\t\t\t\t}\n\t\t\t\tat = d.Seconds()\n\t\t\tdefault:\n\t\t\t\treturn nil, &configErr{tk, \"error parsing leafnode authorization config, 'timeout' wrong type\"}\n\t\t\t}\n\t\t\tif at > (60 * time.Second).Seconds() {\n\t\t\t\treason := fmt.Sprintf(\"timeout of %v (%f seconds) is high, consider keeping it under 60 seconds. possibly caused by unquoted duration; use '1m' instead of 1m, for example\", mv, at)\n\t\t\t\t*warnings = append(*warnings, &configWarningErr{field: mk, configErr: configErr{token: tk, reason: reason}})\n\t\t\t}\n\t\t\tauth.timeout = at\n\t\tcase \"users\":\n\t\t\tusers, err := parseLeafUsers(tk, errors)\n\t\t\tif err != nil {\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tauth.users = users\n\t\tcase \"account\":\n\t\t\tauth.acc = mv.(string)\n\t\tdefault:\n\t\t\tif !tk.IsUsedVariable() {\n\t\t\t\terr := &unknownConfigFieldErr{\n\t\t\t\t\tfield: mk,\n\t\t\t\t\tconfigErr: configErr{\n\t\t\t\t\t\ttoken: tk,\n\t\t\t\t\t},\n\t\t\t\t}\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\t}\n\treturn auth, nil\n}\n\n// This is a trimmed down version of parseUsers that is adapted\n// for the users possibly defined in the authorization{} section\n// of leafnodes {}.\nfunc parseLeafUsers(mv any, errors *[]error) ([]*User, error) {\n\tvar (\n\t\ttk    token\n\t\tlt    token\n\t\tusers = []*User{}\n\t)\n\tdefer convertPanicToErrorList(&lt, errors)\n\n\ttk, mv = unwrapValue(mv, &lt)\n\t// Make sure we have an array\n\tuv, ok := mv.([]any)\n\tif !ok {\n\t\treturn nil, &configErr{tk, fmt.Sprintf(\"Expected users field to be an array, got %v\", mv)}\n\t}\n\tfor _, u := range uv {\n\t\ttk, u = unwrapValue(u, &lt)\n\t\t// Check its a map/struct\n\t\tum, ok := u.(map[string]any)\n\t\tif !ok {\n\t\t\terr := &configErr{tk, fmt.Sprintf(\"Expected user entry to be a map/struct, got %v\", u)}\n\t\t\t*errors = append(*errors, err)\n\t\t\tcontinue\n\t\t}\n\t\tuser := &User{}\n\t\tfor k, v := range um {\n\t\t\ttk, v = unwrapValue(v, &lt)\n\t\t\tswitch strings.ToLower(k) {\n\t\t\tcase \"user\", \"username\":\n\t\t\t\tuser.Username = v.(string)\n\t\t\tcase \"pass\", \"password\":\n\t\t\t\tuser.Password = v.(string)\n\t\t\tcase \"account\":\n\t\t\t\t// We really want to save just the account name here, but\n\t\t\t\t// the User object is *Account. So we create an account object\n\t\t\t\t// but it won't be registered anywhere. The server will just\n\t\t\t\t// use opts.LeafNode.Users[].Account.Name. Alternatively\n\t\t\t\t// we need to create internal objects to store u/p and account\n\t\t\t\t// name and have a server structure to hold that.\n\t\t\t\tuser.Account = NewAccount(v.(string))\n\t\t\tdefault:\n\t\t\t\tif !tk.IsUsedVariable() {\n\t\t\t\t\terr := &unknownConfigFieldErr{\n\t\t\t\t\t\tfield: k,\n\t\t\t\t\t\tconfigErr: configErr{\n\t\t\t\t\t\t\ttoken: tk,\n\t\t\t\t\t\t},\n\t\t\t\t\t}\n\t\t\t\t\t*errors = append(*errors, err)\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tusers = append(users, user)\n\t}\n\treturn users, nil\n}\n\nfunc parseRemoteLeafNodes(v any, errors *[]error, warnings *[]error) ([]*RemoteLeafOpts, error) {\n\tvar lt token\n\tdefer convertPanicToErrorList(&lt, errors)\n\ttk, v := unwrapValue(v, &lt)\n\tra, ok := v.([]any)\n\tif !ok {\n\t\treturn nil, &configErr{tk, fmt.Sprintf(\"Expected remotes field to be an array, got %T\", v)}\n\t}\n\tremotes := make([]*RemoteLeafOpts, 0, len(ra))\n\tfor _, r := range ra {\n\t\ttk, r = unwrapValue(r, &lt)\n\t\t// Check its a map/struct\n\t\trm, ok := r.(map[string]any)\n\t\tif !ok {\n\t\t\t*errors = append(*errors, &configErr{tk, fmt.Sprintf(\"Expected remote leafnode entry to be a map/struct, got %v\", r)})\n\t\t\tcontinue\n\t\t}\n\t\tremote := &RemoteLeafOpts{}\n\t\tfor k, v := range rm {\n\t\t\ttk, v = unwrapValue(v, &lt)\n\t\t\tswitch strings.ToLower(k) {\n\t\t\tcase \"no_randomize\", \"dont_randomize\":\n\t\t\t\tremote.NoRandomize = v.(bool)\n\t\t\tcase \"url\", \"urls\":\n\t\t\t\tswitch v := v.(type) {\n\t\t\t\tcase []any, []string:\n\t\t\t\t\turls, errs := parseURLs(v.([]any), \"leafnode\", warnings)\n\t\t\t\t\tif errs != nil {\n\t\t\t\t\t\t*errors = append(*errors, errs...)\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\t\t\t\t\tremote.URLs = urls\n\t\t\t\tcase string:\n\t\t\t\t\turl, err := parseURL(v, \"leafnode\")\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\t*errors = append(*errors, &configErr{tk, err.Error()})\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\t\t\t\t\tremote.URLs = append(remote.URLs, url)\n\t\t\t\tdefault:\n\t\t\t\t\t*errors = append(*errors, &configErr{tk, fmt.Sprintf(\"Expected remote leafnode url to be an array or string, got %v\", v)})\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\tcase \"account\", \"local\":\n\t\t\t\tremote.LocalAccount = v.(string)\n\t\t\tcase \"creds\", \"credentials\":\n\t\t\t\tp, err := expandPath(v.(string))\n\t\t\t\tif err != nil {\n\t\t\t\t\t*errors = append(*errors, &configErr{tk, err.Error()})\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\t// Can't have both creds and nkey\n\t\t\t\tif remote.Nkey != _EMPTY_ {\n\t\t\t\t\t*errors = append(*errors, &configErr{tk, \"Remote leafnode can not have both creds and nkey defined\"})\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tremote.Credentials = p\n\t\t\tcase \"nkey\", \"seed\":\n\t\t\t\tnk := v.(string)\n\t\t\t\tif pb, _, err := nkeys.DecodeSeed([]byte(nk)); err != nil || pb != nkeys.PrefixByteUser {\n\t\t\t\t\terr := &configErr{tk, fmt.Sprintf(\"Remote leafnode nkey is not a valid seed: %q\", v)}\n\t\t\t\t\t*errors = append(*errors, err)\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tif remote.Credentials != _EMPTY_ {\n\t\t\t\t\t*errors = append(*errors, &configErr{tk, \"Remote leafnode can not have both creds and nkey defined\"})\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tremote.Nkey = nk\n\t\t\tcase \"tls\":\n\t\t\t\ttc, err := parseTLS(tk, true)\n\t\t\t\tif err != nil {\n\t\t\t\t\t*errors = append(*errors, err)\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tif remote.TLSConfig, err = GenTLSConfig(tc); err != nil {\n\t\t\t\t\t*errors = append(*errors, &configErr{tk, err.Error()})\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\t// If ca_file is defined, GenTLSConfig() sets TLSConfig.ClientCAs.\n\t\t\t\t// Set RootCAs since this tls.Config is used when soliciting\n\t\t\t\t// a connection (therefore behaves as a client).\n\t\t\t\tremote.TLSConfig.RootCAs = remote.TLSConfig.ClientCAs\n\t\t\t\tif tc.Timeout > 0 {\n\t\t\t\t\tremote.TLSTimeout = tc.Timeout\n\t\t\t\t} else {\n\t\t\t\t\tremote.TLSTimeout = float64(DEFAULT_LEAF_TLS_TIMEOUT) / float64(time.Second)\n\t\t\t\t}\n\t\t\t\tremote.TLSHandshakeFirst = tc.HandshakeFirst\n\t\t\t\tremote.tlsConfigOpts = tc\n\t\t\tcase \"hub\":\n\t\t\t\tremote.Hub = v.(bool)\n\t\t\tcase \"deny_imports\", \"deny_import\":\n\t\t\t\tsubjects, err := parsePermSubjects(tk, errors)\n\t\t\t\tif err != nil {\n\t\t\t\t\t*errors = append(*errors, err)\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tremote.DenyImports = subjects\n\t\t\tcase \"deny_exports\", \"deny_export\":\n\t\t\t\tsubjects, err := parsePermSubjects(tk, errors)\n\t\t\t\tif err != nil {\n\t\t\t\t\t*errors = append(*errors, err)\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tremote.DenyExports = subjects\n\t\t\tcase \"ws_compress\", \"ws_compression\", \"websocket_compress\", \"websocket_compression\":\n\t\t\t\tremote.Websocket.Compression = v.(bool)\n\t\t\tcase \"ws_no_masking\", \"websocket_no_masking\":\n\t\t\t\tremote.Websocket.NoMasking = v.(bool)\n\t\t\tcase \"jetstream_cluster_migrate\", \"js_cluster_migrate\":\n\t\t\t\tvar lt token\n\n\t\t\t\ttk, v := unwrapValue(v, &lt)\n\t\t\t\tswitch vv := v.(type) {\n\t\t\t\tcase bool:\n\t\t\t\t\tremote.JetStreamClusterMigrate = vv\n\t\t\t\tcase map[string]any:\n\t\t\t\t\tremote.JetStreamClusterMigrate = true\n\t\t\t\t\tmigrateConfig, ok := v.(map[string]any)\n\t\t\t\t\tif !ok {\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\t\t\t\t\tval, ok := migrateConfig[\"leader_migrate_delay\"]\n\t\t\t\t\ttk, delay := unwrapValue(val, &tk)\n\t\t\t\t\tif ok {\n\t\t\t\t\t\tremote.JetStreamClusterMigrateDelay = parseDuration(\"leader_migrate_delay\", tk, delay, errors, warnings)\n\t\t\t\t\t}\n\t\t\t\tdefault:\n\t\t\t\t\t*errors = append(*errors, &configErr{tk, fmt.Sprintf(\"Expected boolean or map for jetstream_cluster_migrate, got %T\", v)})\n\t\t\t\t}\n\t\t\tcase \"compression\":\n\t\t\t\tif err := parseCompression(&remote.Compression, CompressionS2Auto, tk, k, v); err != nil {\n\t\t\t\t\t*errors = append(*errors, err)\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\tcase \"first_info_timeout\":\n\t\t\t\tremote.FirstInfoTimeout = parseDuration(k, tk, v, errors, warnings)\n\t\t\tdefault:\n\t\t\t\tif !tk.IsUsedVariable() {\n\t\t\t\t\terr := &unknownConfigFieldErr{\n\t\t\t\t\t\tfield: k,\n\t\t\t\t\t\tconfigErr: configErr{\n\t\t\t\t\t\t\ttoken: tk,\n\t\t\t\t\t\t},\n\t\t\t\t\t}\n\t\t\t\t\t*errors = append(*errors, err)\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tremotes = append(remotes, remote)\n\t}\n\treturn remotes, nil\n}\n\n// Parse TLS and returns a TLSConfig and TLSTimeout.\n// Used by cluster and gateway parsing.\nfunc getTLSConfig(tk token) (*tls.Config, *TLSConfigOpts, error) {\n\ttc, err := parseTLS(tk, false)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\tconfig, err := GenTLSConfig(tc)\n\tif err != nil {\n\t\terr := &configErr{tk, err.Error()}\n\t\treturn nil, nil, err\n\t}\n\t// For clusters/gateways, we will force strict verification. We also act\n\t// as both client and server, so will mirror the rootCA to the\n\t// clientCA pool.\n\tconfig.ClientAuth = tls.RequireAndVerifyClientCert\n\tconfig.RootCAs = config.ClientCAs\n\treturn config, tc, nil\n}\n\nfunc parseGateways(v any, errors *[]error, warnings *[]error) ([]*RemoteGatewayOpts, error) {\n\tvar lt token\n\tdefer convertPanicToErrorList(&lt, errors)\n\n\ttk, v := unwrapValue(v, &lt)\n\t// Make sure we have an array\n\tga, ok := v.([]any)\n\tif !ok {\n\t\treturn nil, &configErr{tk, fmt.Sprintf(\"Expected gateways field to be an array, got %T\", v)}\n\t}\n\tgateways := []*RemoteGatewayOpts{}\n\tfor _, g := range ga {\n\t\ttk, g = unwrapValue(g, &lt)\n\t\t// Check its a map/struct\n\t\tgm, ok := g.(map[string]any)\n\t\tif !ok {\n\t\t\t*errors = append(*errors, &configErr{tk, fmt.Sprintf(\"Expected gateway entry to be a map/struct, got %v\", g)})\n\t\t\tcontinue\n\t\t}\n\t\tgateway := &RemoteGatewayOpts{}\n\t\tfor k, v := range gm {\n\t\t\ttk, v = unwrapValue(v, &lt)\n\t\t\tswitch strings.ToLower(k) {\n\t\t\tcase \"name\":\n\t\t\t\tgateway.Name = v.(string)\n\t\t\tcase \"tls\":\n\t\t\t\ttls, tlsopts, err := getTLSConfig(tk)\n\t\t\t\tif err != nil {\n\t\t\t\t\t*errors = append(*errors, err)\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tgateway.TLSConfig = tls\n\t\t\t\tgateway.TLSTimeout = tlsopts.Timeout\n\t\t\t\tgateway.tlsConfigOpts = tlsopts\n\t\t\tcase \"url\":\n\t\t\t\turl, err := parseURL(v.(string), \"gateway\")\n\t\t\t\tif err != nil {\n\t\t\t\t\t*errors = append(*errors, &configErr{tk, err.Error()})\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tgateway.URLs = append(gateway.URLs, url)\n\t\t\tcase \"urls\":\n\t\t\t\turls, errs := parseURLs(v.([]any), \"gateway\", warnings)\n\t\t\t\tif errs != nil {\n\t\t\t\t\t*errors = append(*errors, errs...)\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tgateway.URLs = urls\n\t\t\tdefault:\n\t\t\t\tif !tk.IsUsedVariable() {\n\t\t\t\t\terr := &unknownConfigFieldErr{\n\t\t\t\t\t\tfield: k,\n\t\t\t\t\t\tconfigErr: configErr{\n\t\t\t\t\t\t\ttoken: tk,\n\t\t\t\t\t\t},\n\t\t\t\t\t}\n\t\t\t\t\t*errors = append(*errors, err)\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tgateways = append(gateways, gateway)\n\t}\n\treturn gateways, nil\n}\n\n// Sets cluster's permissions based on given pub/sub permissions,\n// doing the appropriate translation.\nfunc setClusterPermissions(opts *ClusterOpts, perms *Permissions) {\n\t// Import is whether or not we will send a SUB for interest to the other side.\n\t// Export is whether or not we will accept a SUB from the remote for a given subject.\n\t// Both only effect interest registration.\n\t// The parsing sets Import into Publish and Export into Subscribe, convert\n\t// accordingly.\n\topts.Permissions = &RoutePermissions{\n\t\tImport: perms.Publish,\n\t\tExport: perms.Subscribe,\n\t}\n}\n\n// Temp structures to hold account import and export defintions since they need\n// to be processed after being parsed.\ntype export struct {\n\tacc  *Account\n\tsub  string\n\taccs []string\n\trt   ServiceRespType\n\tlat  *serviceLatency\n\trthr time.Duration\n\ttPos uint\n\tatrc bool // allow_trace\n}\n\ntype importStream struct {\n\tacc  *Account\n\tan   string\n\tsub  string\n\tto   string\n\tpre  string\n\tatrc bool // allow_trace\n}\n\ntype importService struct {\n\tacc   *Account\n\tan    string\n\tsub   string\n\tto    string\n\tshare bool\n}\n\n// Checks if an account name is reserved.\nfunc isReservedAccount(name string) bool {\n\treturn name == globalAccountName\n}\n\nfunc parseAccountMapDest(v any, tk token, errors *[]error) (*MapDest, *configErr) {\n\t// These should be maps.\n\tmv, ok := v.(map[string]any)\n\tif !ok {\n\t\terr := &configErr{tk, \"Expected an entry for the mapping destination\"}\n\t\t*errors = append(*errors, err)\n\t\treturn nil, err\n\t}\n\n\tmdest := &MapDest{}\n\tvar lt token\n\tvar sw bool\n\n\tfor k, v := range mv {\n\t\ttk, dmv := unwrapValue(v, &lt)\n\t\tswitch strings.ToLower(k) {\n\t\tcase \"dest\", \"destination\":\n\t\t\tmdest.Subject = dmv.(string)\n\t\tcase \"weight\":\n\t\t\tswitch vv := dmv.(type) {\n\t\t\tcase string:\n\t\t\t\tws := vv\n\t\t\t\tws = strings.TrimSuffix(ws, \"%\")\n\t\t\t\tweight, err := strconv.Atoi(ws)\n\t\t\t\tif err != nil {\n\t\t\t\t\terr := &configErr{tk, fmt.Sprintf(\"Invalid weight %q for mapping destination\", ws)}\n\t\t\t\t\t*errors = append(*errors, err)\n\t\t\t\t\treturn nil, err\n\t\t\t\t}\n\t\t\t\tif weight > 100 || weight < 0 {\n\t\t\t\t\terr := &configErr{tk, fmt.Sprintf(\"Invalid weight %d for mapping destination\", weight)}\n\t\t\t\t\t*errors = append(*errors, err)\n\t\t\t\t\treturn nil, err\n\t\t\t\t}\n\t\t\t\tmdest.Weight = uint8(weight)\n\t\t\t\tsw = true\n\t\t\tcase int64:\n\t\t\t\tweight := vv\n\t\t\t\tif weight > 100 || weight < 0 {\n\t\t\t\t\terr := &configErr{tk, fmt.Sprintf(\"Invalid weight %d for mapping destination\", weight)}\n\t\t\t\t\t*errors = append(*errors, err)\n\t\t\t\t\treturn nil, err\n\t\t\t\t}\n\t\t\t\tmdest.Weight = uint8(weight)\n\t\t\t\tsw = true\n\t\t\tdefault:\n\t\t\t\terr := &configErr{tk, fmt.Sprintf(\"Unknown entry type for weight of %v\\n\", vv)}\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\tcase \"cluster\":\n\t\t\tmdest.Cluster = dmv.(string)\n\t\tdefault:\n\t\t\terr := &configErr{tk, fmt.Sprintf(\"Unknown field %q for mapping destination\", k)}\n\t\t\t*errors = append(*errors, err)\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\tif !sw {\n\t\terr := &configErr{tk, fmt.Sprintf(\"Missing weight for mapping destination %q\", mdest.Subject)}\n\t\t*errors = append(*errors, err)\n\t\treturn nil, err\n\t}\n\n\treturn mdest, nil\n}\n\n// parseAccountMappings is called to parse account mappings.\nfunc parseAccountMappings(v any, acc *Account, errors *[]error) error {\n\tvar lt token\n\tdefer convertPanicToErrorList(&lt, errors)\n\n\ttk, v := unwrapValue(v, &lt)\n\tam := v.(map[string]any)\n\tfor subj, mv := range am {\n\t\tif !IsValidSubject(subj) {\n\t\t\terr := &configErr{tk, fmt.Sprintf(\"Subject %q is not a valid subject\", subj)}\n\t\t\t*errors = append(*errors, err)\n\t\t\tcontinue\n\t\t}\n\t\ttk, v := unwrapValue(mv, &lt)\n\n\t\tswitch vv := v.(type) {\n\t\tcase string:\n\t\t\tif err := acc.AddMapping(subj, v.(string)); err != nil {\n\t\t\t\terr := &configErr{tk, fmt.Sprintf(\"Error adding mapping for %q to %q : %v\", subj, v.(string), err)}\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\tcase []any:\n\t\t\tvar mappings []*MapDest\n\t\t\tfor _, mv := range v.([]any) {\n\t\t\t\ttk, amv := unwrapValue(mv, &lt)\n\t\t\t\tmdest, err := parseAccountMapDest(amv, tk, errors)\n\t\t\t\tif err != nil {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tmappings = append(mappings, mdest)\n\t\t\t}\n\n\t\t\t// Now add them in..\n\t\t\tif err := acc.AddWeightedMappings(subj, mappings...); err != nil {\n\t\t\t\terr := &configErr{tk, fmt.Sprintf(\"Error adding mapping for %q : %v\", subj, err)}\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\tcase any:\n\t\t\ttk, amv := unwrapValue(mv, &lt)\n\t\t\tmdest, err := parseAccountMapDest(amv, tk, errors)\n\t\t\tif err != nil {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\t// Now add it in..\n\t\t\tif err := acc.AddWeightedMappings(subj, mdest); err != nil {\n\t\t\t\terr := &configErr{tk, fmt.Sprintf(\"Error adding mapping for %q : %v\", subj, err)}\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\tdefault:\n\t\t\terr := &configErr{tk, fmt.Sprintf(\"Unknown type %T for mapping destination\", vv)}\n\t\t\t*errors = append(*errors, err)\n\t\t\tcontinue\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// parseAccountLimits is called to parse account limits in a server config.\nfunc parseAccountLimits(mv any, acc *Account, errors *[]error) error {\n\tvar lt token\n\tdefer convertPanicToErrorList(&lt, errors)\n\n\ttk, v := unwrapValue(mv, &lt)\n\tam, ok := v.(map[string]any)\n\tif !ok {\n\t\treturn &configErr{tk, fmt.Sprintf(\"Expected account limits to be a map/struct, got %+v\", v)}\n\t}\n\n\tfor k, v := range am {\n\t\ttk, mv = unwrapValue(v, &lt)\n\t\tswitch strings.ToLower(k) {\n\t\tcase \"max_connections\", \"max_conn\":\n\t\t\tacc.mconns = int32(mv.(int64))\n\t\tcase \"max_subscriptions\", \"max_subs\":\n\t\t\tacc.msubs = int32(mv.(int64))\n\t\tcase \"max_payload\", \"max_pay\":\n\t\t\tacc.mpay = int32(mv.(int64))\n\t\tcase \"max_leafnodes\", \"max_leafs\":\n\t\t\tacc.mleafs = int32(mv.(int64))\n\t\tdefault:\n\t\t\tif !tk.IsUsedVariable() {\n\t\t\t\terr := &configErr{tk, fmt.Sprintf(\"Unknown field %q parsing account limits\", k)}\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t}\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc parseAccountMsgTrace(mv any, topKey string, acc *Account) error {\n\tprocessDest := func(tk token, k string, v any) error {\n\t\ttd, ok := v.(string)\n\t\tif !ok {\n\t\t\treturn &configErr{tk, fmt.Sprintf(\"Field %q should be a string, got %T\", k, v)}\n\t\t}\n\t\tif !IsValidPublishSubject(td) {\n\t\t\treturn &configErr{tk, fmt.Sprintf(\"Trace destination %q is not valid\", td)}\n\t\t}\n\t\tacc.traceDest = td\n\t\treturn nil\n\t}\n\tprocessSampling := func(tk token, n int) error {\n\t\tif n <= 0 || n > 100 {\n\t\t\treturn &configErr{tk, fmt.Sprintf(\"Ttrace destination sampling value %d is invalid, needs to be [1..100]\", n)}\n\t\t}\n\t\tacc.traceDestSampling = n\n\t\treturn nil\n\t}\n\n\tvar lt token\n\ttk, v := unwrapValue(mv, &lt)\n\tswitch vv := v.(type) {\n\tcase string:\n\t\treturn processDest(tk, topKey, v)\n\tcase map[string]any:\n\t\tfor k, v := range vv {\n\t\t\ttk, v := unwrapValue(v, &lt)\n\t\t\tswitch strings.ToLower(k) {\n\t\t\tcase \"dest\":\n\t\t\t\tif err := processDest(tk, k, v); err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\tcase \"sampling\":\n\t\t\t\tswitch vv := v.(type) {\n\t\t\t\tcase int64:\n\t\t\t\t\tif err := processSampling(tk, int(vv)); err != nil {\n\t\t\t\t\t\treturn err\n\t\t\t\t\t}\n\t\t\t\tcase string:\n\t\t\t\t\ts := strings.TrimSuffix(vv, \"%\")\n\t\t\t\t\tn, err := strconv.Atoi(s)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn &configErr{tk, fmt.Sprintf(\"Invalid trace destination sampling value %q\", vv)}\n\t\t\t\t\t}\n\t\t\t\t\tif err := processSampling(tk, n); err != nil {\n\t\t\t\t\t\treturn err\n\t\t\t\t\t}\n\t\t\t\tdefault:\n\t\t\t\t\treturn &configErr{tk, fmt.Sprintf(\"Trace destination sampling field %q should be an integer or a percentage, got %T\", k, v)}\n\t\t\t\t}\n\t\t\tdefault:\n\t\t\t\tif !tk.IsUsedVariable() {\n\t\t\t\t\treturn &configErr{tk, fmt.Sprintf(\"Unknown field %q parsing account message trace map/struct %q\", k, topKey)}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\tdefault:\n\t\treturn &configErr{tk, fmt.Sprintf(\"Expected account message trace %q to be a string or a map/struct, got %T\", topKey, v)}\n\t}\n\treturn nil\n}\n\n// parseAccounts will parse the different accounts syntax.\nfunc parseAccounts(v any, opts *Options, errors *[]error, warnings *[]error) error {\n\tvar (\n\t\timportStreams  []*importStream\n\t\timportServices []*importService\n\t\texportStreams  []*export\n\t\texportServices []*export\n\t\tlt             token\n\t)\n\tdefer convertPanicToErrorList(&lt, errors)\n\n\ttk, v := unwrapValue(v, &lt)\n\tswitch vv := v.(type) {\n\t// Simple array of account names.\n\tcase []any, []string:\n\t\tm := make(map[string]struct{}, len(v.([]any)))\n\t\tfor _, n := range v.([]any) {\n\t\t\ttk, name := unwrapValue(n, &lt)\n\t\t\tns := name.(string)\n\t\t\t// Check for reserved names.\n\t\t\tif isReservedAccount(ns) {\n\t\t\t\terr := &configErr{tk, fmt.Sprintf(\"%q is a Reserved Account\", ns)}\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif _, ok := m[ns]; ok {\n\t\t\t\terr := &configErr{tk, fmt.Sprintf(\"Duplicate Account Entry: %s\", ns)}\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\topts.Accounts = append(opts.Accounts, NewAccount(ns))\n\t\t\tm[ns] = struct{}{}\n\t\t}\n\t// More common map entry\n\tcase map[string]any:\n\t\t// Track users across accounts, must be unique across\n\t\t// accounts and nkeys vs users.\n\t\t// We also want to check for users that may have been added in\n\t\t// parseAuthorization{} if that happened first.\n\t\tuorn := setupUsersAndNKeysDuplicateCheckMap(opts)\n\n\t\tfor aname, mv := range vv {\n\t\t\ttk, amv := unwrapValue(mv, &lt)\n\n\t\t\t// Skip referenced config vars within the account block.\n\t\t\tif tk.IsUsedVariable() {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// These should be maps.\n\t\t\tmv, ok := amv.(map[string]any)\n\t\t\tif !ok {\n\t\t\t\terr := &configErr{tk, \"Expected map entries for accounts\"}\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif isReservedAccount(aname) {\n\t\t\t\terr := &configErr{tk, fmt.Sprintf(\"%q is a Reserved Account\", aname)}\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tvar (\n\t\t\t\tusers   []*User\n\t\t\t\tnkeyUsr []*NkeyUser\n\t\t\t\tusersTk token\n\t\t\t)\n\t\t\tacc := NewAccount(aname)\n\t\t\topts.Accounts = append(opts.Accounts, acc)\n\n\t\t\tfor k, v := range mv {\n\t\t\t\ttk, mv := unwrapValue(v, &lt)\n\t\t\t\tswitch strings.ToLower(k) {\n\t\t\t\tcase \"nkey\":\n\t\t\t\t\tnk, ok := mv.(string)\n\t\t\t\t\tif !ok || !nkeys.IsValidPublicAccountKey(nk) {\n\t\t\t\t\t\terr := &configErr{tk, fmt.Sprintf(\"Not a valid public nkey for an account: %q\", mv)}\n\t\t\t\t\t\t*errors = append(*errors, err)\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\t\t\t\t\tacc.Nkey = nk\n\t\t\t\tcase \"imports\":\n\t\t\t\t\tstreams, services, err := parseAccountImports(tk, acc, errors)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\t*errors = append(*errors, err)\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\t\t\t\t\timportStreams = append(importStreams, streams...)\n\t\t\t\t\timportServices = append(importServices, services...)\n\t\t\t\tcase \"exports\":\n\t\t\t\t\tstreams, services, err := parseAccountExports(tk, acc, errors)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\t*errors = append(*errors, err)\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\t\t\t\t\texportStreams = append(exportStreams, streams...)\n\t\t\t\t\texportServices = append(exportServices, services...)\n\t\t\t\tcase \"jetstream\":\n\t\t\t\t\terr := parseJetStreamForAccount(mv, acc, errors)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\t*errors = append(*errors, err)\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\t\t\t\tcase \"users\":\n\t\t\t\t\tvar err error\n\t\t\t\t\tusersTk = tk\n\t\t\t\t\tnkeyUsr, users, err = parseUsers(mv, errors)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\t*errors = append(*errors, err)\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\t\t\t\tcase \"default_permissions\":\n\t\t\t\t\tpermissions, err := parseUserPermissions(tk, errors)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\t*errors = append(*errors, err)\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\t\t\t\t\tacc.defaultPerms = permissions\n\t\t\t\tcase \"mappings\", \"maps\":\n\t\t\t\t\terr := parseAccountMappings(tk, acc, errors)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\t*errors = append(*errors, err)\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\t\t\t\tcase \"limits\":\n\t\t\t\t\terr := parseAccountLimits(tk, acc, errors)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\t*errors = append(*errors, err)\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\t\t\t\tcase \"msg_trace\", \"trace_dest\":\n\t\t\t\t\tif err := parseAccountMsgTrace(tk, k, acc); err != nil {\n\t\t\t\t\t\t*errors = append(*errors, err)\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\t\t\t\t\t// If trace destination is set but no sampling, set it to 100%.\n\t\t\t\t\tif acc.traceDest != _EMPTY_ && acc.traceDestSampling == 0 {\n\t\t\t\t\t\tacc.traceDestSampling = 100\n\t\t\t\t\t} else if acc.traceDestSampling > 0 && acc.traceDest == _EMPTY_ {\n\t\t\t\t\t\t// If no trace destination is provided, no trace would be\n\t\t\t\t\t\t// triggered, so if the user set a sampling value expecting\n\t\t\t\t\t\t// something to happen, want and set the value to 0 for good\n\t\t\t\t\t\t// measure.\n\t\t\t\t\t\t*warnings = append(*warnings,\n\t\t\t\t\t\t\t&configErr{tk, \"Trace destination sampling ignored since no destination was set\"})\n\t\t\t\t\t\tacc.traceDestSampling = 0\n\t\t\t\t\t}\n\t\t\t\tdefault:\n\t\t\t\t\tif !tk.IsUsedVariable() {\n\t\t\t\t\t\terr := &unknownConfigFieldErr{\n\t\t\t\t\t\t\tfield: k,\n\t\t\t\t\t\t\tconfigErr: configErr{\n\t\t\t\t\t\t\t\ttoken: tk,\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t}\n\t\t\t\t\t\t*errors = append(*errors, err)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\t// Report error if there is an authorization{} block\n\t\t\t// with u/p or token and any user defined in accounts{}\n\t\t\tif len(nkeyUsr) > 0 || len(users) > 0 {\n\t\t\t\tif opts.Username != _EMPTY_ {\n\t\t\t\t\terr := &configErr{usersTk, \"Can not have a single user/pass and accounts\"}\n\t\t\t\t\t*errors = append(*errors, err)\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tif opts.Authorization != _EMPTY_ {\n\t\t\t\t\terr := &configErr{usersTk, \"Can not have a token and accounts\"}\n\t\t\t\t\t*errors = append(*errors, err)\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t}\n\t\t\tapplyDefaultPermissions(users, nkeyUsr, acc.defaultPerms)\n\t\t\tfor _, u := range nkeyUsr {\n\t\t\t\tif _, ok := uorn[u.Nkey]; ok {\n\t\t\t\t\terr := &configErr{usersTk, fmt.Sprintf(\"Duplicate nkey %q detected\", u.Nkey)}\n\t\t\t\t\t*errors = append(*errors, err)\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tuorn[u.Nkey] = struct{}{}\n\t\t\t\tu.Account = acc\n\t\t\t}\n\t\t\topts.Nkeys = append(opts.Nkeys, nkeyUsr...)\n\t\t\tfor _, u := range users {\n\t\t\t\tif _, ok := uorn[u.Username]; ok {\n\t\t\t\t\terr := &configErr{usersTk, fmt.Sprintf(\"Duplicate user %q detected\", u.Username)}\n\t\t\t\t\t*errors = append(*errors, err)\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tuorn[u.Username] = struct{}{}\n\t\t\t\tu.Account = acc\n\t\t\t}\n\t\t\topts.Users = append(opts.Users, users...)\n\t\t}\n\t}\n\tlt = tk\n\t// Bail already if there are previous errors.\n\tif len(*errors) > 0 {\n\t\treturn nil\n\t}\n\n\t// Parse Imports and Exports here after all accounts defined.\n\t// Do exports first since they need to be defined for imports to succeed\n\t// since we do permissions checks.\n\n\t// Create a lookup map for accounts lookups.\n\tam := make(map[string]*Account, len(opts.Accounts))\n\tfor _, a := range opts.Accounts {\n\t\tam[a.Name] = a\n\t}\n\t// Do stream exports\n\tfor _, stream := range exportStreams {\n\t\t// Make array of accounts if applicable.\n\t\tvar accounts []*Account\n\t\tfor _, an := range stream.accs {\n\t\t\tta := am[an]\n\t\t\tif ta == nil {\n\t\t\t\tmsg := fmt.Sprintf(\"%q account not defined for stream export\", an)\n\t\t\t\t*errors = append(*errors, &configErr{tk, msg})\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\taccounts = append(accounts, ta)\n\t\t}\n\t\tif err := stream.acc.addStreamExportWithAccountPos(stream.sub, accounts, stream.tPos); err != nil {\n\t\t\tmsg := fmt.Sprintf(\"Error adding stream export %q: %v\", stream.sub, err)\n\t\t\t*errors = append(*errors, &configErr{tk, msg})\n\t\t\tcontinue\n\t\t}\n\t}\n\tfor _, service := range exportServices {\n\t\t// Make array of accounts if applicable.\n\t\tvar accounts []*Account\n\t\tfor _, an := range service.accs {\n\t\t\tta := am[an]\n\t\t\tif ta == nil {\n\t\t\t\tmsg := fmt.Sprintf(\"%q account not defined for service export\", an)\n\t\t\t\t*errors = append(*errors, &configErr{tk, msg})\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\taccounts = append(accounts, ta)\n\t\t}\n\t\tif err := service.acc.addServiceExportWithResponseAndAccountPos(service.sub, service.rt, accounts, service.tPos); err != nil {\n\t\t\tmsg := fmt.Sprintf(\"Error adding service export %q: %v\", service.sub, err)\n\t\t\t*errors = append(*errors, &configErr{tk, msg})\n\t\t\tcontinue\n\t\t}\n\n\t\tif service.rthr != 0 {\n\t\t\t// Response threshold was set in options.\n\t\t\tif err := service.acc.SetServiceExportResponseThreshold(service.sub, service.rthr); err != nil {\n\t\t\t\tmsg := fmt.Sprintf(\"Error adding service export response threshold for %q: %v\", service.sub, err)\n\t\t\t\t*errors = append(*errors, &configErr{tk, msg})\n\t\t\t\tcontinue\n\t\t\t}\n\t\t}\n\n\t\tif service.lat != nil {\n\t\t\t// System accounts are on be default so just make sure we have not opted out..\n\t\t\tif opts.NoSystemAccount {\n\t\t\t\tmsg := fmt.Sprintf(\"Error adding service latency sampling for %q: %v\", service.sub, ErrNoSysAccount.Error())\n\t\t\t\t*errors = append(*errors, &configErr{tk, msg})\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tif err := service.acc.TrackServiceExportWithSampling(service.sub, service.lat.subject, int(service.lat.sampling)); err != nil {\n\t\t\t\tmsg := fmt.Sprintf(\"Error adding service latency sampling for %q on subject %q: %v\", service.sub, service.lat.subject, err)\n\t\t\t\t*errors = append(*errors, &configErr{tk, msg})\n\t\t\t\tcontinue\n\t\t\t}\n\t\t}\n\n\t\tif service.atrc {\n\t\t\tif err := service.acc.SetServiceExportAllowTrace(service.sub, true); err != nil {\n\t\t\t\tmsg := fmt.Sprintf(\"Error adding allow_trace for %q: %v\", service.sub, err)\n\t\t\t\t*errors = append(*errors, &configErr{tk, msg})\n\t\t\t\tcontinue\n\t\t\t}\n\t\t}\n\t}\n\tfor _, stream := range importStreams {\n\t\tta := am[stream.an]\n\t\tif ta == nil {\n\t\t\tmsg := fmt.Sprintf(\"%q account not defined for stream import\", stream.an)\n\t\t\t*errors = append(*errors, &configErr{tk, msg})\n\t\t\tcontinue\n\t\t}\n\t\tif stream.pre != _EMPTY_ {\n\t\t\tif err := stream.acc.addStreamImportWithClaim(ta, stream.sub, stream.pre, stream.atrc, nil); err != nil {\n\t\t\t\tmsg := fmt.Sprintf(\"Error adding stream import %q: %v\", stream.sub, err)\n\t\t\t\t*errors = append(*errors, &configErr{tk, msg})\n\t\t\t\tcontinue\n\t\t\t}\n\t\t} else {\n\t\t\tif err := stream.acc.addMappedStreamImportWithClaim(ta, stream.sub, stream.to, stream.atrc, nil); err != nil {\n\t\t\t\tmsg := fmt.Sprintf(\"Error adding stream import %q: %v\", stream.sub, err)\n\t\t\t\t*errors = append(*errors, &configErr{tk, msg})\n\t\t\t\tcontinue\n\t\t\t}\n\t\t}\n\t}\n\tfor _, service := range importServices {\n\t\tta := am[service.an]\n\t\tif ta == nil {\n\t\t\tmsg := fmt.Sprintf(\"%q account not defined for service import\", service.an)\n\t\t\t*errors = append(*errors, &configErr{tk, msg})\n\t\t\tcontinue\n\t\t}\n\t\tif service.to == _EMPTY_ {\n\t\t\tservice.to = service.sub\n\t\t}\n\t\tif err := service.acc.AddServiceImport(ta, service.to, service.sub); err != nil {\n\t\t\tmsg := fmt.Sprintf(\"Error adding service import %q: %v\", service.sub, err)\n\t\t\t*errors = append(*errors, &configErr{tk, msg})\n\t\t\tcontinue\n\t\t}\n\t\tif err := service.acc.SetServiceImportSharing(ta, service.sub, service.share); err != nil {\n\t\t\tmsg := fmt.Sprintf(\"Error setting service import sharing %q: %v\", service.sub, err)\n\t\t\t*errors = append(*errors, &configErr{tk, msg})\n\t\t\tcontinue\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// Parse the account exports\nfunc parseAccountExports(v any, acc *Account, errors *[]error) ([]*export, []*export, error) {\n\tvar lt token\n\tdefer convertPanicToErrorList(&lt, errors)\n\n\t// This should be an array of objects/maps.\n\ttk, v := unwrapValue(v, &lt)\n\tims, ok := v.([]any)\n\tif !ok {\n\t\treturn nil, nil, &configErr{tk, fmt.Sprintf(\"Exports should be an array, got %T\", v)}\n\t}\n\n\tvar services []*export\n\tvar streams []*export\n\n\tfor _, v := range ims {\n\t\t// Should have stream or service\n\t\tstream, service, err := parseExportStreamOrService(v, errors)\n\t\tif err != nil {\n\t\t\t*errors = append(*errors, err)\n\t\t\tcontinue\n\t\t}\n\t\tif service != nil {\n\t\t\tservice.acc = acc\n\t\t\tservices = append(services, service)\n\t\t}\n\t\tif stream != nil {\n\t\t\tstream.acc = acc\n\t\t\tstreams = append(streams, stream)\n\t\t}\n\t}\n\treturn streams, services, nil\n}\n\n// Parse the account imports\nfunc parseAccountImports(v any, acc *Account, errors *[]error) ([]*importStream, []*importService, error) {\n\tvar lt token\n\tdefer convertPanicToErrorList(&lt, errors)\n\n\t// This should be an array of objects/maps.\n\ttk, v := unwrapValue(v, &lt)\n\tims, ok := v.([]any)\n\tif !ok {\n\t\treturn nil, nil, &configErr{tk, fmt.Sprintf(\"Imports should be an array, got %T\", v)}\n\t}\n\n\tvar services []*importService\n\tvar streams []*importStream\n\tsvcSubjects := map[string][]*importService{}\n\nIMS_LOOP:\n\tfor _, v := range ims {\n\t\t// Should have stream or service\n\t\tstream, service, err := parseImportStreamOrService(v, errors)\n\t\tif err != nil {\n\t\t\t*errors = append(*errors, err)\n\t\t\tcontinue\n\t\t}\n\t\tif service != nil {\n\t\t\tsisPerSubj := svcSubjects[service.to]\n\t\t\tfor _, dup := range sisPerSubj {\n\t\t\t\tif dup.an == service.an {\n\t\t\t\t\ttk, _ := unwrapValue(v, &lt)\n\t\t\t\t\terr := &configErr{tk,\n\t\t\t\t\t\tfmt.Sprintf(\"Duplicate service import subject %q, previously used in import for account %q, subject %q\",\n\t\t\t\t\t\t\tservice.to, dup.an, dup.sub)}\n\t\t\t\t\t*errors = append(*errors, err)\n\t\t\t\t\tcontinue IMS_LOOP\n\t\t\t\t}\n\t\t\t}\n\t\t\tservice.acc = acc\n\t\t\tsisPerSubj = append(sisPerSubj, service)\n\t\t\tsvcSubjects[service.to] = sisPerSubj\n\t\t\tservices = append(services, service)\n\t\t}\n\t\tif stream != nil {\n\t\t\tstream.acc = acc\n\t\t\tstreams = append(streams, stream)\n\t\t}\n\t}\n\treturn streams, services, nil\n}\n\n// Helper to parse an embedded account description for imported services or streams.\nfunc parseAccount(v map[string]any, errors *[]error) (string, string, error) {\n\tvar lt token\n\tdefer convertPanicToErrorList(&lt, errors)\n\n\tvar accountName, subject string\n\tfor mk, mv := range v {\n\t\ttk, mv := unwrapValue(mv, &lt)\n\t\tswitch strings.ToLower(mk) {\n\t\tcase \"account\":\n\t\t\taccountName = mv.(string)\n\t\tcase \"subject\":\n\t\t\tsubject = mv.(string)\n\t\tdefault:\n\t\t\tif !tk.IsUsedVariable() {\n\t\t\t\terr := &unknownConfigFieldErr{\n\t\t\t\t\tfield: mk,\n\t\t\t\t\tconfigErr: configErr{\n\t\t\t\t\t\ttoken: tk,\n\t\t\t\t\t},\n\t\t\t\t}\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t}\n\t\t}\n\t}\n\treturn accountName, subject, nil\n}\n\n// Parse an export stream or service.\n// e.g.\n// {stream: \"public.>\"} # No accounts means public.\n// {stream: \"synadia.private.>\", accounts: [cncf, natsio]}\n// {service: \"pub.request\"} # No accounts means public.\n// {service: \"pub.special.request\", accounts: [nats.io]}\nfunc parseExportStreamOrService(v any, errors *[]error) (*export, *export, error) {\n\tvar (\n\t\tcurStream  *export\n\t\tcurService *export\n\t\taccounts   []string\n\t\trt         ServiceRespType\n\t\trtSeen     bool\n\t\trtToken    token\n\t\tlat        *serviceLatency\n\t\tthreshSeen bool\n\t\tthresh     time.Duration\n\t\tlatToken   token\n\t\tlt         token\n\t\taccTokPos  uint\n\t\tatrc       bool\n\t\tatrcSeen   bool\n\t\tatrcToken  token\n\t)\n\tdefer convertPanicToErrorList(&lt, errors)\n\n\ttk, v := unwrapValue(v, &lt)\n\tvv, ok := v.(map[string]any)\n\tif !ok {\n\t\treturn nil, nil, &configErr{tk, fmt.Sprintf(\"Export Items should be a map with type entry, got %T\", v)}\n\t}\n\tfor mk, mv := range vv {\n\t\ttk, mv := unwrapValue(mv, &lt)\n\t\tswitch strings.ToLower(mk) {\n\t\tcase \"stream\":\n\t\t\tif curService != nil {\n\t\t\t\terr := &configErr{tk, fmt.Sprintf(\"Detected stream %q but already saw a service\", mv)}\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif rtToken != nil {\n\t\t\t\terr := &configErr{rtToken, \"Detected response directive on non-service\"}\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif latToken != nil {\n\t\t\t\terr := &configErr{latToken, \"Detected latency directive on non-service\"}\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif atrcToken != nil {\n\t\t\t\terr := &configErr{atrcToken, \"Detected allow_trace directive on non-service\"}\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tmvs, ok := mv.(string)\n\t\t\tif !ok {\n\t\t\t\terr := &configErr{tk, fmt.Sprintf(\"Expected stream name to be string, got %T\", mv)}\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tcurStream = &export{sub: mvs}\n\t\t\tif accounts != nil {\n\t\t\t\tcurStream.accs = accounts\n\t\t\t}\n\t\tcase \"service\":\n\t\t\tif curStream != nil {\n\t\t\t\terr := &configErr{tk, fmt.Sprintf(\"Detected service %q but already saw a stream\", mv)}\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tmvs, ok := mv.(string)\n\t\t\tif !ok {\n\t\t\t\terr := &configErr{tk, fmt.Sprintf(\"Expected service name to be string, got %T\", mv)}\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tcurService = &export{sub: mvs}\n\t\t\tif accounts != nil {\n\t\t\t\tcurService.accs = accounts\n\t\t\t}\n\t\t\tif rtSeen {\n\t\t\t\tcurService.rt = rt\n\t\t\t}\n\t\t\tif lat != nil {\n\t\t\t\tcurService.lat = lat\n\t\t\t}\n\t\t\tif threshSeen {\n\t\t\t\tcurService.rthr = thresh\n\t\t\t}\n\t\t\tif atrcSeen {\n\t\t\t\tcurService.atrc = atrc\n\t\t\t}\n\t\tcase \"response\", \"response_type\":\n\t\t\tif rtSeen {\n\t\t\t\terr := &configErr{tk, \"Duplicate response type definition\"}\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\trtSeen = true\n\t\t\trtToken = tk\n\t\t\tmvs, ok := mv.(string)\n\t\t\tif !ok {\n\t\t\t\terr := &configErr{tk, fmt.Sprintf(\"Expected response type to be string, got %T\", mv)}\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tswitch strings.ToLower(mvs) {\n\t\t\tcase \"single\", \"singleton\":\n\t\t\t\trt = Singleton\n\t\t\tcase \"stream\":\n\t\t\t\trt = Streamed\n\t\t\tcase \"chunk\", \"chunked\":\n\t\t\t\trt = Chunked\n\t\t\tdefault:\n\t\t\t\terr := &configErr{tk, fmt.Sprintf(\"Unknown response type: %q\", mvs)}\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif curService != nil {\n\t\t\t\tcurService.rt = rt\n\t\t\t}\n\t\t\tif curStream != nil {\n\t\t\t\terr := &configErr{tk, \"Detected response directive on non-service\"}\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t}\n\t\tcase \"threshold\", \"response_threshold\", \"response_max_time\", \"response_time\":\n\t\t\tif threshSeen {\n\t\t\t\terr := &configErr{tk, \"Duplicate response threshold detected\"}\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tthreshSeen = true\n\t\t\tmvs, ok := mv.(string)\n\t\t\tif !ok {\n\t\t\t\terr := &configErr{tk, fmt.Sprintf(\"Expected response threshold to be a parseable time duration, got %T\", mv)}\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tvar err error\n\t\t\tthresh, err = time.ParseDuration(mvs)\n\t\t\tif err != nil {\n\t\t\t\terr := &configErr{tk, fmt.Sprintf(\"Expected response threshold to be a parseable time duration, got %q\", mvs)}\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif curService != nil {\n\t\t\t\tcurService.rthr = thresh\n\t\t\t}\n\t\t\tif curStream != nil {\n\t\t\t\terr := &configErr{tk, \"Detected response directive on non-service\"}\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t}\n\t\tcase \"accounts\":\n\t\t\tfor _, iv := range mv.([]any) {\n\t\t\t\t_, mv := unwrapValue(iv, &lt)\n\t\t\t\taccounts = append(accounts, mv.(string))\n\t\t\t}\n\t\t\tif curStream != nil {\n\t\t\t\tcurStream.accs = accounts\n\t\t\t} else if curService != nil {\n\t\t\t\tcurService.accs = accounts\n\t\t\t}\n\t\tcase \"latency\":\n\t\t\tlatToken = tk\n\t\t\tvar err error\n\t\t\tlat, err = parseServiceLatency(tk, mv)\n\t\t\tif err != nil {\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif curStream != nil {\n\t\t\t\terr = &configErr{tk, \"Detected latency directive on non-service\"}\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif curService != nil {\n\t\t\t\tcurService.lat = lat\n\t\t\t}\n\t\tcase \"account_token_position\":\n\t\t\taccTokPos = uint(mv.(int64))\n\t\tcase \"allow_trace\":\n\t\t\tatrcSeen = true\n\t\t\tatrcToken = tk\n\t\t\tatrc = mv.(bool)\n\t\t\tif curStream != nil {\n\t\t\t\t*errors = append(*errors,\n\t\t\t\t\t&configErr{tk, \"Detected allow_trace directive on non-service\"})\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif curService != nil {\n\t\t\t\tcurService.atrc = atrc\n\t\t\t}\n\t\tdefault:\n\t\t\tif !tk.IsUsedVariable() {\n\t\t\t\terr := &unknownConfigFieldErr{\n\t\t\t\t\tfield: mk,\n\t\t\t\t\tconfigErr: configErr{\n\t\t\t\t\t\ttoken: tk,\n\t\t\t\t\t},\n\t\t\t\t}\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t}\n\t\t}\n\t}\n\tif curStream != nil {\n\t\tcurStream.tPos = accTokPos\n\t}\n\tif curService != nil {\n\t\tcurService.tPos = accTokPos\n\t}\n\treturn curStream, curService, nil\n}\n\n// parseServiceLatency returns a latency config block.\nfunc parseServiceLatency(root token, v any) (l *serviceLatency, retErr error) {\n\tvar lt token\n\tdefer convertPanicToError(&lt, &retErr)\n\n\tif subject, ok := v.(string); ok {\n\t\treturn &serviceLatency{\n\t\t\tsubject:  subject,\n\t\t\tsampling: DEFAULT_SERVICE_LATENCY_SAMPLING,\n\t\t}, nil\n\t}\n\n\tlatency, ok := v.(map[string]any)\n\tif !ok {\n\t\treturn nil, &configErr{token: root,\n\t\t\treason: fmt.Sprintf(\"Expected latency entry to be a map/struct or string, got %T\", v)}\n\t}\n\n\tsl := serviceLatency{\n\t\tsampling: DEFAULT_SERVICE_LATENCY_SAMPLING,\n\t}\n\n\t// Read sampling value.\n\tif v, ok := latency[\"sampling\"]; ok {\n\t\ttk, v := unwrapValue(v, &lt)\n\t\theader := false\n\t\tvar sample int64\n\t\tswitch vv := v.(type) {\n\t\tcase int64:\n\t\t\t// Sample is an int, like 50.\n\t\t\tsample = vv\n\t\tcase string:\n\t\t\t// Sample is a string, like \"50%\".\n\t\t\tif strings.ToLower(strings.TrimSpace(vv)) == \"headers\" {\n\t\t\t\theader = true\n\t\t\t\tsample = 0\n\t\t\t\tbreak\n\t\t\t}\n\t\t\ts := strings.TrimSuffix(vv, \"%\")\n\t\t\tn, err := strconv.Atoi(s)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, &configErr{token: tk,\n\t\t\t\t\treason: fmt.Sprintf(\"Failed to parse latency sample: %v\", err)}\n\t\t\t}\n\t\t\tsample = int64(n)\n\t\tdefault:\n\t\t\treturn nil, &configErr{token: tk,\n\t\t\t\treason: fmt.Sprintf(\"Expected latency sample to be a string or map/struct, got %T\", v)}\n\t\t}\n\t\tif !header {\n\t\t\tif sample < 1 || sample > 100 {\n\t\t\t\treturn nil, &configErr{token: tk,\n\t\t\t\t\treason: ErrBadSampling.Error()}\n\t\t\t}\n\t\t}\n\n\t\tsl.sampling = int8(sample)\n\t}\n\n\t// Read subject value.\n\tv, ok = latency[\"subject\"]\n\tif !ok {\n\t\treturn nil, &configErr{token: root,\n\t\t\treason: \"Latency subject required, but missing\"}\n\t}\n\n\ttk, v := unwrapValue(v, &lt)\n\tsubject, ok := v.(string)\n\tif !ok {\n\t\treturn nil, &configErr{token: tk,\n\t\t\treason: fmt.Sprintf(\"Expected latency subject to be a string, got %T\", subject)}\n\t}\n\tsl.subject = subject\n\n\treturn &sl, nil\n}\n\n// Parse an import stream or service.\n// e.g.\n// {stream: {account: \"synadia\", subject:\"public.synadia\"}, prefix: \"imports.synadia\"}\n// {stream: {account: \"synadia\", subject:\"synadia.private.*\"}}\n// {service: {account: \"synadia\", subject: \"pub.special.request\"}, to: \"synadia.request\"}\nfunc parseImportStreamOrService(v any, errors *[]error) (*importStream, *importService, error) {\n\tvar (\n\t\tcurStream  *importStream\n\t\tcurService *importService\n\t\tpre, to    string\n\t\tshare      bool\n\t\tlt         token\n\t\tatrc       bool\n\t\tatrcSeen   bool\n\t\tatrcToken  token\n\t)\n\tdefer convertPanicToErrorList(&lt, errors)\n\n\ttk, mv := unwrapValue(v, &lt)\n\tvv, ok := mv.(map[string]any)\n\tif !ok {\n\t\treturn nil, nil, &configErr{tk, fmt.Sprintf(\"Import Items should be a map with type entry, got %T\", mv)}\n\t}\n\tfor mk, mv := range vv {\n\t\ttk, mv := unwrapValue(mv, &lt)\n\t\tswitch strings.ToLower(mk) {\n\t\tcase \"stream\":\n\t\t\tif curService != nil {\n\t\t\t\terr := &configErr{tk, \"Detected stream but already saw a service\"}\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tac, ok := mv.(map[string]any)\n\t\t\tif !ok {\n\t\t\t\terr := &configErr{tk, fmt.Sprintf(\"Stream entry should be an account map, got %T\", mv)}\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\t// Make sure this is a map with account and subject\n\t\t\taccountName, subject, err := parseAccount(ac, errors)\n\t\t\tif err != nil {\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif accountName == _EMPTY_ || subject == _EMPTY_ {\n\t\t\t\terr := &configErr{tk, \"Expect an account name and a subject\"}\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tcurStream = &importStream{an: accountName, sub: subject}\n\t\t\tif to != _EMPTY_ {\n\t\t\t\tcurStream.to = to\n\t\t\t}\n\t\t\tif pre != _EMPTY_ {\n\t\t\t\tcurStream.pre = pre\n\t\t\t}\n\t\t\tif atrcSeen {\n\t\t\t\tcurStream.atrc = atrc\n\t\t\t}\n\t\tcase \"service\":\n\t\t\tif curStream != nil {\n\t\t\t\terr := &configErr{tk, \"Detected service but already saw a stream\"}\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif atrcToken != nil {\n\t\t\t\terr := &configErr{atrcToken, \"Detected allow_trace directive on a non-stream\"}\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tac, ok := mv.(map[string]any)\n\t\t\tif !ok {\n\t\t\t\terr := &configErr{tk, fmt.Sprintf(\"Service entry should be an account map, got %T\", mv)}\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\t// Make sure this is a map with account and subject\n\t\t\taccountName, subject, err := parseAccount(ac, errors)\n\t\t\tif err != nil {\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif accountName == _EMPTY_ || subject == _EMPTY_ {\n\t\t\t\terr := &configErr{tk, \"Expect an account name and a subject\"}\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tcurService = &importService{an: accountName, sub: subject}\n\t\t\tif to != _EMPTY_ {\n\t\t\t\tcurService.to = to\n\t\t\t} else {\n\t\t\t\tcurService.to = subject\n\t\t\t}\n\t\t\tcurService.share = share\n\t\tcase \"prefix\":\n\t\t\tpre = mv.(string)\n\t\t\tif curStream != nil {\n\t\t\t\tcurStream.pre = pre\n\t\t\t}\n\t\tcase \"to\":\n\t\t\tto = mv.(string)\n\t\t\tif curService != nil {\n\t\t\t\tcurService.to = to\n\t\t\t}\n\t\t\tif curStream != nil {\n\t\t\t\tcurStream.to = to\n\t\t\t\tif curStream.pre != _EMPTY_ {\n\t\t\t\t\terr := &configErr{tk, \"Stream import can not have a 'prefix' and a 'to' property\"}\n\t\t\t\t\t*errors = append(*errors, err)\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t}\n\t\tcase \"share\":\n\t\t\tshare = mv.(bool)\n\t\t\tif curService != nil {\n\t\t\t\tcurService.share = share\n\t\t\t}\n\t\tcase \"allow_trace\":\n\t\t\tif curService != nil {\n\t\t\t\terr := &configErr{tk, \"Detected allow_trace directive on a non-stream\"}\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tatrcSeen = true\n\t\t\tatrc = mv.(bool)\n\t\t\tatrcToken = tk\n\t\t\tif curStream != nil {\n\t\t\t\tcurStream.atrc = atrc\n\t\t\t}\n\t\tdefault:\n\t\t\tif !tk.IsUsedVariable() {\n\t\t\t\terr := &unknownConfigFieldErr{\n\t\t\t\t\tfield: mk,\n\t\t\t\t\tconfigErr: configErr{\n\t\t\t\t\t\ttoken: tk,\n\t\t\t\t\t},\n\t\t\t\t}\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t}\n\t\t}\n\n\t}\n\treturn curStream, curService, nil\n}\n\n// Apply permission defaults to users/nkeyuser that don't have their own.\nfunc applyDefaultPermissions(users []*User, nkeys []*NkeyUser, defaultP *Permissions) {\n\tif defaultP == nil {\n\t\treturn\n\t}\n\tfor _, user := range users {\n\t\tif user.Permissions == nil {\n\t\t\tuser.Permissions = defaultP\n\t\t}\n\t}\n\tfor _, user := range nkeys {\n\t\tif user.Permissions == nil {\n\t\t\tuser.Permissions = defaultP\n\t\t}\n\t}\n}\n\n// Helper function to parse Authorization configs.\nfunc parseAuthorization(v any, errors, warnings *[]error) (*authorization, error) {\n\tvar (\n\t\tam   map[string]any\n\t\ttk   token\n\t\tlt   token\n\t\tauth = &authorization{}\n\t)\n\tdefer convertPanicToErrorList(&lt, errors)\n\n\t_, v = unwrapValue(v, &lt)\n\tam = v.(map[string]any)\n\tfor mk, mv := range am {\n\t\ttk, mv = unwrapValue(mv, &lt)\n\t\tswitch strings.ToLower(mk) {\n\t\tcase \"user\", \"username\":\n\t\t\tauth.user = mv.(string)\n\t\tcase \"pass\", \"password\":\n\t\t\tauth.pass = mv.(string)\n\t\tcase \"token\":\n\t\t\tauth.token = mv.(string)\n\t\tcase \"timeout\":\n\t\t\tat := float64(0)\n\t\t\tswitch mv := mv.(type) {\n\t\t\tcase int64:\n\t\t\t\tat = float64(mv)\n\t\t\tcase float64:\n\t\t\t\tat = mv\n\t\t\tcase string:\n\t\t\t\td, err := time.ParseDuration(mv)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn nil, &configErr{tk, fmt.Sprintf(\"error parsing authorization config, 'timeout' %s\", err)}\n\t\t\t\t}\n\t\t\t\tat = d.Seconds()\n\t\t\tdefault:\n\t\t\t\treturn nil, &configErr{tk, \"error parsing authorization config, 'timeout' wrong type\"}\n\t\t\t}\n\t\t\tif at > (60 * time.Second).Seconds() {\n\t\t\t\treason := fmt.Sprintf(\"timeout of %v (%f seconds) is high, consider keeping it under 60 seconds. possibly caused by unquoted duration; use '1m' instead of 1m, for example\", mv, at)\n\t\t\t\t*warnings = append(*warnings, &configWarningErr{field: mk, configErr: configErr{token: tk, reason: reason}})\n\t\t\t}\n\t\t\tauth.timeout = at\n\t\tcase \"users\":\n\t\t\tnkeys, users, err := parseUsers(tk, errors)\n\t\t\tif err != nil {\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tauth.users = users\n\t\t\tauth.nkeys = nkeys\n\t\tcase \"default_permission\", \"default_permissions\", \"permissions\":\n\t\t\tpermissions, err := parseUserPermissions(tk, errors)\n\t\t\tif err != nil {\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tauth.defaultPermissions = permissions\n\t\tcase \"auth_callout\", \"auth_hook\":\n\t\t\tac, err := parseAuthCallout(tk, errors)\n\t\t\tif err != nil {\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tauth.callout = ac\n\t\tdefault:\n\t\t\tif !tk.IsUsedVariable() {\n\t\t\t\terr := &unknownConfigFieldErr{\n\t\t\t\t\tfield: mk,\n\t\t\t\t\tconfigErr: configErr{\n\t\t\t\t\t\ttoken: tk,\n\t\t\t\t\t},\n\t\t\t\t}\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\n\t\tapplyDefaultPermissions(auth.users, auth.nkeys, auth.defaultPermissions)\n\t}\n\treturn auth, nil\n}\n\n// Helper function to parse multiple users array with optional permissions.\nfunc parseUsers(mv any, errors *[]error) ([]*NkeyUser, []*User, error) {\n\tvar (\n\t\ttk    token\n\t\tlt    token\n\t\tkeys  []*NkeyUser\n\t\tusers = []*User{}\n\t)\n\tdefer convertPanicToErrorList(&lt, errors)\n\ttk, mv = unwrapValue(mv, &lt)\n\n\t// Make sure we have an array\n\tuv, ok := mv.([]any)\n\tif !ok {\n\t\treturn nil, nil, &configErr{tk, fmt.Sprintf(\"Expected users field to be an array, got %v\", mv)}\n\t}\n\tfor _, u := range uv {\n\t\ttk, u = unwrapValue(u, &lt)\n\n\t\t// Check its a map/struct\n\t\tum, ok := u.(map[string]any)\n\t\tif !ok {\n\t\t\terr := &configErr{tk, fmt.Sprintf(\"Expected user entry to be a map/struct, got %v\", u)}\n\t\t\t*errors = append(*errors, err)\n\t\t\tcontinue\n\t\t}\n\n\t\tvar (\n\t\t\tuser  = &User{}\n\t\t\tnkey  = &NkeyUser{}\n\t\t\tperms *Permissions\n\t\t\terr   error\n\t\t)\n\t\tfor k, v := range um {\n\t\t\t// Also needs to unwrap first\n\t\t\ttk, v = unwrapValue(v, &lt)\n\n\t\t\tswitch strings.ToLower(k) {\n\t\t\tcase \"nkey\":\n\t\t\t\tnkey.Nkey = v.(string)\n\t\t\tcase \"user\", \"username\":\n\t\t\t\tuser.Username = v.(string)\n\t\t\tcase \"pass\", \"password\":\n\t\t\t\tuser.Password = v.(string)\n\t\t\tcase \"permission\", \"permissions\", \"authorization\":\n\t\t\t\tperms, err = parseUserPermissions(tk, errors)\n\t\t\t\tif err != nil {\n\t\t\t\t\t*errors = append(*errors, err)\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\tcase \"allowed_connection_types\", \"connection_types\", \"clients\":\n\t\t\t\tcts := parseAllowedConnectionTypes(tk, &lt, v, errors)\n\t\t\t\tnkey.AllowedConnectionTypes = cts\n\t\t\t\tuser.AllowedConnectionTypes = cts\n\t\t\tdefault:\n\t\t\t\tif !tk.IsUsedVariable() {\n\t\t\t\t\terr := &unknownConfigFieldErr{\n\t\t\t\t\t\tfield: k,\n\t\t\t\t\t\tconfigErr: configErr{\n\t\t\t\t\t\t\ttoken: tk,\n\t\t\t\t\t\t},\n\t\t\t\t\t}\n\t\t\t\t\t*errors = append(*errors, err)\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t// Place perms if we have them.\n\t\tif perms != nil {\n\t\t\t// nkey takes precedent.\n\t\t\tif nkey.Nkey != _EMPTY_ {\n\t\t\t\tnkey.Permissions = perms\n\t\t\t} else {\n\t\t\t\tuser.Permissions = perms\n\t\t\t}\n\t\t}\n\n\t\t// Check to make sure we have at least an nkey or username <password> defined.\n\t\tif nkey.Nkey == _EMPTY_ && user.Username == _EMPTY_ {\n\t\t\treturn nil, nil, &configErr{tk, \"User entry requires a user\"}\n\t\t} else if nkey.Nkey != _EMPTY_ {\n\t\t\t// Make sure the nkey a proper public nkey for a user..\n\t\t\tif !nkeys.IsValidPublicUserKey(nkey.Nkey) {\n\t\t\t\treturn nil, nil, &configErr{tk, \"Not a valid public nkey for a user\"}\n\t\t\t}\n\t\t\t// If we have user or password defined here that is an error.\n\t\t\tif user.Username != _EMPTY_ || user.Password != _EMPTY_ {\n\t\t\t\treturn nil, nil, &configErr{tk, \"Nkey users do not take usernames or passwords\"}\n\t\t\t}\n\t\t\tkeys = append(keys, nkey)\n\t\t} else {\n\t\t\tusers = append(users, user)\n\t\t}\n\t}\n\treturn keys, users, nil\n}\n\nfunc parseAllowedConnectionTypes(tk token, lt *token, mv any, errors *[]error) map[string]struct{} {\n\tcts, err := parseStringArray(\"allowed connection types\", tk, lt, mv, errors)\n\t// If error, it has already been added to the `errors` array, simply return\n\tif err != nil {\n\t\treturn nil\n\t}\n\tm, err := convertAllowedConnectionTypes(cts)\n\tif err != nil {\n\t\t*errors = append(*errors, &configErr{tk, err.Error()})\n\t}\n\treturn m\n}\n\n// Helper function to parse auth callouts.\nfunc parseAuthCallout(mv any, errors *[]error) (*AuthCallout, error) {\n\tvar (\n\t\ttk token\n\t\tlt token\n\t\tac = &AuthCallout{}\n\t)\n\tdefer convertPanicToErrorList(&lt, errors)\n\n\ttk, mv = unwrapValue(mv, &lt)\n\tpm, ok := mv.(map[string]any)\n\tif !ok {\n\t\treturn nil, &configErr{tk, fmt.Sprintf(\"Expected authorization callout to be a map/struct, got %+v\", mv)}\n\t}\n\tfor k, v := range pm {\n\t\ttk, mv = unwrapValue(v, &lt)\n\n\t\tswitch strings.ToLower(k) {\n\t\tcase \"issuer\":\n\t\t\tac.Issuer = mv.(string)\n\t\t\tif !nkeys.IsValidPublicAccountKey(ac.Issuer) {\n\t\t\t\treturn nil, &configErr{tk, fmt.Sprintf(\"Expected callout user to be a valid public account nkey, got %q\", ac.Issuer)}\n\t\t\t}\n\t\tcase \"account\", \"acc\":\n\t\t\tac.Account = mv.(string)\n\t\tcase \"auth_users\", \"users\":\n\t\t\taua, ok := mv.([]any)\n\t\t\tif !ok {\n\t\t\t\treturn nil, &configErr{tk, fmt.Sprintf(\"Expected auth_users field to be an array, got %T\", v)}\n\t\t\t}\n\t\t\tfor _, uv := range aua {\n\t\t\t\t_, uv = unwrapValue(uv, &lt)\n\t\t\t\tac.AuthUsers = append(ac.AuthUsers, uv.(string))\n\t\t\t}\n\t\tcase \"xkey\", \"key\":\n\t\t\tac.XKey = mv.(string)\n\t\t\tif !nkeys.IsValidPublicCurveKey(ac.XKey) {\n\t\t\t\treturn nil, &configErr{tk, fmt.Sprintf(\"Expected callout xkey to be a valid public xkey, got %q\", ac.XKey)}\n\t\t\t}\n\t\tcase \"allowed_accounts\":\n\t\t\taua, ok := mv.([]any)\n\t\t\tif !ok {\n\t\t\t\treturn nil, &configErr{tk, fmt.Sprintf(\"Expected allowed accounts field to be an array, got %T\", v)}\n\t\t\t}\n\t\t\tfor _, uv := range aua {\n\t\t\t\t_, uv = unwrapValue(uv, &lt)\n\t\t\t\tac.AllowedAccounts = append(ac.AllowedAccounts, uv.(string))\n\t\t\t}\n\t\tdefault:\n\t\t\tif !tk.IsUsedVariable() {\n\t\t\t\terr := &configErr{tk, fmt.Sprintf(\"Unknown field %q parsing authorization callout\", k)}\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t}\n\t\t}\n\t}\n\t// Make sure we have all defined. All fields are required.\n\t// If no account specified, selet $G.\n\tif ac.Account == _EMPTY_ {\n\t\tac.Account = globalAccountName\n\t}\n\tif ac.Issuer == _EMPTY_ {\n\t\treturn nil, &configErr{tk, \"Authorization callouts require an issuer to be specified\"}\n\t}\n\tif len(ac.AuthUsers) == 0 {\n\t\treturn nil, &configErr{tk, \"Authorization callouts require authorized users to be specified\"}\n\t}\n\treturn ac, nil\n}\n\n// Helper function to parse user/account permissions\nfunc parseUserPermissions(mv any, errors *[]error) (*Permissions, error) {\n\tvar (\n\t\ttk token\n\t\tlt token\n\t\tp  = &Permissions{}\n\t)\n\tdefer convertPanicToErrorList(&lt, errors)\n\n\ttk, mv = unwrapValue(mv, &lt)\n\tpm, ok := mv.(map[string]any)\n\tif !ok {\n\t\treturn nil, &configErr{tk, fmt.Sprintf(\"Expected permissions to be a map/struct, got %+v\", mv)}\n\t}\n\tfor k, v := range pm {\n\t\ttk, mv = unwrapValue(v, &lt)\n\n\t\tswitch strings.ToLower(k) {\n\t\t// For routes:\n\t\t// Import is Publish\n\t\t// Export is Subscribe\n\t\tcase \"pub\", \"publish\", \"import\":\n\t\t\tperms, err := parseVariablePermissions(mv, errors)\n\t\t\tif err != nil {\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tp.Publish = perms\n\t\tcase \"sub\", \"subscribe\", \"export\":\n\t\t\tperms, err := parseVariablePermissions(mv, errors)\n\t\t\tif err != nil {\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tp.Subscribe = perms\n\t\tcase \"publish_allow_responses\", \"allow_responses\":\n\t\t\trp := &ResponsePermission{\n\t\t\t\tMaxMsgs: DEFAULT_ALLOW_RESPONSE_MAX_MSGS,\n\t\t\t\tExpires: DEFAULT_ALLOW_RESPONSE_EXPIRATION,\n\t\t\t}\n\t\t\t// Try boolean first\n\t\t\tresponses, ok := mv.(bool)\n\t\t\tif ok {\n\t\t\t\tif responses {\n\t\t\t\t\tp.Response = rp\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tp.Response = parseAllowResponses(v, errors)\n\t\t\t}\n\t\t\tif p.Response != nil {\n\t\t\t\tif p.Publish == nil {\n\t\t\t\t\tp.Publish = &SubjectPermission{}\n\t\t\t\t}\n\t\t\t\tif p.Publish.Allow == nil {\n\t\t\t\t\t// We turn off the blanket allow statement.\n\t\t\t\t\tp.Publish.Allow = []string{}\n\t\t\t\t}\n\t\t\t}\n\t\tdefault:\n\t\t\tif !tk.IsUsedVariable() {\n\t\t\t\terr := &configErr{tk, fmt.Sprintf(\"Unknown field %q parsing permissions\", k)}\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t}\n\t\t}\n\t}\n\treturn p, nil\n}\n\n// Top level parser for authorization configurations.\nfunc parseVariablePermissions(v any, errors *[]error) (*SubjectPermission, error) {\n\tswitch vv := v.(type) {\n\tcase map[string]any:\n\t\t// New style with allow and/or deny properties.\n\t\treturn parseSubjectPermission(vv, errors)\n\tdefault:\n\t\t// Old style\n\t\treturn parseOldPermissionStyle(v, errors)\n\t}\n}\n\n// Helper function to parse subject singletons and/or arrays\nfunc parsePermSubjects(v any, errors *[]error) ([]string, error) {\n\tvar lt token\n\tdefer convertPanicToErrorList(&lt, errors)\n\n\ttk, v := unwrapValue(v, &lt)\n\n\tvar subjects []string\n\tswitch vv := v.(type) {\n\tcase string:\n\t\tsubjects = append(subjects, vv)\n\tcase []string:\n\t\tsubjects = vv\n\tcase []any:\n\t\tfor _, i := range vv {\n\t\t\ttk, i := unwrapValue(i, &lt)\n\n\t\t\tsubject, ok := i.(string)\n\t\t\tif !ok {\n\t\t\t\treturn nil, &configErr{tk, \"Subject in permissions array cannot be cast to string\"}\n\t\t\t}\n\t\t\tsubjects = append(subjects, subject)\n\t\t}\n\tdefault:\n\t\treturn nil, &configErr{tk, fmt.Sprintf(\"Expected subject permissions to be a subject, or array of subjects, got %T\", v)}\n\t}\n\tif err := checkPermSubjectArray(subjects); err != nil {\n\t\treturn nil, &configErr{tk, err.Error()}\n\t}\n\treturn subjects, nil\n}\n\n// Helper function to parse a ResponsePermission.\nfunc parseAllowResponses(v any, errors *[]error) *ResponsePermission {\n\tvar lt token\n\tdefer convertPanicToErrorList(&lt, errors)\n\n\ttk, v := unwrapValue(v, &lt)\n\t// Check if this is a map.\n\tpm, ok := v.(map[string]any)\n\tif !ok {\n\t\terr := &configErr{tk, \"error parsing response permissions, expected a boolean or a map\"}\n\t\t*errors = append(*errors, err)\n\t\treturn nil\n\t}\n\n\trp := &ResponsePermission{\n\t\tMaxMsgs: DEFAULT_ALLOW_RESPONSE_MAX_MSGS,\n\t\tExpires: DEFAULT_ALLOW_RESPONSE_EXPIRATION,\n\t}\n\n\tfor k, v := range pm {\n\t\ttk, v = unwrapValue(v, &lt)\n\t\tswitch strings.ToLower(k) {\n\t\tcase \"max\", \"max_msgs\", \"max_messages\", \"max_responses\":\n\t\t\tmax := int(v.(int64))\n\t\t\t// Negative values are accepted (mean infinite), and 0\n\t\t\t// means default value (set above).\n\t\t\tif max != 0 {\n\t\t\t\trp.MaxMsgs = max\n\t\t\t}\n\t\tcase \"expires\", \"expiration\", \"ttl\":\n\t\t\twd, ok := v.(string)\n\t\t\tif ok {\n\t\t\t\tttl, err := time.ParseDuration(wd)\n\t\t\t\tif err != nil {\n\t\t\t\t\terr := &configErr{tk, fmt.Sprintf(\"error parsing expires: %v\", err)}\n\t\t\t\t\t*errors = append(*errors, err)\n\t\t\t\t\treturn nil\n\t\t\t\t}\n\t\t\t\t// Negative values are accepted (mean infinite), and 0\n\t\t\t\t// means default value (set above).\n\t\t\t\tif ttl != 0 {\n\t\t\t\t\trp.Expires = ttl\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\terr := &configErr{tk, \"error parsing expires, not a duration string\"}\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t\treturn nil\n\t\t\t}\n\t\tdefault:\n\t\t\tif !tk.IsUsedVariable() {\n\t\t\t\terr := &configErr{tk, fmt.Sprintf(\"Unknown field %q parsing permissions\", k)}\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t}\n\t\t}\n\t}\n\treturn rp\n}\n\n// Helper function to parse old style authorization configs.\nfunc parseOldPermissionStyle(v any, errors *[]error) (*SubjectPermission, error) {\n\tsubjects, err := parsePermSubjects(v, errors)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &SubjectPermission{Allow: subjects}, nil\n}\n\n// Helper function to parse new style authorization into a SubjectPermission with Allow and Deny.\nfunc parseSubjectPermission(v any, errors *[]error) (*SubjectPermission, error) {\n\tvar lt token\n\tdefer convertPanicToErrorList(&lt, errors)\n\n\tm := v.(map[string]any)\n\tif len(m) == 0 {\n\t\treturn nil, nil\n\t}\n\tp := &SubjectPermission{}\n\tfor k, v := range m {\n\t\ttk, _ := unwrapValue(v, &lt)\n\t\tswitch strings.ToLower(k) {\n\t\tcase \"allow\":\n\t\t\tsubjects, err := parsePermSubjects(tk, errors)\n\t\t\tif err != nil {\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tp.Allow = subjects\n\t\tcase \"deny\":\n\t\t\tsubjects, err := parsePermSubjects(tk, errors)\n\t\t\tif err != nil {\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tp.Deny = subjects\n\t\tdefault:\n\t\t\tif !tk.IsUsedVariable() {\n\t\t\t\terr := &configErr{tk, fmt.Sprintf(\"Unknown field name %q parsing subject permissions, only 'allow' or 'deny' are permitted\", k)}\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t}\n\t\t}\n\t}\n\treturn p, nil\n}\n\n// Helper function to validate permissions subjects.\nfunc checkPermSubjectArray(sa []string) error {\n\tfor _, s := range sa {\n\t\tif !IsValidSubject(s) {\n\t\t\t// Check here if this is a queue group qualified subject.\n\t\t\telements := strings.Fields(s)\n\t\t\tif len(elements) != 2 {\n\t\t\t\treturn fmt.Errorf(\"subject %q is not a valid subject\", s)\n\t\t\t} else if !IsValidSubject(elements[0]) {\n\t\t\t\treturn fmt.Errorf(\"subject %q is not a valid subject\", elements[0])\n\t\t\t}\n\t\t}\n\t}\n\treturn nil\n}\n\n// PrintTLSHelpAndDie prints TLS usage and exits.\nfunc PrintTLSHelpAndDie() {\n\tfmt.Printf(\"%s\", tlsUsage)\n\tfor k := range cipherMap {\n\t\tfmt.Printf(\"    %s\\n\", k)\n\t}\n\tfmt.Printf(\"\\nAvailable curve preferences include:\\n\")\n\tfor k := range curvePreferenceMap {\n\t\tfmt.Printf(\"    %s\\n\", k)\n\t}\n\tif runtime.GOOS == \"windows\" {\n\t\tfmt.Printf(\"%s\\n\", certstore.Usage)\n\t}\n\tfmt.Printf(\"%s\", certidp.OCSPPeerUsage)\n\tfmt.Printf(\"%s\", OCSPResponseCacheUsage)\n\tos.Exit(0)\n}\n\nfunc parseCipher(cipherName string) (uint16, error) {\n\tcipher, exists := cipherMap[cipherName]\n\tif !exists {\n\t\treturn 0, fmt.Errorf(\"unrecognized cipher %s\", cipherName)\n\t}\n\n\treturn cipher, nil\n}\n\nfunc parseCurvePreferences(curveName string) (tls.CurveID, error) {\n\tcurve, exists := curvePreferenceMap[curveName]\n\tif !exists {\n\t\treturn 0, fmt.Errorf(\"unrecognized curve preference %s\", curveName)\n\t}\n\treturn curve, nil\n}\n\nfunc parseTLSVersion(v any) (uint16, error) {\n\tvar tlsVersionNumber uint16\n\tswitch v := v.(type) {\n\tcase string:\n\t\tn, err := tlsVersionFromString(v)\n\t\tif err != nil {\n\t\t\treturn 0, err\n\t\t}\n\t\ttlsVersionNumber = n\n\tdefault:\n\t\treturn 0, fmt.Errorf(\"'min_version' wrong type: %v\", v)\n\t}\n\tif tlsVersionNumber < tls.VersionTLS12 {\n\t\treturn 0, fmt.Errorf(\"unsupported TLS version: %s\", tls.VersionName(tlsVersionNumber))\n\t}\n\treturn tlsVersionNumber, nil\n}\n\n// Helper function to parse TLS configs.\nfunc parseTLS(v any, isClientCtx bool) (t *TLSConfigOpts, retErr error) {\n\tvar (\n\t\ttlsm map[string]any\n\t\ttc   = TLSConfigOpts{}\n\t\tlt   token\n\t)\n\tdefer convertPanicToError(&lt, &retErr)\n\n\ttk, v := unwrapValue(v, &lt)\n\ttlsm = v.(map[string]any)\n\tfor mk, mv := range tlsm {\n\t\ttk, mv := unwrapValue(mv, &lt)\n\t\tswitch strings.ToLower(mk) {\n\t\tcase \"cert_file\":\n\t\t\tcertFile, ok := mv.(string)\n\t\t\tif !ok {\n\t\t\t\treturn nil, &configErr{tk, \"error parsing tls config, expected 'cert_file' to be filename\"}\n\t\t\t}\n\t\t\ttc.CertFile = certFile\n\t\tcase \"key_file\":\n\t\t\tkeyFile, ok := mv.(string)\n\t\t\tif !ok {\n\t\t\t\treturn nil, &configErr{tk, \"error parsing tls config, expected 'key_file' to be filename\"}\n\t\t\t}\n\t\t\ttc.KeyFile = keyFile\n\t\tcase \"ca_file\":\n\t\t\tcaFile, ok := mv.(string)\n\t\t\tif !ok {\n\t\t\t\treturn nil, &configErr{tk, \"error parsing tls config, expected 'ca_file' to be filename\"}\n\t\t\t}\n\t\t\ttc.CaFile = caFile\n\t\tcase \"insecure\":\n\t\t\tinsecure, ok := mv.(bool)\n\t\t\tif !ok {\n\t\t\t\treturn nil, &configErr{tk, \"error parsing tls config, expected 'insecure' to be a boolean\"}\n\t\t\t}\n\t\t\ttc.Insecure = insecure\n\t\tcase \"verify\":\n\t\t\tverify, ok := mv.(bool)\n\t\t\tif !ok {\n\t\t\t\treturn nil, &configErr{tk, \"error parsing tls config, expected 'verify' to be a boolean\"}\n\t\t\t}\n\t\t\ttc.Verify = verify\n\t\tcase \"verify_and_map\":\n\t\t\tverify, ok := mv.(bool)\n\t\t\tif !ok {\n\t\t\t\treturn nil, &configErr{tk, \"error parsing tls config, expected 'verify_and_map' to be a boolean\"}\n\t\t\t}\n\t\t\tif verify {\n\t\t\t\ttc.Verify = verify\n\t\t\t}\n\t\t\ttc.Map = verify\n\t\tcase \"verify_cert_and_check_known_urls\":\n\t\t\tverify, ok := mv.(bool)\n\t\t\tif !ok {\n\t\t\t\treturn nil, &configErr{tk, \"error parsing tls config, expected 'verify_cert_and_check_known_urls' to be a boolean\"}\n\t\t\t}\n\t\t\tif verify && isClientCtx {\n\t\t\t\treturn nil, &configErr{tk, \"verify_cert_and_check_known_urls not supported in this context\"}\n\t\t\t}\n\t\t\tif verify {\n\t\t\t\ttc.Verify = verify\n\t\t\t}\n\t\t\ttc.TLSCheckKnownURLs = verify\n\t\tcase \"cipher_suites\":\n\t\t\tra := mv.([]any)\n\t\t\tif len(ra) == 0 {\n\t\t\t\treturn nil, &configErr{tk, \"error parsing tls config, 'cipher_suites' cannot be empty\"}\n\t\t\t}\n\t\t\ttc.Ciphers = make([]uint16, 0, len(ra))\n\t\t\tfor _, r := range ra {\n\t\t\t\ttk, r := unwrapValue(r, &lt)\n\t\t\t\tcipher, err := parseCipher(r.(string))\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn nil, &configErr{tk, err.Error()}\n\t\t\t\t}\n\t\t\t\ttc.Ciphers = append(tc.Ciphers, cipher)\n\t\t\t}\n\t\tcase \"curve_preferences\":\n\t\t\tra := mv.([]any)\n\t\t\tif len(ra) == 0 {\n\t\t\t\treturn nil, &configErr{tk, \"error parsing tls config, 'curve_preferences' cannot be empty\"}\n\t\t\t}\n\t\t\ttc.CurvePreferences = make([]tls.CurveID, 0, len(ra))\n\t\t\tfor _, r := range ra {\n\t\t\t\ttk, r := unwrapValue(r, &lt)\n\t\t\t\tcps, err := parseCurvePreferences(r.(string))\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn nil, &configErr{tk, err.Error()}\n\t\t\t\t}\n\t\t\t\ttc.CurvePreferences = append(tc.CurvePreferences, cps)\n\t\t\t}\n\t\tcase \"timeout\":\n\t\t\tat := float64(0)\n\t\t\tswitch mv := mv.(type) {\n\t\t\tcase int64:\n\t\t\t\tat = float64(mv)\n\t\t\tcase float64:\n\t\t\t\tat = mv\n\t\t\tcase string:\n\t\t\t\td, err := time.ParseDuration(mv)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn nil, &configErr{tk, fmt.Sprintf(\"error parsing tls config, 'timeout' %s\", err)}\n\t\t\t\t}\n\t\t\t\tat = d.Seconds()\n\t\t\tdefault:\n\t\t\t\treturn nil, &configErr{tk, \"error parsing tls config, 'timeout' wrong type\"}\n\t\t\t}\n\t\t\ttc.Timeout = at\n\t\tcase \"connection_rate_limit\":\n\t\t\tat := int64(0)\n\t\t\tswitch mv := mv.(type) {\n\t\t\tcase int64:\n\t\t\t\tat = mv\n\t\t\tdefault:\n\t\t\t\treturn nil, &configErr{tk, \"error parsing tls config, 'connection_rate_limit' wrong type\"}\n\t\t\t}\n\t\t\ttc.RateLimit = at\n\t\tcase \"pinned_certs\":\n\t\t\tra, ok := mv.([]any)\n\t\t\tif !ok {\n\t\t\t\treturn nil, &configErr{tk, \"error parsing tls config, expected 'pinned_certs' to be a list of hex-encoded sha256 of DER encoded SubjectPublicKeyInfo\"}\n\t\t\t}\n\t\t\tif len(ra) != 0 {\n\t\t\t\twl := PinnedCertSet{}\n\t\t\t\tre := regexp.MustCompile(\"^[A-Fa-f0-9]{64}$\")\n\t\t\t\tfor _, r := range ra {\n\t\t\t\t\ttk, r := unwrapValue(r, &lt)\n\t\t\t\t\tentry := strings.ToLower(r.(string))\n\t\t\t\t\tif !re.MatchString(entry) {\n\t\t\t\t\t\treturn nil, &configErr{tk, fmt.Sprintf(\"error parsing tls config, 'pinned_certs' key %s does not look like hex-encoded sha256 of DER encoded SubjectPublicKeyInfo\", entry)}\n\t\t\t\t\t}\n\t\t\t\t\twl[entry] = struct{}{}\n\t\t\t\t}\n\t\t\t\ttc.PinnedCerts = wl\n\t\t\t}\n\t\tcase \"cert_store\":\n\t\t\tcertStore, ok := mv.(string)\n\t\t\tif !ok || certStore == _EMPTY_ {\n\t\t\t\treturn nil, &configErr{tk, certstore.ErrBadCertStoreField.Error()}\n\t\t\t}\n\t\t\tcertStoreType, err := certstore.ParseCertStore(certStore)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, &configErr{tk, err.Error()}\n\t\t\t}\n\t\t\ttc.CertStore = certStoreType\n\t\tcase \"cert_match_by\":\n\t\t\tcertMatchBy, ok := mv.(string)\n\t\t\tif !ok || certMatchBy == _EMPTY_ {\n\t\t\t\treturn nil, &configErr{tk, certstore.ErrBadCertMatchByField.Error()}\n\t\t\t}\n\t\t\tcertMatchByType, err := certstore.ParseCertMatchBy(certMatchBy)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, &configErr{tk, err.Error()}\n\t\t\t}\n\t\t\ttc.CertMatchBy = certMatchByType\n\t\tcase \"cert_match\":\n\t\t\tcertMatch, ok := mv.(string)\n\t\t\tif !ok || certMatch == _EMPTY_ {\n\t\t\t\treturn nil, &configErr{tk, certstore.ErrBadCertMatchField.Error()}\n\t\t\t}\n\t\t\ttc.CertMatch = certMatch\n\t\tcase \"ca_certs_match\":\n\t\t\trv := []string{}\n\t\t\tswitch mv := mv.(type) {\n\t\t\tcase string:\n\t\t\t\trv = append(rv, mv)\n\t\t\tcase []string:\n\t\t\t\trv = append(rv, mv...)\n\t\t\tcase []any:\n\t\t\t\tfor _, t := range mv {\n\t\t\t\t\tif token, ok := t.(token); ok {\n\t\t\t\t\t\tif ts, ok := token.Value().(string); ok {\n\t\t\t\t\t\t\trv = append(rv, ts)\n\t\t\t\t\t\t\tcontinue\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\treturn nil, &configErr{tk, fmt.Sprintf(\"error parsing ca_cert_match: unsupported type %T where string is expected\", token)}\n\t\t\t\t\t\t}\n\t\t\t\t\t} else {\n\t\t\t\t\t\treturn nil, &configErr{tk, fmt.Sprintf(\"error parsing ca_cert_match: unsupported type %T\", t)}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\ttc.CaCertsMatch = rv\n\t\tcase \"handshake_first\", \"first\", \"immediate\":\n\t\t\tswitch mv := mv.(type) {\n\t\t\tcase bool:\n\t\t\t\ttc.HandshakeFirst = mv\n\t\t\tcase string:\n\t\t\t\tswitch strings.ToLower(mv) {\n\t\t\t\tcase \"true\", \"on\":\n\t\t\t\t\ttc.HandshakeFirst = true\n\t\t\t\tcase \"false\", \"off\":\n\t\t\t\t\ttc.HandshakeFirst = false\n\t\t\t\tcase \"auto\", \"auto_fallback\":\n\t\t\t\t\ttc.HandshakeFirst = true\n\t\t\t\t\ttc.FallbackDelay = DEFAULT_TLS_HANDSHAKE_FIRST_FALLBACK_DELAY\n\t\t\t\tdefault:\n\t\t\t\t\t// Check to see if this is a duration.\n\t\t\t\t\tif dur, err := time.ParseDuration(mv); err == nil {\n\t\t\t\t\t\ttc.HandshakeFirst = true\n\t\t\t\t\t\ttc.FallbackDelay = dur\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\t\t\t\t\treturn nil, &configErr{tk, fmt.Sprintf(\"field %q's value %q is invalid\", mk, mv)}\n\t\t\t\t}\n\t\t\tdefault:\n\t\t\t\treturn nil, &configErr{tk, fmt.Sprintf(\"field %q should be a boolean or a string, got %T\", mk, mv)}\n\t\t\t}\n\t\tcase \"cert_match_skip_invalid\":\n\t\t\tcertMatchSkipInvalid, ok := mv.(bool)\n\t\t\tif !ok {\n\t\t\t\treturn nil, &configErr{tk, certstore.ErrBadCertMatchSkipInvalidField.Error()}\n\t\t\t}\n\t\t\ttc.CertMatchSkipInvalid = certMatchSkipInvalid\n\t\tcase \"ocsp_peer\":\n\t\t\tswitch vv := mv.(type) {\n\t\t\tcase bool:\n\t\t\t\tpc := certidp.NewOCSPPeerConfig()\n\t\t\t\tif vv {\n\t\t\t\t\t// Set enabled\n\t\t\t\t\tpc.Verify = true\n\t\t\t\t\ttc.OCSPPeerConfig = pc\n\t\t\t\t} else {\n\t\t\t\t\t// Set disabled\n\t\t\t\t\tpc.Verify = false\n\t\t\t\t\ttc.OCSPPeerConfig = pc\n\t\t\t\t}\n\t\t\tcase map[string]any:\n\t\t\t\tpc, err := parseOCSPPeer(mv)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn nil, &configErr{tk, err.Error()}\n\t\t\t\t}\n\t\t\t\ttc.OCSPPeerConfig = pc\n\t\t\tdefault:\n\t\t\t\treturn nil, &configErr{tk, fmt.Sprintf(\"error parsing ocsp peer config: unsupported type %T\", v)}\n\t\t\t}\n\t\tcase \"certs\", \"certificates\":\n\t\t\tcerts, ok := mv.([]any)\n\t\t\tif !ok {\n\t\t\t\treturn nil, &configErr{tk, fmt.Sprintf(\"error parsing certificates config: unsupported type %T\", v)}\n\t\t\t}\n\t\t\ttc.Certificates = make([]*TLSCertPairOpt, len(certs))\n\t\t\tfor i, v := range certs {\n\t\t\t\ttk, vv := unwrapValue(v, &lt)\n\t\t\t\tpair, ok := vv.(map[string]any)\n\t\t\t\tif !ok {\n\t\t\t\t\treturn nil, &configErr{tk, fmt.Sprintf(\"error parsing certificates config: unsupported type %T\", vv)}\n\t\t\t\t}\n\t\t\t\tcertPair := &TLSCertPairOpt{}\n\t\t\t\tfor k, v := range pair {\n\t\t\t\t\ttk, vv = unwrapValue(v, &lt)\n\t\t\t\t\tfile, ok := vv.(string)\n\t\t\t\t\tif !ok {\n\t\t\t\t\t\treturn nil, &configErr{tk, fmt.Sprintf(\"error parsing certificates config: unsupported type %T\", vv)}\n\t\t\t\t\t}\n\t\t\t\t\tswitch k {\n\t\t\t\t\tcase \"cert_file\":\n\t\t\t\t\t\tcertPair.CertFile = file\n\t\t\t\t\tcase \"key_file\":\n\t\t\t\t\t\tcertPair.KeyFile = file\n\t\t\t\t\tdefault:\n\t\t\t\t\t\treturn nil, &configErr{tk, fmt.Sprintf(\"error parsing tls certs config, unknown field %q\", k)}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif certPair.CertFile == _EMPTY_ || certPair.KeyFile == _EMPTY_ {\n\t\t\t\t\treturn nil, &configErr{tk, \"error parsing certificates config: both 'cert_file' and 'cert_key' options are required\"}\n\t\t\t\t}\n\t\t\t\ttc.Certificates[i] = certPair\n\t\t\t}\n\t\tcase \"min_version\":\n\t\t\tminVersion, err := parseTLSVersion(mv)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, &configErr{tk, fmt.Sprintf(\"error parsing tls config: %v\", err)}\n\t\t\t}\n\t\t\ttc.MinVersion = minVersion\n\t\tdefault:\n\t\t\treturn nil, &configErr{tk, fmt.Sprintf(\"error parsing tls config, unknown field %q\", mk)}\n\t\t}\n\t}\n\tif len(tc.Certificates) > 0 && tc.CertFile != _EMPTY_ {\n\t\treturn nil, &configErr{tk, \"error parsing tls config, cannot combine 'cert_file' option with 'certs' option\"}\n\t}\n\n\t// If cipher suites were not specified then use the defaults\n\tif tc.Ciphers == nil {\n\t\ttc.Ciphers = defaultCipherSuites()\n\t}\n\n\t// If curve preferences were not specified, then use the defaults\n\tif tc.CurvePreferences == nil {\n\t\ttc.CurvePreferences = defaultCurvePreferences()\n\t}\n\n\treturn &tc, nil\n}\n\nfunc parseSimpleAuth(v any, errors *[]error) *authorization {\n\tvar (\n\t\tam   map[string]any\n\t\ttk   token\n\t\tlt   token\n\t\tauth = &authorization{}\n\t)\n\tdefer convertPanicToErrorList(&lt, errors)\n\n\t_, v = unwrapValue(v, &lt)\n\tam = v.(map[string]any)\n\tfor mk, mv := range am {\n\t\ttk, mv = unwrapValue(mv, &lt)\n\t\tswitch strings.ToLower(mk) {\n\t\tcase \"user\", \"username\":\n\t\t\tauth.user = mv.(string)\n\t\tcase \"pass\", \"password\":\n\t\t\tauth.pass = mv.(string)\n\t\tcase \"token\":\n\t\t\tauth.token = mv.(string)\n\t\tcase \"timeout\":\n\t\t\tat := float64(1)\n\t\t\tswitch mv := mv.(type) {\n\t\t\tcase int64:\n\t\t\t\tat = float64(mv)\n\t\t\tcase float64:\n\t\t\t\tat = mv\n\t\t\t}\n\t\t\tauth.timeout = at\n\t\tdefault:\n\t\t\tif !tk.IsUsedVariable() {\n\t\t\t\terr := &unknownConfigFieldErr{\n\t\t\t\t\tfield: mk,\n\t\t\t\t\tconfigErr: configErr{\n\t\t\t\t\t\ttoken: tk,\n\t\t\t\t\t},\n\t\t\t\t}\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\t}\n\treturn auth\n}\n\nfunc parseStringArray(fieldName string, tk token, lt *token, mv any, errors *[]error) ([]string, error) {\n\tswitch mv := mv.(type) {\n\tcase string:\n\t\treturn []string{mv}, nil\n\tcase []any:\n\t\tstrs := make([]string, 0, len(mv))\n\t\tfor _, val := range mv {\n\t\t\ttk, val = unwrapValue(val, lt)\n\t\t\tif str, ok := val.(string); ok {\n\t\t\t\tstrs = append(strs, str)\n\t\t\t} else {\n\t\t\t\terr := &configErr{tk, fmt.Sprintf(\"error parsing %s: unsupported type in array %T\", fieldName, val)}\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t}\n\t\treturn strs, nil\n\tdefault:\n\t\terr := &configErr{tk, fmt.Sprintf(\"error parsing %s: unsupported type %T\", fieldName, mv)}\n\t\t*errors = append(*errors, err)\n\t\treturn nil, err\n\t}\n}\n\nfunc parseWebsocket(v any, o *Options, errors *[]error) error {\n\tvar lt token\n\tdefer convertPanicToErrorList(&lt, errors)\n\n\ttk, v := unwrapValue(v, &lt)\n\tgm, ok := v.(map[string]any)\n\tif !ok {\n\t\treturn &configErr{tk, fmt.Sprintf(\"Expected websocket to be a map, got %T\", v)}\n\t}\n\tfor mk, mv := range gm {\n\t\t// Again, unwrap token value if line check is required.\n\t\ttk, mv = unwrapValue(mv, &lt)\n\t\tswitch strings.ToLower(mk) {\n\t\tcase \"listen\":\n\t\t\thp, err := parseListen(mv)\n\t\t\tif err != nil {\n\t\t\t\terr := &configErr{tk, err.Error()}\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\to.Websocket.Host = hp.host\n\t\t\to.Websocket.Port = hp.port\n\t\tcase \"port\":\n\t\t\to.Websocket.Port = int(mv.(int64))\n\t\tcase \"host\", \"net\":\n\t\t\to.Websocket.Host = mv.(string)\n\t\tcase \"advertise\":\n\t\t\to.Websocket.Advertise = mv.(string)\n\t\tcase \"no_tls\":\n\t\t\to.Websocket.NoTLS = mv.(bool)\n\t\tcase \"tls\":\n\t\t\ttc, err := parseTLS(tk, true)\n\t\t\tif err != nil {\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif o.Websocket.TLSConfig, err = GenTLSConfig(tc); err != nil {\n\t\t\t\terr := &configErr{tk, err.Error()}\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\to.Websocket.TLSMap = tc.Map\n\t\t\to.Websocket.TLSPinnedCerts = tc.PinnedCerts\n\t\t\to.Websocket.tlsConfigOpts = tc\n\t\tcase \"same_origin\":\n\t\t\to.Websocket.SameOrigin = mv.(bool)\n\t\tcase \"allowed_origins\", \"allowed_origin\", \"allow_origins\", \"allow_origin\", \"origins\", \"origin\":\n\t\t\to.Websocket.AllowedOrigins, _ = parseStringArray(\"allowed origins\", tk, &lt, mv, errors)\n\t\tcase \"handshake_timeout\":\n\t\t\tht := time.Duration(0)\n\t\t\tswitch mv := mv.(type) {\n\t\t\tcase int64:\n\t\t\t\tht = time.Duration(mv) * time.Second\n\t\t\tcase string:\n\t\t\t\tvar err error\n\t\t\t\tht, err = time.ParseDuration(mv)\n\t\t\t\tif err != nil {\n\t\t\t\t\terr := &configErr{tk, err.Error()}\n\t\t\t\t\t*errors = append(*errors, err)\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\tdefault:\n\t\t\t\terr := &configErr{tk, fmt.Sprintf(\"error parsing handshake timeout: unsupported type %T\", mv)}\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t}\n\t\t\to.Websocket.HandshakeTimeout = ht\n\t\tcase \"compress\", \"compression\":\n\t\t\to.Websocket.Compression = mv.(bool)\n\t\tcase \"authorization\", \"authentication\":\n\t\t\tauth := parseSimpleAuth(tk, errors)\n\t\t\to.Websocket.Username = auth.user\n\t\t\to.Websocket.Password = auth.pass\n\t\t\to.Websocket.Token = auth.token\n\t\t\to.Websocket.AuthTimeout = auth.timeout\n\t\tcase \"jwt_cookie\":\n\t\t\to.Websocket.JWTCookie = mv.(string)\n\t\tcase \"user_cookie\":\n\t\t\to.Websocket.UsernameCookie = mv.(string)\n\t\tcase \"pass_cookie\":\n\t\t\to.Websocket.PasswordCookie = mv.(string)\n\t\tcase \"token_cookie\":\n\t\t\to.Websocket.TokenCookie = mv.(string)\n\t\tcase \"no_auth_user\":\n\t\t\to.Websocket.NoAuthUser = mv.(string)\n\t\tcase \"headers\":\n\t\t\tm, ok := mv.(map[string]any)\n\t\t\tif !ok {\n\t\t\t\terr := &configErr{tk, fmt.Sprintf(\"error parsing headers: unsupported type %T\", mv)}\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\to.Websocket.Headers = make(map[string]string)\n\t\t\tfor key, val := range m {\n\t\t\t\ttk, val = unwrapValue(val, &lt)\n\t\t\t\tif headerValue, ok := val.(string); !ok {\n\t\t\t\t\t*errors = append(*errors, &configErr{tk, fmt.Sprintf(\"error parsing header key %s: unsupported type %T\", key, val)})\n\t\t\t\t\tcontinue\n\t\t\t\t} else {\n\t\t\t\t\to.Websocket.Headers[key] = headerValue\n\t\t\t\t}\n\t\t\t}\n\t\tdefault:\n\t\t\tif !tk.IsUsedVariable() {\n\t\t\t\terr := &unknownConfigFieldErr{\n\t\t\t\t\tfield: mk,\n\t\t\t\t\tconfigErr: configErr{\n\t\t\t\t\t\ttoken: tk,\n\t\t\t\t\t},\n\t\t\t\t}\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc parseMQTT(v any, o *Options, errors *[]error, warnings *[]error) error {\n\tvar lt token\n\tdefer convertPanicToErrorList(&lt, errors)\n\n\ttk, v := unwrapValue(v, &lt)\n\tgm, ok := v.(map[string]any)\n\tif !ok {\n\t\treturn &configErr{tk, fmt.Sprintf(\"Expected mqtt to be a map, got %T\", v)}\n\t}\n\tfor mk, mv := range gm {\n\t\t// Again, unwrap token value if line check is required.\n\t\ttk, mv = unwrapValue(mv, &lt)\n\t\tswitch strings.ToLower(mk) {\n\t\tcase \"listen\":\n\t\t\thp, err := parseListen(mv)\n\t\t\tif err != nil {\n\t\t\t\terr := &configErr{tk, err.Error()}\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\to.MQTT.Host = hp.host\n\t\t\to.MQTT.Port = hp.port\n\t\tcase \"port\":\n\t\t\to.MQTT.Port = int(mv.(int64))\n\t\tcase \"host\", \"net\":\n\t\t\to.MQTT.Host = mv.(string)\n\t\tcase \"tls\":\n\t\t\ttc, err := parseTLS(tk, true)\n\t\t\tif err != nil {\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif o.MQTT.TLSConfig, err = GenTLSConfig(tc); err != nil {\n\t\t\t\terr := &configErr{tk, err.Error()}\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\to.MQTT.TLSTimeout = tc.Timeout\n\t\t\to.MQTT.TLSMap = tc.Map\n\t\t\to.MQTT.TLSPinnedCerts = tc.PinnedCerts\n\t\t\to.MQTT.tlsConfigOpts = tc\n\t\tcase \"authorization\", \"authentication\":\n\t\t\tauth := parseSimpleAuth(tk, errors)\n\t\t\to.MQTT.Username = auth.user\n\t\t\to.MQTT.Password = auth.pass\n\t\t\to.MQTT.Token = auth.token\n\t\t\to.MQTT.AuthTimeout = auth.timeout\n\t\tcase \"no_auth_user\":\n\t\t\to.MQTT.NoAuthUser = mv.(string)\n\t\tcase \"ack_wait\", \"ackwait\":\n\t\t\to.MQTT.AckWait = parseDuration(\"ack_wait\", tk, mv, errors, warnings)\n\t\tcase \"js_api_timeout\", \"api_timeout\":\n\t\t\to.MQTT.JSAPITimeout = parseDuration(\"js_api_timeout\", tk, mv, errors, warnings)\n\t\tcase \"max_ack_pending\", \"max_pending\", \"max_inflight\":\n\t\t\ttmp := int(mv.(int64))\n\t\t\tif tmp < 0 || tmp > 0xFFFF {\n\t\t\t\terr := &configErr{tk, fmt.Sprintf(\"invalid value %v, should in [0..%d] range\", tmp, 0xFFFF)}\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t} else {\n\t\t\t\to.MQTT.MaxAckPending = uint16(tmp)\n\t\t\t}\n\t\tcase \"js_domain\":\n\t\t\to.MQTT.JsDomain = mv.(string)\n\t\tcase \"stream_replicas\":\n\t\t\to.MQTT.StreamReplicas = int(mv.(int64))\n\t\tcase \"consumer_replicas\":\n\t\t\terr := &configWarningErr{\n\t\t\t\tfield: mk,\n\t\t\t\tconfigErr: configErr{\n\t\t\t\t\ttoken:  tk,\n\t\t\t\t\treason: `consumer replicas setting ignored in this server version`,\n\t\t\t\t},\n\t\t\t}\n\t\t\t*warnings = append(*warnings, err)\n\t\tcase \"consumer_memory_storage\":\n\t\t\to.MQTT.ConsumerMemoryStorage = mv.(bool)\n\t\tcase \"consumer_inactive_threshold\", \"consumer_auto_cleanup\":\n\t\t\to.MQTT.ConsumerInactiveThreshold = parseDuration(\"consumer_inactive_threshold\", tk, mv, errors, warnings)\n\n\t\tcase \"reject_qos2_publish\":\n\t\t\to.MQTT.rejectQoS2Pub = mv.(bool)\n\t\tcase \"downgrade_qos2_subscribe\":\n\t\t\to.MQTT.downgradeQoS2Sub = mv.(bool)\n\n\t\tdefault:\n\t\t\tif !tk.IsUsedVariable() {\n\t\t\t\terr := &unknownConfigFieldErr{\n\t\t\t\t\tfield: mk,\n\t\t\t\t\tconfigErr: configErr{\n\t\t\t\t\t\ttoken: tk,\n\t\t\t\t\t},\n\t\t\t\t}\n\t\t\t\t*errors = append(*errors, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t}\n\t}\n\treturn nil\n}\n\n// GenTLSConfig loads TLS related configuration parameters.\nfunc GenTLSConfig(tc *TLSConfigOpts) (*tls.Config, error) {\n\t// Create the tls.Config from our options before including the certs.\n\t// It will determine the cipher suites that we prefer.\n\t// FIXME(dlc) change if ARM based.\n\tconfig := tls.Config{\n\t\tMinVersion:               tls.VersionTLS12,\n\t\tCipherSuites:             tc.Ciphers,\n\t\tPreferServerCipherSuites: true,\n\t\tCurvePreferences:         tc.CurvePreferences,\n\t\tInsecureSkipVerify:       tc.Insecure,\n\t}\n\n\tswitch {\n\tcase tc.CertFile != _EMPTY_ && tc.CertStore != certstore.STOREEMPTY:\n\t\treturn nil, certstore.ErrConflictCertFileAndStore\n\tcase tc.CertFile != _EMPTY_ && tc.KeyFile == _EMPTY_:\n\t\treturn nil, fmt.Errorf(\"missing 'key_file' in TLS configuration\")\n\tcase tc.CertFile == _EMPTY_ && tc.KeyFile != _EMPTY_:\n\t\treturn nil, fmt.Errorf(\"missing 'cert_file' in TLS configuration\")\n\tcase tc.CertFile != _EMPTY_ && tc.KeyFile != _EMPTY_:\n\t\t// Now load in cert and private key\n\t\tcert, err := tls.LoadX509KeyPair(tc.CertFile, tc.KeyFile)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"error parsing X509 certificate/key pair: %v\", err)\n\t\t}\n\t\tcert.Leaf, err = x509.ParseCertificate(cert.Certificate[0])\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"error parsing certificate: %v\", err)\n\t\t}\n\t\tconfig.Certificates = []tls.Certificate{cert}\n\tcase tc.CertStore != certstore.STOREEMPTY:\n\t\terr := certstore.TLSConfig(tc.CertStore, tc.CertMatchBy, tc.CertMatch, tc.CaCertsMatch, tc.CertMatchSkipInvalid, &config)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\tcase tc.Certificates != nil:\n\t\t// Multiple certificate support.\n\t\tconfig.Certificates = make([]tls.Certificate, len(tc.Certificates))\n\t\tfor i, certPair := range tc.Certificates {\n\t\t\tcert, err := tls.LoadX509KeyPair(certPair.CertFile, certPair.KeyFile)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, fmt.Errorf(\"error parsing X509 certificate/key pair %d/%d: %v\", i+1, len(tc.Certificates), err)\n\t\t\t}\n\t\t\tcert.Leaf, err = x509.ParseCertificate(cert.Certificate[0])\n\t\t\tif err != nil {\n\t\t\t\treturn nil, fmt.Errorf(\"error parsing certificate %d/%d: %v\", i+1, len(tc.Certificates), err)\n\t\t\t}\n\t\t\tconfig.Certificates[i] = cert\n\t\t}\n\t}\n\n\t// Require client certificates as needed\n\tif tc.Verify {\n\t\tconfig.ClientAuth = tls.RequireAndVerifyClientCert\n\t}\n\t// Add in CAs if applicable.\n\tif tc.CaFile != _EMPTY_ {\n\t\trootPEM, err := os.ReadFile(tc.CaFile)\n\t\tif err != nil || rootPEM == nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tpool := x509.NewCertPool()\n\t\tok := pool.AppendCertsFromPEM(rootPEM)\n\t\tif !ok {\n\t\t\treturn nil, fmt.Errorf(\"failed to parse root ca certificate\")\n\t\t}\n\t\tconfig.ClientCAs = pool\n\t}\n\t// Allow setting TLS minimum version.\n\tif tc.MinVersion > 0 {\n\t\tif tc.MinVersion < tls.VersionTLS12 {\n\t\t\treturn nil, fmt.Errorf(\"unsupported minimum TLS version: %s\", tls.VersionName(tc.MinVersion))\n\t\t}\n\t\tconfig.MinVersion = tc.MinVersion\n\t}\n\n\treturn &config, nil\n}\n\n// MergeOptions will merge two options giving preference to the flagOpts\n// if the item is present.\nfunc MergeOptions(fileOpts, flagOpts *Options) *Options {\n\tif fileOpts == nil {\n\t\treturn flagOpts\n\t}\n\tif flagOpts == nil {\n\t\treturn fileOpts\n\t}\n\t// Merge the two, flagOpts override\n\topts := *fileOpts\n\n\tif flagOpts.Port != 0 {\n\t\topts.Port = flagOpts.Port\n\t}\n\tif flagOpts.Host != _EMPTY_ {\n\t\topts.Host = flagOpts.Host\n\t}\n\tif flagOpts.DontListen {\n\t\topts.DontListen = flagOpts.DontListen\n\t}\n\tif flagOpts.ClientAdvertise != _EMPTY_ {\n\t\topts.ClientAdvertise = flagOpts.ClientAdvertise\n\t}\n\tif flagOpts.Username != _EMPTY_ {\n\t\topts.Username = flagOpts.Username\n\t}\n\tif flagOpts.Password != _EMPTY_ {\n\t\topts.Password = flagOpts.Password\n\t}\n\tif flagOpts.Authorization != _EMPTY_ {\n\t\topts.Authorization = flagOpts.Authorization\n\t}\n\tif flagOpts.HTTPPort != 0 {\n\t\topts.HTTPPort = flagOpts.HTTPPort\n\t}\n\tif flagOpts.HTTPBasePath != _EMPTY_ {\n\t\topts.HTTPBasePath = flagOpts.HTTPBasePath\n\t}\n\tif flagOpts.Debug {\n\t\topts.Debug = true\n\t}\n\tif flagOpts.Trace {\n\t\topts.Trace = true\n\t}\n\tif flagOpts.Logtime {\n\t\topts.Logtime = true\n\t}\n\tif flagOpts.LogFile != _EMPTY_ {\n\t\topts.LogFile = flagOpts.LogFile\n\t}\n\tif flagOpts.PidFile != _EMPTY_ {\n\t\topts.PidFile = flagOpts.PidFile\n\t}\n\tif flagOpts.PortsFileDir != _EMPTY_ {\n\t\topts.PortsFileDir = flagOpts.PortsFileDir\n\t}\n\tif flagOpts.ProfPort != 0 {\n\t\topts.ProfPort = flagOpts.ProfPort\n\t}\n\tif flagOpts.Cluster.ListenStr != _EMPTY_ {\n\t\topts.Cluster.ListenStr = flagOpts.Cluster.ListenStr\n\t}\n\tif flagOpts.Cluster.NoAdvertise {\n\t\topts.Cluster.NoAdvertise = true\n\t}\n\tif flagOpts.Cluster.ConnectRetries != 0 {\n\t\topts.Cluster.ConnectRetries = flagOpts.Cluster.ConnectRetries\n\t}\n\tif flagOpts.Cluster.Advertise != _EMPTY_ {\n\t\topts.Cluster.Advertise = flagOpts.Cluster.Advertise\n\t}\n\tif flagOpts.RoutesStr != _EMPTY_ {\n\t\tmergeRoutes(&opts, flagOpts)\n\t}\n\tif flagOpts.JetStream {\n\t\topts.JetStream = flagOpts.JetStream\n\t}\n\tif flagOpts.StoreDir != _EMPTY_ {\n\t\topts.StoreDir = flagOpts.StoreDir\n\t}\n\treturn &opts\n}\n\n// RoutesFromStr parses route URLs from a string\nfunc RoutesFromStr(routesStr string) []*url.URL {\n\troutes := strings.Split(routesStr, \",\")\n\tif len(routes) == 0 {\n\t\treturn nil\n\t}\n\trouteUrls := []*url.URL{}\n\tfor _, r := range routes {\n\t\tr = strings.TrimSpace(r)\n\t\tu, _ := url.Parse(r)\n\t\trouteUrls = append(routeUrls, u)\n\t}\n\treturn routeUrls\n}\n\n// This will merge the flag routes and override anything that was present.\nfunc mergeRoutes(opts, flagOpts *Options) {\n\trouteUrls := RoutesFromStr(flagOpts.RoutesStr)\n\tif routeUrls == nil {\n\t\treturn\n\t}\n\topts.Routes = routeUrls\n\topts.RoutesStr = flagOpts.RoutesStr\n}\n\nfunc setBaselineOptions(opts *Options) {\n\t// Setup non-standard Go defaults\n\tif opts.Host == _EMPTY_ {\n\t\topts.Host = DEFAULT_HOST\n\t}\n\tif opts.HTTPHost == _EMPTY_ {\n\t\t// Default to same bind from server if left undefined\n\t\topts.HTTPHost = opts.Host\n\t}\n\tif opts.Port == 0 {\n\t\topts.Port = DEFAULT_PORT\n\t} else if opts.Port == RANDOM_PORT {\n\t\t// Choose randomly inside of net.Listen\n\t\topts.Port = 0\n\t}\n\tif opts.MaxConn == 0 {\n\t\topts.MaxConn = DEFAULT_MAX_CONNECTIONS\n\t}\n\tif opts.PingInterval == 0 {\n\t\topts.PingInterval = DEFAULT_PING_INTERVAL\n\t}\n\tif opts.MaxPingsOut == 0 {\n\t\topts.MaxPingsOut = DEFAULT_PING_MAX_OUT\n\t}\n\tif opts.TLSTimeout == 0 {\n\t\topts.TLSTimeout = float64(TLS_TIMEOUT) / float64(time.Second)\n\t}\n\tif opts.AuthTimeout == 0 {\n\t\topts.AuthTimeout = getDefaultAuthTimeout(opts.TLSConfig, opts.TLSTimeout)\n\t}\n\tif opts.Cluster.Port != 0 || opts.Cluster.ListenStr != _EMPTY_ {\n\t\tif opts.Cluster.Host == _EMPTY_ {\n\t\t\topts.Cluster.Host = DEFAULT_HOST\n\t\t}\n\t\tif opts.Cluster.TLSTimeout == 0 {\n\t\t\topts.Cluster.TLSTimeout = float64(TLS_TIMEOUT) / float64(time.Second)\n\t\t}\n\t\tif opts.Cluster.AuthTimeout == 0 {\n\t\t\topts.Cluster.AuthTimeout = getDefaultAuthTimeout(opts.Cluster.TLSConfig, opts.Cluster.TLSTimeout)\n\t\t}\n\t\tif opts.Cluster.PoolSize == 0 {\n\t\t\topts.Cluster.PoolSize = DEFAULT_ROUTE_POOL_SIZE\n\t\t}\n\t\t// Unless pooling/accounts are disabled (by PoolSize being set to -1),\n\t\t// check for Cluster.Accounts. Add the system account if not present and\n\t\t// unless we have a configuration that disabled it.\n\t\tif opts.Cluster.PoolSize > 0 {\n\t\t\tsysAccName := opts.SystemAccount\n\t\t\tif sysAccName == _EMPTY_ && !opts.NoSystemAccount {\n\t\t\t\tsysAccName = DEFAULT_SYSTEM_ACCOUNT\n\t\t\t}\n\t\t\tif sysAccName != _EMPTY_ {\n\t\t\t\tvar found bool\n\t\t\t\tfor _, acc := range opts.Cluster.PinnedAccounts {\n\t\t\t\t\tif acc == sysAccName {\n\t\t\t\t\t\tfound = true\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif !found {\n\t\t\t\t\topts.Cluster.PinnedAccounts = append(opts.Cluster.PinnedAccounts, sysAccName)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t// Default to compression \"accept\", which means that compression is not\n\t\t// initiated, but if the remote selects compression, this server will\n\t\t// use the same.\n\t\tif c := &opts.Cluster.Compression; c.Mode == _EMPTY_ {\n\t\t\tif testDefaultClusterCompression != _EMPTY_ {\n\t\t\t\tc.Mode = testDefaultClusterCompression\n\t\t\t} else {\n\t\t\t\tc.Mode = CompressionAccept\n\t\t\t}\n\t\t}\n\t}\n\tif opts.LeafNode.Port != 0 {\n\t\tif opts.LeafNode.Host == _EMPTY_ {\n\t\t\topts.LeafNode.Host = DEFAULT_HOST\n\t\t}\n\t\tif opts.LeafNode.TLSTimeout == 0 {\n\t\t\topts.LeafNode.TLSTimeout = float64(TLS_TIMEOUT) / float64(time.Second)\n\t\t}\n\t\tif opts.LeafNode.AuthTimeout == 0 {\n\t\t\topts.LeafNode.AuthTimeout = getDefaultAuthTimeout(opts.LeafNode.TLSConfig, opts.LeafNode.TLSTimeout)\n\t\t}\n\t\t// Default to compression \"s2_auto\".\n\t\tif c := &opts.LeafNode.Compression; c.Mode == _EMPTY_ {\n\t\t\tif testDefaultLeafNodeCompression != _EMPTY_ {\n\t\t\t\tc.Mode = testDefaultLeafNodeCompression\n\t\t\t} else {\n\t\t\t\tc.Mode = CompressionS2Auto\n\t\t\t}\n\t\t}\n\t}\n\t// Set baseline connect port for remotes.\n\tfor _, r := range opts.LeafNode.Remotes {\n\t\tif r != nil {\n\t\t\tfor _, u := range r.URLs {\n\t\t\t\tif u.Port() == _EMPTY_ {\n\t\t\t\t\tu.Host = net.JoinHostPort(u.Host, strconv.Itoa(DEFAULT_LEAFNODE_PORT))\n\t\t\t\t}\n\t\t\t}\n\t\t\t// Default to compression \"s2_auto\".\n\t\t\tif c := &r.Compression; c.Mode == _EMPTY_ {\n\t\t\t\tif testDefaultLeafNodeCompression != _EMPTY_ {\n\t\t\t\t\tc.Mode = testDefaultLeafNodeCompression\n\t\t\t\t} else {\n\t\t\t\t\tc.Mode = CompressionS2Auto\n\t\t\t\t}\n\t\t\t}\n\t\t\t// Set default first info timeout value if not set.\n\t\t\tif r.FirstInfoTimeout <= 0 {\n\t\t\t\tr.FirstInfoTimeout = DEFAULT_LEAFNODE_INFO_WAIT\n\t\t\t}\n\t\t}\n\t}\n\n\t// Set this regardless of opts.LeafNode.Port\n\tif opts.LeafNode.ReconnectInterval == 0 {\n\t\topts.LeafNode.ReconnectInterval = DEFAULT_LEAF_NODE_RECONNECT\n\t}\n\n\tif opts.MaxControlLine == 0 {\n\t\topts.MaxControlLine = MAX_CONTROL_LINE_SIZE\n\t}\n\tif opts.MaxPayload == 0 {\n\t\topts.MaxPayload = MAX_PAYLOAD_SIZE\n\t}\n\tif opts.MaxPending == 0 {\n\t\topts.MaxPending = MAX_PENDING_SIZE\n\t}\n\tif opts.WriteDeadline == time.Duration(0) {\n\t\topts.WriteDeadline = DEFAULT_FLUSH_DEADLINE\n\t}\n\tif opts.MaxClosedClients == 0 {\n\t\topts.MaxClosedClients = DEFAULT_MAX_CLOSED_CLIENTS\n\t}\n\tif opts.LameDuckDuration == 0 {\n\t\topts.LameDuckDuration = DEFAULT_LAME_DUCK_DURATION\n\t}\n\tif opts.LameDuckGracePeriod == 0 {\n\t\topts.LameDuckGracePeriod = DEFAULT_LAME_DUCK_GRACE_PERIOD\n\t}\n\tif opts.Gateway.Port != 0 {\n\t\tif opts.Gateway.Host == _EMPTY_ {\n\t\t\topts.Gateway.Host = DEFAULT_HOST\n\t\t}\n\t\tif opts.Gateway.TLSTimeout == 0 {\n\t\t\topts.Gateway.TLSTimeout = float64(TLS_TIMEOUT) / float64(time.Second)\n\t\t}\n\t\tif opts.Gateway.AuthTimeout == 0 {\n\t\t\topts.Gateway.AuthTimeout = getDefaultAuthTimeout(opts.Gateway.TLSConfig, opts.Gateway.TLSTimeout)\n\t\t}\n\t}\n\tif opts.ConnectErrorReports == 0 {\n\t\topts.ConnectErrorReports = DEFAULT_CONNECT_ERROR_REPORTS\n\t}\n\tif opts.ReconnectErrorReports == 0 {\n\t\topts.ReconnectErrorReports = DEFAULT_RECONNECT_ERROR_REPORTS\n\t}\n\tif opts.Websocket.Port != 0 {\n\t\tif opts.Websocket.Host == _EMPTY_ {\n\t\t\topts.Websocket.Host = DEFAULT_HOST\n\t\t}\n\t}\n\tif opts.MQTT.Port != 0 {\n\t\tif opts.MQTT.Host == _EMPTY_ {\n\t\t\topts.MQTT.Host = DEFAULT_HOST\n\t\t}\n\t\tif opts.MQTT.TLSTimeout == 0 {\n\t\t\topts.MQTT.TLSTimeout = float64(TLS_TIMEOUT) / float64(time.Second)\n\t\t}\n\t}\n\t// JetStream\n\tif opts.JetStreamMaxMemory == 0 && !opts.maxMemSet {\n\t\topts.JetStreamMaxMemory = -1\n\t}\n\tif opts.JetStreamMaxStore == 0 && !opts.maxStoreSet {\n\t\topts.JetStreamMaxStore = -1\n\t}\n\tif opts.SyncInterval == 0 && !opts.syncSet {\n\t\topts.SyncInterval = defaultSyncInterval\n\t}\n\tif opts.JetStreamRequestQueueLimit <= 0 {\n\t\topts.JetStreamRequestQueueLimit = JSDefaultRequestQueueLimit\n\t}\n}\n\nfunc getDefaultAuthTimeout(tls *tls.Config, tlsTimeout float64) float64 {\n\tvar authTimeout float64\n\tif tls != nil {\n\t\tauthTimeout = tlsTimeout + 1.0\n\t} else {\n\t\tauthTimeout = float64(AUTH_TIMEOUT / time.Second)\n\t}\n\treturn authTimeout\n}\n\n// ConfigureOptions accepts a flag set and augments it with NATS Server\n// specific flags. On success, an options structure is returned configured\n// based on the selected flags and/or configuration file.\n// The command line options take precedence to the ones in the configuration file.\nfunc ConfigureOptions(fs *flag.FlagSet, args []string, printVersion, printHelp, printTLSHelp func()) (*Options, error) {\n\topts := &Options{}\n\tvar (\n\t\tshowVersion            bool\n\t\tshowHelp               bool\n\t\tshowTLSHelp            bool\n\t\tsignal                 string\n\t\tconfigFile             string\n\t\tdbgAndTrace            bool\n\t\ttrcAndVerboseTrc       bool\n\t\tdbgAndTrcAndVerboseTrc bool\n\t\terr                    error\n\t)\n\n\tfs.BoolVar(&showHelp, \"h\", false, \"Show this message.\")\n\tfs.BoolVar(&showHelp, \"help\", false, \"Show this message.\")\n\tfs.IntVar(&opts.Port, \"port\", 0, \"Port to listen on.\")\n\tfs.IntVar(&opts.Port, \"p\", 0, \"Port to listen on.\")\n\tfs.StringVar(&opts.ServerName, \"n\", _EMPTY_, \"Server name.\")\n\tfs.StringVar(&opts.ServerName, \"name\", _EMPTY_, \"Server name.\")\n\tfs.StringVar(&opts.ServerName, \"server_name\", _EMPTY_, \"Server name.\")\n\tfs.StringVar(&opts.Host, \"addr\", _EMPTY_, \"Network host to listen on.\")\n\tfs.StringVar(&opts.Host, \"a\", _EMPTY_, \"Network host to listen on.\")\n\tfs.StringVar(&opts.Host, \"net\", _EMPTY_, \"Network host to listen on.\")\n\tfs.StringVar(&opts.ClientAdvertise, \"client_advertise\", _EMPTY_, \"Client URL to advertise to other servers.\")\n\tfs.BoolVar(&opts.Debug, \"D\", false, \"Enable Debug logging.\")\n\tfs.BoolVar(&opts.Debug, \"debug\", false, \"Enable Debug logging.\")\n\tfs.BoolVar(&opts.Trace, \"V\", false, \"Enable Trace logging.\")\n\tfs.BoolVar(&trcAndVerboseTrc, \"VV\", false, \"Enable Verbose Trace logging. (Traces system account as well)\")\n\tfs.BoolVar(&opts.Trace, \"trace\", false, \"Enable Trace logging.\")\n\tfs.BoolVar(&dbgAndTrace, \"DV\", false, \"Enable Debug and Trace logging.\")\n\tfs.BoolVar(&dbgAndTrcAndVerboseTrc, \"DVV\", false, \"Enable Debug and Verbose Trace logging. (Traces system account as well)\")\n\tfs.BoolVar(&opts.Logtime, \"T\", true, \"Timestamp log entries.\")\n\tfs.BoolVar(&opts.Logtime, \"logtime\", true, \"Timestamp log entries.\")\n\tfs.BoolVar(&opts.LogtimeUTC, \"logtime_utc\", false, \"Timestamps in UTC instead of local timezone.\")\n\tfs.StringVar(&opts.Username, \"user\", _EMPTY_, \"Username required for connection.\")\n\tfs.StringVar(&opts.Password, \"pass\", _EMPTY_, \"Password required for connection.\")\n\tfs.StringVar(&opts.Authorization, \"auth\", _EMPTY_, \"Authorization token required for connection.\")\n\tfs.IntVar(&opts.HTTPPort, \"m\", 0, \"HTTP Port for /varz, /connz endpoints.\")\n\tfs.IntVar(&opts.HTTPPort, \"http_port\", 0, \"HTTP Port for /varz, /connz endpoints.\")\n\tfs.IntVar(&opts.HTTPSPort, \"ms\", 0, \"HTTPS Port for /varz, /connz endpoints.\")\n\tfs.IntVar(&opts.HTTPSPort, \"https_port\", 0, \"HTTPS Port for /varz, /connz endpoints.\")\n\tfs.StringVar(&configFile, \"c\", _EMPTY_, \"Configuration file.\")\n\tfs.StringVar(&configFile, \"config\", _EMPTY_, \"Configuration file.\")\n\tfs.BoolVar(&opts.CheckConfig, \"t\", false, \"Check configuration and exit.\")\n\tfs.StringVar(&signal, \"sl\", \"\", \"Send signal to nats-server process (ldm, stop, quit, term, reopen, reload).\")\n\tfs.StringVar(&signal, \"signal\", \"\", \"Send signal to nats-server process (ldm, stop, quit, term, reopen, reload).\")\n\tfs.StringVar(&opts.PidFile, \"P\", \"\", \"File to store process pid.\")\n\tfs.StringVar(&opts.PidFile, \"pid\", \"\", \"File to store process pid.\")\n\tfs.StringVar(&opts.PortsFileDir, \"ports_file_dir\", \"\", \"Creates a ports file in the specified directory (<executable_name>_<pid>.ports).\")\n\tfs.StringVar(&opts.LogFile, \"l\", \"\", \"File to store logging output.\")\n\tfs.StringVar(&opts.LogFile, \"log\", \"\", \"File to store logging output.\")\n\tfs.Int64Var(&opts.LogSizeLimit, \"log_size_limit\", 0, \"Logfile size limit being auto-rotated\")\n\tfs.BoolVar(&opts.Syslog, \"s\", false, \"Enable syslog as log method.\")\n\tfs.BoolVar(&opts.Syslog, \"syslog\", false, \"Enable syslog as log method.\")\n\tfs.StringVar(&opts.RemoteSyslog, \"r\", _EMPTY_, \"Syslog server addr (udp://127.0.0.1:514).\")\n\tfs.StringVar(&opts.RemoteSyslog, \"remote_syslog\", _EMPTY_, \"Syslog server addr (udp://127.0.0.1:514).\")\n\tfs.BoolVar(&showVersion, \"version\", false, \"Print version information.\")\n\tfs.BoolVar(&showVersion, \"v\", false, \"Print version information.\")\n\tfs.IntVar(&opts.ProfPort, \"profile\", 0, \"Profiling HTTP port.\")\n\tfs.StringVar(&opts.RoutesStr, \"routes\", _EMPTY_, \"Routes to actively solicit a connection.\")\n\tfs.StringVar(&opts.Cluster.ListenStr, \"cluster\", _EMPTY_, \"Cluster url from which members can solicit routes.\")\n\tfs.StringVar(&opts.Cluster.ListenStr, \"cluster_listen\", _EMPTY_, \"Cluster url from which members can solicit routes.\")\n\tfs.StringVar(&opts.Cluster.Advertise, \"cluster_advertise\", _EMPTY_, \"Cluster URL to advertise to other servers.\")\n\tfs.BoolVar(&opts.Cluster.NoAdvertise, \"no_advertise\", false, \"Advertise known cluster IPs to clients.\")\n\tfs.IntVar(&opts.Cluster.ConnectRetries, \"connect_retries\", 0, \"For implicit routes, number of connect retries.\")\n\tfs.StringVar(&opts.Cluster.Name, \"cluster_name\", _EMPTY_, \"Cluster Name, if not set one will be dynamically generated.\")\n\tfs.BoolVar(&showTLSHelp, \"help_tls\", false, \"TLS help.\")\n\tfs.BoolVar(&opts.TLS, \"tls\", false, \"Enable TLS.\")\n\tfs.BoolVar(&opts.TLSVerify, \"tlsverify\", false, \"Enable TLS with client verification.\")\n\tfs.StringVar(&opts.TLSCert, \"tlscert\", _EMPTY_, \"Server certificate file.\")\n\tfs.StringVar(&opts.TLSKey, \"tlskey\", _EMPTY_, \"Private key for server certificate.\")\n\tfs.StringVar(&opts.TLSCaCert, \"tlscacert\", _EMPTY_, \"Client certificate CA for verification.\")\n\tfs.IntVar(&opts.MaxTracedMsgLen, \"max_traced_msg_len\", 0, \"Maximum printable length for traced messages. 0 for unlimited.\")\n\tfs.BoolVar(&opts.JetStream, \"js\", false, \"Enable JetStream.\")\n\tfs.BoolVar(&opts.JetStream, \"jetstream\", false, \"Enable JetStream.\")\n\tfs.StringVar(&opts.StoreDir, \"sd\", _EMPTY_, \"Storage directory.\")\n\tfs.StringVar(&opts.StoreDir, \"store_dir\", _EMPTY_, \"Storage directory.\")\n\n\t// The flags definition above set \"default\" values to some of the options.\n\t// Calling Parse() here will override the default options with any value\n\t// specified from the command line. This is ok. We will then update the\n\t// options with the content of the configuration file (if present), and then,\n\t// call Parse() again to override the default+config with command line values.\n\t// Calling Parse() before processing config file is necessary since configFile\n\t// itself is a command line argument, and also Parse() is required in order\n\t// to know if user wants simply to show \"help\" or \"version\", etc...\n\tif err := fs.Parse(args); err != nil {\n\t\treturn nil, err\n\t}\n\n\tif showVersion {\n\t\tprintVersion()\n\t\treturn nil, nil\n\t}\n\n\tif showHelp {\n\t\tprintHelp()\n\t\treturn nil, nil\n\t}\n\n\tif showTLSHelp {\n\t\tprintTLSHelp()\n\t\treturn nil, nil\n\t}\n\n\t// Process args looking for non-flag options,\n\t// 'version' and 'help' only for now\n\tshowVersion, showHelp, err = ProcessCommandLineArgs(fs)\n\tif err != nil {\n\t\treturn nil, err\n\t} else if showVersion {\n\t\tprintVersion()\n\t\treturn nil, nil\n\t} else if showHelp {\n\t\tprintHelp()\n\t\treturn nil, nil\n\t}\n\n\t// Snapshot flag options.\n\tFlagSnapshot = opts.Clone()\n\n\t// Keep track of the boolean flags that were explicitly set with their value.\n\tfs.Visit(func(f *flag.Flag) {\n\t\tswitch f.Name {\n\t\tcase \"DVV\":\n\t\t\ttrackExplicitVal(&FlagSnapshot.inCmdLine, \"Debug\", dbgAndTrcAndVerboseTrc)\n\t\t\ttrackExplicitVal(&FlagSnapshot.inCmdLine, \"Trace\", dbgAndTrcAndVerboseTrc)\n\t\t\ttrackExplicitVal(&FlagSnapshot.inCmdLine, \"TraceVerbose\", dbgAndTrcAndVerboseTrc)\n\t\tcase \"DV\":\n\t\t\ttrackExplicitVal(&FlagSnapshot.inCmdLine, \"Debug\", dbgAndTrace)\n\t\t\ttrackExplicitVal(&FlagSnapshot.inCmdLine, \"Trace\", dbgAndTrace)\n\t\tcase \"D\":\n\t\t\tfallthrough\n\t\tcase \"debug\":\n\t\t\ttrackExplicitVal(&FlagSnapshot.inCmdLine, \"Debug\", FlagSnapshot.Debug)\n\t\tcase \"VV\":\n\t\t\ttrackExplicitVal(&FlagSnapshot.inCmdLine, \"Trace\", trcAndVerboseTrc)\n\t\t\ttrackExplicitVal(&FlagSnapshot.inCmdLine, \"TraceVerbose\", trcAndVerboseTrc)\n\t\tcase \"V\":\n\t\t\tfallthrough\n\t\tcase \"trace\":\n\t\t\ttrackExplicitVal(&FlagSnapshot.inCmdLine, \"Trace\", FlagSnapshot.Trace)\n\t\tcase \"T\":\n\t\t\tfallthrough\n\t\tcase \"logtime\":\n\t\t\ttrackExplicitVal(&FlagSnapshot.inCmdLine, \"Logtime\", FlagSnapshot.Logtime)\n\t\tcase \"s\":\n\t\t\tfallthrough\n\t\tcase \"syslog\":\n\t\t\ttrackExplicitVal(&FlagSnapshot.inCmdLine, \"Syslog\", FlagSnapshot.Syslog)\n\t\tcase \"no_advertise\":\n\t\t\ttrackExplicitVal(&FlagSnapshot.inCmdLine, \"Cluster.NoAdvertise\", FlagSnapshot.Cluster.NoAdvertise)\n\t\tcase \"js\":\n\t\t\ttrackExplicitVal(&FlagSnapshot.inCmdLine, \"JetStream\", FlagSnapshot.JetStream)\n\t\t}\n\t})\n\n\t// Process signal control.\n\tif signal != _EMPTY_ {\n\t\tif err := processSignal(signal); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\t// Parse config if given\n\tif configFile != _EMPTY_ {\n\t\t// This will update the options with values from the config file.\n\t\terr := opts.ProcessConfigFile(configFile)\n\t\tif err != nil {\n\t\t\tif opts.CheckConfig {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tif cerr, ok := err.(*processConfigErr); !ok || len(cerr.Errors()) != 0 {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\t// If we get here we only have warnings and can still continue\n\t\t\tfmt.Fprint(os.Stderr, err)\n\t\t} else if opts.CheckConfig {\n\t\t\t// Report configuration file syntax test was successful and exit.\n\t\t\treturn opts, nil\n\t\t}\n\n\t\t// Call this again to override config file options with options from command line.\n\t\t// Note: We don't need to check error here since if there was an error, it would\n\t\t// have been caught the first time this function was called (after setting up the\n\t\t// flags).\n\t\tfs.Parse(args)\n\t} else if opts.CheckConfig {\n\t\treturn nil, fmt.Errorf(\"must specify [-c, --config] option to check configuration file syntax\")\n\t}\n\n\t// Special handling of some flags\n\tvar (\n\t\tflagErr     error\n\t\ttlsDisabled bool\n\t\ttlsOverride bool\n\t)\n\tfs.Visit(func(f *flag.Flag) {\n\t\t// short-circuit if an error was encountered\n\t\tif flagErr != nil {\n\t\t\treturn\n\t\t}\n\t\tif strings.HasPrefix(f.Name, \"tls\") {\n\t\t\tif f.Name == \"tls\" {\n\t\t\t\tif !opts.TLS {\n\t\t\t\t\t// User has specified \"-tls=false\", we need to disable TLS\n\t\t\t\t\topts.TLSConfig = nil\n\t\t\t\t\ttlsDisabled = true\n\t\t\t\t\ttlsOverride = false\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t\ttlsOverride = true\n\t\t\t} else if !tlsDisabled {\n\t\t\t\ttlsOverride = true\n\t\t\t}\n\t\t} else {\n\t\t\tswitch f.Name {\n\t\t\tcase \"VV\":\n\t\t\t\topts.Trace, opts.TraceVerbose = trcAndVerboseTrc, trcAndVerboseTrc\n\t\t\tcase \"DVV\":\n\t\t\t\topts.Trace, opts.Debug, opts.TraceVerbose = dbgAndTrcAndVerboseTrc, dbgAndTrcAndVerboseTrc, dbgAndTrcAndVerboseTrc\n\t\t\tcase \"DV\":\n\t\t\t\t// Check value to support -DV=false\n\t\t\t\topts.Trace, opts.Debug = dbgAndTrace, dbgAndTrace\n\t\t\tcase \"cluster\", \"cluster_listen\":\n\t\t\t\t// Override cluster config if explicitly set via flags.\n\t\t\t\tflagErr = overrideCluster(opts)\n\t\t\tcase \"routes\":\n\t\t\t\t// Keep in mind that the flag has updated opts.RoutesStr at this point.\n\t\t\t\tif opts.RoutesStr == _EMPTY_ {\n\t\t\t\t\t// Set routes array to nil since routes string is empty\n\t\t\t\t\topts.Routes = nil\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t\trouteUrls := RoutesFromStr(opts.RoutesStr)\n\t\t\t\topts.Routes = routeUrls\n\t\t\t}\n\t\t}\n\t})\n\tif flagErr != nil {\n\t\treturn nil, flagErr\n\t}\n\n\t// This will be true if some of the `-tls` params have been set and\n\t// `-tls=false` has not been set.\n\tif tlsOverride {\n\t\tif err := overrideTLS(opts); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\t// If we don't have cluster defined in the configuration\n\t// file and no cluster listen string override, but we do\n\t// have a routes override, we need to report misconfiguration.\n\tif opts.RoutesStr != _EMPTY_ && opts.Cluster.ListenStr == _EMPTY_ && opts.Cluster.Host == _EMPTY_ && opts.Cluster.Port == 0 {\n\t\treturn nil, errors.New(\"solicited routes require cluster capabilities, e.g. --cluster\")\n\t}\n\n\treturn opts, nil\n}\n\nfunc normalizeBasePath(p string) string {\n\tif len(p) == 0 {\n\t\treturn \"/\"\n\t}\n\t// add leading slash\n\tif p[0] != '/' {\n\t\tp = \"/\" + p\n\t}\n\treturn path.Clean(p)\n}\n\n// overrideTLS is called when at least \"-tls=true\" has been set.\nfunc overrideTLS(opts *Options) error {\n\tif opts.TLSCert == _EMPTY_ {\n\t\treturn errors.New(\"TLS Server certificate must be present and valid\")\n\t}\n\tif opts.TLSKey == _EMPTY_ {\n\t\treturn errors.New(\"TLS Server private key must be present and valid\")\n\t}\n\n\ttc := TLSConfigOpts{}\n\ttc.CertFile = opts.TLSCert\n\ttc.KeyFile = opts.TLSKey\n\ttc.CaFile = opts.TLSCaCert\n\ttc.Verify = opts.TLSVerify\n\ttc.Ciphers = defaultCipherSuites()\n\n\tvar err error\n\topts.TLSConfig, err = GenTLSConfig(&tc)\n\treturn err\n}\n\n// overrideCluster updates Options.Cluster if that flag \"cluster\" (or \"cluster_listen\")\n// has explicitly be set in the command line. If it is set to empty string, it will\n// clear the Cluster options.\nfunc overrideCluster(opts *Options) error {\n\tif opts.Cluster.ListenStr == _EMPTY_ {\n\t\t// This one is enough to disable clustering.\n\t\topts.Cluster.Port = 0\n\t\treturn nil\n\t}\n\t// -1 will fail url.Parse, so if we have -1, change it to\n\t// 0, and then after parse, replace the port with -1 so we get\n\t// automatic port allocation\n\twantsRandom := false\n\tif strings.HasSuffix(opts.Cluster.ListenStr, \":-1\") {\n\t\twantsRandom = true\n\t\tcls := fmt.Sprintf(\"%s:0\", opts.Cluster.ListenStr[0:len(opts.Cluster.ListenStr)-3])\n\t\topts.Cluster.ListenStr = cls\n\t}\n\tclusterURL, err := url.Parse(opts.Cluster.ListenStr)\n\tif err != nil {\n\t\treturn err\n\t}\n\th, p, err := net.SplitHostPort(clusterURL.Host)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif wantsRandom {\n\t\tp = \"-1\"\n\t}\n\topts.Cluster.Host = h\n\t_, err = fmt.Sscan(p, &opts.Cluster.Port)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif clusterURL.User != nil {\n\t\tpass, hasPassword := clusterURL.User.Password()\n\t\tif !hasPassword {\n\t\t\treturn errors.New(\"expected cluster password to be set\")\n\t\t}\n\t\topts.Cluster.Password = pass\n\n\t\tuser := clusterURL.User.Username()\n\t\topts.Cluster.Username = user\n\t} else {\n\t\t// Since we override from flag and there is no user/pwd, make\n\t\t// sure we clear what we may have gotten from config file.\n\t\topts.Cluster.Username = _EMPTY_\n\t\topts.Cluster.Password = _EMPTY_\n\t}\n\n\treturn nil\n}\n\nfunc processSignal(signal string) error {\n\tvar (\n\t\tpid           string\n\t\tcommandAndPid = strings.Split(signal, \"=\")\n\t)\n\tif l := len(commandAndPid); l == 2 {\n\t\tpid = maybeReadPidFile(commandAndPid[1])\n\t} else if l > 2 {\n\t\treturn fmt.Errorf(\"invalid signal parameters: %v\", commandAndPid[2:])\n\t}\n\tif err := ProcessSignal(Command(commandAndPid[0]), pid); err != nil {\n\t\treturn err\n\t}\n\tos.Exit(0)\n\treturn nil\n}\n\n// maybeReadPidFile returns a PID or Windows service name obtained via the following method:\n// 1. Try to open a file with path \"pidStr\" (absolute or relative).\n// 2. If such a file exists and can be read, return its contents.\n// 3. Otherwise, return the original \"pidStr\" string.\nfunc maybeReadPidFile(pidStr string) string {\n\tif b, err := os.ReadFile(pidStr); err == nil {\n\t\treturn string(b)\n\t}\n\treturn pidStr\n}\n\nfunc homeDir() (string, error) {\n\tif runtime.GOOS == \"windows\" {\n\t\thomeDrive, homePath := os.Getenv(\"HOMEDRIVE\"), os.Getenv(\"HOMEPATH\")\n\t\tuserProfile := os.Getenv(\"USERPROFILE\")\n\n\t\thome := filepath.Join(homeDrive, homePath)\n\t\tif homeDrive == _EMPTY_ || homePath == _EMPTY_ {\n\t\t\tif userProfile == _EMPTY_ {\n\t\t\t\treturn _EMPTY_, errors.New(\"nats: failed to get home dir, require %HOMEDRIVE% and %HOMEPATH% or %USERPROFILE%\")\n\t\t\t}\n\t\t\thome = userProfile\n\t\t}\n\n\t\treturn home, nil\n\t}\n\n\thome := os.Getenv(\"HOME\")\n\tif home == _EMPTY_ {\n\t\treturn _EMPTY_, errors.New(\"failed to get home dir, require $HOME\")\n\t}\n\treturn home, nil\n}\n\nfunc expandPath(p string) (string, error) {\n\tp = os.ExpandEnv(p)\n\n\tif !strings.HasPrefix(p, \"~\") {\n\t\treturn p, nil\n\t}\n\n\thome, err := homeDir()\n\tif err != nil {\n\t\treturn _EMPTY_, err\n\t}\n\n\treturn filepath.Join(home, p[1:]), nil\n}\n",
    "source_file": "server/opts.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2020-2025 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n//go:build !windows && !openbsd && !netbsd && !wasm\n\npackage server\n\nimport (\n\t\"os\"\n\t\"syscall\"\n)\n\nfunc diskAvailable(storeDir string) int64 {\n\tvar ba int64\n\tif _, err := os.Stat(storeDir); os.IsNotExist(err) {\n\t\tos.MkdirAll(storeDir, defaultDirPerms)\n\t}\n\tvar fs syscall.Statfs_t\n\tif err := syscall.Statfs(storeDir, &fs); err == nil {\n\t\t// Estimate 75% of available storage.\n\t\tba = int64(uint64(fs.Bavail) * uint64(fs.Bsize) / 4 * 3)\n\t} else {\n\t\t// Used 1TB default as a guess if all else fails.\n\t\tba = JetStreamMaxStoreDefault\n\t}\n\treturn ba\n}\n",
    "source_file": "server/disk_avail.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2019-2025 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage server\n\nimport (\n\t\"bytes\"\n\t\"encoding/binary\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"fmt\"\n\t\"math/rand\"\n\t\"reflect\"\n\t\"regexp\"\n\t\"slices\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t\"github.com/nats-io/nats-server/v2/server/avl\"\n\t\"github.com/nats-io/nuid\"\n\t\"golang.org/x/time/rate\"\n)\n\n// Headers sent with Request Timeout\nconst (\n\tJSPullRequestPendingMsgs  = \"Nats-Pending-Messages\"\n\tJSPullRequestPendingBytes = \"Nats-Pending-Bytes\"\n\tJSPullRequestWrongPinID   = \"NATS/1.0 423 Nats-Wrong-Pin-Id\\r\\n\\r\\n\"\n\tJSPullRequestNatsPinId    = \"Nats-Pin-Id\"\n)\n\nvar (\n\tvalidGroupName = regexp.MustCompile(`^[a-zA-Z0-9/_=-]{1,16}$`)\n)\n\n// Headers sent when batch size was completed, but there were remaining bytes.\nconst JsPullRequestRemainingBytesT = \"NATS/1.0 409 Batch Completed\\r\\n%s: %d\\r\\n%s: %d\\r\\n\\r\\n\"\n\ntype ConsumerInfo struct {\n\tStream         string          `json:\"stream_name\"`\n\tName           string          `json:\"name\"`\n\tCreated        time.Time       `json:\"created\"`\n\tConfig         *ConsumerConfig `json:\"config,omitempty\"`\n\tDelivered      SequenceInfo    `json:\"delivered\"`\n\tAckFloor       SequenceInfo    `json:\"ack_floor\"`\n\tNumAckPending  int             `json:\"num_ack_pending\"`\n\tNumRedelivered int             `json:\"num_redelivered\"`\n\tNumWaiting     int             `json:\"num_waiting\"`\n\tNumPending     uint64          `json:\"num_pending\"`\n\tCluster        *ClusterInfo    `json:\"cluster,omitempty\"`\n\tPushBound      bool            `json:\"push_bound,omitempty\"`\n\tPaused         bool            `json:\"paused,omitempty\"`\n\tPauseRemaining time.Duration   `json:\"pause_remaining,omitempty\"`\n\t// TimeStamp indicates when the info was gathered\n\tTimeStamp      time.Time            `json:\"ts\"`\n\tPriorityGroups []PriorityGroupState `json:\"priority_groups,omitempty\"`\n}\n\ntype PriorityGroupState struct {\n\tGroup          string    `json:\"group\"`\n\tPinnedClientID string    `json:\"pinned_client_id,omitempty\"`\n\tPinnedTS       time.Time `json:\"pinned_ts,omitempty\"`\n}\n\ntype ConsumerConfig struct {\n\tDurable         string          `json:\"durable_name,omitempty\"`\n\tName            string          `json:\"name,omitempty\"`\n\tDescription     string          `json:\"description,omitempty\"`\n\tDeliverPolicy   DeliverPolicy   `json:\"deliver_policy\"`\n\tOptStartSeq     uint64          `json:\"opt_start_seq,omitempty\"`\n\tOptStartTime    *time.Time      `json:\"opt_start_time,omitempty\"`\n\tAckPolicy       AckPolicy       `json:\"ack_policy\"`\n\tAckWait         time.Duration   `json:\"ack_wait,omitempty\"`\n\tMaxDeliver      int             `json:\"max_deliver,omitempty\"`\n\tBackOff         []time.Duration `json:\"backoff,omitempty\"`\n\tFilterSubject   string          `json:\"filter_subject,omitempty\"`\n\tFilterSubjects  []string        `json:\"filter_subjects,omitempty\"`\n\tReplayPolicy    ReplayPolicy    `json:\"replay_policy\"`\n\tRateLimit       uint64          `json:\"rate_limit_bps,omitempty\"` // Bits per sec\n\tSampleFrequency string          `json:\"sample_freq,omitempty\"`\n\tMaxWaiting      int             `json:\"max_waiting,omitempty\"`\n\tMaxAckPending   int             `json:\"max_ack_pending,omitempty\"`\n\tFlowControl     bool            `json:\"flow_control,omitempty\"`\n\tHeadersOnly     bool            `json:\"headers_only,omitempty\"`\n\n\t// Pull based options.\n\tMaxRequestBatch    int           `json:\"max_batch,omitempty\"`\n\tMaxRequestExpires  time.Duration `json:\"max_expires,omitempty\"`\n\tMaxRequestMaxBytes int           `json:\"max_bytes,omitempty\"`\n\n\t// Push based consumers.\n\tDeliverSubject string        `json:\"deliver_subject,omitempty\"`\n\tDeliverGroup   string        `json:\"deliver_group,omitempty\"`\n\tHeartbeat      time.Duration `json:\"idle_heartbeat,omitempty\"`\n\n\t// Ephemeral inactivity threshold.\n\tInactiveThreshold time.Duration `json:\"inactive_threshold,omitempty\"`\n\n\t// Generally inherited by parent stream and other markers, now can be configured directly.\n\tReplicas int `json:\"num_replicas\"`\n\t// Force memory storage.\n\tMemoryStorage bool `json:\"mem_storage,omitempty\"`\n\n\t// Don't add to general clients.\n\tDirect bool `json:\"direct,omitempty\"`\n\n\t// Metadata is additional metadata for the Consumer.\n\tMetadata map[string]string `json:\"metadata,omitempty\"`\n\n\t// PauseUntil is for suspending the consumer until the deadline.\n\tPauseUntil *time.Time `json:\"pause_until,omitempty\"`\n\n\t// Priority groups\n\tPriorityGroups []string       `json:\"priority_groups,omitempty\"`\n\tPriorityPolicy PriorityPolicy `json:\"priority_policy,omitempty\"`\n\tPinnedTTL      time.Duration  `json:\"priority_timeout,omitempty\"`\n}\n\n// SequenceInfo has both the consumer and the stream sequence and last activity.\ntype SequenceInfo struct {\n\tConsumer uint64     `json:\"consumer_seq\"`\n\tStream   uint64     `json:\"stream_seq\"`\n\tLast     *time.Time `json:\"last_active,omitempty\"`\n}\n\ntype CreateConsumerRequest struct {\n\tStream   string         `json:\"stream_name\"`\n\tConfig   ConsumerConfig `json:\"config\"`\n\tAction   ConsumerAction `json:\"action\"`\n\tPedantic bool           `json:\"pedantic,omitempty\"`\n}\n\ntype ConsumerAction int\n\nconst (\n\tActionCreateOrUpdate ConsumerAction = iota\n\tActionUpdate\n\tActionCreate\n)\n\nconst (\n\tactionUpdateJSONString         = `\"update\"`\n\tactionCreateJSONString         = `\"create\"`\n\tactionCreateOrUpdateJSONString = `\"\"`\n)\n\nvar (\n\tactionUpdateJSONBytes         = []byte(actionUpdateJSONString)\n\tactionCreateJSONBytes         = []byte(actionCreateJSONString)\n\tactionCreateOrUpdateJSONBytes = []byte(actionCreateOrUpdateJSONString)\n)\n\nfunc (a ConsumerAction) String() string {\n\tswitch a {\n\tcase ActionCreateOrUpdate:\n\t\treturn actionCreateOrUpdateJSONString\n\tcase ActionCreate:\n\t\treturn actionCreateJSONString\n\tcase ActionUpdate:\n\t\treturn actionUpdateJSONString\n\t}\n\treturn actionCreateOrUpdateJSONString\n}\n\nfunc (a ConsumerAction) MarshalJSON() ([]byte, error) {\n\tswitch a {\n\tcase ActionCreate:\n\t\treturn actionCreateJSONBytes, nil\n\tcase ActionUpdate:\n\t\treturn actionUpdateJSONBytes, nil\n\tcase ActionCreateOrUpdate:\n\t\treturn actionCreateOrUpdateJSONBytes, nil\n\tdefault:\n\t\treturn nil, fmt.Errorf(\"can not marshal %v\", a)\n\t}\n}\n\nfunc (a *ConsumerAction) UnmarshalJSON(data []byte) error {\n\tswitch string(data) {\n\tcase actionCreateJSONString:\n\t\t*a = ActionCreate\n\tcase actionUpdateJSONString:\n\t\t*a = ActionUpdate\n\tcase actionCreateOrUpdateJSONString:\n\t\t*a = ActionCreateOrUpdate\n\tdefault:\n\t\treturn fmt.Errorf(\"unknown consumer action: %v\", string(data))\n\t}\n\treturn nil\n}\n\n// ConsumerNakOptions is for optional NAK values, e.g. delay.\ntype ConsumerNakOptions struct {\n\tDelay time.Duration `json:\"delay\"`\n}\n\n// PriorityPolicy determines policy for selecting messages based on priority.\ntype PriorityPolicy int\n\nconst (\n\t// No priority policy.\n\tPriorityNone PriorityPolicy = iota\n\t// Clients will get the messages only if certain criteria are specified.\n\tPriorityOverflow\n\t// Single client takes over handling of the messages, while others are on standby.\n\tPriorityPinnedClient\n)\n\nconst (\n\tPriorityNoneJSONString         = `\"none\"`\n\tPriorityOverflowJSONString     = `\"overflow\"`\n\tPriorityPinnedClientJSONString = `\"pinned_client\"`\n)\n\nvar (\n\tPriorityNoneJSONBytes         = []byte(PriorityNoneJSONString)\n\tPriorityOverflowJSONBytes     = []byte(PriorityOverflowJSONString)\n\tPriorityPinnedClientJSONBytes = []byte(PriorityPinnedClientJSONString)\n)\n\nfunc (pp PriorityPolicy) String() string {\n\tswitch pp {\n\tcase PriorityOverflow:\n\t\treturn PriorityOverflowJSONString\n\tcase PriorityPinnedClient:\n\t\treturn PriorityPinnedClientJSONString\n\tdefault:\n\t\treturn PriorityNoneJSONString\n\t}\n}\n\nfunc (pp PriorityPolicy) MarshalJSON() ([]byte, error) {\n\tswitch pp {\n\tcase PriorityOverflow:\n\t\treturn PriorityOverflowJSONBytes, nil\n\tcase PriorityPinnedClient:\n\t\treturn PriorityPinnedClientJSONBytes, nil\n\tcase PriorityNone:\n\t\treturn PriorityNoneJSONBytes, nil\n\tdefault:\n\t\treturn nil, fmt.Errorf(\"unknown priority policy: %v\", pp)\n\t}\n}\n\nfunc (pp *PriorityPolicy) UnmarshalJSON(data []byte) error {\n\tswitch string(data) {\n\tcase PriorityOverflowJSONString:\n\t\t*pp = PriorityOverflow\n\tcase PriorityPinnedClientJSONString:\n\t\t*pp = PriorityPinnedClient\n\tcase PriorityNoneJSONString:\n\t\t*pp = PriorityNone\n\tdefault:\n\t\treturn fmt.Errorf(\"unknown priority policy: %v\", string(data))\n\t}\n\treturn nil\n}\n\n// DeliverPolicy determines how the consumer should select the first message to deliver.\ntype DeliverPolicy int\n\nconst (\n\t// DeliverAll will be the default so can be omitted from the request.\n\tDeliverAll DeliverPolicy = iota\n\t// DeliverLast will start the consumer with the last sequence received.\n\tDeliverLast\n\t// DeliverNew will only deliver new messages that are sent after the consumer is created.\n\tDeliverNew\n\t// DeliverByStartSequence will look for a defined starting sequence to start.\n\tDeliverByStartSequence\n\t// DeliverByStartTime will select the first messsage with a timestamp >= to StartTime.\n\tDeliverByStartTime\n\t// DeliverLastPerSubject will start the consumer with the last message for all subjects received.\n\tDeliverLastPerSubject\n)\n\nfunc (dp DeliverPolicy) String() string {\n\tswitch dp {\n\tcase DeliverAll:\n\t\treturn \"all\"\n\tcase DeliverLast:\n\t\treturn \"last\"\n\tcase DeliverNew:\n\t\treturn \"new\"\n\tcase DeliverByStartSequence:\n\t\treturn \"by_start_sequence\"\n\tcase DeliverByStartTime:\n\t\treturn \"by_start_time\"\n\tcase DeliverLastPerSubject:\n\t\treturn \"last_per_subject\"\n\tdefault:\n\t\treturn \"undefined\"\n\t}\n}\n\n// AckPolicy determines how the consumer should acknowledge delivered messages.\ntype AckPolicy int\n\nconst (\n\t// AckNone requires no acks for delivered messages.\n\tAckNone AckPolicy = iota\n\t// AckAll when acking a sequence number, this implicitly acks all sequences below this one as well.\n\tAckAll\n\t// AckExplicit requires ack or nack for all messages.\n\tAckExplicit\n)\n\nfunc (a AckPolicy) String() string {\n\tswitch a {\n\tcase AckNone:\n\t\treturn \"none\"\n\tcase AckAll:\n\t\treturn \"all\"\n\tdefault:\n\t\treturn \"explicit\"\n\t}\n}\n\n// ReplayPolicy determines how the consumer should replay messages it already has queued in the stream.\ntype ReplayPolicy int\n\nconst (\n\t// ReplayInstant will replay messages as fast as possible.\n\tReplayInstant ReplayPolicy = iota\n\t// ReplayOriginal will maintain the same timing as the messages were received.\n\tReplayOriginal\n)\n\nfunc (r ReplayPolicy) String() string {\n\tswitch r {\n\tcase ReplayInstant:\n\t\treturn replayInstantPolicyJSONString\n\tdefault:\n\t\treturn replayOriginalPolicyJSONString\n\t}\n}\n\n// OK\nconst OK = \"+OK\"\n\n// Ack responses. Note that a nil or no payload is same as AckAck\nvar (\n\t// Ack\n\tAckAck = []byte(\"+ACK\") // nil or no payload to ack subject also means ACK\n\tAckOK  = []byte(OK)     // deprecated but +OK meant ack as well.\n\n\t// Nack\n\tAckNak = []byte(\"-NAK\")\n\t// Progress indicator\n\tAckProgress = []byte(\"+WPI\")\n\t// Ack + Deliver the next message(s).\n\tAckNext = []byte(\"+NXT\")\n\t// Terminate delivery of the message.\n\tAckTerm = []byte(\"+TERM\")\n)\n\nconst (\n\t// reasons to supply when terminating messages using limits\n\tackTermLimitsReason        = \"Message deleted by stream limits\"\n\tackTermUnackedLimitsReason = \"Unacknowledged message was deleted\"\n)\n\n// Calculate accurate replicas for the consumer config with the parent stream config.\nfunc (consCfg ConsumerConfig) replicas(strCfg *StreamConfig) int {\n\tif consCfg.Replicas == 0 || consCfg.Replicas > strCfg.Replicas {\n\t\tif !isDurableConsumer(&consCfg) && strCfg.Retention == LimitsPolicy && consCfg.Replicas == 0 {\n\t\t\t// Matches old-school ephemerals only, where the replica count is 0.\n\t\t\treturn 1\n\t\t}\n\t\treturn strCfg.Replicas\n\t}\n\treturn consCfg.Replicas\n}\n\n// Consumer is a jetstream consumer.\ntype consumer struct {\n\t// Atomic used to notify that we want to process an ack.\n\t// This will be checked in checkPending to abort processing\n\t// and let ack be processed in priority.\n\tawl               int64\n\tleader            atomic.Bool\n\tmu                sync.RWMutex\n\tjs                *jetStream\n\tmset              *stream\n\tacc               *Account\n\tsrv               *Server\n\tclient            *client\n\tsysc              *client\n\tsid               int\n\tname              string\n\tstream            string\n\tsseq              uint64         // next stream sequence\n\tsubjf             subjectFilters // subject filters and their sequences\n\tfilters           *Sublist       // When we have multiple filters we will use LoadNextMsgMulti and pass this in.\n\tdseq              uint64         // delivered consumer sequence\n\tadflr             uint64         // ack delivery floor\n\tasflr             uint64         // ack store floor\n\tchkflr            uint64         // our check floor, interest streams only.\n\tnpc               int64          // Num Pending Count\n\tnpf               uint64         // Num Pending Floor Sequence\n\tdsubj             string\n\tqgroup            string\n\tlss               *lastSeqSkipList\n\trlimit            *rate.Limiter\n\treqSub            *subscription\n\tackSub            *subscription\n\tackReplyT         string\n\tackSubj           string\n\tnextMsgSubj       string\n\tnextMsgReqs       *ipQueue[*nextMsgReq]\n\tmaxp              int\n\tpblimit           int\n\tmaxpb             int\n\tpbytes            int\n\tfcsz              int\n\tfcid              string\n\tfcSub             *subscription\n\toutq              *jsOutQ\n\tpending           map[uint64]*Pending\n\tptmr              *time.Timer\n\tptmrEnd           time.Time\n\trdq               []uint64\n\trdqi              avl.SequenceSet\n\trdc               map[uint64]uint64\n\treplies           map[uint64]string\n\tpendingDeliveries map[uint64]*jsPubMsg // Messages that can be delivered after achieving quorum.\n\tmaxdc             uint64\n\twaiting           *waitQueue\n\tcfg               ConsumerConfig\n\tici               *ConsumerInfo\n\tstore             ConsumerStore\n\tactive            bool\n\treplay            bool\n\tdtmr              *time.Timer\n\tuptmr             *time.Timer // Unpause timer\n\tgwdtmr            *time.Timer\n\tdthresh           time.Duration\n\tmch               chan struct{} // Message channel\n\tqch               chan struct{} // Quit channel\n\tinch              chan bool     // Interest change channel\n\tsfreq             int32\n\tackEventT         string\n\tnakEventT         string\n\tdeliveryExcEventT string\n\tcreated           time.Time\n\tldt               time.Time\n\tlat               time.Time\n\tlwqic             time.Time\n\tclosed            bool\n\n\t// Clustered.\n\tca        *consumerAssignment\n\tnode      RaftNode\n\tinfoSub   *subscription\n\tlqsent    time.Time\n\tprm       map[string]struct{}\n\tprOk      bool\n\tuch       chan struct{}\n\tretention RetentionPolicy\n\n\tmonitorWg sync.WaitGroup\n\tinMonitor bool\n\n\t// R>1 proposals\n\tpch   chan struct{}\n\tphead *proposal\n\tptail *proposal\n\n\t// Ack queue\n\tackMsgs *ipQueue[*jsAckMsg]\n\n\t// for stream signaling when multiple filters are set.\n\tsigSubs []string\n\n\t// Priority groups\n\t// Details described in ADR-42.\n\n\t// currentPinId is the current nuid for the pinned consumer.\n\t// If the  Consumer is running in `PriorityPinnedClient` mode, server will\n\t// pick up a new nuid and assign it to first pending pull request.\n\tcurrentPinId string\n\t/// pinnedTtl is the remaining time before the current PinId expires.\n\tpinnedTtl *time.Timer\n\tpinnedTS  time.Time\n}\n\n// A single subject filter.\ntype subjectFilter struct {\n\tsubject          string\n\ttokenizedSubject []string\n\thasWildcard      bool\n}\n\ntype subjectFilters []*subjectFilter\n\n// subjects is a helper function used for updating consumers.\n// It is not used and should not be used in hotpath.\nfunc (s subjectFilters) subjects() []string {\n\tsubjects := make([]string, 0, len(s))\n\tfor _, filter := range s {\n\t\tsubjects = append(subjects, filter.subject)\n\t}\n\treturn subjects\n}\n\ntype proposal struct {\n\tdata []byte\n\tnext *proposal\n}\n\nconst (\n\t// JsAckWaitDefault is the default AckWait, only applicable on explicit ack policy consumers.\n\tJsAckWaitDefault = 30 * time.Second\n\t// JsDeleteWaitTimeDefault is the default amount of time we will wait for non-durable\n\t// consumers to be in an inactive state before deleting them.\n\tJsDeleteWaitTimeDefault = 5 * time.Second\n\t// JsFlowControlMaxPending specifies default pending bytes during flow control that can be outstanding.\n\tJsFlowControlMaxPending = 32 * 1024 * 1024\n\t// JsDefaultMaxAckPending is set for consumers with explicit ack that do not set the max ack pending.\n\tJsDefaultMaxAckPending = 1000\n\t// JsDefaultPinnedTTL is the default grace period for the pinned consumer to send a new request before a new pin\n\t// is picked by a server.\n\tJsDefaultPinnedTTL = 2 * time.Minute\n)\n\n// Helper function to set consumer config defaults from above.\nfunc setConsumerConfigDefaults(config *ConsumerConfig, streamCfg *StreamConfig, lim *JSLimitOpts, accLim *JetStreamAccountLimits, pedantic bool) *ApiError {\n\t// Set to default if not specified.\n\tif config.DeliverSubject == _EMPTY_ && config.MaxWaiting == 0 {\n\t\tconfig.MaxWaiting = JSWaitQueueDefaultMax\n\t}\n\t// Setup proper default for ack wait if we are in explicit ack mode.\n\tif config.AckWait == 0 && (config.AckPolicy == AckExplicit || config.AckPolicy == AckAll) {\n\t\tconfig.AckWait = JsAckWaitDefault\n\t}\n\t// Setup default of -1, meaning no limit for MaxDeliver.\n\tif config.MaxDeliver == 0 {\n\t\tconfig.MaxDeliver = -1\n\t}\n\t// If BackOff was specified that will override the AckWait and the MaxDeliver.\n\tif len(config.BackOff) > 0 {\n\t\tif pedantic && config.AckWait != config.BackOff[0] {\n\t\t\treturn NewJSPedanticError(errors.New(\"first backoff value has to equal batch AckWait\"))\n\t\t}\n\t\tconfig.AckWait = config.BackOff[0]\n\t}\n\tif config.MaxAckPending == 0 {\n\t\tif pedantic && streamCfg.ConsumerLimits.MaxAckPending > 0 {\n\t\t\treturn NewJSPedanticError(errors.New(\"max_ack_pending must be set if it's configured in stream limits\"))\n\t\t}\n\t\tconfig.MaxAckPending = streamCfg.ConsumerLimits.MaxAckPending\n\t}\n\tif config.InactiveThreshold == 0 {\n\t\tif pedantic && streamCfg.ConsumerLimits.InactiveThreshold > 0 {\n\t\t\treturn NewJSPedanticError(errors.New(\"inactive_threshold must be set if it's configured in stream limits\"))\n\t\t}\n\t\tconfig.InactiveThreshold = streamCfg.ConsumerLimits.InactiveThreshold\n\t}\n\t// Set proper default for max ack pending if we are ack explicit and none has been set.\n\tif (config.AckPolicy == AckExplicit || config.AckPolicy == AckAll) && config.MaxAckPending == 0 {\n\t\taccPending := JsDefaultMaxAckPending\n\t\tif lim.MaxAckPending > 0 && lim.MaxAckPending < accPending {\n\t\t\taccPending = lim.MaxAckPending\n\t\t}\n\t\tif accLim.MaxAckPending > 0 && accLim.MaxAckPending < accPending {\n\t\t\taccPending = accLim.MaxAckPending\n\t\t}\n\t\tconfig.MaxAckPending = accPending\n\t}\n\t// if applicable set max request batch size\n\tif config.DeliverSubject == _EMPTY_ && config.MaxRequestBatch == 0 && lim.MaxRequestBatch > 0 {\n\t\tif pedantic {\n\t\t\treturn NewJSPedanticError(errors.New(\"max_request_batch must be set if it's JetStream limits are set\"))\n\t\t}\n\t\tconfig.MaxRequestBatch = lim.MaxRequestBatch\n\t}\n\n\t// set the default value only if pinned policy is used.\n\tif config.PriorityPolicy == PriorityPinnedClient && config.PinnedTTL == 0 {\n\t\tconfig.PinnedTTL = JsDefaultPinnedTTL\n\t}\n\treturn nil\n}\n\n// Check the consumer config. If we are recovering don't check filter subjects.\nfunc checkConsumerCfg(\n\tconfig *ConsumerConfig,\n\tsrvLim *JSLimitOpts,\n\tcfg *StreamConfig,\n\t_ *Account,\n\taccLim *JetStreamAccountLimits,\n\tisRecovering bool,\n) *ApiError {\n\n\t// Check if replicas is defined but exceeds parent stream.\n\tif config.Replicas > 0 && config.Replicas > cfg.Replicas {\n\t\treturn NewJSConsumerReplicasExceedsStreamError()\n\t}\n\t// Check that it is not negative\n\tif config.Replicas < 0 {\n\t\treturn NewJSReplicasCountCannotBeNegativeError()\n\t}\n\t// If the stream is interest or workqueue retention make sure the replicas\n\t// match that of the stream. This is REQUIRED for now.\n\tif cfg.Retention == InterestPolicy || cfg.Retention == WorkQueuePolicy {\n\t\t// Only error here if not recovering.\n\t\t// We handle recovering in a different spot to allow consumer to come up\n\t\t// if previous version allowed it to be created. We do not want it to not come up.\n\t\tif !isRecovering && config.Replicas != 0 && config.Replicas != cfg.Replicas {\n\t\t\treturn NewJSConsumerReplicasShouldMatchStreamError()\n\t\t}\n\t}\n\n\t// Check if we have a BackOff defined that MaxDeliver is within range etc.\n\tif lbo := len(config.BackOff); lbo > 0 && config.MaxDeliver != -1 && lbo > config.MaxDeliver {\n\t\treturn NewJSConsumerMaxDeliverBackoffError()\n\t}\n\n\tif len(config.Description) > JSMaxDescriptionLen {\n\t\treturn NewJSConsumerDescriptionTooLongError(JSMaxDescriptionLen)\n\t}\n\n\t// For now expect a literal subject if its not empty. Empty means work queue mode (pull mode).\n\tif config.DeliverSubject != _EMPTY_ {\n\t\tif !subjectIsLiteral(config.DeliverSubject) {\n\t\t\treturn NewJSConsumerDeliverToWildcardsError()\n\t\t}\n\t\tif !IsValidSubject(config.DeliverSubject) {\n\t\t\treturn NewJSConsumerInvalidDeliverSubjectError()\n\t\t}\n\t\tif deliveryFormsCycle(cfg, config.DeliverSubject) {\n\t\t\treturn NewJSConsumerDeliverCycleError()\n\t\t}\n\t\tif config.MaxWaiting != 0 {\n\t\t\treturn NewJSConsumerPushMaxWaitingError()\n\t\t}\n\t\tif config.MaxAckPending > 0 && config.AckPolicy == AckNone {\n\t\t\treturn NewJSConsumerMaxPendingAckPolicyRequiredError()\n\t\t}\n\t\tif config.Heartbeat > 0 && config.Heartbeat < 100*time.Millisecond {\n\t\t\treturn NewJSConsumerSmallHeartbeatError()\n\t\t}\n\t} else {\n\t\t// Pull mode with work queue retention from the stream requires an explicit ack.\n\t\tif config.AckPolicy == AckNone && cfg.Retention == WorkQueuePolicy {\n\t\t\treturn NewJSConsumerPullRequiresAckError()\n\t\t}\n\t\tif config.RateLimit > 0 {\n\t\t\treturn NewJSConsumerPullWithRateLimitError()\n\t\t}\n\t\tif config.MaxWaiting < 0 {\n\t\t\treturn NewJSConsumerMaxWaitingNegativeError()\n\t\t}\n\t\tif config.Heartbeat > 0 {\n\t\t\treturn NewJSConsumerHBRequiresPushError()\n\t\t}\n\t\tif config.FlowControl {\n\t\t\treturn NewJSConsumerFCRequiresPushError()\n\t\t}\n\t\tif config.MaxRequestBatch < 0 {\n\t\t\treturn NewJSConsumerMaxRequestBatchNegativeError()\n\t\t}\n\t\tif config.MaxRequestExpires != 0 && config.MaxRequestExpires < time.Millisecond {\n\t\t\treturn NewJSConsumerMaxRequestExpiresToSmallError()\n\t\t}\n\t\tif srvLim.MaxRequestBatch > 0 && config.MaxRequestBatch > srvLim.MaxRequestBatch {\n\t\t\treturn NewJSConsumerMaxRequestBatchExceededError(srvLim.MaxRequestBatch)\n\t\t}\n\t}\n\tif srvLim.MaxAckPending > 0 && config.MaxAckPending > srvLim.MaxAckPending {\n\t\treturn NewJSConsumerMaxPendingAckExcessError(srvLim.MaxAckPending)\n\t}\n\tif accLim.MaxAckPending > 0 && config.MaxAckPending > accLim.MaxAckPending {\n\t\treturn NewJSConsumerMaxPendingAckExcessError(accLim.MaxAckPending)\n\t}\n\tif cfg.ConsumerLimits.MaxAckPending > 0 && config.MaxAckPending > cfg.ConsumerLimits.MaxAckPending {\n\t\treturn NewJSConsumerMaxPendingAckExcessError(cfg.ConsumerLimits.MaxAckPending)\n\t}\n\tif cfg.ConsumerLimits.InactiveThreshold > 0 && config.InactiveThreshold > cfg.ConsumerLimits.InactiveThreshold {\n\t\treturn NewJSConsumerInactiveThresholdExcessError(cfg.ConsumerLimits.InactiveThreshold)\n\t}\n\n\t// Direct need to be non-mapped ephemerals.\n\tif config.Direct {\n\t\tif config.DeliverSubject == _EMPTY_ {\n\t\t\treturn NewJSConsumerDirectRequiresPushError()\n\t\t}\n\t\tif isDurableConsumer(config) {\n\t\t\treturn NewJSConsumerDirectRequiresEphemeralError()\n\t\t}\n\t}\n\n\t// Do not allow specifying both FilterSubject and FilterSubjects,\n\t// as that's probably unintentional without any difference from passing\n\t// all filters in FilterSubjects.\n\tif config.FilterSubject != _EMPTY_ && len(config.FilterSubjects) > 0 {\n\t\treturn NewJSConsumerDuplicateFilterSubjectsError()\n\t}\n\n\tif config.FilterSubject != _EMPTY_ && !IsValidSubject(config.FilterSubject) {\n\t\treturn NewJSStreamInvalidConfigError(ErrBadSubject)\n\t}\n\n\t// We treat FilterSubjects: []string{\"\"} as a misconfig, so we validate against it.\n\tfor _, filter := range config.FilterSubjects {\n\t\tif filter == _EMPTY_ {\n\t\t\treturn NewJSConsumerEmptyFilterError()\n\t\t}\n\t}\n\tsubjectFilters := gatherSubjectFilters(config.FilterSubject, config.FilterSubjects)\n\n\t// Check subject filters do not overlap.\n\tfor outer, subject := range subjectFilters {\n\t\tif !IsValidSubject(subject) {\n\t\t\treturn NewJSStreamInvalidConfigError(ErrBadSubject)\n\t\t}\n\t\tfor inner, ssubject := range subjectFilters {\n\t\t\tif inner != outer && SubjectsCollide(subject, ssubject) {\n\t\t\t\treturn NewJSConsumerOverlappingSubjectFiltersError()\n\t\t\t}\n\t\t}\n\t}\n\n\t// Helper function to formulate similar errors.\n\tbadStart := func(dp, start string) error {\n\t\treturn fmt.Errorf(\"consumer delivery policy is deliver %s, but optional start %s is also set\", dp, start)\n\t}\n\tnotSet := func(dp, notSet string) error {\n\t\treturn fmt.Errorf(\"consumer delivery policy is deliver %s, but optional %s is not set\", dp, notSet)\n\t}\n\n\t// Check on start position conflicts.\n\tswitch config.DeliverPolicy {\n\tcase DeliverAll:\n\t\tif config.OptStartSeq > 0 {\n\t\t\treturn NewJSConsumerInvalidPolicyError(badStart(\"all\", \"sequence\"))\n\t\t}\n\t\tif config.OptStartTime != nil {\n\t\t\treturn NewJSConsumerInvalidPolicyError(badStart(\"all\", \"time\"))\n\t\t}\n\tcase DeliverLast:\n\t\tif config.OptStartSeq > 0 {\n\t\t\treturn NewJSConsumerInvalidPolicyError(badStart(\"last\", \"sequence\"))\n\t\t}\n\t\tif config.OptStartTime != nil {\n\t\t\treturn NewJSConsumerInvalidPolicyError(badStart(\"last\", \"time\"))\n\t\t}\n\tcase DeliverLastPerSubject:\n\t\tif config.OptStartSeq > 0 {\n\t\t\treturn NewJSConsumerInvalidPolicyError(badStart(\"last per subject\", \"sequence\"))\n\t\t}\n\t\tif config.OptStartTime != nil {\n\t\t\treturn NewJSConsumerInvalidPolicyError(badStart(\"last per subject\", \"time\"))\n\t\t}\n\t\tif config.FilterSubject == _EMPTY_ && len(config.FilterSubjects) == 0 {\n\t\t\treturn NewJSConsumerInvalidPolicyError(notSet(\"last per subject\", \"filter subject\"))\n\t\t}\n\tcase DeliverNew:\n\t\tif config.OptStartSeq > 0 {\n\t\t\treturn NewJSConsumerInvalidPolicyError(badStart(\"new\", \"sequence\"))\n\t\t}\n\t\tif config.OptStartTime != nil {\n\t\t\treturn NewJSConsumerInvalidPolicyError(badStart(\"new\", \"time\"))\n\t\t}\n\tcase DeliverByStartSequence:\n\t\tif config.OptStartSeq == 0 {\n\t\t\treturn NewJSConsumerInvalidPolicyError(notSet(\"by start sequence\", \"start sequence\"))\n\t\t}\n\t\tif config.OptStartTime != nil {\n\t\t\treturn NewJSConsumerInvalidPolicyError(badStart(\"by start sequence\", \"time\"))\n\t\t}\n\tcase DeliverByStartTime:\n\t\tif config.OptStartTime == nil {\n\t\t\treturn NewJSConsumerInvalidPolicyError(notSet(\"by start time\", \"start time\"))\n\t\t}\n\t\tif config.OptStartSeq != 0 {\n\t\t\treturn NewJSConsumerInvalidPolicyError(badStart(\"by start time\", \"start sequence\"))\n\t\t}\n\t}\n\n\tif config.SampleFrequency != _EMPTY_ {\n\t\ts := strings.TrimSuffix(config.SampleFrequency, \"%\")\n\t\tif sampleFreq, err := strconv.Atoi(s); err != nil || sampleFreq < 0 {\n\t\t\treturn NewJSConsumerInvalidSamplingError(err)\n\t\t}\n\t}\n\n\t// We reject if flow control is set without heartbeats.\n\tif config.FlowControl && config.Heartbeat == 0 {\n\t\treturn NewJSConsumerWithFlowControlNeedsHeartbeatsError()\n\t}\n\n\tif config.Durable != _EMPTY_ && config.Name != _EMPTY_ {\n\t\tif config.Name != config.Durable {\n\t\t\treturn NewJSConsumerCreateDurableAndNameMismatchError()\n\t\t}\n\t}\n\n\tvar metadataLen int\n\tfor k, v := range config.Metadata {\n\t\tmetadataLen += len(k) + len(v)\n\t}\n\tif metadataLen > JSMaxMetadataLen {\n\t\treturn NewJSConsumerMetadataLengthError(fmt.Sprintf(\"%dKB\", JSMaxMetadataLen/1024))\n\t}\n\n\tif config.PriorityPolicy != PriorityNone {\n\t\tif len(config.PriorityGroups) == 0 {\n\t\t\treturn NewJSConsumerPriorityPolicyWithoutGroupError()\n\t\t}\n\n\t\tfor _, group := range config.PriorityGroups {\n\t\t\tif group == _EMPTY_ {\n\t\t\t\treturn NewJSConsumerEmptyGroupNameError()\n\t\t\t}\n\t\t\tif !validGroupName.MatchString(group) {\n\t\t\t\treturn NewJSConsumerInvalidGroupNameError()\n\t\t\t}\n\t\t}\n\t}\n\n\t// For now don't allow preferred server in placement.\n\tif cfg.Placement != nil && cfg.Placement.Preferred != _EMPTY_ {\n\t\treturn NewJSStreamInvalidConfigError(fmt.Errorf(\"preferred server not permitted in placement\"))\n\t}\n\n\treturn nil\n}\n\nfunc (mset *stream) addConsumerWithAction(config *ConsumerConfig, action ConsumerAction, pedantic bool) (*consumer, error) {\n\treturn mset.addConsumerWithAssignment(config, _EMPTY_, nil, false, action, pedantic)\n}\n\nfunc (mset *stream) addConsumer(config *ConsumerConfig) (*consumer, error) {\n\treturn mset.addConsumerWithAction(config, ActionCreateOrUpdate, false)\n}\n\nfunc (mset *stream) addConsumerWithAssignment(config *ConsumerConfig, oname string, ca *consumerAssignment, isRecovering bool, action ConsumerAction, pedantic bool) (*consumer, error) {\n\t// Check if this stream has closed.\n\tif mset.closed.Load() {\n\t\treturn nil, NewJSStreamInvalidError()\n\t}\n\n\tmset.mu.RLock()\n\ts, jsa, cfg, acc := mset.srv, mset.jsa, mset.cfg, mset.acc\n\tmset.mu.RUnlock()\n\n\t// If we do not have the consumer currently assigned to us in cluster mode we will proceed but warn.\n\t// This can happen on startup with restored state where on meta replay we still do not have\n\t// the assignment. Running in single server mode this always returns true.\n\tif oname != _EMPTY_ && !jsa.consumerAssigned(mset.name(), oname) {\n\t\ts.Debugf(\"Consumer %q > %q does not seem to be assigned to this server\", mset.name(), oname)\n\t}\n\n\tif config == nil {\n\t\treturn nil, NewJSConsumerConfigRequiredError()\n\t}\n\n\tselectedLimits, _, _, _ := acc.selectLimits(config.replicas(&cfg))\n\tif selectedLimits == nil {\n\t\treturn nil, NewJSNoLimitsError()\n\t}\n\n\tsrvLim := &s.getOpts().JetStreamLimits\n\t// Make sure we have sane defaults. Do so with the JS lock, otherwise a\n\t// badly timed meta snapshot can result in a race condition.\n\tmset.js.mu.Lock()\n\terr := setConsumerConfigDefaults(config, &cfg, srvLim, selectedLimits, pedantic)\n\tmset.js.mu.Unlock()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tif err := checkConsumerCfg(config, srvLim, &cfg, acc, selectedLimits, isRecovering); err != nil {\n\t\treturn nil, err\n\t}\n\tsampleFreq := 0\n\tif config.SampleFrequency != _EMPTY_ {\n\t\t// Can't fail as checkConsumerCfg checks correct format\n\t\tsampleFreq, _ = strconv.Atoi(strings.TrimSuffix(config.SampleFrequency, \"%\"))\n\t}\n\n\t// Grab the client, account and server reference.\n\tc := mset.client\n\tif c == nil {\n\t\treturn nil, NewJSStreamInvalidError()\n\t}\n\tvar accName string\n\tc.mu.Lock()\n\ts, a := c.srv, c.acc\n\tif a != nil {\n\t\taccName = a.Name\n\t}\n\tc.mu.Unlock()\n\n\t// Hold mset lock here.\n\tmset.mu.Lock()\n\tif mset.client == nil || mset.store == nil || mset.consumers == nil {\n\t\tmset.mu.Unlock()\n\t\treturn nil, NewJSStreamInvalidError()\n\t}\n\n\t// If this one is durable and already exists, we let that be ok as long as only updating what should be allowed.\n\tvar cName string\n\tif isDurableConsumer(config) {\n\t\tcName = config.Durable\n\t} else if config.Name != _EMPTY_ {\n\t\tcName = config.Name\n\t}\n\tif cName != _EMPTY_ {\n\t\tif eo, ok := mset.consumers[cName]; ok {\n\t\t\tmset.mu.Unlock()\n\t\t\tif action == ActionCreate {\n\t\t\t\tocfg := eo.config()\n\t\t\t\tcopyConsumerMetadata(config, &ocfg)\n\t\t\t\tif !reflect.DeepEqual(config, &ocfg) {\n\t\t\t\t\treturn nil, NewJSConsumerAlreadyExistsError()\n\t\t\t\t}\n\t\t\t}\n\t\t\t// Check for overlapping subjects if we are a workqueue\n\t\t\tif cfg.Retention == WorkQueuePolicy {\n\t\t\t\tsubjects := gatherSubjectFilters(config.FilterSubject, config.FilterSubjects)\n\t\t\t\tif !mset.partitionUnique(cName, subjects) {\n\t\t\t\t\treturn nil, NewJSConsumerWQConsumerNotUniqueError()\n\t\t\t\t}\n\t\t\t}\n\t\t\terr := eo.updateConfig(config)\n\t\t\tif err == nil {\n\t\t\t\treturn eo, nil\n\t\t\t}\n\t\t\treturn nil, NewJSConsumerCreateError(err, Unless(err))\n\t\t}\n\t}\n\tif action == ActionUpdate {\n\t\tmset.mu.Unlock()\n\t\treturn nil, NewJSConsumerDoesNotExistError()\n\t}\n\n\t// Check for any limits, if the config for the consumer sets a limit we check against that\n\t// but if not we use the value from account limits, if account limits is more restrictive\n\t// than stream config we prefer the account limits to handle cases where account limits are\n\t// updated during the lifecycle of the stream\n\tmaxc := cfg.MaxConsumers\n\tif maxc <= 0 || (selectedLimits.MaxConsumers > 0 && selectedLimits.MaxConsumers < maxc) {\n\t\tmaxc = selectedLimits.MaxConsumers\n\t}\n\tif maxc > 0 && mset.numPublicConsumers() >= maxc {\n\t\tmset.mu.Unlock()\n\t\treturn nil, NewJSMaximumConsumersLimitError()\n\t}\n\n\t// Check on stream type conflicts with WorkQueues.\n\tif cfg.Retention == WorkQueuePolicy && !config.Direct {\n\t\t// Force explicit acks here.\n\t\tif config.AckPolicy != AckExplicit {\n\t\t\tmset.mu.Unlock()\n\t\t\treturn nil, NewJSConsumerWQRequiresExplicitAckError()\n\t\t}\n\n\t\tif len(mset.consumers) > 0 {\n\t\t\tsubjects := gatherSubjectFilters(config.FilterSubject, config.FilterSubjects)\n\t\t\tif len(subjects) == 0 {\n\t\t\t\tmset.mu.Unlock()\n\t\t\t\treturn nil, NewJSConsumerWQMultipleUnfilteredError()\n\t\t\t} else if !mset.partitionUnique(cName, subjects) {\n\t\t\t\t// Prior to v2.9.7, on a stream with WorkQueue policy, the servers\n\t\t\t\t// were not catching the error of having multiple consumers with\n\t\t\t\t// overlapping filter subjects depending on the scope, for instance\n\t\t\t\t// creating \"foo.*.bar\" and then \"foo.>\" was not detected, while\n\t\t\t\t// \"foo.>\" and then \"foo.*.bar\" would have been. Failing here\n\t\t\t\t// in recovery mode would leave the rejected consumer in a bad state,\n\t\t\t\t// so we will simply warn here, asking the user to remove this\n\t\t\t\t// consumer administratively. Otherwise, if this is the creation\n\t\t\t\t// of a new consumer, we will return the error.\n\t\t\t\tif isRecovering {\n\t\t\t\t\ts.Warnf(\"Consumer %q > %q has a filter subject that overlaps \"+\n\t\t\t\t\t\t\"with other consumers, which is not allowed for a stream \"+\n\t\t\t\t\t\t\"with WorkQueue policy, it should be administratively deleted\",\n\t\t\t\t\t\tcfg.Name, cName)\n\t\t\t\t} else {\n\t\t\t\t\t// We have a partition but it is not unique amongst the others.\n\t\t\t\t\tmset.mu.Unlock()\n\t\t\t\t\treturn nil, NewJSConsumerWQConsumerNotUniqueError()\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tif config.DeliverPolicy != DeliverAll {\n\t\t\tmset.mu.Unlock()\n\t\t\treturn nil, NewJSConsumerWQConsumerNotDeliverAllError()\n\t\t}\n\t}\n\n\t// Set name, which will be durable name if set, otherwise we create one at random.\n\to := &consumer{\n\t\tmset:      mset,\n\t\tjs:        s.getJetStream(),\n\t\tacc:       a,\n\t\tsrv:       s,\n\t\tclient:    s.createInternalJetStreamClient(),\n\t\tsysc:      s.createInternalJetStreamClient(),\n\t\tcfg:       *config,\n\t\tdsubj:     config.DeliverSubject,\n\t\toutq:      mset.outq,\n\t\tactive:    true,\n\t\tqch:       make(chan struct{}),\n\t\tuch:       make(chan struct{}, 1),\n\t\tmch:       make(chan struct{}, 1),\n\t\tsfreq:     int32(sampleFreq),\n\t\tmaxdc:     uint64(config.MaxDeliver),\n\t\tmaxp:      config.MaxAckPending,\n\t\tretention: cfg.Retention,\n\t\tcreated:   time.Now().UTC(),\n\t}\n\n\t// Bind internal client to the user account.\n\to.client.registerWithAccount(a)\n\t// Bind to the system account.\n\to.sysc.registerWithAccount(s.SystemAccount())\n\n\tif isDurableConsumer(config) {\n\t\tif len(config.Durable) > JSMaxNameLen {\n\t\t\tmset.mu.Unlock()\n\t\t\to.deleteWithoutAdvisory()\n\t\t\treturn nil, NewJSConsumerNameTooLongError(JSMaxNameLen)\n\t\t}\n\t\to.name = config.Durable\n\t} else if oname != _EMPTY_ {\n\t\to.name = oname\n\t} else {\n\t\tif config.Name != _EMPTY_ {\n\t\t\to.name = config.Name\n\t\t} else {\n\t\t\t// Legacy ephemeral auto-generated.\n\t\t\tfor {\n\t\t\t\to.name = createConsumerName()\n\t\t\t\tif _, ok := mset.consumers[o.name]; !ok {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t\tconfig.Name = o.name\n\t\t}\n\t}\n\t// Create ackMsgs queue now that we have a consumer name\n\to.ackMsgs = newIPQueue[*jsAckMsg](s, fmt.Sprintf(\"[ACC:%s] consumer '%s' on stream '%s' ackMsgs\", accName, o.name, cfg.Name))\n\n\t// Create our request waiting queue.\n\tif o.isPullMode() {\n\t\to.waiting = newWaitQueue(config.MaxWaiting)\n\t\t// Create our internal queue for next msg requests.\n\t\to.nextMsgReqs = newIPQueue[*nextMsgReq](s, fmt.Sprintf(\"[ACC:%s] consumer '%s' on stream '%s' pull requests\", accName, o.name, cfg.Name))\n\t}\n\n\t// already under lock, mset.Name() would deadlock\n\to.stream = cfg.Name\n\to.ackEventT = JSMetricConsumerAckPre + \".\" + o.stream + \".\" + o.name\n\to.nakEventT = JSAdvisoryConsumerMsgNakPre + \".\" + o.stream + \".\" + o.name\n\to.deliveryExcEventT = JSAdvisoryConsumerMaxDeliveryExceedPre + \".\" + o.stream + \".\" + o.name\n\n\tif !isValidName(o.name) {\n\t\tmset.mu.Unlock()\n\t\to.deleteWithoutAdvisory()\n\t\treturn nil, NewJSConsumerBadDurableNameError()\n\t}\n\n\t// Setup our storage if not a direct consumer.\n\tif !config.Direct {\n\t\tstore, err := mset.store.ConsumerStore(o.name, config)\n\t\tif err != nil {\n\t\t\tmset.mu.Unlock()\n\t\t\to.deleteWithoutAdvisory()\n\t\t\treturn nil, NewJSConsumerStoreFailedError(err)\n\t\t}\n\t\to.store = store\n\t}\n\n\tfor _, filter := range gatherSubjectFilters(o.cfg.FilterSubject, o.cfg.FilterSubjects) {\n\t\tsub := &subjectFilter{\n\t\t\tsubject:          filter,\n\t\t\thasWildcard:      subjectHasWildcard(filter),\n\t\t\ttokenizedSubject: tokenizeSubjectIntoSlice(nil, filter),\n\t\t}\n\t\to.subjf = append(o.subjf, sub)\n\t}\n\n\t// If we have multiple filter subjects, create a sublist which we will use\n\t// in calling store.LoadNextMsgMulti.\n\tif len(o.cfg.FilterSubjects) > 0 {\n\t\to.filters = NewSublistNoCache()\n\t\tfor _, filter := range o.cfg.FilterSubjects {\n\t\t\to.filters.Insert(&subscription{subject: []byte(filter)})\n\t\t}\n\t} else {\n\t\t// Make sure this is nil otherwise.\n\t\to.filters = nil\n\t}\n\n\tif o.store != nil && o.store.HasState() {\n\t\t// Restore our saved state.\n\t\to.mu.Lock()\n\t\to.readStoredState()\n\t\to.mu.Unlock()\n\t} else {\n\t\t// Select starting sequence number\n\t\to.selectStartingSeqNo()\n\t}\n\n\t// Now register with mset and create the ack subscription.\n\t// Check if we already have this one registered.\n\tif eo, ok := mset.consumers[o.name]; ok {\n\t\tmset.mu.Unlock()\n\t\tif !o.isDurable() || !o.isPushMode() {\n\t\t\to.name = _EMPTY_ // Prevent removal since same name.\n\t\t\to.deleteWithoutAdvisory()\n\t\t\treturn nil, NewJSConsumerNameExistError()\n\t\t}\n\t\t// If we are here we have already registered this durable. If it is still active that is an error.\n\t\tif eo.isActive() {\n\t\t\to.name = _EMPTY_ // Prevent removal since same name.\n\t\t\to.deleteWithoutAdvisory()\n\t\t\treturn nil, NewJSConsumerExistingActiveError()\n\t\t}\n\t\t// Since we are here this means we have a potentially new durable so we should update here.\n\t\t// Check that configs are the same.\n\t\tif !configsEqualSansDelivery(o.cfg, eo.cfg) {\n\t\t\to.name = _EMPTY_ // Prevent removal since same name.\n\t\t\to.deleteWithoutAdvisory()\n\t\t\treturn nil, NewJSConsumerReplacementWithDifferentNameError()\n\t\t}\n\t\t// Once we are here we have a replacement push-based durable.\n\t\teo.updateDeliverSubject(o.cfg.DeliverSubject)\n\t\treturn eo, nil\n\t}\n\n\t// Set up the ack subscription for this consumer. Will use wildcard for all acks.\n\t// We will remember the template to generate replies with sequence numbers and use\n\t// that to scanf them back in.\n\t// Escape '%' in consumer and stream names, as `pre` is used as a template later\n\t// in consumer.ackReply(), resulting in erroneous formatting of the ack subject.\n\tmn := strings.ReplaceAll(cfg.Name, \"%\", \"%%\")\n\tpre := fmt.Sprintf(jsAckT, mn, strings.ReplaceAll(o.name, \"%\", \"%%\"))\n\to.ackReplyT = fmt.Sprintf(\"%s.%%d.%%d.%%d.%%d.%%d\", pre)\n\to.ackSubj = fmt.Sprintf(\"%s.*.*.*.*.*\", pre)\n\to.nextMsgSubj = fmt.Sprintf(JSApiRequestNextT, mn, o.name)\n\n\t// Check/update the inactive threshold\n\to.updateInactiveThreshold(&o.cfg)\n\n\tif o.isPushMode() {\n\t\t// Check if we are running only 1 replica and that the delivery subject has interest.\n\t\t// Check in place here for interest. Will setup properly in setLeader.\n\t\tif config.replicas(&cfg) == 1 {\n\t\t\tinterest := o.acc.sl.HasInterest(o.cfg.DeliverSubject)\n\t\t\tif !o.hasDeliveryInterest(interest) {\n\t\t\t\t// Let the interest come to us eventually, but setup delete timer.\n\t\t\t\to.updateDeliveryInterest(false)\n\t\t\t}\n\t\t}\n\t}\n\n\t// Set our ca.\n\tif ca != nil {\n\t\to.setConsumerAssignment(ca)\n\t}\n\n\t// Check if we have a rate limit set.\n\tif config.RateLimit != 0 {\n\t\to.setRateLimit(config.RateLimit)\n\t}\n\n\tmset.setConsumer(o)\n\tmset.mu.Unlock()\n\n\tif config.Direct || (!s.JetStreamIsClustered() && s.standAloneMode()) {\n\t\to.setLeader(true)\n\t}\n\n\t// This is always true in single server mode.\n\tif o.IsLeader() {\n\t\t// Send advisory.\n\t\tvar suppress bool\n\t\tif !s.standAloneMode() && ca == nil {\n\t\t\tsuppress = true\n\t\t} else if ca != nil {\n\t\t\tsuppress = ca.responded\n\t\t}\n\t\tif !suppress {\n\t\t\to.sendCreateAdvisory()\n\t\t}\n\t}\n\n\treturn o, nil\n}\n\n// Updates the consumer `dthresh` delete timer duration and set\n// cfg.InactiveThreshold to JsDeleteWaitTimeDefault for ephemerals\n// if not explicitly already specified by the user.\n// Lock should be held.\nfunc (o *consumer) updateInactiveThreshold(cfg *ConsumerConfig) {\n\t// Ephemerals will always have inactive thresholds.\n\tif !o.isDurable() && cfg.InactiveThreshold <= 0 {\n\t\t// Add in 1 sec of jitter above and beyond the default of 5s.\n\t\to.dthresh = JsDeleteWaitTimeDefault + 100*time.Millisecond + time.Duration(rand.Int63n(900))*time.Millisecond\n\t\t// Only stamp config with default sans jitter.\n\t\tcfg.InactiveThreshold = JsDeleteWaitTimeDefault\n\t} else if cfg.InactiveThreshold > 0 {\n\t\t// Add in up to 1 sec of jitter if pull mode.\n\t\tif o.isPullMode() {\n\t\t\to.dthresh = cfg.InactiveThreshold + 100*time.Millisecond + time.Duration(rand.Int63n(900))*time.Millisecond\n\t\t} else {\n\t\t\to.dthresh = cfg.InactiveThreshold\n\t\t}\n\t} else if cfg.InactiveThreshold <= 0 {\n\t\t// We accept InactiveThreshold be set to 0 (for durables)\n\t\to.dthresh = 0\n\t}\n}\n\n// Updates the paused state. If we are the leader and the pause deadline\n// hasn't passed yet then we will start a timer to kick the consumer once\n// that deadline is reached. Lock should be held.\nfunc (o *consumer) updatePauseState(cfg *ConsumerConfig) {\n\tif o.uptmr != nil {\n\t\tstopAndClearTimer(&o.uptmr)\n\t}\n\tif !o.isLeader() {\n\t\t// Only the leader will run the timer as only the leader will run\n\t\t// loopAndGatherMsgs.\n\t\treturn\n\t}\n\tif cfg.PauseUntil == nil || cfg.PauseUntil.IsZero() || cfg.PauseUntil.Before(time.Now()) {\n\t\t// Either the PauseUntil is unset (is effectively zero) or the\n\t\t// deadline has already passed, in which case there is nothing\n\t\t// to do.\n\t\treturn\n\t}\n\to.uptmr = time.AfterFunc(time.Until(*cfg.PauseUntil), func() {\n\t\to.mu.Lock()\n\t\tdefer o.mu.Unlock()\n\n\t\tstopAndClearTimer(&o.uptmr)\n\t\to.sendPauseAdvisoryLocked(&o.cfg)\n\t\to.signalNewMessages()\n\t})\n}\n\nfunc (o *consumer) consumerAssignment() *consumerAssignment {\n\to.mu.RLock()\n\tdefer o.mu.RUnlock()\n\treturn o.ca\n}\n\nfunc (o *consumer) setConsumerAssignment(ca *consumerAssignment) {\n\to.mu.Lock()\n\tdefer o.mu.Unlock()\n\n\to.ca = ca\n\tif ca == nil {\n\t\treturn\n\t}\n\t// Set our node.\n\to.node = ca.Group.node\n\n\t// Trigger update chan.\n\tselect {\n\tcase o.uch <- struct{}{}:\n\tdefault:\n\t}\n}\n\nfunc (o *consumer) updateC() <-chan struct{} {\n\to.mu.RLock()\n\tdefer o.mu.RUnlock()\n\treturn o.uch\n}\n\n// checkQueueInterest will check on our interest's queue group status.\n// Lock should be held.\nfunc (o *consumer) checkQueueInterest() {\n\tif !o.active || o.cfg.DeliverSubject == _EMPTY_ {\n\t\treturn\n\t}\n\tsubj := o.dsubj\n\tif subj == _EMPTY_ {\n\t\tsubj = o.cfg.DeliverSubject\n\t}\n\n\tif rr := o.acc.sl.Match(subj); len(rr.qsubs) > 0 {\n\t\t// Just grab first\n\t\tif qsubs := rr.qsubs[0]; len(qsubs) > 0 {\n\t\t\tif sub := rr.qsubs[0][0]; len(sub.queue) > 0 {\n\t\t\t\to.qgroup = string(sub.queue)\n\t\t\t}\n\t\t}\n\t}\n}\n\n// clears our node if we have one. When we scale down to 1.\nfunc (o *consumer) clearNode() {\n\to.mu.Lock()\n\tdefer o.mu.Unlock()\n\tif o.node != nil {\n\t\to.node.Delete()\n\t\to.node = nil\n\t}\n}\n\n// IsLeader will return if we are the current leader.\nfunc (o *consumer) IsLeader() bool {\n\treturn o.isLeader()\n}\n\n// Lock should be held.\nfunc (o *consumer) isLeader() bool {\n\treturn o.leader.Load()\n}\n\nfunc (o *consumer) setLeader(isLeader bool) {\n\to.mu.RLock()\n\tmset, closed := o.mset, o.closed\n\tmovingToClustered := o.node != nil && o.pch == nil\n\tmovingToNonClustered := o.node == nil && o.pch != nil\n\twasLeader := o.leader.Swap(isLeader)\n\to.mu.RUnlock()\n\n\t// If we are here we have a change in leader status.\n\tif isLeader {\n\t\tif closed || mset == nil {\n\t\t\treturn\n\t\t}\n\n\t\tif wasLeader {\n\t\t\t// If we detect we are scaling up, make sure to create clustered routines and channels.\n\t\t\tif movingToClustered {\n\t\t\t\to.mu.Lock()\n\t\t\t\t// We are moving from R1 to clustered.\n\t\t\t\to.pch = make(chan struct{}, 1)\n\t\t\t\tgo o.loopAndForwardProposals(o.qch)\n\t\t\t\tif o.phead != nil {\n\t\t\t\t\tselect {\n\t\t\t\t\tcase o.pch <- struct{}{}:\n\t\t\t\t\tdefault:\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\to.mu.Unlock()\n\t\t\t} else if movingToNonClustered {\n\t\t\t\t// We are moving from clustered to non-clustered now.\n\t\t\t\t// Set pch to nil so if we scale back up we will recreate the loopAndForward from above.\n\t\t\t\to.mu.Lock()\n\t\t\t\tpch := o.pch\n\t\t\t\to.pch = nil\n\t\t\t\tselect {\n\t\t\t\tcase pch <- struct{}{}:\n\t\t\t\tdefault:\n\t\t\t\t}\n\t\t\t\to.mu.Unlock()\n\t\t\t}\n\t\t\treturn\n\t\t}\n\n\t\tmset.mu.RLock()\n\t\ts, jsa, stream := mset.srv, mset.jsa, mset.getCfgName()\n\t\tmset.mu.RUnlock()\n\n\t\to.mu.Lock()\n\t\to.rdq = nil\n\t\to.rdqi.Empty()\n\n\t\t// Restore our saved state.\n\t\t// During non-leader status we just update our underlying store when not clustered.\n\t\t// If clustered we need to propose our initial (possibly skipped ahead) o.sseq to the group.\n\t\tif o.node == nil || o.dseq > 1 || (o.store != nil && o.store.HasState()) {\n\t\t\to.readStoredState()\n\t\t} else if o.node != nil && o.sseq >= 1 {\n\t\t\to.updateSkipped(o.sseq)\n\t\t}\n\n\t\t// Setup initial num pending.\n\t\to.streamNumPending()\n\n\t\t// Cleanup lss when we take over in clustered mode.\n\t\tif o.hasSkipListPending() && o.sseq >= o.lss.resume {\n\t\t\to.lss = nil\n\t\t}\n\n\t\t// Do info sub.\n\t\tif o.infoSub == nil && jsa != nil {\n\t\t\tisubj := fmt.Sprintf(clusterConsumerInfoT, jsa.acc(), stream, o.name)\n\t\t\t// Note below the way we subscribe here is so that we can send requests to ourselves.\n\t\t\to.infoSub, _ = s.systemSubscribe(isubj, _EMPTY_, false, o.sysc, o.handleClusterConsumerInfoRequest)\n\t\t}\n\n\t\tvar err error\n\t\tif o.cfg.AckPolicy != AckNone {\n\t\t\tif o.ackSub, err = o.subscribeInternal(o.ackSubj, o.pushAck); err != nil {\n\t\t\t\to.mu.Unlock()\n\t\t\t\to.deleteWithoutAdvisory()\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\n\t\t// Setup the internal sub for next message requests regardless.\n\t\t// Will error if wrong mode to provide feedback to users.\n\t\tif o.reqSub, err = o.subscribeInternal(o.nextMsgSubj, o.processNextMsgReq); err != nil {\n\t\t\to.mu.Unlock()\n\t\t\to.deleteWithoutAdvisory()\n\t\t\treturn\n\t\t}\n\n\t\t// Check on flow control settings.\n\t\tif o.cfg.FlowControl {\n\t\t\to.setMaxPendingBytes(JsFlowControlMaxPending)\n\t\t\tfcsubj := fmt.Sprintf(jsFlowControl, stream, o.name)\n\t\t\tif o.fcSub, err = o.subscribeInternal(fcsubj, o.processFlowControl); err != nil {\n\t\t\t\to.mu.Unlock()\n\t\t\t\to.deleteWithoutAdvisory()\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\n\t\t// If push mode, register for notifications on interest.\n\t\tif o.isPushMode() {\n\t\t\to.inch = make(chan bool, 8)\n\t\t\to.acc.sl.registerNotification(o.cfg.DeliverSubject, o.cfg.DeliverGroup, o.inch)\n\t\t\tif o.active = <-o.inch; o.active {\n\t\t\t\to.checkQueueInterest()\n\t\t\t}\n\n\t\t\t// Check gateways in case they are enabled.\n\t\t\tif s.gateway.enabled {\n\t\t\t\tif !o.active {\n\t\t\t\t\to.active = s.hasGatewayInterest(o.acc.Name, o.cfg.DeliverSubject)\n\t\t\t\t}\n\t\t\t\tstopAndClearTimer(&o.gwdtmr)\n\t\t\t\to.gwdtmr = time.AfterFunc(time.Second, func() { o.watchGWinterest() })\n\t\t\t}\n\t\t}\n\n\t\tif o.dthresh > 0 && (o.isPullMode() || !o.active) {\n\t\t\t// Pull consumer. We run the dtmr all the time for this one.\n\t\t\tstopAndClearTimer(&o.dtmr)\n\t\t\to.dtmr = time.AfterFunc(o.dthresh, o.deleteNotActive)\n\t\t}\n\n\t\t// Update the consumer pause tracking.\n\t\to.updatePauseState(&o.cfg)\n\n\t\t// If we are not in ReplayInstant mode mark us as in replay state until resolved.\n\t\tif o.cfg.ReplayPolicy != ReplayInstant {\n\t\t\to.replay = true\n\t\t}\n\n\t\t// Recreate quit channel.\n\t\to.qch = make(chan struct{})\n\t\tqch := o.qch\n\t\tnode := o.node\n\t\tif node != nil && o.pch == nil {\n\t\t\to.pch = make(chan struct{}, 1)\n\t\t}\n\t\tpullMode := o.isPullMode()\n\t\to.mu.Unlock()\n\n\t\t// Check if there are any pending we might need to clean up etc.\n\t\to.checkPending()\n\n\t\t// Snapshot initial info.\n\t\to.infoWithSnap(true)\n\n\t\t// These are the labels we will use to annotate our goroutines.\n\t\tlabels := pprofLabels{\n\t\t\t\"type\":     \"consumer\",\n\t\t\t\"account\":  mset.accName(),\n\t\t\t\"stream\":   mset.name(),\n\t\t\t\"consumer\": o.name,\n\t\t}\n\n\t\t// Now start up Go routine to deliver msgs.\n\t\tgo func() {\n\t\t\tsetGoRoutineLabels(labels)\n\t\t\to.loopAndGatherMsgs(qch)\n\t\t}()\n\n\t\t// Now start up Go routine to process acks.\n\t\tgo func() {\n\t\t\tsetGoRoutineLabels(labels)\n\t\t\to.processInboundAcks(qch)\n\t\t}()\n\n\t\tif pullMode {\n\t\t\t// Now start up Go routine to process inbound next message requests.\n\t\t\tgo func() {\n\t\t\t\tsetGoRoutineLabels(labels)\n\t\t\t\to.processInboundNextMsgReqs(qch)\n\t\t\t}()\n\t\t}\n\n\t\t// If we are R>1 spin up our proposal loop.\n\t\tif node != nil {\n\t\t\t// Determine if we can send pending requests info to the group.\n\t\t\t// They must be on server versions >= 2.7.1\n\t\t\to.checkAndSetPendingRequestsOk()\n\t\t\to.checkPendingRequests()\n\t\t\tgo func() {\n\t\t\t\tsetGoRoutineLabels(labels)\n\t\t\t\to.loopAndForwardProposals(qch)\n\t\t\t}()\n\t\t}\n\n\t} else {\n\t\t// Shutdown the go routines and the subscriptions.\n\t\to.mu.Lock()\n\t\tif o.qch != nil {\n\t\t\tclose(o.qch)\n\t\t\to.qch = nil\n\t\t}\n\t\t// Stop any inactivity timers. Should only be running on leaders.\n\t\tstopAndClearTimer(&o.dtmr)\n\t\t// Stop any unpause timers. Should only be running on leaders.\n\t\tstopAndClearTimer(&o.uptmr)\n\t\t// Make sure to clear out any re-deliver queues\n\t\to.stopAndClearPtmr()\n\t\to.rdq = nil\n\t\to.rdqi.Empty()\n\t\to.pending = nil\n\t\to.resetPendingDeliveries()\n\t\t// ok if they are nil, we protect inside unsubscribe()\n\t\to.unsubscribe(o.ackSub)\n\t\to.unsubscribe(o.reqSub)\n\t\to.unsubscribe(o.fcSub)\n\t\to.ackSub, o.reqSub, o.fcSub = nil, nil, nil\n\t\tif o.infoSub != nil {\n\t\t\to.srv.sysUnsubscribe(o.infoSub)\n\t\t\to.infoSub = nil\n\t\t}\n\t\t// Reset waiting if we are in pull mode.\n\t\tif o.isPullMode() {\n\t\t\to.waiting = newWaitQueue(o.cfg.MaxWaiting)\n\t\t\to.nextMsgReqs.drain()\n\t\t} else if o.srv.gateway.enabled {\n\t\t\tstopAndClearTimer(&o.gwdtmr)\n\t\t}\n\t\t// If we were the leader make sure to drain queued up acks.\n\t\tif wasLeader {\n\t\t\to.ackMsgs.drain()\n\t\t\t// Reset amount of acks that need to be processed.\n\t\t\tatomic.StoreInt64(&o.awl, 0)\n\t\t\t// Also remove any pending replies since we should not be the one to respond at this point.\n\t\t\to.replies = nil\n\t\t}\n\t\to.mu.Unlock()\n\t}\n}\n\n// This is coming on the wire so do not block here.\nfunc (o *consumer) handleClusterConsumerInfoRequest(sub *subscription, c *client, _ *Account, subject, reply string, msg []byte) {\n\tgo o.infoWithSnapAndReply(false, reply)\n}\n\n// Lock should be held.\nfunc (o *consumer) subscribeInternal(subject string, cb msgHandler) (*subscription, error) {\n\tc := o.client\n\tif c == nil {\n\t\treturn nil, fmt.Errorf(\"invalid consumer\")\n\t}\n\tif !c.srv.EventsEnabled() {\n\t\treturn nil, ErrNoSysAccount\n\t}\n\tif cb == nil {\n\t\treturn nil, fmt.Errorf(\"undefined message handler\")\n\t}\n\n\to.sid++\n\n\t// Now create the subscription\n\treturn c.processSub([]byte(subject), nil, []byte(strconv.Itoa(o.sid)), cb, false)\n}\n\n// Unsubscribe from our subscription.\n// Lock should be held.\nfunc (o *consumer) unsubscribe(sub *subscription) {\n\tif sub == nil || o.client == nil {\n\t\treturn\n\t}\n\to.client.processUnsub(sub.sid)\n}\n\n// We need to make sure we protect access to the outq.\n// Do all advisory sends here.\nfunc (o *consumer) sendAdvisory(subject string, e any) {\n\tif o.acc == nil {\n\t\treturn\n\t}\n\n\t// If there is no one listening for this advisory then save ourselves the effort\n\t// and don't bother encoding the JSON or sending it.\n\tif sl := o.acc.sl; (sl != nil && !sl.HasInterest(subject)) && !o.srv.hasGatewayInterest(o.acc.Name, subject) {\n\t\treturn\n\t}\n\n\tj, err := json.Marshal(e)\n\tif err != nil {\n\t\treturn\n\t}\n\n\to.outq.sendMsg(subject, j)\n}\n\nfunc (o *consumer) sendDeleteAdvisoryLocked() {\n\te := JSConsumerActionAdvisory{\n\t\tTypedEvent: TypedEvent{\n\t\t\tType: JSConsumerActionAdvisoryType,\n\t\t\tID:   nuid.Next(),\n\t\t\tTime: time.Now().UTC(),\n\t\t},\n\t\tStream:   o.stream,\n\t\tConsumer: o.name,\n\t\tAction:   DeleteEvent,\n\t\tDomain:   o.srv.getOpts().JetStreamDomain,\n\t}\n\n\tsubj := JSAdvisoryConsumerDeletedPre + \".\" + o.stream + \".\" + o.name\n\to.sendAdvisory(subj, e)\n}\n\nfunc (o *consumer) sendPinnedAdvisoryLocked(group string) {\n\te := JSConsumerGroupPinnedAdvisory{\n\t\tTypedEvent: TypedEvent{\n\t\t\tType: JSConsumerGroupPinnedAdvisoryType,\n\t\t\tID:   nuid.Next(),\n\t\t\tTime: time.Now().UTC(),\n\t\t},\n\t\tAccount:        o.acc.Name,\n\t\tStream:         o.stream,\n\t\tConsumer:       o.name,\n\t\tDomain:         o.srv.getOpts().JetStreamDomain,\n\t\tPinnedClientId: o.currentPinId,\n\t\tGroup:          group,\n\t}\n\n\tsubj := JSAdvisoryConsumerPinnedPre + \".\" + o.stream + \".\" + o.name\n\to.sendAdvisory(subj, e)\n\n}\nfunc (o *consumer) sendUnpinnedAdvisoryLocked(group string, reason string) {\n\te := JSConsumerGroupUnpinnedAdvisory{\n\t\tTypedEvent: TypedEvent{\n\t\t\tType: JSConsumerGroupUnpinnedAdvisoryType,\n\t\t\tID:   nuid.Next(),\n\t\t\tTime: time.Now().UTC(),\n\t\t},\n\t\tAccount:  o.acc.Name,\n\t\tStream:   o.stream,\n\t\tConsumer: o.name,\n\t\tDomain:   o.srv.getOpts().JetStreamDomain,\n\t\tGroup:    group,\n\t\tReason:   reason,\n\t}\n\n\tsubj := JSAdvisoryConsumerUnpinnedPre + \".\" + o.stream + \".\" + o.name\n\to.sendAdvisory(subj, e)\n\n}\n\nfunc (o *consumer) sendCreateAdvisory() {\n\to.mu.Lock()\n\tdefer o.mu.Unlock()\n\n\te := JSConsumerActionAdvisory{\n\t\tTypedEvent: TypedEvent{\n\t\t\tType: JSConsumerActionAdvisoryType,\n\t\t\tID:   nuid.Next(),\n\t\t\tTime: time.Now().UTC(),\n\t\t},\n\t\tStream:   o.stream,\n\t\tConsumer: o.name,\n\t\tAction:   CreateEvent,\n\t\tDomain:   o.srv.getOpts().JetStreamDomain,\n\t}\n\n\tsubj := JSAdvisoryConsumerCreatedPre + \".\" + o.stream + \".\" + o.name\n\to.sendAdvisory(subj, e)\n}\n\nfunc (o *consumer) sendPauseAdvisoryLocked(cfg *ConsumerConfig) {\n\te := JSConsumerPauseAdvisory{\n\t\tTypedEvent: TypedEvent{\n\t\t\tType: JSConsumerPauseAdvisoryType,\n\t\t\tID:   nuid.Next(),\n\t\t\tTime: time.Now().UTC(),\n\t\t},\n\t\tStream:   o.stream,\n\t\tConsumer: o.name,\n\t\tDomain:   o.srv.getOpts().JetStreamDomain,\n\t}\n\n\tif cfg.PauseUntil != nil {\n\t\te.PauseUntil = *cfg.PauseUntil\n\t\te.Paused = time.Now().Before(e.PauseUntil)\n\t}\n\n\tsubj := JSAdvisoryConsumerPausePre + \".\" + o.stream + \".\" + o.name\n\to.sendAdvisory(subj, e)\n}\n\n// Created returns created time.\nfunc (o *consumer) createdTime() time.Time {\n\to.mu.Lock()\n\tcreated := o.created\n\to.mu.Unlock()\n\treturn created\n}\n\n// Internal to allow creation time to be restored.\nfunc (o *consumer) setCreatedTime(created time.Time) {\n\to.mu.Lock()\n\to.created = created\n\to.mu.Unlock()\n}\n\n// This will check for extended interest in a subject. If we have local interest we just return\n// that, but in the absence of local interest and presence of gateways or service imports we need\n// to check those as well.\nfunc (o *consumer) hasDeliveryInterest(localInterest bool) bool {\n\to.mu.RLock()\n\tmset := o.mset\n\tif mset == nil {\n\t\to.mu.RUnlock()\n\t\treturn false\n\t}\n\tacc := o.acc\n\tdeliver := o.cfg.DeliverSubject\n\to.mu.RUnlock()\n\n\tif localInterest {\n\t\treturn true\n\t}\n\n\t// If we are here check gateways.\n\tif s := acc.srv; s != nil && s.hasGatewayInterest(acc.Name, deliver) {\n\t\treturn true\n\t}\n\treturn false\n}\n\nfunc (s *Server) hasGatewayInterest(account, subject string) bool {\n\tgw := s.gateway\n\tif !gw.enabled {\n\t\treturn false\n\t}\n\tgw.RLock()\n\tdefer gw.RUnlock()\n\tfor _, gwc := range gw.outo {\n\t\tpsi, qr := gwc.gatewayInterest(account, stringToBytes(subject))\n\t\tif psi || qr != nil {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\n// This processes an update to the local interest for a deliver subject.\nfunc (o *consumer) updateDeliveryInterest(localInterest bool) bool {\n\tinterest := o.hasDeliveryInterest(localInterest)\n\n\to.mu.Lock()\n\tdefer o.mu.Unlock()\n\n\tmset := o.mset\n\tif mset == nil || o.isPullMode() {\n\t\treturn false\n\t}\n\n\tif interest && !o.active {\n\t\to.signalNewMessages()\n\t}\n\t// Update active status, if not active clear any queue group we captured.\n\tif o.active = interest; !o.active {\n\t\to.qgroup = _EMPTY_\n\t} else {\n\t\to.checkQueueInterest()\n\t}\n\n\t// If the delete timer has already been set do not clear here and return.\n\t// Note that durable can now have an inactive threshold, so don't check\n\t// for durable status, instead check for dthresh > 0.\n\tif o.dtmr != nil && o.dthresh > 0 && !interest {\n\t\treturn true\n\t}\n\n\t// Stop and clear the delete timer always.\n\tstopAndClearTimer(&o.dtmr)\n\n\t// If we do not have interest anymore and have a delete threshold set, then set\n\t// a timer to delete us. We wait for a bit in case of server reconnect.\n\tif !interest && o.dthresh > 0 {\n\t\to.dtmr = time.AfterFunc(o.dthresh, o.deleteNotActive)\n\t\treturn true\n\t}\n\treturn false\n}\n\nconst (\n\tdefaultConsumerNotActiveStartInterval = 30 * time.Second\n\tdefaultConsumerNotActiveMaxInterval   = 5 * time.Minute\n)\n\nvar (\n\tconsumerNotActiveStartInterval = defaultConsumerNotActiveStartInterval\n\tconsumerNotActiveMaxInterval   = defaultConsumerNotActiveMaxInterval\n)\n\n// deleteNotActive must only be called from time.AfterFunc or in its own\n// goroutine, as it can block on clean-up.\nfunc (o *consumer) deleteNotActive() {\n\t// Take a copy of these when the goroutine starts, mostly it avoids a\n\t// race condition with tests that modify these consts, such as\n\t// TestJetStreamClusterGhostEphemeralsAfterRestart.\n\tcnaMax := consumerNotActiveMaxInterval\n\tcnaStart := consumerNotActiveStartInterval\n\n\to.mu.Lock()\n\tif o.mset == nil {\n\t\to.mu.Unlock()\n\t\treturn\n\t}\n\t// Push mode just look at active.\n\tif o.isPushMode() {\n\t\t// If we are active simply return.\n\t\tif o.active {\n\t\t\to.mu.Unlock()\n\t\t\treturn\n\t\t}\n\t} else {\n\t\t// Pull mode.\n\t\telapsed := time.Since(o.waiting.last)\n\t\tif elapsed <= o.cfg.InactiveThreshold {\n\t\t\t// These need to keep firing so reset but use delta.\n\t\t\tif o.dtmr != nil {\n\t\t\t\to.dtmr.Reset(o.dthresh - elapsed)\n\t\t\t} else {\n\t\t\t\to.dtmr = time.AfterFunc(o.dthresh-elapsed, o.deleteNotActive)\n\t\t\t}\n\t\t\to.mu.Unlock()\n\t\t\treturn\n\t\t}\n\t\t// Check if we still have valid requests waiting.\n\t\tif o.checkWaitingForInterest() {\n\t\t\tif o.dtmr != nil {\n\t\t\t\to.dtmr.Reset(o.dthresh)\n\t\t\t} else {\n\t\t\t\to.dtmr = time.AfterFunc(o.dthresh, o.deleteNotActive)\n\t\t\t}\n\t\t\to.mu.Unlock()\n\t\t\treturn\n\t\t}\n\t}\n\n\ts, js := o.mset.srv, o.srv.js.Load()\n\tacc, stream, name, isDirect := o.acc.Name, o.stream, o.name, o.cfg.Direct\n\tvar qch, cqch chan struct{}\n\tif o.srv != nil {\n\t\tqch = o.srv.quitCh\n\t}\n\to.mu.Unlock()\n\tif js != nil {\n\t\tcqch = js.clusterQuitC()\n\t}\n\n\t// Useful for pprof.\n\tsetGoRoutineLabels(pprofLabels{\n\t\t\"account\":  acc,\n\t\t\"stream\":   stream,\n\t\t\"consumer\": name,\n\t})\n\n\t// We will delete locally regardless.\n\tdefer o.delete()\n\n\t// If we are clustered, check if we still have this consumer assigned.\n\t// If we do forward a proposal to delete ourselves to the metacontroller leader.\n\tif !isDirect && s.JetStreamIsClustered() {\n\t\tjs.mu.RLock()\n\t\tvar (\n\t\t\tcca         consumerAssignment\n\t\t\tmeta        RaftNode\n\t\t\tremoveEntry []byte\n\t\t)\n\t\tca, cc := js.consumerAssignment(acc, stream, name), js.cluster\n\t\tif ca != nil && cc != nil {\n\t\t\tmeta = cc.meta\n\t\t\tcca = *ca\n\t\t\tcca.Reply = _EMPTY_\n\t\t\tremoveEntry = encodeDeleteConsumerAssignment(&cca)\n\t\t\tmeta.ForwardProposal(removeEntry)\n\t\t}\n\t\tjs.mu.RUnlock()\n\n\t\tif ca != nil && cc != nil {\n\t\t\t// Check to make sure we went away.\n\t\t\t// Don't think this needs to be a monitored go routine.\n\t\t\tjitter := time.Duration(rand.Int63n(int64(cnaStart)))\n\t\t\tinterval := cnaStart + jitter\n\t\t\tticker := time.NewTicker(interval)\n\t\t\tdefer ticker.Stop()\n\t\t\tfor {\n\t\t\t\tselect {\n\t\t\t\tcase <-ticker.C:\n\t\t\t\tcase <-qch:\n\t\t\t\t\treturn\n\t\t\t\tcase <-cqch:\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t\tjs.mu.RLock()\n\t\t\t\tif js.shuttingDown {\n\t\t\t\t\tjs.mu.RUnlock()\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t\tnca := js.consumerAssignment(acc, stream, name)\n\t\t\t\tjs.mu.RUnlock()\n\t\t\t\t// Make sure this is the same consumer assignment, and not a new consumer with the same name.\n\t\t\t\tif nca != nil && reflect.DeepEqual(nca, ca) {\n\t\t\t\t\ts.Warnf(\"Consumer assignment for '%s > %s > %s' not cleaned up, retrying\", acc, stream, name)\n\t\t\t\t\tmeta.ForwardProposal(removeEntry)\n\t\t\t\t\tif interval < cnaMax {\n\t\t\t\t\t\tinterval *= 2\n\t\t\t\t\t\tticker.Reset(interval)\n\t\t\t\t\t}\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\t// We saw that consumer has been removed, all done.\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc (o *consumer) watchGWinterest() {\n\tpa := o.isActive()\n\t// If there is no local interest...\n\tif o.hasNoLocalInterest() {\n\t\to.updateDeliveryInterest(false)\n\t\tif !pa && o.isActive() {\n\t\t\to.signalNewMessages()\n\t\t}\n\t}\n\n\t// We want this to always be running so we can also pick up on interest returning.\n\to.mu.Lock()\n\tif o.gwdtmr != nil {\n\t\to.gwdtmr.Reset(time.Second)\n\t} else {\n\t\tstopAndClearTimer(&o.gwdtmr)\n\t\to.gwdtmr = time.AfterFunc(time.Second, func() { o.watchGWinterest() })\n\t}\n\to.mu.Unlock()\n}\n\n// Config returns the consumer's configuration.\nfunc (o *consumer) config() ConsumerConfig {\n\to.mu.Lock()\n\tdefer o.mu.Unlock()\n\treturn o.cfg\n}\n\n// Check if we have hit max deliveries. If so do notification and cleanup.\n// Return whether or not the max was hit.\n// Lock should be held.\nfunc (o *consumer) hasMaxDeliveries(seq uint64) bool {\n\tif o.maxdc == 0 {\n\t\treturn false\n\t}\n\tif dc := o.deliveryCount(seq); dc >= o.maxdc {\n\t\t// We have hit our max deliveries for this sequence.\n\t\t// Only send the advisory once.\n\t\tif dc == o.maxdc {\n\t\t\to.notifyDeliveryExceeded(seq, dc)\n\t\t}\n\t\t// Determine if we signal to start flow of messages again.\n\t\tif o.maxp > 0 && len(o.pending) >= o.maxp {\n\t\t\to.signalNewMessages()\n\t\t}\n\t\t// Make sure to remove from pending.\n\t\tif p, ok := o.pending[seq]; ok && p != nil {\n\t\t\tdelete(o.pending, seq)\n\t\t\to.updateDelivered(p.Sequence, seq, dc, p.Timestamp)\n\t\t}\n\t\t// Ensure redelivered state is set, if not already.\n\t\tif o.rdc == nil {\n\t\t\to.rdc = make(map[uint64]uint64)\n\t\t}\n\t\to.rdc[seq] = dc\n\t\treturn true\n\t}\n\treturn false\n}\n\n// Force expiration of all pending.\n// Lock should be held.\nfunc (o *consumer) forceExpirePending() {\n\tvar expired []uint64\n\tfor seq := range o.pending {\n\t\tif !o.onRedeliverQueue(seq) && !o.hasMaxDeliveries(seq) {\n\t\t\texpired = append(expired, seq)\n\t\t}\n\t}\n\tif len(expired) > 0 {\n\t\tslices.Sort(expired)\n\t\to.addToRedeliverQueue(expired...)\n\t\t// Now we should update the timestamp here since we are redelivering.\n\t\t// We will use an incrementing time to preserve order for any other redelivery.\n\t\toff := time.Now().UnixNano() - o.pending[expired[0]].Timestamp\n\t\tfor _, seq := range expired {\n\t\t\tif p, ok := o.pending[seq]; ok && p != nil {\n\t\t\t\tp.Timestamp += off\n\t\t\t}\n\t\t}\n\t\to.resetPtmr(o.ackWait(0))\n\t}\n\to.signalNewMessages()\n}\n\n// Acquire proper locks and update rate limit.\n// Will use what is in config.\nfunc (o *consumer) setRateLimitNeedsLocks() {\n\to.mu.RLock()\n\tmset := o.mset\n\to.mu.RUnlock()\n\n\tif mset == nil {\n\t\treturn\n\t}\n\n\tmset.mu.RLock()\n\to.mu.Lock()\n\to.setRateLimit(o.cfg.RateLimit)\n\to.mu.Unlock()\n\tmset.mu.RUnlock()\n}\n\n// Set the rate limiter\n// Both mset and consumer lock should be held.\nfunc (o *consumer) setRateLimit(bps uint64) {\n\tif bps == 0 {\n\t\to.rlimit = nil\n\t\treturn\n\t}\n\n\t// TODO(dlc) - Make sane values or error if not sane?\n\t// We are configured in bits per sec so adjust to bytes.\n\trl := rate.Limit(bps / 8)\n\tmset := o.mset\n\n\t// Burst should be set to maximum msg size for this account, etc.\n\tvar burst int\n\t// We don't need to get cfgMu's rlock here since this function\n\t// is already invoked under mset.mu.RLock(), which superseeds cfgMu.\n\tif mset.cfg.MaxMsgSize > 0 {\n\t\tburst = int(mset.cfg.MaxMsgSize)\n\t} else if mset.jsa.account.limits.mpay > 0 {\n\t\tburst = int(mset.jsa.account.limits.mpay)\n\t} else {\n\t\ts := mset.jsa.account.srv\n\t\tburst = int(s.getOpts().MaxPayload)\n\t}\n\n\to.rlimit = rate.NewLimiter(rl, burst)\n}\n\n// Check if new consumer config allowed vs old.\nfunc (acc *Account) checkNewConsumerConfig(cfg, ncfg *ConsumerConfig) error {\n\tif reflect.DeepEqual(cfg, ncfg) {\n\t\treturn nil\n\t}\n\t// Something different, so check since we only allow certain things to be updated.\n\tif cfg.DeliverPolicy != ncfg.DeliverPolicy {\n\t\treturn errors.New(\"deliver policy can not be updated\")\n\t}\n\tif cfg.OptStartSeq != ncfg.OptStartSeq {\n\t\treturn errors.New(\"start sequence can not be updated\")\n\t}\n\tif cfg.OptStartTime != nil && ncfg.OptStartTime != nil {\n\t\t// Both have start times set, compare them directly:\n\t\tif !cfg.OptStartTime.Equal(*ncfg.OptStartTime) {\n\t\t\treturn errors.New(\"start time can not be updated\")\n\t\t}\n\t} else if cfg.OptStartTime != nil || ncfg.OptStartTime != nil {\n\t\t// At least one start time is set and the other is not\n\t\treturn errors.New(\"start time can not be updated\")\n\t}\n\tif cfg.AckPolicy != ncfg.AckPolicy {\n\t\treturn errors.New(\"ack policy can not be updated\")\n\t}\n\tif cfg.ReplayPolicy != ncfg.ReplayPolicy {\n\t\treturn errors.New(\"replay policy can not be updated\")\n\t}\n\tif cfg.Heartbeat != ncfg.Heartbeat {\n\t\treturn errors.New(\"heart beats can not be updated\")\n\t}\n\tif cfg.FlowControl != ncfg.FlowControl {\n\t\treturn errors.New(\"flow control can not be updated\")\n\t}\n\n\t// Deliver Subject is conditional on if its bound.\n\tif cfg.DeliverSubject != ncfg.DeliverSubject {\n\t\tif cfg.DeliverSubject == _EMPTY_ {\n\t\t\treturn errors.New(\"can not update pull consumer to push based\")\n\t\t}\n\t\tif ncfg.DeliverSubject == _EMPTY_ {\n\t\t\treturn errors.New(\"can not update push consumer to pull based\")\n\t\t}\n\t\tif acc.sl.HasInterest(cfg.DeliverSubject) {\n\t\t\treturn NewJSConsumerNameExistError()\n\t\t}\n\t}\n\n\tif cfg.MaxWaiting != ncfg.MaxWaiting {\n\t\treturn errors.New(\"max waiting can not be updated\")\n\t}\n\n\t// Check if BackOff is defined, MaxDeliver is within range.\n\tif lbo := len(ncfg.BackOff); lbo > 0 && ncfg.MaxDeliver != -1 && lbo > ncfg.MaxDeliver {\n\t\treturn NewJSConsumerMaxDeliverBackoffError()\n\t}\n\n\treturn nil\n}\n\n// Update the config based on the new config, or error if update not allowed.\nfunc (o *consumer) updateConfig(cfg *ConsumerConfig) error {\n\to.mu.Lock()\n\tdefer o.mu.Unlock()\n\n\tif o.closed || o.mset == nil {\n\t\treturn NewJSConsumerDoesNotExistError()\n\t}\n\n\tif err := o.acc.checkNewConsumerConfig(&o.cfg, cfg); err != nil {\n\t\treturn err\n\t}\n\n\t// Make sure we always store PauseUntil in UTC.\n\tif cfg.PauseUntil != nil {\n\t\tutc := (*cfg.PauseUntil).UTC()\n\t\tcfg.PauseUntil = &utc\n\t}\n\n\tif o.store != nil {\n\t\t// Update local state always.\n\t\tif err := o.store.UpdateConfig(cfg); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\t// DeliverSubject\n\tif cfg.DeliverSubject != o.cfg.DeliverSubject {\n\t\to.updateDeliverSubjectLocked(cfg.DeliverSubject)\n\t}\n\n\t// MaxAckPending\n\tif cfg.MaxAckPending != o.cfg.MaxAckPending {\n\t\to.maxp = cfg.MaxAckPending\n\t\to.signalNewMessages()\n\t\t// If MaxAckPending is lowered, we could have allocated a pending deliveries map of larger size.\n\t\t// Reset it here, so we can shrink the map.\n\t\tif cfg.MaxAckPending < o.cfg.MaxAckPending {\n\t\t\to.resetPendingDeliveries()\n\t\t}\n\t}\n\t// AckWait\n\tif cfg.AckWait != o.cfg.AckWait {\n\t\tif o.ptmr != nil {\n\t\t\to.resetPtmr(100 * time.Millisecond)\n\t\t}\n\t}\n\t// Rate Limit\n\tif cfg.RateLimit != o.cfg.RateLimit {\n\t\t// We need both locks here so do in Go routine.\n\t\tgo o.setRateLimitNeedsLocks()\n\t}\n\tif cfg.SampleFrequency != o.cfg.SampleFrequency {\n\t\ts := strings.TrimSuffix(cfg.SampleFrequency, \"%\")\n\t\t// String has been already verified for validity up in the stack, so no\n\t\t// need to check for error here.\n\t\tsampleFreq, _ := strconv.Atoi(s)\n\t\to.sfreq = int32(sampleFreq)\n\t}\n\t// Set MaxDeliver if changed\n\tif cfg.MaxDeliver != o.cfg.MaxDeliver {\n\t\to.maxdc = uint64(cfg.MaxDeliver)\n\t}\n\t// Set InactiveThreshold if changed.\n\tif val := cfg.InactiveThreshold; val != o.cfg.InactiveThreshold {\n\t\to.updateInactiveThreshold(cfg)\n\t\tstopAndClearTimer(&o.dtmr)\n\t\t// Restart timer only if we are the leader.\n\t\tif o.isLeader() && o.dthresh > 0 {\n\t\t\to.dtmr = time.AfterFunc(o.dthresh, o.deleteNotActive)\n\t\t}\n\t}\n\t// Check whether the pause has changed\n\t{\n\t\tvar old, new time.Time\n\t\tif o.cfg.PauseUntil != nil {\n\t\t\told = *o.cfg.PauseUntil\n\t\t}\n\t\tif cfg.PauseUntil != nil {\n\t\t\tnew = *cfg.PauseUntil\n\t\t}\n\t\tif !old.Equal(new) {\n\t\t\to.updatePauseState(cfg)\n\t\t\tif o.isLeader() {\n\t\t\t\to.sendPauseAdvisoryLocked(cfg)\n\t\t\t}\n\t\t}\n\t}\n\n\t// Check for Subject Filters update.\n\tnewSubjects := gatherSubjectFilters(cfg.FilterSubject, cfg.FilterSubjects)\n\tif !subjectSliceEqual(newSubjects, o.subjf.subjects()) {\n\t\tnewSubjf := make(subjectFilters, 0, len(newSubjects))\n\t\tfor _, newFilter := range newSubjects {\n\t\t\tfs := &subjectFilter{\n\t\t\t\tsubject:          newFilter,\n\t\t\t\thasWildcard:      subjectHasWildcard(newFilter),\n\t\t\t\ttokenizedSubject: tokenizeSubjectIntoSlice(nil, newFilter),\n\t\t\t}\n\t\t\tnewSubjf = append(newSubjf, fs)\n\t\t}\n\t\t// Make sure we have correct signaling setup.\n\t\t// Consumer lock can not be held.\n\t\tmset := o.mset\n\t\to.mu.Unlock()\n\t\tmset.swapSigSubs(o, newSubjf.subjects())\n\t\to.mu.Lock()\n\n\t\t// When we're done with signaling, we can replace the subjects.\n\t\t// If filters were removed, set `o.subjf` to nil.\n\t\tif len(newSubjf) == 0 {\n\t\t\to.subjf = nil\n\t\t\to.filters = nil\n\t\t} else {\n\t\t\to.subjf = newSubjf\n\t\t\tif len(o.subjf) == 1 {\n\t\t\t\to.filters = nil\n\t\t\t} else {\n\t\t\t\to.filters = NewSublistNoCache()\n\t\t\t\tfor _, filter := range o.subjf {\n\t\t\t\t\to.filters.Insert(&subscription{subject: []byte(filter.subject)})\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\t// Record new config for others that do not need special handling.\n\t// Allowed but considered no-op, [Description, SampleFrequency, MaxWaiting, HeadersOnly]\n\to.cfg = *cfg\n\n\t// Cleanup messages that lost interest.\n\tif o.retention == InterestPolicy {\n\t\to.mu.Unlock()\n\t\to.cleanupNoInterestMessages(o.mset, false)\n\t\to.mu.Lock()\n\t}\n\n\t// Re-calculate num pending on update.\n\to.streamNumPending()\n\n\treturn nil\n}\n\n// This is a config change for the delivery subject for a\n// push based consumer.\nfunc (o *consumer) updateDeliverSubject(newDeliver string) {\n\t// Update the config and the dsubj\n\to.mu.Lock()\n\tdefer o.mu.Unlock()\n\to.updateDeliverSubjectLocked(newDeliver)\n}\n\n// This is a config change for the delivery subject for a\n// push based consumer.\nfunc (o *consumer) updateDeliverSubjectLocked(newDeliver string) {\n\tif o.closed || o.isPullMode() || o.cfg.DeliverSubject == newDeliver {\n\t\treturn\n\t}\n\n\t// Force redeliver of all pending on change of delivery subject.\n\tif len(o.pending) > 0 {\n\t\to.forceExpirePending()\n\t}\n\n\to.acc.sl.clearNotification(o.dsubj, o.cfg.DeliverGroup, o.inch)\n\to.dsubj, o.cfg.DeliverSubject = newDeliver, newDeliver\n\t// When we register new one it will deliver to update state loop.\n\to.acc.sl.registerNotification(newDeliver, o.cfg.DeliverGroup, o.inch)\n}\n\n// Check that configs are equal but allow delivery subjects to be different.\nfunc configsEqualSansDelivery(a, b ConsumerConfig) bool {\n\t// These were copied in so can set Delivery here.\n\ta.DeliverSubject, b.DeliverSubject = _EMPTY_, _EMPTY_\n\treturn reflect.DeepEqual(a, b)\n}\n\n// Helper to send a reply to an ack.\nfunc (o *consumer) sendAckReply(subj string) {\n\to.mu.RLock()\n\tdefer o.mu.RUnlock()\n\to.outq.sendMsg(subj, nil)\n}\n\ntype jsAckMsg struct {\n\tsubject string\n\treply   string\n\thdr     int\n\tmsg     []byte\n}\n\nvar jsAckMsgPool sync.Pool\n\nfunc newJSAckMsg(subj, reply string, hdr int, msg []byte) *jsAckMsg {\n\tvar m *jsAckMsg\n\tam := jsAckMsgPool.Get()\n\tif am != nil {\n\t\tm = am.(*jsAckMsg)\n\t} else {\n\t\tm = &jsAckMsg{}\n\t}\n\t// When getting something from a pool it is critical that all fields are\n\t// initialized. Doing this way guarantees that if someone adds a field to\n\t// the structure, the compiler will fail the build if this line is not updated.\n\t(*m) = jsAckMsg{subj, reply, hdr, msg}\n\treturn m\n}\n\nfunc (am *jsAckMsg) returnToPool() {\n\tif am == nil {\n\t\treturn\n\t}\n\tam.subject, am.reply, am.hdr, am.msg = _EMPTY_, _EMPTY_, -1, nil\n\tjsAckMsgPool.Put(am)\n}\n\n// Push the ack message to the consumer's ackMsgs queue\nfunc (o *consumer) pushAck(_ *subscription, c *client, _ *Account, subject, reply string, rmsg []byte) {\n\tatomic.AddInt64(&o.awl, 1)\n\to.ackMsgs.push(newJSAckMsg(subject, reply, c.pa.hdr, copyBytes(rmsg)))\n}\n\n// Processes a message for the ack reply subject delivered with a message.\nfunc (o *consumer) processAck(subject, reply string, hdr int, rmsg []byte) {\n\tdefer atomic.AddInt64(&o.awl, -1)\n\n\tvar msg []byte\n\tif hdr > 0 {\n\t\tmsg = rmsg[hdr:]\n\t} else {\n\t\tmsg = rmsg\n\t}\n\n\tsseq, dseq, dc := ackReplyInfo(subject)\n\n\tskipAckReply := sseq == 0\n\n\tswitch {\n\tcase len(msg) == 0, bytes.Equal(msg, AckAck), bytes.Equal(msg, AckOK):\n\t\tif !o.processAckMsg(sseq, dseq, dc, reply, true) {\n\t\t\t// We handle replies for acks in updateAcks\n\t\t\tskipAckReply = true\n\t\t}\n\tcase bytes.HasPrefix(msg, AckNext):\n\t\to.processAckMsg(sseq, dseq, dc, _EMPTY_, true)\n\t\to.processNextMsgRequest(reply, msg[len(AckNext):])\n\t\tskipAckReply = true\n\tcase bytes.HasPrefix(msg, AckNak):\n\t\to.processNak(sseq, dseq, dc, msg)\n\tcase bytes.Equal(msg, AckProgress):\n\t\to.progressUpdate(sseq)\n\tcase bytes.HasPrefix(msg, AckTerm):\n\t\tvar reason string\n\t\tif buf := msg[len(AckTerm):]; len(buf) > 0 {\n\t\t\treason = string(bytes.TrimSpace(buf))\n\t\t}\n\t\tif !o.processTerm(sseq, dseq, dc, reason, reply) {\n\t\t\t// We handle replies for acks in updateAcks\n\t\t\tskipAckReply = true\n\t\t}\n\t}\n\n\t// Ack the ack if requested.\n\tif len(reply) > 0 && !skipAckReply {\n\t\to.sendAckReply(reply)\n\t}\n}\n\n// Used to process a working update to delay redelivery.\nfunc (o *consumer) progressUpdate(seq uint64) {\n\to.mu.Lock()\n\tdefer o.mu.Unlock()\n\n\tif p, ok := o.pending[seq]; ok {\n\t\tp.Timestamp = time.Now().UnixNano()\n\t\t// Update store system.\n\t\to.updateDelivered(p.Sequence, seq, 1, p.Timestamp)\n\t}\n}\n\n// Lock should be held.\nfunc (o *consumer) updateSkipped(seq uint64) {\n\t// Clustered mode and R>1 only.\n\tif o.node == nil || !o.isLeader() {\n\t\treturn\n\t}\n\tvar b [1 + 8]byte\n\tb[0] = byte(updateSkipOp)\n\tvar le = binary.LittleEndian\n\tle.PutUint64(b[1:], seq)\n\to.propose(b[:])\n}\n\nfunc (o *consumer) loopAndForwardProposals(qch chan struct{}) {\n\t// On exit make sure we nil out pch.\n\tdefer func() {\n\t\to.mu.Lock()\n\t\to.pch = nil\n\t\to.mu.Unlock()\n\t}()\n\n\to.mu.RLock()\n\tnode, pch := o.node, o.pch\n\to.mu.RUnlock()\n\n\tif node == nil || pch == nil {\n\t\treturn\n\t}\n\n\tforwardProposals := func() error {\n\t\to.mu.Lock()\n\t\tif o.node == nil || !o.node.Leader() {\n\t\t\to.mu.Unlock()\n\t\t\treturn errors.New(\"no longer leader\")\n\t\t}\n\t\tproposal := o.phead\n\t\to.phead, o.ptail = nil, nil\n\t\to.mu.Unlock()\n\t\t// 256k max for now per batch.\n\t\tconst maxBatch = 256 * 1024\n\t\tvar entries []*Entry\n\t\tfor sz := 0; proposal != nil; proposal = proposal.next {\n\t\t\tentries = append(entries, newEntry(EntryNormal, proposal.data))\n\t\t\tsz += len(proposal.data)\n\t\t\tif sz > maxBatch {\n\t\t\t\tnode.ProposeMulti(entries)\n\t\t\t\t// We need to re-create `entries` because there is a reference\n\t\t\t\t// to it in the node's pae map.\n\t\t\t\tsz, entries = 0, nil\n\t\t\t}\n\t\t}\n\t\tif len(entries) > 0 {\n\t\t\tnode.ProposeMulti(entries)\n\t\t}\n\t\treturn nil\n\t}\n\n\t// In case we have anything pending on entry.\n\tforwardProposals()\n\n\tfor {\n\t\tselect {\n\t\tcase <-qch:\n\t\t\tforwardProposals()\n\t\t\treturn\n\t\tcase <-pch:\n\t\t\tif err := forwardProposals(); err != nil {\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}\n}\n\n// Lock should be held.\nfunc (o *consumer) propose(entry []byte) {\n\tp := &proposal{data: entry}\n\tif o.phead == nil {\n\t\to.phead = p\n\t} else {\n\t\to.ptail.next = p\n\t}\n\to.ptail = p\n\n\t// Kick our looper routine.\n\tselect {\n\tcase o.pch <- struct{}{}:\n\tdefault:\n\t}\n}\n\n// Lock should be held.\nfunc (o *consumer) updateDelivered(dseq, sseq, dc uint64, ts int64) {\n\t// Clustered mode and R>1.\n\tif o.node != nil {\n\t\t// Inline for now, use variable compression.\n\t\tvar b [4*binary.MaxVarintLen64 + 1]byte\n\t\tb[0] = byte(updateDeliveredOp)\n\t\tn := 1\n\t\tn += binary.PutUvarint(b[n:], dseq)\n\t\tn += binary.PutUvarint(b[n:], sseq)\n\t\tn += binary.PutUvarint(b[n:], dc)\n\t\tn += binary.PutVarint(b[n:], ts)\n\t\to.propose(b[:n])\n\t} else if o.store != nil {\n\t\to.store.UpdateDelivered(dseq, sseq, dc, ts)\n\t}\n\t// Update activity.\n\to.ldt = time.Now()\n}\n\n// Used to remember a pending ack reply in a replicated consumer.\n// Lock should be held.\nfunc (o *consumer) addAckReply(sseq uint64, reply string) {\n\tif o.replies == nil {\n\t\to.replies = make(map[uint64]string)\n\t}\n\to.replies[sseq] = reply\n}\n\n// Used to remember messages that need to be sent for a replicated consumer, after delivered quorum.\n// Lock should be held.\nfunc (o *consumer) addReplicatedQueuedMsg(pmsg *jsPubMsg) {\n\t// Is not explicitly limited in size, but will at maximum hold maximum ack pending.\n\tif o.pendingDeliveries == nil {\n\t\to.pendingDeliveries = make(map[uint64]*jsPubMsg)\n\t}\n\to.pendingDeliveries[pmsg.seq] = pmsg\n}\n\n// Lock should be held.\nfunc (o *consumer) updateAcks(dseq, sseq uint64, reply string) {\n\tif o.node != nil {\n\t\t// Inline for now, use variable compression.\n\t\tvar b [2*binary.MaxVarintLen64 + 1]byte\n\t\tb[0] = byte(updateAcksOp)\n\t\tn := 1\n\t\tn += binary.PutUvarint(b[n:], dseq)\n\t\tn += binary.PutUvarint(b[n:], sseq)\n\t\to.propose(b[:n])\n\t\tif reply != _EMPTY_ {\n\t\t\to.addAckReply(sseq, reply)\n\t\t}\n\t} else if o.store != nil {\n\t\to.store.UpdateAcks(dseq, sseq)\n\t\tif reply != _EMPTY_ {\n\t\t\t// Already locked so send direct.\n\t\t\to.outq.sendMsg(reply, nil)\n\t\t}\n\t}\n\t// Update activity.\n\to.lat = time.Now()\n}\n\n// Communicate to the cluster an addition of a pending request.\n// Lock should be held.\nfunc (o *consumer) addClusterPendingRequest(reply string) {\n\tif o.node == nil || !o.pendingRequestsOk() {\n\t\treturn\n\t}\n\tb := make([]byte, len(reply)+1)\n\tb[0] = byte(addPendingRequest)\n\tcopy(b[1:], reply)\n\to.propose(b)\n}\n\n// Communicate to the cluster a removal of a pending request.\n// Lock should be held.\nfunc (o *consumer) removeClusterPendingRequest(reply string) {\n\tif o.node == nil || !o.pendingRequestsOk() {\n\t\treturn\n\t}\n\tb := make([]byte, len(reply)+1)\n\tb[0] = byte(removePendingRequest)\n\tcopy(b[1:], reply)\n\to.propose(b)\n}\n\n// Set whether or not we can send pending requests to followers.\nfunc (o *consumer) setPendingRequestsOk(ok bool) {\n\to.mu.Lock()\n\to.prOk = ok\n\to.mu.Unlock()\n}\n\n// Lock should be held.\nfunc (o *consumer) pendingRequestsOk() bool {\n\treturn o.prOk\n}\n\n// Set whether or not we can send info about pending pull requests to our group.\n// Will require all peers have a minimum version.\nfunc (o *consumer) checkAndSetPendingRequestsOk() {\n\to.mu.RLock()\n\ts, isValid := o.srv, o.mset != nil\n\to.mu.RUnlock()\n\tif !isValid {\n\t\treturn\n\t}\n\n\tif ca := o.consumerAssignment(); ca != nil && len(ca.Group.Peers) > 1 {\n\t\tfor _, pn := range ca.Group.Peers {\n\t\t\tif si, ok := s.nodeToInfo.Load(pn); ok {\n\t\t\t\tif !versionAtLeast(si.(nodeInfo).version, 2, 7, 1) {\n\t\t\t\t\t// We expect all of our peers to eventually be up to date.\n\t\t\t\t\t// So check again in awhile.\n\t\t\t\t\ttime.AfterFunc(eventsHBInterval, func() { o.checkAndSetPendingRequestsOk() })\n\t\t\t\t\to.setPendingRequestsOk(false)\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\to.setPendingRequestsOk(true)\n}\n\n// On leadership change make sure we alert the pending requests that they are no longer valid.\nfunc (o *consumer) checkPendingRequests() {\n\to.mu.Lock()\n\tdefer o.mu.Unlock()\n\tif o.mset == nil || o.outq == nil {\n\t\treturn\n\t}\n\thdr := []byte(\"NATS/1.0 409 Leadership Change\\r\\n\\r\\n\")\n\tfor reply := range o.prm {\n\t\to.outq.send(newJSPubMsg(reply, _EMPTY_, _EMPTY_, hdr, nil, nil, 0))\n\t}\n\to.prm = nil\n}\n\n// This will release any pending pull requests if applicable.\n// Should be called only by the leader being deleted or stopped.\n// Lock should be held.\nfunc (o *consumer) releaseAnyPendingRequests(isAssigned bool) {\n\tif o.mset == nil || o.outq == nil || o.waiting.len() == 0 {\n\t\treturn\n\t}\n\tvar hdr []byte\n\tif !isAssigned {\n\t\thdr = []byte(\"NATS/1.0 409 Consumer Deleted\\r\\n\\r\\n\")\n\t}\n\n\twq := o.waiting\n\tfor wr := wq.head; wr != nil; {\n\t\tif hdr != nil {\n\t\t\to.outq.send(newJSPubMsg(wr.reply, _EMPTY_, _EMPTY_, hdr, nil, nil, 0))\n\t\t}\n\t\tnext := wr.next\n\t\twr.recycle()\n\t\twr = next\n\t}\n\t// Nil out old queue.\n\to.waiting = nil\n}\n\n// Process a NAK.\nfunc (o *consumer) processNak(sseq, dseq, dc uint64, nak []byte) {\n\to.mu.Lock()\n\tdefer o.mu.Unlock()\n\n\t// Check for out of range.\n\tif dseq <= o.adflr || dseq > o.dseq {\n\t\treturn\n\t}\n\t// If we are explicit ack make sure this is still on our pending list.\n\tif _, ok := o.pending[sseq]; !ok {\n\t\treturn\n\t}\n\n\t// Deliver an advisory\n\te := JSConsumerDeliveryNakAdvisory{\n\t\tTypedEvent: TypedEvent{\n\t\t\tType: JSConsumerDeliveryNakAdvisoryType,\n\t\t\tID:   nuid.Next(),\n\t\t\tTime: time.Now().UTC(),\n\t\t},\n\t\tStream:      o.stream,\n\t\tConsumer:    o.name,\n\t\tConsumerSeq: dseq,\n\t\tStreamSeq:   sseq,\n\t\tDeliveries:  dc,\n\t\tDomain:      o.srv.getOpts().JetStreamDomain,\n\t}\n\n\to.sendAdvisory(o.nakEventT, e)\n\n\t// Check to see if we have delays attached.\n\tif len(nak) > len(AckNak) {\n\t\targ := bytes.TrimSpace(nak[len(AckNak):])\n\t\tif len(arg) > 0 {\n\t\t\tvar d time.Duration\n\t\t\tvar err error\n\t\t\tif arg[0] == '{' {\n\t\t\t\tvar nd ConsumerNakOptions\n\t\t\t\tif err = json.Unmarshal(arg, &nd); err == nil {\n\t\t\t\t\td = nd.Delay\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\td, err = time.ParseDuration(string(arg))\n\t\t\t}\n\t\t\tif err != nil {\n\t\t\t\t// Treat this as normal NAK.\n\t\t\t\to.srv.Warnf(\"JetStream consumer '%s > %s > %s' bad NAK delay value: %q\", o.acc.Name, o.stream, o.name, arg)\n\t\t\t} else {\n\t\t\t\t// We have a parsed duration that the user wants us to wait before retrying.\n\t\t\t\t// Make sure we are not on the rdq.\n\t\t\t\to.removeFromRedeliverQueue(sseq)\n\t\t\t\tif p, ok := o.pending[sseq]; ok {\n\t\t\t\t\t// now - ackWait is expired now, so offset from there.\n\t\t\t\t\tp.Timestamp = time.Now().Add(-o.cfg.AckWait).Add(d).UnixNano()\n\t\t\t\t\t// Update store system which will update followers as well.\n\t\t\t\t\to.updateDelivered(p.Sequence, sseq, dc, p.Timestamp)\n\t\t\t\t\tif o.ptmr != nil {\n\t\t\t\t\t\t// Want checkPending to run and figure out the next timer ttl.\n\t\t\t\t\t\t// TODO(dlc) - We could optimize this maybe a bit more and track when we expect the timer to fire.\n\t\t\t\t\t\to.resetPtmr(10 * time.Millisecond)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\t// Nothing else for use to do now so return.\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}\n\n\t// If already queued up also ignore.\n\tif !o.onRedeliverQueue(sseq) {\n\t\to.addToRedeliverQueue(sseq)\n\t}\n\n\to.signalNewMessages()\n}\n\n// Process a TERM\n// Returns `true` if the ack was processed in place and the sender can now respond\n// to the client, or `false` if there was an error or the ack is replicated (in which\n// case the reply will be sent later).\nfunc (o *consumer) processTerm(sseq, dseq, dc uint64, reason, reply string) bool {\n\t// Treat like an ack to suppress redelivery.\n\tackedInPlace := o.processAckMsg(sseq, dseq, dc, reply, false)\n\n\to.mu.Lock()\n\tdefer o.mu.Unlock()\n\n\t// Deliver an advisory\n\te := JSConsumerDeliveryTerminatedAdvisory{\n\t\tTypedEvent: TypedEvent{\n\t\t\tType: JSConsumerDeliveryTerminatedAdvisoryType,\n\t\t\tID:   nuid.Next(),\n\t\t\tTime: time.Now().UTC(),\n\t\t},\n\t\tStream:      o.stream,\n\t\tConsumer:    o.name,\n\t\tConsumerSeq: dseq,\n\t\tStreamSeq:   sseq,\n\t\tDeliveries:  dc,\n\t\tReason:      reason,\n\t\tDomain:      o.srv.getOpts().JetStreamDomain,\n\t}\n\n\tsubj := JSAdvisoryConsumerMsgTerminatedPre + \".\" + o.stream + \".\" + o.name\n\to.sendAdvisory(subj, e)\n\treturn ackedInPlace\n}\n\n// Introduce a small delay in when timer fires to check pending.\n// Allows bursts to be treated in same time frame.\nconst ackWaitDelay = time.Millisecond\n\n// ackWait returns how long to wait to fire the pending timer.\nfunc (o *consumer) ackWait(next time.Duration) time.Duration {\n\tif next > 0 {\n\t\treturn next + ackWaitDelay\n\t}\n\treturn o.cfg.AckWait + ackWaitDelay\n}\n\n// Due to bug in calculation of sequences on restoring redelivered let's do quick sanity check.\n// Lock should be held.\nfunc (o *consumer) checkRedelivered() {\n\tvar shouldUpdateState bool\n\tfor sseq := range o.rdc {\n\t\tif sseq <= o.asflr {\n\t\t\tdelete(o.rdc, sseq)\n\t\t\to.removeFromRedeliverQueue(sseq)\n\t\t\tshouldUpdateState = true\n\t\t}\n\t}\n\tif shouldUpdateState {\n\t\tif err := o.writeStoreStateUnlocked(); err != nil && o.srv != nil && o.mset != nil && !o.closed {\n\t\t\ts, acc, mset, name := o.srv, o.acc, o.mset, o.name\n\t\t\ts.Warnf(\"Consumer '%s > %s > %s' error on write store state from check redelivered: %v\", acc, mset.getCfgName(), name, err)\n\t\t}\n\t}\n}\n\n// This will restore the state from disk.\n// Lock should be held.\nfunc (o *consumer) readStoredState() error {\n\tif o.store == nil {\n\t\treturn nil\n\t}\n\tstate, err := o.store.State()\n\tif err == nil {\n\t\to.applyState(state)\n\t\tif len(o.rdc) > 0 {\n\t\t\to.checkRedelivered()\n\t\t}\n\t}\n\treturn err\n}\n\n// Apply the consumer stored state.\n// Lock should be held.\nfunc (o *consumer) applyState(state *ConsumerState) {\n\tif state == nil {\n\t\treturn\n\t}\n\n\to.sseq = state.Delivered.Stream + 1\n\to.dseq = state.Delivered.Consumer + 1\n\to.adflr = state.AckFloor.Consumer\n\to.asflr = state.AckFloor.Stream\n\to.pending = state.Pending\n\to.rdc = state.Redelivered\n\n\t// Setup tracking timer if we have restored pending.\n\tif o.isLeader() && len(o.pending) > 0 {\n\t\t// This is on startup or leader change. We want to check pending\n\t\t// sooner in case there are inconsistencies etc. Pick between 500ms - 1.5s\n\t\tdelay := 500*time.Millisecond + time.Duration(rand.Int63n(1000))*time.Millisecond\n\n\t\t// If normal is lower than this just use that.\n\t\tif o.cfg.AckWait < delay {\n\t\t\tdelay = o.ackWait(0)\n\t\t}\n\t\to.resetPtmr(delay)\n\t}\n}\n\n// Sets our store state from another source. Used in clustered mode on snapshot restore.\n// Lock should be held.\nfunc (o *consumer) setStoreState(state *ConsumerState) error {\n\tif state == nil || o.store == nil {\n\t\treturn nil\n\t}\n\terr := o.store.Update(state)\n\tif err == nil {\n\t\to.applyState(state)\n\t}\n\treturn err\n}\n\n// Update our state to the store.\nfunc (o *consumer) writeStoreState() error {\n\to.mu.Lock()\n\tdefer o.mu.Unlock()\n\treturn o.writeStoreStateUnlocked()\n}\n\n// Update our state to the store.\n// Lock should be held.\nfunc (o *consumer) writeStoreStateUnlocked() error {\n\tif o.store == nil {\n\t\treturn nil\n\t}\n\tstate := ConsumerState{\n\t\tDelivered: SequencePair{\n\t\t\tConsumer: o.dseq - 1,\n\t\t\tStream:   o.sseq - 1,\n\t\t},\n\t\tAckFloor: SequencePair{\n\t\t\tConsumer: o.adflr,\n\t\t\tStream:   o.asflr,\n\t\t},\n\t\tPending:     o.pending,\n\t\tRedelivered: o.rdc,\n\t}\n\treturn o.store.Update(&state)\n}\n\n// Returns an initial info. Only applicable for non-clustered consumers.\n// We will clear after we return it, so one shot.\nfunc (o *consumer) initialInfo() *ConsumerInfo {\n\to.mu.Lock()\n\tici := o.ici\n\to.ici = nil // gc friendly\n\to.mu.Unlock()\n\tif ici == nil {\n\t\tici = o.info()\n\t}\n\treturn ici\n}\n\n// Clears our initial info.\n// Used when we have a leader change in cluster mode but do not send a response.\nfunc (o *consumer) clearInitialInfo() {\n\to.mu.Lock()\n\to.ici = nil // gc friendly\n\to.mu.Unlock()\n}\n\n// Info returns our current consumer state.\nfunc (o *consumer) info() *ConsumerInfo {\n\treturn o.infoWithSnap(false)\n}\n\nfunc (o *consumer) infoWithSnap(snap bool) *ConsumerInfo {\n\treturn o.infoWithSnapAndReply(snap, _EMPTY_)\n}\n\nfunc (o *consumer) infoWithSnapAndReply(snap bool, reply string) *ConsumerInfo {\n\to.mu.Lock()\n\tmset := o.mset\n\tif o.closed || mset == nil || mset.srv == nil {\n\t\to.mu.Unlock()\n\t\treturn nil\n\t}\n\tjs := o.js\n\tif js == nil {\n\t\to.mu.Unlock()\n\t\treturn nil\n\t}\n\n\t// Capture raftGroup.\n\tvar rg *raftGroup\n\tif o.ca != nil {\n\t\trg = o.ca.Group\n\t}\n\n\tpriorityGroups := []PriorityGroupState{}\n\t// TODO(jrm): when we introduce supporting many priority groups, we need to update assigning `o.currentNuid` for each group.\n\tif len(o.cfg.PriorityGroups) > 0 {\n\t\tpriorityGroups = append(priorityGroups, PriorityGroupState{\n\t\t\tGroup:          o.cfg.PriorityGroups[0],\n\t\t\tPinnedClientID: o.currentPinId,\n\t\t\tPinnedTS:       o.pinnedTS,\n\t\t})\n\t}\n\n\tcfg := o.cfg\n\tinfo := &ConsumerInfo{\n\t\tStream:  o.stream,\n\t\tName:    o.name,\n\t\tCreated: o.created,\n\t\tConfig:  &cfg,\n\t\tDelivered: SequenceInfo{\n\t\t\tConsumer: o.dseq - 1,\n\t\t\tStream:   o.sseq - 1,\n\t\t},\n\t\tAckFloor: SequenceInfo{\n\t\t\tConsumer: o.adflr,\n\t\t\tStream:   o.asflr,\n\t\t},\n\t\tNumAckPending:  len(o.pending),\n\t\tNumRedelivered: len(o.rdc),\n\t\tNumPending:     o.checkNumPending(),\n\t\tPushBound:      o.isPushMode() && o.active,\n\t\tTimeStamp:      time.Now().UTC(),\n\t\tPriorityGroups: priorityGroups,\n\t}\n\t// Reset redelivered for MaxDeliver 1. Redeliveries are disabled so must not report it (is confusing otherwise).\n\t// The state does still keep track of these messages.\n\tif o.cfg.MaxDeliver == 1 {\n\t\tinfo.NumRedelivered = 0\n\t}\n\tif o.cfg.PauseUntil != nil {\n\t\tp := *o.cfg.PauseUntil\n\t\tif info.Paused = time.Now().Before(p); info.Paused {\n\t\t\tinfo.PauseRemaining = time.Until(p)\n\t\t}\n\t}\n\n\t// We always need to pull certain data from our store.\n\tif o.store != nil {\n\t\tstate, err := o.store.BorrowState()\n\t\tif err != nil {\n\t\t\to.mu.Unlock()\n\t\t\treturn nil\n\t\t}\n\n\t\t// If we are the leader we could have o.sseq that is skipped ahead.\n\t\t// To maintain consistency in reporting (e.g. jsz) we always take the state for our delivered/ackfloor stream sequence.\n\t\t// Only use skipped ahead o.sseq if we're a new consumer and have not yet replicated this state yet.\n\t\tleader := o.isLeader()\n\t\tif !leader || o.store.HasState() {\n\t\t\tinfo.Delivered.Consumer, info.Delivered.Stream = state.Delivered.Consumer, state.Delivered.Stream\n\t\t}\n\t\tinfo.AckFloor.Consumer, info.AckFloor.Stream = state.AckFloor.Consumer, state.AckFloor.Stream\n\t\tif !leader {\n\t\t\tinfo.NumAckPending = len(state.Pending)\n\t\t\tinfo.NumRedelivered = len(state.Redelivered)\n\t\t}\n\t}\n\n\t// Adjust active based on non-zero etc. Also make UTC here.\n\tif !o.ldt.IsZero() {\n\t\tldt := o.ldt.UTC() // This copies as well.\n\t\tinfo.Delivered.Last = &ldt\n\t}\n\tif !o.lat.IsZero() {\n\t\tlat := o.lat.UTC() // This copies as well.\n\t\tinfo.AckFloor.Last = &lat\n\t}\n\n\t// If we are a pull mode consumer, report on number of waiting requests.\n\tif o.isPullMode() {\n\t\to.processWaiting(false)\n\t\tinfo.NumWaiting = o.waiting.len()\n\t}\n\t// If we were asked to snapshot do so here.\n\tif snap {\n\t\to.ici = info\n\t}\n\tsysc := o.sysc\n\to.mu.Unlock()\n\n\t// Do cluster.\n\tif rg != nil {\n\t\tinfo.Cluster = js.clusterInfo(rg)\n\t}\n\n\t// If we have a reply subject send the response here.\n\tif reply != _EMPTY_ && sysc != nil {\n\t\tsysc.sendInternalMsg(reply, _EMPTY_, nil, info)\n\t}\n\n\treturn info\n}\n\n// Will signal us that new messages are available. Will break out of waiting.\nfunc (o *consumer) signalNewMessages() {\n\t// Kick our new message channel\n\tselect {\n\tcase o.mch <- struct{}{}:\n\tdefault:\n\t}\n}\n\n// shouldSample lets us know if we are sampling metrics on acks.\nfunc (o *consumer) shouldSample() bool {\n\tswitch {\n\tcase o.sfreq <= 0:\n\t\treturn false\n\tcase o.sfreq >= 100:\n\t\treturn true\n\t}\n\n\t// TODO(ripienaar) this is a tad slow so we need to rethink here, however this will only\n\t// hit for those with sampling enabled and its not the default\n\treturn rand.Int31n(100) <= o.sfreq\n}\n\nfunc (o *consumer) sampleAck(sseq, dseq, dc uint64) {\n\tif !o.shouldSample() {\n\t\treturn\n\t}\n\n\tnow := time.Now().UTC()\n\tunow := now.UnixNano()\n\n\te := JSConsumerAckMetric{\n\t\tTypedEvent: TypedEvent{\n\t\t\tType: JSConsumerAckMetricType,\n\t\t\tID:   nuid.Next(),\n\t\t\tTime: now,\n\t\t},\n\t\tStream:      o.stream,\n\t\tConsumer:    o.name,\n\t\tConsumerSeq: dseq,\n\t\tStreamSeq:   sseq,\n\t\tDelay:       unow - o.pending[sseq].Timestamp,\n\t\tDeliveries:  dc,\n\t\tDomain:      o.srv.getOpts().JetStreamDomain,\n\t}\n\n\to.sendAdvisory(o.ackEventT, e)\n}\n\n// Process an ACK.\n// Returns `true` if the ack was processed in place and the sender can now respond\n// to the client, or `false` if there was an error or the ack is replicated (in which\n// case the reply will be sent later).\nfunc (o *consumer) processAckMsg(sseq, dseq, dc uint64, reply string, doSample bool) bool {\n\to.mu.Lock()\n\tif o.closed {\n\t\to.mu.Unlock()\n\t\treturn false\n\t}\n\n\tmset := o.mset\n\tif mset == nil || mset.closed.Load() {\n\t\to.mu.Unlock()\n\t\treturn false\n\t}\n\n\t// Check if this ack is above the current pointer to our next to deliver.\n\tif sseq >= o.sseq {\n\t\t// Let's make sure this is valid.\n\t\t// This is only received on the consumer leader, so should never be higher\n\t\t// than the last stream sequence. But could happen if we've just become\n\t\t// consumer leader, and we are not up-to-date on the stream yet.\n\t\tvar ss StreamState\n\t\tmset.store.FastState(&ss)\n\t\tif sseq > ss.LastSeq {\n\t\t\to.srv.Warnf(\"JetStream consumer '%s > %s > %s' ACK sequence %d past last stream sequence of %d\",\n\t\t\t\to.acc.Name, o.stream, o.name, sseq, ss.LastSeq)\n\t\t\t// FIXME(dlc) - For 2.11 onwards should we return an error here to the caller?\n\t\t}\n\t\t// Even though another leader must have delivered a message with this sequence, we must not adjust\n\t\t// the current pointer. This could otherwise result in a stuck consumer, where messages below this\n\t\t// sequence can't be redelivered, and we'll have incorrect pending state and ack floors.\n\t\to.mu.Unlock()\n\t\treturn false\n\t}\n\n\t// Let the owning stream know if we are interest or workqueue retention based.\n\t// If this consumer is clustered (o.node != nil) this will be handled by\n\t// processReplicatedAck after the ack has propagated.\n\tackInPlace := o.node == nil && o.retention != LimitsPolicy\n\n\tvar sgap, floor uint64\n\tvar needSignal bool\n\n\tswitch o.cfg.AckPolicy {\n\tcase AckExplicit:\n\t\tif p, ok := o.pending[sseq]; ok {\n\t\t\tif doSample {\n\t\t\t\to.sampleAck(sseq, dseq, dc)\n\t\t\t}\n\t\t\tif o.maxp > 0 && len(o.pending) >= o.maxp {\n\t\t\t\tneedSignal = true\n\t\t\t}\n\t\t\tdelete(o.pending, sseq)\n\t\t\t// Use the original deliver sequence from our pending record.\n\t\t\tdseq = p.Sequence\n\n\t\t\t// Only move floors if we matched an existing pending.\n\t\t\tif len(o.pending) == 0 {\n\t\t\t\to.adflr = o.dseq - 1\n\t\t\t\to.asflr = o.sseq - 1\n\t\t\t} else if dseq == o.adflr+1 {\n\t\t\t\to.adflr, o.asflr = dseq, sseq\n\t\t\t\tfor ss := sseq + 1; ss < o.sseq; ss++ {\n\t\t\t\t\tif p, ok := o.pending[ss]; ok {\n\t\t\t\t\t\tif p.Sequence > 0 {\n\t\t\t\t\t\t\to.adflr, o.asflr = p.Sequence-1, ss-1\n\t\t\t\t\t\t}\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tdelete(o.rdc, sseq)\n\t\to.removeFromRedeliverQueue(sseq)\n\tcase AckAll:\n\t\t// no-op\n\t\tif dseq <= o.adflr || sseq <= o.asflr {\n\t\t\to.mu.Unlock()\n\t\t\t// Return true to let caller respond back to the client.\n\t\t\treturn true\n\t\t}\n\t\tif o.maxp > 0 && len(o.pending) >= o.maxp {\n\t\t\tneedSignal = true\n\t\t}\n\t\tsgap = sseq - o.asflr\n\t\tfloor = sseq // start at same and set lower as we go.\n\t\to.adflr, o.asflr = dseq, sseq\n\n\t\tremove := func(seq uint64) {\n\t\t\tdelete(o.pending, seq)\n\t\t\tdelete(o.rdc, seq)\n\t\t\to.removeFromRedeliverQueue(seq)\n\t\t\tif seq < floor {\n\t\t\t\tfloor = seq\n\t\t\t}\n\t\t}\n\t\t// Determine if smarter to walk all of pending vs the sequence range.\n\t\tif sgap > uint64(len(o.pending)) {\n\t\t\tfor seq := range o.pending {\n\t\t\t\tif seq <= sseq {\n\t\t\t\t\tremove(seq)\n\t\t\t\t}\n\t\t\t}\n\t\t} else {\n\t\t\tfor seq := sseq; seq > sseq-sgap && len(o.pending) > 0; seq-- {\n\t\t\t\tremove(seq)\n\t\t\t}\n\t\t}\n\tcase AckNone:\n\t\t// FIXME(dlc) - This is error but do we care?\n\t\to.mu.Unlock()\n\t\treturn ackInPlace\n\t}\n\n\t// No ack replication, so we set reply to \"\" so that updateAcks does not\n\t// send the reply. The caller will.\n\tif ackInPlace {\n\t\treply = _EMPTY_\n\t}\n\t// Update underlying store.\n\to.updateAcks(dseq, sseq, reply)\n\to.mu.Unlock()\n\n\tif ackInPlace {\n\t\tif sgap > 1 {\n\t\t\t// FIXME(dlc) - This can very inefficient, will need to fix.\n\t\t\tfor seq := sseq; seq >= floor; seq-- {\n\t\t\t\tmset.ackMsg(o, seq)\n\t\t\t}\n\t\t} else {\n\t\t\tmset.ackMsg(o, sseq)\n\t\t}\n\t}\n\n\t// If we had max ack pending set and were at limit we need to unblock ourselves.\n\tif needSignal {\n\t\to.signalNewMessages()\n\t}\n\treturn ackInPlace\n}\n\n// Determine if this is a truly filtered consumer. Modern clients will place filtered subjects\n// even if the stream only has a single non-wildcard subject designation.\n// Read lock should be held.\nfunc (o *consumer) isFiltered() bool {\n\tif o.subjf == nil {\n\t\treturn false\n\t}\n\t// If we are here we want to check if the filtered subject is\n\t// a direct match for our only listed subject.\n\tmset := o.mset\n\tif mset == nil {\n\t\treturn true\n\t}\n\n\t// Protect access to mset.cfg with the cfgMu mutex.\n\tmset.cfgMu.RLock()\n\tmsetSubjects := mset.cfg.Subjects\n\tmset.cfgMu.RUnlock()\n\n\t// `isFiltered` need to be performant, so we do\n\t// as any checks as possible to avoid unnecessary work.\n\t// Here we avoid iteration over slices if there is only one subject in stream\n\t// and one filter for the consumer.\n\tif len(msetSubjects) == 1 && len(o.subjf) == 1 {\n\t\treturn msetSubjects[0] != o.subjf[0].subject\n\t}\n\n\t// if the list is not equal length, we can return early, as this is filtered.\n\tif len(msetSubjects) != len(o.subjf) {\n\t\treturn true\n\t}\n\n\t// if in rare case scenario that user passed all stream subjects as consumer filters,\n\t// we need to do a more expensive operation.\n\t// reflect.DeepEqual would return false if the filters are the same, but in different order\n\t// so it can't be used here.\n\tcfilters := make(map[string]struct{}, len(o.subjf))\n\tfor _, val := range o.subjf {\n\t\tcfilters[val.subject] = struct{}{}\n\t}\n\tfor _, val := range msetSubjects {\n\t\tif _, ok := cfilters[val]; !ok {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\n// Check if we need an ack for this store seq.\n// This is called for interest based retention streams to remove messages.\nfunc (o *consumer) needAck(sseq uint64, subj string) bool {\n\tvar needAck bool\n\tvar asflr, osseq uint64\n\tvar pending map[uint64]*Pending\n\tvar rdc map[uint64]uint64\n\n\to.mu.RLock()\n\tdefer o.mu.RUnlock()\n\n\tisFiltered := o.isFiltered()\n\tif isFiltered && o.mset == nil {\n\t\treturn false\n\t}\n\n\t// Check if we are filtered, and if so check if this is even applicable to us.\n\tif isFiltered {\n\t\tif subj == _EMPTY_ {\n\t\t\tvar err error\n\t\t\tif subj, err = o.mset.store.SubjectForSeq(sseq); err != nil {\n\t\t\t\treturn false\n\t\t\t}\n\t\t}\n\t\tif !o.isFilteredMatch(subj) {\n\t\t\treturn false\n\t\t}\n\t}\n\tif o.isLeader() {\n\t\tasflr, osseq = o.asflr, o.sseq\n\t\tpending, rdc = o.pending, o.rdc\n\t} else {\n\t\tif o.store == nil {\n\t\t\treturn false\n\t\t}\n\t\tstate, err := o.store.BorrowState()\n\t\tif err != nil || state == nil {\n\t\t\t// Fall back to what we track internally for now.\n\t\t\treturn sseq > o.asflr && !o.isFiltered()\n\t\t}\n\t\t// If loading state as here, the osseq is +1.\n\t\tasflr, osseq, pending, rdc = state.AckFloor.Stream, state.Delivered.Stream+1, state.Pending, state.Redelivered\n\t}\n\n\tswitch o.cfg.AckPolicy {\n\tcase AckNone, AckAll:\n\t\tneedAck = sseq > asflr\n\tcase AckExplicit:\n\t\tif sseq > asflr {\n\t\t\tif sseq >= osseq {\n\t\t\t\tneedAck = true\n\t\t\t} else {\n\t\t\t\t_, needAck = pending[sseq]\n\t\t\t}\n\t\t}\n\t}\n\n\t// Finally check if redelivery of this message is tracked.\n\t// If the message is not pending, it should be preserved if it reached max delivery.\n\tif !needAck {\n\t\t_, needAck = rdc[sseq]\n\t}\n\n\treturn needAck\n}\n\ntype PriorityGroup struct {\n\tGroup         string `json:\"group,omitempty\"`\n\tMinPending    int64  `json:\"min_pending,omitempty\"`\n\tMinAckPending int64  `json:\"min_ack_pending,omitempty\"`\n\tId            string `json:\"id,omitempty\"`\n}\n\n// Used in nextReqFromMsg, since the json.Unmarshal causes the request\n// struct to escape to the heap always. This should reduce GC pressure.\nvar jsGetNextPool = sync.Pool{\n\tNew: func() any {\n\t\treturn &JSApiConsumerGetNextRequest{}\n\t},\n}\n\n// Helper for the next message requests.\nfunc nextReqFromMsg(msg []byte) (time.Time, int, int, bool, time.Duration, time.Time, *PriorityGroup, error) {\n\treq := bytes.TrimSpace(msg)\n\n\tswitch {\n\tcase len(req) == 0:\n\t\treturn time.Time{}, 1, 0, false, 0, time.Time{}, nil, nil\n\n\tcase req[0] == '{':\n\t\tcr := jsGetNextPool.Get().(*JSApiConsumerGetNextRequest)\n\t\tdefer func() {\n\t\t\t*cr = JSApiConsumerGetNextRequest{}\n\t\t\tjsGetNextPool.Put(cr)\n\t\t}()\n\t\tif err := json.Unmarshal(req, &cr); err != nil {\n\t\t\treturn time.Time{}, -1, 0, false, 0, time.Time{}, nil, err\n\t\t}\n\t\tvar hbt time.Time\n\t\tif cr.Heartbeat > 0 {\n\t\t\tif cr.Heartbeat*2 > cr.Expires {\n\t\t\t\treturn time.Time{}, 1, 0, false, 0, time.Time{}, nil, errors.New(\"heartbeat value too large\")\n\t\t\t}\n\t\t\thbt = time.Now().Add(cr.Heartbeat)\n\t\t}\n\t\tpriorityGroup := cr.PriorityGroup\n\t\tif cr.Expires == time.Duration(0) {\n\t\t\treturn time.Time{}, cr.Batch, cr.MaxBytes, cr.NoWait, cr.Heartbeat, hbt, &priorityGroup, nil\n\t\t}\n\t\treturn time.Now().Add(cr.Expires), cr.Batch, cr.MaxBytes, cr.NoWait, cr.Heartbeat, hbt, &priorityGroup, nil\n\tdefault:\n\t\tif n, err := strconv.Atoi(string(req)); err == nil {\n\t\t\treturn time.Time{}, n, 0, false, 0, time.Time{}, nil, nil\n\t\t}\n\t}\n\n\treturn time.Time{}, 1, 0, false, 0, time.Time{}, nil, nil\n}\n\n// Represents a request that is on the internal waiting queue\ntype waitingRequest struct {\n\tnext          *waitingRequest\n\tacc           *Account\n\tinterest      string\n\treply         string\n\tn             int // For batching\n\td             int // num delivered\n\tb             int // For max bytes tracking\n\texpires       time.Time\n\treceived      time.Time\n\thb            time.Duration\n\thbt           time.Time\n\tnoWait        bool\n\tpriorityGroup *PriorityGroup\n}\n\n// sync.Pool for waiting requests.\nvar wrPool = sync.Pool{\n\tNew: func() any {\n\t\treturn new(waitingRequest)\n\t},\n}\n\n// Recycle this request. This request can not be accessed after this call.\nfunc (wr *waitingRequest) recycleIfDone() bool {\n\tif wr != nil && wr.n <= 0 {\n\t\twr.recycle()\n\t\treturn true\n\t}\n\treturn false\n}\n\n// Force a recycle.\nfunc (wr *waitingRequest) recycle() {\n\tif wr != nil {\n\t\twr.next, wr.acc, wr.interest, wr.reply = nil, nil, _EMPTY_, _EMPTY_\n\t\twrPool.Put(wr)\n\t}\n}\n\n// waiting queue for requests that are waiting for new messages to arrive.\ntype waitQueue struct {\n\tn, max int\n\tlast   time.Time\n\thead   *waitingRequest\n\ttail   *waitingRequest\n}\n\n// Create a new ring buffer with at most max items.\nfunc newWaitQueue(max int) *waitQueue {\n\treturn &waitQueue{max: max}\n}\n\nvar (\n\terrWaitQueueFull = errors.New(\"wait queue is full\")\n\terrWaitQueueNil  = errors.New(\"wait queue is nil\")\n)\n\n// Adds in a new request.\nfunc (wq *waitQueue) add(wr *waitingRequest) error {\n\tif wq == nil {\n\t\treturn errWaitQueueNil\n\t}\n\tif wq.isFull() {\n\t\treturn errWaitQueueFull\n\t}\n\tif wq.head == nil {\n\t\twq.head = wr\n\t} else {\n\t\twq.tail.next = wr\n\t}\n\t// Always set tail.\n\twq.tail = wr\n\t// Make sure nil\n\twr.next = nil\n\n\t// Track last active via when we receive a request.\n\twq.last = wr.received\n\twq.n++\n\treturn nil\n}\n\nfunc (wq *waitQueue) isFull() bool {\n\tif wq == nil {\n\t\treturn false\n\t}\n\treturn wq.n == wq.max\n}\n\nfunc (wq *waitQueue) isEmpty() bool {\n\tif wq == nil {\n\t\treturn true\n\t}\n\treturn wq.n == 0\n}\n\nfunc (wq *waitQueue) len() int {\n\tif wq == nil {\n\t\treturn 0\n\t}\n\treturn wq.n\n}\n\n// Peek will return the next request waiting or nil if empty.\nfunc (wq *waitQueue) peek() *waitingRequest {\n\tif wq == nil {\n\t\treturn nil\n\t}\n\treturn wq.head\n}\n\nfunc (wq *waitQueue) cycle() {\n\twr := wq.peek()\n\tif wr != nil {\n\t\t// Always remove current now on a pop, and move to end if still valid.\n\t\t// If we were the only one don't need to remove since this can be a no-op.\n\t\twq.removeCurrent()\n\t\twq.add(wr)\n\t}\n}\n\n// pop will return the next request and move the read cursor.\n// This will now place a request that still has pending items at the ends of the list.\nfunc (wq *waitQueue) pop() *waitingRequest {\n\twr := wq.peek()\n\tif wr != nil {\n\t\twr.d++\n\t\twr.n--\n\t\t// Always remove current now on a pop, and move to end if still valid.\n\t\t// If we were the only one don't need to remove since this can be a no-op.\n\t\tif wr.n > 0 && wq.n > 1 {\n\t\t\twq.removeCurrent()\n\t\t\twq.add(wr)\n\t\t} else if wr.n <= 0 {\n\t\t\twq.removeCurrent()\n\t\t}\n\t}\n\treturn wr\n}\n\n// Removes the current read pointer (head FIFO) entry.\nfunc (wq *waitQueue) removeCurrent() {\n\twq.remove(nil, wq.head)\n}\n\n// Remove the wr element from the wait queue.\nfunc (wq *waitQueue) remove(pre, wr *waitingRequest) {\n\tif wr == nil {\n\t\treturn\n\t}\n\tif pre != nil {\n\t\tpre.next = wr.next\n\t} else if wr == wq.head {\n\t\t// We are removing head here.\n\t\twq.head = wr.next\n\t}\n\t// Check if wr was our tail.\n\tif wr == wq.tail {\n\t\t// Check if we need to assign to pre.\n\t\tif wr.next == nil {\n\t\t\twq.tail = pre\n\t\t} else {\n\t\t\twq.tail = wr.next\n\t\t}\n\t}\n\twq.n--\n}\n\n// Return the map of pending requests keyed by the reply subject.\n// No-op if push consumer or invalid etc.\nfunc (o *consumer) pendingRequests() map[string]*waitingRequest {\n\tif o.waiting == nil {\n\t\treturn nil\n\t}\n\twq, m := o.waiting, make(map[string]*waitingRequest)\n\tfor wr := wq.head; wr != nil; wr = wr.next {\n\t\tm[wr.reply] = wr\n\t}\n\n\treturn m\n}\n\nfunc (o *consumer) setPinnedTimer(priorityGroup string) {\n\tif o.pinnedTtl != nil {\n\t\to.pinnedTtl.Reset(o.cfg.PinnedTTL)\n\t} else {\n\t\to.pinnedTtl = time.AfterFunc(o.cfg.PinnedTTL, func() {\n\t\t\to.mu.Lock()\n\t\t\to.currentPinId = _EMPTY_\n\t\t\to.sendUnpinnedAdvisoryLocked(priorityGroup, \"timeout\")\n\t\t\to.mu.Unlock()\n\t\t\to.signalNewMessages()\n\t\t})\n\t}\n}\n\n// Return next waiting request. This will check for expirations but not noWait or interest.\n// That will be handled by processWaiting.\n// Lock should be held.\nfunc (o *consumer) nextWaiting(sz int) *waitingRequest {\n\tif o.waiting == nil || o.waiting.isEmpty() {\n\t\treturn nil\n\t}\n\n\t// Check if server needs to assign a new pin id.\n\tneedNewPin := o.currentPinId == _EMPTY_ && o.cfg.PriorityPolicy == PriorityPinnedClient\n\t// As long as we support only one priority group, we can capture  that group here and reuse it.\n\tvar priorityGroup string\n\tif len(o.cfg.PriorityGroups) > 0 {\n\t\tpriorityGroup = o.cfg.PriorityGroups[0]\n\t}\n\n\tnumCycled := 0\n\tfor wr := o.waiting.peek(); !o.waiting.isEmpty(); wr = o.waiting.peek() {\n\t\tif wr == nil {\n\t\t\tbreak\n\t\t}\n\t\t// Check if we have max bytes set.\n\t\tif wr.b > 0 {\n\t\t\tif sz <= wr.b {\n\t\t\t\twr.b -= sz\n\t\t\t\t// If we are right now at zero, set batch to 1 to deliver this one but stop after.\n\t\t\t\tif wr.b == 0 {\n\t\t\t\t\twr.n = 1\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\t// Since we can't send that message to the requestor, we need to\n\t\t\t\t// notify that we are closing the request.\n\t\t\t\tconst maxBytesT = \"NATS/1.0 409 Message Size Exceeds MaxBytes\\r\\n%s: %d\\r\\n%s: %d\\r\\n\\r\\n\"\n\t\t\t\thdr := fmt.Appendf(nil, maxBytesT, JSPullRequestPendingMsgs, wr.n, JSPullRequestPendingBytes, wr.b)\n\t\t\t\to.outq.send(newJSPubMsg(wr.reply, _EMPTY_, _EMPTY_, hdr, nil, nil, 0))\n\t\t\t\t// Remove the current one, no longer valid due to max bytes limit.\n\t\t\t\to.waiting.removeCurrent()\n\t\t\t\tif o.node != nil {\n\t\t\t\t\to.removeClusterPendingRequest(wr.reply)\n\t\t\t\t}\n\t\t\t\twr.recycle()\n\t\t\t\tcontinue\n\t\t\t}\n\t\t}\n\n\t\tif wr.expires.IsZero() || time.Now().Before(wr.expires) {\n\t\t\tif needNewPin {\n\t\t\t\tif wr.priorityGroup.Id == _EMPTY_ {\n\t\t\t\t\to.currentPinId = nuid.Next()\n\t\t\t\t\to.pinnedTS = time.Now().UTC()\n\t\t\t\t\twr.priorityGroup.Id = o.currentPinId\n\t\t\t\t\to.setPinnedTimer(priorityGroup)\n\n\t\t\t\t} else {\n\t\t\t\t\t// There is pin id set, but not a matching one. Send a notification to the client and remove the request.\n\t\t\t\t\t// Probably this is the old pin id.\n\t\t\t\t\to.outq.send(newJSPubMsg(wr.reply, _EMPTY_, _EMPTY_, []byte(JSPullRequestWrongPinID), nil, nil, 0))\n\t\t\t\t\to.waiting.removeCurrent()\n\t\t\t\t\tif o.node != nil {\n\t\t\t\t\t\to.removeClusterPendingRequest(wr.reply)\n\t\t\t\t\t}\n\t\t\t\t\twr.recycle()\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t} else if o.currentPinId != _EMPTY_ {\n\t\t\t\t// Check if we have a match on the currentNuid\n\t\t\t\tif wr.priorityGroup != nil && wr.priorityGroup.Id == o.currentPinId {\n\t\t\t\t\t// If we have a match, we do nothing here and will deliver the message later down the code path.\n\t\t\t\t} else if wr.priorityGroup.Id == _EMPTY_ {\n\t\t\t\t\to.waiting.cycle()\n\t\t\t\t\tnumCycled++\n\t\t\t\t\tif numCycled >= o.waiting.len() {\n\t\t\t\t\t\treturn nil\n\t\t\t\t\t}\n\t\t\t\t\tcontinue\n\t\t\t\t} else {\n\t\t\t\t\t// There is pin id set, but not a matching one. Send a notification to the client and remove the request.\n\t\t\t\t\to.outq.send(newJSPubMsg(wr.reply, _EMPTY_, _EMPTY_, []byte(JSPullRequestWrongPinID), nil, nil, 0))\n\t\t\t\t\to.waiting.removeCurrent()\n\t\t\t\t\tif o.node != nil {\n\t\t\t\t\t\to.removeClusterPendingRequest(wr.reply)\n\t\t\t\t\t}\n\t\t\t\t\twr.recycle()\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif o.cfg.PriorityPolicy == PriorityOverflow {\n\t\t\t\tif wr.priorityGroup != nil &&\n\t\t\t\t\t// We need to check o.npc+1, because before calling nextWaiting, we do o.npc--\n\t\t\t\t\t(wr.priorityGroup.MinPending > 0 && wr.priorityGroup.MinPending > o.npc+1 ||\n\t\t\t\t\t\twr.priorityGroup.MinAckPending > 0 && wr.priorityGroup.MinAckPending > int64(len(o.pending))) {\n\t\t\t\t\to.waiting.cycle()\n\t\t\t\t\tnumCycled++\n\t\t\t\t\t// We're done cycling through the requests.\n\t\t\t\t\tif numCycled >= o.waiting.len() {\n\t\t\t\t\t\treturn nil\n\t\t\t\t\t}\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t}\n\t\t\tif wr.acc.sl.HasInterest(wr.interest) {\n\t\t\t\tif needNewPin {\n\t\t\t\t\to.sendPinnedAdvisoryLocked(priorityGroup)\n\t\t\t\t}\n\t\t\t\treturn o.waiting.pop()\n\t\t\t} else if time.Since(wr.received) < defaultGatewayRecentSubExpiration && (o.srv.leafNodeEnabled || o.srv.gateway.enabled) {\n\t\t\t\tif needNewPin {\n\t\t\t\t\to.sendPinnedAdvisoryLocked(priorityGroup)\n\t\t\t\t}\n\t\t\t\treturn o.waiting.pop()\n\t\t\t} else if o.srv.gateway.enabled && o.srv.hasGatewayInterest(wr.acc.Name, wr.interest) {\n\t\t\t\tif needNewPin {\n\t\t\t\t\to.sendPinnedAdvisoryLocked(priorityGroup)\n\t\t\t\t}\n\t\t\t\treturn o.waiting.pop()\n\t\t\t}\n\t\t} else {\n\t\t\t// We do check for expiration in `processWaiting`, but it is possible to hit the expiry here, and not there.\n\t\t\thdr := fmt.Appendf(nil, \"NATS/1.0 408 Request Timeout\\r\\n%s: %d\\r\\n%s: %d\\r\\n\\r\\n\", JSPullRequestPendingMsgs, wr.n, JSPullRequestPendingBytes, wr.b)\n\t\t\to.outq.send(newJSPubMsg(wr.reply, _EMPTY_, _EMPTY_, hdr, nil, nil, 0))\n\t\t\to.waiting.removeCurrent()\n\t\t\tif o.node != nil {\n\t\t\t\to.removeClusterPendingRequest(wr.reply)\n\t\t\t}\n\t\t\twr.recycle()\n\t\t\tcontinue\n\n\t\t}\n\t\tif wr.interest != wr.reply {\n\t\t\tconst intExpT = \"NATS/1.0 408 Interest Expired\\r\\n%s: %d\\r\\n%s: %d\\r\\n\\r\\n\"\n\t\t\thdr := fmt.Appendf(nil, intExpT, JSPullRequestPendingMsgs, wr.n, JSPullRequestPendingBytes, wr.b)\n\t\t\to.outq.send(newJSPubMsg(wr.reply, _EMPTY_, _EMPTY_, hdr, nil, nil, 0))\n\t\t}\n\t\t// Remove the current one, no longer valid.\n\t\to.waiting.removeCurrent()\n\t\tif o.node != nil {\n\t\t\to.removeClusterPendingRequest(wr.reply)\n\t\t}\n\t\twr.recycle()\n\t}\n\n\treturn nil\n}\n\n// Next message request.\ntype nextMsgReq struct {\n\treply string\n\tmsg   []byte\n}\n\nvar nextMsgReqPool sync.Pool\n\nfunc newNextMsgReq(reply string, msg []byte) *nextMsgReq {\n\tvar nmr *nextMsgReq\n\tm := nextMsgReqPool.Get()\n\tif m != nil {\n\t\tnmr = m.(*nextMsgReq)\n\t} else {\n\t\tnmr = &nextMsgReq{}\n\t}\n\t// When getting something from a pool it is critical that all fields are\n\t// initialized. Doing this way guarantees that if someone adds a field to\n\t// the structure, the compiler will fail the build if this line is not updated.\n\t(*nmr) = nextMsgReq{reply, msg}\n\treturn nmr\n}\n\nfunc (nmr *nextMsgReq) returnToPool() {\n\tif nmr == nil {\n\t\treturn\n\t}\n\tnmr.reply, nmr.msg = _EMPTY_, nil\n\tnextMsgReqPool.Put(nmr)\n}\n\n// processNextMsgReq will process a request for the next message available. A nil message payload means deliver\n// a single message. If the payload is a formal request or a number parseable with Atoi(), then we will send a\n// batch of messages without requiring another request to this endpoint, or an ACK.\nfunc (o *consumer) processNextMsgReq(_ *subscription, c *client, _ *Account, _, reply string, msg []byte) {\n\tif reply == _EMPTY_ {\n\t\treturn\n\t}\n\n\t// Short circuit error here.\n\tif o.nextMsgReqs == nil {\n\t\thdr := []byte(\"NATS/1.0 409 Consumer is push based\\r\\n\\r\\n\")\n\t\to.outq.send(newJSPubMsg(reply, _EMPTY_, _EMPTY_, hdr, nil, nil, 0))\n\t\treturn\n\t}\n\n\t_, msg = c.msgParts(msg)\n\to.nextMsgReqs.push(newNextMsgReq(reply, copyBytes(msg)))\n}\n\nfunc (o *consumer) processNextMsgRequest(reply string, msg []byte) {\n\to.mu.Lock()\n\tdefer o.mu.Unlock()\n\n\tmset := o.mset\n\tif mset == nil {\n\t\treturn\n\t}\n\n\tsendErr := func(status int, description string) {\n\t\thdr := fmt.Appendf(nil, \"NATS/1.0 %d %s\\r\\n\\r\\n\", status, description)\n\t\to.outq.send(newJSPubMsg(reply, _EMPTY_, _EMPTY_, hdr, nil, nil, 0))\n\t}\n\n\tif o.isPushMode() || o.waiting == nil {\n\t\tsendErr(409, \"Consumer is push based\")\n\t\treturn\n\t}\n\n\t// Check payload here to see if they sent in batch size or a formal request.\n\texpires, batchSize, maxBytes, noWait, hb, hbt, priorityGroup, err := nextReqFromMsg(msg)\n\tif err != nil {\n\t\tsendErr(400, fmt.Sprintf(\"Bad Request - %v\", err))\n\t\treturn\n\t}\n\n\t// Check for request limits\n\tif o.cfg.MaxRequestBatch > 0 && batchSize > o.cfg.MaxRequestBatch {\n\t\tsendErr(409, fmt.Sprintf(\"Exceeded MaxRequestBatch of %d\", o.cfg.MaxRequestBatch))\n\t\treturn\n\t}\n\n\tif !expires.IsZero() && o.cfg.MaxRequestExpires > 0 && expires.After(time.Now().Add(o.cfg.MaxRequestExpires)) {\n\t\tsendErr(409, fmt.Sprintf(\"Exceeded MaxRequestExpires of %v\", o.cfg.MaxRequestExpires))\n\t\treturn\n\t}\n\n\tif maxBytes > 0 && o.cfg.MaxRequestMaxBytes > 0 && maxBytes > o.cfg.MaxRequestMaxBytes {\n\t\tsendErr(409, fmt.Sprintf(\"Exceeded MaxRequestMaxBytes of %v\", o.cfg.MaxRequestMaxBytes))\n\t\treturn\n\t}\n\n\tif priorityGroup != nil {\n\t\tif (priorityGroup.MinPending != 0 || priorityGroup.MinAckPending != 0) && o.cfg.PriorityPolicy != PriorityOverflow {\n\t\t\tsendErr(400, \"Bad Request - Not a Overflow Priority consumer\")\n\t\t}\n\n\t\tif priorityGroup.Id != _EMPTY_ && o.cfg.PriorityPolicy != PriorityPinnedClient {\n\t\t\tsendErr(400, \"Bad Request - Not a Pinned Client Priority consumer\")\n\t\t}\n\t}\n\n\tif priorityGroup != nil && o.cfg.PriorityPolicy != PriorityNone {\n\t\tif priorityGroup.Group == _EMPTY_ {\n\t\t\tsendErr(400, \"Bad Request - Priority Group missing\")\n\t\t\treturn\n\t\t}\n\n\t\tfound := false\n\t\tfor _, group := range o.cfg.PriorityGroups {\n\t\t\tif group == priorityGroup.Group {\n\t\t\t\tfound = true\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\tif !found {\n\t\t\tsendErr(400, \"Bad Request - Invalid Priority Group\")\n\t\t\treturn\n\t\t}\n\n\t\tif o.currentPinId != _EMPTY_ {\n\t\t\tif priorityGroup.Id == o.currentPinId {\n\t\t\t\to.setPinnedTimer(priorityGroup.Group)\n\t\t\t} else if priorityGroup.Id != _EMPTY_ {\n\t\t\t\tsendErr(423, \"Nats-Pin-Id mismatch\")\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}\n\n\t// If we have the max number of requests already pending try to expire.\n\tif o.waiting.isFull() {\n\t\t// Try to expire some of the requests.\n\t\t// We do not want to push too hard here so at maximum process once per sec.\n\t\tif time.Since(o.lwqic) > time.Second {\n\t\t\to.processWaiting(false)\n\t\t}\n\t}\n\n\t// If the request is for noWait and we have pending requests already, check if we have room.\n\tif noWait {\n\t\tmsgsPending := o.numPending() + uint64(len(o.rdq))\n\t\t// If no pending at all, decide what to do with request.\n\t\t// If no expires was set then fail.\n\t\tif msgsPending == 0 && expires.IsZero() {\n\t\t\to.waiting.last = time.Now()\n\t\t\tsendErr(404, \"No Messages\")\n\t\t\treturn\n\t\t}\n\t\tif msgsPending > 0 {\n\t\t\t_, _, batchPending, _ := o.processWaiting(false)\n\t\t\tif msgsPending < uint64(batchPending) {\n\t\t\t\to.waiting.last = time.Now()\n\t\t\t\tsendErr(408, \"Requests Pending\")\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t\t// If we are here this should be considered a one-shot situation.\n\t\t// We will wait for expires but will return as soon as we have any messages.\n\t}\n\n\t// If we receive this request though an account export, we need to track that interest subject and account.\n\tacc, interest := trackDownAccountAndInterest(o.acc, reply)\n\n\t// Create a waiting request.\n\twr := wrPool.Get().(*waitingRequest)\n\twr.acc, wr.interest, wr.reply, wr.n, wr.d, wr.noWait, wr.expires, wr.hb, wr.hbt, wr.priorityGroup = acc, interest, reply, batchSize, 0, noWait, expires, hb, hbt, priorityGroup\n\twr.b = maxBytes\n\twr.received = time.Now()\n\n\tif err := o.waiting.add(wr); err != nil {\n\t\tsendErr(409, \"Exceeded MaxWaiting\")\n\t\twr.recycle()\n\t\treturn\n\t}\n\to.signalNewMessages()\n\t// If we are clustered update our followers about this request.\n\tif o.node != nil {\n\t\to.addClusterPendingRequest(wr.reply)\n\t}\n}\n\nfunc trackDownAccountAndInterest(acc *Account, interest string) (*Account, string) {\n\tfor strings.HasPrefix(interest, replyPrefix) {\n\t\toa := acc\n\t\toa.mu.RLock()\n\t\tif oa.exports.responses == nil {\n\t\t\toa.mu.RUnlock()\n\t\t\tbreak\n\t\t}\n\t\tsi := oa.exports.responses[interest]\n\t\tif si == nil {\n\t\t\toa.mu.RUnlock()\n\t\t\tbreak\n\t\t}\n\t\tacc, interest = si.acc, si.to\n\t\toa.mu.RUnlock()\n\t}\n\treturn acc, interest\n}\n\n// Return current delivery count for a given sequence.\nfunc (o *consumer) deliveryCount(seq uint64) uint64 {\n\tif o.rdc == nil {\n\t\treturn 1\n\t}\n\tif dc := o.rdc[seq]; dc >= 1 {\n\t\treturn dc\n\t}\n\treturn 1\n}\n\n// Increase the delivery count for this message.\n// ONLY used on redelivery semantics.\n// Lock should be held.\nfunc (o *consumer) incDeliveryCount(sseq uint64) uint64 {\n\tif o.rdc == nil {\n\t\to.rdc = make(map[uint64]uint64)\n\t}\n\to.rdc[sseq] += 1\n\treturn o.rdc[sseq] + 1\n}\n\n// Used if we have to adjust on failed delivery or bad lookups.\n// Those failed attempts should not increase deliver count.\n// Lock should be held.\nfunc (o *consumer) decDeliveryCount(sseq uint64) {\n\tif o.rdc == nil {\n\t\to.rdc = make(map[uint64]uint64)\n\t}\n\to.rdc[sseq] -= 1\n}\n\n// send a delivery exceeded advisory.\nfunc (o *consumer) notifyDeliveryExceeded(sseq, dc uint64) {\n\te := JSConsumerDeliveryExceededAdvisory{\n\t\tTypedEvent: TypedEvent{\n\t\t\tType: JSConsumerDeliveryExceededAdvisoryType,\n\t\t\tID:   nuid.Next(),\n\t\t\tTime: time.Now().UTC(),\n\t\t},\n\t\tStream:     o.stream,\n\t\tConsumer:   o.name,\n\t\tStreamSeq:  sseq,\n\t\tDeliveries: dc,\n\t\tDomain:     o.srv.getOpts().JetStreamDomain,\n\t}\n\n\to.sendAdvisory(o.deliveryExcEventT, e)\n}\n\n// Check if the candidate subject matches a filter if its present.\n// Lock should be held.\nfunc (o *consumer) isFilteredMatch(subj string) bool {\n\t// No filter is automatic match.\n\tif o.subjf == nil {\n\t\treturn true\n\t}\n\tfor _, filter := range o.subjf {\n\t\tif !filter.hasWildcard && subj == filter.subject {\n\t\t\treturn true\n\t\t}\n\t}\n\t// It's quicker to first check for non-wildcard filters, then\n\t// iterate again to check for subset match.\n\ttsa := [32]string{}\n\ttts := tokenizeSubjectIntoSlice(tsa[:0], subj)\n\tfor _, filter := range o.subjf {\n\t\tif isSubsetMatchTokenized(tts, filter.tokenizedSubject) {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\n// Check if the candidate filter subject is equal to or a subset match\n// of one of the filter subjects.\n// Lock should be held.\nfunc (o *consumer) isEqualOrSubsetMatch(subj string) bool {\n\tfor _, filter := range o.subjf {\n\t\tif !filter.hasWildcard && subj == filter.subject {\n\t\t\treturn true\n\t\t}\n\t}\n\ttsa := [32]string{}\n\ttts := tokenizeSubjectIntoSlice(tsa[:0], subj)\n\tfor _, filter := range o.subjf {\n\t\tif isSubsetMatchTokenized(filter.tokenizedSubject, tts) {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\nvar (\n\terrMaxAckPending = errors.New(\"max ack pending reached\")\n\terrBadConsumer   = errors.New(\"consumer not valid\")\n\terrNoInterest    = errors.New(\"consumer requires interest for delivery subject when ephemeral\")\n)\n\n// Get next available message from underlying store.\n// Is partition aware and redeliver aware.\n// Lock should be held.\nfunc (o *consumer) getNextMsg() (*jsPubMsg, uint64, error) {\n\tif o.mset == nil || o.mset.store == nil {\n\t\treturn nil, 0, errBadConsumer\n\t}\n\t// Process redelivered messages before looking at possibly \"skip list\" (deliver last per subject)\n\tif o.hasRedeliveries() {\n\t\tvar seq, dc uint64\n\t\tfor seq = o.getNextToRedeliver(); seq > 0; seq = o.getNextToRedeliver() {\n\t\t\tdc = o.incDeliveryCount(seq)\n\t\t\tif o.maxdc > 0 && dc > o.maxdc {\n\t\t\t\t// Only send once\n\t\t\t\tif dc == o.maxdc+1 {\n\t\t\t\t\to.notifyDeliveryExceeded(seq, dc-1)\n\t\t\t\t}\n\t\t\t\t// Make sure to remove from pending.\n\t\t\t\tif p, ok := o.pending[seq]; ok && p != nil {\n\t\t\t\t\tdelete(o.pending, seq)\n\t\t\t\t\to.updateDelivered(p.Sequence, seq, dc, p.Timestamp)\n\t\t\t\t}\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tpmsg := getJSPubMsgFromPool()\n\t\t\tsm, err := o.mset.store.LoadMsg(seq, &pmsg.StoreMsg)\n\t\t\tif sm == nil || err != nil {\n\t\t\t\tpmsg.returnToPool()\n\t\t\t\tpmsg, dc = nil, 0\n\t\t\t\t// Adjust back deliver count.\n\t\t\t\to.decDeliveryCount(seq)\n\t\t\t}\n\t\t\t// Message was scheduled for redelivery but was removed in the meantime.\n\t\t\tif err == ErrStoreMsgNotFound || err == errDeletedMsg {\n\t\t\t\t// This is a race condition where the message is still in o.pending and\n\t\t\t\t// scheduled for redelivery, but it has been removed from the stream.\n\t\t\t\t// o.processTerm is called in a goroutine so could run after we get here.\n\t\t\t\t// That will correct the pending state and delivery/ack floors, so just skip here.\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\treturn pmsg, dc, err\n\t\t}\n\t}\n\n\t// Check if we have max pending.\n\tif o.maxp > 0 && len(o.pending) >= o.maxp {\n\t\t// maxp only set when ack policy != AckNone and user set MaxAckPending\n\t\t// Stall if we have hit max pending.\n\t\treturn nil, 0, errMaxAckPending\n\t}\n\n\tif o.hasSkipListPending() {\n\t\tseq := o.lss.seqs[0]\n\t\tif len(o.lss.seqs) == 1 {\n\t\t\to.sseq = o.lss.resume\n\t\t\to.lss = nil\n\t\t\to.updateSkipped(o.sseq)\n\t\t} else {\n\t\t\to.lss.seqs = o.lss.seqs[1:]\n\t\t}\n\t\tpmsg := getJSPubMsgFromPool()\n\t\tsm, err := o.mset.store.LoadMsg(seq, &pmsg.StoreMsg)\n\t\tif sm == nil || err != nil {\n\t\t\tpmsg.returnToPool()\n\t\t}\n\t\to.sseq++\n\t\treturn pmsg, 1, err\n\t}\n\n\t// Hold onto this since we release the lock.\n\tstore := o.mset.store\n\n\tvar sseq uint64\n\tvar err error\n\tvar sm *StoreMsg\n\tvar pmsg = getJSPubMsgFromPool()\n\n\t// Grab next message applicable to us.\n\tfilters, subjf, fseq := o.filters, o.subjf, o.sseq\n\t// Check if we are multi-filtered or not.\n\tif filters != nil {\n\t\tsm, sseq, err = store.LoadNextMsgMulti(filters, fseq, &pmsg.StoreMsg)\n\t} else if len(subjf) > 0 { // Means single filtered subject since o.filters means > 1.\n\t\tfilter, wc := subjf[0].subject, subjf[0].hasWildcard\n\t\tsm, sseq, err = store.LoadNextMsg(filter, wc, fseq, &pmsg.StoreMsg)\n\t} else {\n\t\t// No filter here.\n\t\tsm, sseq, err = store.LoadNextMsg(_EMPTY_, false, fseq, &pmsg.StoreMsg)\n\t}\n\tif sm == nil {\n\t\tpmsg.returnToPool()\n\t\tpmsg = nil\n\t}\n\t// Check if we should move our o.sseq.\n\tif sseq >= o.sseq {\n\t\t// If we are moving step by step then sseq == o.sseq.\n\t\t// If we have jumped we should update skipped for other replicas.\n\t\tif sseq != o.sseq && err == ErrStoreEOF {\n\t\t\to.updateSkipped(sseq + 1)\n\t\t}\n\t\to.sseq = sseq + 1\n\t}\n\treturn pmsg, 1, err\n}\n\n// Will check for expiration and lack of interest on waiting requests.\n// Will also do any heartbeats and return the next expiration or HB interval.\nfunc (o *consumer) processWaiting(eos bool) (int, int, int, time.Time) {\n\tvar fexp time.Time\n\tif o.srv == nil || o.waiting.isEmpty() {\n\t\treturn 0, 0, 0, fexp\n\t}\n\t// Mark our last check time.\n\to.lwqic = time.Now()\n\n\tvar expired, brp int\n\ts, now := o.srv, time.Now()\n\n\twq := o.waiting\n\tremove := func(pre, wr *waitingRequest) *waitingRequest {\n\t\texpired++\n\t\tif o.node != nil {\n\t\t\to.removeClusterPendingRequest(wr.reply)\n\t\t}\n\t\tnext := wr.next\n\t\twq.remove(pre, wr)\n\t\twr.recycle()\n\t\treturn next\n\t}\n\n\tvar pre *waitingRequest\n\tfor wr := wq.head; wr != nil; {\n\t\t// Check expiration.\n\t\tif (eos && wr.noWait && wr.d > 0) || (!wr.expires.IsZero() && now.After(wr.expires)) {\n\t\t\thdr := fmt.Appendf(nil, \"NATS/1.0 408 Request Timeout\\r\\n%s: %d\\r\\n%s: %d\\r\\n\\r\\n\", JSPullRequestPendingMsgs, wr.n, JSPullRequestPendingBytes, wr.b)\n\t\t\to.outq.send(newJSPubMsg(wr.reply, _EMPTY_, _EMPTY_, hdr, nil, nil, 0))\n\t\t\twr = remove(pre, wr)\n\t\t\tcontinue\n\t\t}\n\t\t// Now check interest.\n\t\tinterest := wr.acc.sl.HasInterest(wr.interest)\n\t\tif !interest && (s.leafNodeEnabled || s.gateway.enabled) {\n\t\t\t// If we are here check on gateways and leaf nodes (as they can mask gateways on the other end).\n\t\t\t// If we have interest or the request is too young break and do not expire.\n\t\t\tif time.Since(wr.received) < defaultGatewayRecentSubExpiration {\n\t\t\t\tinterest = true\n\t\t\t} else if s.gateway.enabled && s.hasGatewayInterest(wr.acc.Name, wr.interest) {\n\t\t\t\tinterest = true\n\t\t\t}\n\t\t}\n\t\t// Check if we have interest.\n\t\tif !interest {\n\t\t\t// No more interest here so go ahead and remove this one from our list.\n\t\t\twr = remove(pre, wr)\n\t\t\tcontinue\n\t\t}\n\n\t\t// If interest, update batch pending requests counter and update fexp timer.\n\t\tbrp += wr.n\n\t\tif !wr.hbt.IsZero() {\n\t\t\tif now.After(wr.hbt) {\n\t\t\t\t// Fire off a heartbeat here.\n\t\t\t\to.sendIdleHeartbeat(wr.reply)\n\t\t\t\t// Update next HB.\n\t\t\t\twr.hbt = now.Add(wr.hb)\n\t\t\t}\n\t\t\tif fexp.IsZero() || wr.hbt.Before(fexp) {\n\t\t\t\tfexp = wr.hbt\n\t\t\t}\n\t\t}\n\t\tif !wr.expires.IsZero() && (fexp.IsZero() || wr.expires.Before(fexp)) {\n\t\t\tfexp = wr.expires\n\t\t}\n\t\t// Update pre and wr here.\n\t\tpre = wr\n\t\twr = wr.next\n\t}\n\n\treturn expired, wq.len(), brp, fexp\n}\n\n// Will check to make sure those waiting still have registered interest.\nfunc (o *consumer) checkWaitingForInterest() bool {\n\to.processWaiting(true)\n\treturn o.waiting.len() > 0\n}\n\n// Lock should be held.\nfunc (o *consumer) hbTimer() (time.Duration, *time.Timer) {\n\tif o.cfg.Heartbeat == 0 {\n\t\treturn 0, nil\n\t}\n\treturn o.cfg.Heartbeat, time.NewTimer(o.cfg.Heartbeat)\n}\n\n// Check here for conditions when our ack floor may have drifted below the streams first sequence.\n// In general this is accounted for in normal operations, but if the consumer misses the signal from\n// the stream it will not clear the message and move the ack state.\n// Should only be called from consumer leader.\nfunc (o *consumer) checkAckFloor() {\n\to.mu.RLock()\n\tmset, closed, asflr, numPending := o.mset, o.closed, o.asflr, len(o.pending)\n\to.mu.RUnlock()\n\n\tif asflr == 0 || closed || mset == nil {\n\t\treturn\n\t}\n\n\tvar ss StreamState\n\tmset.store.FastState(&ss)\n\n\t// If our floor is equal or greater that is normal and nothing for us to do.\n\tif ss.FirstSeq == 0 || asflr >= ss.FirstSeq-1 {\n\t\treturn\n\t}\n\n\t// Check which linear space is less to walk.\n\tif ss.FirstSeq-asflr-1 < uint64(numPending) {\n\t\t// Process all messages that no longer exist.\n\t\tfor seq := asflr + 1; seq < ss.FirstSeq; seq++ {\n\t\t\t// Check if this message was pending.\n\t\t\to.mu.RLock()\n\t\t\tp, isPending := o.pending[seq]\n\t\t\trdc := o.deliveryCount(seq)\n\t\t\to.mu.RUnlock()\n\t\t\t// If it was pending for us, get rid of it.\n\t\t\tif isPending {\n\t\t\t\to.processTerm(seq, p.Sequence, rdc, ackTermLimitsReason, _EMPTY_)\n\t\t\t}\n\t\t}\n\t} else if numPending > 0 {\n\t\t// here it is shorter to walk pending.\n\t\t// toTerm is seq, dseq, rcd for each entry.\n\t\ttoTerm := make([]uint64, 0, numPending*3)\n\t\to.mu.RLock()\n\t\tfor seq, p := range o.pending {\n\t\t\tif seq < ss.FirstSeq {\n\t\t\t\tvar dseq uint64 = 1\n\t\t\t\tif p != nil {\n\t\t\t\t\tdseq = p.Sequence\n\t\t\t\t}\n\t\t\t\trdc := o.deliveryCount(seq)\n\t\t\t\ttoTerm = append(toTerm, seq, dseq, rdc)\n\t\t\t}\n\t\t}\n\t\to.mu.RUnlock()\n\n\t\tfor i := 0; i < len(toTerm); i += 3 {\n\t\t\tseq, dseq, rdc := toTerm[i], toTerm[i+1], toTerm[i+2]\n\t\t\to.processTerm(seq, dseq, rdc, ackTermLimitsReason, _EMPTY_)\n\t\t}\n\t}\n\n\t// Do one final check here.\n\to.mu.Lock()\n\tdefer o.mu.Unlock()\n\n\t// If we are closed do not change anything and simply return.\n\tif o.closed {\n\t\treturn\n\t}\n\n\t// If we are here, and this should be rare, we still are off with our ack floor.\n\t// We will make sure we are not doing un-necessary work here if only off by a bit\n\t// since this could be normal for a high activity wq or stream.\n\t// We will set it explicitly to 1 behind our current lowest in pending, or if\n\t// pending is empty, to our current delivered -1.\n\tconst minOffThreshold = 50\n\tif ss.FirstSeq >= minOffThreshold && o.asflr < ss.FirstSeq-minOffThreshold {\n\t\tvar psseq, pdseq uint64\n\t\tfor seq, p := range o.pending {\n\t\t\tif psseq == 0 || seq < psseq {\n\t\t\t\tpsseq, pdseq = seq, p.Sequence\n\t\t\t}\n\t\t}\n\t\t// If we still have none, set to current delivered -1.\n\t\tif psseq == 0 {\n\t\t\tpsseq, pdseq = o.sseq-1, o.dseq-1\n\t\t\t// If still not adjusted.\n\t\t\tif psseq < ss.FirstSeq-1 {\n\t\t\t\tpsseq = ss.FirstSeq - 1\n\t\t\t}\n\t\t} else {\n\t\t\t// Since this was set via the pending, we should not include\n\t\t\t// it directly but set floors to -1.\n\t\t\tpsseq, pdseq = psseq-1, pdseq-1\n\t\t}\n\t\to.asflr, o.adflr = psseq, pdseq\n\t}\n}\n\nfunc (o *consumer) processInboundAcks(qch chan struct{}) {\n\t// Grab the server lock to watch for server quit.\n\to.mu.RLock()\n\ts, mset := o.srv, o.mset\n\thasInactiveThresh := o.cfg.InactiveThreshold > 0\n\n\to.mu.RUnlock()\n\n\tif s == nil || mset == nil {\n\t\treturn\n\t}\n\n\t// We will check this on entry and periodically.\n\to.checkAckFloor()\n\n\t// How often we will check for ack floor drift.\n\t// Spread these out for large numbers on a server restart.\n\tdelta := time.Duration(rand.Int63n(int64(time.Minute)))\n\tticker := time.NewTicker(time.Minute + delta)\n\tdefer ticker.Stop()\n\n\tfor {\n\t\tselect {\n\t\tcase <-o.ackMsgs.ch:\n\t\t\tacks := o.ackMsgs.pop()\n\t\t\tfor _, ack := range acks {\n\t\t\t\to.processAck(ack.subject, ack.reply, ack.hdr, ack.msg)\n\t\t\t\tack.returnToPool()\n\t\t\t}\n\t\t\to.ackMsgs.recycle(&acks)\n\t\t\t// If we have an inactiveThreshold set, mark our activity.\n\t\t\tif hasInactiveThresh {\n\t\t\t\to.suppressDeletion()\n\t\t\t}\n\t\tcase <-ticker.C:\n\t\t\to.checkAckFloor()\n\t\tcase <-qch:\n\t\t\treturn\n\t\tcase <-s.quitCh:\n\t\t\treturn\n\t\t}\n\t}\n}\n\n// Process inbound next message requests.\nfunc (o *consumer) processInboundNextMsgReqs(qch chan struct{}) {\n\t// Grab the server lock to watch for server quit.\n\to.mu.RLock()\n\ts := o.srv\n\to.mu.RUnlock()\n\n\tfor {\n\t\tselect {\n\t\tcase <-o.nextMsgReqs.ch:\n\t\t\treqs := o.nextMsgReqs.pop()\n\t\t\tfor _, req := range reqs {\n\t\t\t\to.processNextMsgRequest(req.reply, req.msg)\n\t\t\t\treq.returnToPool()\n\t\t\t}\n\t\t\to.nextMsgReqs.recycle(&reqs)\n\t\tcase <-qch:\n\t\t\treturn\n\t\tcase <-s.quitCh:\n\t\t\treturn\n\t\t}\n\t}\n}\n\n// Suppress auto cleanup on ack activity of any kind.\nfunc (o *consumer) suppressDeletion() {\n\to.mu.Lock()\n\tdefer o.mu.Unlock()\n\n\tif o.closed {\n\t\treturn\n\t}\n\n\tif o.isPushMode() && o.dtmr != nil {\n\t\t// if dtmr is not nil we have started the countdown, simply reset to threshold.\n\t\to.dtmr.Reset(o.dthresh)\n\t} else if o.isPullMode() && o.waiting != nil {\n\t\t// Pull mode always has timer running, just update last on waiting queue.\n\t\to.waiting.last = time.Now()\n\t}\n}\n\n// loopAndGatherMsgs waits for messages for the consumer. qch is the quit channel,\n// upch is the unpause channel which fires when the PauseUntil deadline is reached.\nfunc (o *consumer) loopAndGatherMsgs(qch chan struct{}) {\n\t// On startup check to see if we are in a reply situation where replay policy is not instant.\n\tvar (\n\t\tlts  int64 // last time stamp seen, used for replay.\n\t\tlseq uint64\n\t)\n\n\to.mu.RLock()\n\tmset := o.mset\n\tgetLSeq := o.replay\n\to.mu.RUnlock()\n\t// consumer is closed when mset is set to nil.\n\tif mset == nil {\n\t\treturn\n\t}\n\tif getLSeq {\n\t\tlseq = mset.state().LastSeq\n\t}\n\n\to.mu.Lock()\n\ts := o.srv\n\t// need to check again if consumer is closed\n\tif o.mset == nil {\n\t\to.mu.Unlock()\n\t\treturn\n\t}\n\t// For idle heartbeat support.\n\tvar hbc <-chan time.Time\n\thbd, hb := o.hbTimer()\n\tif hb != nil {\n\t\thbc = hb.C\n\t}\n\t// Interest changes.\n\tinch := o.inch\n\to.mu.Unlock()\n\n\t// Grab the stream's retention policy and name\n\tmset.cfgMu.RLock()\n\tstream, rp := mset.cfg.Name, mset.cfg.Retention\n\tmset.cfgMu.RUnlock()\n\n\tvar err error\n\n\t// Deliver all the msgs we have now, once done or on a condition, we wait for new ones.\n\tfor {\n\t\tvar (\n\t\t\tpmsg     *jsPubMsg\n\t\t\tdc       uint64\n\t\t\tdsubj    string\n\t\t\tackReply string\n\t\t\tdelay    time.Duration\n\t\t\tsz       int\n\t\t\twrn, wrb int\n\t\t\twrNoWait bool\n\t\t)\n\n\t\to.mu.Lock()\n\n\t\t// consumer is closed when mset is set to nil.\n\t\tif o.closed || o.mset == nil {\n\t\t\to.mu.Unlock()\n\t\t\treturn\n\t\t}\n\n\t\t// Clear last error.\n\t\terr = nil\n\n\t\t// If the consumer is paused then stop sending.\n\t\tif o.cfg.PauseUntil != nil && !o.cfg.PauseUntil.IsZero() && time.Now().Before(*o.cfg.PauseUntil) {\n\t\t\t// If the consumer is paused and we haven't reached the deadline yet then\n\t\t\t// go back to waiting.\n\t\t\tgoto waitForMsgs\n\t\t}\n\n\t\t// If we are in push mode and not active or under flowcontrol let's stop sending.\n\t\tif o.isPushMode() {\n\t\t\tif !o.active || (o.maxpb > 0 && o.pbytes > o.maxpb) {\n\t\t\t\tgoto waitForMsgs\n\t\t\t}\n\t\t} else if o.waiting.isEmpty() {\n\t\t\t// If we are in pull mode and no one is waiting already break and wait.\n\t\t\tgoto waitForMsgs\n\t\t}\n\n\t\t// Grab our next msg.\n\t\tpmsg, dc, err = o.getNextMsg()\n\n\t\t// We can release the lock now under getNextMsg so need to check this condition again here.\n\t\tif o.closed || o.mset == nil {\n\t\t\to.mu.Unlock()\n\t\t\treturn\n\t\t}\n\n\t\t// On error either wait or return.\n\t\tif err != nil || pmsg == nil {\n\t\t\t// On EOF we can optionally fast sync num pending state.\n\t\t\tif err == ErrStoreEOF {\n\t\t\t\to.checkNumPendingOnEOF()\n\t\t\t}\n\t\t\tif err == ErrStoreMsgNotFound || err == errDeletedMsg || err == ErrStoreEOF || err == errMaxAckPending {\n\t\t\t\tgoto waitForMsgs\n\t\t\t} else if err == errPartialCache {\n\t\t\t\ts.Warnf(\"Unexpected partial cache error looking up message for consumer '%s > %s > %s'\",\n\t\t\t\t\to.mset.acc, stream, o.cfg.Name)\n\t\t\t\tgoto waitForMsgs\n\n\t\t\t} else {\n\t\t\t\ts.Errorf(\"Received an error looking up message for consumer '%s > %s > %s': %v\",\n\t\t\t\t\to.mset.acc, stream, o.cfg.Name, err)\n\t\t\t\tgoto waitForMsgs\n\t\t\t}\n\t\t}\n\n\t\t// Update our cached num pending here first.\n\t\tif dc == 1 {\n\t\t\to.npc--\n\t\t}\n\t\t// Pre-calculate ackReply\n\t\tackReply = o.ackReply(pmsg.seq, o.dseq, dc, pmsg.ts, o.numPending())\n\n\t\t// If headers only do not send msg payload.\n\t\t// Add in msg size itself as header.\n\t\tif o.cfg.HeadersOnly {\n\t\t\tconvertToHeadersOnly(pmsg)\n\t\t}\n\t\t// Calculate payload size. This can be calculated on client side.\n\t\t// We do not include transport subject here since not generally known on client.\n\t\tsz = len(pmsg.subj) + len(ackReply) + len(pmsg.hdr) + len(pmsg.msg)\n\n\t\tif o.isPushMode() {\n\t\t\tdsubj = o.dsubj\n\t\t} else if wr := o.nextWaiting(sz); wr != nil {\n\t\t\twrn, wrb, wrNoWait = wr.n, wr.b, wr.noWait\n\t\t\tdsubj = wr.reply\n\t\t\tif o.cfg.PriorityPolicy == PriorityPinnedClient {\n\t\t\t\t// FIXME(jrm): Can we make this prettier?\n\t\t\t\tif len(pmsg.hdr) == 0 {\n\t\t\t\t\tpmsg.hdr = genHeader(pmsg.hdr, JSPullRequestNatsPinId, o.currentPinId)\n\t\t\t\t\tpmsg.buf = append(pmsg.hdr, pmsg.msg...)\n\t\t\t\t} else {\n\t\t\t\t\tpmsg.hdr = genHeader(pmsg.hdr, JSPullRequestNatsPinId, o.currentPinId)\n\t\t\t\t\tbufLen := len(pmsg.hdr) + len(pmsg.msg)\n\t\t\t\t\tpmsg.buf = make([]byte, bufLen)\n\t\t\t\t\tpmsg.buf = append(pmsg.hdr, pmsg.msg...)\n\t\t\t\t}\n\n\t\t\t\tsz = len(pmsg.subj) + len(ackReply) + len(pmsg.hdr) + len(pmsg.msg)\n\n\t\t\t}\n\t\t\tif done := wr.recycleIfDone(); done && o.node != nil {\n\t\t\t\to.removeClusterPendingRequest(dsubj)\n\t\t\t} else if !done && wr.hb > 0 {\n\t\t\t\twr.hbt = time.Now().Add(wr.hb)\n\t\t\t}\n\t\t} else {\n\t\t\t// We will redo this one as long as this is not a redelivery.\n\t\t\t// Need to also test that this is not going backwards since if\n\t\t\t// we fail to deliver we can end up here from rdq but we do not\n\t\t\t// want to decrement o.sseq if that is the case.\n\t\t\tif dc == 1 && pmsg.seq == o.sseq-1 {\n\t\t\t\to.sseq--\n\t\t\t\to.npc++\n\t\t\t} else if !o.onRedeliverQueue(pmsg.seq) {\n\t\t\t\t// We are not on the rdq so decrement the delivery count\n\t\t\t\t// and add it back.\n\t\t\t\to.decDeliveryCount(pmsg.seq)\n\t\t\t\to.addToRedeliverQueue(pmsg.seq)\n\t\t\t}\n\t\t\tpmsg.returnToPool()\n\t\t\tgoto waitForMsgs\n\t\t}\n\n\t\t// If we are in a replay scenario and have not caught up check if we need to delay here.\n\t\tif o.replay && lts > 0 {\n\t\t\tif delay = time.Duration(pmsg.ts - lts); delay > time.Millisecond {\n\t\t\t\to.mu.Unlock()\n\t\t\t\tselect {\n\t\t\t\tcase <-qch:\n\t\t\t\t\tpmsg.returnToPool()\n\t\t\t\t\treturn\n\t\t\t\tcase <-time.After(delay):\n\t\t\t\t}\n\t\t\t\to.mu.Lock()\n\t\t\t}\n\t\t}\n\n\t\t// Track this regardless.\n\t\tlts = pmsg.ts\n\n\t\t// If we have a rate limit set make sure we check that here.\n\t\tif o.rlimit != nil {\n\t\t\tnow := time.Now()\n\t\t\tr := o.rlimit.ReserveN(now, sz)\n\t\t\tdelay := r.DelayFrom(now)\n\t\t\tif delay > 0 {\n\t\t\t\to.mu.Unlock()\n\t\t\t\tselect {\n\t\t\t\tcase <-qch:\n\t\t\t\t\tpmsg.returnToPool()\n\t\t\t\t\treturn\n\t\t\t\tcase <-time.After(delay):\n\t\t\t\t}\n\t\t\t\to.mu.Lock()\n\t\t\t}\n\t\t}\n\n\t\t// Do actual delivery.\n\t\to.deliverMsg(dsubj, ackReply, pmsg, dc, rp, wrNoWait)\n\n\t\t// If given request fulfilled batch size, but there are still pending bytes, send information about it.\n\t\tif wrn <= 0 && wrb > 0 {\n\t\t\tmsg := fmt.Appendf(nil, JsPullRequestRemainingBytesT, JSPullRequestPendingMsgs, wrn, JSPullRequestPendingBytes, wrb)\n\t\t\to.outq.send(newJSPubMsg(dsubj, _EMPTY_, _EMPTY_, msg, nil, nil, 0))\n\t\t}\n\t\t// Reset our idle heartbeat timer if set.\n\t\tif hb != nil {\n\t\t\thb.Reset(hbd)\n\t\t}\n\n\t\to.mu.Unlock()\n\t\tcontinue\n\n\twaitForMsgs:\n\t\t// If we were in a replay state check to see if we are caught up. If so clear.\n\t\tif o.replay && o.sseq > lseq {\n\t\t\to.replay = false\n\t\t}\n\n\t\t// Make sure to process any expired requests that are pending.\n\t\tvar wrExp <-chan time.Time\n\t\tif o.isPullMode() {\n\t\t\t// Dont expire oneshots if we are here because of max ack pending limit.\n\t\t\t_, _, _, fexp := o.processWaiting(err != errMaxAckPending)\n\t\t\tif !fexp.IsZero() {\n\t\t\t\texpires := time.Until(fexp)\n\t\t\t\tif expires <= 0 {\n\t\t\t\t\texpires = time.Millisecond\n\t\t\t\t}\n\t\t\t\twrExp = time.NewTimer(expires).C\n\t\t\t}\n\t\t}\n\n\t\t// We will wait here for new messages to arrive.\n\t\tmch, odsubj := o.mch, o.cfg.DeliverSubject\n\t\to.mu.Unlock()\n\n\t\tselect {\n\t\tcase <-mch:\n\t\t\t// Messages are waiting.\n\t\tcase interest := <-inch:\n\t\t\t// inch can be nil on pull-based, but then this will\n\t\t\t// just block and not fire.\n\t\t\to.updateDeliveryInterest(interest)\n\t\tcase <-qch:\n\t\t\treturn\n\t\tcase <-wrExp:\n\t\t\to.mu.Lock()\n\t\t\to.processWaiting(true)\n\t\t\to.mu.Unlock()\n\t\tcase <-hbc:\n\t\t\tif o.isActive() {\n\t\t\t\to.mu.RLock()\n\t\t\t\to.sendIdleHeartbeat(odsubj)\n\t\t\t\to.mu.RUnlock()\n\t\t\t}\n\t\t\t// Reset our idle heartbeat timer.\n\t\t\thb.Reset(hbd)\n\t\t}\n\t}\n}\n\n// Lock should be held.\nfunc (o *consumer) sendIdleHeartbeat(subj string) {\n\tconst t = \"NATS/1.0 100 Idle Heartbeat\\r\\n%s: %d\\r\\n%s: %d\\r\\n\\r\\n\"\n\tsseq, dseq := o.sseq-1, o.dseq-1\n\thdr := fmt.Appendf(nil, t, JSLastConsumerSeq, dseq, JSLastStreamSeq, sseq)\n\tif fcp := o.fcid; fcp != _EMPTY_ {\n\t\t// Add in that we are stalled on flow control here.\n\t\taddOn := fmt.Appendf(nil, \"%s: %s\\r\\n\\r\\n\", JSConsumerStalled, fcp)\n\t\thdr = append(hdr[:len(hdr)-LEN_CR_LF], []byte(addOn)...)\n\t}\n\to.outq.send(newJSPubMsg(subj, _EMPTY_, _EMPTY_, hdr, nil, nil, 0))\n}\n\nfunc (o *consumer) ackReply(sseq, dseq, dc uint64, ts int64, pending uint64) string {\n\treturn fmt.Sprintf(o.ackReplyT, dc, sseq, dseq, ts, pending)\n}\n\n// Used mostly for testing. Sets max pending bytes for flow control setups.\nfunc (o *consumer) setMaxPendingBytes(limit int) {\n\to.pblimit = limit\n\to.maxpb = limit / 16\n\tif o.maxpb == 0 {\n\t\to.maxpb = 1\n\t}\n}\n\n// Does some sanity checks to see if we should re-calculate.\n// Since there is a race when decrementing when there is contention at the beginning of the stream.\n// The race is a getNextMsg skips a deleted msg, and then the decStreamPending call fires.\n// This does some quick sanity checks to see if we should re-calculate num pending.\n// Lock should be held.\nfunc (o *consumer) checkNumPending() uint64 {\n\tif o.mset != nil {\n\t\tvar state StreamState\n\t\to.mset.store.FastState(&state)\n\t\tnpc := o.numPending()\n\t\tif o.sseq > state.LastSeq && npc > 0 || npc > state.Msgs {\n\t\t\t// Re-calculate.\n\t\t\to.streamNumPending()\n\t\t}\n\t}\n\treturn o.numPending()\n}\n\n// Lock should be held.\nfunc (o *consumer) numPending() uint64 {\n\tif o.npc < 0 {\n\t\treturn 0\n\t}\n\treturn uint64(o.npc)\n}\n\n// This will do a quick sanity check on num pending when we encounter\n// and EOF in the loop and gather.\n// Lock should be held.\nfunc (o *consumer) checkNumPendingOnEOF() {\n\tif o.mset == nil {\n\t\treturn\n\t}\n\tvar state StreamState\n\to.mset.store.FastState(&state)\n\tif o.sseq > state.LastSeq && o.npc != 0 {\n\t\t// We know here we can reset our running state for num pending.\n\t\to.npc, o.npf = 0, state.LastSeq\n\t}\n}\n\n// Call into streamNumPending after acquiring the consumer lock.\nfunc (o *consumer) streamNumPendingLocked() uint64 {\n\to.mu.Lock()\n\tdefer o.mu.Unlock()\n\treturn o.streamNumPending()\n}\n\n// Will force a set from the stream store of num pending.\n// Depends on delivery policy, for last per subject we calculate differently.\n// Lock should be held.\nfunc (o *consumer) streamNumPending() uint64 {\n\tif o.mset == nil || o.mset.store == nil {\n\t\to.npc, o.npf = 0, 0\n\t\treturn 0\n\t}\n\tnpc, npf := o.calculateNumPending()\n\to.npc, o.npf = int64(npc), npf\n\treturn o.numPending()\n}\n\n// Will calculate num pending but only requires a read lock.\n// Depends on delivery policy, for last per subject we calculate differently.\n// At least RLock should be held.\nfunc (o *consumer) calculateNumPending() (npc, npf uint64) {\n\tif o.mset == nil || o.mset.store == nil {\n\t\treturn 0, 0\n\t}\n\n\tisLastPerSubject := o.cfg.DeliverPolicy == DeliverLastPerSubject\n\tfilters, subjf := o.filters, o.subjf\n\n\tif filters != nil {\n\t\treturn o.mset.store.NumPendingMulti(o.sseq, filters, isLastPerSubject)\n\t} else if len(subjf) > 0 {\n\t\tfilter := subjf[0].subject\n\t\treturn o.mset.store.NumPending(o.sseq, filter, isLastPerSubject)\n\t}\n\treturn o.mset.store.NumPending(o.sseq, _EMPTY_, isLastPerSubject)\n}\n\nfunc convertToHeadersOnly(pmsg *jsPubMsg) {\n\t// If headers only do not send msg payload.\n\t// Add in msg size itself as header.\n\thdr, msg := pmsg.hdr, pmsg.msg\n\tvar bb bytes.Buffer\n\tif len(hdr) == 0 {\n\t\tbb.WriteString(hdrLine)\n\t} else {\n\t\tbb.Write(hdr)\n\t\tbb.Truncate(len(hdr) - LEN_CR_LF)\n\t}\n\tbb.WriteString(JSMsgSize)\n\tbb.WriteString(\": \")\n\tbb.WriteString(strconv.FormatInt(int64(len(msg)), 10))\n\tbb.WriteString(CR_LF)\n\tbb.WriteString(CR_LF)\n\t// Replace underlying buf which we can use directly when we send.\n\t// TODO(dlc) - Probably just use directly when forming bytes.Buffer?\n\tpmsg.buf = pmsg.buf[:0]\n\tpmsg.buf = append(pmsg.buf, bb.Bytes()...)\n\t// Replace with new header.\n\tpmsg.hdr = pmsg.buf\n\t// Cancel msg payload\n\tpmsg.msg = nil\n}\n\n// Deliver a msg to the consumer.\n// Lock should be held and o.mset validated to be non-nil.\nfunc (o *consumer) deliverMsg(dsubj, ackReply string, pmsg *jsPubMsg, dc uint64, rp RetentionPolicy, wrNoWait bool) {\n\tif o.mset == nil {\n\t\tpmsg.returnToPool()\n\t\treturn\n\t}\n\n\tdseq := o.dseq\n\to.dseq++\n\n\tpmsg.dsubj, pmsg.reply, pmsg.o = dsubj, ackReply, o\n\tpsz := pmsg.size()\n\n\tif o.maxpb > 0 {\n\t\to.pbytes += psz\n\t}\n\n\tmset := o.mset\n\tap := o.cfg.AckPolicy\n\n\t// Cant touch pmsg after this sending so capture what we need.\n\tseq, ts := pmsg.seq, pmsg.ts\n\n\t// Update delivered first.\n\to.updateDelivered(dseq, seq, dc, ts)\n\n\tif ap == AckExplicit || ap == AckAll {\n\t\to.trackPending(seq, dseq)\n\t} else if ap == AckNone {\n\t\to.adflr = dseq\n\t\to.asflr = seq\n\t}\n\n\t// Send message.\n\t// If we're replicated we MUST only send the message AFTER we've got quorum for updating\n\t// delivered state. Otherwise, we could be in an invalid state after a leader change.\n\t// We can send immediately if not replicated, not using acks, or using flow control (incompatible).\n\t// TODO(mvv): If NoWait we also bypass replicating first.\n\t//  Ideally we'd only send the NoWait request timeout after replication and delivery.\n\tif o.node == nil || ap == AckNone || o.cfg.FlowControl || wrNoWait {\n\t\to.outq.send(pmsg)\n\t} else {\n\t\to.addReplicatedQueuedMsg(pmsg)\n\t}\n\n\t// Flow control.\n\tif o.maxpb > 0 && o.needFlowControl(psz) {\n\t\to.sendFlowControl()\n\t}\n\n\t// If pull mode and we have inactivity threshold, signaled by dthresh, update last activity.\n\tif o.isPullMode() && o.dthresh > 0 {\n\t\to.waiting.last = time.Now()\n\t}\n\n\t// If we are ack none and mset is interest only we should make sure stream removes interest.\n\tif ap == AckNone && rp != LimitsPolicy {\n\t\tif mset != nil && mset.ackq != nil && (o.node == nil || o.cfg.Direct) {\n\t\t\tmset.ackq.push(seq)\n\t\t} else {\n\t\t\to.updateAcks(dseq, seq, _EMPTY_)\n\t\t}\n\t}\n}\n\nfunc (o *consumer) needFlowControl(sz int) bool {\n\tif o.maxpb == 0 {\n\t\treturn false\n\t}\n\t// Decide whether to send a flow control message which we will need the user to respond.\n\t// We send when we are over 50% of our current window limit.\n\tif o.fcid == _EMPTY_ && o.pbytes > o.maxpb/2 {\n\t\treturn true\n\t}\n\t// If we have an existing outstanding FC, check to see if we need to expand the o.fcsz\n\tif o.fcid != _EMPTY_ && (o.pbytes-o.fcsz) >= o.maxpb {\n\t\to.fcsz += sz\n\t}\n\treturn false\n}\n\nfunc (o *consumer) processFlowControl(_ *subscription, c *client, _ *Account, subj, _ string, _ []byte) {\n\to.mu.Lock()\n\tdefer o.mu.Unlock()\n\n\t// Ignore if not the latest we have sent out.\n\tif subj != o.fcid {\n\t\treturn\n\t}\n\n\t// For slow starts and ramping up.\n\tif o.maxpb < o.pblimit {\n\t\to.maxpb *= 2\n\t\tif o.maxpb > o.pblimit {\n\t\t\to.maxpb = o.pblimit\n\t\t}\n\t}\n\n\t// Update accounting.\n\to.pbytes -= o.fcsz\n\tif o.pbytes < 0 {\n\t\to.pbytes = 0\n\t}\n\to.fcid, o.fcsz = _EMPTY_, 0\n\n\to.signalNewMessages()\n}\n\n// Lock should be held.\nfunc (o *consumer) fcReply() string {\n\tvar sb strings.Builder\n\tsb.WriteString(jsFlowControlPre)\n\tsb.WriteString(o.stream)\n\tsb.WriteByte(btsep)\n\tsb.WriteString(o.name)\n\tsb.WriteByte(btsep)\n\tvar b [4]byte\n\trn := rand.Int63()\n\tfor i, l := 0, rn; i < len(b); i++ {\n\t\tb[i] = digits[l%base]\n\t\tl /= base\n\t}\n\tsb.Write(b[:])\n\treturn sb.String()\n}\n\n// sendFlowControl will send a flow control packet to the consumer.\n// Lock should be held.\nfunc (o *consumer) sendFlowControl() {\n\tif !o.isPushMode() {\n\t\treturn\n\t}\n\tsubj, rply := o.cfg.DeliverSubject, o.fcReply()\n\to.fcsz, o.fcid = o.pbytes, rply\n\thdr := []byte(\"NATS/1.0 100 FlowControl Request\\r\\n\\r\\n\")\n\to.outq.send(newJSPubMsg(subj, _EMPTY_, rply, hdr, nil, nil, 0))\n}\n\n// Tracks our outstanding pending acks. Only applicable to AckExplicit mode.\n// Lock should be held.\nfunc (o *consumer) trackPending(sseq, dseq uint64) {\n\tif o.pending == nil {\n\t\to.pending = make(map[uint64]*Pending)\n\t}\n\n\t// We could have a backoff that set a timer higher than what we need for this message.\n\t// In that case, reset to lowest backoff required for a message redelivery.\n\tminDelay := o.ackWait(0)\n\tif l := len(o.cfg.BackOff); l > 0 {\n\t\tbi := int(o.rdc[sseq])\n\t\tif bi < 0 {\n\t\t\tbi = 0\n\t\t} else if bi >= l {\n\t\t\tbi = l - 1\n\t\t}\n\t\tminDelay = o.ackWait(o.cfg.BackOff[bi])\n\t}\n\tminDeadline := time.Now().Add(minDelay)\n\tif o.ptmr == nil || o.ptmrEnd.After(minDeadline) {\n\t\to.resetPtmr(minDelay)\n\t}\n\n\tif p, ok := o.pending[sseq]; ok {\n\t\t// Update timestamp but keep original consumer delivery sequence.\n\t\t// So do not update p.Sequence.\n\t\tp.Timestamp = time.Now().UnixNano()\n\t} else {\n\t\to.pending[sseq] = &Pending{dseq, time.Now().UnixNano()}\n\t}\n}\n\n// Credit back a failed delivery.\n// lock should be held.\nfunc (o *consumer) creditWaitingRequest(reply string) {\n\twq := o.waiting\n\tfor wr := wq.head; wr != nil; wr = wr.next {\n\t\tif wr.reply == reply {\n\t\t\twr.n++\n\t\t\twr.d--\n\t\t\treturn\n\t\t}\n\t}\n}\n\n// didNotDeliver is called when a delivery for a consumer message failed.\n// Depending on our state, we will process the failure.\nfunc (o *consumer) didNotDeliver(seq uint64, subj string) {\n\to.mu.Lock()\n\tmset := o.mset\n\tif mset == nil {\n\t\to.mu.Unlock()\n\t\treturn\n\t}\n\t// Adjust back deliver count.\n\to.decDeliveryCount(seq)\n\n\tvar checkDeliveryInterest bool\n\tif o.isPushMode() {\n\t\to.active = false\n\t\tcheckDeliveryInterest = true\n\t} else if o.pending != nil {\n\t\t// Good chance we did not deliver because no interest so force a check.\n\t\to.processWaiting(false)\n\t\t// If it is still there credit it back.\n\t\to.creditWaitingRequest(subj)\n\t\t// pull mode and we have pending.\n\t\tif _, ok := o.pending[seq]; ok {\n\t\t\t// We found this messsage on pending, we need\n\t\t\t// to queue it up for immediate redelivery since\n\t\t\t// we know it was not delivered\n\t\t\tif !o.onRedeliverQueue(seq) {\n\t\t\t\to.addToRedeliverQueue(seq)\n\t\t\t\tif !o.waiting.isEmpty() {\n\t\t\t\t\to.signalNewMessages()\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\to.mu.Unlock()\n\n\tif checkDeliveryInterest {\n\t\tlocalInterest := !o.hasNoLocalInterest()\n\t\to.updateDeliveryInterest(localInterest)\n\t}\n}\n\n// Lock should be held.\nfunc (o *consumer) addToRedeliverQueue(seqs ...uint64) {\n\to.rdq = append(o.rdq, seqs...)\n\tfor _, seq := range seqs {\n\t\to.rdqi.Insert(seq)\n\t}\n}\n\n// Lock should be held.\nfunc (o *consumer) hasRedeliveries() bool {\n\treturn len(o.rdq) > 0\n}\n\nfunc (o *consumer) getNextToRedeliver() uint64 {\n\tif len(o.rdq) == 0 {\n\t\treturn 0\n\t}\n\tseq := o.rdq[0]\n\tif len(o.rdq) == 1 {\n\t\to.rdq = nil\n\t\to.rdqi.Empty()\n\t} else {\n\t\to.rdq = append(o.rdq[:0], o.rdq[1:]...)\n\t\to.rdqi.Delete(seq)\n\t}\n\treturn seq\n}\n\n// This checks if we already have this sequence queued for redelivery.\n// FIXME(dlc) - This is O(n) but should be fast with small redeliver size.\n// Lock should be held.\nfunc (o *consumer) onRedeliverQueue(seq uint64) bool {\n\treturn o.rdqi.Exists(seq)\n}\n\n// Remove a sequence from the redelivery queue.\n// Lock should be held.\nfunc (o *consumer) removeFromRedeliverQueue(seq uint64) bool {\n\tif !o.onRedeliverQueue(seq) {\n\t\treturn false\n\t}\n\tfor i, rseq := range o.rdq {\n\t\tif rseq == seq {\n\t\t\tif len(o.rdq) == 1 {\n\t\t\t\to.rdq = nil\n\t\t\t\to.rdqi.Empty()\n\t\t\t} else {\n\t\t\t\to.rdq = append(o.rdq[:i], o.rdq[i+1:]...)\n\t\t\t\to.rdqi.Delete(seq)\n\t\t\t}\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\n// Checks the pending messages.\nfunc (o *consumer) checkPending() {\n\to.mu.Lock()\n\tdefer o.mu.Unlock()\n\n\tmset := o.mset\n\t// On stop, mset and timer will be nil.\n\tif o.closed || mset == nil || o.ptmr == nil {\n\t\to.stopAndClearPtmr()\n\t\treturn\n\t}\n\n\tvar shouldUpdateState bool\n\tvar state StreamState\n\tmset.store.FastState(&state)\n\tfseq := state.FirstSeq\n\n\tnow := time.Now().UnixNano()\n\tttl := int64(o.cfg.AckWait)\n\tnext := int64(o.ackWait(0))\n\t// However, if there is backoff, initializes with the largest backoff.\n\t// It will be adjusted as needed.\n\tif l := len(o.cfg.BackOff); l > 0 {\n\t\tnext = int64(o.cfg.BackOff[l-1])\n\t}\n\n\t// Since we can update timestamps, we have to review all pending.\n\t// We will now bail if we see an ack pending inbound to us via o.awl.\n\tvar expired []uint64\n\tcheck := len(o.pending) > 1024\n\tfor seq, p := range o.pending {\n\t\tif check && atomic.LoadInt64(&o.awl) > 0 {\n\t\t\to.resetPtmr(100 * time.Millisecond)\n\t\t\treturn\n\t\t}\n\t\t// Check if these are no longer valid.\n\t\tif seq < fseq || seq <= o.asflr {\n\t\t\tdelete(o.pending, seq)\n\t\t\tdelete(o.rdc, seq)\n\t\t\to.removeFromRedeliverQueue(seq)\n\t\t\tshouldUpdateState = true\n\t\t\t// Check if we need to move ack floors.\n\t\t\tif seq > o.asflr {\n\t\t\t\to.asflr = seq\n\t\t\t}\n\t\t\tif p.Sequence > o.adflr {\n\t\t\t\to.adflr = p.Sequence\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\t\telapsed, deadline := now-p.Timestamp, ttl\n\t\tif len(o.cfg.BackOff) > 0 {\n\t\t\t// This is ok even if o.rdc is nil, we would get dc == 0, which is what we want.\n\t\t\tdc := int(o.rdc[seq])\n\t\t\tif dc < 0 {\n\t\t\t\t// Prevent consumer backoff from going backwards.\n\t\t\t\tdc = 0\n\t\t\t}\n\t\t\t// This will be the index for the next backoff, will set to last element if needed.\n\t\t\tnbi := dc + 1\n\t\t\tif dc+1 >= len(o.cfg.BackOff) {\n\t\t\t\tdc = len(o.cfg.BackOff) - 1\n\t\t\t\tnbi = dc\n\t\t\t}\n\t\t\tdeadline = int64(o.cfg.BackOff[dc])\n\t\t\t// Set `next` to the next backoff (if smaller than current `next` value).\n\t\t\tif nextBackoff := int64(o.cfg.BackOff[nbi]); nextBackoff < next {\n\t\t\t\tnext = nextBackoff\n\t\t\t}\n\t\t}\n\t\tif elapsed >= deadline {\n\t\t\t// We will check if we have hit our max deliveries. Previously we would do this on getNextMsg() which\n\t\t\t// worked well for push consumers, but with pull based consumers would require a new pull request to be\n\t\t\t// present to process and redelivered could be reported incorrectly.\n\t\t\tif !o.onRedeliverQueue(seq) && !o.hasMaxDeliveries(seq) {\n\t\t\t\texpired = append(expired, seq)\n\t\t\t}\n\t\t} else if deadline-elapsed < next {\n\t\t\t// Update when we should fire next.\n\t\t\tnext = deadline - elapsed\n\t\t}\n\t}\n\n\tif len(expired) > 0 {\n\t\t// We need to sort.\n\t\tslices.Sort(expired)\n\t\to.addToRedeliverQueue(expired...)\n\t\t// Now we should update the timestamp here since we are redelivering.\n\t\t// We will use an incrementing time to preserve order for any other redelivery.\n\t\toff := now - o.pending[expired[0]].Timestamp\n\t\tfor _, seq := range expired {\n\t\t\tif p, ok := o.pending[seq]; ok {\n\t\t\t\tp.Timestamp += off\n\t\t\t}\n\t\t}\n\t\to.signalNewMessages()\n\t}\n\n\tif len(o.pending) > 0 {\n\t\to.resetPtmr(time.Duration(next))\n\t} else {\n\t\t// Make sure to stop timer and clear out any re delivery queues\n\t\to.stopAndClearPtmr()\n\t\to.rdq = nil\n\t\to.rdqi.Empty()\n\t\to.pending = nil\n\t\t// Mimic behavior in processAckMsg when pending is empty.\n\t\to.adflr, o.asflr = o.dseq-1, o.sseq-1\n\t}\n\n\t// Update our state if needed.\n\tif shouldUpdateState {\n\t\tif err := o.writeStoreStateUnlocked(); err != nil && o.srv != nil && o.mset != nil && !o.closed {\n\t\t\ts, acc, mset, name := o.srv, o.acc, o.mset, o.name\n\t\t\ts.Warnf(\"Consumer '%s > %s > %s' error on write store state from check pending: %v\", acc, mset.getCfgName(), name, err)\n\t\t}\n\t}\n}\n\n// SeqFromReply will extract a sequence number from a reply subject.\nfunc (o *consumer) seqFromReply(reply string) uint64 {\n\t_, dseq, _ := ackReplyInfo(reply)\n\treturn dseq\n}\n\n// StreamSeqFromReply will extract the stream sequence from the reply subject.\nfunc (o *consumer) streamSeqFromReply(reply string) uint64 {\n\tsseq, _, _ := ackReplyInfo(reply)\n\treturn sseq\n}\n\n// Quick parser for positive numbers in ack reply encoding.\nfunc parseAckReplyNum(d string) (n int64) {\n\tif len(d) == 0 {\n\t\treturn -1\n\t}\n\tfor _, dec := range d {\n\t\tif dec < asciiZero || dec > asciiNine {\n\t\t\treturn -1\n\t\t}\n\t\tn = n*10 + (int64(dec) - asciiZero)\n\t}\n\treturn n\n}\n\nconst expectedNumReplyTokens = 9\n\n// Grab encoded information in the reply subject for a delivered message.\nfunc replyInfo(subject string) (sseq, dseq, dc uint64, ts int64, pending uint64) {\n\ttsa := [expectedNumReplyTokens]string{}\n\tstart, tokens := 0, tsa[:0]\n\tfor i := 0; i < len(subject); i++ {\n\t\tif subject[i] == btsep {\n\t\t\ttokens = append(tokens, subject[start:i])\n\t\t\tstart = i + 1\n\t\t}\n\t}\n\ttokens = append(tokens, subject[start:])\n\tif len(tokens) != expectedNumReplyTokens || tokens[0] != \"$JS\" || tokens[1] != \"ACK\" {\n\t\treturn 0, 0, 0, 0, 0\n\t}\n\t// TODO(dlc) - Should we error if we do not match consumer name?\n\t// stream is tokens[2], consumer is 3.\n\tdc = uint64(parseAckReplyNum(tokens[4]))\n\tsseq, dseq = uint64(parseAckReplyNum(tokens[5])), uint64(parseAckReplyNum(tokens[6]))\n\tts = parseAckReplyNum(tokens[7])\n\tpending = uint64(parseAckReplyNum(tokens[8]))\n\n\treturn sseq, dseq, dc, ts, pending\n}\n\nfunc ackReplyInfo(subject string) (sseq, dseq, dc uint64) {\n\ttsa := [expectedNumReplyTokens]string{}\n\tstart, tokens := 0, tsa[:0]\n\tfor i := 0; i < len(subject); i++ {\n\t\tif subject[i] == btsep {\n\t\t\ttokens = append(tokens, subject[start:i])\n\t\t\tstart = i + 1\n\t\t}\n\t}\n\ttokens = append(tokens, subject[start:])\n\tif len(tokens) != expectedNumReplyTokens || tokens[0] != \"$JS\" || tokens[1] != \"ACK\" {\n\t\treturn 0, 0, 0\n\t}\n\tdc = uint64(parseAckReplyNum(tokens[4]))\n\tsseq, dseq = uint64(parseAckReplyNum(tokens[5])), uint64(parseAckReplyNum(tokens[6]))\n\n\treturn sseq, dseq, dc\n}\n\n// NextSeq returns the next delivered sequence number for this consumer.\nfunc (o *consumer) nextSeq() uint64 {\n\to.mu.RLock()\n\tdseq := o.dseq\n\to.mu.RUnlock()\n\treturn dseq\n}\n\n// Used to hold skip list when deliver policy is last per subject.\ntype lastSeqSkipList struct {\n\tresume uint64\n\tseqs   []uint64\n}\n\n// Let's us know we have a skip list, which is for deliver last per subject and we are just starting.\n// Lock should be held.\nfunc (o *consumer) hasSkipListPending() bool {\n\treturn o.lss != nil && len(o.lss.seqs) > 0\n}\n\n// Will select the starting sequence.\nfunc (o *consumer) selectStartingSeqNo() {\n\tif o.mset == nil || o.mset.store == nil {\n\t\to.sseq = 1\n\t} else {\n\t\tvar state StreamState\n\t\to.mset.store.FastState(&state)\n\t\tif o.cfg.OptStartSeq == 0 {\n\t\t\tif o.cfg.DeliverPolicy == DeliverAll {\n\t\t\t\to.sseq = state.FirstSeq\n\t\t\t} else if o.cfg.DeliverPolicy == DeliverLast {\n\t\t\t\tif o.subjf == nil {\n\t\t\t\t\to.sseq = state.LastSeq\n\t\t\t\t} else {\n\t\t\t\t\t// If we are partitioned here this will be properly set when we become leader.\n\t\t\t\t\tfor _, filter := range o.subjf {\n\t\t\t\t\t\tss := o.mset.store.FilteredState(1, filter.subject)\n\t\t\t\t\t\tif ss.Last > o.sseq {\n\t\t\t\t\t\t\to.sseq = ss.Last\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t} else if o.cfg.DeliverPolicy == DeliverLastPerSubject {\n\t\t\t\t// If our parent stream is set to max msgs per subject of 1 this is just\n\t\t\t\t// a normal consumer at this point. We can avoid any heavy lifting.\n\t\t\t\to.mset.cfgMu.RLock()\n\t\t\t\tmmp := o.mset.cfg.MaxMsgsPer\n\t\t\t\to.mset.cfgMu.RUnlock()\n\t\t\t\tif mmp == 1 {\n\t\t\t\t\to.sseq = state.FirstSeq\n\t\t\t\t} else {\n\t\t\t\t\tvar filters []string\n\t\t\t\t\tif o.subjf == nil {\n\t\t\t\t\t\tfilters = append(filters, o.cfg.FilterSubject)\n\t\t\t\t\t} else {\n\t\t\t\t\t\tfor _, filter := range o.subjf {\n\t\t\t\t\t\t\tfilters = append(filters, filter.subject)\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t\tlss := &lastSeqSkipList{resume: state.LastSeq}\n\t\t\t\t\tlss.seqs, _ = o.mset.store.MultiLastSeqs(filters, 0, 0)\n\n\t\t\t\t\tif len(lss.seqs) == 0 {\n\t\t\t\t\t\to.sseq = state.LastSeq\n\t\t\t\t\t} else {\n\t\t\t\t\t\to.sseq = lss.seqs[0]\n\t\t\t\t\t}\n\t\t\t\t\t// Assign skip list.\n\t\t\t\t\to.lss = lss\n\t\t\t\t}\n\t\t\t} else if o.cfg.OptStartTime != nil {\n\t\t\t\t// If we are here we are time based.\n\t\t\t\t// TODO(dlc) - Once clustered can't rely on this.\n\t\t\t\to.sseq = o.mset.store.GetSeqFromTime(*o.cfg.OptStartTime)\n\t\t\t\t// Here we want to see if we are filtered, and if so possibly close the gap\n\t\t\t\t// to the nearest first given our starting sequence from time. This is so we do\n\t\t\t\t// not force the system to do a linear walk between o.sseq and the real first.\n\t\t\t\tif len(o.subjf) > 0 {\n\t\t\t\t\tnseq := state.LastSeq\n\t\t\t\t\tfor _, filter := range o.subjf {\n\t\t\t\t\t\t// Use first sequence since this is more optimized atm.\n\t\t\t\t\t\tss := o.mset.store.FilteredState(state.FirstSeq, filter.subject)\n\t\t\t\t\t\tif ss.First >= o.sseq && ss.First < nseq {\n\t\t\t\t\t\t\tnseq = ss.First\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\t// Skip ahead if possible.\n\t\t\t\t\tif nseq > o.sseq && nseq < state.LastSeq {\n\t\t\t\t\t\to.sseq = nseq\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\t// DeliverNew\n\t\t\t\to.sseq = state.LastSeq + 1\n\t\t\t}\n\t\t} else {\n\t\t\to.sseq = o.cfg.OptStartSeq\n\t\t}\n\n\t\tif state.FirstSeq == 0 && (o.cfg.Direct || o.cfg.OptStartSeq == 0) {\n\t\t\t// If the stream is empty, deliver only new.\n\t\t\t// But only if mirroring/sourcing, or start seq is unset, otherwise need to respect provided value.\n\t\t\to.sseq = 1\n\t\t} else if o.sseq > state.LastSeq && (o.cfg.Direct || o.cfg.OptStartSeq == 0) {\n\t\t\t// If selected sequence is in the future, clamp back down.\n\t\t\t// But only if mirroring/sourcing, or start seq is unset, otherwise need to respect provided value.\n\t\t\to.sseq = state.LastSeq + 1\n\t\t} else if o.sseq < state.FirstSeq {\n\t\t\t// If the first sequence is further ahead than the starting sequence,\n\t\t\t// there are no messages there anymore, so move the sequence up.\n\t\t\to.sseq = state.FirstSeq\n\t\t}\n\t}\n\n\t// Always set delivery sequence to 1.\n\to.dseq = 1\n\t// Set ack delivery floor to delivery-1\n\to.adflr = o.dseq - 1\n\t// Set ack store floor to store-1\n\to.asflr = o.sseq - 1\n\t// Set our starting sequence state.\n\t// But only if we're not clustered, if clustered we propose upon becoming leader.\n\tif o.store != nil && o.sseq > 0 && o.cfg.replicas(&o.mset.cfg) == 1 {\n\t\to.store.SetStarting(o.sseq - 1)\n\t}\n}\n\n// Test whether a config represents a durable subscriber.\nfunc isDurableConsumer(config *ConsumerConfig) bool {\n\treturn config != nil && config.Durable != _EMPTY_\n}\n\nfunc (o *consumer) isDurable() bool {\n\treturn o.cfg.Durable != _EMPTY_\n}\n\n// Are we in push mode, delivery subject, etc.\nfunc (o *consumer) isPushMode() bool {\n\treturn o.cfg.DeliverSubject != _EMPTY_\n}\n\nfunc (o *consumer) isPullMode() bool {\n\treturn o.cfg.DeliverSubject == _EMPTY_\n}\n\n// Name returns the name of this consumer.\nfunc (o *consumer) String() string {\n\to.mu.RLock()\n\tn := o.name\n\to.mu.RUnlock()\n\treturn n\n}\n\nfunc createConsumerName() string {\n\treturn getHash(nuid.Next())\n}\n\n// deleteConsumer will delete the consumer from this stream.\nfunc (mset *stream) deleteConsumer(o *consumer) error {\n\treturn o.delete()\n}\n\nfunc (o *consumer) getStream() *stream {\n\to.mu.RLock()\n\tmset := o.mset\n\to.mu.RUnlock()\n\treturn mset\n}\n\nfunc (o *consumer) streamName() string {\n\to.mu.RLock()\n\tmset := o.mset\n\to.mu.RUnlock()\n\tif mset != nil {\n\t\treturn mset.name()\n\t}\n\treturn _EMPTY_\n}\n\n// Active indicates if this consumer is still active.\nfunc (o *consumer) isActive() bool {\n\to.mu.RLock()\n\tactive := o.active && o.mset != nil\n\to.mu.RUnlock()\n\treturn active\n}\n\n// hasNoLocalInterest return true if we have no local interest.\nfunc (o *consumer) hasNoLocalInterest() bool {\n\to.mu.RLock()\n\tinterest := o.acc.sl.HasInterest(o.cfg.DeliverSubject)\n\to.mu.RUnlock()\n\treturn !interest\n}\n\n// This is when the underlying stream has been purged.\n// sseq is the new first seq for the stream after purge.\n// Lock should NOT be held.\nfunc (o *consumer) purge(sseq uint64, slseq uint64, isWider bool) {\n\t// Do not update our state unless we know we are the leader.\n\tif !o.isLeader() {\n\t\treturn\n\t}\n\t// Signals all have been purged for this consumer.\n\tif sseq == 0 && !isWider {\n\t\tsseq = slseq + 1\n\t}\n\n\tvar store StreamStore\n\tif isWider {\n\t\to.mu.RLock()\n\t\tif o.mset != nil {\n\t\t\tstore = o.mset.store\n\t\t}\n\t\to.mu.RUnlock()\n\t}\n\n\to.mu.Lock()\n\t// Do not go backwards\n\tif o.sseq < sseq {\n\t\to.sseq = sseq\n\t}\n\n\tif o.asflr < sseq {\n\t\to.asflr = sseq - 1\n\t\t// We need to remove those no longer relevant from pending.\n\t\tfor seq, p := range o.pending {\n\t\t\tif seq <= o.asflr {\n\t\t\t\tif p.Sequence > o.adflr {\n\t\t\t\t\to.adflr = p.Sequence\n\t\t\t\t\tif o.adflr > o.dseq {\n\t\t\t\t\t\to.dseq = o.adflr\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tdelete(o.pending, seq)\n\t\t\t\tdelete(o.rdc, seq)\n\t\t\t\t// rdq handled below.\n\t\t\t}\n\t\t\tif isWider && store != nil {\n\t\t\t\t// Our filtered subject, which could be all, is wider than the underlying purge.\n\t\t\t\t// We need to check if the pending items left are still valid.\n\t\t\t\tvar smv StoreMsg\n\t\t\t\tif _, err := store.LoadMsg(seq, &smv); err == errDeletedMsg || err == ErrStoreMsgNotFound {\n\t\t\t\t\tif p.Sequence > o.adflr {\n\t\t\t\t\t\to.adflr = p.Sequence\n\t\t\t\t\t\tif o.adflr > o.dseq {\n\t\t\t\t\t\t\to.dseq = o.adflr\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tdelete(o.pending, seq)\n\t\t\t\t\tdelete(o.rdc, seq)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\t// This means we can reset everything at this point.\n\tif len(o.pending) == 0 {\n\t\to.pending, o.rdc = nil, nil\n\t\to.adflr, o.asflr = o.dseq-1, o.sseq-1\n\t}\n\n\t// We need to remove all those being queued for redelivery under o.rdq\n\tif len(o.rdq) > 0 {\n\t\trdq := o.rdq\n\t\to.rdq = nil\n\t\to.rdqi.Empty()\n\t\tfor _, sseq := range rdq {\n\t\t\tif sseq >= o.sseq {\n\t\t\t\to.addToRedeliverQueue(sseq)\n\t\t\t}\n\t\t}\n\t}\n\t// Grab some info in case of error below.\n\ts, acc, mset, name := o.srv, o.acc, o.mset, o.name\n\to.mu.Unlock()\n\n\tif err := o.writeStoreState(); err != nil && s != nil && mset != nil {\n\t\ts.Warnf(\"Consumer '%s > %s > %s' error on write store state from purge: %v\", acc, mset.name(), name, err)\n\t}\n}\n\nfunc stopAndClearTimer(tp **time.Timer) {\n\tif *tp == nil {\n\t\treturn\n\t}\n\t// Will get drained in normal course, do not try to\n\t// drain here.\n\t(*tp).Stop()\n\t*tp = nil\n}\n\n// Stop will shutdown  the consumer for the associated stream.\nfunc (o *consumer) stop() error {\n\treturn o.stopWithFlags(false, false, true, false)\n}\n\nfunc (o *consumer) deleteWithoutAdvisory() error {\n\treturn o.stopWithFlags(true, false, true, false)\n}\n\n// Delete will delete the consumer for the associated stream and send advisories.\nfunc (o *consumer) delete() error {\n\treturn o.stopWithFlags(true, false, true, true)\n}\n\n// To test for closed state.\nfunc (o *consumer) isClosed() bool {\n\to.mu.RLock()\n\tdefer o.mu.RUnlock()\n\treturn o.closed\n}\n\nfunc (o *consumer) stopWithFlags(dflag, sdflag, doSignal, advisory bool) error {\n\t// If dflag is true determine if we are still assigned.\n\tvar isAssigned bool\n\tif dflag {\n\t\to.mu.RLock()\n\t\tacc, stream, consumer := o.acc, o.stream, o.name\n\t\tisClustered := o.js != nil && o.js.isClustered()\n\t\to.mu.RUnlock()\n\t\tif isClustered {\n\t\t\t// Grab jsa to check assignment.\n\t\t\tvar jsa *jsAccount\n\t\t\tif acc != nil {\n\t\t\t\t// Need lock here to avoid data race.\n\t\t\t\tacc.mu.RLock()\n\t\t\t\tjsa = acc.js\n\t\t\t\tacc.mu.RUnlock()\n\t\t\t}\n\t\t\tif jsa != nil {\n\t\t\t\tisAssigned = jsa.consumerAssigned(stream, consumer)\n\t\t\t}\n\t\t}\n\t}\n\n\to.mu.Lock()\n\tif o.closed {\n\t\to.mu.Unlock()\n\t\treturn nil\n\t}\n\to.closed = true\n\n\t// Check if we are the leader and are being deleted (as a node).\n\tif dflag && o.isLeader() {\n\t\t// If we are clustered and node leader (probable from above), stepdown.\n\t\tif node := o.node; node != nil {\n\t\t\tnode.StepDown()\n\t\t}\n\n\t\t// dflag does not necessarily mean that the consumer is being deleted,\n\t\t// just that the consumer node is being removed from this peer, so we\n\t\t// send delete advisories only if we are no longer assigned at the meta layer,\n\t\t// or we are not clustered.\n\t\tif !isAssigned && advisory {\n\t\t\to.sendDeleteAdvisoryLocked()\n\t\t}\n\t\tif o.isPullMode() {\n\t\t\t// Release any pending.\n\t\t\to.releaseAnyPendingRequests(isAssigned)\n\t\t}\n\t}\n\n\tif o.qch != nil {\n\t\tclose(o.qch)\n\t\to.qch = nil\n\t}\n\n\ta := o.acc\n\tstore := o.store\n\tmset := o.mset\n\to.mset = nil\n\to.active = false\n\to.unsubscribe(o.ackSub)\n\to.unsubscribe(o.reqSub)\n\to.unsubscribe(o.fcSub)\n\to.ackSub = nil\n\to.reqSub = nil\n\to.fcSub = nil\n\tif o.infoSub != nil {\n\t\to.srv.sysUnsubscribe(o.infoSub)\n\t\to.infoSub = nil\n\t}\n\tc := o.client\n\to.client = nil\n\tsysc := o.sysc\n\to.sysc = nil\n\to.stopAndClearPtmr()\n\tstopAndClearTimer(&o.dtmr)\n\tstopAndClearTimer(&o.gwdtmr)\n\tdelivery := o.cfg.DeliverSubject\n\to.waiting = nil\n\t// Break us out of the readLoop.\n\tif doSignal {\n\t\to.signalNewMessages()\n\t}\n\tn := o.node\n\tqgroup := o.cfg.DeliverGroup\n\to.ackMsgs.unregister()\n\tif o.nextMsgReqs != nil {\n\t\to.nextMsgReqs.unregister()\n\t}\n\n\t// For cleaning up the node assignment.\n\tvar ca *consumerAssignment\n\tif dflag {\n\t\tca = o.ca\n\t}\n\tjs := o.js\n\to.mu.Unlock()\n\n\tif c != nil {\n\t\tc.closeConnection(ClientClosed)\n\t}\n\tif sysc != nil {\n\t\tsysc.closeConnection(ClientClosed)\n\t}\n\n\tif delivery != _EMPTY_ {\n\t\ta.sl.clearNotification(delivery, qgroup, o.inch)\n\t}\n\n\tvar rp RetentionPolicy\n\tif mset != nil {\n\t\tmset.mu.Lock()\n\t\tmset.removeConsumer(o)\n\t\t// No need for cfgMu's lock since mset.mu.Lock superseeds it.\n\t\trp = mset.cfg.Retention\n\t\tmset.mu.Unlock()\n\t}\n\n\t// Cleanup messages that lost interest.\n\tif dflag && rp == InterestPolicy {\n\t\to.cleanupNoInterestMessages(mset, true)\n\t}\n\n\t// Cluster cleanup.\n\tif n != nil {\n\t\tif dflag {\n\t\t\tn.Delete()\n\t\t} else {\n\t\t\tn.Stop()\n\t\t}\n\t}\n\n\tif ca != nil {\n\t\tjs.mu.Lock()\n\t\tif ca.Group != nil {\n\t\t\tca.Group.node = nil\n\t\t}\n\t\tjs.mu.Unlock()\n\t}\n\n\t// Clean up our store.\n\tvar err error\n\tif store != nil {\n\t\tif dflag {\n\t\t\tif sdflag {\n\t\t\t\terr = store.StreamDelete()\n\t\t\t} else {\n\t\t\t\terr = store.Delete()\n\t\t\t}\n\t\t} else {\n\t\t\terr = store.Stop()\n\t\t}\n\t}\n\n\treturn err\n}\n\n// We need to optionally remove all messages since we are interest based retention.\n// We will do this consistently on all replicas. Note that if in clustered mode the non-leader\n// consumers will need to restore state first.\n// ignoreInterest marks whether the consumer should be ignored when determining interest.\n// No lock held on entry.\nfunc (o *consumer) cleanupNoInterestMessages(mset *stream, ignoreInterest bool) {\n\to.mu.Lock()\n\tif !o.isLeader() {\n\t\to.readStoredState()\n\t}\n\tstart := o.asflr\n\to.mu.Unlock()\n\n\t// Make sure we start at worst with first sequence in the stream.\n\tstate := mset.state()\n\tif start < state.FirstSeq {\n\t\tstart = state.FirstSeq\n\t}\n\tstop := state.LastSeq\n\n\t// Consumer's interests are ignored by default. If we should not ignore interest, unset.\n\tco := o\n\tif !ignoreInterest {\n\t\tco = nil\n\t}\n\n\tvar rmseqs []uint64\n\tmset.mu.RLock()\n\n\t// If over this amount of messages to check, optimistically call to checkInterestState().\n\t// It will not always do the right thing in removing messages that lost interest, but ensures\n\t// we don't degrade performance by doing a linear scan through the whole stream.\n\t// Messages might need to expire based on limits to be cleaned up.\n\t// TODO(dlc) - Better way?\n\tconst bailThresh = 100_000\n\n\t// Check if we would be spending too much time here and defer to separate go routine.\n\tif len(mset.consumers) == 0 {\n\t\tmset.mu.RUnlock()\n\t\tmset.mu.Lock()\n\t\tdefer mset.mu.Unlock()\n\t\tmset.store.Purge()\n\t\tvar state StreamState\n\t\tmset.store.FastState(&state)\n\t\tmset.lseq = state.LastSeq\n\t\t// Also make sure we clear any pending acks.\n\t\tmset.clearAllPreAcksBelowFloor(state.FirstSeq)\n\t\treturn\n\t} else if stop-start > bailThresh {\n\t\tmset.mu.RUnlock()\n\t\tgo mset.checkInterestState()\n\t\treturn\n\t}\n\n\tmset.mu.RUnlock()\n\tmset.mu.Lock()\n\tfor seq := start; seq <= stop; seq++ {\n\t\tif mset.noInterest(seq, co) {\n\t\t\trmseqs = append(rmseqs, seq)\n\t\t}\n\t}\n\tmset.mu.Unlock()\n\n\t// These can be removed.\n\tfor _, seq := range rmseqs {\n\t\tmset.store.RemoveMsg(seq)\n\t}\n}\n\n// Check that we do not form a cycle by delivering to a delivery subject\n// that is part of the interest group.\nfunc deliveryFormsCycle(cfg *StreamConfig, deliverySubject string) bool {\n\tfor _, subject := range cfg.Subjects {\n\t\tif subjectIsSubsetMatch(deliverySubject, subject) {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\n// switchToEphemeral is called on startup when recovering ephemerals.\nfunc (o *consumer) switchToEphemeral() {\n\to.mu.Lock()\n\to.cfg.Durable = _EMPTY_\n\tstore, ok := o.store.(*consumerFileStore)\n\tinterest := o.acc.sl.HasInterest(o.cfg.DeliverSubject)\n\t// Setup dthresh.\n\to.updateInactiveThreshold(&o.cfg)\n\to.updatePauseState(&o.cfg)\n\to.mu.Unlock()\n\n\t// Update interest\n\to.updateDeliveryInterest(interest)\n\t// Write out new config\n\tif ok {\n\t\tstore.updateConfig(o.cfg)\n\t}\n}\n\n// RequestNextMsgSubject returns the subject to request the next message when in pull or worker mode.\n// Returns empty otherwise.\nfunc (o *consumer) requestNextMsgSubject() string {\n\treturn o.nextMsgSubj\n}\n\nfunc (o *consumer) decStreamPending(sseq uint64, subj string) {\n\to.mu.Lock()\n\n\t// Update our cached num pending only if we think deliverMsg has not done so.\n\tif sseq >= o.sseq && o.isFilteredMatch(subj) {\n\t\to.npc--\n\t}\n\n\t// Check if this message was pending.\n\tp, wasPending := o.pending[sseq]\n\tvar rdc uint64\n\tif wasPending {\n\t\trdc = o.deliveryCount(sseq)\n\t}\n\n\to.mu.Unlock()\n\n\t// If it was pending process it like an ack.\n\tif wasPending {\n\t\t// We could have the lock for the stream so do this in a go routine.\n\t\t// TODO(dlc) - We should do this with ipq vs naked go routines.\n\t\tgo o.processTerm(sseq, p.Sequence, rdc, ackTermUnackedLimitsReason, _EMPTY_)\n\t}\n}\n\nfunc (o *consumer) account() *Account {\n\to.mu.RLock()\n\ta := o.acc\n\to.mu.RUnlock()\n\treturn a\n}\n\n// Creates a sublist for consumer.\n// All subjects share the same callback.\nfunc (o *consumer) signalSubs() []string {\n\to.mu.Lock()\n\tdefer o.mu.Unlock()\n\n\tif o.sigSubs != nil {\n\t\treturn o.sigSubs\n\t}\n\n\tif len(o.subjf) == 0 {\n\t\tsubs := []string{fwcs}\n\t\to.sigSubs = subs\n\t\treturn subs\n\t}\n\n\tsubs := make([]string, 0, len(o.subjf))\n\tfor _, filter := range o.subjf {\n\t\tsubs = append(subs, filter.subject)\n\t}\n\to.sigSubs = subs\n\treturn subs\n}\n\n// This is what will be called when our parent stream wants to kick us regarding a new message.\n// We know that this subject matches us by how the parent handles registering us with the signaling sublist,\n// but we must check if we are leader.\n// We do need the sequence of the message however and we use the msg as the encoded seq.\nfunc (o *consumer) processStreamSignal(seq uint64) {\n\t// We can get called here now when not leader, so bail fast\n\t// and without acquiring any locks.\n\tif !o.leader.Load() {\n\t\treturn\n\t}\n\to.mu.Lock()\n\tdefer o.mu.Unlock()\n\tif o.mset == nil {\n\t\treturn\n\t}\n\tif seq > o.npf {\n\t\to.npc++\n\t}\n\tif seq < o.sseq {\n\t\treturn\n\t}\n\tif o.isPushMode() && o.active || o.isPullMode() && !o.waiting.isEmpty() {\n\t\to.signalNewMessages()\n\t}\n}\n\n// Used to compare if two multiple filtered subject lists are equal.\nfunc subjectSliceEqual(slice1 []string, slice2 []string) bool {\n\tif len(slice1) != len(slice2) {\n\t\treturn false\n\t}\n\tset2 := make(map[string]struct{}, len(slice2))\n\tfor _, val := range slice2 {\n\t\tset2[val] = struct{}{}\n\t}\n\tfor _, val := range slice1 {\n\t\tif _, ok := set2[val]; !ok {\n\t\t\treturn false\n\t\t}\n\t}\n\treturn true\n}\n\n// Utility for simpler if conditions in Consumer config checks.\n// In future iteration, we can immediately create `o.subjf` and\n// use it to validate things.\nfunc gatherSubjectFilters(filter string, filters []string) []string {\n\tif filter != _EMPTY_ {\n\t\tfilters = append(filters, filter)\n\t}\n\t// list of filters should never contain non-empty filter.\n\treturn filters\n}\n\n// shouldStartMonitor will return true if we should start a monitor\n// goroutine or will return false if one is already running.\nfunc (o *consumer) shouldStartMonitor() bool {\n\to.mu.Lock()\n\tdefer o.mu.Unlock()\n\n\tif o.inMonitor {\n\t\treturn false\n\t}\n\to.monitorWg.Add(1)\n\to.inMonitor = true\n\treturn true\n}\n\n// Clear the monitor running state. The monitor goroutine should\n// call this in a defer to clean up on exit.\nfunc (o *consumer) clearMonitorRunning() {\n\to.mu.Lock()\n\tdefer o.mu.Unlock()\n\n\tif o.inMonitor {\n\t\to.monitorWg.Done()\n\t\to.inMonitor = false\n\t}\n}\n\n// Test whether we are in the monitor routine.\nfunc (o *consumer) isMonitorRunning() bool {\n\to.mu.RLock()\n\tdefer o.mu.RUnlock()\n\treturn o.inMonitor\n}\n\n// If we detect that our ackfloor is higher than the stream's last sequence, return this error.\nvar errAckFloorHigherThanLastSeq = errors.New(\"consumer ack floor is higher than streams last sequence\")\nvar errAckFloorInvalid = errors.New(\"consumer ack floor is invalid\")\n\n// If we are a consumer of an interest or workqueue policy stream, process that state and make sure consistent.\nfunc (o *consumer) checkStateForInterestStream(ss *StreamState) error {\n\to.mu.RLock()\n\t// See if we need to process this update if our parent stream is not a limits policy stream.\n\tmset := o.mset\n\tshouldProcessState := mset != nil && o.retention != LimitsPolicy\n\tif o.closed || !shouldProcessState || o.store == nil || ss == nil {\n\t\to.mu.RUnlock()\n\t\treturn nil\n\t}\n\tstore := mset.store\n\tstate, err := o.store.State()\n\n\tfilters, subjf, filter := o.filters, o.subjf, _EMPTY_\n\tvar wc bool\n\tif filters == nil && subjf != nil {\n\t\tfilter, wc = subjf[0].subject, subjf[0].hasWildcard\n\t}\n\tchkfloor := o.chkflr\n\to.mu.RUnlock()\n\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tasflr := state.AckFloor.Stream\n\t// Protect ourselves against rolling backwards.\n\tif asflr&(1<<63) != 0 {\n\t\treturn errAckFloorInvalid\n\t}\n\n\t// Check if the underlying stream's last sequence is less than our floor.\n\t// This can happen if the stream has been reset and has not caught up yet.\n\tif asflr > ss.LastSeq {\n\t\treturn errAckFloorHigherThanLastSeq\n\t}\n\n\tvar smv StoreMsg\n\tvar seq, nseq uint64\n\t// Start at first stream seq or a previous check floor, whichever is higher.\n\t// Note this will really help for interest retention, with WQ the loadNextMsg\n\t// gets us a long way already since it will skip deleted msgs not for our filter.\n\tfseq := ss.FirstSeq\n\tif chkfloor > fseq {\n\t\tfseq = chkfloor\n\t}\n\n\tvar retryAsflr uint64\n\tfor seq = fseq; asflr > 0 && seq <= asflr; seq++ {\n\t\tif filters != nil {\n\t\t\t_, nseq, err = store.LoadNextMsgMulti(filters, seq, &smv)\n\t\t} else {\n\t\t\t_, nseq, err = store.LoadNextMsg(filter, wc, seq, &smv)\n\t\t}\n\t\t// if we advanced sequence update our seq. This can be on no error and EOF.\n\t\tif nseq > seq {\n\t\t\tseq = nseq\n\t\t}\n\t\t// Only ack though if no error and seq <= ack floor.\n\t\tif err == nil && seq <= asflr {\n\t\t\tdidRemove := mset.ackMsg(o, seq)\n\t\t\t// Removing the message could fail. For example if clustered since we need to propose it.\n\t\t\t// Overwrite retry floor (only the first time) to allow us to check next time if the removal was successful.\n\t\t\tif didRemove && retryAsflr == 0 {\n\t\t\t\tretryAsflr = seq\n\t\t\t}\n\t\t}\n\t}\n\t// If retry floor was not overwritten, set to ack floor+1, we don't need to account for any retries below it.\n\tif retryAsflr == 0 {\n\t\tretryAsflr = asflr + 1\n\t}\n\n\to.mu.Lock()\n\t// Update our check floor.\n\t// Check floor must never be greater than ack floor+1, otherwise subsequent calls to this function would skip work.\n\tif retryAsflr > o.chkflr {\n\t\to.chkflr = retryAsflr\n\t}\n\t// See if we need to process this update if our parent stream is not a limits policy stream.\n\tstate, _ = o.store.State()\n\to.mu.Unlock()\n\n\t// If we have pending, we will need to walk through to delivered in case we missed any of those acks as well.\n\tif state != nil && len(state.Pending) > 0 && state.AckFloor.Stream > 0 {\n\t\tfor seq := state.AckFloor.Stream + 1; seq <= state.Delivered.Stream; seq++ {\n\t\t\tif _, ok := state.Pending[seq]; !ok {\n\t\t\t\t// Want to call needAck since it is filter aware.\n\t\t\t\tif o.needAck(seq, _EMPTY_) {\n\t\t\t\t\tmset.ackMsg(o, seq)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc (o *consumer) resetPtmr(delay time.Duration) {\n\tif o.ptmr == nil {\n\t\to.ptmr = time.AfterFunc(delay, o.checkPending)\n\t} else {\n\t\to.ptmr.Reset(delay)\n\t}\n\to.ptmrEnd = time.Now().Add(delay)\n}\n\nfunc (o *consumer) stopAndClearPtmr() {\n\tstopAndClearTimer(&o.ptmr)\n\to.ptmrEnd = time.Time{}\n}\n\nfunc (o *consumer) resetPendingDeliveries() {\n\tfor _, pmsg := range o.pendingDeliveries {\n\t\tpmsg.returnToPool()\n\t}\n\to.pendingDeliveries = nil\n}\n",
    "source_file": "server/consumer.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2019-2025 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage server\n\nimport (\n\t\"bufio\"\n\t\"bytes\"\n\t\"crypto/tls\"\n\t\"encoding/base64\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"io\"\n\t\"math/rand\"\n\t\"net\"\n\t\"net/http\"\n\t\"net/url\"\n\t\"os\"\n\t\"path\"\n\t\"reflect\"\n\t\"regexp\"\n\t\"runtime\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t\"github.com/klauspost/compress/s2\"\n\t\"github.com/nats-io/jwt/v2\"\n\t\"github.com/nats-io/nkeys\"\n\t\"github.com/nats-io/nuid\"\n)\n\nconst (\n\t// Warning when user configures leafnode TLS insecure\n\tleafnodeTLSInsecureWarning = \"TLS certificate chain and hostname of solicited leafnodes will not be verified. DO NOT USE IN PRODUCTION!\"\n\n\t// When a loop is detected, delay the reconnect of solicited connection.\n\tleafNodeReconnectDelayAfterLoopDetected = 30 * time.Second\n\n\t// When a server receives a message causing a permission violation, the\n\t// connection is closed and it won't attempt to reconnect for that long.\n\tleafNodeReconnectAfterPermViolation = 30 * time.Second\n\n\t// When we have the same cluster name as the hub.\n\tleafNodeReconnectDelayAfterClusterNameSame = 30 * time.Second\n\n\t// Prefix for loop detection subject\n\tleafNodeLoopDetectionSubjectPrefix = \"$LDS.\"\n\n\t// Path added to URL to indicate to WS server that the connection is a\n\t// LEAF connection as opposed to a CLIENT.\n\tleafNodeWSPath = \"/leafnode\"\n\n\t// This is the time the server will wait, when receiving a CONNECT,\n\t// before closing the connection if the required minimum version is not met.\n\tleafNodeWaitBeforeClose = 5 * time.Second\n)\n\ntype leaf struct {\n\t// We have any auth stuff here for solicited connections.\n\tremote *leafNodeCfg\n\t// isSpoke tells us what role we are playing.\n\t// Used when we receive a connection but otherside tells us they are a hub.\n\tisSpoke bool\n\t// remoteCluster is when we are a hub but the spoke leafnode is part of a cluster.\n\tremoteCluster string\n\t// remoteServer holds onto the remove server's name or ID.\n\tremoteServer string\n\t// domain name of remote server\n\tremoteDomain string\n\t// account name of remote server\n\tremoteAccName string\n\t// Used to suppress sub and unsub interest. Same as routes but our audience\n\t// here is tied to this leaf node. This will hold all subscriptions except this\n\t// leaf nodes. This represents all the interest we want to send to the other side.\n\tsmap map[string]int32\n\t// This map will contain all the subscriptions that have been added to the smap\n\t// during initLeafNodeSmapAndSendSubs. It is short lived and is there to avoid\n\t// race between processing of a sub where sub is added to account sublist but\n\t// updateSmap has not be called on that \"thread\", while in the LN readloop,\n\t// when processing CONNECT, initLeafNodeSmapAndSendSubs is invoked and add\n\t// this subscription to smap. When processing of the sub then calls updateSmap,\n\t// we would add it a second time in the smap causing later unsub to suppress the LS-.\n\ttsub  map[*subscription]struct{}\n\ttsubt *time.Timer\n\t// Selected compression mode, which may be different from the server configured mode.\n\tcompression string\n\t// This is for GW map replies.\n\tgwSub *subscription\n}\n\n// Used for remote (solicited) leafnodes.\ntype leafNodeCfg struct {\n\tsync.RWMutex\n\t*RemoteLeafOpts\n\turls           []*url.URL\n\tcurURL         *url.URL\n\ttlsName        string\n\tusername       string\n\tpassword       string\n\tperms          *Permissions\n\tconnDelay      time.Duration // Delay before a connect, could be used while detecting loop condition, etc..\n\tjsMigrateTimer *time.Timer\n}\n\n// Check to see if this is a solicited leafnode. We do special processing for solicited.\nfunc (c *client) isSolicitedLeafNode() bool {\n\treturn c.kind == LEAF && c.leaf.remote != nil\n}\n\n// Returns true if this is a solicited leafnode and is not configured to be treated as a hub or a receiving\n// connection leafnode where the otherside has declared itself to be the hub.\nfunc (c *client) isSpokeLeafNode() bool {\n\treturn c.kind == LEAF && c.leaf.isSpoke\n}\n\nfunc (c *client) isHubLeafNode() bool {\n\treturn c.kind == LEAF && !c.leaf.isSpoke\n}\n\n// This will spin up go routines to solicit the remote leaf node connections.\nfunc (s *Server) solicitLeafNodeRemotes(remotes []*RemoteLeafOpts) {\n\tsysAccName := _EMPTY_\n\tsAcc := s.SystemAccount()\n\tif sAcc != nil {\n\t\tsysAccName = sAcc.Name\n\t}\n\taddRemote := func(r *RemoteLeafOpts, isSysAccRemote bool) *leafNodeCfg {\n\t\ts.mu.Lock()\n\t\tremote := newLeafNodeCfg(r)\n\t\tcreds := remote.Credentials\n\t\taccName := remote.LocalAccount\n\t\ts.leafRemoteCfgs = append(s.leafRemoteCfgs, remote)\n\t\t// Print notice if\n\t\tif isSysAccRemote {\n\t\t\tif len(remote.DenyExports) > 0 {\n\t\t\t\ts.Noticef(\"Remote for System Account uses restricted export permissions\")\n\t\t\t}\n\t\t\tif len(remote.DenyImports) > 0 {\n\t\t\t\ts.Noticef(\"Remote for System Account uses restricted import permissions\")\n\t\t\t}\n\t\t}\n\t\ts.mu.Unlock()\n\t\tif creds != _EMPTY_ {\n\t\t\tcontents, err := os.ReadFile(creds)\n\t\t\tdefer wipeSlice(contents)\n\t\t\tif err != nil {\n\t\t\t\ts.Errorf(\"Error reading LeafNode Remote Credentials file %q: %v\", creds, err)\n\t\t\t} else if items := credsRe.FindAllSubmatch(contents, -1); len(items) < 2 {\n\t\t\t\ts.Errorf(\"LeafNode Remote Credentials file %q malformed\", creds)\n\t\t\t} else if _, err := nkeys.FromSeed(items[1][1]); err != nil {\n\t\t\t\ts.Errorf(\"LeafNode Remote Credentials file %q has malformed seed\", creds)\n\t\t\t} else if uc, err := jwt.DecodeUserClaims(string(items[0][1])); err != nil {\n\t\t\t\ts.Errorf(\"LeafNode Remote Credentials file %q has malformed user jwt\", creds)\n\t\t\t} else if isSysAccRemote {\n\t\t\t\tif !uc.Permissions.Pub.Empty() || !uc.Permissions.Sub.Empty() || uc.Permissions.Resp != nil {\n\t\t\t\t\ts.Noticef(\"LeafNode Remote for System Account uses credentials file %q with restricted permissions\", creds)\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tif !uc.Permissions.Pub.Empty() || !uc.Permissions.Sub.Empty() || uc.Permissions.Resp != nil {\n\t\t\t\t\ts.Noticef(\"LeafNode Remote for Account %s uses credentials file %q with restricted permissions\", accName, creds)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\treturn remote\n\t}\n\tfor _, r := range remotes {\n\t\tremote := addRemote(r, r.LocalAccount == sysAccName)\n\t\ts.startGoRoutine(func() { s.connectToRemoteLeafNode(remote, true) })\n\t}\n}\n\nfunc (s *Server) remoteLeafNodeStillValid(remote *leafNodeCfg) bool {\n\tfor _, ri := range s.getOpts().LeafNode.Remotes {\n\t\t// FIXME(dlc) - What about auth changes?\n\t\tif reflect.DeepEqual(ri.URLs, remote.URLs) {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\n// Ensure that leafnode is properly configured.\nfunc validateLeafNode(o *Options) error {\n\tif err := validateLeafNodeAuthOptions(o); err != nil {\n\t\treturn err\n\t}\n\n\t// Users can bind to any local account, if its empty we will assume the $G account.\n\tfor _, r := range o.LeafNode.Remotes {\n\t\tif r.LocalAccount == _EMPTY_ {\n\t\t\tr.LocalAccount = globalAccountName\n\t\t}\n\t}\n\n\t// In local config mode, check that leafnode configuration refers to accounts that exist.\n\tif len(o.TrustedOperators) == 0 {\n\t\taccNames := map[string]struct{}{}\n\t\tfor _, a := range o.Accounts {\n\t\t\taccNames[a.Name] = struct{}{}\n\t\t}\n\t\t// global account is always created\n\t\taccNames[DEFAULT_GLOBAL_ACCOUNT] = struct{}{}\n\t\t// in the context of leaf nodes, empty account means global account\n\t\taccNames[_EMPTY_] = struct{}{}\n\t\t// system account either exists or, if not disabled, will be created\n\t\tif o.SystemAccount == _EMPTY_ && !o.NoSystemAccount {\n\t\t\taccNames[DEFAULT_SYSTEM_ACCOUNT] = struct{}{}\n\t\t}\n\t\tcheckAccountExists := func(accName string, cfgType string) error {\n\t\t\tif _, ok := accNames[accName]; !ok {\n\t\t\t\treturn fmt.Errorf(\"cannot find local account %q specified in leafnode %s\", accName, cfgType)\n\t\t\t}\n\t\t\treturn nil\n\t\t}\n\t\tif err := checkAccountExists(o.LeafNode.Account, \"authorization\"); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tfor _, lu := range o.LeafNode.Users {\n\t\t\tif lu.Account == nil { // means global account\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif err := checkAccountExists(lu.Account.Name, \"authorization\"); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t\tfor _, r := range o.LeafNode.Remotes {\n\t\t\tif err := checkAccountExists(r.LocalAccount, \"remote\"); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t} else {\n\t\tif len(o.LeafNode.Users) != 0 {\n\t\t\treturn fmt.Errorf(\"operator mode does not allow specifying users in leafnode config\")\n\t\t}\n\t\tfor _, r := range o.LeafNode.Remotes {\n\t\t\tif !nkeys.IsValidPublicAccountKey(r.LocalAccount) {\n\t\t\t\treturn fmt.Errorf(\n\t\t\t\t\t\"operator mode requires account nkeys in remotes. \" +\n\t\t\t\t\t\t\"Please add an `account` key to each remote in your `leafnodes` section, to assign it to an account. \" +\n\t\t\t\t\t\t\"Each account value should be a 56 character public key, starting with the letter 'A'\")\n\t\t\t}\n\t\t}\n\t\tif o.LeafNode.Port != 0 && o.LeafNode.Account != \"\" && !nkeys.IsValidPublicAccountKey(o.LeafNode.Account) {\n\t\t\treturn fmt.Errorf(\"operator mode and non account nkeys are incompatible\")\n\t\t}\n\t}\n\n\t// Validate compression settings\n\tif o.LeafNode.Compression.Mode != _EMPTY_ {\n\t\tif err := validateAndNormalizeCompressionOption(&o.LeafNode.Compression, CompressionS2Auto); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\t// If a remote has a websocket scheme, all need to have it.\n\tfor _, rcfg := range o.LeafNode.Remotes {\n\t\tif len(rcfg.URLs) >= 2 {\n\t\t\tfirstIsWS, ok := isWSURL(rcfg.URLs[0]), true\n\t\t\tfor i := 1; i < len(rcfg.URLs); i++ {\n\t\t\t\tu := rcfg.URLs[i]\n\t\t\t\tif isWS := isWSURL(u); isWS && !firstIsWS || !isWS && firstIsWS {\n\t\t\t\t\tok = false\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t\tif !ok {\n\t\t\t\treturn fmt.Errorf(\"remote leaf node configuration cannot have a mix of websocket and non-websocket urls: %q\", redactURLList(rcfg.URLs))\n\t\t\t}\n\t\t}\n\t\t// Validate compression settings\n\t\tif rcfg.Compression.Mode != _EMPTY_ {\n\t\t\tif err := validateAndNormalizeCompressionOption(&rcfg.Compression, CompressionS2Auto); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t}\n\n\tif o.LeafNode.Port == 0 {\n\t\treturn nil\n\t}\n\n\t// If MinVersion is defined, check that it is valid.\n\tif mv := o.LeafNode.MinVersion; mv != _EMPTY_ {\n\t\tif err := checkLeafMinVersionConfig(mv); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\t// The checks below will be done only when detecting that we are configured\n\t// with gateways. So if an option validation needs to be done regardless,\n\t// it MUST be done before this point!\n\n\tif o.Gateway.Name == _EMPTY_ && o.Gateway.Port == 0 {\n\t\treturn nil\n\t}\n\t// If we are here we have both leaf nodes and gateways defined, make sure there\n\t// is a system account defined.\n\tif o.SystemAccount == _EMPTY_ {\n\t\treturn fmt.Errorf(\"leaf nodes and gateways (both being defined) require a system account to also be configured\")\n\t}\n\tif err := validatePinnedCerts(o.LeafNode.TLSPinnedCerts); err != nil {\n\t\treturn fmt.Errorf(\"leafnode: %v\", err)\n\t}\n\treturn nil\n}\n\nfunc checkLeafMinVersionConfig(mv string) error {\n\tif ok, err := versionAtLeastCheckError(mv, 2, 8, 0); !ok || err != nil {\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"invalid leafnode's minimum version: %v\", err)\n\t\t} else {\n\t\t\treturn fmt.Errorf(\"the minimum version should be at least 2.8.0\")\n\t\t}\n\t}\n\treturn nil\n}\n\n// Used to validate user names in LeafNode configuration.\n// - rejects mix of single and multiple users.\n// - rejects duplicate user names.\nfunc validateLeafNodeAuthOptions(o *Options) error {\n\tif len(o.LeafNode.Users) == 0 {\n\t\treturn nil\n\t}\n\tif o.LeafNode.Username != _EMPTY_ {\n\t\treturn fmt.Errorf(\"can not have a single user/pass and a users array\")\n\t}\n\tif o.LeafNode.Nkey != _EMPTY_ {\n\t\treturn fmt.Errorf(\"can not have a single nkey and a users array\")\n\t}\n\tusers := map[string]struct{}{}\n\tfor _, u := range o.LeafNode.Users {\n\t\tif _, exists := users[u.Username]; exists {\n\t\t\treturn fmt.Errorf(\"duplicate user %q detected in leafnode authorization\", u.Username)\n\t\t}\n\t\tusers[u.Username] = struct{}{}\n\t}\n\treturn nil\n}\n\n// Update remote LeafNode TLS configurations after a config reload.\nfunc (s *Server) updateRemoteLeafNodesTLSConfig(opts *Options) {\n\tmax := len(opts.LeafNode.Remotes)\n\tif max == 0 {\n\t\treturn\n\t}\n\n\ts.mu.RLock()\n\tdefer s.mu.RUnlock()\n\n\t// Changes in the list of remote leaf nodes is not supported.\n\t// However, make sure that we don't go over the arrays.\n\tif len(s.leafRemoteCfgs) < max {\n\t\tmax = len(s.leafRemoteCfgs)\n\t}\n\tfor i := 0; i < max; i++ {\n\t\tro := opts.LeafNode.Remotes[i]\n\t\tcfg := s.leafRemoteCfgs[i]\n\t\tif ro.TLSConfig != nil {\n\t\t\tcfg.Lock()\n\t\t\tcfg.TLSConfig = ro.TLSConfig.Clone()\n\t\t\tcfg.TLSHandshakeFirst = ro.TLSHandshakeFirst\n\t\t\tcfg.Unlock()\n\t\t}\n\t}\n}\n\nfunc (s *Server) reConnectToRemoteLeafNode(remote *leafNodeCfg) {\n\tdelay := s.getOpts().LeafNode.ReconnectInterval\n\tselect {\n\tcase <-time.After(delay):\n\tcase <-s.quitCh:\n\t\ts.grWG.Done()\n\t\treturn\n\t}\n\ts.connectToRemoteLeafNode(remote, false)\n}\n\n// Creates a leafNodeCfg object that wraps the RemoteLeafOpts.\nfunc newLeafNodeCfg(remote *RemoteLeafOpts) *leafNodeCfg {\n\tcfg := &leafNodeCfg{\n\t\tRemoteLeafOpts: remote,\n\t\turls:           make([]*url.URL, 0, len(remote.URLs)),\n\t}\n\tif len(remote.DenyExports) > 0 || len(remote.DenyImports) > 0 {\n\t\tperms := &Permissions{}\n\t\tif len(remote.DenyExports) > 0 {\n\t\t\tperms.Publish = &SubjectPermission{Deny: remote.DenyExports}\n\t\t}\n\t\tif len(remote.DenyImports) > 0 {\n\t\t\tperms.Subscribe = &SubjectPermission{Deny: remote.DenyImports}\n\t\t}\n\t\tcfg.perms = perms\n\t}\n\t// Start with the one that is configured. We will add to this\n\t// array when receiving async leafnode INFOs.\n\tcfg.urls = append(cfg.urls, cfg.URLs...)\n\t// If allowed to randomize, do it on our copy of URLs\n\tif !remote.NoRandomize {\n\t\trand.Shuffle(len(cfg.urls), func(i, j int) {\n\t\t\tcfg.urls[i], cfg.urls[j] = cfg.urls[j], cfg.urls[i]\n\t\t})\n\t}\n\t// If we are TLS make sure we save off a proper servername if possible.\n\t// Do same for user/password since we may need them to connect to\n\t// a bare URL that we get from INFO protocol.\n\tfor _, u := range cfg.urls {\n\t\tcfg.saveTLSHostname(u)\n\t\tcfg.saveUserPassword(u)\n\t\t// If the url(s) have the \"wss://\" scheme, and we don't have a TLS\n\t\t// config, mark that we should be using TLS anyway.\n\t\tif !cfg.TLS && isWSSURL(u) {\n\t\t\tcfg.TLS = true\n\t\t}\n\t}\n\treturn cfg\n}\n\n// Will pick an URL from the list of available URLs.\nfunc (cfg *leafNodeCfg) pickNextURL() *url.URL {\n\tcfg.Lock()\n\tdefer cfg.Unlock()\n\t// If the current URL is the first in the list and we have more than\n\t// one URL, then move that one to end of the list.\n\tif cfg.curURL != nil && len(cfg.urls) > 1 && urlsAreEqual(cfg.curURL, cfg.urls[0]) {\n\t\tfirst := cfg.urls[0]\n\t\tcopy(cfg.urls, cfg.urls[1:])\n\t\tcfg.urls[len(cfg.urls)-1] = first\n\t}\n\tcfg.curURL = cfg.urls[0]\n\treturn cfg.curURL\n}\n\n// Returns the current URL\nfunc (cfg *leafNodeCfg) getCurrentURL() *url.URL {\n\tcfg.RLock()\n\tdefer cfg.RUnlock()\n\treturn cfg.curURL\n}\n\n// Returns how long the server should wait before attempting\n// to solicit a remote leafnode connection.\nfunc (cfg *leafNodeCfg) getConnectDelay() time.Duration {\n\tcfg.RLock()\n\tdelay := cfg.connDelay\n\tcfg.RUnlock()\n\treturn delay\n}\n\n// Sets the connect delay.\nfunc (cfg *leafNodeCfg) setConnectDelay(delay time.Duration) {\n\tcfg.Lock()\n\tcfg.connDelay = delay\n\tcfg.Unlock()\n}\n\n// Ensure that non-exported options (used in tests) have\n// been properly set.\nfunc (s *Server) setLeafNodeNonExportedOptions() {\n\topts := s.getOpts()\n\ts.leafNodeOpts.dialTimeout = opts.LeafNode.dialTimeout\n\tif s.leafNodeOpts.dialTimeout == 0 {\n\t\t// Use same timeouts as routes for now.\n\t\ts.leafNodeOpts.dialTimeout = DEFAULT_ROUTE_DIAL\n\t}\n\ts.leafNodeOpts.resolver = opts.LeafNode.resolver\n\tif s.leafNodeOpts.resolver == nil {\n\t\ts.leafNodeOpts.resolver = net.DefaultResolver\n\t}\n}\n\nconst sharedSysAccDelay = 250 * time.Millisecond\n\nfunc (s *Server) connectToRemoteLeafNode(remote *leafNodeCfg, firstConnect bool) {\n\tdefer s.grWG.Done()\n\n\tif remote == nil || len(remote.URLs) == 0 {\n\t\ts.Debugf(\"Empty remote leafnode definition, nothing to connect\")\n\t\treturn\n\t}\n\n\topts := s.getOpts()\n\treconnectDelay := opts.LeafNode.ReconnectInterval\n\ts.mu.RLock()\n\tdialTimeout := s.leafNodeOpts.dialTimeout\n\tresolver := s.leafNodeOpts.resolver\n\tvar isSysAcc bool\n\tif s.eventsEnabled() {\n\t\tisSysAcc = remote.LocalAccount == s.sys.account.Name\n\t}\n\tjetstreamMigrateDelay := remote.JetStreamClusterMigrateDelay\n\ts.mu.RUnlock()\n\n\t// If we are sharing a system account and we are not standalone delay to gather some info prior.\n\tif firstConnect && isSysAcc && !s.standAloneMode() {\n\t\ts.Debugf(\"Will delay first leafnode connect to shared system account due to clustering\")\n\t\tremote.setConnectDelay(sharedSysAccDelay)\n\t}\n\n\tif connDelay := remote.getConnectDelay(); connDelay > 0 {\n\t\tselect {\n\t\tcase <-time.After(connDelay):\n\t\tcase <-s.quitCh:\n\t\t\treturn\n\t\t}\n\t\tremote.setConnectDelay(0)\n\t}\n\n\tvar conn net.Conn\n\n\tconst connErrFmt = \"Error trying to connect as leafnode to remote server %q (attempt %v): %v\"\n\n\tattempts := 0\n\n\tfor s.isRunning() && s.remoteLeafNodeStillValid(remote) {\n\t\trURL := remote.pickNextURL()\n\t\turl, err := s.getRandomIP(resolver, rURL.Host, nil)\n\t\tif err == nil {\n\t\t\tvar ipStr string\n\t\t\tif url != rURL.Host {\n\t\t\t\tipStr = fmt.Sprintf(\" (%s)\", url)\n\t\t\t}\n\t\t\t// Some test may want to disable remotes from connecting\n\t\t\tif s.isLeafConnectDisabled() {\n\t\t\t\ts.Debugf(\"Will not attempt to connect to remote server on %q%s, leafnodes currently disabled\", rURL.Host, ipStr)\n\t\t\t\terr = ErrLeafNodeDisabled\n\t\t\t} else {\n\t\t\t\ts.Debugf(\"Trying to connect as leafnode to remote server on %q%s\", rURL.Host, ipStr)\n\t\t\t\tconn, err = natsDialTimeout(\"tcp\", url, dialTimeout)\n\t\t\t}\n\t\t}\n\t\tif err != nil {\n\t\t\tjitter := time.Duration(rand.Int63n(int64(reconnectDelay)))\n\t\t\tdelay := reconnectDelay + jitter\n\t\t\tattempts++\n\t\t\tif s.shouldReportConnectErr(firstConnect, attempts) {\n\t\t\t\ts.Errorf(connErrFmt, rURL.Host, attempts, err)\n\t\t\t} else {\n\t\t\t\ts.Debugf(connErrFmt, rURL.Host, attempts, err)\n\t\t\t}\n\t\t\tremote.Lock()\n\t\t\t// if we are using a delay to start migrating assets, kick off a migrate timer.\n\t\t\tif remote.jsMigrateTimer == nil && jetstreamMigrateDelay > 0 {\n\t\t\t\tremote.jsMigrateTimer = time.AfterFunc(jetstreamMigrateDelay, func() {\n\t\t\t\t\ts.checkJetStreamMigrate(remote)\n\t\t\t\t})\n\t\t\t}\n\t\t\tremote.Unlock()\n\t\t\tselect {\n\t\t\tcase <-s.quitCh:\n\t\t\t\tremote.cancelMigrateTimer()\n\t\t\t\treturn\n\t\t\tcase <-time.After(delay):\n\t\t\t\t// Check if we should migrate any JetStream assets immediately while this remote is down.\n\t\t\t\t// This will be used if JetStreamClusterMigrateDelay was not set\n\t\t\t\tif jetstreamMigrateDelay == 0 {\n\t\t\t\t\ts.checkJetStreamMigrate(remote)\n\t\t\t\t}\n\t\t\t\tcontinue\n\t\t\t}\n\t\t}\n\t\tremote.cancelMigrateTimer()\n\t\tif !s.remoteLeafNodeStillValid(remote) {\n\t\t\tconn.Close()\n\t\t\treturn\n\t\t}\n\n\t\t// We have a connection here to a remote server.\n\t\t// Go ahead and create our leaf node and return.\n\t\ts.createLeafNode(conn, rURL, remote, nil)\n\n\t\t// Clear any observer states if we had them.\n\t\ts.clearObserverState(remote)\n\n\t\treturn\n\t}\n}\n\nfunc (cfg *leafNodeCfg) cancelMigrateTimer() {\n\tcfg.Lock()\n\tstopAndClearTimer(&cfg.jsMigrateTimer)\n\tcfg.Unlock()\n}\n\n// This will clear any observer state such that stream or consumer assets on this server can become leaders again.\nfunc (s *Server) clearObserverState(remote *leafNodeCfg) {\n\ts.mu.RLock()\n\taccName := remote.LocalAccount\n\ts.mu.RUnlock()\n\n\tacc, err := s.LookupAccount(accName)\n\tif err != nil {\n\t\ts.Warnf(\"Error looking up account [%s] checking for JetStream clear observer state on a leafnode\", accName)\n\t\treturn\n\t}\n\n\tacc.jscmMu.Lock()\n\tdefer acc.jscmMu.Unlock()\n\n\t// Walk all streams looking for any clustered stream, skip otherwise.\n\tfor _, mset := range acc.streams() {\n\t\tnode := mset.raftNode()\n\t\tif node == nil {\n\t\t\t// Not R>1\n\t\t\tcontinue\n\t\t}\n\t\t// Check consumers\n\t\tfor _, o := range mset.getConsumers() {\n\t\t\tif n := o.raftNode(); n != nil {\n\t\t\t\t// Ensure we can become a leader again.\n\t\t\t\tn.SetObserver(false)\n\t\t\t}\n\t\t}\n\t\t// Ensure we can not become a leader again.\n\t\tnode.SetObserver(false)\n\t}\n}\n\n// Check to see if we should migrate any assets from this account.\nfunc (s *Server) checkJetStreamMigrate(remote *leafNodeCfg) {\n\ts.mu.RLock()\n\taccName, shouldMigrate := remote.LocalAccount, remote.JetStreamClusterMigrate\n\ts.mu.RUnlock()\n\n\tif !shouldMigrate {\n\t\treturn\n\t}\n\n\tacc, err := s.LookupAccount(accName)\n\tif err != nil {\n\t\ts.Warnf(\"Error looking up account [%s] checking for JetStream migration on a leafnode\", accName)\n\t\treturn\n\t}\n\n\tacc.jscmMu.Lock()\n\tdefer acc.jscmMu.Unlock()\n\n\t// Walk all streams looking for any clustered stream, skip otherwise.\n\t// If we are the leader force stepdown.\n\tfor _, mset := range acc.streams() {\n\t\tnode := mset.raftNode()\n\t\tif node == nil {\n\t\t\t// Not R>1\n\t\t\tcontinue\n\t\t}\n\t\t// Collect any consumers\n\t\tfor _, o := range mset.getConsumers() {\n\t\t\tif n := o.raftNode(); n != nil {\n\t\t\t\tn.StepDown()\n\t\t\t\t// Ensure we can not become a leader while in this state.\n\t\t\t\tn.SetObserver(true)\n\t\t\t}\n\t\t}\n\t\t// Stepdown if this stream was leader.\n\t\tnode.StepDown()\n\t\t// Ensure we can not become a leader while in this state.\n\t\tnode.SetObserver(true)\n\t}\n}\n\n// Helper for checking.\nfunc (s *Server) isLeafConnectDisabled() bool {\n\ts.mu.RLock()\n\tdefer s.mu.RUnlock()\n\treturn s.leafDisableConnect\n}\n\n// Save off the tlsName for when we use TLS and mix hostnames and IPs. IPs usually\n// come from the server we connect to.\n//\n// We used to save the name only if there was a TLSConfig or scheme equal to \"tls\".\n// However, this was causing failures for users that did not set the scheme (and\n// their remote connections did not have a tls{} block).\n// We now save the host name regardless in case the remote returns an INFO indicating\n// that TLS is required.\nfunc (cfg *leafNodeCfg) saveTLSHostname(u *url.URL) {\n\tif cfg.tlsName == _EMPTY_ && net.ParseIP(u.Hostname()) == nil {\n\t\tcfg.tlsName = u.Hostname()\n\t}\n}\n\n// Save off the username/password for when we connect using a bare URL\n// that we get from the INFO protocol.\nfunc (cfg *leafNodeCfg) saveUserPassword(u *url.URL) {\n\tif cfg.username == _EMPTY_ && u.User != nil {\n\t\tcfg.username = u.User.Username()\n\t\tcfg.password, _ = u.User.Password()\n\t}\n}\n\n// This starts the leafnode accept loop in a go routine, unless it\n// is detected that the server has already been shutdown.\nfunc (s *Server) startLeafNodeAcceptLoop() {\n\t// Snapshot server options.\n\topts := s.getOpts()\n\n\tport := opts.LeafNode.Port\n\tif port == -1 {\n\t\tport = 0\n\t}\n\n\tif s.isShuttingDown() {\n\t\treturn\n\t}\n\n\ts.mu.Lock()\n\thp := net.JoinHostPort(opts.LeafNode.Host, strconv.Itoa(port))\n\tl, e := natsListen(\"tcp\", hp)\n\ts.leafNodeListenerErr = e\n\tif e != nil {\n\t\ts.mu.Unlock()\n\t\ts.Fatalf(\"Error listening on leafnode port: %d - %v\", opts.LeafNode.Port, e)\n\t\treturn\n\t}\n\n\ts.Noticef(\"Listening for leafnode connections on %s\",\n\t\tnet.JoinHostPort(opts.LeafNode.Host, strconv.Itoa(l.Addr().(*net.TCPAddr).Port)))\n\n\ttlsRequired := opts.LeafNode.TLSConfig != nil\n\ttlsVerify := tlsRequired && opts.LeafNode.TLSConfig.ClientAuth == tls.RequireAndVerifyClientCert\n\t// Do not set compression in this Info object, it would possibly cause\n\t// issues when sending asynchronous INFO to the remote.\n\tinfo := Info{\n\t\tID:            s.info.ID,\n\t\tName:          s.info.Name,\n\t\tVersion:       s.info.Version,\n\t\tGitCommit:     gitCommit,\n\t\tGoVersion:     runtime.Version(),\n\t\tAuthRequired:  true,\n\t\tTLSRequired:   tlsRequired,\n\t\tTLSVerify:     tlsVerify,\n\t\tMaxPayload:    s.info.MaxPayload, // TODO(dlc) - Allow override?\n\t\tHeaders:       s.supportsHeaders(),\n\t\tJetStream:     opts.JetStream,\n\t\tDomain:        opts.JetStreamDomain,\n\t\tProto:         s.getServerProto(),\n\t\tInfoOnConnect: true,\n\t}\n\t// If we have selected a random port...\n\tif port == 0 {\n\t\t// Write resolved port back to options.\n\t\topts.LeafNode.Port = l.Addr().(*net.TCPAddr).Port\n\t}\n\n\ts.leafNodeInfo = info\n\t// Possibly override Host/Port and set IP based on Cluster.Advertise\n\tif err := s.setLeafNodeInfoHostPortAndIP(); err != nil {\n\t\ts.Fatalf(\"Error setting leafnode INFO with LeafNode.Advertise value of %s, err=%v\", opts.LeafNode.Advertise, err)\n\t\tl.Close()\n\t\ts.mu.Unlock()\n\t\treturn\n\t}\n\ts.leafURLsMap[s.leafNodeInfo.IP]++\n\ts.generateLeafNodeInfoJSON()\n\n\t// Setup state that can enable shutdown\n\ts.leafNodeListener = l\n\n\t// As of now, a server that does not have remotes configured would\n\t// never solicit a connection, so we should not have to warn if\n\t// InsecureSkipVerify is set in main LeafNodes config (since\n\t// this TLS setting matters only when soliciting a connection).\n\t// Still, warn if insecure is set in any of LeafNode block.\n\t// We need to check remotes, even if tls is not required on accept.\n\twarn := tlsRequired && opts.LeafNode.TLSConfig.InsecureSkipVerify\n\tif !warn {\n\t\tfor _, r := range opts.LeafNode.Remotes {\n\t\t\tif r.TLSConfig != nil && r.TLSConfig.InsecureSkipVerify {\n\t\t\t\twarn = true\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t}\n\tif warn {\n\t\ts.Warnf(leafnodeTLSInsecureWarning)\n\t}\n\tgo s.acceptConnections(l, \"Leafnode\", func(conn net.Conn) { s.createLeafNode(conn, nil, nil, nil) }, nil)\n\ts.mu.Unlock()\n}\n\n// RegEx to match a creds file with user JWT and Seed.\nvar credsRe = regexp.MustCompile(`\\s*(?:(?:[-]{3,}.*[-]{3,}\\r?\\n)([\\w\\-.=]+)(?:\\r?\\n[-]{3,}.*[-]{3,}(\\r?\\n|\\z)))`)\n\n// clusterName is provided as argument to avoid lock ordering issues with the locked client c\n// Lock should be held entering here.\nfunc (c *client) sendLeafConnect(clusterName string, headers bool) error {\n\t// We support basic user/pass and operator based user JWT with signatures.\n\tcinfo := leafConnectInfo{\n\t\tVersion:       VERSION,\n\t\tID:            c.srv.info.ID,\n\t\tDomain:        c.srv.info.Domain,\n\t\tName:          c.srv.info.Name,\n\t\tHub:           c.leaf.remote.Hub,\n\t\tCluster:       clusterName,\n\t\tHeaders:       headers,\n\t\tJetStream:     c.acc.jetStreamConfigured(),\n\t\tDenyPub:       c.leaf.remote.DenyImports,\n\t\tCompression:   c.leaf.compression,\n\t\tRemoteAccount: c.acc.GetName(),\n\t\tProto:         c.srv.getServerProto(),\n\t}\n\n\t// If a signature callback is specified, this takes precedence over anything else.\n\tif cb := c.leaf.remote.SignatureCB; cb != nil {\n\t\tnonce := c.nonce\n\t\tc.mu.Unlock()\n\t\tjwt, sigraw, err := cb(nonce)\n\t\tc.mu.Lock()\n\t\tif err == nil && c.isClosed() {\n\t\t\terr = ErrConnectionClosed\n\t\t}\n\t\tif err != nil {\n\t\t\tc.Errorf(\"Error signing the nonce: %v\", err)\n\t\t\treturn err\n\t\t}\n\t\tsig := base64.RawURLEncoding.EncodeToString(sigraw)\n\t\tcinfo.JWT, cinfo.Sig = jwt, sig\n\n\t} else if creds := c.leaf.remote.Credentials; creds != _EMPTY_ {\n\t\t// Check for credentials first, that will take precedence..\n\t\tc.Debugf(\"Authenticating with credentials file %q\", c.leaf.remote.Credentials)\n\t\tcontents, err := os.ReadFile(creds)\n\t\tif err != nil {\n\t\t\tc.Errorf(\"%v\", err)\n\t\t\treturn err\n\t\t}\n\t\tdefer wipeSlice(contents)\n\t\titems := credsRe.FindAllSubmatch(contents, -1)\n\t\tif len(items) < 2 {\n\t\t\tc.Errorf(\"Credentials file malformed\")\n\t\t\treturn err\n\t\t}\n\t\t// First result should be the user JWT.\n\t\t// We copy here so that the file containing the seed will be wiped appropriately.\n\t\traw := items[0][1]\n\t\ttmp := make([]byte, len(raw))\n\t\tcopy(tmp, raw)\n\t\t// Seed is second item.\n\t\tkp, err := nkeys.FromSeed(items[1][1])\n\t\tif err != nil {\n\t\t\tc.Errorf(\"Credentials file has malformed seed\")\n\t\t\treturn err\n\t\t}\n\t\t// Wipe our key on exit.\n\t\tdefer kp.Wipe()\n\n\t\tsigraw, _ := kp.Sign(c.nonce)\n\t\tsig := base64.RawURLEncoding.EncodeToString(sigraw)\n\t\tcinfo.JWT = bytesToString(tmp)\n\t\tcinfo.Sig = sig\n\t} else if nkey := c.leaf.remote.Nkey; nkey != _EMPTY_ {\n\t\tkp, err := nkeys.FromSeed([]byte(nkey))\n\t\tif err != nil {\n\t\t\tc.Errorf(\"Remote nkey has malformed seed\")\n\t\t\treturn err\n\t\t}\n\t\t// Wipe our key on exit.\n\t\tdefer kp.Wipe()\n\t\tsigraw, _ := kp.Sign(c.nonce)\n\t\tsig := base64.RawURLEncoding.EncodeToString(sigraw)\n\t\tpkey, _ := kp.PublicKey()\n\t\tcinfo.Nkey = pkey\n\t\tcinfo.Sig = sig\n\t}\n\t// In addition, and this is to allow auth callout, set user/password or\n\t// token if applicable.\n\tif userInfo := c.leaf.remote.curURL.User; userInfo != nil {\n\t\t// For backward compatibility, if only username is provided, set both\n\t\t// Token and User, not just Token.\n\t\tcinfo.User = userInfo.Username()\n\t\tvar ok bool\n\t\tcinfo.Pass, ok = userInfo.Password()\n\t\tif !ok {\n\t\t\tcinfo.Token = cinfo.User\n\t\t}\n\t} else if c.leaf.remote.username != _EMPTY_ {\n\t\tcinfo.User = c.leaf.remote.username\n\t\tcinfo.Pass = c.leaf.remote.password\n\t}\n\tb, err := json.Marshal(cinfo)\n\tif err != nil {\n\t\tc.Errorf(\"Error marshaling CONNECT to remote leafnode: %v\\n\", err)\n\t\treturn err\n\t}\n\t// Although this call is made before the writeLoop is created,\n\t// we don't really need to send in place. The protocol will be\n\t// sent out by the writeLoop.\n\tc.enqueueProto([]byte(fmt.Sprintf(ConProto, b)))\n\treturn nil\n}\n\n// Makes a deep copy of the LeafNode Info structure.\n// The server lock is held on entry.\nfunc (s *Server) copyLeafNodeInfo() *Info {\n\tclone := s.leafNodeInfo\n\t// Copy the array of urls.\n\tif len(s.leafNodeInfo.LeafNodeURLs) > 0 {\n\t\tclone.LeafNodeURLs = append([]string(nil), s.leafNodeInfo.LeafNodeURLs...)\n\t}\n\treturn &clone\n}\n\n// Adds a LeafNode URL that we get when a route connects to the Info structure.\n// Regenerates the JSON byte array so that it can be sent to LeafNode connections.\n// Returns a boolean indicating if the URL was added or not.\n// Server lock is held on entry\nfunc (s *Server) addLeafNodeURL(urlStr string) bool {\n\tif s.leafURLsMap.addUrl(urlStr) {\n\t\ts.generateLeafNodeInfoJSON()\n\t\treturn true\n\t}\n\treturn false\n}\n\n// Removes a LeafNode URL of the route that is disconnecting from the Info structure.\n// Regenerates the JSON byte array so that it can be sent to LeafNode connections.\n// Returns a boolean indicating if the URL was removed or not.\n// Server lock is held on entry.\nfunc (s *Server) removeLeafNodeURL(urlStr string) bool {\n\t// Don't need to do this if we are removing the route connection because\n\t// we are shuting down...\n\tif s.isShuttingDown() {\n\t\treturn false\n\t}\n\tif s.leafURLsMap.removeUrl(urlStr) {\n\t\ts.generateLeafNodeInfoJSON()\n\t\treturn true\n\t}\n\treturn false\n}\n\n// Server lock is held on entry\nfunc (s *Server) generateLeafNodeInfoJSON() {\n\ts.leafNodeInfo.Cluster = s.cachedClusterName()\n\ts.leafNodeInfo.LeafNodeURLs = s.leafURLsMap.getAsStringSlice()\n\ts.leafNodeInfo.WSConnectURLs = s.websocket.connectURLsMap.getAsStringSlice()\n\ts.leafNodeInfoJSON = generateInfoJSON(&s.leafNodeInfo)\n}\n\n// Sends an async INFO protocol so that the connected servers can update\n// their list of LeafNode urls.\nfunc (s *Server) sendAsyncLeafNodeInfo() {\n\tfor _, c := range s.leafs {\n\t\tc.mu.Lock()\n\t\tc.enqueueProto(s.leafNodeInfoJSON)\n\t\tc.mu.Unlock()\n\t}\n}\n\n// Called when an inbound leafnode connection is accepted or we create one for a solicited leafnode.\nfunc (s *Server) createLeafNode(conn net.Conn, rURL *url.URL, remote *leafNodeCfg, ws *websocket) *client {\n\t// Snapshot server options.\n\topts := s.getOpts()\n\n\tmaxPay := int32(opts.MaxPayload)\n\tmaxSubs := int32(opts.MaxSubs)\n\t// For system, maxSubs of 0 means unlimited, so re-adjust here.\n\tif maxSubs == 0 {\n\t\tmaxSubs = -1\n\t}\n\tnow := time.Now().UTC()\n\n\tc := &client{srv: s, nc: conn, kind: LEAF, opts: defaultOpts, mpay: maxPay, msubs: maxSubs, start: now, last: now}\n\t// Do not update the smap here, we need to do it in initLeafNodeSmapAndSendSubs\n\tc.leaf = &leaf{}\n\n\t// For accepted LN connections, ws will be != nil if it was accepted\n\t// through the Websocket port.\n\tc.ws = ws\n\n\t// For remote, check if the scheme starts with \"ws\", if so, we will initiate\n\t// a remote Leaf Node connection as a websocket connection.\n\tif remote != nil && rURL != nil && isWSURL(rURL) {\n\t\tremote.RLock()\n\t\tc.ws = &websocket{compress: remote.Websocket.Compression, maskwrite: !remote.Websocket.NoMasking}\n\t\tremote.RUnlock()\n\t}\n\n\t// Determines if we are soliciting the connection or not.\n\tvar solicited bool\n\tvar acc *Account\n\tvar remoteSuffix string\n\tif remote != nil {\n\t\t// For now, if lookup fails, we will constantly try\n\t\t// to recreate this LN connection.\n\t\tlacc := remote.LocalAccount\n\t\tvar err error\n\t\tacc, err = s.LookupAccount(lacc)\n\t\tif err != nil {\n\t\t\t// An account not existing is something that can happen with nats/http account resolver and the account\n\t\t\t// has not yet been pushed, or the request failed for other reasons.\n\t\t\t// remote needs to be set or retry won't happen\n\t\t\tc.leaf.remote = remote\n\t\t\tc.closeConnection(MissingAccount)\n\t\t\ts.Errorf(\"Unable to lookup account %s for solicited leafnode connection: %v\", lacc, err)\n\t\t\treturn nil\n\t\t}\n\t\tremoteSuffix = fmt.Sprintf(\" for account: %s\", acc.traceLabel())\n\t}\n\n\tc.mu.Lock()\n\tc.initClient()\n\tc.Noticef(\"Leafnode connection created%s %s\", remoteSuffix, c.opts.Name)\n\n\tvar (\n\t\ttlsFirst         bool\n\t\ttlsFirstFallback time.Duration\n\t\tinfoTimeout      time.Duration\n\t)\n\tif remote != nil {\n\t\tsolicited = true\n\t\tremote.Lock()\n\t\tc.leaf.remote = remote\n\t\tc.setPermissions(remote.perms)\n\t\tif !c.leaf.remote.Hub {\n\t\t\tc.leaf.isSpoke = true\n\t\t}\n\t\ttlsFirst = remote.TLSHandshakeFirst\n\t\tinfoTimeout = remote.FirstInfoTimeout\n\t\tremote.Unlock()\n\t\tc.acc = acc\n\t} else {\n\t\tc.flags.set(expectConnect)\n\t\tif ws != nil {\n\t\t\tc.Debugf(\"Leafnode compression=%v\", c.ws.compress)\n\t\t}\n\t\ttlsFirst = opts.LeafNode.TLSHandshakeFirst\n\t\tif f := opts.LeafNode.TLSHandshakeFirstFallback; f > 0 {\n\t\t\ttlsFirstFallback = f\n\t\t}\n\t}\n\tc.mu.Unlock()\n\n\tvar nonce [nonceLen]byte\n\tvar info *Info\n\n\t// Grab this before the client lock below.\n\tif !solicited {\n\t\t// Grab server variables\n\t\ts.mu.Lock()\n\t\tinfo = s.copyLeafNodeInfo()\n\t\t// For tests that want to simulate old servers, do not set the compression\n\t\t// on the INFO protocol if configured with CompressionNotSupported.\n\t\tif cm := opts.LeafNode.Compression.Mode; cm != CompressionNotSupported {\n\t\t\tinfo.Compression = cm\n\t\t}\n\t\ts.generateNonce(nonce[:])\n\t\ts.mu.Unlock()\n\t}\n\n\t// Grab lock\n\tc.mu.Lock()\n\n\tvar preBuf []byte\n\tif solicited {\n\t\t// For websocket connection, we need to send an HTTP request,\n\t\t// and get the response before starting the readLoop to get\n\t\t// the INFO, etc..\n\t\tif c.isWebsocket() {\n\t\t\tvar err error\n\t\t\tvar closeReason ClosedState\n\n\t\t\tpreBuf, closeReason, err = c.leafNodeSolicitWSConnection(opts, rURL, remote)\n\t\t\tif err != nil {\n\t\t\t\tc.Errorf(\"Error soliciting websocket connection: %v\", err)\n\t\t\t\tc.mu.Unlock()\n\t\t\t\tif closeReason != 0 {\n\t\t\t\t\tc.closeConnection(closeReason)\n\t\t\t\t}\n\t\t\t\treturn nil\n\t\t\t}\n\t\t} else {\n\t\t\t// If configured to do TLS handshake first\n\t\t\tif tlsFirst {\n\t\t\t\tif _, err := c.leafClientHandshakeIfNeeded(remote, opts); err != nil {\n\t\t\t\t\tc.mu.Unlock()\n\t\t\t\t\treturn nil\n\t\t\t\t}\n\t\t\t}\n\t\t\t// We need to wait for the info, but not for too long.\n\t\t\tc.nc.SetReadDeadline(time.Now().Add(infoTimeout))\n\t\t}\n\n\t\t// We will process the INFO from the readloop and finish by\n\t\t// sending the CONNECT and finish registration later.\n\t} else {\n\t\t// Send our info to the other side.\n\t\t// Remember the nonce we sent here for signatures, etc.\n\t\tc.nonce = make([]byte, nonceLen)\n\t\tcopy(c.nonce, nonce[:])\n\t\tinfo.Nonce = bytesToString(c.nonce)\n\t\tinfo.CID = c.cid\n\t\tproto := generateInfoJSON(info)\n\n\t\tvar pre []byte\n\t\t// We need first to check for \"TLS First\" fallback delay.\n\t\tif tlsFirstFallback > 0 {\n\t\t\t// We wait and see if we are getting any data. Since we did not send\n\t\t\t// the INFO protocol yet, only clients that use TLS first should be\n\t\t\t// sending data (the TLS handshake). We don't really check the content:\n\t\t\t// if it is a rogue agent and not an actual client performing the\n\t\t\t// TLS handshake, the error will be detected when performing the\n\t\t\t// handshake on our side.\n\t\t\tpre = make([]byte, 4)\n\t\t\tc.nc.SetReadDeadline(time.Now().Add(tlsFirstFallback))\n\t\t\tn, _ := io.ReadFull(c.nc, pre[:])\n\t\t\tc.nc.SetReadDeadline(time.Time{})\n\t\t\t// If we get any data (regardless of possible timeout), we will proceed\n\t\t\t// with the TLS handshake.\n\t\t\tif n > 0 {\n\t\t\t\tpre = pre[:n]\n\t\t\t} else {\n\t\t\t\t// We did not get anything so we will send the INFO protocol.\n\t\t\t\tpre = nil\n\t\t\t\t// Set the boolean to false for the rest of the function.\n\t\t\t\ttlsFirst = false\n\t\t\t}\n\t\t}\n\n\t\tif !tlsFirst {\n\t\t\t// We have to send from this go routine because we may\n\t\t\t// have to block for TLS handshake before we start our\n\t\t\t// writeLoop go routine. The other side needs to receive\n\t\t\t// this before it can initiate the TLS handshake..\n\t\t\tc.sendProtoNow(proto)\n\n\t\t\t// The above call could have marked the connection as closed (due to TCP error).\n\t\t\tif c.isClosed() {\n\t\t\t\tc.mu.Unlock()\n\t\t\t\tc.closeConnection(WriteError)\n\t\t\t\treturn nil\n\t\t\t}\n\t\t}\n\n\t\t// Check to see if we need to spin up TLS.\n\t\tif !c.isWebsocket() && info.TLSRequired {\n\t\t\t// If we have a prebuffer create a multi-reader.\n\t\t\tif len(pre) > 0 {\n\t\t\t\tc.nc = &tlsMixConn{c.nc, bytes.NewBuffer(pre)}\n\t\t\t}\n\t\t\t// Perform server-side TLS handshake.\n\t\t\tif err := c.doTLSServerHandshake(tlsHandshakeLeaf, opts.LeafNode.TLSConfig, opts.LeafNode.TLSTimeout, opts.LeafNode.TLSPinnedCerts); err != nil {\n\t\t\t\tc.mu.Unlock()\n\t\t\t\treturn nil\n\t\t\t}\n\t\t}\n\n\t\t// If the user wants the TLS handshake to occur first, now that it is\n\t\t// done, send the INFO protocol.\n\t\tif tlsFirst {\n\t\t\tc.flags.set(didTLSFirst)\n\t\t\tc.sendProtoNow(proto)\n\t\t\tif c.isClosed() {\n\t\t\t\tc.mu.Unlock()\n\t\t\t\tc.closeConnection(WriteError)\n\t\t\t\treturn nil\n\t\t\t}\n\t\t}\n\n\t\t// Leaf nodes will always require a CONNECT to let us know\n\t\t// when we are properly bound to an account.\n\t\t//\n\t\t// If compression is configured, we can't set the authTimer here because\n\t\t// it would cause the parser to fail any incoming protocol that is not a\n\t\t// CONNECT (and we need to exchange INFO protocols for compression\n\t\t// negotiation). So instead, use the ping timer until we are done with\n\t\t// negotiation and can set the auth timer.\n\t\ttimeout := secondsToDuration(opts.LeafNode.AuthTimeout)\n\t\tif needsCompression(opts.LeafNode.Compression.Mode) {\n\t\t\tc.ping.tmr = time.AfterFunc(timeout, func() {\n\t\t\t\tc.authTimeout()\n\t\t\t})\n\t\t} else {\n\t\t\tc.setAuthTimer(timeout)\n\t\t}\n\t}\n\n\t// Keep track in case server is shutdown before we can successfully register.\n\tif !s.addToTempClients(c.cid, c) {\n\t\tc.mu.Unlock()\n\t\tc.setNoReconnect()\n\t\tc.closeConnection(ServerShutdown)\n\t\treturn nil\n\t}\n\n\t// Spin up the read loop.\n\ts.startGoRoutine(func() { c.readLoop(preBuf) })\n\n\t// We will spin the write loop for solicited connections only\n\t// when processing the INFO and after switching to TLS if needed.\n\tif !solicited {\n\t\ts.startGoRoutine(func() { c.writeLoop() })\n\t}\n\n\tc.mu.Unlock()\n\n\treturn c\n}\n\n// Will perform the client-side TLS handshake if needed. Assumes that this\n// is called by the solicit side (remote will be non nil). Returns `true`\n// if TLS is required, `false` otherwise.\n// Lock held on entry.\nfunc (c *client) leafClientHandshakeIfNeeded(remote *leafNodeCfg, opts *Options) (bool, error) {\n\t// Check if TLS is required and gather TLS config variables.\n\ttlsRequired, tlsConfig, tlsName, tlsTimeout := c.leafNodeGetTLSConfigForSolicit(remote)\n\tif !tlsRequired {\n\t\treturn false, nil\n\t}\n\n\t// If TLS required, peform handshake.\n\t// Get the URL that was used to connect to the remote server.\n\trURL := remote.getCurrentURL()\n\n\t// Perform the client-side TLS handshake.\n\tif resetTLSName, err := c.doTLSClientHandshake(tlsHandshakeLeaf, rURL, tlsConfig, tlsName, tlsTimeout, opts.LeafNode.TLSPinnedCerts); err != nil {\n\t\t// Check if we need to reset the remote's TLS name.\n\t\tif resetTLSName {\n\t\t\tremote.Lock()\n\t\t\tremote.tlsName = _EMPTY_\n\t\t\tremote.Unlock()\n\t\t}\n\t\treturn false, err\n\t}\n\treturn true, nil\n}\n\nfunc (c *client) processLeafnodeInfo(info *Info) {\n\tc.mu.Lock()\n\tif c.leaf == nil || c.isClosed() {\n\t\tc.mu.Unlock()\n\t\treturn\n\t}\n\ts := c.srv\n\topts := s.getOpts()\n\tremote := c.leaf.remote\n\tdidSolicit := remote != nil\n\tfirstINFO := !c.flags.isSet(infoReceived)\n\n\t// In case of websocket, the TLS handshake has been already done.\n\t// So check only for non websocket connections and for configurations\n\t// where the TLS Handshake was not done first.\n\tif didSolicit && !c.flags.isSet(handshakeComplete) && !c.isWebsocket() && !remote.TLSHandshakeFirst {\n\t\t// If the server requires TLS, we need to set this in the remote\n\t\t// otherwise if there is no TLS configuration block for the remote,\n\t\t// the solicit side will not attempt to perform the TLS handshake.\n\t\tif firstINFO && info.TLSRequired {\n\t\t\tremote.TLS = true\n\t\t}\n\t\tif _, err := c.leafClientHandshakeIfNeeded(remote, opts); err != nil {\n\t\t\tc.mu.Unlock()\n\t\t\treturn\n\t\t}\n\t}\n\n\t// Check for compression, unless already done.\n\tif firstINFO && !c.flags.isSet(compressionNegotiated) {\n\t\t// Prevent from getting back here.\n\t\tc.flags.set(compressionNegotiated)\n\n\t\tvar co *CompressionOpts\n\t\tif !didSolicit {\n\t\t\tco = &opts.LeafNode.Compression\n\t\t} else {\n\t\t\tco = &remote.Compression\n\t\t}\n\t\tif needsCompression(co.Mode) {\n\t\t\t// Release client lock since following function will need server lock.\n\t\t\tc.mu.Unlock()\n\t\t\tcompress, err := s.negotiateLeafCompression(c, didSolicit, info.Compression, co)\n\t\t\tif err != nil {\n\t\t\t\tc.sendErrAndErr(err.Error())\n\t\t\t\tc.closeConnection(ProtocolViolation)\n\t\t\t\treturn\n\t\t\t}\n\t\t\tif compress {\n\t\t\t\t// Done for now, will get back another INFO protocol...\n\t\t\t\treturn\n\t\t\t}\n\t\t\t// No compression because one side does not want/can't, so proceed.\n\t\t\tc.mu.Lock()\n\t\t\t// Check that the connection did not close if the lock was released.\n\t\t\tif c.isClosed() {\n\t\t\t\tc.mu.Unlock()\n\t\t\t\treturn\n\t\t\t}\n\t\t} else {\n\t\t\t// Coming from an old server, the Compression field would be the empty\n\t\t\t// string. For servers that are configured with CompressionNotSupported,\n\t\t\t// this makes them behave as old servers.\n\t\t\tif info.Compression == _EMPTY_ || co.Mode == CompressionNotSupported {\n\t\t\t\tc.leaf.compression = CompressionNotSupported\n\t\t\t} else {\n\t\t\t\tc.leaf.compression = CompressionOff\n\t\t\t}\n\t\t}\n\t\t// Accepting side does not normally process an INFO protocol during\n\t\t// initial connection handshake. So we keep it consistent by returning\n\t\t// if we are not soliciting.\n\t\tif !didSolicit {\n\t\t\t// If we had created the ping timer instead of the auth timer, we will\n\t\t\t// clear the ping timer and set the auth timer now that the compression\n\t\t\t// negotiation is done.\n\t\t\tif info.Compression != _EMPTY_ && c.ping.tmr != nil {\n\t\t\t\tclearTimer(&c.ping.tmr)\n\t\t\t\tc.setAuthTimer(secondsToDuration(opts.LeafNode.AuthTimeout))\n\t\t\t}\n\t\t\tc.mu.Unlock()\n\t\t\treturn\n\t\t}\n\t\t// Fall through and process the INFO protocol as usual.\n\t}\n\n\t// Note: For now, only the initial INFO has a nonce. We\n\t// will probably do auto key rotation at some point.\n\tif firstINFO {\n\t\t// Mark that the INFO protocol has been received.\n\t\tc.flags.set(infoReceived)\n\t\t// Prevent connecting to non leafnode port. Need to do this only for\n\t\t// the first INFO, not for async INFO updates...\n\t\t//\n\t\t// Content of INFO sent by the server when accepting a tcp connection.\n\t\t// -------------------------------------------------------------------\n\t\t// Listen Port Of | CID | ClientConnectURLs | LeafNodeURLs | Gateway |\n\t\t// -------------------------------------------------------------------\n\t\t//      CLIENT    |  X* |        X**        |              |         |\n\t\t//      ROUTE     |     |        X**        |      X***    |         |\n\t\t//     GATEWAY    |     |                   |              |    X    |\n\t\t//     LEAFNODE   |  X  |                   |       X      |         |\n\t\t// -------------------------------------------------------------------\n\t\t// *   Not on older servers.\n\t\t// **  Not if \"no advertise\" is enabled.\n\t\t// *** Not if leafnode's \"no advertise\" is enabled.\n\t\t//\n\t\t// As seen from above, a solicited LeafNode connection should receive\n\t\t// from the remote server an INFO with CID and LeafNodeURLs. Anything\n\t\t// else should be considered an attempt to connect to a wrong port.\n\t\tif didSolicit && (info.CID == 0 || info.LeafNodeURLs == nil) {\n\t\t\tc.mu.Unlock()\n\t\t\tc.Errorf(ErrConnectedToWrongPort.Error())\n\t\t\tc.closeConnection(WrongPort)\n\t\t\treturn\n\t\t}\n\t\t// Reject a cluster that contains spaces.\n\t\tif info.Cluster != _EMPTY_ && strings.Contains(info.Cluster, \" \") {\n\t\t\tc.mu.Unlock()\n\t\t\tc.sendErrAndErr(ErrClusterNameHasSpaces.Error())\n\t\t\tc.closeConnection(ProtocolViolation)\n\t\t\treturn\n\t\t}\n\t\t// Capture a nonce here.\n\t\tc.nonce = []byte(info.Nonce)\n\t\tif info.TLSRequired && didSolicit {\n\t\t\tremote.TLS = true\n\t\t}\n\t\tsupportsHeaders := c.srv.supportsHeaders()\n\t\tc.headers = supportsHeaders && info.Headers\n\n\t\t// Remember the remote server.\n\t\t// Pre 2.2.0 servers are not sending their server name.\n\t\t// In that case, use info.ID, which, for those servers, matches\n\t\t// the content of the field `Name` in the leafnode CONNECT protocol.\n\t\tif info.Name == _EMPTY_ {\n\t\t\tc.leaf.remoteServer = info.ID\n\t\t} else {\n\t\t\tc.leaf.remoteServer = info.Name\n\t\t}\n\t\tc.leaf.remoteDomain = info.Domain\n\t\tc.leaf.remoteCluster = info.Cluster\n\t\t// We send the protocol version in the INFO protocol.\n\t\t// Keep track of it, so we know if this connection supports message\n\t\t// tracing for instance.\n\t\tc.opts.Protocol = info.Proto\n\t}\n\n\t// For both initial INFO and async INFO protocols, Possibly\n\t// update our list of remote leafnode URLs we can connect to.\n\tif didSolicit && (len(info.LeafNodeURLs) > 0 || len(info.WSConnectURLs) > 0) {\n\t\t// Consider the incoming array as the most up-to-date\n\t\t// representation of the remote cluster's list of URLs.\n\t\tc.updateLeafNodeURLs(info)\n\t}\n\n\t// Check to see if we have permissions updates here.\n\tif info.Import != nil || info.Export != nil {\n\t\tperms := &Permissions{\n\t\t\tPublish:   info.Export,\n\t\t\tSubscribe: info.Import,\n\t\t}\n\t\t// Check if we have local deny clauses that we need to merge.\n\t\tif remote := c.leaf.remote; remote != nil {\n\t\t\tif len(remote.DenyExports) > 0 {\n\t\t\t\tif perms.Publish == nil {\n\t\t\t\t\tperms.Publish = &SubjectPermission{}\n\t\t\t\t}\n\t\t\t\tperms.Publish.Deny = append(perms.Publish.Deny, remote.DenyExports...)\n\t\t\t}\n\t\t\tif len(remote.DenyImports) > 0 {\n\t\t\t\tif perms.Subscribe == nil {\n\t\t\t\t\tperms.Subscribe = &SubjectPermission{}\n\t\t\t\t}\n\t\t\t\tperms.Subscribe.Deny = append(perms.Subscribe.Deny, remote.DenyImports...)\n\t\t\t}\n\t\t}\n\t\tc.setPermissions(perms)\n\t}\n\n\tvar resumeConnect, checkSyncConsumers bool\n\n\t// If this is a remote connection and this is the first INFO protocol,\n\t// then we need to finish the connect process by sending CONNECT, etc..\n\tif firstINFO && didSolicit {\n\t\t// Clear deadline that was set in createLeafNode while waiting for the INFO.\n\t\tc.nc.SetDeadline(time.Time{})\n\t\tresumeConnect = true\n\t} else if !firstINFO && didSolicit {\n\t\tc.leaf.remoteAccName = info.RemoteAccount\n\t\tcheckSyncConsumers = info.JetStream\n\t}\n\n\t// Check if we have the remote account information and if so make sure it's stored.\n\tif info.RemoteAccount != _EMPTY_ {\n\t\ts.leafRemoteAccounts.Store(c.acc.Name, info.RemoteAccount)\n\t}\n\tc.mu.Unlock()\n\n\tfinishConnect := info.ConnectInfo\n\tif resumeConnect && s != nil {\n\t\ts.leafNodeResumeConnectProcess(c)\n\t\tif !info.InfoOnConnect {\n\t\t\tfinishConnect = true\n\t\t}\n\t}\n\tif finishConnect {\n\t\ts.leafNodeFinishConnectProcess(c)\n\t}\n\n\t// If we have JS enabled and so does the other side, we will\n\t// check to see if we need to kick any internal source or mirror consumers.\n\tif checkSyncConsumers {\n\t\ts.checkInternalSyncConsumers(c.acc, info.Domain)\n\t}\n}\n\nfunc (s *Server) negotiateLeafCompression(c *client, didSolicit bool, infoCompression string, co *CompressionOpts) (bool, error) {\n\t// Negotiate the appropriate compression mode (or no compression)\n\tcm, err := selectCompressionMode(co.Mode, infoCompression)\n\tif err != nil {\n\t\treturn false, err\n\t}\n\tc.mu.Lock()\n\t// For \"auto\" mode, set the initial compression mode based on RTT\n\tif cm == CompressionS2Auto {\n\t\tif c.rttStart.IsZero() {\n\t\t\tc.rtt = computeRTT(c.start)\n\t\t}\n\t\tcm = selectS2AutoModeBasedOnRTT(c.rtt, co.RTTThresholds)\n\t}\n\t// Keep track of the negotiated compression mode.\n\tc.leaf.compression = cm\n\tcid := c.cid\n\tvar nonce string\n\tif !didSolicit {\n\t\tnonce = bytesToString(c.nonce)\n\t}\n\tc.mu.Unlock()\n\n\tif !needsCompression(cm) {\n\t\treturn false, nil\n\t}\n\n\t// If we end-up doing compression...\n\n\t// Generate an INFO with the chosen compression mode.\n\ts.mu.Lock()\n\tinfo := s.copyLeafNodeInfo()\n\tinfo.Compression, info.CID, info.Nonce = compressionModeForInfoProtocol(co, cm), cid, nonce\n\tinfoProto := generateInfoJSON(info)\n\ts.mu.Unlock()\n\n\t// If we solicited, then send this INFO protocol BEFORE switching\n\t// to compression writer. However, if we did not, we send it after.\n\tc.mu.Lock()\n\tif didSolicit {\n\t\tc.enqueueProto(infoProto)\n\t\t// Make sure it is completely flushed (the pending bytes goes to\n\t\t// 0) before proceeding.\n\t\tfor c.out.pb > 0 && !c.isClosed() {\n\t\t\tc.flushOutbound()\n\t\t}\n\t}\n\t// This is to notify the readLoop that it should switch to a\n\t// (de)compression reader.\n\tc.in.flags.set(switchToCompression)\n\t// Create the compress writer before queueing the INFO protocol for\n\t// a route that did not solicit. It will make sure that that proto\n\t// is sent with compression on.\n\tc.out.cw = s2.NewWriter(nil, s2WriterOptions(cm)...)\n\tif !didSolicit {\n\t\tc.enqueueProto(infoProto)\n\t}\n\tc.mu.Unlock()\n\treturn true, nil\n}\n\n// When getting a leaf node INFO protocol, use the provided\n// array of urls to update the list of possible endpoints.\nfunc (c *client) updateLeafNodeURLs(info *Info) {\n\tcfg := c.leaf.remote\n\tcfg.Lock()\n\tdefer cfg.Unlock()\n\n\t// We have ensured that if a remote has a WS scheme, then all are.\n\t// So check if first is WS, then add WS URLs, otherwise, add non WS ones.\n\tif len(cfg.URLs) > 0 && isWSURL(cfg.URLs[0]) {\n\t\t// It does not really matter if we use \"ws://\" or \"wss://\" here since\n\t\t// we will have already marked that the remote should use TLS anyway.\n\t\t// But use proper scheme for log statements, etc...\n\t\tproto := wsSchemePrefix\n\t\tif cfg.TLS {\n\t\t\tproto = wsSchemePrefixTLS\n\t\t}\n\t\tc.doUpdateLNURLs(cfg, proto, info.WSConnectURLs)\n\t\treturn\n\t}\n\tc.doUpdateLNURLs(cfg, \"nats-leaf\", info.LeafNodeURLs)\n}\n\nfunc (c *client) doUpdateLNURLs(cfg *leafNodeCfg, scheme string, URLs []string) {\n\tcfg.urls = make([]*url.URL, 0, 1+len(URLs))\n\t// Add the ones we receive in the protocol\n\tfor _, surl := range URLs {\n\t\turl, err := url.Parse(fmt.Sprintf(\"%s://%s\", scheme, surl))\n\t\tif err != nil {\n\t\t\t// As per below, the URLs we receive should not have contained URL info, so this should be safe to log.\n\t\t\tc.Errorf(\"Error parsing url %q: %v\", surl, err)\n\t\t\tcontinue\n\t\t}\n\t\t// Do not add if it's the same as what we already have configured.\n\t\tvar dup bool\n\t\tfor _, u := range cfg.URLs {\n\t\t\t// URLs that we receive never have user info, but the\n\t\t\t// ones that were configured may have. Simply compare\n\t\t\t// host and port to decide if they are equal or not.\n\t\t\tif url.Host == u.Host && url.Port() == u.Port() {\n\t\t\t\tdup = true\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\tif !dup {\n\t\t\tcfg.urls = append(cfg.urls, url)\n\t\t\tcfg.saveTLSHostname(url)\n\t\t}\n\t}\n\t// Add the configured one\n\tcfg.urls = append(cfg.urls, cfg.URLs...)\n}\n\n// Similar to setInfoHostPortAndGenerateJSON, but for leafNodeInfo.\nfunc (s *Server) setLeafNodeInfoHostPortAndIP() error {\n\topts := s.getOpts()\n\tif opts.LeafNode.Advertise != _EMPTY_ {\n\t\tadvHost, advPort, err := parseHostPort(opts.LeafNode.Advertise, opts.LeafNode.Port)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\ts.leafNodeInfo.Host = advHost\n\t\ts.leafNodeInfo.Port = advPort\n\t} else {\n\t\ts.leafNodeInfo.Host = opts.LeafNode.Host\n\t\ts.leafNodeInfo.Port = opts.LeafNode.Port\n\t\t// If the host is \"0.0.0.0\" or \"::\" we need to resolve to a public IP.\n\t\t// This will return at most 1 IP.\n\t\thostIsIPAny, ips, err := s.getNonLocalIPsIfHostIsIPAny(s.leafNodeInfo.Host, false)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif hostIsIPAny {\n\t\t\tif len(ips) == 0 {\n\t\t\t\ts.Errorf(\"Could not find any non-local IP for leafnode's listen specification %q\",\n\t\t\t\t\ts.leafNodeInfo.Host)\n\t\t\t} else {\n\t\t\t\t// Take the first from the list...\n\t\t\t\ts.leafNodeInfo.Host = ips[0]\n\t\t\t}\n\t\t}\n\t}\n\t// Use just host:port for the IP\n\ts.leafNodeInfo.IP = net.JoinHostPort(s.leafNodeInfo.Host, strconv.Itoa(s.leafNodeInfo.Port))\n\tif opts.LeafNode.Advertise != _EMPTY_ {\n\t\ts.Noticef(\"Advertise address for leafnode is set to %s\", s.leafNodeInfo.IP)\n\t}\n\treturn nil\n}\n\n// Add the connection to the map of leaf nodes.\n// If `checkForDup` is true (invoked when a leafnode is accepted), then we check\n// if a connection already exists for the same server name and account.\n// That can happen when the remote is attempting to reconnect while the accepting\n// side did not detect the connection as broken yet.\n// But it can also happen when there is a misconfiguration and the remote is\n// creating two (or more) connections that bind to the same account on the accept\n// side.\n// When a duplicate is found, the new connection is accepted and the old is closed\n// (this solves the stale connection situation). An error is returned to help the\n// remote detect the misconfiguration when the duplicate is the result of that\n// misconfiguration.\nfunc (s *Server) addLeafNodeConnection(c *client, srvName, clusterName string, checkForDup bool) {\n\tvar accName string\n\tc.mu.Lock()\n\tcid := c.cid\n\tacc := c.acc\n\tif acc != nil {\n\t\taccName = acc.Name\n\t}\n\tmyRemoteDomain := c.leaf.remoteDomain\n\tmySrvName := c.leaf.remoteServer\n\tremoteAccName := c.leaf.remoteAccName\n\tmyClustName := c.leaf.remoteCluster\n\tsolicited := c.leaf.remote != nil\n\tc.mu.Unlock()\n\n\tvar old *client\n\ts.mu.Lock()\n\t// We check for empty because in some test we may send empty CONNECT{}\n\tif checkForDup && srvName != _EMPTY_ {\n\t\tfor _, ol := range s.leafs {\n\t\t\tol.mu.Lock()\n\t\t\t// We care here only about non solicited Leafnode. This function\n\t\t\t// is more about replacing stale connections than detecting loops.\n\t\t\t// We have code for the loop detection elsewhere, which also delays\n\t\t\t// attempt to reconnect.\n\t\t\tif !ol.isSolicitedLeafNode() && ol.leaf.remoteServer == srvName &&\n\t\t\t\tol.leaf.remoteCluster == clusterName && ol.acc.Name == accName &&\n\t\t\t\tremoteAccName != _EMPTY_ && ol.leaf.remoteAccName == remoteAccName {\n\t\t\t\told = ol\n\t\t\t}\n\t\t\tol.mu.Unlock()\n\t\t\tif old != nil {\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t}\n\t// Store new connection in the map\n\ts.leafs[cid] = c\n\ts.mu.Unlock()\n\ts.removeFromTempClients(cid)\n\n\t// If applicable, evict the old one.\n\tif old != nil {\n\t\told.sendErrAndErr(DuplicateRemoteLeafnodeConnection.String())\n\t\told.closeConnection(DuplicateRemoteLeafnodeConnection)\n\t\tc.Warnf(\"Replacing connection from same server\")\n\t}\n\n\tsrvDecorated := func() string {\n\t\tif myClustName == _EMPTY_ {\n\t\t\treturn mySrvName\n\t\t}\n\t\treturn fmt.Sprintf(\"%s/%s\", mySrvName, myClustName)\n\t}\n\n\topts := s.getOpts()\n\tsysAcc := s.SystemAccount()\n\tjs := s.getJetStream()\n\tvar meta *raft\n\tif js != nil {\n\t\tif mg := js.getMetaGroup(); mg != nil {\n\t\t\tmeta = mg.(*raft)\n\t\t}\n\t}\n\tblockMappingOutgoing := false\n\t// Deny (non domain) JetStream API traffic unless system account is shared\n\t// and domain names are identical and extending is not disabled\n\n\t// Check if backwards compatibility has been enabled and needs to be acted on\n\tforceSysAccDeny := false\n\tif len(opts.JsAccDefaultDomain) > 0 {\n\t\tif acc == sysAcc {\n\t\t\tfor _, d := range opts.JsAccDefaultDomain {\n\t\t\t\tif d == _EMPTY_ {\n\t\t\t\t\t// Extending JetStream via leaf node is mutually exclusive with a domain mapping to the empty/default domain.\n\t\t\t\t\t// As soon as one mapping to \"\" is found, disable the ability to extend JS via a leaf node.\n\t\t\t\t\tc.Noticef(\"Not extending remote JetStream domain %q due to presence of empty default domain\", myRemoteDomain)\n\t\t\t\t\tforceSysAccDeny = true\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t} else if domain, ok := opts.JsAccDefaultDomain[accName]; ok && domain == _EMPTY_ {\n\t\t\t// for backwards compatibility with old setups that do not have a domain name set\n\t\t\tc.Debugf(\"Skipping deny %q for account %q due to default domain\", jsAllAPI, accName)\n\t\t\treturn\n\t\t}\n\t}\n\n\t// If the server has JS disabled, it may still be part of a JetStream that could be extended.\n\t// This is either signaled by js being disabled and a domain set,\n\t// or in cases where no domain name exists, an extension hint is set.\n\t// However, this is only relevant in mixed setups.\n\t//\n\t// If the system account connects but default domains are present, JetStream can't be extended.\n\tif opts.JetStreamDomain != myRemoteDomain || (!opts.JetStream && (opts.JetStreamDomain == _EMPTY_ && opts.JetStreamExtHint != jsWillExtend)) ||\n\t\tsysAcc == nil || acc == nil || forceSysAccDeny {\n\t\t// If domain names mismatch always deny. This applies to system accounts as well as non system accounts.\n\t\t// Not having a system account, account or JetStream disabled is considered a mismatch as well.\n\t\tif acc != nil && acc == sysAcc {\n\t\t\tc.Noticef(\"System account connected from %s\", srvDecorated())\n\t\t\tc.Noticef(\"JetStream not extended, domains differ\")\n\t\t\tc.mergeDenyPermissionsLocked(both, denyAllJs)\n\t\t\t// When a remote with a system account is present in a server, unless otherwise disabled, the server will be\n\t\t\t// started in observer mode. Now that it is clear that this not used, turn the observer mode off.\n\t\t\tif solicited && meta != nil && meta.IsObserver() {\n\t\t\t\tmeta.setObserver(false, extNotExtended)\n\t\t\t\tc.Debugf(\"Turning JetStream metadata controller Observer Mode off\")\n\t\t\t\t// Take note that the domain was not extended to avoid this state from startup.\n\t\t\t\twritePeerState(js.config.StoreDir, meta.currentPeerState())\n\t\t\t\t// Meta controller can't be leader yet.\n\t\t\t\t// Yet it is possible that due to observer mode every server already stopped campaigning.\n\t\t\t\t// Therefore this server needs to be kicked into campaigning gear explicitly.\n\t\t\t\tmeta.Campaign()\n\t\t\t}\n\t\t} else {\n\t\t\tc.Noticef(\"JetStream using domains: local %q, remote %q\", opts.JetStreamDomain, myRemoteDomain)\n\t\t\tc.mergeDenyPermissionsLocked(both, denyAllClientJs)\n\t\t}\n\t\tblockMappingOutgoing = true\n\t} else if acc == sysAcc {\n\t\t// system account and same domain\n\t\ts.sys.client.Noticef(\"Extending JetStream domain %q as System Account connected from server %s\",\n\t\t\tmyRemoteDomain, srvDecorated())\n\t\t// In an extension use case, pin leadership to server remotes connect to.\n\t\t// Therefore, server with a remote that are not already in observer mode, need to be put into it.\n\t\tif solicited && meta != nil && !meta.IsObserver() {\n\t\t\tmeta.setObserver(true, extExtended)\n\t\t\tc.Debugf(\"Turning JetStream metadata controller Observer Mode on - System Account Connected\")\n\t\t\t// Take note that the domain was not extended to avoid this state next startup.\n\t\t\twritePeerState(js.config.StoreDir, meta.currentPeerState())\n\t\t\t// If this server is the leader already, step down so a new leader can be elected (that is not an observer)\n\t\t\tmeta.StepDown()\n\t\t}\n\t} else {\n\t\t// This deny is needed in all cases (system account shared or not)\n\t\t// If the system account is shared, jsAllAPI traffic will go through the system account.\n\t\t// So in order to prevent duplicate delivery (from system and actual account) suppress it on the account.\n\t\t// If the system account is NOT shared, jsAllAPI traffic has no business\n\t\tc.Debugf(\"Adding deny %+v for account %q\", denyAllClientJs, accName)\n\t\tc.mergeDenyPermissionsLocked(both, denyAllClientJs)\n\t}\n\t// If we have a specified JetStream domain we will want to add a mapping to\n\t// allow access cross domain for each non-system account.\n\tif opts.JetStreamDomain != _EMPTY_ && opts.JetStream && acc != nil && acc != sysAcc {\n\t\tfor src, dest := range generateJSMappingTable(opts.JetStreamDomain) {\n\t\t\tif err := acc.AddMapping(src, dest); err != nil {\n\t\t\t\tc.Debugf(\"Error adding JetStream domain mapping: %s\", err.Error())\n\t\t\t} else {\n\t\t\t\tc.Debugf(\"Adding JetStream Domain Mapping %q -> %s to account %q\", src, dest, accName)\n\t\t\t}\n\t\t}\n\t\tif blockMappingOutgoing {\n\t\t\tsrc := fmt.Sprintf(jsDomainAPI, opts.JetStreamDomain)\n\t\t\t// make sure that messages intended for this domain, do not leave the cluster via this leaf node connection\n\t\t\t// This is a guard against a miss-config with two identical domain names and will only cover some forms\n\t\t\t// of this issue, not all of them.\n\t\t\t// This guards against a hub and a spoke having the same domain name.\n\t\t\t// But not two spokes having the same one and the request coming from the hub.\n\t\t\tc.mergeDenyPermissionsLocked(pub, []string{src})\n\t\t\tc.Debugf(\"Adding deny %q for outgoing messages to account %q\", src, accName)\n\t\t}\n\t}\n}\n\nfunc (s *Server) removeLeafNodeConnection(c *client) {\n\tc.mu.Lock()\n\tcid := c.cid\n\tif c.leaf != nil {\n\t\tif c.leaf.tsubt != nil {\n\t\t\tc.leaf.tsubt.Stop()\n\t\t\tc.leaf.tsubt = nil\n\t\t}\n\t\tif c.leaf.gwSub != nil {\n\t\t\ts.gwLeafSubs.Remove(c.leaf.gwSub)\n\t\t\t// We need to set this to nil for GC to release the connection\n\t\t\tc.leaf.gwSub = nil\n\t\t}\n\t}\n\tc.mu.Unlock()\n\ts.mu.Lock()\n\tdelete(s.leafs, cid)\n\ts.mu.Unlock()\n\ts.removeFromTempClients(cid)\n}\n\n// Connect information for solicited leafnodes.\ntype leafConnectInfo struct {\n\tVersion   string   `json:\"version,omitempty\"`\n\tNkey      string   `json:\"nkey,omitempty\"`\n\tJWT       string   `json:\"jwt,omitempty\"`\n\tSig       string   `json:\"sig,omitempty\"`\n\tUser      string   `json:\"user,omitempty\"`\n\tPass      string   `json:\"pass,omitempty\"`\n\tToken     string   `json:\"auth_token,omitempty\"`\n\tID        string   `json:\"server_id,omitempty\"`\n\tDomain    string   `json:\"domain,omitempty\"`\n\tName      string   `json:\"name,omitempty\"`\n\tHub       bool     `json:\"is_hub,omitempty\"`\n\tCluster   string   `json:\"cluster,omitempty\"`\n\tHeaders   bool     `json:\"headers,omitempty\"`\n\tJetStream bool     `json:\"jetstream,omitempty\"`\n\tDenyPub   []string `json:\"deny_pub,omitempty\"`\n\n\t// There was an existing field called:\n\t// >> Comp bool `json:\"compression,omitempty\"`\n\t// that has never been used. With support for compression, we now need\n\t// a field that is a string. So we use a different json tag:\n\tCompression string `json:\"compress_mode,omitempty\"`\n\n\t// Just used to detect wrong connection attempts.\n\tGateway string `json:\"gateway,omitempty\"`\n\n\t// Tells the accept side which account the remote is binding to.\n\tRemoteAccount string `json:\"remote_account,omitempty\"`\n\n\t// The accept side of a LEAF connection, unlike ROUTER and GATEWAY, receives\n\t// only the CONNECT protocol, and no INFO. So we need to send the protocol\n\t// version as part of the CONNECT. It will indicate if a connection supports\n\t// some features, such as message tracing.\n\t// We use `protocol` as the JSON tag, so this is automatically unmarshal'ed\n\t// in the low level process CONNECT.\n\tProto int `json:\"protocol,omitempty\"`\n}\n\n// processLeafNodeConnect will process the inbound connect args.\n// Once we are here we are bound to an account, so can send any interest that\n// we would have to the other side.\nfunc (c *client) processLeafNodeConnect(s *Server, arg []byte, lang string) error {\n\t// Way to detect clients that incorrectly connect to the route listen\n\t// port. Client provided \"lang\" in the CONNECT protocol while LEAFNODEs don't.\n\tif lang != _EMPTY_ {\n\t\tc.sendErrAndErr(ErrClientConnectedToLeafNodePort.Error())\n\t\tc.closeConnection(WrongPort)\n\t\treturn ErrClientConnectedToLeafNodePort\n\t}\n\n\t// Unmarshal as a leaf node connect protocol\n\tproto := &leafConnectInfo{}\n\tif err := json.Unmarshal(arg, proto); err != nil {\n\t\treturn err\n\t}\n\n\t// Reject a cluster that contains spaces.\n\tif proto.Cluster != _EMPTY_ && strings.Contains(proto.Cluster, \" \") {\n\t\tc.sendErrAndErr(ErrClusterNameHasSpaces.Error())\n\t\tc.closeConnection(ProtocolViolation)\n\t\treturn ErrClusterNameHasSpaces\n\t}\n\n\t// Check for cluster name collisions.\n\tif cn := s.cachedClusterName(); cn != _EMPTY_ && proto.Cluster != _EMPTY_ && proto.Cluster == cn {\n\t\tc.sendErrAndErr(ErrLeafNodeHasSameClusterName.Error())\n\t\tc.closeConnection(ClusterNamesIdentical)\n\t\treturn ErrLeafNodeHasSameClusterName\n\t}\n\n\t// Reject if this has Gateway which means that it would be from a gateway\n\t// connection that incorrectly connects to the leafnode port.\n\tif proto.Gateway != _EMPTY_ {\n\t\terrTxt := fmt.Sprintf(\"Rejecting connection from gateway %q on the leafnode port\", proto.Gateway)\n\t\tc.Errorf(errTxt)\n\t\tc.sendErr(errTxt)\n\t\tc.closeConnection(WrongGateway)\n\t\treturn ErrWrongGateway\n\t}\n\n\tif mv := s.getOpts().LeafNode.MinVersion; mv != _EMPTY_ {\n\t\tmajor, minor, update, _ := versionComponents(mv)\n\t\tif !versionAtLeast(proto.Version, major, minor, update) {\n\t\t\t// We are going to send back an INFO because otherwise recent\n\t\t\t// versions of the remote server would simply break the connection\n\t\t\t// after 2 seconds if not receiving it. Instead, we want the\n\t\t\t// other side to just \"stall\" until we finish waiting for the holding\n\t\t\t// period and close the connection below.\n\t\t\ts.sendPermsAndAccountInfo(c)\n\t\t\tc.sendErrAndErr(fmt.Sprintf(\"connection rejected since minimum version required is %q\", mv))\n\t\t\tselect {\n\t\t\tcase <-c.srv.quitCh:\n\t\t\tcase <-time.After(leafNodeWaitBeforeClose):\n\t\t\t}\n\t\t\tc.closeConnection(MinimumVersionRequired)\n\t\t\treturn ErrMinimumVersionRequired\n\t\t}\n\t}\n\n\t// Check if this server supports headers.\n\tsupportHeaders := c.srv.supportsHeaders()\n\n\tc.mu.Lock()\n\t// Leaf Nodes do not do echo or verbose or pedantic.\n\tc.opts.Verbose = false\n\tc.opts.Echo = false\n\tc.opts.Pedantic = false\n\t// This inbound connection will be marked as supporting headers if this server\n\t// support headers and the remote has sent in the CONNECT protocol that it does\n\t// support headers too.\n\tc.headers = supportHeaders && proto.Headers\n\t// If the compression level is still not set, set it based on what has been\n\t// given to us in the CONNECT protocol.\n\tif c.leaf.compression == _EMPTY_ {\n\t\t// But if proto.Compression is _EMPTY_, set it to CompressionNotSupported\n\t\tif proto.Compression == _EMPTY_ {\n\t\t\tc.leaf.compression = CompressionNotSupported\n\t\t} else {\n\t\t\tc.leaf.compression = proto.Compression\n\t\t}\n\t}\n\n\t// Remember the remote server.\n\tc.leaf.remoteServer = proto.Name\n\t// Remember the remote account name\n\tc.leaf.remoteAccName = proto.RemoteAccount\n\n\t// If the other side has declared itself a hub, so we will take on the spoke role.\n\tif proto.Hub {\n\t\tc.leaf.isSpoke = true\n\t}\n\n\t// The soliciting side is part of a cluster.\n\tif proto.Cluster != _EMPTY_ {\n\t\tc.leaf.remoteCluster = proto.Cluster\n\t}\n\n\tc.leaf.remoteDomain = proto.Domain\n\n\t// When a leaf solicits a connection to a hub, the perms that it will use on the soliciting leafnode's\n\t// behalf are correct for them, but inside the hub need to be reversed since data is flowing in the opposite direction.\n\tif !c.isSolicitedLeafNode() && c.perms != nil {\n\t\tsp, pp := c.perms.sub, c.perms.pub\n\t\tc.perms.sub, c.perms.pub = pp, sp\n\t\tif c.opts.Import != nil {\n\t\t\tc.darray = c.opts.Import.Deny\n\t\t} else {\n\t\t\tc.darray = nil\n\t\t}\n\t}\n\n\t// Set the Ping timer\n\tc.setFirstPingTimer()\n\n\t// If we received pub deny permissions from the other end, merge with existing ones.\n\tc.mergeDenyPermissions(pub, proto.DenyPub)\n\n\tacc := c.acc\n\tc.mu.Unlock()\n\n\t// Register the cluster, even if empty, as long as we are acting as a hub.\n\tif !proto.Hub {\n\t\tacc.registerLeafNodeCluster(proto.Cluster)\n\t}\n\n\t// Add in the leafnode here since we passed through auth at this point.\n\ts.addLeafNodeConnection(c, proto.Name, proto.Cluster, true)\n\n\t// If we have permissions bound to this leafnode we need to send then back to the\n\t// origin server for local enforcement.\n\ts.sendPermsAndAccountInfo(c)\n\n\t// Create and initialize the smap since we know our bound account now.\n\t// This will send all registered subs too.\n\ts.initLeafNodeSmapAndSendSubs(c)\n\n\t// Announce the account connect event for a leaf node.\n\t// This will be a no-op as needed.\n\ts.sendLeafNodeConnect(c.acc)\n\n\t// If we have JS enabled and so does the other side, we will\n\t// check to see if we need to kick any internal source or mirror consumers.\n\tif proto.JetStream {\n\t\ts.checkInternalSyncConsumers(acc, proto.Domain)\n\t}\n\treturn nil\n}\n\n// checkInternalSyncConsumers\nfunc (s *Server) checkInternalSyncConsumers(acc *Account, remoteDomain string) {\n\t// Grab our js\n\tjs := s.getJetStream()\n\n\t// Only applicable if we have JS and the leafnode has JS as well.\n\t// We check for remote JS outside.\n\tif !js.isEnabled() || acc == nil {\n\t\treturn\n\t}\n\n\t// We will check all streams in our local account. They must be a leader and\n\t// be sourcing or mirroring. We will check the external config on the stream itself\n\t// if this is cross domain, or if the remote domain is empty, meaning we might be\n\t// extedning the system across this leafnode connection and hence we would be extending\n\t// our own domain.\n\tjsa := js.lookupAccount(acc)\n\tif jsa == nil {\n\t\treturn\n\t}\n\tvar streams []*stream\n\tjsa.mu.RLock()\n\tfor _, mset := range jsa.streams {\n\t\tmset.cfgMu.RLock()\n\t\t// We need to have a mirror or source defined.\n\t\t// We do not want to force another lock here to look for leader status,\n\t\t// so collect and after we release jsa will make sure.\n\t\tif mset.cfg.Mirror != nil || len(mset.cfg.Sources) > 0 {\n\t\t\tstreams = append(streams, mset)\n\t\t}\n\t\tmset.cfgMu.RUnlock()\n\t}\n\tjsa.mu.RUnlock()\n\n\t// Now loop through all candidates and check if we are the leader and have NOT\n\t// created the sync up consumer.\n\tfor _, mset := range streams {\n\t\tmset.retryDisconnectedSyncConsumers(remoteDomain)\n\t}\n}\n\n// Returns the remote cluster name. This is set only once so does not require a lock.\nfunc (c *client) remoteCluster() string {\n\tif c.leaf == nil {\n\t\treturn _EMPTY_\n\t}\n\treturn c.leaf.remoteCluster\n}\n\n// Sends back an info block to the soliciting leafnode to let it know about\n// its permission settings for local enforcement.\nfunc (s *Server) sendPermsAndAccountInfo(c *client) {\n\t// Copy\n\ts.mu.Lock()\n\tinfo := s.copyLeafNodeInfo()\n\ts.mu.Unlock()\n\tc.mu.Lock()\n\tinfo.CID = c.cid\n\tinfo.Import = c.opts.Import\n\tinfo.Export = c.opts.Export\n\tinfo.RemoteAccount = c.acc.Name\n\t// s.SystemAccount() uses an atomic operation and does not get the server lock, so this is safe.\n\tinfo.IsSystemAccount = c.acc == s.SystemAccount()\n\tinfo.ConnectInfo = true\n\tc.enqueueProto(generateInfoJSON(info))\n\tc.mu.Unlock()\n}\n\n// Snapshot the current subscriptions from the sublist into our smap which\n// we will keep updated from now on.\n// Also send the registered subscriptions.\nfunc (s *Server) initLeafNodeSmapAndSendSubs(c *client) {\n\tacc := c.acc\n\tif acc == nil {\n\t\tc.Debugf(\"Leafnode does not have an account bound\")\n\t\treturn\n\t}\n\t// Collect all account subs here.\n\t_subs := [1024]*subscription{}\n\tsubs := _subs[:0]\n\tims := []string{}\n\n\t// Hold the client lock otherwise there can be a race and miss some subs.\n\tc.mu.Lock()\n\tdefer c.mu.Unlock()\n\n\tacc.mu.RLock()\n\taccName := acc.Name\n\taccNTag := acc.nameTag\n\n\t// To make printing look better when no friendly name present.\n\tif accNTag != _EMPTY_ {\n\t\taccNTag = \"/\" + accNTag\n\t}\n\n\t// If we are solicited we only send interest for local clients.\n\tif c.isSpokeLeafNode() {\n\t\tacc.sl.localSubs(&subs, true)\n\t} else {\n\t\tacc.sl.All(&subs)\n\t}\n\n\t// Check if we have an existing service import reply.\n\tsiReply := copyBytes(acc.siReply)\n\n\t// Since leaf nodes only send on interest, if the bound\n\t// account has import services we need to send those over.\n\tfor isubj := range acc.imports.services {\n\t\tif c.isSpokeLeafNode() && !c.canSubscribe(isubj) {\n\t\t\tc.Debugf(\"Not permitted to import service %q on behalf of %s%s\", isubj, accName, accNTag)\n\t\t\tcontinue\n\t\t}\n\t\tims = append(ims, isubj)\n\t}\n\t// Likewise for mappings.\n\tfor _, m := range acc.mappings {\n\t\tif c.isSpokeLeafNode() && !c.canSubscribe(m.src) {\n\t\t\tc.Debugf(\"Not permitted to import mapping %q on behalf of %s%s\", m.src, accName, accNTag)\n\t\t\tcontinue\n\t\t}\n\t\tims = append(ims, m.src)\n\t}\n\n\t// Create a unique subject that will be used for loop detection.\n\tlds := acc.lds\n\tacc.mu.RUnlock()\n\n\t// Check if we have to create the LDS.\n\tif lds == _EMPTY_ {\n\t\tlds = leafNodeLoopDetectionSubjectPrefix + nuid.Next()\n\t\tacc.mu.Lock()\n\t\tacc.lds = lds\n\t\tacc.mu.Unlock()\n\t}\n\n\t// Now check for gateway interest. Leafnodes will put this into\n\t// the proper mode to propagate, but they are not held in the account.\n\tgwsa := [16]*client{}\n\tgws := gwsa[:0]\n\ts.getOutboundGatewayConnections(&gws)\n\tfor _, cgw := range gws {\n\t\tcgw.mu.Lock()\n\t\tgw := cgw.gw\n\t\tcgw.mu.Unlock()\n\t\tif gw != nil {\n\t\t\tif ei, _ := gw.outsim.Load(accName); ei != nil {\n\t\t\t\tif e := ei.(*outsie); e != nil && e.sl != nil {\n\t\t\t\t\te.sl.All(&subs)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tapplyGlobalRouting := s.gateway.enabled\n\tif c.isSpokeLeafNode() {\n\t\t// Add a fake subscription for this solicited leafnode connection\n\t\t// so that we can send back directly for mapped GW replies.\n\t\t// We need to keep track of this subscription so it can be removed\n\t\t// when the connection is closed so that the GC can release it.\n\t\tc.leaf.gwSub = &subscription{client: c, subject: []byte(gwReplyPrefix + \">\")}\n\t\tc.srv.gwLeafSubs.Insert(c.leaf.gwSub)\n\t}\n\n\t// Now walk the results and add them to our smap\n\trc := c.leaf.remoteCluster\n\tc.leaf.smap = make(map[string]int32)\n\tfor _, sub := range subs {\n\t\t// Check perms regardless of role.\n\t\tif c.perms != nil && !c.canSubscribe(string(sub.subject)) {\n\t\t\tc.Debugf(\"Not permitted to subscribe to %q on behalf of %s%s\", sub.subject, accName, accNTag)\n\t\t\tcontinue\n\t\t}\n\t\t// We ignore ourselves here.\n\t\t// Also don't add the subscription if it has a origin cluster and the\n\t\t// cluster name matches the one of the client we are sending to.\n\t\tif c != sub.client && (sub.origin == nil || (bytesToString(sub.origin) != rc)) {\n\t\t\tcount := int32(1)\n\t\t\tif len(sub.queue) > 0 && sub.qw > 0 {\n\t\t\t\tcount = sub.qw\n\t\t\t}\n\t\t\tc.leaf.smap[keyFromSub(sub)] += count\n\t\t\tif c.leaf.tsub == nil {\n\t\t\t\tc.leaf.tsub = make(map[*subscription]struct{})\n\t\t\t}\n\t\t\tc.leaf.tsub[sub] = struct{}{}\n\t\t}\n\t}\n\t// FIXME(dlc) - We need to update appropriately on an account claims update.\n\tfor _, isubj := range ims {\n\t\tc.leaf.smap[isubj]++\n\t}\n\t// If we have gateways enabled we need to make sure the other side sends us responses\n\t// that have been augmented from the original subscription.\n\t// TODO(dlc) - Should we lock this down more?\n\tif applyGlobalRouting {\n\t\tc.leaf.smap[oldGWReplyPrefix+\"*.>\"]++\n\t\tc.leaf.smap[gwReplyPrefix+\">\"]++\n\t}\n\t// Detect loops by subscribing to a specific subject and checking\n\t// if this sub is coming back to us.\n\tc.leaf.smap[lds]++\n\n\t// Check if we need to add an existing siReply to our map.\n\t// This will be a prefix so add on the wildcard.\n\tif siReply != nil {\n\t\twcsub := append(siReply, '>')\n\t\tc.leaf.smap[string(wcsub)]++\n\t}\n\t// Queue all protocols. There is no max pending limit for LN connection,\n\t// so we don't need chunking. The writes will happen from the writeLoop.\n\tvar b bytes.Buffer\n\tfor key, n := range c.leaf.smap {\n\t\tc.writeLeafSub(&b, key, n)\n\t}\n\tif b.Len() > 0 {\n\t\tc.enqueueProto(b.Bytes())\n\t}\n\tif c.leaf.tsub != nil {\n\t\t// Clear the tsub map after 5 seconds.\n\t\tc.leaf.tsubt = time.AfterFunc(5*time.Second, func() {\n\t\t\tc.mu.Lock()\n\t\t\tif c.leaf != nil {\n\t\t\t\tc.leaf.tsub = nil\n\t\t\t\tc.leaf.tsubt = nil\n\t\t\t}\n\t\t\tc.mu.Unlock()\n\t\t})\n\t}\n}\n\n// updateInterestForAccountOnGateway called from gateway code when processing RS+ and RS-.\nfunc (s *Server) updateInterestForAccountOnGateway(accName string, sub *subscription, delta int32) {\n\tacc, err := s.LookupAccount(accName)\n\tif acc == nil || err != nil {\n\t\ts.Debugf(\"No or bad account for %q, failed to update interest from gateway\", accName)\n\t\treturn\n\t}\n\tacc.updateLeafNodes(sub, delta)\n}\n\n// updateLeafNodes will make sure to update the account smap for the subscription.\n// Will also forward to all leaf nodes as needed.\nfunc (acc *Account) updateLeafNodes(sub *subscription, delta int32) {\n\tif acc == nil || sub == nil {\n\t\treturn\n\t}\n\n\t// We will do checks for no leafnodes and same cluster here inline and under the\n\t// general account read lock.\n\t// If we feel we need to update the leafnodes we will do that out of line to avoid\n\t// blocking routes or GWs.\n\n\tacc.mu.RLock()\n\t// First check if we even have leafnodes here.\n\tif acc.nleafs == 0 {\n\t\tacc.mu.RUnlock()\n\t\treturn\n\t}\n\n\t// Is this a loop detection subject.\n\tisLDS := bytes.HasPrefix(sub.subject, []byte(leafNodeLoopDetectionSubjectPrefix))\n\n\t// Capture the cluster even if its empty.\n\tvar cluster string\n\tif sub.origin != nil {\n\t\tcluster = bytesToString(sub.origin)\n\t}\n\n\t// If we have an isolated cluster we can return early, as long as it is not a loop detection subject.\n\t// Empty clusters will return false for the check.\n\tif !isLDS && acc.isLeafNodeClusterIsolated(cluster) {\n\t\tacc.mu.RUnlock()\n\t\treturn\n\t}\n\n\t// We can release the general account lock.\n\tacc.mu.RUnlock()\n\n\t// We can hold the list lock here to avoid having to copy a large slice.\n\tacc.lmu.RLock()\n\tdefer acc.lmu.RUnlock()\n\n\t// Do this once.\n\tsubject := string(sub.subject)\n\n\t// Walk the connected leafnodes.\n\tfor _, ln := range acc.lleafs {\n\t\tif ln == sub.client {\n\t\t\tcontinue\n\t\t}\n\t\t// Check to make sure this sub does not have an origin cluster that matches the leafnode.\n\t\tln.mu.Lock()\n\t\t// If skipped, make sure that we still let go the \"$LDS.\" subscription that allows\n\t\t// the detection of loops as long as different cluster.\n\t\tclusterDifferent := cluster != ln.remoteCluster()\n\t\tif (isLDS && clusterDifferent) || ((cluster == _EMPTY_ || clusterDifferent) && (delta <= 0 || ln.canSubscribe(subject))) {\n\t\t\tln.updateSmap(sub, delta, isLDS)\n\t\t}\n\t\tln.mu.Unlock()\n\t}\n}\n\n// This will make an update to our internal smap and determine if we should send out\n// an interest update to the remote side.\n// Lock should be held.\nfunc (c *client) updateSmap(sub *subscription, delta int32, isLDS bool) {\n\tif c.leaf.smap == nil {\n\t\treturn\n\t}\n\n\t// If we are solicited make sure this is a local client or a non-solicited leaf node\n\tskind := sub.client.kind\n\tupdateClient := skind == CLIENT || skind == SYSTEM || skind == JETSTREAM || skind == ACCOUNT\n\tif !isLDS && c.isSpokeLeafNode() && !(updateClient || (skind == LEAF && !sub.client.isSpokeLeafNode())) {\n\t\treturn\n\t}\n\n\t// For additions, check if that sub has just been processed during initLeafNodeSmapAndSendSubs\n\tif delta > 0 && c.leaf.tsub != nil {\n\t\tif _, present := c.leaf.tsub[sub]; present {\n\t\t\tdelete(c.leaf.tsub, sub)\n\t\t\tif len(c.leaf.tsub) == 0 {\n\t\t\t\tc.leaf.tsub = nil\n\t\t\t\tc.leaf.tsubt.Stop()\n\t\t\t\tc.leaf.tsubt = nil\n\t\t\t}\n\t\t\treturn\n\t\t}\n\t}\n\n\tkey := keyFromSub(sub)\n\tn, ok := c.leaf.smap[key]\n\tif delta < 0 && !ok {\n\t\treturn\n\t}\n\n\t// We will update if its a queue, if count is zero (or negative), or we were 0 and are N > 0.\n\tupdate := sub.queue != nil || (n <= 0 && n+delta > 0) || (n > 0 && n+delta <= 0)\n\tn += delta\n\tif n > 0 {\n\t\tc.leaf.smap[key] = n\n\t} else {\n\t\tdelete(c.leaf.smap, key)\n\t}\n\tif update {\n\t\tc.sendLeafNodeSubUpdate(key, n)\n\t}\n}\n\n// Used to force add subjects to the subject map.\nfunc (c *client) forceAddToSmap(subj string) {\n\tc.mu.Lock()\n\tdefer c.mu.Unlock()\n\n\tif c.leaf.smap == nil {\n\t\treturn\n\t}\n\tn := c.leaf.smap[subj]\n\tif n != 0 {\n\t\treturn\n\t}\n\t// Place into the map since it was not there.\n\tc.leaf.smap[subj] = 1\n\tc.sendLeafNodeSubUpdate(subj, 1)\n}\n\n// Used to force remove a subject from the subject map.\nfunc (c *client) forceRemoveFromSmap(subj string) {\n\tc.mu.Lock()\n\tdefer c.mu.Unlock()\n\n\tif c.leaf.smap == nil {\n\t\treturn\n\t}\n\tn := c.leaf.smap[subj]\n\tif n == 0 {\n\t\treturn\n\t}\n\tn--\n\tif n == 0 {\n\t\t// Remove is now zero\n\t\tdelete(c.leaf.smap, subj)\n\t\tc.sendLeafNodeSubUpdate(subj, 0)\n\t} else {\n\t\tc.leaf.smap[subj] = n\n\t}\n}\n\n// Send the subscription interest change to the other side.\n// Lock should be held.\nfunc (c *client) sendLeafNodeSubUpdate(key string, n int32) {\n\t// If we are a spoke, we need to check if we are allowed to send this subscription over to the hub.\n\tif c.isSpokeLeafNode() {\n\t\tcheckPerms := true\n\t\tif len(key) > 0 && (key[0] == '$' || key[0] == '_') {\n\t\t\tif strings.HasPrefix(key, leafNodeLoopDetectionSubjectPrefix) ||\n\t\t\t\tstrings.HasPrefix(key, oldGWReplyPrefix) ||\n\t\t\t\tstrings.HasPrefix(key, gwReplyPrefix) {\n\t\t\t\tcheckPerms = false\n\t\t\t}\n\t\t}\n\t\tif checkPerms {\n\t\t\tvar subject string\n\t\t\tif sep := strings.IndexByte(key, ' '); sep != -1 {\n\t\t\t\tsubject = key[:sep]\n\t\t\t} else {\n\t\t\t\tsubject = key\n\t\t\t}\n\t\t\tif !c.canSubscribe(subject) {\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}\n\t// If we are here we can send over to the other side.\n\t_b := [64]byte{}\n\tb := bytes.NewBuffer(_b[:0])\n\tc.writeLeafSub(b, key, n)\n\tc.enqueueProto(b.Bytes())\n}\n\n// Helper function to build the key.\nfunc keyFromSub(sub *subscription) string {\n\tvar sb strings.Builder\n\tsb.Grow(len(sub.subject) + len(sub.queue) + 1)\n\tsb.Write(sub.subject)\n\tif sub.queue != nil {\n\t\t// Just make the key subject spc group, e.g. 'foo bar'\n\t\tsb.WriteByte(' ')\n\t\tsb.Write(sub.queue)\n\t}\n\treturn sb.String()\n}\n\nconst (\n\tkeyRoutedSub         = \"R\"\n\tkeyRoutedSubByte     = 'R'\n\tkeyRoutedLeafSub     = \"L\"\n\tkeyRoutedLeafSubByte = 'L'\n)\n\n// Helper function to build the key that prevents collisions between normal\n// routed subscriptions and routed subscriptions on behalf of a leafnode.\n// Keys will look like this:\n// \"R foo\"          -> plain routed sub on \"foo\"\n// \"R foo bar\"      -> queue routed sub on \"foo\", queue \"bar\"\n// \"L foo bar\"      -> plain routed leaf sub on \"foo\", leaf \"bar\"\n// \"L foo bar baz\"  -> queue routed sub on \"foo\", queue \"bar\", leaf \"baz\"\nfunc keyFromSubWithOrigin(sub *subscription) string {\n\tvar sb strings.Builder\n\tsb.Grow(2 + len(sub.origin) + 1 + len(sub.subject) + 1 + len(sub.queue))\n\tleaf := len(sub.origin) > 0\n\tif leaf {\n\t\tsb.WriteByte(keyRoutedLeafSubByte)\n\t} else {\n\t\tsb.WriteByte(keyRoutedSubByte)\n\t}\n\tsb.WriteByte(' ')\n\tsb.Write(sub.subject)\n\tif sub.queue != nil {\n\t\tsb.WriteByte(' ')\n\t\tsb.Write(sub.queue)\n\t}\n\tif leaf {\n\t\tsb.WriteByte(' ')\n\t\tsb.Write(sub.origin)\n\t}\n\treturn sb.String()\n}\n\n// Lock should be held.\nfunc (c *client) writeLeafSub(w *bytes.Buffer, key string, n int32) {\n\tif key == _EMPTY_ {\n\t\treturn\n\t}\n\tif n > 0 {\n\t\tw.WriteString(\"LS+ \" + key)\n\t\t// Check for queue semantics, if found write n.\n\t\tif strings.Contains(key, \" \") {\n\t\t\tw.WriteString(\" \")\n\t\t\tvar b [12]byte\n\t\t\tvar i = len(b)\n\t\t\tfor l := n; l > 0; l /= 10 {\n\t\t\t\ti--\n\t\t\t\tb[i] = digits[l%10]\n\t\t\t}\n\t\t\tw.Write(b[i:])\n\t\t\tif c.trace {\n\t\t\t\targ := fmt.Sprintf(\"%s %d\", key, n)\n\t\t\t\tc.traceOutOp(\"LS+\", []byte(arg))\n\t\t\t}\n\t\t} else if c.trace {\n\t\t\tc.traceOutOp(\"LS+\", []byte(key))\n\t\t}\n\t} else {\n\t\tw.WriteString(\"LS- \" + key)\n\t\tif c.trace {\n\t\t\tc.traceOutOp(\"LS-\", []byte(key))\n\t\t}\n\t}\n\tw.WriteString(CR_LF)\n}\n\n// processLeafSub will process an inbound sub request for the remote leaf node.\nfunc (c *client) processLeafSub(argo []byte) (err error) {\n\t// Indicate activity.\n\tc.in.subs++\n\n\tsrv := c.srv\n\tif srv == nil {\n\t\treturn nil\n\t}\n\n\t// Copy so we do not reference a potentially large buffer\n\targ := make([]byte, len(argo))\n\tcopy(arg, argo)\n\n\targs := splitArg(arg)\n\tsub := &subscription{client: c}\n\n\tdelta := int32(1)\n\tswitch len(args) {\n\tcase 1:\n\t\tsub.queue = nil\n\tcase 3:\n\t\tsub.queue = args[1]\n\t\tsub.qw = int32(parseSize(args[2]))\n\t\t// TODO: (ik) We should have a non empty queue name and a queue\n\t\t// weight >= 1. For 2.11, we may want to return an error if that\n\t\t// is not the case, but for now just overwrite `delta` if queue\n\t\t// weight is greater than 1 (it is possible after a reconnect/\n\t\t// server restart to receive a queue weight > 1 for a new sub).\n\t\tif sub.qw > 1 {\n\t\t\tdelta = sub.qw\n\t\t}\n\tdefault:\n\t\treturn fmt.Errorf(\"processLeafSub Parse Error: '%s'\", arg)\n\t}\n\tsub.subject = args[0]\n\n\tc.mu.Lock()\n\tif c.isClosed() {\n\t\tc.mu.Unlock()\n\t\treturn nil\n\t}\n\n\tacc := c.acc\n\t// Check if we have a loop.\n\tldsPrefix := bytes.HasPrefix(sub.subject, []byte(leafNodeLoopDetectionSubjectPrefix))\n\n\tif ldsPrefix && bytesToString(sub.subject) == acc.getLDSubject() {\n\t\tc.mu.Unlock()\n\t\tc.handleLeafNodeLoop(true)\n\t\treturn nil\n\t}\n\n\t// Check permissions if applicable. (but exclude the $LDS, $GR and _GR_)\n\tcheckPerms := true\n\tif sub.subject[0] == '$' || sub.subject[0] == '_' {\n\t\tif ldsPrefix ||\n\t\t\tbytes.HasPrefix(sub.subject, []byte(oldGWReplyPrefix)) ||\n\t\t\tbytes.HasPrefix(sub.subject, []byte(gwReplyPrefix)) {\n\t\t\tcheckPerms = false\n\t\t}\n\t}\n\n\t// If we are a hub check that we can publish to this subject.\n\tif checkPerms {\n\t\tsubj := string(sub.subject)\n\t\tif subjectIsLiteral(subj) && !c.pubAllowedFullCheck(subj, true, true) {\n\t\t\tc.mu.Unlock()\n\t\t\tc.leafSubPermViolation(sub.subject)\n\t\t\tc.Debugf(fmt.Sprintf(\"Permissions Violation for Subscription to %q\", sub.subject))\n\t\t\treturn nil\n\t\t}\n\t}\n\n\t// Check if we have a maximum on the number of subscriptions.\n\tif c.subsAtLimit() {\n\t\tc.mu.Unlock()\n\t\tc.maxSubsExceeded()\n\t\treturn nil\n\t}\n\n\t// If we have an origin cluster associated mark that in the sub.\n\tif rc := c.remoteCluster(); rc != _EMPTY_ {\n\t\tsub.origin = []byte(rc)\n\t}\n\n\t// Like Routes, we store local subs by account and subject and optionally queue name.\n\t// If we have a queue it will have a trailing weight which we do not want.\n\tif sub.queue != nil {\n\t\tsub.sid = arg[:len(arg)-len(args[2])-1]\n\t} else {\n\t\tsub.sid = arg\n\t}\n\tkey := bytesToString(sub.sid)\n\tosub := c.subs[key]\n\tif osub == nil {\n\t\tc.subs[key] = sub\n\t\t// Now place into the account sl.\n\t\tif err := acc.sl.Insert(sub); err != nil {\n\t\t\tdelete(c.subs, key)\n\t\t\tc.mu.Unlock()\n\t\t\tc.Errorf(\"Could not insert subscription: %v\", err)\n\t\t\tc.sendErr(\"Invalid Subscription\")\n\t\t\treturn nil\n\t\t}\n\t} else if sub.queue != nil {\n\t\t// For a queue we need to update the weight.\n\t\tdelta = sub.qw - atomic.LoadInt32(&osub.qw)\n\t\tatomic.StoreInt32(&osub.qw, sub.qw)\n\t\tacc.sl.UpdateRemoteQSub(osub)\n\t}\n\tspoke := c.isSpokeLeafNode()\n\tc.mu.Unlock()\n\n\t// Only add in shadow subs if a new sub or qsub.\n\tif osub == nil {\n\t\tif err := c.addShadowSubscriptions(acc, sub, true); err != nil {\n\t\t\tc.Errorf(err.Error())\n\t\t}\n\t}\n\n\t// If we are not solicited, treat leaf node subscriptions similar to a\n\t// client subscription, meaning we forward them to routes, gateways and\n\t// other leaf nodes as needed.\n\tif !spoke {\n\t\t// If we are routing add to the route map for the associated account.\n\t\tsrv.updateRouteSubscriptionMap(acc, sub, delta)\n\t\tif srv.gateway.enabled {\n\t\t\tsrv.gatewayUpdateSubInterest(acc.Name, sub, delta)\n\t\t}\n\t}\n\t// Now check on leafnode updates for other leaf nodes. We understand solicited\n\t// and non-solicited state in this call so we will do the right thing.\n\tacc.updateLeafNodes(sub, delta)\n\n\treturn nil\n}\n\n// If the leafnode is a solicited, set the connect delay based on default\n// or private option (for tests). Sends the error to the other side, log and\n// close the connection.\nfunc (c *client) handleLeafNodeLoop(sendErr bool) {\n\taccName, delay := c.setLeafConnectDelayIfSoliciting(leafNodeReconnectDelayAfterLoopDetected)\n\terrTxt := fmt.Sprintf(\"Loop detected for leafnode account=%q. Delaying attempt to reconnect for %v\", accName, delay)\n\tif sendErr {\n\t\tc.sendErr(errTxt)\n\t}\n\n\tc.Errorf(errTxt)\n\t// If we are here with \"sendErr\" false, it means that this is the server\n\t// that received the error. The other side will have closed the connection,\n\t// but does not hurt to close here too.\n\tc.closeConnection(ProtocolViolation)\n}\n\n// processLeafUnsub will process an inbound unsub request for the remote leaf node.\nfunc (c *client) processLeafUnsub(arg []byte) error {\n\t// Indicate any activity, so pub and sub or unsubs.\n\tc.in.subs++\n\n\tacc := c.acc\n\tsrv := c.srv\n\n\tc.mu.Lock()\n\tif c.isClosed() {\n\t\tc.mu.Unlock()\n\t\treturn nil\n\t}\n\n\tspoke := c.isSpokeLeafNode()\n\t// We store local subs by account and subject and optionally queue name.\n\t// LS- will have the arg exactly as the key.\n\tsub, ok := c.subs[string(arg)]\n\tif !ok {\n\t\t// If not found, don't try to update routes/gws/leaf nodes.\n\t\tc.mu.Unlock()\n\t\treturn nil\n\t}\n\tdelta := int32(1)\n\tif len(sub.queue) > 0 {\n\t\tdelta = sub.qw\n\t}\n\tc.mu.Unlock()\n\n\tc.unsubscribe(acc, sub, true, true)\n\tif !spoke {\n\t\t// If we are routing subtract from the route map for the associated account.\n\t\tsrv.updateRouteSubscriptionMap(acc, sub, -delta)\n\t\t// Gateways\n\t\tif srv.gateway.enabled {\n\t\t\tsrv.gatewayUpdateSubInterest(acc.Name, sub, -delta)\n\t\t}\n\t}\n\t// Now check on leafnode updates for other leaf nodes.\n\tacc.updateLeafNodes(sub, -delta)\n\treturn nil\n}\n\nfunc (c *client) processLeafHeaderMsgArgs(arg []byte) error {\n\t// Unroll splitArgs to avoid runtime/heap issues\n\ta := [MAX_MSG_ARGS][]byte{}\n\targs := a[:0]\n\tstart := -1\n\tfor i, b := range arg {\n\t\tswitch b {\n\t\tcase ' ', '\\t', '\\r', '\\n':\n\t\t\tif start >= 0 {\n\t\t\t\targs = append(args, arg[start:i])\n\t\t\t\tstart = -1\n\t\t\t}\n\t\tdefault:\n\t\t\tif start < 0 {\n\t\t\t\tstart = i\n\t\t\t}\n\t\t}\n\t}\n\tif start >= 0 {\n\t\targs = append(args, arg[start:])\n\t}\n\n\tc.pa.arg = arg\n\tswitch len(args) {\n\tcase 0, 1, 2:\n\t\treturn fmt.Errorf(\"processLeafHeaderMsgArgs Parse Error: '%s'\", args)\n\tcase 3:\n\t\tc.pa.reply = nil\n\t\tc.pa.queues = nil\n\t\tc.pa.hdb = args[1]\n\t\tc.pa.hdr = parseSize(args[1])\n\t\tc.pa.szb = args[2]\n\t\tc.pa.size = parseSize(args[2])\n\tcase 4:\n\t\tc.pa.reply = args[1]\n\t\tc.pa.queues = nil\n\t\tc.pa.hdb = args[2]\n\t\tc.pa.hdr = parseSize(args[2])\n\t\tc.pa.szb = args[3]\n\t\tc.pa.size = parseSize(args[3])\n\tdefault:\n\t\t// args[1] is our reply indicator. Should be + or | normally.\n\t\tif len(args[1]) != 1 {\n\t\t\treturn fmt.Errorf(\"processLeafHeaderMsgArgs Bad or Missing Reply Indicator: '%s'\", args[1])\n\t\t}\n\t\tswitch args[1][0] {\n\t\tcase '+':\n\t\t\tc.pa.reply = args[2]\n\t\tcase '|':\n\t\t\tc.pa.reply = nil\n\t\tdefault:\n\t\t\treturn fmt.Errorf(\"processLeafHeaderMsgArgs Bad or Missing Reply Indicator: '%s'\", args[1])\n\t\t}\n\t\t// Grab header size.\n\t\tc.pa.hdb = args[len(args)-2]\n\t\tc.pa.hdr = parseSize(c.pa.hdb)\n\n\t\t// Grab size.\n\t\tc.pa.szb = args[len(args)-1]\n\t\tc.pa.size = parseSize(c.pa.szb)\n\n\t\t// Grab queue names.\n\t\tif c.pa.reply != nil {\n\t\t\tc.pa.queues = args[3 : len(args)-2]\n\t\t} else {\n\t\t\tc.pa.queues = args[2 : len(args)-2]\n\t\t}\n\t}\n\tif c.pa.hdr < 0 {\n\t\treturn fmt.Errorf(\"processLeafHeaderMsgArgs Bad or Missing Header Size: '%s'\", arg)\n\t}\n\tif c.pa.size < 0 {\n\t\treturn fmt.Errorf(\"processLeafHeaderMsgArgs Bad or Missing Size: '%s'\", args)\n\t}\n\tif c.pa.hdr > c.pa.size {\n\t\treturn fmt.Errorf(\"processLeafHeaderMsgArgs Header Size larger then TotalSize: '%s'\", arg)\n\t}\n\n\t// Common ones processed after check for arg length\n\tc.pa.subject = args[0]\n\n\treturn nil\n}\n\nfunc (c *client) processLeafMsgArgs(arg []byte) error {\n\t// Unroll splitArgs to avoid runtime/heap issues\n\ta := [MAX_MSG_ARGS][]byte{}\n\targs := a[:0]\n\tstart := -1\n\tfor i, b := range arg {\n\t\tswitch b {\n\t\tcase ' ', '\\t', '\\r', '\\n':\n\t\t\tif start >= 0 {\n\t\t\t\targs = append(args, arg[start:i])\n\t\t\t\tstart = -1\n\t\t\t}\n\t\tdefault:\n\t\t\tif start < 0 {\n\t\t\t\tstart = i\n\t\t\t}\n\t\t}\n\t}\n\tif start >= 0 {\n\t\targs = append(args, arg[start:])\n\t}\n\n\tc.pa.arg = arg\n\tswitch len(args) {\n\tcase 0, 1:\n\t\treturn fmt.Errorf(\"processLeafMsgArgs Parse Error: '%s'\", args)\n\tcase 2:\n\t\tc.pa.reply = nil\n\t\tc.pa.queues = nil\n\t\tc.pa.szb = args[1]\n\t\tc.pa.size = parseSize(args[1])\n\tcase 3:\n\t\tc.pa.reply = args[1]\n\t\tc.pa.queues = nil\n\t\tc.pa.szb = args[2]\n\t\tc.pa.size = parseSize(args[2])\n\tdefault:\n\t\t// args[1] is our reply indicator. Should be + or | normally.\n\t\tif len(args[1]) != 1 {\n\t\t\treturn fmt.Errorf(\"processLeafMsgArgs Bad or Missing Reply Indicator: '%s'\", args[1])\n\t\t}\n\t\tswitch args[1][0] {\n\t\tcase '+':\n\t\t\tc.pa.reply = args[2]\n\t\tcase '|':\n\t\t\tc.pa.reply = nil\n\t\tdefault:\n\t\t\treturn fmt.Errorf(\"processLeafMsgArgs Bad or Missing Reply Indicator: '%s'\", args[1])\n\t\t}\n\t\t// Grab size.\n\t\tc.pa.szb = args[len(args)-1]\n\t\tc.pa.size = parseSize(c.pa.szb)\n\n\t\t// Grab queue names.\n\t\tif c.pa.reply != nil {\n\t\t\tc.pa.queues = args[3 : len(args)-1]\n\t\t} else {\n\t\t\tc.pa.queues = args[2 : len(args)-1]\n\t\t}\n\t}\n\tif c.pa.size < 0 {\n\t\treturn fmt.Errorf(\"processLeafMsgArgs Bad or Missing Size: '%s'\", args)\n\t}\n\n\t// Common ones processed after check for arg length\n\tc.pa.subject = args[0]\n\n\treturn nil\n}\n\n// processInboundLeafMsg is called to process an inbound msg from a leaf node.\nfunc (c *client) processInboundLeafMsg(msg []byte) {\n\t// Update statistics\n\t// The msg includes the CR_LF, so pull back out for accounting.\n\tc.in.msgs++\n\tc.in.bytes += int32(len(msg) - LEN_CR_LF)\n\n\tsrv, acc, subject := c.srv, c.acc, string(c.pa.subject)\n\n\t// Mostly under testing scenarios.\n\tif srv == nil || acc == nil {\n\t\treturn\n\t}\n\n\t// Match the subscriptions. We will use our own L1 map if\n\t// it's still valid, avoiding contention on the shared sublist.\n\tvar r *SublistResult\n\tvar ok bool\n\n\tgenid := atomic.LoadUint64(&c.acc.sl.genid)\n\tif genid == c.in.genid && c.in.results != nil {\n\t\tr, ok = c.in.results[subject]\n\t} else {\n\t\t// Reset our L1 completely.\n\t\tc.in.results = make(map[string]*SublistResult)\n\t\tc.in.genid = genid\n\t}\n\n\t// Go back to the sublist data structure.\n\tif !ok {\n\t\tr = c.acc.sl.Match(subject)\n\t\t// Prune the results cache. Keeps us from unbounded growth. Random delete.\n\t\tif len(c.in.results) >= maxResultCacheSize {\n\t\t\tn := 0\n\t\t\tfor subj := range c.in.results {\n\t\t\t\tdelete(c.in.results, subj)\n\t\t\t\tif n++; n > pruneSize {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t// Then add the new cache entry.\n\t\tc.in.results[subject] = r\n\t}\n\n\t// Collect queue names if needed.\n\tvar qnames [][]byte\n\n\t// Check for no interest, short circuit if so.\n\t// This is the fanout scale.\n\tif len(r.psubs)+len(r.qsubs) > 0 {\n\t\tflag := pmrNoFlag\n\t\t// If we have queue subs in this cluster, then if we run in gateway\n\t\t// mode and the remote gateways have queue subs, then we need to\n\t\t// collect the queue groups this message was sent to so that we\n\t\t// exclude them when sending to gateways.\n\t\tif len(r.qsubs) > 0 && c.srv.gateway.enabled &&\n\t\t\tatomic.LoadInt64(&c.srv.gateway.totalQSubs) > 0 {\n\t\t\tflag |= pmrCollectQueueNames\n\t\t}\n\t\t// If this is a mapped subject that means the mapped interest\n\t\t// is what got us here, but this might not have a queue designation\n\t\t// If that is the case, make sure we ignore to process local queue subscribers.\n\t\tif len(c.pa.mapped) > 0 && len(c.pa.queues) == 0 {\n\t\t\tflag |= pmrIgnoreEmptyQueueFilter\n\t\t}\n\t\t_, qnames = c.processMsgResults(acc, r, msg, nil, c.pa.subject, c.pa.reply, flag)\n\t}\n\n\t// Now deal with gateways\n\tif c.srv.gateway.enabled {\n\t\tc.sendMsgToGateways(acc, msg, c.pa.subject, c.pa.reply, qnames, true)\n\t}\n}\n\n// Handles a subscription permission violation.\n// See leafPermViolation() for details.\nfunc (c *client) leafSubPermViolation(subj []byte) {\n\tc.leafPermViolation(false, subj)\n}\n\n// Common function to process publish or subscribe leafnode permission violation.\n// Sends the permission violation error to the remote, logs it and closes the connection.\n// If this is from a server soliciting, the reconnection will be delayed.\nfunc (c *client) leafPermViolation(pub bool, subj []byte) {\n\tif c.isSpokeLeafNode() {\n\t\t// For spokes these are no-ops since the hub server told us our permissions.\n\t\t// We just need to not send these over to the other side since we will get cutoff.\n\t\treturn\n\t}\n\t// FIXME(dlc) ?\n\tc.setLeafConnectDelayIfSoliciting(leafNodeReconnectAfterPermViolation)\n\tvar action string\n\tif pub {\n\t\tc.sendErr(fmt.Sprintf(\"Permissions Violation for Publish to %q\", subj))\n\t\taction = \"Publish\"\n\t} else {\n\t\tc.sendErr(fmt.Sprintf(\"Permissions Violation for Subscription to %q\", subj))\n\t\taction = \"Subscription\"\n\t}\n\tc.Errorf(\"%s Violation on %q - Check other side configuration\", action, subj)\n\t// TODO: add a new close reason that is more appropriate?\n\tc.closeConnection(ProtocolViolation)\n}\n\n// Invoked from generic processErr() for LEAF connections.\nfunc (c *client) leafProcessErr(errStr string) {\n\t// Check if we got a cluster name collision.\n\tif strings.Contains(errStr, ErrLeafNodeHasSameClusterName.Error()) {\n\t\t_, delay := c.setLeafConnectDelayIfSoliciting(leafNodeReconnectDelayAfterClusterNameSame)\n\t\tc.Errorf(\"Leafnode connection dropped with same cluster name error. Delaying attempt to reconnect for %v\", delay)\n\t\treturn\n\t}\n\n\t// We will look for Loop detected error coming from the other side.\n\t// If we solicit, set the connect delay.\n\tif !strings.Contains(errStr, \"Loop detected\") {\n\t\treturn\n\t}\n\tc.handleLeafNodeLoop(false)\n}\n\n// If this leaf connection solicits, sets the connect delay to the given value,\n// or the one from the server option's LeafNode.connDelay if one is set (for tests).\n// Returns the connection's account name and delay.\nfunc (c *client) setLeafConnectDelayIfSoliciting(delay time.Duration) (string, time.Duration) {\n\tc.mu.Lock()\n\tif c.isSolicitedLeafNode() {\n\t\tif s := c.srv; s != nil {\n\t\t\tif srvdelay := s.getOpts().LeafNode.connDelay; srvdelay != 0 {\n\t\t\t\tdelay = srvdelay\n\t\t\t}\n\t\t}\n\t\tc.leaf.remote.setConnectDelay(delay)\n\t}\n\taccName := c.acc.Name\n\tc.mu.Unlock()\n\treturn accName, delay\n}\n\n// For the given remote Leafnode configuration, this function returns\n// if TLS is required, and if so, will return a clone of the TLS Config\n// (since some fields will be changed during handshake), the TLS server\n// name that is remembered, and the TLS timeout.\nfunc (c *client) leafNodeGetTLSConfigForSolicit(remote *leafNodeCfg) (bool, *tls.Config, string, float64) {\n\tvar (\n\t\ttlsConfig  *tls.Config\n\t\ttlsName    string\n\t\ttlsTimeout float64\n\t)\n\n\tremote.RLock()\n\tdefer remote.RUnlock()\n\n\ttlsRequired := remote.TLS || remote.TLSConfig != nil\n\tif tlsRequired {\n\t\tif remote.TLSConfig != nil {\n\t\t\ttlsConfig = remote.TLSConfig.Clone()\n\t\t} else {\n\t\t\ttlsConfig = &tls.Config{MinVersion: tls.VersionTLS12}\n\t\t}\n\t\ttlsName = remote.tlsName\n\t\ttlsTimeout = remote.TLSTimeout\n\t\tif tlsTimeout == 0 {\n\t\t\ttlsTimeout = float64(TLS_TIMEOUT / time.Second)\n\t\t}\n\t}\n\n\treturn tlsRequired, tlsConfig, tlsName, tlsTimeout\n}\n\n// Initiates the LeafNode Websocket connection by:\n// - doing the TLS handshake if needed\n// - sending the HTTP request\n// - waiting for the HTTP response\n//\n// Since some bufio reader is used to consume the HTTP response, this function\n// returns the slice of buffered bytes (if any) so that the readLoop that will\n// be started after that consume those first before reading from the socket.\n// The boolean\n//\n// Lock held on entry.\nfunc (c *client) leafNodeSolicitWSConnection(opts *Options, rURL *url.URL, remote *leafNodeCfg) ([]byte, ClosedState, error) {\n\tremote.RLock()\n\tcompress := remote.Websocket.Compression\n\t// By default the server will mask outbound frames, but it can be disabled with this option.\n\tnoMasking := remote.Websocket.NoMasking\n\tinfoTimeout := remote.FirstInfoTimeout\n\tremote.RUnlock()\n\t// Will do the client-side TLS handshake if needed.\n\ttlsRequired, err := c.leafClientHandshakeIfNeeded(remote, opts)\n\tif err != nil {\n\t\t// 0 will indicate that the connection was already closed\n\t\treturn nil, 0, err\n\t}\n\n\t// For http request, we need the passed URL to contain either http or https scheme.\n\tscheme := \"http\"\n\tif tlsRequired {\n\t\tscheme = \"https\"\n\t}\n\t// We will use the `/leafnode` path to tell the accepting WS server that it should\n\t// create a LEAF connection, not a CLIENT.\n\t// In case we use the user's URL path in the future, make sure we append the user's\n\t// path to our `/leafnode` path.\n\tlpath := leafNodeWSPath\n\tif curPath := rURL.EscapedPath(); curPath != _EMPTY_ {\n\t\tif curPath[0] == '/' {\n\t\t\tcurPath = curPath[1:]\n\t\t}\n\t\tlpath = path.Join(curPath, lpath)\n\t} else {\n\t\tlpath = lpath[1:]\n\t}\n\tustr := fmt.Sprintf(\"%s://%s/%s\", scheme, rURL.Host, lpath)\n\tu, _ := url.Parse(ustr)\n\treq := &http.Request{\n\t\tMethod:     \"GET\",\n\t\tURL:        u,\n\t\tProto:      \"HTTP/1.1\",\n\t\tProtoMajor: 1,\n\t\tProtoMinor: 1,\n\t\tHeader:     make(http.Header),\n\t\tHost:       u.Host,\n\t}\n\twsKey, err := wsMakeChallengeKey()\n\tif err != nil {\n\t\treturn nil, WriteError, err\n\t}\n\n\treq.Header[\"Upgrade\"] = []string{\"websocket\"}\n\treq.Header[\"Connection\"] = []string{\"Upgrade\"}\n\treq.Header[\"Sec-WebSocket-Key\"] = []string{wsKey}\n\treq.Header[\"Sec-WebSocket-Version\"] = []string{\"13\"}\n\tif compress {\n\t\treq.Header.Add(\"Sec-WebSocket-Extensions\", wsPMCReqHeaderValue)\n\t}\n\tif noMasking {\n\t\treq.Header.Add(wsNoMaskingHeader, wsNoMaskingValue)\n\t}\n\tc.nc.SetDeadline(time.Now().Add(infoTimeout))\n\tif err := req.Write(c.nc); err != nil {\n\t\treturn nil, WriteError, err\n\t}\n\n\tvar resp *http.Response\n\n\tbr := bufio.NewReaderSize(c.nc, MAX_CONTROL_LINE_SIZE)\n\tresp, err = http.ReadResponse(br, req)\n\tif err == nil &&\n\t\t(resp.StatusCode != 101 ||\n\t\t\t!strings.EqualFold(resp.Header.Get(\"Upgrade\"), \"websocket\") ||\n\t\t\t!strings.EqualFold(resp.Header.Get(\"Connection\"), \"upgrade\") ||\n\t\t\tresp.Header.Get(\"Sec-Websocket-Accept\") != wsAcceptKey(wsKey)) {\n\n\t\terr = fmt.Errorf(\"invalid websocket connection\")\n\t}\n\t// Check compression extension...\n\tif err == nil && c.ws.compress {\n\t\t// Check that not only permessage-deflate extension is present, but that\n\t\t// we also have server and client no context take over.\n\t\tsrvCompress, noCtxTakeover := wsPMCExtensionSupport(resp.Header, false)\n\n\t\t// If server does not support compression, then simply disable it in our side.\n\t\tif !srvCompress {\n\t\t\tc.ws.compress = false\n\t\t} else if !noCtxTakeover {\n\t\t\terr = fmt.Errorf(\"compression negotiation error\")\n\t\t}\n\t}\n\t// Same for no masking...\n\tif err == nil && noMasking {\n\t\t// Check if server accepts no masking\n\t\tif resp.Header.Get(wsNoMaskingHeader) != wsNoMaskingValue {\n\t\t\t// Nope, need to mask our writes as any client would do.\n\t\t\tc.ws.maskwrite = true\n\t\t}\n\t}\n\tif resp != nil {\n\t\tresp.Body.Close()\n\t}\n\tif err != nil {\n\t\treturn nil, ReadError, err\n\t}\n\tc.Debugf(\"Leafnode compression=%v masking=%v\", c.ws.compress, c.ws.maskwrite)\n\n\tvar preBuf []byte\n\t// We have to slurp whatever is in the bufio reader and pass that to the readloop.\n\tif n := br.Buffered(); n != 0 {\n\t\tpreBuf, _ = br.Peek(n)\n\t}\n\treturn preBuf, 0, nil\n}\n\nconst connectProcessTimeout = 2 * time.Second\n\n// This is invoked for remote LEAF remote connections after processing the INFO\n// protocol.\nfunc (s *Server) leafNodeResumeConnectProcess(c *client) {\n\tclusterName := s.ClusterName()\n\n\tc.mu.Lock()\n\tif c.isClosed() {\n\t\tc.mu.Unlock()\n\t\treturn\n\t}\n\tif err := c.sendLeafConnect(clusterName, c.headers); err != nil {\n\t\tc.mu.Unlock()\n\t\tc.closeConnection(WriteError)\n\t\treturn\n\t}\n\n\t// Spin up the write loop.\n\ts.startGoRoutine(func() { c.writeLoop() })\n\n\t// timeout leafNodeFinishConnectProcess\n\tc.ping.tmr = time.AfterFunc(connectProcessTimeout, func() {\n\t\tc.mu.Lock()\n\t\t// check if leafNodeFinishConnectProcess was called and prevent later leafNodeFinishConnectProcess\n\t\tif !c.flags.setIfNotSet(connectProcessFinished) {\n\t\t\tc.mu.Unlock()\n\t\t\treturn\n\t\t}\n\t\tclearTimer(&c.ping.tmr)\n\t\tclosed := c.isClosed()\n\t\tc.mu.Unlock()\n\t\tif !closed {\n\t\t\tc.sendErrAndDebug(\"Stale Leaf Node Connection - Closing\")\n\t\t\tc.closeConnection(StaleConnection)\n\t\t}\n\t})\n\tc.mu.Unlock()\n\tc.Debugf(\"Remote leafnode connect msg sent\")\n}\n\n// This is invoked for remote LEAF connections after processing the INFO\n// protocol and leafNodeResumeConnectProcess.\n// This will send LS+ the CONNECT protocol and register the leaf node.\nfunc (s *Server) leafNodeFinishConnectProcess(c *client) {\n\tc.mu.Lock()\n\tif !c.flags.setIfNotSet(connectProcessFinished) {\n\t\tc.mu.Unlock()\n\t\treturn\n\t}\n\tif c.isClosed() {\n\t\tc.mu.Unlock()\n\t\ts.removeLeafNodeConnection(c)\n\t\treturn\n\t}\n\tremote := c.leaf.remote\n\t// Check if we will need to send the system connect event.\n\tremote.RLock()\n\tsendSysConnectEvent := remote.Hub\n\tremote.RUnlock()\n\n\t// Capture account before releasing lock\n\tacc := c.acc\n\t// cancel connectProcessTimeout\n\tclearTimer(&c.ping.tmr)\n\tc.mu.Unlock()\n\n\t// Make sure we register with the account here.\n\tif err := c.registerWithAccount(acc); err != nil {\n\t\tif err == ErrTooManyAccountConnections {\n\t\t\tc.maxAccountConnExceeded()\n\t\t\treturn\n\t\t} else if err == ErrLeafNodeLoop {\n\t\t\tc.handleLeafNodeLoop(true)\n\t\t\treturn\n\t\t}\n\t\tc.Errorf(\"Registering leaf with account %s resulted in error: %v\", acc.Name, err)\n\t\tc.closeConnection(ProtocolViolation)\n\t\treturn\n\t}\n\ts.addLeafNodeConnection(c, _EMPTY_, _EMPTY_, false)\n\ts.initLeafNodeSmapAndSendSubs(c)\n\tif sendSysConnectEvent {\n\t\ts.sendLeafNodeConnect(acc)\n\t}\n\n\t// The above functions are not atomically under the client\n\t// lock doing those operations. It is possible - since we\n\t// have started the read/write loops - that the connection\n\t// is closed before or in between. This would leave the\n\t// closed LN connection possible registered with the account\n\t// and/or the server's leafs map. So check if connection\n\t// is closed, and if so, manually cleanup.\n\tc.mu.Lock()\n\tclosed := c.isClosed()\n\tif !closed {\n\t\tc.setFirstPingTimer()\n\t}\n\tc.mu.Unlock()\n\tif closed {\n\t\ts.removeLeafNodeConnection(c)\n\t\tif prev := acc.removeClient(c); prev == 1 {\n\t\t\ts.decActiveAccounts()\n\t\t}\n\t}\n}\n",
    "source_file": "server/leafnode.go",
    "chunk_type": "code"
  },
  {
    "content": "// Generated code, do not edit. See errors.json and run go generate to update\n\npackage server\n\nimport \"strings\"\n\nconst (\n\t// JSAccountResourcesExceededErr resource limits exceeded for account\n\tJSAccountResourcesExceededErr ErrorIdentifier = 10002\n\n\t// JSBadRequestErr bad request\n\tJSBadRequestErr ErrorIdentifier = 10003\n\n\t// JSClusterIncompleteErr incomplete results\n\tJSClusterIncompleteErr ErrorIdentifier = 10004\n\n\t// JSClusterNoPeersErrF Error causing no peers to be available ({err})\n\tJSClusterNoPeersErrF ErrorIdentifier = 10005\n\n\t// JSClusterNotActiveErr JetStream not in clustered mode\n\tJSClusterNotActiveErr ErrorIdentifier = 10006\n\n\t// JSClusterNotAssignedErr JetStream cluster not assigned to this server\n\tJSClusterNotAssignedErr ErrorIdentifier = 10007\n\n\t// JSClusterNotAvailErr JetStream system temporarily unavailable\n\tJSClusterNotAvailErr ErrorIdentifier = 10008\n\n\t// JSClusterNotLeaderErr JetStream cluster can not handle request\n\tJSClusterNotLeaderErr ErrorIdentifier = 10009\n\n\t// JSClusterPeerNotMemberErr peer not a member\n\tJSClusterPeerNotMemberErr ErrorIdentifier = 10040\n\n\t// JSClusterRequiredErr JetStream clustering support required\n\tJSClusterRequiredErr ErrorIdentifier = 10010\n\n\t// JSClusterServerNotMemberErr server is not a member of the cluster\n\tJSClusterServerNotMemberErr ErrorIdentifier = 10044\n\n\t// JSClusterTagsErr tags placement not supported for operation\n\tJSClusterTagsErr ErrorIdentifier = 10011\n\n\t// JSClusterUnSupportFeatureErr not currently supported in clustered mode\n\tJSClusterUnSupportFeatureErr ErrorIdentifier = 10036\n\n\t// JSConsumerAlreadyExists action CREATE is used for a existing consumer with a different config (consumer already exists)\n\tJSConsumerAlreadyExists ErrorIdentifier = 10148\n\n\t// JSConsumerBadDurableNameErr durable name can not contain '.', '*', '>'\n\tJSConsumerBadDurableNameErr ErrorIdentifier = 10103\n\n\t// JSConsumerConfigRequiredErr consumer config required\n\tJSConsumerConfigRequiredErr ErrorIdentifier = 10078\n\n\t// JSConsumerCreateDurableAndNameMismatch Consumer Durable and Name have to be equal if both are provided\n\tJSConsumerCreateDurableAndNameMismatch ErrorIdentifier = 10132\n\n\t// JSConsumerCreateErrF General consumer creation failure string ({err})\n\tJSConsumerCreateErrF ErrorIdentifier = 10012\n\n\t// JSConsumerCreateFilterSubjectMismatchErr Consumer create request did not match filtered subject from create subject\n\tJSConsumerCreateFilterSubjectMismatchErr ErrorIdentifier = 10131\n\n\t// JSConsumerDeliverCycleErr consumer deliver subject forms a cycle\n\tJSConsumerDeliverCycleErr ErrorIdentifier = 10081\n\n\t// JSConsumerDeliverToWildcardsErr consumer deliver subject has wildcards\n\tJSConsumerDeliverToWildcardsErr ErrorIdentifier = 10079\n\n\t// JSConsumerDescriptionTooLongErrF consumer description is too long, maximum allowed is {max}\n\tJSConsumerDescriptionTooLongErrF ErrorIdentifier = 10107\n\n\t// JSConsumerDirectRequiresEphemeralErr consumer direct requires an ephemeral consumer\n\tJSConsumerDirectRequiresEphemeralErr ErrorIdentifier = 10091\n\n\t// JSConsumerDirectRequiresPushErr consumer direct requires a push based consumer\n\tJSConsumerDirectRequiresPushErr ErrorIdentifier = 10090\n\n\t// JSConsumerDoesNotExist action UPDATE is used for a nonexisting consumer (consumer does not exist)\n\tJSConsumerDoesNotExist ErrorIdentifier = 10149\n\n\t// JSConsumerDuplicateFilterSubjects consumer cannot have both FilterSubject and FilterSubjects specified\n\tJSConsumerDuplicateFilterSubjects ErrorIdentifier = 10136\n\n\t// JSConsumerDurableNameNotInSubjectErr consumer expected to be durable but no durable name set in subject\n\tJSConsumerDurableNameNotInSubjectErr ErrorIdentifier = 10016\n\n\t// JSConsumerDurableNameNotMatchSubjectErr consumer name in subject does not match durable name in request\n\tJSConsumerDurableNameNotMatchSubjectErr ErrorIdentifier = 10017\n\n\t// JSConsumerDurableNameNotSetErr consumer expected to be durable but a durable name was not set\n\tJSConsumerDurableNameNotSetErr ErrorIdentifier = 10018\n\n\t// JSConsumerEmptyFilter consumer filter in FilterSubjects cannot be empty\n\tJSConsumerEmptyFilter ErrorIdentifier = 10139\n\n\t// JSConsumerEmptyGroupName Group name cannot be an empty string\n\tJSConsumerEmptyGroupName ErrorIdentifier = 10161\n\n\t// JSConsumerEphemeralWithDurableInSubjectErr consumer expected to be ephemeral but detected a durable name set in subject\n\tJSConsumerEphemeralWithDurableInSubjectErr ErrorIdentifier = 10019\n\n\t// JSConsumerEphemeralWithDurableNameErr consumer expected to be ephemeral but a durable name was set in request\n\tJSConsumerEphemeralWithDurableNameErr ErrorIdentifier = 10020\n\n\t// JSConsumerExistingActiveErr consumer already exists and is still active\n\tJSConsumerExistingActiveErr ErrorIdentifier = 10105\n\n\t// JSConsumerFCRequiresPushErr consumer flow control requires a push based consumer\n\tJSConsumerFCRequiresPushErr ErrorIdentifier = 10089\n\n\t// JSConsumerFilterNotSubsetErr consumer filter subject is not a valid subset of the interest subjects\n\tJSConsumerFilterNotSubsetErr ErrorIdentifier = 10093\n\n\t// JSConsumerHBRequiresPushErr consumer idle heartbeat requires a push based consumer\n\tJSConsumerHBRequiresPushErr ErrorIdentifier = 10088\n\n\t// JSConsumerInactiveThresholdExcess consumer inactive threshold exceeds system limit of {limit}\n\tJSConsumerInactiveThresholdExcess ErrorIdentifier = 10153\n\n\t// JSConsumerInvalidDeliverSubject invalid push consumer deliver subject\n\tJSConsumerInvalidDeliverSubject ErrorIdentifier = 10112\n\n\t// JSConsumerInvalidGroupNameErr Valid priority group name must match A-Z, a-z, 0-9, -_/=)+ and may not exceed 16 characters\n\tJSConsumerInvalidGroupNameErr ErrorIdentifier = 10162\n\n\t// JSConsumerInvalidPolicyErrF Generic delivery policy error ({err})\n\tJSConsumerInvalidPolicyErrF ErrorIdentifier = 10094\n\n\t// JSConsumerInvalidPriorityGroupErr Provided priority group does not exist for this consumer\n\tJSConsumerInvalidPriorityGroupErr ErrorIdentifier = 10160\n\n\t// JSConsumerInvalidSamplingErrF failed to parse consumer sampling configuration: {err}\n\tJSConsumerInvalidSamplingErrF ErrorIdentifier = 10095\n\n\t// JSConsumerMaxDeliverBackoffErr max deliver is required to be > length of backoff values\n\tJSConsumerMaxDeliverBackoffErr ErrorIdentifier = 10116\n\n\t// JSConsumerMaxPendingAckExcessErrF consumer max ack pending exceeds system limit of {limit}\n\tJSConsumerMaxPendingAckExcessErrF ErrorIdentifier = 10121\n\n\t// JSConsumerMaxPendingAckPolicyRequiredErr consumer requires ack policy for max ack pending\n\tJSConsumerMaxPendingAckPolicyRequiredErr ErrorIdentifier = 10082\n\n\t// JSConsumerMaxRequestBatchExceededF consumer max request batch exceeds server limit of {limit}\n\tJSConsumerMaxRequestBatchExceededF ErrorIdentifier = 10125\n\n\t// JSConsumerMaxRequestBatchNegativeErr consumer max request batch needs to be > 0\n\tJSConsumerMaxRequestBatchNegativeErr ErrorIdentifier = 10114\n\n\t// JSConsumerMaxRequestExpiresToSmall consumer max request expires needs to be >= 1ms\n\tJSConsumerMaxRequestExpiresToSmall ErrorIdentifier = 10115\n\n\t// JSConsumerMaxWaitingNegativeErr consumer max waiting needs to be positive\n\tJSConsumerMaxWaitingNegativeErr ErrorIdentifier = 10087\n\n\t// JSConsumerMetadataLengthErrF consumer metadata exceeds maximum size of {limit}\n\tJSConsumerMetadataLengthErrF ErrorIdentifier = 10135\n\n\t// JSConsumerMultipleFiltersNotAllowed consumer with multiple subject filters cannot use subject based API\n\tJSConsumerMultipleFiltersNotAllowed ErrorIdentifier = 10137\n\n\t// JSConsumerNameContainsPathSeparatorsErr Consumer name can not contain path separators\n\tJSConsumerNameContainsPathSeparatorsErr ErrorIdentifier = 10127\n\n\t// JSConsumerNameExistErr consumer name already in use\n\tJSConsumerNameExistErr ErrorIdentifier = 10013\n\n\t// JSConsumerNameTooLongErrF consumer name is too long, maximum allowed is {max}\n\tJSConsumerNameTooLongErrF ErrorIdentifier = 10102\n\n\t// JSConsumerNotFoundErr consumer not found\n\tJSConsumerNotFoundErr ErrorIdentifier = 10014\n\n\t// JSConsumerOfflineErr consumer is offline\n\tJSConsumerOfflineErr ErrorIdentifier = 10119\n\n\t// JSConsumerOnMappedErr consumer direct on a mapped consumer\n\tJSConsumerOnMappedErr ErrorIdentifier = 10092\n\n\t// JSConsumerOverlappingSubjectFilters consumer subject filters cannot overlap\n\tJSConsumerOverlappingSubjectFilters ErrorIdentifier = 10138\n\n\t// JSConsumerPriorityPolicyWithoutGroup Setting PriorityPolicy requires at least one PriorityGroup to be set\n\tJSConsumerPriorityPolicyWithoutGroup ErrorIdentifier = 10159\n\n\t// JSConsumerPullNotDurableErr consumer in pull mode requires a durable name\n\tJSConsumerPullNotDurableErr ErrorIdentifier = 10085\n\n\t// JSConsumerPullRequiresAckErr consumer in pull mode requires explicit ack policy on workqueue stream\n\tJSConsumerPullRequiresAckErr ErrorIdentifier = 10084\n\n\t// JSConsumerPullWithRateLimitErr consumer in pull mode can not have rate limit set\n\tJSConsumerPullWithRateLimitErr ErrorIdentifier = 10086\n\n\t// JSConsumerPushMaxWaitingErr consumer in push mode can not set max waiting\n\tJSConsumerPushMaxWaitingErr ErrorIdentifier = 10080\n\n\t// JSConsumerReplacementWithDifferentNameErr consumer replacement durable config not the same\n\tJSConsumerReplacementWithDifferentNameErr ErrorIdentifier = 10106\n\n\t// JSConsumerReplicasExceedsStream consumer config replica count exceeds parent stream\n\tJSConsumerReplicasExceedsStream ErrorIdentifier = 10126\n\n\t// JSConsumerReplicasShouldMatchStream consumer config replicas must match interest retention stream's replicas\n\tJSConsumerReplicasShouldMatchStream ErrorIdentifier = 10134\n\n\t// JSConsumerSmallHeartbeatErr consumer idle heartbeat needs to be >= 100ms\n\tJSConsumerSmallHeartbeatErr ErrorIdentifier = 10083\n\n\t// JSConsumerStoreFailedErrF error creating store for consumer: {err}\n\tJSConsumerStoreFailedErrF ErrorIdentifier = 10104\n\n\t// JSConsumerWQConsumerNotDeliverAllErr consumer must be deliver all on workqueue stream\n\tJSConsumerWQConsumerNotDeliverAllErr ErrorIdentifier = 10101\n\n\t// JSConsumerWQConsumerNotUniqueErr filtered consumer not unique on workqueue stream\n\tJSConsumerWQConsumerNotUniqueErr ErrorIdentifier = 10100\n\n\t// JSConsumerWQMultipleUnfilteredErr multiple non-filtered consumers not allowed on workqueue stream\n\tJSConsumerWQMultipleUnfilteredErr ErrorIdentifier = 10099\n\n\t// JSConsumerWQRequiresExplicitAckErr workqueue stream requires explicit ack\n\tJSConsumerWQRequiresExplicitAckErr ErrorIdentifier = 10098\n\n\t// JSConsumerWithFlowControlNeedsHeartbeats consumer with flow control also needs heartbeats\n\tJSConsumerWithFlowControlNeedsHeartbeats ErrorIdentifier = 10108\n\n\t// JSInsufficientResourcesErr insufficient resources\n\tJSInsufficientResourcesErr ErrorIdentifier = 10023\n\n\t// JSInvalidJSONErr invalid JSON: {err}\n\tJSInvalidJSONErr ErrorIdentifier = 10025\n\n\t// JSMaximumConsumersLimitErr maximum consumers limit reached\n\tJSMaximumConsumersLimitErr ErrorIdentifier = 10026\n\n\t// JSMaximumStreamsLimitErr maximum number of streams reached\n\tJSMaximumStreamsLimitErr ErrorIdentifier = 10027\n\n\t// JSMemoryResourcesExceededErr insufficient memory resources available\n\tJSMemoryResourcesExceededErr ErrorIdentifier = 10028\n\n\t// JSMessageTTLDisabledErr per-message TTL is disabled\n\tJSMessageTTLDisabledErr ErrorIdentifier = 10166\n\n\t// JSMessageTTLInvalidErr invalid per-message TTL\n\tJSMessageTTLInvalidErr ErrorIdentifier = 10165\n\n\t// JSMirrorConsumerSetupFailedErrF generic mirror consumer setup failure string ({err})\n\tJSMirrorConsumerSetupFailedErrF ErrorIdentifier = 10029\n\n\t// JSMirrorInvalidStreamName mirrored stream name is invalid\n\tJSMirrorInvalidStreamName ErrorIdentifier = 10142\n\n\t// JSMirrorInvalidSubjectFilter mirror transform source: {err}\n\tJSMirrorInvalidSubjectFilter ErrorIdentifier = 10151\n\n\t// JSMirrorInvalidTransformDestination mirror transform: {err}\n\tJSMirrorInvalidTransformDestination ErrorIdentifier = 10154\n\n\t// JSMirrorMaxMessageSizeTooBigErr stream mirror must have max message size >= source\n\tJSMirrorMaxMessageSizeTooBigErr ErrorIdentifier = 10030\n\n\t// JSMirrorMultipleFiltersNotAllowed mirror with multiple subject transforms cannot also have a single subject filter\n\tJSMirrorMultipleFiltersNotAllowed ErrorIdentifier = 10150\n\n\t// JSMirrorOverlappingSubjectFilters mirror subject filters can not overlap\n\tJSMirrorOverlappingSubjectFilters ErrorIdentifier = 10152\n\n\t// JSMirrorWithFirstSeqErr stream mirrors can not have first sequence configured\n\tJSMirrorWithFirstSeqErr ErrorIdentifier = 10143\n\n\t// JSMirrorWithSourcesErr stream mirrors can not also contain other sources\n\tJSMirrorWithSourcesErr ErrorIdentifier = 10031\n\n\t// JSMirrorWithStartSeqAndTimeErr stream mirrors can not have both start seq and start time configured\n\tJSMirrorWithStartSeqAndTimeErr ErrorIdentifier = 10032\n\n\t// JSMirrorWithSubjectFiltersErr stream mirrors can not contain filtered subjects\n\tJSMirrorWithSubjectFiltersErr ErrorIdentifier = 10033\n\n\t// JSMirrorWithSubjectsErr stream mirrors can not contain subjects\n\tJSMirrorWithSubjectsErr ErrorIdentifier = 10034\n\n\t// JSNoAccountErr account not found\n\tJSNoAccountErr ErrorIdentifier = 10035\n\n\t// JSNoLimitsErr no JetStream default or applicable tiered limit present\n\tJSNoLimitsErr ErrorIdentifier = 10120\n\n\t// JSNoMessageFoundErr no message found\n\tJSNoMessageFoundErr ErrorIdentifier = 10037\n\n\t// JSNotEmptyRequestErr expected an empty request payload\n\tJSNotEmptyRequestErr ErrorIdentifier = 10038\n\n\t// JSNotEnabledErr JetStream not enabled\n\tJSNotEnabledErr ErrorIdentifier = 10076\n\n\t// JSNotEnabledForAccountErr JetStream not enabled for account\n\tJSNotEnabledForAccountErr ErrorIdentifier = 10039\n\n\t// JSPedanticErrF pedantic mode: {err}\n\tJSPedanticErrF ErrorIdentifier = 10157\n\n\t// JSPeerRemapErr peer remap failed\n\tJSPeerRemapErr ErrorIdentifier = 10075\n\n\t// JSRaftGeneralErrF General RAFT error string ({err})\n\tJSRaftGeneralErrF ErrorIdentifier = 10041\n\n\t// JSReplicasCountCannotBeNegative replicas count cannot be negative\n\tJSReplicasCountCannotBeNegative ErrorIdentifier = 10133\n\n\t// JSRestoreSubscribeFailedErrF JetStream unable to subscribe to restore snapshot {subject}: {err}\n\tJSRestoreSubscribeFailedErrF ErrorIdentifier = 10042\n\n\t// JSSequenceNotFoundErrF sequence {seq} not found\n\tJSSequenceNotFoundErrF ErrorIdentifier = 10043\n\n\t// JSSnapshotDeliverSubjectInvalidErr deliver subject not valid\n\tJSSnapshotDeliverSubjectInvalidErr ErrorIdentifier = 10015\n\n\t// JSSourceConsumerSetupFailedErrF General source consumer setup failure string ({err})\n\tJSSourceConsumerSetupFailedErrF ErrorIdentifier = 10045\n\n\t// JSSourceDuplicateDetected source stream, filter and transform (plus external if present) must form a unique combination (duplicate source configuration detected)\n\tJSSourceDuplicateDetected ErrorIdentifier = 10140\n\n\t// JSSourceInvalidStreamName sourced stream name is invalid\n\tJSSourceInvalidStreamName ErrorIdentifier = 10141\n\n\t// JSSourceInvalidSubjectFilter source transform source: {err}\n\tJSSourceInvalidSubjectFilter ErrorIdentifier = 10145\n\n\t// JSSourceInvalidTransformDestination source transform: {err}\n\tJSSourceInvalidTransformDestination ErrorIdentifier = 10146\n\n\t// JSSourceMaxMessageSizeTooBigErr stream source must have max message size >= target\n\tJSSourceMaxMessageSizeTooBigErr ErrorIdentifier = 10046\n\n\t// JSSourceMultipleFiltersNotAllowed source with multiple subject transforms cannot also have a single subject filter\n\tJSSourceMultipleFiltersNotAllowed ErrorIdentifier = 10144\n\n\t// JSSourceOverlappingSubjectFilters source filters can not overlap\n\tJSSourceOverlappingSubjectFilters ErrorIdentifier = 10147\n\n\t// JSStorageResourcesExceededErr insufficient storage resources available\n\tJSStorageResourcesExceededErr ErrorIdentifier = 10047\n\n\t// JSStreamAssignmentErrF Generic stream assignment error string ({err})\n\tJSStreamAssignmentErrF ErrorIdentifier = 10048\n\n\t// JSStreamCreateErrF Generic stream creation error string ({err})\n\tJSStreamCreateErrF ErrorIdentifier = 10049\n\n\t// JSStreamDeleteErrF General stream deletion error string ({err})\n\tJSStreamDeleteErrF ErrorIdentifier = 10050\n\n\t// JSStreamDuplicateMessageConflict duplicate message id is in process\n\tJSStreamDuplicateMessageConflict ErrorIdentifier = 10158\n\n\t// JSStreamExpectedLastSeqPerSubjectNotReady expected last sequence per subject temporarily unavailable\n\tJSStreamExpectedLastSeqPerSubjectNotReady ErrorIdentifier = 10163\n\n\t// JSStreamExternalApiOverlapErrF stream external api prefix {prefix} must not overlap with {subject}\n\tJSStreamExternalApiOverlapErrF ErrorIdentifier = 10021\n\n\t// JSStreamExternalDelPrefixOverlapsErrF stream external delivery prefix {prefix} overlaps with stream subject {subject}\n\tJSStreamExternalDelPrefixOverlapsErrF ErrorIdentifier = 10022\n\n\t// JSStreamGeneralErrorF General stream failure string ({err})\n\tJSStreamGeneralErrorF ErrorIdentifier = 10051\n\n\t// JSStreamHeaderExceedsMaximumErr header size exceeds maximum allowed of 64k\n\tJSStreamHeaderExceedsMaximumErr ErrorIdentifier = 10097\n\n\t// JSStreamInfoMaxSubjectsErr subject details would exceed maximum allowed\n\tJSStreamInfoMaxSubjectsErr ErrorIdentifier = 10117\n\n\t// JSStreamInvalidConfigF Stream configuration validation error string ({err})\n\tJSStreamInvalidConfigF ErrorIdentifier = 10052\n\n\t// JSStreamInvalidErr stream not valid\n\tJSStreamInvalidErr ErrorIdentifier = 10096\n\n\t// JSStreamInvalidExternalDeliverySubjErrF stream external delivery prefix {prefix} must not contain wildcards\n\tJSStreamInvalidExternalDeliverySubjErrF ErrorIdentifier = 10024\n\n\t// JSStreamLimitsErrF General stream limits exceeded error string ({err})\n\tJSStreamLimitsErrF ErrorIdentifier = 10053\n\n\t// JSStreamMaxBytesRequired account requires a stream config to have max bytes set\n\tJSStreamMaxBytesRequired ErrorIdentifier = 10113\n\n\t// JSStreamMaxStreamBytesExceeded stream max bytes exceeds account limit max stream bytes\n\tJSStreamMaxStreamBytesExceeded ErrorIdentifier = 10122\n\n\t// JSStreamMessageExceedsMaximumErr message size exceeds maximum allowed\n\tJSStreamMessageExceedsMaximumErr ErrorIdentifier = 10054\n\n\t// JSStreamMirrorNotUpdatableErr stream mirror configuration can not be updated\n\tJSStreamMirrorNotUpdatableErr ErrorIdentifier = 10055\n\n\t// JSStreamMismatchErr stream name in subject does not match request\n\tJSStreamMismatchErr ErrorIdentifier = 10056\n\n\t// JSStreamMoveAndScaleErr can not move and scale a stream in a single update\n\tJSStreamMoveAndScaleErr ErrorIdentifier = 10123\n\n\t// JSStreamMoveInProgressF stream move already in progress: {msg}\n\tJSStreamMoveInProgressF ErrorIdentifier = 10124\n\n\t// JSStreamMoveNotInProgress stream move not in progress\n\tJSStreamMoveNotInProgress ErrorIdentifier = 10129\n\n\t// JSStreamMsgDeleteFailedF Generic message deletion failure error string ({err})\n\tJSStreamMsgDeleteFailedF ErrorIdentifier = 10057\n\n\t// JSStreamNameContainsPathSeparatorsErr Stream name can not contain path separators\n\tJSStreamNameContainsPathSeparatorsErr ErrorIdentifier = 10128\n\n\t// JSStreamNameExistErr stream name already in use with a different configuration\n\tJSStreamNameExistErr ErrorIdentifier = 10058\n\n\t// JSStreamNameExistRestoreFailedErr stream name already in use, cannot restore\n\tJSStreamNameExistRestoreFailedErr ErrorIdentifier = 10130\n\n\t// JSStreamNotFoundErr stream not found\n\tJSStreamNotFoundErr ErrorIdentifier = 10059\n\n\t// JSStreamNotMatchErr expected stream does not match\n\tJSStreamNotMatchErr ErrorIdentifier = 10060\n\n\t// JSStreamOfflineErr stream is offline\n\tJSStreamOfflineErr ErrorIdentifier = 10118\n\n\t// JSStreamPurgeFailedF Generic stream purge failure error string ({err})\n\tJSStreamPurgeFailedF ErrorIdentifier = 10110\n\n\t// JSStreamReplicasNotSupportedErr replicas > 1 not supported in non-clustered mode\n\tJSStreamReplicasNotSupportedErr ErrorIdentifier = 10074\n\n\t// JSStreamReplicasNotUpdatableErr Replicas configuration can not be updated\n\tJSStreamReplicasNotUpdatableErr ErrorIdentifier = 10061\n\n\t// JSStreamRestoreErrF restore failed: {err}\n\tJSStreamRestoreErrF ErrorIdentifier = 10062\n\n\t// JSStreamRollupFailedF Generic stream rollup failure error string ({err})\n\tJSStreamRollupFailedF ErrorIdentifier = 10111\n\n\t// JSStreamSealedErr invalid operation on sealed stream\n\tJSStreamSealedErr ErrorIdentifier = 10109\n\n\t// JSStreamSequenceNotMatchErr expected stream sequence does not match\n\tJSStreamSequenceNotMatchErr ErrorIdentifier = 10063\n\n\t// JSStreamSnapshotErrF snapshot failed: {err}\n\tJSStreamSnapshotErrF ErrorIdentifier = 10064\n\n\t// JSStreamStoreFailedF Generic error when storing a message failed ({err})\n\tJSStreamStoreFailedF ErrorIdentifier = 10077\n\n\t// JSStreamSubjectOverlapErr subjects overlap with an existing stream\n\tJSStreamSubjectOverlapErr ErrorIdentifier = 10065\n\n\t// JSStreamTemplateCreateErrF Generic template creation failed string ({err})\n\tJSStreamTemplateCreateErrF ErrorIdentifier = 10066\n\n\t// JSStreamTemplateDeleteErrF Generic stream template deletion failed error string ({err})\n\tJSStreamTemplateDeleteErrF ErrorIdentifier = 10067\n\n\t// JSStreamTemplateNotFoundErr template not found\n\tJSStreamTemplateNotFoundErr ErrorIdentifier = 10068\n\n\t// JSStreamTooManyRequests too many requests\n\tJSStreamTooManyRequests ErrorIdentifier = 10167\n\n\t// JSStreamTransformInvalidDestination stream transform: {err}\n\tJSStreamTransformInvalidDestination ErrorIdentifier = 10156\n\n\t// JSStreamTransformInvalidSource stream transform source: {err}\n\tJSStreamTransformInvalidSource ErrorIdentifier = 10155\n\n\t// JSStreamUpdateErrF Generic stream update error string ({err})\n\tJSStreamUpdateErrF ErrorIdentifier = 10069\n\n\t// JSStreamWrongLastMsgIDErrF wrong last msg ID: {id}\n\tJSStreamWrongLastMsgIDErrF ErrorIdentifier = 10070\n\n\t// JSStreamWrongLastSequenceConstantErr wrong last sequence\n\tJSStreamWrongLastSequenceConstantErr ErrorIdentifier = 10164\n\n\t// JSStreamWrongLastSequenceErrF wrong last sequence: {seq}\n\tJSStreamWrongLastSequenceErrF ErrorIdentifier = 10071\n\n\t// JSTempStorageFailedErr JetStream unable to open temp storage for restore\n\tJSTempStorageFailedErr ErrorIdentifier = 10072\n\n\t// JSTemplateNameNotMatchSubjectErr template name in subject does not match request\n\tJSTemplateNameNotMatchSubjectErr ErrorIdentifier = 10073\n)\n\nvar (\n\tApiErrors = map[ErrorIdentifier]*ApiError{\n\t\tJSAccountResourcesExceededErr:              {Code: 400, ErrCode: 10002, Description: \"resource limits exceeded for account\"},\n\t\tJSBadRequestErr:                            {Code: 400, ErrCode: 10003, Description: \"bad request\"},\n\t\tJSClusterIncompleteErr:                     {Code: 503, ErrCode: 10004, Description: \"incomplete results\"},\n\t\tJSClusterNoPeersErrF:                       {Code: 400, ErrCode: 10005, Description: \"{err}\"},\n\t\tJSClusterNotActiveErr:                      {Code: 500, ErrCode: 10006, Description: \"JetStream not in clustered mode\"},\n\t\tJSClusterNotAssignedErr:                    {Code: 500, ErrCode: 10007, Description: \"JetStream cluster not assigned to this server\"},\n\t\tJSClusterNotAvailErr:                       {Code: 503, ErrCode: 10008, Description: \"JetStream system temporarily unavailable\"},\n\t\tJSClusterNotLeaderErr:                      {Code: 500, ErrCode: 10009, Description: \"JetStream cluster can not handle request\"},\n\t\tJSClusterPeerNotMemberErr:                  {Code: 400, ErrCode: 10040, Description: \"peer not a member\"},\n\t\tJSClusterRequiredErr:                       {Code: 503, ErrCode: 10010, Description: \"JetStream clustering support required\"},\n\t\tJSClusterServerNotMemberErr:                {Code: 400, ErrCode: 10044, Description: \"server is not a member of the cluster\"},\n\t\tJSClusterTagsErr:                           {Code: 400, ErrCode: 10011, Description: \"tags placement not supported for operation\"},\n\t\tJSClusterUnSupportFeatureErr:               {Code: 503, ErrCode: 10036, Description: \"not currently supported in clustered mode\"},\n\t\tJSConsumerAlreadyExists:                    {Code: 400, ErrCode: 10148, Description: \"consumer already exists\"},\n\t\tJSConsumerBadDurableNameErr:                {Code: 400, ErrCode: 10103, Description: \"durable name can not contain '.', '*', '>'\"},\n\t\tJSConsumerConfigRequiredErr:                {Code: 400, ErrCode: 10078, Description: \"consumer config required\"},\n\t\tJSConsumerCreateDurableAndNameMismatch:     {Code: 400, ErrCode: 10132, Description: \"Consumer Durable and Name have to be equal if both are provided\"},\n\t\tJSConsumerCreateErrF:                       {Code: 500, ErrCode: 10012, Description: \"{err}\"},\n\t\tJSConsumerCreateFilterSubjectMismatchErr:   {Code: 400, ErrCode: 10131, Description: \"Consumer create request did not match filtered subject from create subject\"},\n\t\tJSConsumerDeliverCycleErr:                  {Code: 400, ErrCode: 10081, Description: \"consumer deliver subject forms a cycle\"},\n\t\tJSConsumerDeliverToWildcardsErr:            {Code: 400, ErrCode: 10079, Description: \"consumer deliver subject has wildcards\"},\n\t\tJSConsumerDescriptionTooLongErrF:           {Code: 400, ErrCode: 10107, Description: \"consumer description is too long, maximum allowed is {max}\"},\n\t\tJSConsumerDirectRequiresEphemeralErr:       {Code: 400, ErrCode: 10091, Description: \"consumer direct requires an ephemeral consumer\"},\n\t\tJSConsumerDirectRequiresPushErr:            {Code: 400, ErrCode: 10090, Description: \"consumer direct requires a push based consumer\"},\n\t\tJSConsumerDoesNotExist:                     {Code: 400, ErrCode: 10149, Description: \"consumer does not exist\"},\n\t\tJSConsumerDuplicateFilterSubjects:          {Code: 400, ErrCode: 10136, Description: \"consumer cannot have both FilterSubject and FilterSubjects specified\"},\n\t\tJSConsumerDurableNameNotInSubjectErr:       {Code: 400, ErrCode: 10016, Description: \"consumer expected to be durable but no durable name set in subject\"},\n\t\tJSConsumerDurableNameNotMatchSubjectErr:    {Code: 400, ErrCode: 10017, Description: \"consumer name in subject does not match durable name in request\"},\n\t\tJSConsumerDurableNameNotSetErr:             {Code: 400, ErrCode: 10018, Description: \"consumer expected to be durable but a durable name was not set\"},\n\t\tJSConsumerEmptyFilter:                      {Code: 400, ErrCode: 10139, Description: \"consumer filter in FilterSubjects cannot be empty\"},\n\t\tJSConsumerEmptyGroupName:                   {Code: 400, ErrCode: 10161, Description: \"Group name cannot be an empty string\"},\n\t\tJSConsumerEphemeralWithDurableInSubjectErr: {Code: 400, ErrCode: 10019, Description: \"consumer expected to be ephemeral but detected a durable name set in subject\"},\n\t\tJSConsumerEphemeralWithDurableNameErr:      {Code: 400, ErrCode: 10020, Description: \"consumer expected to be ephemeral but a durable name was set in request\"},\n\t\tJSConsumerExistingActiveErr:                {Code: 400, ErrCode: 10105, Description: \"consumer already exists and is still active\"},\n\t\tJSConsumerFCRequiresPushErr:                {Code: 400, ErrCode: 10089, Description: \"consumer flow control requires a push based consumer\"},\n\t\tJSConsumerFilterNotSubsetErr:               {Code: 400, ErrCode: 10093, Description: \"consumer filter subject is not a valid subset of the interest subjects\"},\n\t\tJSConsumerHBRequiresPushErr:                {Code: 400, ErrCode: 10088, Description: \"consumer idle heartbeat requires a push based consumer\"},\n\t\tJSConsumerInactiveThresholdExcess:          {Code: 400, ErrCode: 10153, Description: \"consumer inactive threshold exceeds system limit of {limit}\"},\n\t\tJSConsumerInvalidDeliverSubject:            {Code: 400, ErrCode: 10112, Description: \"invalid push consumer deliver subject\"},\n\t\tJSConsumerInvalidGroupNameErr:              {Code: 400, ErrCode: 10162, Description: \"Valid priority group name must match A-Z, a-z, 0-9, -_/=)+ and may not exceed 16 characters\"},\n\t\tJSConsumerInvalidPolicyErrF:                {Code: 400, ErrCode: 10094, Description: \"{err}\"},\n\t\tJSConsumerInvalidPriorityGroupErr:          {Code: 400, ErrCode: 10160, Description: \"Provided priority group does not exist for this consumer\"},\n\t\tJSConsumerInvalidSamplingErrF:              {Code: 400, ErrCode: 10095, Description: \"failed to parse consumer sampling configuration: {err}\"},\n\t\tJSConsumerMaxDeliverBackoffErr:             {Code: 400, ErrCode: 10116, Description: \"max deliver is required to be > length of backoff values\"},\n\t\tJSConsumerMaxPendingAckExcessErrF:          {Code: 400, ErrCode: 10121, Description: \"consumer max ack pending exceeds system limit of {limit}\"},\n\t\tJSConsumerMaxPendingAckPolicyRequiredErr:   {Code: 400, ErrCode: 10082, Description: \"consumer requires ack policy for max ack pending\"},\n\t\tJSConsumerMaxRequestBatchExceededF:         {Code: 400, ErrCode: 10125, Description: \"consumer max request batch exceeds server limit of {limit}\"},\n\t\tJSConsumerMaxRequestBatchNegativeErr:       {Code: 400, ErrCode: 10114, Description: \"consumer max request batch needs to be > 0\"},\n\t\tJSConsumerMaxRequestExpiresToSmall:         {Code: 400, ErrCode: 10115, Description: \"consumer max request expires needs to be >= 1ms\"},\n\t\tJSConsumerMaxWaitingNegativeErr:            {Code: 400, ErrCode: 10087, Description: \"consumer max waiting needs to be positive\"},\n\t\tJSConsumerMetadataLengthErrF:               {Code: 400, ErrCode: 10135, Description: \"consumer metadata exceeds maximum size of {limit}\"},\n\t\tJSConsumerMultipleFiltersNotAllowed:        {Code: 400, ErrCode: 10137, Description: \"consumer with multiple subject filters cannot use subject based API\"},\n\t\tJSConsumerNameContainsPathSeparatorsErr:    {Code: 400, ErrCode: 10127, Description: \"Consumer name can not contain path separators\"},\n\t\tJSConsumerNameExistErr:                     {Code: 400, ErrCode: 10013, Description: \"consumer name already in use\"},\n\t\tJSConsumerNameTooLongErrF:                  {Code: 400, ErrCode: 10102, Description: \"consumer name is too long, maximum allowed is {max}\"},\n\t\tJSConsumerNotFoundErr:                      {Code: 404, ErrCode: 10014, Description: \"consumer not found\"},\n\t\tJSConsumerOfflineErr:                       {Code: 500, ErrCode: 10119, Description: \"consumer is offline\"},\n\t\tJSConsumerOnMappedErr:                      {Code: 400, ErrCode: 10092, Description: \"consumer direct on a mapped consumer\"},\n\t\tJSConsumerOverlappingSubjectFilters:        {Code: 400, ErrCode: 10138, Description: \"consumer subject filters cannot overlap\"},\n\t\tJSConsumerPriorityPolicyWithoutGroup:       {Code: 400, ErrCode: 10159, Description: \"Setting PriorityPolicy requires at least one PriorityGroup to be set\"},\n\t\tJSConsumerPullNotDurableErr:                {Code: 400, ErrCode: 10085, Description: \"consumer in pull mode requires a durable name\"},\n\t\tJSConsumerPullRequiresAckErr:               {Code: 400, ErrCode: 10084, Description: \"consumer in pull mode requires explicit ack policy on workqueue stream\"},\n\t\tJSConsumerPullWithRateLimitErr:             {Code: 400, ErrCode: 10086, Description: \"consumer in pull mode can not have rate limit set\"},\n\t\tJSConsumerPushMaxWaitingErr:                {Code: 400, ErrCode: 10080, Description: \"consumer in push mode can not set max waiting\"},\n\t\tJSConsumerReplacementWithDifferentNameErr:  {Code: 400, ErrCode: 10106, Description: \"consumer replacement durable config not the same\"},\n\t\tJSConsumerReplicasExceedsStream:            {Code: 400, ErrCode: 10126, Description: \"consumer config replica count exceeds parent stream\"},\n\t\tJSConsumerReplicasShouldMatchStream:        {Code: 400, ErrCode: 10134, Description: \"consumer config replicas must match interest retention stream's replicas\"},\n\t\tJSConsumerSmallHeartbeatErr:                {Code: 400, ErrCode: 10083, Description: \"consumer idle heartbeat needs to be >= 100ms\"},\n\t\tJSConsumerStoreFailedErrF:                  {Code: 500, ErrCode: 10104, Description: \"error creating store for consumer: {err}\"},\n\t\tJSConsumerWQConsumerNotDeliverAllErr:       {Code: 400, ErrCode: 10101, Description: \"consumer must be deliver all on workqueue stream\"},\n\t\tJSConsumerWQConsumerNotUniqueErr:           {Code: 400, ErrCode: 10100, Description: \"filtered consumer not unique on workqueue stream\"},\n\t\tJSConsumerWQMultipleUnfilteredErr:          {Code: 400, ErrCode: 10099, Description: \"multiple non-filtered consumers not allowed on workqueue stream\"},\n\t\tJSConsumerWQRequiresExplicitAckErr:         {Code: 400, ErrCode: 10098, Description: \"workqueue stream requires explicit ack\"},\n\t\tJSConsumerWithFlowControlNeedsHeartbeats:   {Code: 400, ErrCode: 10108, Description: \"consumer with flow control also needs heartbeats\"},\n\t\tJSInsufficientResourcesErr:                 {Code: 503, ErrCode: 10023, Description: \"insufficient resources\"},\n\t\tJSInvalidJSONErr:                           {Code: 400, ErrCode: 10025, Description: \"invalid JSON: {err}\"},\n\t\tJSMaximumConsumersLimitErr:                 {Code: 400, ErrCode: 10026, Description: \"maximum consumers limit reached\"},\n\t\tJSMaximumStreamsLimitErr:                   {Code: 400, ErrCode: 10027, Description: \"maximum number of streams reached\"},\n\t\tJSMemoryResourcesExceededErr:               {Code: 500, ErrCode: 10028, Description: \"insufficient memory resources available\"},\n\t\tJSMessageTTLDisabledErr:                    {Code: 400, ErrCode: 10166, Description: \"per-message TTL is disabled\"},\n\t\tJSMessageTTLInvalidErr:                     {Code: 400, ErrCode: 10165, Description: \"invalid per-message TTL\"},\n\t\tJSMirrorConsumerSetupFailedErrF:            {Code: 500, ErrCode: 10029, Description: \"{err}\"},\n\t\tJSMirrorInvalidStreamName:                  {Code: 400, ErrCode: 10142, Description: \"mirrored stream name is invalid\"},\n\t\tJSMirrorInvalidSubjectFilter:               {Code: 400, ErrCode: 10151, Description: \"mirror transform source: {err}\"},\n\t\tJSMirrorInvalidTransformDestination:        {Code: 400, ErrCode: 10154, Description: \"mirror transform: {err}\"},\n\t\tJSMirrorMaxMessageSizeTooBigErr:            {Code: 400, ErrCode: 10030, Description: \"stream mirror must have max message size >= source\"},\n\t\tJSMirrorMultipleFiltersNotAllowed:          {Code: 400, ErrCode: 10150, Description: \"mirror with multiple subject transforms cannot also have a single subject filter\"},\n\t\tJSMirrorOverlappingSubjectFilters:          {Code: 400, ErrCode: 10152, Description: \"mirror subject filters can not overlap\"},\n\t\tJSMirrorWithFirstSeqErr:                    {Code: 400, ErrCode: 10143, Description: \"stream mirrors can not have first sequence configured\"},\n\t\tJSMirrorWithSourcesErr:                     {Code: 400, ErrCode: 10031, Description: \"stream mirrors can not also contain other sources\"},\n\t\tJSMirrorWithStartSeqAndTimeErr:             {Code: 400, ErrCode: 10032, Description: \"stream mirrors can not have both start seq and start time configured\"},\n\t\tJSMirrorWithSubjectFiltersErr:              {Code: 400, ErrCode: 10033, Description: \"stream mirrors can not contain filtered subjects\"},\n\t\tJSMirrorWithSubjectsErr:                    {Code: 400, ErrCode: 10034, Description: \"stream mirrors can not contain subjects\"},\n\t\tJSNoAccountErr:                             {Code: 503, ErrCode: 10035, Description: \"account not found\"},\n\t\tJSNoLimitsErr:                              {Code: 400, ErrCode: 10120, Description: \"no JetStream default or applicable tiered limit present\"},\n\t\tJSNoMessageFoundErr:                        {Code: 404, ErrCode: 10037, Description: \"no message found\"},\n\t\tJSNotEmptyRequestErr:                       {Code: 400, ErrCode: 10038, Description: \"expected an empty request payload\"},\n\t\tJSNotEnabledErr:                            {Code: 503, ErrCode: 10076, Description: \"JetStream not enabled\"},\n\t\tJSNotEnabledForAccountErr:                  {Code: 503, ErrCode: 10039, Description: \"JetStream not enabled for account\"},\n\t\tJSPedanticErrF:                             {Code: 400, ErrCode: 10157, Description: \"pedantic mode: {err}\"},\n\t\tJSPeerRemapErr:                             {Code: 503, ErrCode: 10075, Description: \"peer remap failed\"},\n\t\tJSRaftGeneralErrF:                          {Code: 500, ErrCode: 10041, Description: \"{err}\"},\n\t\tJSReplicasCountCannotBeNegative:            {Code: 400, ErrCode: 10133, Description: \"replicas count cannot be negative\"},\n\t\tJSRestoreSubscribeFailedErrF:               {Code: 500, ErrCode: 10042, Description: \"JetStream unable to subscribe to restore snapshot {subject}: {err}\"},\n\t\tJSSequenceNotFoundErrF:                     {Code: 400, ErrCode: 10043, Description: \"sequence {seq} not found\"},\n\t\tJSSnapshotDeliverSubjectInvalidErr:         {Code: 400, ErrCode: 10015, Description: \"deliver subject not valid\"},\n\t\tJSSourceConsumerSetupFailedErrF:            {Code: 500, ErrCode: 10045, Description: \"{err}\"},\n\t\tJSSourceDuplicateDetected:                  {Code: 400, ErrCode: 10140, Description: \"duplicate source configuration detected\"},\n\t\tJSSourceInvalidStreamName:                  {Code: 400, ErrCode: 10141, Description: \"sourced stream name is invalid\"},\n\t\tJSSourceInvalidSubjectFilter:               {Code: 400, ErrCode: 10145, Description: \"source transform source: {err}\"},\n\t\tJSSourceInvalidTransformDestination:        {Code: 400, ErrCode: 10146, Description: \"source transform: {err}\"},\n\t\tJSSourceMaxMessageSizeTooBigErr:            {Code: 400, ErrCode: 10046, Description: \"stream source must have max message size >= target\"},\n\t\tJSSourceMultipleFiltersNotAllowed:          {Code: 400, ErrCode: 10144, Description: \"source with multiple subject transforms cannot also have a single subject filter\"},\n\t\tJSSourceOverlappingSubjectFilters:          {Code: 400, ErrCode: 10147, Description: \"source filters can not overlap\"},\n\t\tJSStorageResourcesExceededErr:              {Code: 500, ErrCode: 10047, Description: \"insufficient storage resources available\"},\n\t\tJSStreamAssignmentErrF:                     {Code: 500, ErrCode: 10048, Description: \"{err}\"},\n\t\tJSStreamCreateErrF:                         {Code: 500, ErrCode: 10049, Description: \"{err}\"},\n\t\tJSStreamDeleteErrF:                         {Code: 500, ErrCode: 10050, Description: \"{err}\"},\n\t\tJSStreamDuplicateMessageConflict:           {Code: 409, ErrCode: 10158, Description: \"duplicate message id is in process\"},\n\t\tJSStreamExpectedLastSeqPerSubjectNotReady:  {Code: 503, ErrCode: 10163, Description: \"expected last sequence per subject temporarily unavailable\"},\n\t\tJSStreamExternalApiOverlapErrF:             {Code: 400, ErrCode: 10021, Description: \"stream external api prefix {prefix} must not overlap with {subject}\"},\n\t\tJSStreamExternalDelPrefixOverlapsErrF:      {Code: 400, ErrCode: 10022, Description: \"stream external delivery prefix {prefix} overlaps with stream subject {subject}\"},\n\t\tJSStreamGeneralErrorF:                      {Code: 500, ErrCode: 10051, Description: \"{err}\"},\n\t\tJSStreamHeaderExceedsMaximumErr:            {Code: 400, ErrCode: 10097, Description: \"header size exceeds maximum allowed of 64k\"},\n\t\tJSStreamInfoMaxSubjectsErr:                 {Code: 500, ErrCode: 10117, Description: \"subject details would exceed maximum allowed\"},\n\t\tJSStreamInvalidConfigF:                     {Code: 500, ErrCode: 10052, Description: \"{err}\"},\n\t\tJSStreamInvalidErr:                         {Code: 500, ErrCode: 10096, Description: \"stream not valid\"},\n\t\tJSStreamInvalidExternalDeliverySubjErrF:    {Code: 400, ErrCode: 10024, Description: \"stream external delivery prefix {prefix} must not contain wildcards\"},\n\t\tJSStreamLimitsErrF:                         {Code: 500, ErrCode: 10053, Description: \"{err}\"},\n\t\tJSStreamMaxBytesRequired:                   {Code: 400, ErrCode: 10113, Description: \"account requires a stream config to have max bytes set\"},\n\t\tJSStreamMaxStreamBytesExceeded:             {Code: 400, ErrCode: 10122, Description: \"stream max bytes exceeds account limit max stream bytes\"},\n\t\tJSStreamMessageExceedsMaximumErr:           {Code: 400, ErrCode: 10054, Description: \"message size exceeds maximum allowed\"},\n\t\tJSStreamMirrorNotUpdatableErr:              {Code: 400, ErrCode: 10055, Description: \"stream mirror configuration can not be updated\"},\n\t\tJSStreamMismatchErr:                        {Code: 400, ErrCode: 10056, Description: \"stream name in subject does not match request\"},\n\t\tJSStreamMoveAndScaleErr:                    {Code: 400, ErrCode: 10123, Description: \"can not move and scale a stream in a single update\"},\n\t\tJSStreamMoveInProgressF:                    {Code: 400, ErrCode: 10124, Description: \"stream move already in progress: {msg}\"},\n\t\tJSStreamMoveNotInProgress:                  {Code: 400, ErrCode: 10129, Description: \"stream move not in progress\"},\n\t\tJSStreamMsgDeleteFailedF:                   {Code: 500, ErrCode: 10057, Description: \"{err}\"},\n\t\tJSStreamNameContainsPathSeparatorsErr:      {Code: 400, ErrCode: 10128, Description: \"Stream name can not contain path separators\"},\n\t\tJSStreamNameExistErr:                       {Code: 400, ErrCode: 10058, Description: \"stream name already in use with a different configuration\"},\n\t\tJSStreamNameExistRestoreFailedErr:          {Code: 400, ErrCode: 10130, Description: \"stream name already in use, cannot restore\"},\n\t\tJSStreamNotFoundErr:                        {Code: 404, ErrCode: 10059, Description: \"stream not found\"},\n\t\tJSStreamNotMatchErr:                        {Code: 400, ErrCode: 10060, Description: \"expected stream does not match\"},\n\t\tJSStreamOfflineErr:                         {Code: 500, ErrCode: 10118, Description: \"stream is offline\"},\n\t\tJSStreamPurgeFailedF:                       {Code: 500, ErrCode: 10110, Description: \"{err}\"},\n\t\tJSStreamReplicasNotSupportedErr:            {Code: 500, ErrCode: 10074, Description: \"replicas > 1 not supported in non-clustered mode\"},\n\t\tJSStreamReplicasNotUpdatableErr:            {Code: 400, ErrCode: 10061, Description: \"Replicas configuration can not be updated\"},\n\t\tJSStreamRestoreErrF:                        {Code: 500, ErrCode: 10062, Description: \"restore failed: {err}\"},\n\t\tJSStreamRollupFailedF:                      {Code: 500, ErrCode: 10111, Description: \"{err}\"},\n\t\tJSStreamSealedErr:                          {Code: 400, ErrCode: 10109, Description: \"invalid operation on sealed stream\"},\n\t\tJSStreamSequenceNotMatchErr:                {Code: 503, ErrCode: 10063, Description: \"expected stream sequence does not match\"},\n\t\tJSStreamSnapshotErrF:                       {Code: 500, ErrCode: 10064, Description: \"snapshot failed: {err}\"},\n\t\tJSStreamStoreFailedF:                       {Code: 503, ErrCode: 10077, Description: \"{err}\"},\n\t\tJSStreamSubjectOverlapErr:                  {Code: 400, ErrCode: 10065, Description: \"subjects overlap with an existing stream\"},\n\t\tJSStreamTemplateCreateErrF:                 {Code: 500, ErrCode: 10066, Description: \"{err}\"},\n\t\tJSStreamTemplateDeleteErrF:                 {Code: 500, ErrCode: 10067, Description: \"{err}\"},\n\t\tJSStreamTemplateNotFoundErr:                {Code: 404, ErrCode: 10068, Description: \"template not found\"},\n\t\tJSStreamTooManyRequests:                    {Code: 429, ErrCode: 10167, Description: \"too many requests\"},\n\t\tJSStreamTransformInvalidDestination:        {Code: 400, ErrCode: 10156, Description: \"stream transform: {err}\"},\n\t\tJSStreamTransformInvalidSource:             {Code: 400, ErrCode: 10155, Description: \"stream transform source: {err}\"},\n\t\tJSStreamUpdateErrF:                         {Code: 500, ErrCode: 10069, Description: \"{err}\"},\n\t\tJSStreamWrongLastMsgIDErrF:                 {Code: 400, ErrCode: 10070, Description: \"wrong last msg ID: {id}\"},\n\t\tJSStreamWrongLastSequenceConstantErr:       {Code: 400, ErrCode: 10164, Description: \"wrong last sequence\"},\n\t\tJSStreamWrongLastSequenceErrF:              {Code: 400, ErrCode: 10071, Description: \"wrong last sequence: {seq}\"},\n\t\tJSTempStorageFailedErr:                     {Code: 500, ErrCode: 10072, Description: \"JetStream unable to open temp storage for restore\"},\n\t\tJSTemplateNameNotMatchSubjectErr:           {Code: 400, ErrCode: 10073, Description: \"template name in subject does not match request\"},\n\t}\n\t// ErrJetStreamNotClustered Deprecated by JSClusterNotActiveErr ApiError, use IsNatsError() for comparisons\n\tErrJetStreamNotClustered = ApiErrors[JSClusterNotActiveErr]\n\t// ErrJetStreamNotAssigned Deprecated by JSClusterNotAssignedErr ApiError, use IsNatsError() for comparisons\n\tErrJetStreamNotAssigned = ApiErrors[JSClusterNotAssignedErr]\n\t// ErrJetStreamNotLeader Deprecated by JSClusterNotLeaderErr ApiError, use IsNatsError() for comparisons\n\tErrJetStreamNotLeader = ApiErrors[JSClusterNotLeaderErr]\n\t// ErrJetStreamConsumerAlreadyUsed Deprecated by JSConsumerNameExistErr ApiError, use IsNatsError() for comparisons\n\tErrJetStreamConsumerAlreadyUsed = ApiErrors[JSConsumerNameExistErr]\n\t// ErrJetStreamResourcesExceeded Deprecated by JSInsufficientResourcesErr ApiError, use IsNatsError() for comparisons\n\tErrJetStreamResourcesExceeded = ApiErrors[JSInsufficientResourcesErr]\n\t// ErrMemoryResourcesExceeded Deprecated by JSMemoryResourcesExceededErr ApiError, use IsNatsError() for comparisons\n\tErrMemoryResourcesExceeded = ApiErrors[JSMemoryResourcesExceededErr]\n\t// ErrJetStreamNotEnabled Deprecated by JSNotEnabledErr ApiError, use IsNatsError() for comparisons\n\tErrJetStreamNotEnabled = ApiErrors[JSNotEnabledErr]\n\t// ErrStorageResourcesExceeded Deprecated by JSStorageResourcesExceededErr ApiError, use IsNatsError() for comparisons\n\tErrStorageResourcesExceeded = ApiErrors[JSStorageResourcesExceededErr]\n\t// ErrJetStreamStreamAlreadyUsed Deprecated by JSStreamNameExistErr ApiError, use IsNatsError() for comparisons\n\tErrJetStreamStreamAlreadyUsed = ApiErrors[JSStreamNameExistErr]\n\t// ErrJetStreamStreamNotFound Deprecated by JSStreamNotFoundErr ApiError, use IsNatsError() for comparisons\n\tErrJetStreamStreamNotFound = ApiErrors[JSStreamNotFoundErr]\n\t// ErrReplicasNotSupported Deprecated by JSStreamReplicasNotSupportedErr ApiError, use IsNatsError() for comparisons\n\tErrReplicasNotSupported = ApiErrors[JSStreamReplicasNotSupportedErr]\n)\n\n// NewJSAccountResourcesExceededError creates a new JSAccountResourcesExceededErr error: \"resource limits exceeded for account\"\nfunc NewJSAccountResourcesExceededError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSAccountResourcesExceededErr]\n}\n\n// NewJSBadRequestError creates a new JSBadRequestErr error: \"bad request\"\nfunc NewJSBadRequestError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSBadRequestErr]\n}\n\n// NewJSClusterIncompleteError creates a new JSClusterIncompleteErr error: \"incomplete results\"\nfunc NewJSClusterIncompleteError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSClusterIncompleteErr]\n}\n\n// NewJSClusterNoPeersError creates a new JSClusterNoPeersErrF error: \"{err}\"\nfunc NewJSClusterNoPeersError(err error, opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\te := ApiErrors[JSClusterNoPeersErrF]\n\targs := e.toReplacerArgs([]interface{}{\"{err}\", err})\n\treturn &ApiError{\n\t\tCode:        e.Code,\n\t\tErrCode:     e.ErrCode,\n\t\tDescription: strings.NewReplacer(args...).Replace(e.Description),\n\t}\n}\n\n// NewJSClusterNotActiveError creates a new JSClusterNotActiveErr error: \"JetStream not in clustered mode\"\nfunc NewJSClusterNotActiveError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSClusterNotActiveErr]\n}\n\n// NewJSClusterNotAssignedError creates a new JSClusterNotAssignedErr error: \"JetStream cluster not assigned to this server\"\nfunc NewJSClusterNotAssignedError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSClusterNotAssignedErr]\n}\n\n// NewJSClusterNotAvailError creates a new JSClusterNotAvailErr error: \"JetStream system temporarily unavailable\"\nfunc NewJSClusterNotAvailError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSClusterNotAvailErr]\n}\n\n// NewJSClusterNotLeaderError creates a new JSClusterNotLeaderErr error: \"JetStream cluster can not handle request\"\nfunc NewJSClusterNotLeaderError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSClusterNotLeaderErr]\n}\n\n// NewJSClusterPeerNotMemberError creates a new JSClusterPeerNotMemberErr error: \"peer not a member\"\nfunc NewJSClusterPeerNotMemberError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSClusterPeerNotMemberErr]\n}\n\n// NewJSClusterRequiredError creates a new JSClusterRequiredErr error: \"JetStream clustering support required\"\nfunc NewJSClusterRequiredError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSClusterRequiredErr]\n}\n\n// NewJSClusterServerNotMemberError creates a new JSClusterServerNotMemberErr error: \"server is not a member of the cluster\"\nfunc NewJSClusterServerNotMemberError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSClusterServerNotMemberErr]\n}\n\n// NewJSClusterTagsError creates a new JSClusterTagsErr error: \"tags placement not supported for operation\"\nfunc NewJSClusterTagsError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSClusterTagsErr]\n}\n\n// NewJSClusterUnSupportFeatureError creates a new JSClusterUnSupportFeatureErr error: \"not currently supported in clustered mode\"\nfunc NewJSClusterUnSupportFeatureError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSClusterUnSupportFeatureErr]\n}\n\n// NewJSConsumerAlreadyExistsError creates a new JSConsumerAlreadyExists error: \"consumer already exists\"\nfunc NewJSConsumerAlreadyExistsError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSConsumerAlreadyExists]\n}\n\n// NewJSConsumerBadDurableNameError creates a new JSConsumerBadDurableNameErr error: \"durable name can not contain '.', '*', '>'\"\nfunc NewJSConsumerBadDurableNameError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSConsumerBadDurableNameErr]\n}\n\n// NewJSConsumerConfigRequiredError creates a new JSConsumerConfigRequiredErr error: \"consumer config required\"\nfunc NewJSConsumerConfigRequiredError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSConsumerConfigRequiredErr]\n}\n\n// NewJSConsumerCreateDurableAndNameMismatchError creates a new JSConsumerCreateDurableAndNameMismatch error: \"Consumer Durable and Name have to be equal if both are provided\"\nfunc NewJSConsumerCreateDurableAndNameMismatchError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSConsumerCreateDurableAndNameMismatch]\n}\n\n// NewJSConsumerCreateError creates a new JSConsumerCreateErrF error: \"{err}\"\nfunc NewJSConsumerCreateError(err error, opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\te := ApiErrors[JSConsumerCreateErrF]\n\targs := e.toReplacerArgs([]interface{}{\"{err}\", err})\n\treturn &ApiError{\n\t\tCode:        e.Code,\n\t\tErrCode:     e.ErrCode,\n\t\tDescription: strings.NewReplacer(args...).Replace(e.Description),\n\t}\n}\n\n// NewJSConsumerCreateFilterSubjectMismatchError creates a new JSConsumerCreateFilterSubjectMismatchErr error: \"Consumer create request did not match filtered subject from create subject\"\nfunc NewJSConsumerCreateFilterSubjectMismatchError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSConsumerCreateFilterSubjectMismatchErr]\n}\n\n// NewJSConsumerDeliverCycleError creates a new JSConsumerDeliverCycleErr error: \"consumer deliver subject forms a cycle\"\nfunc NewJSConsumerDeliverCycleError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSConsumerDeliverCycleErr]\n}\n\n// NewJSConsumerDeliverToWildcardsError creates a new JSConsumerDeliverToWildcardsErr error: \"consumer deliver subject has wildcards\"\nfunc NewJSConsumerDeliverToWildcardsError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSConsumerDeliverToWildcardsErr]\n}\n\n// NewJSConsumerDescriptionTooLongError creates a new JSConsumerDescriptionTooLongErrF error: \"consumer description is too long, maximum allowed is {max}\"\nfunc NewJSConsumerDescriptionTooLongError(max interface{}, opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\te := ApiErrors[JSConsumerDescriptionTooLongErrF]\n\targs := e.toReplacerArgs([]interface{}{\"{max}\", max})\n\treturn &ApiError{\n\t\tCode:        e.Code,\n\t\tErrCode:     e.ErrCode,\n\t\tDescription: strings.NewReplacer(args...).Replace(e.Description),\n\t}\n}\n\n// NewJSConsumerDirectRequiresEphemeralError creates a new JSConsumerDirectRequiresEphemeralErr error: \"consumer direct requires an ephemeral consumer\"\nfunc NewJSConsumerDirectRequiresEphemeralError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSConsumerDirectRequiresEphemeralErr]\n}\n\n// NewJSConsumerDirectRequiresPushError creates a new JSConsumerDirectRequiresPushErr error: \"consumer direct requires a push based consumer\"\nfunc NewJSConsumerDirectRequiresPushError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSConsumerDirectRequiresPushErr]\n}\n\n// NewJSConsumerDoesNotExistError creates a new JSConsumerDoesNotExist error: \"consumer does not exist\"\nfunc NewJSConsumerDoesNotExistError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSConsumerDoesNotExist]\n}\n\n// NewJSConsumerDuplicateFilterSubjectsError creates a new JSConsumerDuplicateFilterSubjects error: \"consumer cannot have both FilterSubject and FilterSubjects specified\"\nfunc NewJSConsumerDuplicateFilterSubjectsError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSConsumerDuplicateFilterSubjects]\n}\n\n// NewJSConsumerDurableNameNotInSubjectError creates a new JSConsumerDurableNameNotInSubjectErr error: \"consumer expected to be durable but no durable name set in subject\"\nfunc NewJSConsumerDurableNameNotInSubjectError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSConsumerDurableNameNotInSubjectErr]\n}\n\n// NewJSConsumerDurableNameNotMatchSubjectError creates a new JSConsumerDurableNameNotMatchSubjectErr error: \"consumer name in subject does not match durable name in request\"\nfunc NewJSConsumerDurableNameNotMatchSubjectError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSConsumerDurableNameNotMatchSubjectErr]\n}\n\n// NewJSConsumerDurableNameNotSetError creates a new JSConsumerDurableNameNotSetErr error: \"consumer expected to be durable but a durable name was not set\"\nfunc NewJSConsumerDurableNameNotSetError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSConsumerDurableNameNotSetErr]\n}\n\n// NewJSConsumerEmptyFilterError creates a new JSConsumerEmptyFilter error: \"consumer filter in FilterSubjects cannot be empty\"\nfunc NewJSConsumerEmptyFilterError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSConsumerEmptyFilter]\n}\n\n// NewJSConsumerEmptyGroupNameError creates a new JSConsumerEmptyGroupName error: \"Group name cannot be an empty string\"\nfunc NewJSConsumerEmptyGroupNameError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSConsumerEmptyGroupName]\n}\n\n// NewJSConsumerEphemeralWithDurableInSubjectError creates a new JSConsumerEphemeralWithDurableInSubjectErr error: \"consumer expected to be ephemeral but detected a durable name set in subject\"\nfunc NewJSConsumerEphemeralWithDurableInSubjectError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSConsumerEphemeralWithDurableInSubjectErr]\n}\n\n// NewJSConsumerEphemeralWithDurableNameError creates a new JSConsumerEphemeralWithDurableNameErr error: \"consumer expected to be ephemeral but a durable name was set in request\"\nfunc NewJSConsumerEphemeralWithDurableNameError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSConsumerEphemeralWithDurableNameErr]\n}\n\n// NewJSConsumerExistingActiveError creates a new JSConsumerExistingActiveErr error: \"consumer already exists and is still active\"\nfunc NewJSConsumerExistingActiveError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSConsumerExistingActiveErr]\n}\n\n// NewJSConsumerFCRequiresPushError creates a new JSConsumerFCRequiresPushErr error: \"consumer flow control requires a push based consumer\"\nfunc NewJSConsumerFCRequiresPushError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSConsumerFCRequiresPushErr]\n}\n\n// NewJSConsumerFilterNotSubsetError creates a new JSConsumerFilterNotSubsetErr error: \"consumer filter subject is not a valid subset of the interest subjects\"\nfunc NewJSConsumerFilterNotSubsetError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSConsumerFilterNotSubsetErr]\n}\n\n// NewJSConsumerHBRequiresPushError creates a new JSConsumerHBRequiresPushErr error: \"consumer idle heartbeat requires a push based consumer\"\nfunc NewJSConsumerHBRequiresPushError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSConsumerHBRequiresPushErr]\n}\n\n// NewJSConsumerInactiveThresholdExcessError creates a new JSConsumerInactiveThresholdExcess error: \"consumer inactive threshold exceeds system limit of {limit}\"\nfunc NewJSConsumerInactiveThresholdExcessError(limit interface{}, opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\te := ApiErrors[JSConsumerInactiveThresholdExcess]\n\targs := e.toReplacerArgs([]interface{}{\"{limit}\", limit})\n\treturn &ApiError{\n\t\tCode:        e.Code,\n\t\tErrCode:     e.ErrCode,\n\t\tDescription: strings.NewReplacer(args...).Replace(e.Description),\n\t}\n}\n\n// NewJSConsumerInvalidDeliverSubjectError creates a new JSConsumerInvalidDeliverSubject error: \"invalid push consumer deliver subject\"\nfunc NewJSConsumerInvalidDeliverSubjectError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSConsumerInvalidDeliverSubject]\n}\n\n// NewJSConsumerInvalidGroupNameError creates a new JSConsumerInvalidGroupNameErr error: \"Valid priority group name must match A-Z, a-z, 0-9, -_/=)+ and may not exceed 16 characters\"\nfunc NewJSConsumerInvalidGroupNameError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSConsumerInvalidGroupNameErr]\n}\n\n// NewJSConsumerInvalidPolicyError creates a new JSConsumerInvalidPolicyErrF error: \"{err}\"\nfunc NewJSConsumerInvalidPolicyError(err error, opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\te := ApiErrors[JSConsumerInvalidPolicyErrF]\n\targs := e.toReplacerArgs([]interface{}{\"{err}\", err})\n\treturn &ApiError{\n\t\tCode:        e.Code,\n\t\tErrCode:     e.ErrCode,\n\t\tDescription: strings.NewReplacer(args...).Replace(e.Description),\n\t}\n}\n\n// NewJSConsumerInvalidPriorityGroupError creates a new JSConsumerInvalidPriorityGroupErr error: \"Provided priority group does not exist for this consumer\"\nfunc NewJSConsumerInvalidPriorityGroupError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSConsumerInvalidPriorityGroupErr]\n}\n\n// NewJSConsumerInvalidSamplingError creates a new JSConsumerInvalidSamplingErrF error: \"failed to parse consumer sampling configuration: {err}\"\nfunc NewJSConsumerInvalidSamplingError(err error, opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\te := ApiErrors[JSConsumerInvalidSamplingErrF]\n\targs := e.toReplacerArgs([]interface{}{\"{err}\", err})\n\treturn &ApiError{\n\t\tCode:        e.Code,\n\t\tErrCode:     e.ErrCode,\n\t\tDescription: strings.NewReplacer(args...).Replace(e.Description),\n\t}\n}\n\n// NewJSConsumerMaxDeliverBackoffError creates a new JSConsumerMaxDeliverBackoffErr error: \"max deliver is required to be > length of backoff values\"\nfunc NewJSConsumerMaxDeliverBackoffError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSConsumerMaxDeliverBackoffErr]\n}\n\n// NewJSConsumerMaxPendingAckExcessError creates a new JSConsumerMaxPendingAckExcessErrF error: \"consumer max ack pending exceeds system limit of {limit}\"\nfunc NewJSConsumerMaxPendingAckExcessError(limit interface{}, opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\te := ApiErrors[JSConsumerMaxPendingAckExcessErrF]\n\targs := e.toReplacerArgs([]interface{}{\"{limit}\", limit})\n\treturn &ApiError{\n\t\tCode:        e.Code,\n\t\tErrCode:     e.ErrCode,\n\t\tDescription: strings.NewReplacer(args...).Replace(e.Description),\n\t}\n}\n\n// NewJSConsumerMaxPendingAckPolicyRequiredError creates a new JSConsumerMaxPendingAckPolicyRequiredErr error: \"consumer requires ack policy for max ack pending\"\nfunc NewJSConsumerMaxPendingAckPolicyRequiredError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSConsumerMaxPendingAckPolicyRequiredErr]\n}\n\n// NewJSConsumerMaxRequestBatchExceededError creates a new JSConsumerMaxRequestBatchExceededF error: \"consumer max request batch exceeds server limit of {limit}\"\nfunc NewJSConsumerMaxRequestBatchExceededError(limit interface{}, opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\te := ApiErrors[JSConsumerMaxRequestBatchExceededF]\n\targs := e.toReplacerArgs([]interface{}{\"{limit}\", limit})\n\treturn &ApiError{\n\t\tCode:        e.Code,\n\t\tErrCode:     e.ErrCode,\n\t\tDescription: strings.NewReplacer(args...).Replace(e.Description),\n\t}\n}\n\n// NewJSConsumerMaxRequestBatchNegativeError creates a new JSConsumerMaxRequestBatchNegativeErr error: \"consumer max request batch needs to be > 0\"\nfunc NewJSConsumerMaxRequestBatchNegativeError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSConsumerMaxRequestBatchNegativeErr]\n}\n\n// NewJSConsumerMaxRequestExpiresToSmallError creates a new JSConsumerMaxRequestExpiresToSmall error: \"consumer max request expires needs to be >= 1ms\"\nfunc NewJSConsumerMaxRequestExpiresToSmallError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSConsumerMaxRequestExpiresToSmall]\n}\n\n// NewJSConsumerMaxWaitingNegativeError creates a new JSConsumerMaxWaitingNegativeErr error: \"consumer max waiting needs to be positive\"\nfunc NewJSConsumerMaxWaitingNegativeError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSConsumerMaxWaitingNegativeErr]\n}\n\n// NewJSConsumerMetadataLengthError creates a new JSConsumerMetadataLengthErrF error: \"consumer metadata exceeds maximum size of {limit}\"\nfunc NewJSConsumerMetadataLengthError(limit interface{}, opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\te := ApiErrors[JSConsumerMetadataLengthErrF]\n\targs := e.toReplacerArgs([]interface{}{\"{limit}\", limit})\n\treturn &ApiError{\n\t\tCode:        e.Code,\n\t\tErrCode:     e.ErrCode,\n\t\tDescription: strings.NewReplacer(args...).Replace(e.Description),\n\t}\n}\n\n// NewJSConsumerMultipleFiltersNotAllowedError creates a new JSConsumerMultipleFiltersNotAllowed error: \"consumer with multiple subject filters cannot use subject based API\"\nfunc NewJSConsumerMultipleFiltersNotAllowedError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSConsumerMultipleFiltersNotAllowed]\n}\n\n// NewJSConsumerNameContainsPathSeparatorsError creates a new JSConsumerNameContainsPathSeparatorsErr error: \"Consumer name can not contain path separators\"\nfunc NewJSConsumerNameContainsPathSeparatorsError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSConsumerNameContainsPathSeparatorsErr]\n}\n\n// NewJSConsumerNameExistError creates a new JSConsumerNameExistErr error: \"consumer name already in use\"\nfunc NewJSConsumerNameExistError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSConsumerNameExistErr]\n}\n\n// NewJSConsumerNameTooLongError creates a new JSConsumerNameTooLongErrF error: \"consumer name is too long, maximum allowed is {max}\"\nfunc NewJSConsumerNameTooLongError(max interface{}, opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\te := ApiErrors[JSConsumerNameTooLongErrF]\n\targs := e.toReplacerArgs([]interface{}{\"{max}\", max})\n\treturn &ApiError{\n\t\tCode:        e.Code,\n\t\tErrCode:     e.ErrCode,\n\t\tDescription: strings.NewReplacer(args...).Replace(e.Description),\n\t}\n}\n\n// NewJSConsumerNotFoundError creates a new JSConsumerNotFoundErr error: \"consumer not found\"\nfunc NewJSConsumerNotFoundError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSConsumerNotFoundErr]\n}\n\n// NewJSConsumerOfflineError creates a new JSConsumerOfflineErr error: \"consumer is offline\"\nfunc NewJSConsumerOfflineError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSConsumerOfflineErr]\n}\n\n// NewJSConsumerOnMappedError creates a new JSConsumerOnMappedErr error: \"consumer direct on a mapped consumer\"\nfunc NewJSConsumerOnMappedError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSConsumerOnMappedErr]\n}\n\n// NewJSConsumerOverlappingSubjectFiltersError creates a new JSConsumerOverlappingSubjectFilters error: \"consumer subject filters cannot overlap\"\nfunc NewJSConsumerOverlappingSubjectFiltersError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSConsumerOverlappingSubjectFilters]\n}\n\n// NewJSConsumerPriorityPolicyWithoutGroupError creates a new JSConsumerPriorityPolicyWithoutGroup error: \"Setting PriorityPolicy requires at least one PriorityGroup to be set\"\nfunc NewJSConsumerPriorityPolicyWithoutGroupError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSConsumerPriorityPolicyWithoutGroup]\n}\n\n// NewJSConsumerPullNotDurableError creates a new JSConsumerPullNotDurableErr error: \"consumer in pull mode requires a durable name\"\nfunc NewJSConsumerPullNotDurableError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSConsumerPullNotDurableErr]\n}\n\n// NewJSConsumerPullRequiresAckError creates a new JSConsumerPullRequiresAckErr error: \"consumer in pull mode requires explicit ack policy on workqueue stream\"\nfunc NewJSConsumerPullRequiresAckError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSConsumerPullRequiresAckErr]\n}\n\n// NewJSConsumerPullWithRateLimitError creates a new JSConsumerPullWithRateLimitErr error: \"consumer in pull mode can not have rate limit set\"\nfunc NewJSConsumerPullWithRateLimitError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSConsumerPullWithRateLimitErr]\n}\n\n// NewJSConsumerPushMaxWaitingError creates a new JSConsumerPushMaxWaitingErr error: \"consumer in push mode can not set max waiting\"\nfunc NewJSConsumerPushMaxWaitingError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSConsumerPushMaxWaitingErr]\n}\n\n// NewJSConsumerReplacementWithDifferentNameError creates a new JSConsumerReplacementWithDifferentNameErr error: \"consumer replacement durable config not the same\"\nfunc NewJSConsumerReplacementWithDifferentNameError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSConsumerReplacementWithDifferentNameErr]\n}\n\n// NewJSConsumerReplicasExceedsStreamError creates a new JSConsumerReplicasExceedsStream error: \"consumer config replica count exceeds parent stream\"\nfunc NewJSConsumerReplicasExceedsStreamError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSConsumerReplicasExceedsStream]\n}\n\n// NewJSConsumerReplicasShouldMatchStreamError creates a new JSConsumerReplicasShouldMatchStream error: \"consumer config replicas must match interest retention stream's replicas\"\nfunc NewJSConsumerReplicasShouldMatchStreamError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSConsumerReplicasShouldMatchStream]\n}\n\n// NewJSConsumerSmallHeartbeatError creates a new JSConsumerSmallHeartbeatErr error: \"consumer idle heartbeat needs to be >= 100ms\"\nfunc NewJSConsumerSmallHeartbeatError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSConsumerSmallHeartbeatErr]\n}\n\n// NewJSConsumerStoreFailedError creates a new JSConsumerStoreFailedErrF error: \"error creating store for consumer: {err}\"\nfunc NewJSConsumerStoreFailedError(err error, opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\te := ApiErrors[JSConsumerStoreFailedErrF]\n\targs := e.toReplacerArgs([]interface{}{\"{err}\", err})\n\treturn &ApiError{\n\t\tCode:        e.Code,\n\t\tErrCode:     e.ErrCode,\n\t\tDescription: strings.NewReplacer(args...).Replace(e.Description),\n\t}\n}\n\n// NewJSConsumerWQConsumerNotDeliverAllError creates a new JSConsumerWQConsumerNotDeliverAllErr error: \"consumer must be deliver all on workqueue stream\"\nfunc NewJSConsumerWQConsumerNotDeliverAllError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSConsumerWQConsumerNotDeliverAllErr]\n}\n\n// NewJSConsumerWQConsumerNotUniqueError creates a new JSConsumerWQConsumerNotUniqueErr error: \"filtered consumer not unique on workqueue stream\"\nfunc NewJSConsumerWQConsumerNotUniqueError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSConsumerWQConsumerNotUniqueErr]\n}\n\n// NewJSConsumerWQMultipleUnfilteredError creates a new JSConsumerWQMultipleUnfilteredErr error: \"multiple non-filtered consumers not allowed on workqueue stream\"\nfunc NewJSConsumerWQMultipleUnfilteredError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSConsumerWQMultipleUnfilteredErr]\n}\n\n// NewJSConsumerWQRequiresExplicitAckError creates a new JSConsumerWQRequiresExplicitAckErr error: \"workqueue stream requires explicit ack\"\nfunc NewJSConsumerWQRequiresExplicitAckError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSConsumerWQRequiresExplicitAckErr]\n}\n\n// NewJSConsumerWithFlowControlNeedsHeartbeatsError creates a new JSConsumerWithFlowControlNeedsHeartbeats error: \"consumer with flow control also needs heartbeats\"\nfunc NewJSConsumerWithFlowControlNeedsHeartbeatsError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSConsumerWithFlowControlNeedsHeartbeats]\n}\n\n// NewJSInsufficientResourcesError creates a new JSInsufficientResourcesErr error: \"insufficient resources\"\nfunc NewJSInsufficientResourcesError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSInsufficientResourcesErr]\n}\n\n// NewJSInvalidJSONError creates a new JSInvalidJSONErr error: \"invalid JSON: {err}\"\nfunc NewJSInvalidJSONError(err error, opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\te := ApiErrors[JSInvalidJSONErr]\n\targs := e.toReplacerArgs([]interface{}{\"{err}\", err})\n\treturn &ApiError{\n\t\tCode:        e.Code,\n\t\tErrCode:     e.ErrCode,\n\t\tDescription: strings.NewReplacer(args...).Replace(e.Description),\n\t}\n}\n\n// NewJSMaximumConsumersLimitError creates a new JSMaximumConsumersLimitErr error: \"maximum consumers limit reached\"\nfunc NewJSMaximumConsumersLimitError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSMaximumConsumersLimitErr]\n}\n\n// NewJSMaximumStreamsLimitError creates a new JSMaximumStreamsLimitErr error: \"maximum number of streams reached\"\nfunc NewJSMaximumStreamsLimitError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSMaximumStreamsLimitErr]\n}\n\n// NewJSMemoryResourcesExceededError creates a new JSMemoryResourcesExceededErr error: \"insufficient memory resources available\"\nfunc NewJSMemoryResourcesExceededError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSMemoryResourcesExceededErr]\n}\n\n// NewJSMessageTTLDisabledError creates a new JSMessageTTLDisabledErr error: \"per-message TTL is disabled\"\nfunc NewJSMessageTTLDisabledError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSMessageTTLDisabledErr]\n}\n\n// NewJSMessageTTLInvalidError creates a new JSMessageTTLInvalidErr error: \"invalid per-message TTL\"\nfunc NewJSMessageTTLInvalidError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSMessageTTLInvalidErr]\n}\n\n// NewJSMirrorConsumerSetupFailedError creates a new JSMirrorConsumerSetupFailedErrF error: \"{err}\"\nfunc NewJSMirrorConsumerSetupFailedError(err error, opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\te := ApiErrors[JSMirrorConsumerSetupFailedErrF]\n\targs := e.toReplacerArgs([]interface{}{\"{err}\", err})\n\treturn &ApiError{\n\t\tCode:        e.Code,\n\t\tErrCode:     e.ErrCode,\n\t\tDescription: strings.NewReplacer(args...).Replace(e.Description),\n\t}\n}\n\n// NewJSMirrorInvalidStreamNameError creates a new JSMirrorInvalidStreamName error: \"mirrored stream name is invalid\"\nfunc NewJSMirrorInvalidStreamNameError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSMirrorInvalidStreamName]\n}\n\n// NewJSMirrorInvalidSubjectFilterError creates a new JSMirrorInvalidSubjectFilter error: \"mirror transform source: {err}\"\nfunc NewJSMirrorInvalidSubjectFilterError(err error, opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\te := ApiErrors[JSMirrorInvalidSubjectFilter]\n\targs := e.toReplacerArgs([]interface{}{\"{err}\", err})\n\treturn &ApiError{\n\t\tCode:        e.Code,\n\t\tErrCode:     e.ErrCode,\n\t\tDescription: strings.NewReplacer(args...).Replace(e.Description),\n\t}\n}\n\n// NewJSMirrorInvalidTransformDestinationError creates a new JSMirrorInvalidTransformDestination error: \"mirror transform: {err}\"\nfunc NewJSMirrorInvalidTransformDestinationError(err error, opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\te := ApiErrors[JSMirrorInvalidTransformDestination]\n\targs := e.toReplacerArgs([]interface{}{\"{err}\", err})\n\treturn &ApiError{\n\t\tCode:        e.Code,\n\t\tErrCode:     e.ErrCode,\n\t\tDescription: strings.NewReplacer(args...).Replace(e.Description),\n\t}\n}\n\n// NewJSMirrorMaxMessageSizeTooBigError creates a new JSMirrorMaxMessageSizeTooBigErr error: \"stream mirror must have max message size >= source\"\nfunc NewJSMirrorMaxMessageSizeTooBigError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSMirrorMaxMessageSizeTooBigErr]\n}\n\n// NewJSMirrorMultipleFiltersNotAllowedError creates a new JSMirrorMultipleFiltersNotAllowed error: \"mirror with multiple subject transforms cannot also have a single subject filter\"\nfunc NewJSMirrorMultipleFiltersNotAllowedError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSMirrorMultipleFiltersNotAllowed]\n}\n\n// NewJSMirrorOverlappingSubjectFiltersError creates a new JSMirrorOverlappingSubjectFilters error: \"mirror subject filters can not overlap\"\nfunc NewJSMirrorOverlappingSubjectFiltersError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSMirrorOverlappingSubjectFilters]\n}\n\n// NewJSMirrorWithFirstSeqError creates a new JSMirrorWithFirstSeqErr error: \"stream mirrors can not have first sequence configured\"\nfunc NewJSMirrorWithFirstSeqError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSMirrorWithFirstSeqErr]\n}\n\n// NewJSMirrorWithSourcesError creates a new JSMirrorWithSourcesErr error: \"stream mirrors can not also contain other sources\"\nfunc NewJSMirrorWithSourcesError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSMirrorWithSourcesErr]\n}\n\n// NewJSMirrorWithStartSeqAndTimeError creates a new JSMirrorWithStartSeqAndTimeErr error: \"stream mirrors can not have both start seq and start time configured\"\nfunc NewJSMirrorWithStartSeqAndTimeError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSMirrorWithStartSeqAndTimeErr]\n}\n\n// NewJSMirrorWithSubjectFiltersError creates a new JSMirrorWithSubjectFiltersErr error: \"stream mirrors can not contain filtered subjects\"\nfunc NewJSMirrorWithSubjectFiltersError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSMirrorWithSubjectFiltersErr]\n}\n\n// NewJSMirrorWithSubjectsError creates a new JSMirrorWithSubjectsErr error: \"stream mirrors can not contain subjects\"\nfunc NewJSMirrorWithSubjectsError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSMirrorWithSubjectsErr]\n}\n\n// NewJSNoAccountError creates a new JSNoAccountErr error: \"account not found\"\nfunc NewJSNoAccountError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSNoAccountErr]\n}\n\n// NewJSNoLimitsError creates a new JSNoLimitsErr error: \"no JetStream default or applicable tiered limit present\"\nfunc NewJSNoLimitsError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSNoLimitsErr]\n}\n\n// NewJSNoMessageFoundError creates a new JSNoMessageFoundErr error: \"no message found\"\nfunc NewJSNoMessageFoundError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSNoMessageFoundErr]\n}\n\n// NewJSNotEmptyRequestError creates a new JSNotEmptyRequestErr error: \"expected an empty request payload\"\nfunc NewJSNotEmptyRequestError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSNotEmptyRequestErr]\n}\n\n// NewJSNotEnabledError creates a new JSNotEnabledErr error: \"JetStream not enabled\"\nfunc NewJSNotEnabledError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSNotEnabledErr]\n}\n\n// NewJSNotEnabledForAccountError creates a new JSNotEnabledForAccountErr error: \"JetStream not enabled for account\"\nfunc NewJSNotEnabledForAccountError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSNotEnabledForAccountErr]\n}\n\n// NewJSPedanticError creates a new JSPedanticErrF error: \"pedantic mode: {err}\"\nfunc NewJSPedanticError(err error, opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\te := ApiErrors[JSPedanticErrF]\n\targs := e.toReplacerArgs([]interface{}{\"{err}\", err})\n\treturn &ApiError{\n\t\tCode:        e.Code,\n\t\tErrCode:     e.ErrCode,\n\t\tDescription: strings.NewReplacer(args...).Replace(e.Description),\n\t}\n}\n\n// NewJSPeerRemapError creates a new JSPeerRemapErr error: \"peer remap failed\"\nfunc NewJSPeerRemapError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSPeerRemapErr]\n}\n\n// NewJSRaftGeneralError creates a new JSRaftGeneralErrF error: \"{err}\"\nfunc NewJSRaftGeneralError(err error, opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\te := ApiErrors[JSRaftGeneralErrF]\n\targs := e.toReplacerArgs([]interface{}{\"{err}\", err})\n\treturn &ApiError{\n\t\tCode:        e.Code,\n\t\tErrCode:     e.ErrCode,\n\t\tDescription: strings.NewReplacer(args...).Replace(e.Description),\n\t}\n}\n\n// NewJSReplicasCountCannotBeNegativeError creates a new JSReplicasCountCannotBeNegative error: \"replicas count cannot be negative\"\nfunc NewJSReplicasCountCannotBeNegativeError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSReplicasCountCannotBeNegative]\n}\n\n// NewJSRestoreSubscribeFailedError creates a new JSRestoreSubscribeFailedErrF error: \"JetStream unable to subscribe to restore snapshot {subject}: {err}\"\nfunc NewJSRestoreSubscribeFailedError(err error, subject interface{}, opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\te := ApiErrors[JSRestoreSubscribeFailedErrF]\n\targs := e.toReplacerArgs([]interface{}{\"{err}\", err, \"{subject}\", subject})\n\treturn &ApiError{\n\t\tCode:        e.Code,\n\t\tErrCode:     e.ErrCode,\n\t\tDescription: strings.NewReplacer(args...).Replace(e.Description),\n\t}\n}\n\n// NewJSSequenceNotFoundError creates a new JSSequenceNotFoundErrF error: \"sequence {seq} not found\"\nfunc NewJSSequenceNotFoundError(seq uint64, opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\te := ApiErrors[JSSequenceNotFoundErrF]\n\targs := e.toReplacerArgs([]interface{}{\"{seq}\", seq})\n\treturn &ApiError{\n\t\tCode:        e.Code,\n\t\tErrCode:     e.ErrCode,\n\t\tDescription: strings.NewReplacer(args...).Replace(e.Description),\n\t}\n}\n\n// NewJSSnapshotDeliverSubjectInvalidError creates a new JSSnapshotDeliverSubjectInvalidErr error: \"deliver subject not valid\"\nfunc NewJSSnapshotDeliverSubjectInvalidError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSSnapshotDeliverSubjectInvalidErr]\n}\n\n// NewJSSourceConsumerSetupFailedError creates a new JSSourceConsumerSetupFailedErrF error: \"{err}\"\nfunc NewJSSourceConsumerSetupFailedError(err error, opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\te := ApiErrors[JSSourceConsumerSetupFailedErrF]\n\targs := e.toReplacerArgs([]interface{}{\"{err}\", err})\n\treturn &ApiError{\n\t\tCode:        e.Code,\n\t\tErrCode:     e.ErrCode,\n\t\tDescription: strings.NewReplacer(args...).Replace(e.Description),\n\t}\n}\n\n// NewJSSourceDuplicateDetectedError creates a new JSSourceDuplicateDetected error: \"duplicate source configuration detected\"\nfunc NewJSSourceDuplicateDetectedError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSSourceDuplicateDetected]\n}\n\n// NewJSSourceInvalidStreamNameError creates a new JSSourceInvalidStreamName error: \"sourced stream name is invalid\"\nfunc NewJSSourceInvalidStreamNameError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSSourceInvalidStreamName]\n}\n\n// NewJSSourceInvalidSubjectFilterError creates a new JSSourceInvalidSubjectFilter error: \"source transform source: {err}\"\nfunc NewJSSourceInvalidSubjectFilterError(err error, opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\te := ApiErrors[JSSourceInvalidSubjectFilter]\n\targs := e.toReplacerArgs([]interface{}{\"{err}\", err})\n\treturn &ApiError{\n\t\tCode:        e.Code,\n\t\tErrCode:     e.ErrCode,\n\t\tDescription: strings.NewReplacer(args...).Replace(e.Description),\n\t}\n}\n\n// NewJSSourceInvalidTransformDestinationError creates a new JSSourceInvalidTransformDestination error: \"source transform: {err}\"\nfunc NewJSSourceInvalidTransformDestinationError(err error, opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\te := ApiErrors[JSSourceInvalidTransformDestination]\n\targs := e.toReplacerArgs([]interface{}{\"{err}\", err})\n\treturn &ApiError{\n\t\tCode:        e.Code,\n\t\tErrCode:     e.ErrCode,\n\t\tDescription: strings.NewReplacer(args...).Replace(e.Description),\n\t}\n}\n\n// NewJSSourceMaxMessageSizeTooBigError creates a new JSSourceMaxMessageSizeTooBigErr error: \"stream source must have max message size >= target\"\nfunc NewJSSourceMaxMessageSizeTooBigError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSSourceMaxMessageSizeTooBigErr]\n}\n\n// NewJSSourceMultipleFiltersNotAllowedError creates a new JSSourceMultipleFiltersNotAllowed error: \"source with multiple subject transforms cannot also have a single subject filter\"\nfunc NewJSSourceMultipleFiltersNotAllowedError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSSourceMultipleFiltersNotAllowed]\n}\n\n// NewJSSourceOverlappingSubjectFiltersError creates a new JSSourceOverlappingSubjectFilters error: \"source filters can not overlap\"\nfunc NewJSSourceOverlappingSubjectFiltersError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSSourceOverlappingSubjectFilters]\n}\n\n// NewJSStorageResourcesExceededError creates a new JSStorageResourcesExceededErr error: \"insufficient storage resources available\"\nfunc NewJSStorageResourcesExceededError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSStorageResourcesExceededErr]\n}\n\n// NewJSStreamAssignmentError creates a new JSStreamAssignmentErrF error: \"{err}\"\nfunc NewJSStreamAssignmentError(err error, opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\te := ApiErrors[JSStreamAssignmentErrF]\n\targs := e.toReplacerArgs([]interface{}{\"{err}\", err})\n\treturn &ApiError{\n\t\tCode:        e.Code,\n\t\tErrCode:     e.ErrCode,\n\t\tDescription: strings.NewReplacer(args...).Replace(e.Description),\n\t}\n}\n\n// NewJSStreamCreateError creates a new JSStreamCreateErrF error: \"{err}\"\nfunc NewJSStreamCreateError(err error, opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\te := ApiErrors[JSStreamCreateErrF]\n\targs := e.toReplacerArgs([]interface{}{\"{err}\", err})\n\treturn &ApiError{\n\t\tCode:        e.Code,\n\t\tErrCode:     e.ErrCode,\n\t\tDescription: strings.NewReplacer(args...).Replace(e.Description),\n\t}\n}\n\n// NewJSStreamDeleteError creates a new JSStreamDeleteErrF error: \"{err}\"\nfunc NewJSStreamDeleteError(err error, opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\te := ApiErrors[JSStreamDeleteErrF]\n\targs := e.toReplacerArgs([]interface{}{\"{err}\", err})\n\treturn &ApiError{\n\t\tCode:        e.Code,\n\t\tErrCode:     e.ErrCode,\n\t\tDescription: strings.NewReplacer(args...).Replace(e.Description),\n\t}\n}\n\n// NewJSStreamDuplicateMessageConflictError creates a new JSStreamDuplicateMessageConflict error: \"duplicate message id is in process\"\nfunc NewJSStreamDuplicateMessageConflictError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSStreamDuplicateMessageConflict]\n}\n\n// NewJSStreamExpectedLastSeqPerSubjectNotReadyError creates a new JSStreamExpectedLastSeqPerSubjectNotReady error: \"expected last sequence per subject temporarily unavailable\"\nfunc NewJSStreamExpectedLastSeqPerSubjectNotReadyError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSStreamExpectedLastSeqPerSubjectNotReady]\n}\n\n// NewJSStreamExternalApiOverlapError creates a new JSStreamExternalApiOverlapErrF error: \"stream external api prefix {prefix} must not overlap with {subject}\"\nfunc NewJSStreamExternalApiOverlapError(prefix interface{}, subject interface{}, opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\te := ApiErrors[JSStreamExternalApiOverlapErrF]\n\targs := e.toReplacerArgs([]interface{}{\"{prefix}\", prefix, \"{subject}\", subject})\n\treturn &ApiError{\n\t\tCode:        e.Code,\n\t\tErrCode:     e.ErrCode,\n\t\tDescription: strings.NewReplacer(args...).Replace(e.Description),\n\t}\n}\n\n// NewJSStreamExternalDelPrefixOverlapsError creates a new JSStreamExternalDelPrefixOverlapsErrF error: \"stream external delivery prefix {prefix} overlaps with stream subject {subject}\"\nfunc NewJSStreamExternalDelPrefixOverlapsError(prefix interface{}, subject interface{}, opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\te := ApiErrors[JSStreamExternalDelPrefixOverlapsErrF]\n\targs := e.toReplacerArgs([]interface{}{\"{prefix}\", prefix, \"{subject}\", subject})\n\treturn &ApiError{\n\t\tCode:        e.Code,\n\t\tErrCode:     e.ErrCode,\n\t\tDescription: strings.NewReplacer(args...).Replace(e.Description),\n\t}\n}\n\n// NewJSStreamGeneralError creates a new JSStreamGeneralErrorF error: \"{err}\"\nfunc NewJSStreamGeneralError(err error, opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\te := ApiErrors[JSStreamGeneralErrorF]\n\targs := e.toReplacerArgs([]interface{}{\"{err}\", err})\n\treturn &ApiError{\n\t\tCode:        e.Code,\n\t\tErrCode:     e.ErrCode,\n\t\tDescription: strings.NewReplacer(args...).Replace(e.Description),\n\t}\n}\n\n// NewJSStreamHeaderExceedsMaximumError creates a new JSStreamHeaderExceedsMaximumErr error: \"header size exceeds maximum allowed of 64k\"\nfunc NewJSStreamHeaderExceedsMaximumError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSStreamHeaderExceedsMaximumErr]\n}\n\n// NewJSStreamInfoMaxSubjectsError creates a new JSStreamInfoMaxSubjectsErr error: \"subject details would exceed maximum allowed\"\nfunc NewJSStreamInfoMaxSubjectsError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSStreamInfoMaxSubjectsErr]\n}\n\n// NewJSStreamInvalidConfigError creates a new JSStreamInvalidConfigF error: \"{err}\"\nfunc NewJSStreamInvalidConfigError(err error, opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\te := ApiErrors[JSStreamInvalidConfigF]\n\targs := e.toReplacerArgs([]interface{}{\"{err}\", err})\n\treturn &ApiError{\n\t\tCode:        e.Code,\n\t\tErrCode:     e.ErrCode,\n\t\tDescription: strings.NewReplacer(args...).Replace(e.Description),\n\t}\n}\n\n// NewJSStreamInvalidError creates a new JSStreamInvalidErr error: \"stream not valid\"\nfunc NewJSStreamInvalidError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSStreamInvalidErr]\n}\n\n// NewJSStreamInvalidExternalDeliverySubjError creates a new JSStreamInvalidExternalDeliverySubjErrF error: \"stream external delivery prefix {prefix} must not contain wildcards\"\nfunc NewJSStreamInvalidExternalDeliverySubjError(prefix interface{}, opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\te := ApiErrors[JSStreamInvalidExternalDeliverySubjErrF]\n\targs := e.toReplacerArgs([]interface{}{\"{prefix}\", prefix})\n\treturn &ApiError{\n\t\tCode:        e.Code,\n\t\tErrCode:     e.ErrCode,\n\t\tDescription: strings.NewReplacer(args...).Replace(e.Description),\n\t}\n}\n\n// NewJSStreamLimitsError creates a new JSStreamLimitsErrF error: \"{err}\"\nfunc NewJSStreamLimitsError(err error, opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\te := ApiErrors[JSStreamLimitsErrF]\n\targs := e.toReplacerArgs([]interface{}{\"{err}\", err})\n\treturn &ApiError{\n\t\tCode:        e.Code,\n\t\tErrCode:     e.ErrCode,\n\t\tDescription: strings.NewReplacer(args...).Replace(e.Description),\n\t}\n}\n\n// NewJSStreamMaxBytesRequiredError creates a new JSStreamMaxBytesRequired error: \"account requires a stream config to have max bytes set\"\nfunc NewJSStreamMaxBytesRequiredError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSStreamMaxBytesRequired]\n}\n\n// NewJSStreamMaxStreamBytesExceededError creates a new JSStreamMaxStreamBytesExceeded error: \"stream max bytes exceeds account limit max stream bytes\"\nfunc NewJSStreamMaxStreamBytesExceededError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSStreamMaxStreamBytesExceeded]\n}\n\n// NewJSStreamMessageExceedsMaximumError creates a new JSStreamMessageExceedsMaximumErr error: \"message size exceeds maximum allowed\"\nfunc NewJSStreamMessageExceedsMaximumError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSStreamMessageExceedsMaximumErr]\n}\n\n// NewJSStreamMirrorNotUpdatableError creates a new JSStreamMirrorNotUpdatableErr error: \"stream mirror configuration can not be updated\"\nfunc NewJSStreamMirrorNotUpdatableError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSStreamMirrorNotUpdatableErr]\n}\n\n// NewJSStreamMismatchError creates a new JSStreamMismatchErr error: \"stream name in subject does not match request\"\nfunc NewJSStreamMismatchError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSStreamMismatchErr]\n}\n\n// NewJSStreamMoveAndScaleError creates a new JSStreamMoveAndScaleErr error: \"can not move and scale a stream in a single update\"\nfunc NewJSStreamMoveAndScaleError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSStreamMoveAndScaleErr]\n}\n\n// NewJSStreamMoveInProgressError creates a new JSStreamMoveInProgressF error: \"stream move already in progress: {msg}\"\nfunc NewJSStreamMoveInProgressError(msg interface{}, opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\te := ApiErrors[JSStreamMoveInProgressF]\n\targs := e.toReplacerArgs([]interface{}{\"{msg}\", msg})\n\treturn &ApiError{\n\t\tCode:        e.Code,\n\t\tErrCode:     e.ErrCode,\n\t\tDescription: strings.NewReplacer(args...).Replace(e.Description),\n\t}\n}\n\n// NewJSStreamMoveNotInProgressError creates a new JSStreamMoveNotInProgress error: \"stream move not in progress\"\nfunc NewJSStreamMoveNotInProgressError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSStreamMoveNotInProgress]\n}\n\n// NewJSStreamMsgDeleteFailedError creates a new JSStreamMsgDeleteFailedF error: \"{err}\"\nfunc NewJSStreamMsgDeleteFailedError(err error, opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\te := ApiErrors[JSStreamMsgDeleteFailedF]\n\targs := e.toReplacerArgs([]interface{}{\"{err}\", err})\n\treturn &ApiError{\n\t\tCode:        e.Code,\n\t\tErrCode:     e.ErrCode,\n\t\tDescription: strings.NewReplacer(args...).Replace(e.Description),\n\t}\n}\n\n// NewJSStreamNameContainsPathSeparatorsError creates a new JSStreamNameContainsPathSeparatorsErr error: \"Stream name can not contain path separators\"\nfunc NewJSStreamNameContainsPathSeparatorsError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSStreamNameContainsPathSeparatorsErr]\n}\n\n// NewJSStreamNameExistError creates a new JSStreamNameExistErr error: \"stream name already in use with a different configuration\"\nfunc NewJSStreamNameExistError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSStreamNameExistErr]\n}\n\n// NewJSStreamNameExistRestoreFailedError creates a new JSStreamNameExistRestoreFailedErr error: \"stream name already in use, cannot restore\"\nfunc NewJSStreamNameExistRestoreFailedError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSStreamNameExistRestoreFailedErr]\n}\n\n// NewJSStreamNotFoundError creates a new JSStreamNotFoundErr error: \"stream not found\"\nfunc NewJSStreamNotFoundError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSStreamNotFoundErr]\n}\n\n// NewJSStreamNotMatchError creates a new JSStreamNotMatchErr error: \"expected stream does not match\"\nfunc NewJSStreamNotMatchError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSStreamNotMatchErr]\n}\n\n// NewJSStreamOfflineError creates a new JSStreamOfflineErr error: \"stream is offline\"\nfunc NewJSStreamOfflineError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSStreamOfflineErr]\n}\n\n// NewJSStreamPurgeFailedError creates a new JSStreamPurgeFailedF error: \"{err}\"\nfunc NewJSStreamPurgeFailedError(err error, opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\te := ApiErrors[JSStreamPurgeFailedF]\n\targs := e.toReplacerArgs([]interface{}{\"{err}\", err})\n\treturn &ApiError{\n\t\tCode:        e.Code,\n\t\tErrCode:     e.ErrCode,\n\t\tDescription: strings.NewReplacer(args...).Replace(e.Description),\n\t}\n}\n\n// NewJSStreamReplicasNotSupportedError creates a new JSStreamReplicasNotSupportedErr error: \"replicas > 1 not supported in non-clustered mode\"\nfunc NewJSStreamReplicasNotSupportedError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSStreamReplicasNotSupportedErr]\n}\n\n// NewJSStreamReplicasNotUpdatableError creates a new JSStreamReplicasNotUpdatableErr error: \"Replicas configuration can not be updated\"\nfunc NewJSStreamReplicasNotUpdatableError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSStreamReplicasNotUpdatableErr]\n}\n\n// NewJSStreamRestoreError creates a new JSStreamRestoreErrF error: \"restore failed: {err}\"\nfunc NewJSStreamRestoreError(err error, opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\te := ApiErrors[JSStreamRestoreErrF]\n\targs := e.toReplacerArgs([]interface{}{\"{err}\", err})\n\treturn &ApiError{\n\t\tCode:        e.Code,\n\t\tErrCode:     e.ErrCode,\n\t\tDescription: strings.NewReplacer(args...).Replace(e.Description),\n\t}\n}\n\n// NewJSStreamRollupFailedError creates a new JSStreamRollupFailedF error: \"{err}\"\nfunc NewJSStreamRollupFailedError(err error, opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\te := ApiErrors[JSStreamRollupFailedF]\n\targs := e.toReplacerArgs([]interface{}{\"{err}\", err})\n\treturn &ApiError{\n\t\tCode:        e.Code,\n\t\tErrCode:     e.ErrCode,\n\t\tDescription: strings.NewReplacer(args...).Replace(e.Description),\n\t}\n}\n\n// NewJSStreamSealedError creates a new JSStreamSealedErr error: \"invalid operation on sealed stream\"\nfunc NewJSStreamSealedError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSStreamSealedErr]\n}\n\n// NewJSStreamSequenceNotMatchError creates a new JSStreamSequenceNotMatchErr error: \"expected stream sequence does not match\"\nfunc NewJSStreamSequenceNotMatchError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSStreamSequenceNotMatchErr]\n}\n\n// NewJSStreamSnapshotError creates a new JSStreamSnapshotErrF error: \"snapshot failed: {err}\"\nfunc NewJSStreamSnapshotError(err error, opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\te := ApiErrors[JSStreamSnapshotErrF]\n\targs := e.toReplacerArgs([]interface{}{\"{err}\", err})\n\treturn &ApiError{\n\t\tCode:        e.Code,\n\t\tErrCode:     e.ErrCode,\n\t\tDescription: strings.NewReplacer(args...).Replace(e.Description),\n\t}\n}\n\n// NewJSStreamStoreFailedError creates a new JSStreamStoreFailedF error: \"{err}\"\nfunc NewJSStreamStoreFailedError(err error, opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\te := ApiErrors[JSStreamStoreFailedF]\n\targs := e.toReplacerArgs([]interface{}{\"{err}\", err})\n\treturn &ApiError{\n\t\tCode:        e.Code,\n\t\tErrCode:     e.ErrCode,\n\t\tDescription: strings.NewReplacer(args...).Replace(e.Description),\n\t}\n}\n\n// NewJSStreamSubjectOverlapError creates a new JSStreamSubjectOverlapErr error: \"subjects overlap with an existing stream\"\nfunc NewJSStreamSubjectOverlapError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSStreamSubjectOverlapErr]\n}\n\n// NewJSStreamTemplateCreateError creates a new JSStreamTemplateCreateErrF error: \"{err}\"\nfunc NewJSStreamTemplateCreateError(err error, opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\te := ApiErrors[JSStreamTemplateCreateErrF]\n\targs := e.toReplacerArgs([]interface{}{\"{err}\", err})\n\treturn &ApiError{\n\t\tCode:        e.Code,\n\t\tErrCode:     e.ErrCode,\n\t\tDescription: strings.NewReplacer(args...).Replace(e.Description),\n\t}\n}\n\n// NewJSStreamTemplateDeleteError creates a new JSStreamTemplateDeleteErrF error: \"{err}\"\nfunc NewJSStreamTemplateDeleteError(err error, opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\te := ApiErrors[JSStreamTemplateDeleteErrF]\n\targs := e.toReplacerArgs([]interface{}{\"{err}\", err})\n\treturn &ApiError{\n\t\tCode:        e.Code,\n\t\tErrCode:     e.ErrCode,\n\t\tDescription: strings.NewReplacer(args...).Replace(e.Description),\n\t}\n}\n\n// NewJSStreamTemplateNotFoundError creates a new JSStreamTemplateNotFoundErr error: \"template not found\"\nfunc NewJSStreamTemplateNotFoundError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSStreamTemplateNotFoundErr]\n}\n\n// NewJSStreamTooManyRequestsError creates a new JSStreamTooManyRequests error: \"too many requests\"\nfunc NewJSStreamTooManyRequestsError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSStreamTooManyRequests]\n}\n\n// NewJSStreamTransformInvalidDestinationError creates a new JSStreamTransformInvalidDestination error: \"stream transform: {err}\"\nfunc NewJSStreamTransformInvalidDestinationError(err error, opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\te := ApiErrors[JSStreamTransformInvalidDestination]\n\targs := e.toReplacerArgs([]interface{}{\"{err}\", err})\n\treturn &ApiError{\n\t\tCode:        e.Code,\n\t\tErrCode:     e.ErrCode,\n\t\tDescription: strings.NewReplacer(args...).Replace(e.Description),\n\t}\n}\n\n// NewJSStreamTransformInvalidSourceError creates a new JSStreamTransformInvalidSource error: \"stream transform source: {err}\"\nfunc NewJSStreamTransformInvalidSourceError(err error, opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\te := ApiErrors[JSStreamTransformInvalidSource]\n\targs := e.toReplacerArgs([]interface{}{\"{err}\", err})\n\treturn &ApiError{\n\t\tCode:        e.Code,\n\t\tErrCode:     e.ErrCode,\n\t\tDescription: strings.NewReplacer(args...).Replace(e.Description),\n\t}\n}\n\n// NewJSStreamUpdateError creates a new JSStreamUpdateErrF error: \"{err}\"\nfunc NewJSStreamUpdateError(err error, opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\te := ApiErrors[JSStreamUpdateErrF]\n\targs := e.toReplacerArgs([]interface{}{\"{err}\", err})\n\treturn &ApiError{\n\t\tCode:        e.Code,\n\t\tErrCode:     e.ErrCode,\n\t\tDescription: strings.NewReplacer(args...).Replace(e.Description),\n\t}\n}\n\n// NewJSStreamWrongLastMsgIDError creates a new JSStreamWrongLastMsgIDErrF error: \"wrong last msg ID: {id}\"\nfunc NewJSStreamWrongLastMsgIDError(id interface{}, opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\te := ApiErrors[JSStreamWrongLastMsgIDErrF]\n\targs := e.toReplacerArgs([]interface{}{\"{id}\", id})\n\treturn &ApiError{\n\t\tCode:        e.Code,\n\t\tErrCode:     e.ErrCode,\n\t\tDescription: strings.NewReplacer(args...).Replace(e.Description),\n\t}\n}\n\n// NewJSStreamWrongLastSequenceConstantError creates a new JSStreamWrongLastSequenceConstantErr error: \"wrong last sequence\"\nfunc NewJSStreamWrongLastSequenceConstantError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSStreamWrongLastSequenceConstantErr]\n}\n\n// NewJSStreamWrongLastSequenceError creates a new JSStreamWrongLastSequenceErrF error: \"wrong last sequence: {seq}\"\nfunc NewJSStreamWrongLastSequenceError(seq uint64, opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\te := ApiErrors[JSStreamWrongLastSequenceErrF]\n\targs := e.toReplacerArgs([]interface{}{\"{seq}\", seq})\n\treturn &ApiError{\n\t\tCode:        e.Code,\n\t\tErrCode:     e.ErrCode,\n\t\tDescription: strings.NewReplacer(args...).Replace(e.Description),\n\t}\n}\n\n// NewJSTempStorageFailedError creates a new JSTempStorageFailedErr error: \"JetStream unable to open temp storage for restore\"\nfunc NewJSTempStorageFailedError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSTempStorageFailedErr]\n}\n\n// NewJSTemplateNameNotMatchSubjectError creates a new JSTemplateNameNotMatchSubjectErr error: \"template name in subject does not match request\"\nfunc NewJSTemplateNameNotMatchSubjectError(opts ...ErrorOption) *ApiError {\n\teopts := parseOpts(opts)\n\tif ae, ok := eopts.err.(*ApiError); ok {\n\t\treturn ae\n\t}\n\n\treturn ApiErrors[JSTemplateNameNotMatchSubjectErr]\n}\n",
    "source_file": "server/jetstream_errors_generated.go",
    "chunk_type": "code"
  },
  {
    "content": "package server\n\nimport (\n\t\"fmt\"\n)\n\ntype errOpts struct {\n\terr error\n}\n\n// ErrorOption configures a NATS Error helper\ntype ErrorOption func(*errOpts)\n\n// Unless ensures that if err is a ApiErr that err will be returned rather than the one being created via the helper\nfunc Unless(err error) ErrorOption {\n\treturn func(opts *errOpts) {\n\t\topts.err = err\n\t}\n}\n\nfunc parseOpts(opts []ErrorOption) *errOpts {\n\teopts := &errOpts{}\n\tfor _, opt := range opts {\n\t\topt(eopts)\n\t}\n\treturn eopts\n}\n\ntype ErrorIdentifier uint16\n\n// IsNatsErr determines if an error matches ID, if multiple IDs are given if the error matches any of these the function will be true\nfunc IsNatsErr(err error, ids ...ErrorIdentifier) bool {\n\tif err == nil {\n\t\treturn false\n\t}\n\n\tce, ok := err.(*ApiError)\n\tif !ok || ce == nil {\n\t\treturn false\n\t}\n\n\tfor _, id := range ids {\n\t\tae, ok := ApiErrors[id]\n\t\tif !ok || ae == nil {\n\t\t\tcontinue\n\t\t}\n\n\t\tif ce.ErrCode == ae.ErrCode {\n\t\t\treturn true\n\t\t}\n\t}\n\n\treturn false\n}\n\n// ApiError is included in all responses if there was an error.\ntype ApiError struct {\n\tCode        int    `json:\"code\"`\n\tErrCode     uint16 `json:\"err_code,omitempty\"`\n\tDescription string `json:\"description,omitempty\"`\n}\n\n// ErrorsData is the source data for generated errors as found in errors.json\ntype ErrorsData struct {\n\tConstant    string `json:\"constant\"`\n\tCode        int    `json:\"code\"`\n\tErrCode     uint16 `json:\"error_code\"`\n\tDescription string `json:\"description\"`\n\tComment     string `json:\"comment\"`\n\tHelp        string `json:\"help\"`\n\tURL         string `json:\"url\"`\n\tDeprecates  string `json:\"deprecates\"`\n}\n\nfunc (e *ApiError) Error() string {\n\treturn fmt.Sprintf(\"%s (%d)\", e.Description, e.ErrCode)\n}\n\nfunc (e *ApiError) toReplacerArgs(replacements []any) []string {\n\tvar (\n\t\tra  []string\n\t\tkey string\n\t)\n\n\tfor i, replacement := range replacements {\n\t\tif i%2 == 0 {\n\t\t\tkey = replacement.(string)\n\t\t\tcontinue\n\t\t}\n\n\t\tswitch v := replacement.(type) {\n\t\tcase string:\n\t\t\tra = append(ra, key, v)\n\t\tcase error:\n\t\t\tra = append(ra, key, v.Error())\n\t\tdefault:\n\t\t\tra = append(ra, key, fmt.Sprintf(\"%v\", v))\n\t\t}\n\t}\n\n\treturn ra\n}\n",
    "source_file": "server/jetstream_errors.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2016-2025 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage server\n\nimport (\n\t\"crypto/tls\"\n)\n\n// Where we maintain all of the available ciphers\nvar cipherMap = map[string]uint16{\n\t\"TLS_RSA_WITH_RC4_128_SHA\":                tls.TLS_RSA_WITH_RC4_128_SHA,\n\t\"TLS_RSA_WITH_3DES_EDE_CBC_SHA\":           tls.TLS_RSA_WITH_3DES_EDE_CBC_SHA,\n\t\"TLS_RSA_WITH_AES_128_CBC_SHA\":            tls.TLS_RSA_WITH_AES_128_CBC_SHA,\n\t\"TLS_RSA_WITH_AES_128_CBC_SHA256\":         tls.TLS_RSA_WITH_AES_128_CBC_SHA256,\n\t\"TLS_RSA_WITH_AES_256_CBC_SHA\":            tls.TLS_RSA_WITH_AES_256_CBC_SHA,\n\t\"TLS_RSA_WITH_AES_256_GCM_SHA384\":         tls.TLS_RSA_WITH_AES_256_GCM_SHA384,\n\t\"TLS_ECDHE_ECDSA_WITH_RC4_128_SHA\":        tls.TLS_ECDHE_ECDSA_WITH_RC4_128_SHA,\n\t\"TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA\":    tls.TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA,\n\t\"TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA\":    tls.TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA,\n\t\"TLS_ECDHE_RSA_WITH_RC4_128_SHA\":          tls.TLS_ECDHE_RSA_WITH_RC4_128_SHA,\n\t\"TLS_ECDHE_RSA_WITH_3DES_EDE_CBC_SHA\":     tls.TLS_ECDHE_RSA_WITH_3DES_EDE_CBC_SHA,\n\t\"TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA\":      tls.TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA,\n\t\"TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256\":   tls.TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256,\n\t\"TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256\": tls.TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,\n\t\"TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA\":      tls.TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA,\n\t\"TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\":   tls.TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,\n\t\"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256\": tls.TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,\n\t\"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\":   tls.TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,\n\t\"TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384\": tls.TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,\n\t\"TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305\":    tls.TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,\n\t\"TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\":  tls.TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,\n\t\"TLS_AES_128_GCM_SHA256\":                  tls.TLS_AES_128_GCM_SHA256,\n\t\"TLS_AES_256_GCM_SHA384\":                  tls.TLS_AES_256_GCM_SHA384,\n\t\"TLS_CHACHA20_POLY1305_SHA256\":            tls.TLS_CHACHA20_POLY1305_SHA256,\n}\n\nvar cipherMapByID = map[uint16]string{\n\ttls.TLS_RSA_WITH_RC4_128_SHA:                \"TLS_RSA_WITH_RC4_128_SHA\",\n\ttls.TLS_RSA_WITH_3DES_EDE_CBC_SHA:           \"TLS_RSA_WITH_3DES_EDE_CBC_SHA\",\n\ttls.TLS_RSA_WITH_AES_128_CBC_SHA:            \"TLS_RSA_WITH_AES_128_CBC_SHA\",\n\ttls.TLS_RSA_WITH_AES_128_CBC_SHA256:         \"TLS_RSA_WITH_AES_128_CBC_SHA256\",\n\ttls.TLS_RSA_WITH_AES_256_CBC_SHA:            \"TLS_RSA_WITH_AES_256_CBC_SHA\",\n\ttls.TLS_RSA_WITH_AES_256_GCM_SHA384:         \"TLS_RSA_WITH_AES_256_GCM_SHA384\",\n\ttls.TLS_ECDHE_ECDSA_WITH_RC4_128_SHA:        \"TLS_ECDHE_ECDSA_WITH_RC4_128_SHA\",\n\ttls.TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA:    \"TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA\",\n\ttls.TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA:    \"TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA\",\n\ttls.TLS_ECDHE_RSA_WITH_RC4_128_SHA:          \"TLS_ECDHE_RSA_WITH_RC4_128_SHA\",\n\ttls.TLS_ECDHE_RSA_WITH_3DES_EDE_CBC_SHA:     \"TLS_ECDHE_RSA_WITH_3DES_EDE_CBC_SHA\",\n\ttls.TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA:      \"TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA\",\n\ttls.TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256:   \"TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256\",\n\ttls.TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256: \"TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256\",\n\ttls.TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA:      \"TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA\",\n\ttls.TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256:   \"TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\",\n\ttls.TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256: \"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256\",\n\ttls.TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384:   \"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\",\n\ttls.TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384: \"TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384\",\n\ttls.TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305:    \"TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305\",\n\ttls.TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305:  \"TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\",\n\ttls.TLS_AES_128_GCM_SHA256:                  \"TLS_AES_128_GCM_SHA256\",\n\ttls.TLS_AES_256_GCM_SHA384:                  \"TLS_AES_256_GCM_SHA384\",\n\ttls.TLS_CHACHA20_POLY1305_SHA256:            \"TLS_CHACHA20_POLY1305_SHA256\",\n}\n\nfunc defaultCipherSuites() []uint16 {\n\treturn []uint16{\n\t\ttls.TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,\n\t\ttls.TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,\n\t\ttls.TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,\n\t\ttls.TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,\n\t\ttls.TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,\n\t\ttls.TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,\n\t}\n}\n\n// Where we maintain available curve preferences\nvar curvePreferenceMap = map[string]tls.CurveID{\n\t\"X25519\":    tls.X25519,\n\t\"CurveP256\": tls.CurveP256,\n\t\"CurveP384\": tls.CurveP384,\n\t\"CurveP521\": tls.CurveP521,\n}\n\n// reorder to default to the highest level of security.  See:\n// https://blog.bracebin.com/achieving-perfect-ssl-labs-score-with-go\nfunc defaultCurvePreferences() []tls.CurveID {\n\treturn []tls.CurveID{\n\t\ttls.X25519, // faster than P256, arguably more secure\n\t\ttls.CurveP256,\n\t\ttls.CurveP384,\n\t\ttls.CurveP521,\n\t}\n}\n",
    "source_file": "server/ciphersuites.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2012-2025 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage server\n\nimport (\n\t\"bufio\"\n\t\"bytes\"\n\t\"fmt\"\n\t\"net/http\"\n\t\"net/textproto\"\n)\n\ntype parserState int\ntype parseState struct {\n\tstate   parserState\n\top      byte\n\tas      int\n\tdrop    int\n\tpa      pubArg\n\targBuf  []byte\n\tmsgBuf  []byte\n\theader  http.Header // access via getHeader\n\tscratch [MAX_CONTROL_LINE_SIZE]byte\n}\n\ntype pubArg struct {\n\targ       []byte\n\tpacache   []byte\n\torigin    []byte\n\taccount   []byte\n\tsubject   []byte\n\tdeliver   []byte\n\tmapped    []byte\n\treply     []byte\n\tszb       []byte\n\thdb       []byte\n\tqueues    [][]byte\n\tsize      int\n\thdr       int\n\tpsi       []*serviceImport\n\ttrace     *msgTrace\n\tdelivered bool // Only used for service imports\n}\n\n// Parser constants\nconst (\n\tOP_START parserState = iota\n\tOP_PLUS\n\tOP_PLUS_O\n\tOP_PLUS_OK\n\tOP_MINUS\n\tOP_MINUS_E\n\tOP_MINUS_ER\n\tOP_MINUS_ERR\n\tOP_MINUS_ERR_SPC\n\tMINUS_ERR_ARG\n\tOP_C\n\tOP_CO\n\tOP_CON\n\tOP_CONN\n\tOP_CONNE\n\tOP_CONNEC\n\tOP_CONNECT\n\tCONNECT_ARG\n\tOP_H\n\tOP_HP\n\tOP_HPU\n\tOP_HPUB\n\tOP_HPUB_SPC\n\tHPUB_ARG\n\tOP_HM\n\tOP_HMS\n\tOP_HMSG\n\tOP_HMSG_SPC\n\tHMSG_ARG\n\tOP_P\n\tOP_PU\n\tOP_PUB\n\tOP_PUB_SPC\n\tPUB_ARG\n\tOP_PI\n\tOP_PIN\n\tOP_PING\n\tOP_PO\n\tOP_PON\n\tOP_PONG\n\tMSG_PAYLOAD\n\tMSG_END_R\n\tMSG_END_N\n\tOP_S\n\tOP_SU\n\tOP_SUB\n\tOP_SUB_SPC\n\tSUB_ARG\n\tOP_A\n\tOP_ASUB\n\tOP_ASUB_SPC\n\tASUB_ARG\n\tOP_AUSUB\n\tOP_AUSUB_SPC\n\tAUSUB_ARG\n\tOP_L\n\tOP_LS\n\tOP_R\n\tOP_RS\n\tOP_U\n\tOP_UN\n\tOP_UNS\n\tOP_UNSU\n\tOP_UNSUB\n\tOP_UNSUB_SPC\n\tUNSUB_ARG\n\tOP_M\n\tOP_MS\n\tOP_MSG\n\tOP_MSG_SPC\n\tMSG_ARG\n\tOP_I\n\tOP_IN\n\tOP_INF\n\tOP_INFO\n\tINFO_ARG\n)\n\nfunc (c *client) parse(buf []byte) error {\n\t// Branch out to mqtt clients. c.mqtt is immutable, but should it become\n\t// an issue (say data race detection), we could branch outside in readLoop\n\tif c.isMqtt() {\n\t\treturn c.mqttParse(buf)\n\t}\n\tvar i int\n\tvar b byte\n\tvar lmsg bool\n\n\t// Snapshots\n\tc.mu.Lock()\n\t// Snapshot and then reset when we receive a\n\t// proper CONNECT if needed.\n\tauthSet := c.awaitingAuth()\n\t// Snapshot max control line as well.\n\ts, mcl, trace := c.srv, c.mcl, c.trace\n\tc.mu.Unlock()\n\n\t// Move to loop instead of range syntax to allow jumping of i\n\tfor i = 0; i < len(buf); i++ {\n\t\tb = buf[i]\n\n\t\tswitch c.state {\n\t\tcase OP_START:\n\t\t\tc.op = b\n\t\t\tif b != 'C' && b != 'c' {\n\t\t\t\tif authSet {\n\t\t\t\t\tif s == nil {\n\t\t\t\t\t\tgoto authErr\n\t\t\t\t\t}\n\t\t\t\t\tvar ok bool\n\t\t\t\t\t// Check here for NoAuthUser. If this is set allow non CONNECT protos as our first.\n\t\t\t\t\t// E.g. telnet proto demos.\n\t\t\t\t\tif noAuthUser := s.getOpts().NoAuthUser; noAuthUser != _EMPTY_ {\n\t\t\t\t\t\ts.mu.Lock()\n\t\t\t\t\t\tuser, exists := s.users[noAuthUser]\n\t\t\t\t\t\ts.mu.Unlock()\n\t\t\t\t\t\tif exists {\n\t\t\t\t\t\t\tc.RegisterUser(user)\n\t\t\t\t\t\t\tc.mu.Lock()\n\t\t\t\t\t\t\tc.clearAuthTimer()\n\t\t\t\t\t\t\tc.flags.set(connectReceived)\n\t\t\t\t\t\t\tc.mu.Unlock()\n\t\t\t\t\t\t\tauthSet, ok = false, true\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tif !ok {\n\t\t\t\t\t\tgoto authErr\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\t// If the connection is a gateway connection, make sure that\n\t\t\t\t// if this is an inbound, it starts with a CONNECT.\n\t\t\t\tif c.kind == GATEWAY && !c.gw.outbound && !c.gw.connected {\n\t\t\t\t\t// Use auth violation since no CONNECT was sent.\n\t\t\t\t\t// It could be a parseErr too.\n\t\t\t\t\tgoto authErr\n\t\t\t\t}\n\t\t\t}\n\t\t\tswitch b {\n\t\t\tcase 'P', 'p':\n\t\t\t\tc.state = OP_P\n\t\t\tcase 'H', 'h':\n\t\t\t\tc.state = OP_H\n\t\t\tcase 'S', 's':\n\t\t\t\tc.state = OP_S\n\t\t\tcase 'U', 'u':\n\t\t\t\tc.state = OP_U\n\t\t\tcase 'R', 'r':\n\t\t\t\tif c.kind == CLIENT {\n\t\t\t\t\tgoto parseErr\n\t\t\t\t} else {\n\t\t\t\t\tc.state = OP_R\n\t\t\t\t}\n\t\t\tcase 'L', 'l':\n\t\t\t\tif c.kind != LEAF && c.kind != ROUTER {\n\t\t\t\t\tgoto parseErr\n\t\t\t\t} else {\n\t\t\t\t\tc.state = OP_L\n\t\t\t\t}\n\t\t\tcase 'A', 'a':\n\t\t\t\tif c.kind == CLIENT {\n\t\t\t\t\tgoto parseErr\n\t\t\t\t} else {\n\t\t\t\t\tc.state = OP_A\n\t\t\t\t}\n\t\t\tcase 'C', 'c':\n\t\t\t\tc.state = OP_C\n\t\t\tcase 'I', 'i':\n\t\t\t\tc.state = OP_I\n\t\t\tcase '+':\n\t\t\t\tc.state = OP_PLUS\n\t\t\tcase '-':\n\t\t\t\tc.state = OP_MINUS\n\t\t\tdefault:\n\t\t\t\tgoto parseErr\n\t\t\t}\n\t\tcase OP_H:\n\t\t\tswitch b {\n\t\t\tcase 'P', 'p':\n\t\t\t\tc.state = OP_HP\n\t\t\tcase 'M', 'm':\n\t\t\t\tc.state = OP_HM\n\t\t\tdefault:\n\t\t\t\tgoto parseErr\n\t\t\t}\n\t\tcase OP_HP:\n\t\t\tswitch b {\n\t\t\tcase 'U', 'u':\n\t\t\t\tc.state = OP_HPU\n\t\t\tdefault:\n\t\t\t\tgoto parseErr\n\t\t\t}\n\t\tcase OP_HPU:\n\t\t\tswitch b {\n\t\t\tcase 'B', 'b':\n\t\t\t\tc.state = OP_HPUB\n\t\t\tdefault:\n\t\t\t\tgoto parseErr\n\t\t\t}\n\t\tcase OP_HPUB:\n\t\t\tswitch b {\n\t\t\tcase ' ', '\\t':\n\t\t\t\tc.state = OP_HPUB_SPC\n\t\t\tdefault:\n\t\t\t\tgoto parseErr\n\t\t\t}\n\t\tcase OP_HPUB_SPC:\n\t\t\tswitch b {\n\t\t\tcase ' ', '\\t':\n\t\t\t\tcontinue\n\t\t\tdefault:\n\t\t\t\tc.pa.hdr = 0\n\t\t\t\tc.state = HPUB_ARG\n\t\t\t\tc.as = i\n\t\t\t}\n\t\tcase HPUB_ARG:\n\t\t\tswitch b {\n\t\t\tcase '\\r':\n\t\t\t\tc.drop = 1\n\t\t\tcase '\\n':\n\t\t\t\tvar arg []byte\n\t\t\t\tif c.argBuf != nil {\n\t\t\t\t\targ = c.argBuf\n\t\t\t\t\tc.argBuf = nil\n\t\t\t\t} else {\n\t\t\t\t\targ = buf[c.as : i-c.drop]\n\t\t\t\t}\n\t\t\t\tif err := c.overMaxControlLineLimit(arg, mcl); err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tif trace {\n\t\t\t\t\tc.traceInOp(\"HPUB\", arg)\n\t\t\t\t}\n\t\t\t\tvar remaining []byte\n\t\t\t\tif i < len(buf) {\n\t\t\t\t\tremaining = buf[i+1:]\n\t\t\t\t}\n\t\t\t\tif err := c.processHeaderPub(arg, remaining); err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\n\t\t\t\tc.drop, c.as, c.state = 0, i+1, MSG_PAYLOAD\n\t\t\t\t// If we don't have a saved buffer then jump ahead with\n\t\t\t\t// the index. If this overruns what is left we fall out\n\t\t\t\t// and process split buffer.\n\t\t\t\tif c.msgBuf == nil {\n\t\t\t\t\ti = c.as + c.pa.size - LEN_CR_LF\n\t\t\t\t}\n\t\t\tdefault:\n\t\t\t\tif c.argBuf != nil {\n\t\t\t\t\tc.argBuf = append(c.argBuf, b)\n\t\t\t\t}\n\t\t\t}\n\t\tcase OP_HM:\n\t\t\tswitch b {\n\t\t\tcase 'S', 's':\n\t\t\t\tc.state = OP_HMS\n\t\t\tdefault:\n\t\t\t\tgoto parseErr\n\t\t\t}\n\t\tcase OP_HMS:\n\t\t\tswitch b {\n\t\t\tcase 'G', 'g':\n\t\t\t\tc.state = OP_HMSG\n\t\t\tdefault:\n\t\t\t\tgoto parseErr\n\t\t\t}\n\t\tcase OP_HMSG:\n\t\t\tswitch b {\n\t\t\tcase ' ', '\\t':\n\t\t\t\tc.state = OP_HMSG_SPC\n\t\t\tdefault:\n\t\t\t\tgoto parseErr\n\t\t\t}\n\t\tcase OP_HMSG_SPC:\n\t\t\tswitch b {\n\t\t\tcase ' ', '\\t':\n\t\t\t\tcontinue\n\t\t\tdefault:\n\t\t\t\tc.pa.hdr = 0\n\t\t\t\tc.state = HMSG_ARG\n\t\t\t\tc.as = i\n\t\t\t}\n\t\tcase HMSG_ARG:\n\t\t\tswitch b {\n\t\t\tcase '\\r':\n\t\t\t\tc.drop = 1\n\t\t\tcase '\\n':\n\t\t\t\tvar arg []byte\n\t\t\t\tif c.argBuf != nil {\n\t\t\t\t\targ = c.argBuf\n\t\t\t\t\tc.argBuf = nil\n\t\t\t\t} else {\n\t\t\t\t\targ = buf[c.as : i-c.drop]\n\t\t\t\t}\n\t\t\t\tif err := c.overMaxControlLineLimit(arg, mcl); err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tvar err error\n\t\t\t\tif c.kind == ROUTER || c.kind == GATEWAY {\n\t\t\t\t\tif trace {\n\t\t\t\t\t\tc.traceInOp(\"HMSG\", arg)\n\t\t\t\t\t}\n\t\t\t\t\terr = c.processRoutedHeaderMsgArgs(arg)\n\t\t\t\t} else if c.kind == LEAF {\n\t\t\t\t\tif trace {\n\t\t\t\t\t\tc.traceInOp(\"HMSG\", arg)\n\t\t\t\t\t}\n\t\t\t\t\terr = c.processLeafHeaderMsgArgs(arg)\n\t\t\t\t}\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tc.drop, c.as, c.state = 0, i+1, MSG_PAYLOAD\n\n\t\t\t\t// jump ahead with the index. If this overruns\n\t\t\t\t// what is left we fall out and process split\n\t\t\t\t// buffer.\n\t\t\t\ti = c.as + c.pa.size - LEN_CR_LF\n\t\t\tdefault:\n\t\t\t\tif c.argBuf != nil {\n\t\t\t\t\tc.argBuf = append(c.argBuf, b)\n\t\t\t\t}\n\t\t\t}\n\t\tcase OP_P:\n\t\t\tswitch b {\n\t\t\tcase 'U', 'u':\n\t\t\t\tc.state = OP_PU\n\t\t\tcase 'I', 'i':\n\t\t\t\tc.state = OP_PI\n\t\t\tcase 'O', 'o':\n\t\t\t\tc.state = OP_PO\n\t\t\tdefault:\n\t\t\t\tgoto parseErr\n\t\t\t}\n\t\tcase OP_PU:\n\t\t\tswitch b {\n\t\t\tcase 'B', 'b':\n\t\t\t\tc.state = OP_PUB\n\t\t\tdefault:\n\t\t\t\tgoto parseErr\n\t\t\t}\n\t\tcase OP_PUB:\n\t\t\tswitch b {\n\t\t\tcase ' ', '\\t':\n\t\t\t\tc.state = OP_PUB_SPC\n\t\t\tdefault:\n\t\t\t\tgoto parseErr\n\t\t\t}\n\t\tcase OP_PUB_SPC:\n\t\t\tswitch b {\n\t\t\tcase ' ', '\\t':\n\t\t\t\tcontinue\n\t\t\tdefault:\n\t\t\t\tc.pa.hdr = -1\n\t\t\t\tc.state = PUB_ARG\n\t\t\t\tc.as = i\n\t\t\t}\n\t\tcase PUB_ARG:\n\t\t\tswitch b {\n\t\t\tcase '\\r':\n\t\t\t\tc.drop = 1\n\t\t\tcase '\\n':\n\t\t\t\tvar arg []byte\n\t\t\t\tif c.argBuf != nil {\n\t\t\t\t\targ = c.argBuf\n\t\t\t\t\tc.argBuf = nil\n\t\t\t\t} else {\n\t\t\t\t\targ = buf[c.as : i-c.drop]\n\t\t\t\t}\n\t\t\t\tif err := c.overMaxControlLineLimit(arg, mcl); err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tif trace {\n\t\t\t\t\tc.traceInOp(\"PUB\", arg)\n\t\t\t\t}\n\t\t\t\tif err := c.processPub(arg); err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\n\t\t\t\tc.drop, c.as, c.state = 0, i+1, MSG_PAYLOAD\n\t\t\t\t// If we don't have a saved buffer then jump ahead with\n\t\t\t\t// the index. If this overruns what is left we fall out\n\t\t\t\t// and process split buffer.\n\t\t\t\tif c.msgBuf == nil {\n\t\t\t\t\ti = c.as + c.pa.size - LEN_CR_LF\n\t\t\t\t}\n\t\t\tdefault:\n\t\t\t\tif c.argBuf != nil {\n\t\t\t\t\tc.argBuf = append(c.argBuf, b)\n\t\t\t\t}\n\t\t\t}\n\t\tcase MSG_PAYLOAD:\n\t\t\tif c.msgBuf != nil {\n\t\t\t\t// copy as much as we can to the buffer and skip ahead.\n\t\t\t\ttoCopy := c.pa.size - len(c.msgBuf)\n\t\t\t\tavail := len(buf) - i\n\t\t\t\tif avail < toCopy {\n\t\t\t\t\ttoCopy = avail\n\t\t\t\t}\n\t\t\t\tif toCopy > 0 {\n\t\t\t\t\tstart := len(c.msgBuf)\n\t\t\t\t\t// This is needed for copy to work.\n\t\t\t\t\tc.msgBuf = c.msgBuf[:start+toCopy]\n\t\t\t\t\tcopy(c.msgBuf[start:], buf[i:i+toCopy])\n\t\t\t\t\t// Update our index\n\t\t\t\t\ti = (i + toCopy) - 1\n\t\t\t\t} else {\n\t\t\t\t\t// Fall back to append if needed.\n\t\t\t\t\tc.msgBuf = append(c.msgBuf, b)\n\t\t\t\t}\n\t\t\t\tif len(c.msgBuf) >= c.pa.size {\n\t\t\t\t\tc.state = MSG_END_R\n\t\t\t\t}\n\t\t\t} else if i-c.as+1 >= c.pa.size {\n\t\t\t\tc.state = MSG_END_R\n\t\t\t}\n\t\tcase MSG_END_R:\n\t\t\tif b != '\\r' {\n\t\t\t\tgoto parseErr\n\t\t\t}\n\t\t\tif c.msgBuf != nil {\n\t\t\t\tc.msgBuf = append(c.msgBuf, b)\n\t\t\t}\n\t\t\tc.state = MSG_END_N\n\t\tcase MSG_END_N:\n\t\t\tif b != '\\n' {\n\t\t\t\tgoto parseErr\n\t\t\t}\n\t\t\tif c.msgBuf != nil {\n\t\t\t\tc.msgBuf = append(c.msgBuf, b)\n\t\t\t} else {\n\t\t\t\tc.msgBuf = buf[c.as : i+1]\n\t\t\t}\n\n\t\t\tvar mt *msgTrace\n\t\t\tif c.pa.hdr > 0 {\n\t\t\t\tmt = c.initMsgTrace()\n\t\t\t}\n\t\t\t// Check for mappings.\n\t\t\tif (c.kind == CLIENT || c.kind == LEAF) && c.in.flags.isSet(hasMappings) {\n\t\t\t\tchanged := c.selectMappedSubject()\n\t\t\t\tif changed {\n\t\t\t\t\tif trace {\n\t\t\t\t\t\tc.traceInOp(\"MAPPING\", []byte(fmt.Sprintf(\"%s -> %s\", c.pa.mapped, c.pa.subject)))\n\t\t\t\t\t}\n\t\t\t\t\t// c.pa.subject is the subject the original is now mapped to.\n\t\t\t\t\tmt.addSubjectMappingEvent(c.pa.subject)\n\t\t\t\t}\n\t\t\t}\n\t\t\tif trace {\n\t\t\t\tc.traceMsg(c.msgBuf)\n\t\t\t}\n\n\t\t\tc.processInboundMsg(c.msgBuf)\n\n\t\t\tmt.sendEvent()\n\t\t\tc.argBuf, c.msgBuf, c.header = nil, nil, nil\n\t\t\tc.drop, c.as, c.state = 0, i+1, OP_START\n\t\t\t// Drop all pub args\n\t\t\tc.pa.arg, c.pa.pacache, c.pa.origin, c.pa.account, c.pa.subject, c.pa.mapped = nil, nil, nil, nil, nil, nil\n\t\t\tc.pa.reply, c.pa.hdr, c.pa.size, c.pa.szb, c.pa.hdb, c.pa.queues = nil, -1, 0, nil, nil, nil\n\t\t\tc.pa.trace = nil\n\t\t\tc.pa.delivered = false\n\t\t\tlmsg = false\n\t\tcase OP_A:\n\t\t\tswitch b {\n\t\t\tcase '+':\n\t\t\t\tc.state = OP_ASUB\n\t\t\tcase '-', 'u':\n\t\t\t\tc.state = OP_AUSUB\n\t\t\tdefault:\n\t\t\t\tgoto parseErr\n\t\t\t}\n\t\tcase OP_ASUB:\n\t\t\tswitch b {\n\t\t\tcase ' ', '\\t':\n\t\t\t\tc.state = OP_ASUB_SPC\n\t\t\tdefault:\n\t\t\t\tgoto parseErr\n\t\t\t}\n\t\tcase OP_ASUB_SPC:\n\t\t\tswitch b {\n\t\t\tcase ' ', '\\t':\n\t\t\t\tcontinue\n\t\t\tdefault:\n\t\t\t\tc.state = ASUB_ARG\n\t\t\t\tc.as = i\n\t\t\t}\n\t\tcase ASUB_ARG:\n\t\t\tswitch b {\n\t\t\tcase '\\r':\n\t\t\t\tc.drop = 1\n\t\t\tcase '\\n':\n\t\t\t\tvar arg []byte\n\t\t\t\tif c.argBuf != nil {\n\t\t\t\t\targ = c.argBuf\n\t\t\t\t\tc.argBuf = nil\n\t\t\t\t} else {\n\t\t\t\t\targ = buf[c.as : i-c.drop]\n\t\t\t\t}\n\t\t\t\tif err := c.overMaxControlLineLimit(arg, mcl); err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tif trace {\n\t\t\t\t\tc.traceInOp(\"A+\", arg)\n\t\t\t\t}\n\t\t\t\tif err := c.processAccountSub(arg); err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tc.drop, c.as, c.state = 0, i+1, OP_START\n\t\t\tdefault:\n\t\t\t\tif c.argBuf != nil {\n\t\t\t\t\tc.argBuf = append(c.argBuf, b)\n\t\t\t\t}\n\t\t\t}\n\t\tcase OP_AUSUB:\n\t\t\tswitch b {\n\t\t\tcase ' ', '\\t':\n\t\t\t\tc.state = OP_AUSUB_SPC\n\t\t\tdefault:\n\t\t\t\tgoto parseErr\n\t\t\t}\n\t\tcase OP_AUSUB_SPC:\n\t\t\tswitch b {\n\t\t\tcase ' ', '\\t':\n\t\t\t\tcontinue\n\t\t\tdefault:\n\t\t\t\tc.state = AUSUB_ARG\n\t\t\t\tc.as = i\n\t\t\t}\n\t\tcase AUSUB_ARG:\n\t\t\tswitch b {\n\t\t\tcase '\\r':\n\t\t\t\tc.drop = 1\n\t\t\tcase '\\n':\n\t\t\t\tvar arg []byte\n\t\t\t\tif c.argBuf != nil {\n\t\t\t\t\targ = c.argBuf\n\t\t\t\t\tc.argBuf = nil\n\t\t\t\t} else {\n\t\t\t\t\targ = buf[c.as : i-c.drop]\n\t\t\t\t}\n\t\t\t\tif err := c.overMaxControlLineLimit(arg, mcl); err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tif trace {\n\t\t\t\t\tc.traceInOp(\"A-\", arg)\n\t\t\t\t}\n\t\t\t\tc.processAccountUnsub(arg)\n\t\t\t\tc.drop, c.as, c.state = 0, i+1, OP_START\n\t\t\tdefault:\n\t\t\t\tif c.argBuf != nil {\n\t\t\t\t\tc.argBuf = append(c.argBuf, b)\n\t\t\t\t}\n\t\t\t}\n\t\tcase OP_S:\n\t\t\tswitch b {\n\t\t\tcase 'U', 'u':\n\t\t\t\tc.state = OP_SU\n\t\t\tdefault:\n\t\t\t\tgoto parseErr\n\t\t\t}\n\t\tcase OP_SU:\n\t\t\tswitch b {\n\t\t\tcase 'B', 'b':\n\t\t\t\tc.state = OP_SUB\n\t\t\tdefault:\n\t\t\t\tgoto parseErr\n\t\t\t}\n\t\tcase OP_SUB:\n\t\t\tswitch b {\n\t\t\tcase ' ', '\\t':\n\t\t\t\tc.state = OP_SUB_SPC\n\t\t\tdefault:\n\t\t\t\tgoto parseErr\n\t\t\t}\n\t\tcase OP_SUB_SPC:\n\t\t\tswitch b {\n\t\t\tcase ' ', '\\t':\n\t\t\t\tcontinue\n\t\t\tdefault:\n\t\t\t\tc.state = SUB_ARG\n\t\t\t\tc.as = i\n\t\t\t}\n\t\tcase SUB_ARG:\n\t\t\tswitch b {\n\t\t\tcase '\\r':\n\t\t\t\tc.drop = 1\n\t\t\tcase '\\n':\n\t\t\t\tvar arg []byte\n\t\t\t\tif c.argBuf != nil {\n\t\t\t\t\targ = c.argBuf\n\t\t\t\t\tc.argBuf = nil\n\t\t\t\t} else {\n\t\t\t\t\targ = buf[c.as : i-c.drop]\n\t\t\t\t}\n\t\t\t\tif err := c.overMaxControlLineLimit(arg, mcl); err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tvar err error\n\n\t\t\t\tswitch c.kind {\n\t\t\t\tcase CLIENT:\n\t\t\t\t\tif trace {\n\t\t\t\t\t\tc.traceInOp(\"SUB\", arg)\n\t\t\t\t\t}\n\t\t\t\t\terr = c.parseSub(arg, false)\n\t\t\t\tcase ROUTER:\n\t\t\t\t\tswitch c.op {\n\t\t\t\t\tcase 'R', 'r':\n\t\t\t\t\t\tif trace {\n\t\t\t\t\t\t\tc.traceInOp(\"RS+\", arg)\n\t\t\t\t\t\t}\n\t\t\t\t\t\terr = c.processRemoteSub(arg, false)\n\t\t\t\t\tcase 'L', 'l':\n\t\t\t\t\t\tif trace {\n\t\t\t\t\t\t\tc.traceInOp(\"LS+\", arg)\n\t\t\t\t\t\t}\n\t\t\t\t\t\terr = c.processRemoteSub(arg, true)\n\t\t\t\t\t}\n\t\t\t\tcase GATEWAY:\n\t\t\t\t\tif trace {\n\t\t\t\t\t\tc.traceInOp(\"RS+\", arg)\n\t\t\t\t\t}\n\t\t\t\t\terr = c.processGatewayRSub(arg)\n\t\t\t\tcase LEAF:\n\t\t\t\t\tif trace {\n\t\t\t\t\t\tc.traceInOp(\"LS+\", arg)\n\t\t\t\t\t}\n\t\t\t\t\terr = c.processLeafSub(arg)\n\t\t\t\t}\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tc.drop, c.as, c.state = 0, i+1, OP_START\n\t\t\tdefault:\n\t\t\t\tif c.argBuf != nil {\n\t\t\t\t\tc.argBuf = append(c.argBuf, b)\n\t\t\t\t}\n\t\t\t}\n\t\tcase OP_L:\n\t\t\tswitch b {\n\t\t\tcase 'S', 's':\n\t\t\t\tc.state = OP_LS\n\t\t\tcase 'M', 'm':\n\t\t\t\tc.state = OP_M\n\t\t\tdefault:\n\t\t\t\tgoto parseErr\n\t\t\t}\n\t\tcase OP_LS:\n\t\t\tswitch b {\n\t\t\tcase '+':\n\t\t\t\tc.state = OP_SUB\n\t\t\tcase '-':\n\t\t\t\tc.state = OP_UNSUB\n\t\t\tdefault:\n\t\t\t\tgoto parseErr\n\t\t\t}\n\t\tcase OP_R:\n\t\t\tswitch b {\n\t\t\tcase 'S', 's':\n\t\t\t\tc.state = OP_RS\n\t\t\tcase 'M', 'm':\n\t\t\t\tc.state = OP_M\n\t\t\tdefault:\n\t\t\t\tgoto parseErr\n\t\t\t}\n\t\tcase OP_RS:\n\t\t\tswitch b {\n\t\t\tcase '+':\n\t\t\t\tc.state = OP_SUB\n\t\t\tcase '-':\n\t\t\t\tc.state = OP_UNSUB\n\t\t\tdefault:\n\t\t\t\tgoto parseErr\n\t\t\t}\n\t\tcase OP_U:\n\t\t\tswitch b {\n\t\t\tcase 'N', 'n':\n\t\t\t\tc.state = OP_UN\n\t\t\tdefault:\n\t\t\t\tgoto parseErr\n\t\t\t}\n\t\tcase OP_UN:\n\t\t\tswitch b {\n\t\t\tcase 'S', 's':\n\t\t\t\tc.state = OP_UNS\n\t\t\tdefault:\n\t\t\t\tgoto parseErr\n\t\t\t}\n\t\tcase OP_UNS:\n\t\t\tswitch b {\n\t\t\tcase 'U', 'u':\n\t\t\t\tc.state = OP_UNSU\n\t\t\tdefault:\n\t\t\t\tgoto parseErr\n\t\t\t}\n\t\tcase OP_UNSU:\n\t\t\tswitch b {\n\t\t\tcase 'B', 'b':\n\t\t\t\tc.state = OP_UNSUB\n\t\t\tdefault:\n\t\t\t\tgoto parseErr\n\t\t\t}\n\t\tcase OP_UNSUB:\n\t\t\tswitch b {\n\t\t\tcase ' ', '\\t':\n\t\t\t\tc.state = OP_UNSUB_SPC\n\t\t\tdefault:\n\t\t\t\tgoto parseErr\n\t\t\t}\n\t\tcase OP_UNSUB_SPC:\n\t\t\tswitch b {\n\t\t\tcase ' ', '\\t':\n\t\t\t\tcontinue\n\t\t\tdefault:\n\t\t\t\tc.state = UNSUB_ARG\n\t\t\t\tc.as = i\n\t\t\t}\n\t\tcase UNSUB_ARG:\n\t\t\tswitch b {\n\t\t\tcase '\\r':\n\t\t\t\tc.drop = 1\n\t\t\tcase '\\n':\n\t\t\t\tvar arg []byte\n\t\t\t\tif c.argBuf != nil {\n\t\t\t\t\targ = c.argBuf\n\t\t\t\t\tc.argBuf = nil\n\t\t\t\t} else {\n\t\t\t\t\targ = buf[c.as : i-c.drop]\n\t\t\t\t}\n\t\t\t\tif err := c.overMaxControlLineLimit(arg, mcl); err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tvar err error\n\n\t\t\t\tswitch c.kind {\n\t\t\t\tcase CLIENT:\n\t\t\t\t\tif trace {\n\t\t\t\t\t\tc.traceInOp(\"UNSUB\", arg)\n\t\t\t\t\t}\n\t\t\t\t\terr = c.processUnsub(arg)\n\t\t\t\tcase ROUTER:\n\t\t\t\t\tif trace && c.srv != nil {\n\t\t\t\t\t\tswitch c.op {\n\t\t\t\t\t\tcase 'R', 'r':\n\t\t\t\t\t\t\tc.traceInOp(\"RS-\", arg)\n\t\t\t\t\t\tcase 'L', 'l':\n\t\t\t\t\t\t\tc.traceInOp(\"LS-\", arg)\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tleafUnsub := c.op == 'L' || c.op == 'l'\n\t\t\t\t\terr = c.processRemoteUnsub(arg, leafUnsub)\n\t\t\t\tcase GATEWAY:\n\t\t\t\t\tif trace {\n\t\t\t\t\t\tc.traceInOp(\"RS-\", arg)\n\t\t\t\t\t}\n\t\t\t\t\terr = c.processGatewayRUnsub(arg)\n\t\t\t\tcase LEAF:\n\t\t\t\t\tif trace {\n\t\t\t\t\t\tc.traceInOp(\"LS-\", arg)\n\t\t\t\t\t}\n\t\t\t\t\terr = c.processLeafUnsub(arg)\n\t\t\t\t}\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tc.drop, c.as, c.state = 0, i+1, OP_START\n\t\t\tdefault:\n\t\t\t\tif c.argBuf != nil {\n\t\t\t\t\tc.argBuf = append(c.argBuf, b)\n\t\t\t\t}\n\t\t\t}\n\t\tcase OP_PI:\n\t\t\tswitch b {\n\t\t\tcase 'N', 'n':\n\t\t\t\tc.state = OP_PIN\n\t\t\tdefault:\n\t\t\t\tgoto parseErr\n\t\t\t}\n\t\tcase OP_PIN:\n\t\t\tswitch b {\n\t\t\tcase 'G', 'g':\n\t\t\t\tc.state = OP_PING\n\t\t\tdefault:\n\t\t\t\tgoto parseErr\n\t\t\t}\n\t\tcase OP_PING:\n\t\t\tswitch b {\n\t\t\tcase '\\n':\n\t\t\t\tif trace {\n\t\t\t\t\tc.traceInOp(\"PING\", nil)\n\t\t\t\t}\n\t\t\t\tc.processPing()\n\t\t\t\tc.drop, c.state = 0, OP_START\n\t\t\t}\n\t\tcase OP_PO:\n\t\t\tswitch b {\n\t\t\tcase 'N', 'n':\n\t\t\t\tc.state = OP_PON\n\t\t\tdefault:\n\t\t\t\tgoto parseErr\n\t\t\t}\n\t\tcase OP_PON:\n\t\t\tswitch b {\n\t\t\tcase 'G', 'g':\n\t\t\t\tc.state = OP_PONG\n\t\t\tdefault:\n\t\t\t\tgoto parseErr\n\t\t\t}\n\t\tcase OP_PONG:\n\t\t\tswitch b {\n\t\t\tcase '\\n':\n\t\t\t\tif trace {\n\t\t\t\t\tc.traceInOp(\"PONG\", nil)\n\t\t\t\t}\n\t\t\t\tc.processPong()\n\t\t\t\tc.drop, c.state = 0, OP_START\n\t\t\t}\n\t\tcase OP_C:\n\t\t\tswitch b {\n\t\t\tcase 'O', 'o':\n\t\t\t\tc.state = OP_CO\n\t\t\tdefault:\n\t\t\t\tgoto parseErr\n\t\t\t}\n\t\tcase OP_CO:\n\t\t\tswitch b {\n\t\t\tcase 'N', 'n':\n\t\t\t\tc.state = OP_CON\n\t\t\tdefault:\n\t\t\t\tgoto parseErr\n\t\t\t}\n\t\tcase OP_CON:\n\t\t\tswitch b {\n\t\t\tcase 'N', 'n':\n\t\t\t\tc.state = OP_CONN\n\t\t\tdefault:\n\t\t\t\tgoto parseErr\n\t\t\t}\n\t\tcase OP_CONN:\n\t\t\tswitch b {\n\t\t\tcase 'E', 'e':\n\t\t\t\tc.state = OP_CONNE\n\t\t\tdefault:\n\t\t\t\tgoto parseErr\n\t\t\t}\n\t\tcase OP_CONNE:\n\t\t\tswitch b {\n\t\t\tcase 'C', 'c':\n\t\t\t\tc.state = OP_CONNEC\n\t\t\tdefault:\n\t\t\t\tgoto parseErr\n\t\t\t}\n\t\tcase OP_CONNEC:\n\t\t\tswitch b {\n\t\t\tcase 'T', 't':\n\t\t\t\tc.state = OP_CONNECT\n\t\t\tdefault:\n\t\t\t\tgoto parseErr\n\t\t\t}\n\t\tcase OP_CONNECT:\n\t\t\tswitch b {\n\t\t\tcase ' ', '\\t':\n\t\t\t\tcontinue\n\t\t\tdefault:\n\t\t\t\tc.state = CONNECT_ARG\n\t\t\t\tc.as = i\n\t\t\t}\n\t\tcase CONNECT_ARG:\n\t\t\tswitch b {\n\t\t\tcase '\\r':\n\t\t\t\tc.drop = 1\n\t\t\tcase '\\n':\n\t\t\t\tvar arg []byte\n\t\t\t\tif c.argBuf != nil {\n\t\t\t\t\targ = c.argBuf\n\t\t\t\t\tc.argBuf = nil\n\t\t\t\t} else {\n\t\t\t\t\targ = buf[c.as : i-c.drop]\n\t\t\t\t}\n\t\t\t\tif err := c.overMaxControlLineLimit(arg, mcl); err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tif trace {\n\t\t\t\t\tc.traceInOp(\"CONNECT\", removeSecretsFromTrace(arg))\n\t\t\t\t}\n\t\t\t\tif err := c.processConnect(arg); err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tc.drop, c.state = 0, OP_START\n\t\t\t\t// Reset notion on authSet\n\t\t\t\tc.mu.Lock()\n\t\t\t\tauthSet = c.awaitingAuth()\n\t\t\t\tc.mu.Unlock()\n\t\t\tdefault:\n\t\t\t\tif c.argBuf != nil {\n\t\t\t\t\tc.argBuf = append(c.argBuf, b)\n\t\t\t\t}\n\t\t\t}\n\t\tcase OP_M:\n\t\t\tswitch b {\n\t\t\tcase 'S', 's':\n\t\t\t\tc.state = OP_MS\n\t\t\tdefault:\n\t\t\t\tgoto parseErr\n\t\t\t}\n\t\tcase OP_MS:\n\t\t\tswitch b {\n\t\t\tcase 'G', 'g':\n\t\t\t\tc.state = OP_MSG\n\t\t\tdefault:\n\t\t\t\tgoto parseErr\n\t\t\t}\n\t\tcase OP_MSG:\n\t\t\tswitch b {\n\t\t\tcase ' ', '\\t':\n\t\t\t\tc.state = OP_MSG_SPC\n\t\t\tdefault:\n\t\t\t\tgoto parseErr\n\t\t\t}\n\t\tcase OP_MSG_SPC:\n\t\t\tswitch b {\n\t\t\tcase ' ', '\\t':\n\t\t\t\tcontinue\n\t\t\tdefault:\n\t\t\t\tc.pa.hdr = -1\n\t\t\t\tc.state = MSG_ARG\n\t\t\t\tc.as = i\n\t\t\t}\n\t\tcase MSG_ARG:\n\t\t\tswitch b {\n\t\t\tcase '\\r':\n\t\t\t\tc.drop = 1\n\t\t\tcase '\\n':\n\t\t\t\tvar arg []byte\n\t\t\t\tif c.argBuf != nil {\n\t\t\t\t\targ = c.argBuf\n\t\t\t\t\tc.argBuf = nil\n\t\t\t\t} else {\n\t\t\t\t\targ = buf[c.as : i-c.drop]\n\t\t\t\t}\n\t\t\t\tif err := c.overMaxControlLineLimit(arg, mcl); err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tvar err error\n\t\t\t\tif c.kind == ROUTER || c.kind == GATEWAY {\n\t\t\t\t\tswitch c.op {\n\t\t\t\t\tcase 'R', 'r':\n\t\t\t\t\t\tif trace {\n\t\t\t\t\t\t\tc.traceInOp(\"RMSG\", arg)\n\t\t\t\t\t\t}\n\t\t\t\t\t\terr = c.processRoutedMsgArgs(arg)\n\t\t\t\t\tcase 'L', 'l':\n\t\t\t\t\t\tif trace {\n\t\t\t\t\t\t\tc.traceInOp(\"LMSG\", arg)\n\t\t\t\t\t\t}\n\t\t\t\t\t\tlmsg = true\n\t\t\t\t\t\terr = c.processRoutedOriginClusterMsgArgs(arg)\n\t\t\t\t\t}\n\t\t\t\t} else if c.kind == LEAF {\n\t\t\t\t\tif trace {\n\t\t\t\t\t\tc.traceInOp(\"LMSG\", arg)\n\t\t\t\t\t}\n\t\t\t\t\terr = c.processLeafMsgArgs(arg)\n\t\t\t\t}\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tc.drop, c.as, c.state = 0, i+1, MSG_PAYLOAD\n\n\t\t\t\t// jump ahead with the index. If this overruns\n\t\t\t\t// what is left we fall out and process split\n\t\t\t\t// buffer.\n\t\t\t\ti = c.as + c.pa.size - LEN_CR_LF\n\t\t\tdefault:\n\t\t\t\tif c.argBuf != nil {\n\t\t\t\t\tc.argBuf = append(c.argBuf, b)\n\t\t\t\t}\n\t\t\t}\n\t\tcase OP_I:\n\t\t\tswitch b {\n\t\t\tcase 'N', 'n':\n\t\t\t\tc.state = OP_IN\n\t\t\tdefault:\n\t\t\t\tgoto parseErr\n\t\t\t}\n\t\tcase OP_IN:\n\t\t\tswitch b {\n\t\t\tcase 'F', 'f':\n\t\t\t\tc.state = OP_INF\n\t\t\tdefault:\n\t\t\t\tgoto parseErr\n\t\t\t}\n\t\tcase OP_INF:\n\t\t\tswitch b {\n\t\t\tcase 'O', 'o':\n\t\t\t\tc.state = OP_INFO\n\t\t\tdefault:\n\t\t\t\tgoto parseErr\n\t\t\t}\n\t\tcase OP_INFO:\n\t\t\tswitch b {\n\t\t\tcase ' ', '\\t':\n\t\t\t\tcontinue\n\t\t\tdefault:\n\t\t\t\tc.state = INFO_ARG\n\t\t\t\tc.as = i\n\t\t\t}\n\t\tcase INFO_ARG:\n\t\t\tswitch b {\n\t\t\tcase '\\r':\n\t\t\t\tc.drop = 1\n\t\t\tcase '\\n':\n\t\t\t\tvar arg []byte\n\t\t\t\tif c.argBuf != nil {\n\t\t\t\t\targ = c.argBuf\n\t\t\t\t\tc.argBuf = nil\n\t\t\t\t} else {\n\t\t\t\t\targ = buf[c.as : i-c.drop]\n\t\t\t\t}\n\t\t\t\tif err := c.overMaxControlLineLimit(arg, mcl); err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tif err := c.processInfo(arg); err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tc.drop, c.as, c.state = 0, i+1, OP_START\n\t\t\tdefault:\n\t\t\t\tif c.argBuf != nil {\n\t\t\t\t\tc.argBuf = append(c.argBuf, b)\n\t\t\t\t}\n\t\t\t}\n\t\tcase OP_PLUS:\n\t\t\tswitch b {\n\t\t\tcase 'O', 'o':\n\t\t\t\tc.state = OP_PLUS_O\n\t\t\tdefault:\n\t\t\t\tgoto parseErr\n\t\t\t}\n\t\tcase OP_PLUS_O:\n\t\t\tswitch b {\n\t\t\tcase 'K', 'k':\n\t\t\t\tc.state = OP_PLUS_OK\n\t\t\tdefault:\n\t\t\t\tgoto parseErr\n\t\t\t}\n\t\tcase OP_PLUS_OK:\n\t\t\tswitch b {\n\t\t\tcase '\\n':\n\t\t\t\tc.drop, c.state = 0, OP_START\n\t\t\t}\n\t\tcase OP_MINUS:\n\t\t\tswitch b {\n\t\t\tcase 'E', 'e':\n\t\t\t\tc.state = OP_MINUS_E\n\t\t\tdefault:\n\t\t\t\tgoto parseErr\n\t\t\t}\n\t\tcase OP_MINUS_E:\n\t\t\tswitch b {\n\t\t\tcase 'R', 'r':\n\t\t\t\tc.state = OP_MINUS_ER\n\t\t\tdefault:\n\t\t\t\tgoto parseErr\n\t\t\t}\n\t\tcase OP_MINUS_ER:\n\t\t\tswitch b {\n\t\t\tcase 'R', 'r':\n\t\t\t\tc.state = OP_MINUS_ERR\n\t\t\tdefault:\n\t\t\t\tgoto parseErr\n\t\t\t}\n\t\tcase OP_MINUS_ERR:\n\t\t\tswitch b {\n\t\t\tcase ' ', '\\t':\n\t\t\t\tc.state = OP_MINUS_ERR_SPC\n\t\t\tdefault:\n\t\t\t\tgoto parseErr\n\t\t\t}\n\t\tcase OP_MINUS_ERR_SPC:\n\t\t\tswitch b {\n\t\t\tcase ' ', '\\t':\n\t\t\t\tcontinue\n\t\t\tdefault:\n\t\t\t\tc.state = MINUS_ERR_ARG\n\t\t\t\tc.as = i\n\t\t\t}\n\t\tcase MINUS_ERR_ARG:\n\t\t\tswitch b {\n\t\t\tcase '\\r':\n\t\t\t\tc.drop = 1\n\t\t\tcase '\\n':\n\t\t\t\tvar arg []byte\n\t\t\t\tif c.argBuf != nil {\n\t\t\t\t\targ = c.argBuf\n\t\t\t\t\tc.argBuf = nil\n\t\t\t\t} else {\n\t\t\t\t\targ = buf[c.as : i-c.drop]\n\t\t\t\t}\n\t\t\t\tif err := c.overMaxControlLineLimit(arg, mcl); err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tc.processErr(string(arg))\n\t\t\t\tc.drop, c.as, c.state = 0, i+1, OP_START\n\t\t\tdefault:\n\t\t\t\tif c.argBuf != nil {\n\t\t\t\t\tc.argBuf = append(c.argBuf, b)\n\t\t\t\t}\n\t\t\t}\n\t\tdefault:\n\t\t\tgoto parseErr\n\t\t}\n\t}\n\n\t// Check for split buffer scenarios for any ARG state.\n\tif c.state == SUB_ARG || c.state == UNSUB_ARG ||\n\t\tc.state == PUB_ARG || c.state == HPUB_ARG ||\n\t\tc.state == ASUB_ARG || c.state == AUSUB_ARG ||\n\t\tc.state == MSG_ARG || c.state == HMSG_ARG ||\n\t\tc.state == MINUS_ERR_ARG || c.state == CONNECT_ARG || c.state == INFO_ARG {\n\n\t\t// Setup a holder buffer to deal with split buffer scenario.\n\t\tif c.argBuf == nil {\n\t\t\tc.argBuf = c.scratch[:0]\n\t\t\tc.argBuf = append(c.argBuf, buf[c.as:i-c.drop]...)\n\t\t}\n\t\t// Check for violations of control line length here. Note that this is not\n\t\t// exact at all but the performance hit is too great to be precise, and\n\t\t// catching here should prevent memory exhaustion attacks.\n\t\tif err := c.overMaxControlLineLimit(c.argBuf, mcl); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\t// Check for split msg\n\tif (c.state == MSG_PAYLOAD || c.state == MSG_END_R || c.state == MSG_END_N) && c.msgBuf == nil {\n\t\t// We need to clone the pubArg if it is still referencing the\n\t\t// read buffer and we are not able to process the msg.\n\n\t\tif c.argBuf == nil {\n\t\t\t// Works also for MSG_ARG, when message comes from ROUTE or GATEWAY.\n\t\t\tif err := c.clonePubArg(lmsg); err != nil {\n\t\t\t\tgoto parseErr\n\t\t\t}\n\t\t}\n\n\t\t// If we will overflow the scratch buffer, just create a\n\t\t// new buffer to hold the split message.\n\t\tif c.pa.size > cap(c.scratch)-len(c.argBuf) {\n\t\t\tlrem := len(buf[c.as:])\n\t\t\t// Consider it a protocol error when the remaining payload\n\t\t\t// is larger than the reported size for PUB. It can happen\n\t\t\t// when processing incomplete messages from rogue clients.\n\t\t\tif lrem > c.pa.size+LEN_CR_LF {\n\t\t\t\tgoto parseErr\n\t\t\t}\n\t\t\tc.msgBuf = make([]byte, lrem, c.pa.size+LEN_CR_LF)\n\t\t\tcopy(c.msgBuf, buf[c.as:])\n\t\t} else {\n\t\t\tc.msgBuf = c.scratch[len(c.argBuf):len(c.argBuf)]\n\t\t\tc.msgBuf = append(c.msgBuf, (buf[c.as:])...)\n\t\t}\n\t}\n\n\treturn nil\n\nauthErr:\n\tc.authViolation()\n\treturn ErrAuthentication\n\nparseErr:\n\tc.sendErr(\"Unknown Protocol Operation\")\n\tsnip := protoSnippet(i, PROTO_SNIPPET_SIZE, buf)\n\terr := fmt.Errorf(\"%s parser ERROR, state=%d, i=%d: proto='%s...'\", c.kindString(), c.state, i, snip)\n\treturn err\n}\n\nfunc protoSnippet(start, max int, buf []byte) string {\n\tstop := start + max\n\tbufSize := len(buf)\n\tif start >= bufSize {\n\t\treturn `\"\"`\n\t}\n\tif stop > bufSize {\n\t\tstop = bufSize - 1\n\t}\n\treturn fmt.Sprintf(\"%q\", buf[start:stop])\n}\n\n// Check if the length of buffer `arg` is over the max control line limit `mcl`.\n// If so, an error is sent to the client and the connection is closed.\n// The error ErrMaxControlLine is returned.\nfunc (c *client) overMaxControlLineLimit(arg []byte, mcl int32) error {\n\tif c.kind != CLIENT {\n\t\treturn nil\n\t}\n\tif len(arg) > int(mcl) {\n\t\terr := NewErrorCtx(ErrMaxControlLine, \"State %d, max_control_line %d, Buffer len %d (snip: %s...)\",\n\t\t\tc.state, int(mcl), len(c.argBuf), protoSnippet(0, MAX_CONTROL_LINE_SNIPPET_SIZE, arg))\n\t\tc.sendErr(err.Error())\n\t\tc.closeConnection(MaxControlLineExceeded)\n\t\treturn err\n\t}\n\treturn nil\n}\n\n// clonePubArg is used when the split buffer scenario has the pubArg in the existing read buffer, but\n// we need to hold onto it into the next read.\nfunc (c *client) clonePubArg(lmsg bool) error {\n\t// Just copy and re-process original arg buffer.\n\tc.argBuf = c.scratch[:0]\n\tc.argBuf = append(c.argBuf, c.pa.arg...)\n\n\tswitch c.kind {\n\tcase ROUTER, GATEWAY:\n\t\tif lmsg {\n\t\t\treturn c.processRoutedOriginClusterMsgArgs(c.argBuf)\n\t\t}\n\t\tif c.pa.hdr < 0 {\n\t\t\treturn c.processRoutedMsgArgs(c.argBuf)\n\t\t} else {\n\t\t\treturn c.processRoutedHeaderMsgArgs(c.argBuf)\n\t\t}\n\tcase LEAF:\n\t\tif c.pa.hdr < 0 {\n\t\t\treturn c.processLeafMsgArgs(c.argBuf)\n\t\t} else {\n\t\t\treturn c.processLeafHeaderMsgArgs(c.argBuf)\n\t\t}\n\tdefault:\n\t\tif c.pa.hdr < 0 {\n\t\t\treturn c.processPub(c.argBuf)\n\t\t} else {\n\t\t\treturn c.processHeaderPub(c.argBuf, nil)\n\t\t}\n\t}\n}\n\nfunc (ps *parseState) getHeader() http.Header {\n\tif ps.header == nil {\n\t\tif hdr := ps.pa.hdr; hdr > 0 {\n\t\t\treader := bufio.NewReader(bytes.NewReader(ps.msgBuf[0:hdr]))\n\t\t\ttp := textproto.NewReader(reader)\n\t\t\ttp.ReadLine() // skip over first line, contains version\n\t\t\tif mimeHeader, err := tp.ReadMIMEHeader(); err == nil {\n\t\t\t\tps.header = http.Header(mimeHeader)\n\t\t\t}\n\t\t}\n\t}\n\treturn ps.header\n}\n",
    "source_file": "server/parser.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2020-2025 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage server\n\nimport (\n\t\"bytes\"\n\tcrand \"crypto/rand\"\n\t\"crypto/sha1\"\n\t\"crypto/tls\"\n\t\"encoding/base64\"\n\t\"encoding/binary\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"log\"\n\tmrand \"math/rand\"\n\t\"net\"\n\t\"net/http\"\n\t\"net/url\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\t\"unicode/utf8\"\n\n\t\"github.com/klauspost/compress/flate\"\n)\n\ntype wsOpCode int\n\nconst (\n\t// From https://tools.ietf.org/html/rfc6455#section-5.2\n\twsTextMessage   = wsOpCode(1)\n\twsBinaryMessage = wsOpCode(2)\n\twsCloseMessage  = wsOpCode(8)\n\twsPingMessage   = wsOpCode(9)\n\twsPongMessage   = wsOpCode(10)\n\n\twsFinalBit = 1 << 7\n\twsRsv1Bit  = 1 << 6 // Used for compression, from https://tools.ietf.org/html/rfc7692#section-6\n\twsRsv2Bit  = 1 << 5\n\twsRsv3Bit  = 1 << 4\n\n\twsMaskBit = 1 << 7\n\n\twsContinuationFrame     = 0\n\twsMaxFrameHeaderSize    = 14 // Since LeafNode may need to behave as a client\n\twsMaxControlPayloadSize = 125\n\twsFrameSizeForBrowsers  = 4096 // From experiment, webrowsers behave better with limited frame size\n\twsCompressThreshold     = 64   // Don't compress for small buffer(s)\n\twsCloseSatusSize        = 2\n\n\t// From https://tools.ietf.org/html/rfc6455#section-11.7\n\twsCloseStatusNormalClosure      = 1000\n\twsCloseStatusGoingAway          = 1001\n\twsCloseStatusProtocolError      = 1002\n\twsCloseStatusUnsupportedData    = 1003\n\twsCloseStatusNoStatusReceived   = 1005\n\twsCloseStatusInvalidPayloadData = 1007\n\twsCloseStatusPolicyViolation    = 1008\n\twsCloseStatusMessageTooBig      = 1009\n\twsCloseStatusInternalSrvError   = 1011\n\twsCloseStatusTLSHandshake       = 1015\n\n\twsFirstFrame        = true\n\twsContFrame         = false\n\twsFinalFrame        = true\n\twsUncompressedFrame = false\n\n\twsSchemePrefix    = \"ws\"\n\twsSchemePrefixTLS = \"wss\"\n\n\twsNoMaskingHeader       = \"Nats-No-Masking\"\n\twsNoMaskingValue        = \"true\"\n\twsXForwardedForHeader   = \"X-Forwarded-For\"\n\twsNoMaskingFullResponse = wsNoMaskingHeader + \": \" + wsNoMaskingValue + CR_LF\n\twsPMCExtension          = \"permessage-deflate\" // per-message compression\n\twsPMCSrvNoCtx           = \"server_no_context_takeover\"\n\twsPMCCliNoCtx           = \"client_no_context_takeover\"\n\twsPMCReqHeaderValue     = wsPMCExtension + \"; \" + wsPMCSrvNoCtx + \"; \" + wsPMCCliNoCtx\n\twsPMCFullResponse       = \"Sec-WebSocket-Extensions: \" + wsPMCExtension + \"; \" + wsPMCSrvNoCtx + \"; \" + wsPMCCliNoCtx + _CRLF_\n\twsSecProto              = \"Sec-Websocket-Protocol\"\n\twsMQTTSecProtoVal       = \"mqtt\"\n\twsMQTTSecProto          = wsSecProto + \": \" + wsMQTTSecProtoVal + CR_LF\n)\n\nvar decompressorPool sync.Pool\nvar compressLastBlock = []byte{0x00, 0x00, 0xff, 0xff, 0x01, 0x00, 0x00, 0xff, 0xff}\n\n// From https://tools.ietf.org/html/rfc6455#section-1.3\nvar wsGUID = []byte(\"258EAFA5-E914-47DA-95CA-C5AB0DC85B11\")\n\n// Test can enable this so that server does not support \"no-masking\" requests.\nvar wsTestRejectNoMasking = false\n\ntype websocket struct {\n\tframes         net.Buffers\n\tfs             int64\n\tcloseMsg       []byte\n\tcompress       bool\n\tcloseSent      bool\n\tbrowser        bool\n\tnocompfrag     bool // No fragment for compressed frames\n\tmaskread       bool\n\tmaskwrite      bool\n\tcompressor     *flate.Writer\n\tcookieJwt      string\n\tcookieUsername string\n\tcookiePassword string\n\tcookieToken    string\n\tclientIP       string\n}\n\ntype srvWebsocket struct {\n\tmu             sync.RWMutex\n\tserver         *http.Server\n\tlistener       net.Listener\n\tlistenerErr    error\n\tallowedOrigins map[string]*allowedOrigin // host will be the key\n\tsameOrigin     bool\n\tconnectURLs    []string\n\tconnectURLsMap refCountedUrlSet\n\tauthOverride   bool   // indicate if there is auth override in websocket config\n\trawHeaders     string // raw headers to be used in the upgrade response.\n\n\t// These are immutable and can be accessed without lock.\n\t// This is the case when generating the client INFO.\n\ttls  bool   // True if TLS is required (TLSConfig is specified).\n\thost string // Host/IP the webserver is listening on (shortcut to opts.Websocket.Host).\n\tport int    // Port the webserver is listening on. This is after an ephemeral port may have been selected (shortcut to opts.Websocket.Port).\n}\n\ntype allowedOrigin struct {\n\tscheme string\n\tport   string\n}\n\ntype wsUpgradeResult struct {\n\tconn net.Conn\n\tws   *websocket\n\tkind int\n}\n\ntype wsReadInfo struct {\n\trem   int\n\tfs    bool\n\tff    bool\n\tfc    bool\n\tmask  bool // Incoming leafnode connections may not have masking.\n\tmkpos byte\n\tmkey  [4]byte\n\tcbufs [][]byte\n\tcoff  int\n}\n\nfunc (r *wsReadInfo) init() {\n\tr.fs, r.ff = true, true\n}\n\n// Returns a slice containing `needed` bytes from the given buffer `buf`\n// starting at position `pos`, and possibly read from the given reader `r`.\n// When bytes are present in `buf`, the `pos` is incremented by the number\n// of bytes found up to `needed` and the new position is returned. If not\n// enough bytes are found, the bytes found in `buf` are copied to the returned\n// slice and the remaning bytes are read from `r`.\nfunc wsGet(r io.Reader, buf []byte, pos, needed int) ([]byte, int, error) {\n\tavail := len(buf) - pos\n\tif avail >= needed {\n\t\treturn buf[pos : pos+needed], pos + needed, nil\n\t}\n\tb := make([]byte, needed)\n\tstart := copy(b, buf[pos:])\n\tfor start != needed {\n\t\tn, err := r.Read(b[start:cap(b)])\n\t\tif err != nil {\n\t\t\treturn nil, 0, err\n\t\t}\n\t\tstart += n\n\t}\n\treturn b, pos + avail, nil\n}\n\n// Returns true if this connection is from a Websocket client.\n// Lock held on entry.\nfunc (c *client) isWebsocket() bool {\n\treturn c.ws != nil\n}\n\n// Returns a slice of byte slices corresponding to payload of websocket frames.\n// The byte slice `buf` is filled with bytes from the connection's read loop.\n// This function will decode the frame headers and unmask the payload(s).\n// It is possible that the returned slices point to the given `buf` slice, so\n// `buf` should not be overwritten until the returned slices have been parsed.\n//\n// Client lock MUST NOT be held on entry.\nfunc (c *client) wsRead(r *wsReadInfo, ior io.Reader, buf []byte) ([][]byte, error) {\n\tvar (\n\t\tbufs   [][]byte\n\t\ttmpBuf []byte\n\t\terr    error\n\t\tpos    int\n\t\tmax    = len(buf)\n\t)\n\tfor pos != max {\n\t\tif r.fs {\n\t\t\tb0 := buf[pos]\n\t\t\tframeType := wsOpCode(b0 & 0xF)\n\t\t\tfinal := b0&wsFinalBit != 0\n\t\t\tcompressed := b0&wsRsv1Bit != 0\n\t\t\tpos++\n\n\t\t\ttmpBuf, pos, err = wsGet(ior, buf, pos, 1)\n\t\t\tif err != nil {\n\t\t\t\treturn bufs, err\n\t\t\t}\n\t\t\tb1 := tmpBuf[0]\n\n\t\t\t// Clients MUST set the mask bit. If not set, reject.\n\t\t\t// However, LEAF by default will not have masking, unless they are forced to, by configuration.\n\t\t\tif r.mask && b1&wsMaskBit == 0 {\n\t\t\t\treturn bufs, c.wsHandleProtocolError(\"mask bit missing\")\n\t\t\t}\n\n\t\t\t// Store size in case it is < 125\n\t\t\tr.rem = int(b1 & 0x7F)\n\n\t\t\tswitch frameType {\n\t\t\tcase wsPingMessage, wsPongMessage, wsCloseMessage:\n\t\t\t\tif r.rem > wsMaxControlPayloadSize {\n\t\t\t\t\treturn bufs, c.wsHandleProtocolError(\n\t\t\t\t\t\tfmt.Sprintf(\"control frame length bigger than maximum allowed of %v bytes\",\n\t\t\t\t\t\t\twsMaxControlPayloadSize))\n\t\t\t\t}\n\t\t\t\tif !final {\n\t\t\t\t\treturn bufs, c.wsHandleProtocolError(\"control frame does not have final bit set\")\n\t\t\t\t}\n\t\t\tcase wsTextMessage, wsBinaryMessage:\n\t\t\t\tif !r.ff {\n\t\t\t\t\treturn bufs, c.wsHandleProtocolError(\"new message started before final frame for previous message was received\")\n\t\t\t\t}\n\t\t\t\tr.ff = final\n\t\t\t\tr.fc = compressed\n\t\t\tcase wsContinuationFrame:\n\t\t\t\t// Compressed bit must be only set in the first frame\n\t\t\t\tif r.ff || compressed {\n\t\t\t\t\treturn bufs, c.wsHandleProtocolError(\"invalid continuation frame\")\n\t\t\t\t}\n\t\t\t\tr.ff = final\n\t\t\tdefault:\n\t\t\t\treturn bufs, c.wsHandleProtocolError(fmt.Sprintf(\"unknown opcode %v\", frameType))\n\t\t\t}\n\n\t\t\tswitch r.rem {\n\t\t\tcase 126:\n\t\t\t\ttmpBuf, pos, err = wsGet(ior, buf, pos, 2)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn bufs, err\n\t\t\t\t}\n\t\t\t\tr.rem = int(binary.BigEndian.Uint16(tmpBuf))\n\t\t\tcase 127:\n\t\t\t\ttmpBuf, pos, err = wsGet(ior, buf, pos, 8)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn bufs, err\n\t\t\t\t}\n\t\t\t\tr.rem = int(binary.BigEndian.Uint64(tmpBuf))\n\t\t\t}\n\n\t\t\tif r.mask {\n\t\t\t\t// Read masking key\n\t\t\t\ttmpBuf, pos, err = wsGet(ior, buf, pos, 4)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn bufs, err\n\t\t\t\t}\n\t\t\t\tcopy(r.mkey[:], tmpBuf)\n\t\t\t\tr.mkpos = 0\n\t\t\t}\n\n\t\t\t// Handle control messages in place...\n\t\t\tif wsIsControlFrame(frameType) {\n\t\t\t\tpos, err = c.wsHandleControlFrame(r, frameType, ior, buf, pos)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn bufs, err\n\t\t\t\t}\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// Done with the frame header\n\t\t\tr.fs = false\n\t\t}\n\t\tif pos < max {\n\t\t\tvar b []byte\n\t\t\tvar n int\n\n\t\t\tn = r.rem\n\t\t\tif pos+n > max {\n\t\t\t\tn = max - pos\n\t\t\t}\n\t\t\tb = buf[pos : pos+n]\n\t\t\tpos += n\n\t\t\tr.rem -= n\n\t\t\t// If needed, unmask the buffer\n\t\t\tif r.mask {\n\t\t\t\tr.unmask(b)\n\t\t\t}\n\t\t\taddToBufs := true\n\t\t\t// Handle compressed message\n\t\t\tif r.fc {\n\t\t\t\t// Assume that we may have continuation frames or not the full payload.\n\t\t\t\taddToBufs = false\n\t\t\t\t// Make a copy of the buffer before adding it to the list\n\t\t\t\t// of compressed fragments.\n\t\t\t\tr.cbufs = append(r.cbufs, append([]byte(nil), b...))\n\t\t\t\t// When we have the final frame and we have read the full payload,\n\t\t\t\t// we can decompress it.\n\t\t\t\tif r.ff && r.rem == 0 {\n\t\t\t\t\tb, err = r.decompress()\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn bufs, err\n\t\t\t\t\t}\n\t\t\t\t\tr.fc = false\n\t\t\t\t\t// Now we can add to `bufs`\n\t\t\t\t\taddToBufs = true\n\t\t\t\t}\n\t\t\t}\n\t\t\t// For non compressed frames, or when we have decompressed the\n\t\t\t// whole message.\n\t\t\tif addToBufs {\n\t\t\t\tbufs = append(bufs, b)\n\t\t\t}\n\t\t\t// If payload has been fully read, then indicate that next\n\t\t\t// is the start of a frame.\n\t\t\tif r.rem == 0 {\n\t\t\t\tr.fs = true\n\t\t\t}\n\t\t}\n\t}\n\treturn bufs, nil\n}\n\nfunc (r *wsReadInfo) Read(dst []byte) (int, error) {\n\tif len(dst) == 0 {\n\t\treturn 0, nil\n\t}\n\tif len(r.cbufs) == 0 {\n\t\treturn 0, io.EOF\n\t}\n\tcopied := 0\n\trem := len(dst)\n\tfor buf := r.cbufs[0]; buf != nil && rem > 0; {\n\t\tn := len(buf[r.coff:])\n\t\tif n > rem {\n\t\t\tn = rem\n\t\t}\n\t\tcopy(dst[copied:], buf[r.coff:r.coff+n])\n\t\tcopied += n\n\t\trem -= n\n\t\tr.coff += n\n\t\tbuf = r.nextCBuf()\n\t}\n\treturn copied, nil\n}\n\nfunc (r *wsReadInfo) nextCBuf() []byte {\n\t// We still have remaining data in the first buffer\n\tif r.coff != len(r.cbufs[0]) {\n\t\treturn r.cbufs[0]\n\t}\n\t// We read the full first buffer. Reset offset.\n\tr.coff = 0\n\t// We were at the last buffer, so we are done.\n\tif len(r.cbufs) == 1 {\n\t\tr.cbufs = nil\n\t\treturn nil\n\t}\n\t// Here we move to the next buffer.\n\tr.cbufs = r.cbufs[1:]\n\treturn r.cbufs[0]\n}\n\nfunc (r *wsReadInfo) ReadByte() (byte, error) {\n\tif len(r.cbufs) == 0 {\n\t\treturn 0, io.EOF\n\t}\n\tb := r.cbufs[0][r.coff]\n\tr.coff++\n\tr.nextCBuf()\n\treturn b, nil\n}\n\nfunc (r *wsReadInfo) decompress() ([]byte, error) {\n\tr.coff = 0\n\t// As per https://tools.ietf.org/html/rfc7692#section-7.2.2\n\t// add 0x00, 0x00, 0xff, 0xff and then a final block so that flate reader\n\t// does not report unexpected EOF.\n\tr.cbufs = append(r.cbufs, compressLastBlock)\n\t// Get a decompressor from the pool and bind it to this object (wsReadInfo)\n\t// that provides Read() and ReadByte() APIs that will consume the compressed\n\t// buffers (r.cbufs).\n\td, _ := decompressorPool.Get().(io.ReadCloser)\n\tif d == nil {\n\t\td = flate.NewReader(r)\n\t} else {\n\t\td.(flate.Resetter).Reset(r, nil)\n\t}\n\t// This will do the decompression.\n\tb, err := io.ReadAll(d)\n\tdecompressorPool.Put(d)\n\t// Now reset the compressed buffers list.\n\tr.cbufs = nil\n\treturn b, err\n}\n\n// Handles the PING, PONG and CLOSE websocket control frames.\n//\n// Client lock MUST NOT be held on entry.\nfunc (c *client) wsHandleControlFrame(r *wsReadInfo, frameType wsOpCode, nc io.Reader, buf []byte, pos int) (int, error) {\n\tvar payload []byte\n\tvar err error\n\n\tif r.rem > 0 {\n\t\tpayload, pos, err = wsGet(nc, buf, pos, r.rem)\n\t\tif err != nil {\n\t\t\treturn pos, err\n\t\t}\n\t\tif r.mask {\n\t\t\tr.unmask(payload)\n\t\t}\n\t\tr.rem = 0\n\t}\n\tswitch frameType {\n\tcase wsCloseMessage:\n\t\tstatus := wsCloseStatusNoStatusReceived\n\t\tvar body string\n\t\tlp := len(payload)\n\t\t// If there is a payload, the status is represented as a 2-byte\n\t\t// unsigned integer (in network byte order). Then, there may be an\n\t\t// optional body.\n\t\thasStatus, hasBody := lp >= wsCloseSatusSize, lp > wsCloseSatusSize\n\t\tif hasStatus {\n\t\t\t// Decode the status\n\t\t\tstatus = int(binary.BigEndian.Uint16(payload[:wsCloseSatusSize]))\n\t\t\t// Now if there is a body, capture it and make sure this is a valid UTF-8.\n\t\t\tif hasBody {\n\t\t\t\tbody = string(payload[wsCloseSatusSize:])\n\t\t\t\tif !utf8.ValidString(body) {\n\t\t\t\t\t// https://tools.ietf.org/html/rfc6455#section-5.5.1\n\t\t\t\t\t// If body is present, it must be a valid utf8\n\t\t\t\t\tstatus = wsCloseStatusInvalidPayloadData\n\t\t\t\t\tbody = \"invalid utf8 body in close frame\"\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t// If the status indicates that nothing was received, then we don't\n\t\t// send anything back.\n\t\t// From https://datatracker.ietf.org/doc/html/rfc6455#section-7.4\n\t\t// it says that code 1005 is a reserved value and MUST NOT be set as a\n\t\t// status code in a Close control frame by an endpoint.  It is\n\t\t// designated for use in applications expecting a status code to indicate\n\t\t// that no status code was actually present.\n\t\tvar clm []byte\n\t\tif status != wsCloseStatusNoStatusReceived {\n\t\t\tclm = wsCreateCloseMessage(status, body)\n\t\t}\n\t\tc.wsEnqueueControlMessage(wsCloseMessage, clm)\n\t\tif len(clm) > 0 {\n\t\t\tnbPoolPut(clm) // wsEnqueueControlMessage has taken a copy.\n\t\t}\n\t\t// Return io.EOF so that readLoop will close the connection as ClientClosed\n\t\t// after processing pending buffers.\n\t\treturn pos, io.EOF\n\tcase wsPingMessage:\n\t\tc.wsEnqueueControlMessage(wsPongMessage, payload)\n\tcase wsPongMessage:\n\t\t// Nothing to do..\n\t}\n\treturn pos, nil\n}\n\n// Unmask the given slice.\nfunc (r *wsReadInfo) unmask(buf []byte) {\n\tp := int(r.mkpos)\n\tif len(buf) < 16 {\n\t\tfor i := 0; i < len(buf); i++ {\n\t\t\tbuf[i] ^= r.mkey[p&3]\n\t\t\tp++\n\t\t}\n\t\tr.mkpos = byte(p & 3)\n\t\treturn\n\t}\n\tvar k [8]byte\n\tfor i := 0; i < 8; i++ {\n\t\tk[i] = r.mkey[(p+i)&3]\n\t}\n\tkm := binary.BigEndian.Uint64(k[:])\n\tn := (len(buf) / 8) * 8\n\tfor i := 0; i < n; i += 8 {\n\t\ttmp := binary.BigEndian.Uint64(buf[i : i+8])\n\t\ttmp ^= km\n\t\tbinary.BigEndian.PutUint64(buf[i:], tmp)\n\t}\n\tbuf = buf[n:]\n\tfor i := 0; i < len(buf); i++ {\n\t\tbuf[i] ^= r.mkey[p&3]\n\t\tp++\n\t}\n\tr.mkpos = byte(p & 3)\n}\n\n// Returns true if the op code corresponds to a control frame.\nfunc wsIsControlFrame(frameType wsOpCode) bool {\n\treturn frameType >= wsCloseMessage\n}\n\n// Create the frame header.\n// Encodes the frame type and optional compression flag, and the size of the payload.\nfunc wsCreateFrameHeader(useMasking, compressed bool, frameType wsOpCode, l int) ([]byte, []byte) {\n\tfh := nbPoolGet(wsMaxFrameHeaderSize)[:wsMaxFrameHeaderSize]\n\tn, key := wsFillFrameHeader(fh, useMasking, wsFirstFrame, wsFinalFrame, compressed, frameType, l)\n\treturn fh[:n], key\n}\n\nfunc wsFillFrameHeader(fh []byte, useMasking, first, final, compressed bool, frameType wsOpCode, l int) (int, []byte) {\n\tvar n int\n\tvar b byte\n\tif first {\n\t\tb = byte(frameType)\n\t}\n\tif final {\n\t\tb |= wsFinalBit\n\t}\n\tif compressed {\n\t\tb |= wsRsv1Bit\n\t}\n\tb1 := byte(0)\n\tif useMasking {\n\t\tb1 |= wsMaskBit\n\t}\n\tswitch {\n\tcase l <= 125:\n\t\tn = 2\n\t\tfh[0] = b\n\t\tfh[1] = b1 | byte(l)\n\tcase l < 65536:\n\t\tn = 4\n\t\tfh[0] = b\n\t\tfh[1] = b1 | 126\n\t\tbinary.BigEndian.PutUint16(fh[2:], uint16(l))\n\tdefault:\n\t\tn = 10\n\t\tfh[0] = b\n\t\tfh[1] = b1 | 127\n\t\tbinary.BigEndian.PutUint64(fh[2:], uint64(l))\n\t}\n\tvar key []byte\n\tif useMasking {\n\t\tvar keyBuf [4]byte\n\t\tif _, err := io.ReadFull(crand.Reader, keyBuf[:4]); err != nil {\n\t\t\tkv := mrand.Int31()\n\t\t\tbinary.LittleEndian.PutUint32(keyBuf[:4], uint32(kv))\n\t\t}\n\t\tcopy(fh[n:], keyBuf[:4])\n\t\tkey = fh[n : n+4]\n\t\tn += 4\n\t}\n\treturn n, key\n}\n\n// Invokes wsEnqueueControlMessageLocked under client lock.\n//\n// Client lock MUST NOT be held on entry\nfunc (c *client) wsEnqueueControlMessage(controlMsg wsOpCode, payload []byte) {\n\tc.mu.Lock()\n\tc.wsEnqueueControlMessageLocked(controlMsg, payload)\n\tc.mu.Unlock()\n}\n\n// Mask the buffer with the given key\nfunc wsMaskBuf(key, buf []byte) {\n\tfor i := 0; i < len(buf); i++ {\n\t\tbuf[i] ^= key[i&3]\n\t}\n}\n\n// Mask the buffers, as if they were contiguous, with the given key\nfunc wsMaskBufs(key []byte, bufs [][]byte) {\n\tpos := 0\n\tfor i := 0; i < len(bufs); i++ {\n\t\tbuf := bufs[i]\n\t\tfor j := 0; j < len(buf); j++ {\n\t\t\tbuf[j] ^= key[pos&3]\n\t\t\tpos++\n\t\t}\n\t}\n}\n\n// Enqueues a websocket control message.\n// If the control message is a wsCloseMessage, then marks this client\n// has having sent the close message (since only one should be sent).\n// This will prevent the generic closeConnection() to enqueue one.\n//\n// Client lock held on entry.\nfunc (c *client) wsEnqueueControlMessageLocked(controlMsg wsOpCode, payload []byte) {\n\t// Control messages are never compressed and their size will be\n\t// less than wsMaxControlPayloadSize, which means the frame header\n\t// will be only 2 or 6 bytes.\n\tuseMasking := c.ws.maskwrite\n\tsz := 2\n\tif useMasking {\n\t\tsz += 4\n\t}\n\tcm := nbPoolGet(sz + len(payload))\n\tcm = cm[:cap(cm)]\n\tn, key := wsFillFrameHeader(cm, useMasking, wsFirstFrame, wsFinalFrame, wsUncompressedFrame, controlMsg, len(payload))\n\tcm = cm[:n]\n\t// Note that payload is optional.\n\tif len(payload) > 0 {\n\t\tcm = append(cm, payload...)\n\t\tif useMasking {\n\t\t\twsMaskBuf(key, cm[n:])\n\t\t}\n\t}\n\tc.out.pb += int64(len(cm))\n\tif controlMsg == wsCloseMessage {\n\t\t// We can't add the close message to the frames buffers\n\t\t// now. It will be done on a flushOutbound() when there\n\t\t// are no more pending buffers to send.\n\t\tc.ws.closeSent = true\n\t\tc.ws.closeMsg = cm\n\t} else {\n\t\tc.ws.frames = append(c.ws.frames, cm)\n\t\tc.ws.fs += int64(len(cm))\n\t}\n\tc.flushSignal()\n}\n\n// Enqueues a websocket close message with a status mapped from the given `reason`.\n//\n// Client lock held on entry\nfunc (c *client) wsEnqueueCloseMessage(reason ClosedState) {\n\tvar status int\n\tswitch reason {\n\tcase ClientClosed:\n\t\tstatus = wsCloseStatusNormalClosure\n\tcase AuthenticationTimeout, AuthenticationViolation, SlowConsumerPendingBytes, SlowConsumerWriteDeadline,\n\t\tMaxAccountConnectionsExceeded, MaxConnectionsExceeded, MaxControlLineExceeded, MaxSubscriptionsExceeded,\n\t\tMissingAccount, AuthenticationExpired, Revocation:\n\t\tstatus = wsCloseStatusPolicyViolation\n\tcase TLSHandshakeError:\n\t\tstatus = wsCloseStatusTLSHandshake\n\tcase ParseError, ProtocolViolation, BadClientProtocolVersion:\n\t\tstatus = wsCloseStatusProtocolError\n\tcase MaxPayloadExceeded:\n\t\tstatus = wsCloseStatusMessageTooBig\n\tcase WriteError, ReadError, StaleConnection, ServerShutdown:\n\t\t// We used to have WriteError, ReadError and StaleConnection result in\n\t\t// code 1006, which the spec says that it must not be used to set the\n\t\t// status in the close message. So using this one instead.\n\t\tstatus = wsCloseStatusGoingAway\n\tdefault:\n\t\tstatus = wsCloseStatusInternalSrvError\n\t}\n\tbody := wsCreateCloseMessage(status, reason.String())\n\tc.wsEnqueueControlMessageLocked(wsCloseMessage, body)\n\tnbPoolPut(body) // wsEnqueueControlMessageLocked has taken a copy.\n}\n\n// Create and then enqueue a close message with a protocol error and the\n// given message. This is invoked when parsing websocket frames.\n//\n// Lock MUST NOT be held on entry.\nfunc (c *client) wsHandleProtocolError(message string) error {\n\tbuf := wsCreateCloseMessage(wsCloseStatusProtocolError, message)\n\tc.wsEnqueueControlMessage(wsCloseMessage, buf)\n\tnbPoolPut(buf) // wsEnqueueControlMessage has taken a copy.\n\treturn errors.New(message)\n}\n\n// Create a close message with the given `status` and `body`.\n// If the `body` is more than the maximum allows control frame payload size,\n// it is truncated and \"...\" is added at the end (as a hint that message\n// is not complete).\nfunc wsCreateCloseMessage(status int, body string) []byte {\n\t// Since a control message payload is limited in size, we\n\t// will limit the text and add trailing \"...\" if truncated.\n\t// The body of a Close Message must be preceded with 2 bytes,\n\t// so take that into account for limiting the body length.\n\tif len(body) > wsMaxControlPayloadSize-2 {\n\t\tbody = body[:wsMaxControlPayloadSize-5]\n\t\tbody += \"...\"\n\t}\n\tbuf := nbPoolGet(2 + len(body))[:2+len(body)]\n\t// We need to have a 2 byte unsigned int that represents the error status code\n\t// https://tools.ietf.org/html/rfc6455#section-5.5.1\n\tbinary.BigEndian.PutUint16(buf[:2], uint16(status))\n\tcopy(buf[2:], []byte(body))\n\treturn buf\n}\n\n// Process websocket client handshake. On success, returns the raw net.Conn that\n// will be used to create a *client object.\n// Invoked from the HTTP server listening on websocket port.\nfunc (s *Server) wsUpgrade(w http.ResponseWriter, r *http.Request) (*wsUpgradeResult, error) {\n\tkind := CLIENT\n\tif r.URL != nil {\n\t\tep := r.URL.EscapedPath()\n\t\tif strings.HasSuffix(ep, leafNodeWSPath) {\n\t\t\tkind = LEAF\n\t\t} else if strings.HasSuffix(ep, mqttWSPath) {\n\t\t\tkind = MQTT\n\t\t}\n\t}\n\n\topts := s.getOpts()\n\n\t// From https://tools.ietf.org/html/rfc6455#section-4.2.1\n\t// Point 1.\n\tif r.Method != \"GET\" {\n\t\treturn nil, wsReturnHTTPError(w, r, http.StatusMethodNotAllowed, \"request method must be GET\")\n\t}\n\t// Point 2.\n\tif r.Host == _EMPTY_ {\n\t\treturn nil, wsReturnHTTPError(w, r, http.StatusBadRequest, \"'Host' missing in request\")\n\t}\n\t// Point 3.\n\tif !wsHeaderContains(r.Header, \"Upgrade\", \"websocket\") {\n\t\treturn nil, wsReturnHTTPError(w, r, http.StatusBadRequest, \"invalid value for header 'Upgrade'\")\n\t}\n\t// Point 4.\n\tif !wsHeaderContains(r.Header, \"Connection\", \"Upgrade\") {\n\t\treturn nil, wsReturnHTTPError(w, r, http.StatusBadRequest, \"invalid value for header 'Connection'\")\n\t}\n\t// Point 5.\n\tkey := r.Header.Get(\"Sec-Websocket-Key\")\n\tif key == _EMPTY_ {\n\t\treturn nil, wsReturnHTTPError(w, r, http.StatusBadRequest, \"key missing\")\n\t}\n\t// Point 6.\n\tif !wsHeaderContains(r.Header, \"Sec-Websocket-Version\", \"13\") {\n\t\treturn nil, wsReturnHTTPError(w, r, http.StatusBadRequest, \"invalid version\")\n\t}\n\t// Others are optional\n\t// Point 7.\n\tif err := s.websocket.checkOrigin(r); err != nil {\n\t\treturn nil, wsReturnHTTPError(w, r, http.StatusForbidden, fmt.Sprintf(\"origin not allowed: %v\", err))\n\t}\n\t// Point 8.\n\t// We don't have protocols, so ignore.\n\t// Point 9.\n\t// Extensions, only support for compression at the moment\n\tcompress := opts.Websocket.Compression\n\tif compress {\n\t\t// Simply check if permessage-deflate extension is present.\n\t\tcompress, _ = wsPMCExtensionSupport(r.Header, true)\n\t}\n\t// We will do masking if asked (unless we reject for tests)\n\tnoMasking := r.Header.Get(wsNoMaskingHeader) == wsNoMaskingValue && !wsTestRejectNoMasking\n\n\th := w.(http.Hijacker)\n\tconn, brw, err := h.Hijack()\n\tif err != nil {\n\t\tif conn != nil {\n\t\t\tconn.Close()\n\t\t}\n\t\treturn nil, wsReturnHTTPError(w, r, http.StatusInternalServerError, err.Error())\n\t}\n\tif brw.Reader.Buffered() > 0 {\n\t\tconn.Close()\n\t\treturn nil, wsReturnHTTPError(w, r, http.StatusBadRequest, \"client sent data before handshake is complete\")\n\t}\n\n\tvar buf [1024]byte\n\tp := buf[:0]\n\n\t// From https://tools.ietf.org/html/rfc6455#section-4.2.2\n\tp = append(p, \"HTTP/1.1 101 Switching Protocols\\r\\nUpgrade: websocket\\r\\nConnection: Upgrade\\r\\nSec-WebSocket-Accept: \"...)\n\tp = append(p, wsAcceptKey(key)...)\n\tp = append(p, _CRLF_...)\n\tif compress {\n\t\tp = append(p, wsPMCFullResponse...)\n\t}\n\tif noMasking {\n\t\tp = append(p, wsNoMaskingFullResponse...)\n\t}\n\tif kind == MQTT {\n\t\tp = append(p, wsMQTTSecProto...)\n\t}\n\tif s.websocket.rawHeaders != _EMPTY_ {\n\t\tp = append(p, s.websocket.rawHeaders...)\n\t}\n\tp = append(p, _CRLF_...)\n\n\tif _, err = conn.Write(p); err != nil {\n\t\tconn.Close()\n\t\treturn nil, err\n\t}\n\t// If there was a deadline set for the handshake, clear it now.\n\tif opts.Websocket.HandshakeTimeout > 0 {\n\t\tconn.SetDeadline(time.Time{})\n\t}\n\t// Server always expect \"clients\" to send masked payload, unless the option\n\t// \"no-masking\" has been enabled.\n\tws := &websocket{compress: compress, maskread: !noMasking}\n\n\t// Check for X-Forwarded-For header\n\tif cips, ok := r.Header[wsXForwardedForHeader]; ok {\n\t\tcip := cips[0]\n\t\tif net.ParseIP(cip) != nil {\n\t\t\tws.clientIP = cip\n\t\t}\n\t}\n\n\tif kind == CLIENT || kind == MQTT {\n\t\t// Indicate if this is likely coming from a browser.\n\t\tif ua := r.Header.Get(\"User-Agent\"); ua != _EMPTY_ && strings.HasPrefix(ua, \"Mozilla/\") {\n\t\t\tws.browser = true\n\t\t\t// Disable fragmentation of compressed frames for Safari browsers.\n\t\t\t// Unfortunately, you could be running Chrome on macOS and this\n\t\t\t// string will contain \"Safari/\" (along \"Chrome/\"). However, what\n\t\t\t// I have found is that actual Safari browser also have \"Version/\".\n\t\t\t// So make the combination of the two.\n\t\t\tws.nocompfrag = ws.compress && strings.Contains(ua, \"Version/\") && strings.Contains(ua, \"Safari/\")\n\t\t}\n\n\t\tif cookies := r.Cookies(); len(cookies) > 0 {\n\t\t\tows := &opts.Websocket\n\t\t\tfor _, c := range cookies {\n\t\t\t\tif ows.JWTCookie == c.Name {\n\t\t\t\t\tws.cookieJwt = c.Value\n\t\t\t\t} else if ows.UsernameCookie == c.Name {\n\t\t\t\t\tws.cookieUsername = c.Value\n\t\t\t\t} else if ows.PasswordCookie == c.Name {\n\t\t\t\t\tws.cookiePassword = c.Value\n\t\t\t\t} else if ows.TokenCookie == c.Name {\n\t\t\t\t\tws.cookieToken = c.Value\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn &wsUpgradeResult{conn: conn, ws: ws, kind: kind}, nil\n}\n\n// Returns true if the header named `name` contains a token with value `value`.\nfunc wsHeaderContains(header http.Header, name string, value string) bool {\n\tfor _, s := range header[name] {\n\t\ttokens := strings.Split(s, \",\")\n\t\tfor _, t := range tokens {\n\t\t\tt = strings.Trim(t, \" \\t\")\n\t\t\tif strings.EqualFold(t, value) {\n\t\t\t\treturn true\n\t\t\t}\n\t\t}\n\t}\n\treturn false\n}\n\nfunc wsPMCExtensionSupport(header http.Header, checkPMCOnly bool) (bool, bool) {\n\tfor _, extensionList := range header[\"Sec-Websocket-Extensions\"] {\n\t\textensions := strings.Split(extensionList, \",\")\n\t\tfor _, extension := range extensions {\n\t\t\textension = strings.Trim(extension, \" \\t\")\n\t\t\tparams := strings.Split(extension, \";\")\n\t\t\tfor i, p := range params {\n\t\t\t\tp = strings.Trim(p, \" \\t\")\n\t\t\t\tif strings.EqualFold(p, wsPMCExtension) {\n\t\t\t\t\tif checkPMCOnly {\n\t\t\t\t\t\treturn true, false\n\t\t\t\t\t}\n\t\t\t\t\tvar snc bool\n\t\t\t\t\tvar cnc bool\n\t\t\t\t\tfor j := i + 1; j < len(params); j++ {\n\t\t\t\t\t\tp = params[j]\n\t\t\t\t\t\tp = strings.Trim(p, \" \\t\")\n\t\t\t\t\t\tif strings.EqualFold(p, wsPMCSrvNoCtx) {\n\t\t\t\t\t\t\tsnc = true\n\t\t\t\t\t\t} else if strings.EqualFold(p, wsPMCCliNoCtx) {\n\t\t\t\t\t\t\tcnc = true\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif snc && cnc {\n\t\t\t\t\t\t\treturn true, true\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\treturn true, false\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn false, false\n}\n\n// Send an HTTP error with the given `status` to the given http response writer `w`.\n// Return an error created based on the `reason` string.\nfunc wsReturnHTTPError(w http.ResponseWriter, r *http.Request, status int, reason string) error {\n\terr := fmt.Errorf(\"%s - websocket handshake error: %s\", r.RemoteAddr, reason)\n\tw.Header().Set(\"Sec-Websocket-Version\", \"13\")\n\thttp.Error(w, http.StatusText(status), status)\n\treturn err\n}\n\n// If the server is configured to accept any origin, then this function returns\n// `nil` without checking if the Origin is present and valid. This is also\n// the case if the request does not have the Origin header.\n// Otherwise, this will check that the Origin matches the same origin or\n// any origin in the allowed list.\nfunc (w *srvWebsocket) checkOrigin(r *http.Request) error {\n\tw.mu.RLock()\n\tcheckSame := w.sameOrigin\n\tlistEmpty := len(w.allowedOrigins) == 0\n\tw.mu.RUnlock()\n\tif !checkSame && listEmpty {\n\t\treturn nil\n\t}\n\torigin := r.Header.Get(\"Origin\")\n\tif origin == _EMPTY_ {\n\t\torigin = r.Header.Get(\"Sec-Websocket-Origin\")\n\t}\n\t// If the header is not present, we will accept.\n\t// From https://datatracker.ietf.org/doc/html/rfc6455#section-1.6\n\t// \"Naturally, when the WebSocket Protocol is used by a dedicated client\n\t// directly (i.e., not from a web page through a web browser), the origin\n\t// model is not useful, as the client can provide any arbitrary origin string.\"\n\tif origin == _EMPTY_ {\n\t\treturn nil\n\t}\n\tu, err := url.ParseRequestURI(origin)\n\tif err != nil {\n\t\treturn err\n\t}\n\toh, op, err := wsGetHostAndPort(u.Scheme == \"https\", u.Host)\n\tif err != nil {\n\t\treturn err\n\t}\n\t// If checking same origin, compare with the http's request's Host.\n\tif checkSame {\n\t\trh, rp, err := wsGetHostAndPort(r.TLS != nil, r.Host)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif oh != rh || op != rp {\n\t\t\treturn errors.New(\"not same origin\")\n\t\t}\n\t\t// I guess it is possible to have cases where one wants to check\n\t\t// same origin, but also that the origin is in the allowed list.\n\t\t// So continue with the next check.\n\t}\n\tif !listEmpty {\n\t\tw.mu.RLock()\n\t\tao := w.allowedOrigins[oh]\n\t\tw.mu.RUnlock()\n\t\tif ao == nil || u.Scheme != ao.scheme || op != ao.port {\n\t\t\treturn errors.New(\"not in the allowed list\")\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc wsGetHostAndPort(tls bool, hostport string) (string, string, error) {\n\thost, port, err := net.SplitHostPort(hostport)\n\tif err != nil {\n\t\t// If error is missing port, then use defaults based on the scheme\n\t\tif ae, ok := err.(*net.AddrError); ok && strings.Contains(ae.Err, \"missing port\") {\n\t\t\terr = nil\n\t\t\thost = hostport\n\t\t\tif tls {\n\t\t\t\tport = \"443\"\n\t\t\t} else {\n\t\t\t\tport = \"80\"\n\t\t\t}\n\t\t}\n\t}\n\treturn strings.ToLower(host), port, err\n}\n\n// Concatenate the key sent by the client with the GUID, then computes the SHA1 hash\n// and returns it as a based64 encoded string.\nfunc wsAcceptKey(key string) string {\n\th := sha1.New()\n\th.Write([]byte(key))\n\th.Write(wsGUID)\n\treturn base64.StdEncoding.EncodeToString(h.Sum(nil))\n}\n\nfunc wsMakeChallengeKey() (string, error) {\n\tp := make([]byte, 16)\n\tif _, err := io.ReadFull(crand.Reader, p); err != nil {\n\t\treturn _EMPTY_, err\n\t}\n\treturn base64.StdEncoding.EncodeToString(p), nil\n}\n\n// Validate the websocket related options.\nfunc validateWebsocketOptions(o *Options) error {\n\two := &o.Websocket\n\t// If no port is defined, we don't care about other options\n\tif wo.Port == 0 {\n\t\treturn nil\n\t}\n\t// Enforce TLS... unless NoTLS is set to true.\n\tif wo.TLSConfig == nil && !wo.NoTLS {\n\t\treturn errors.New(\"websocket requires TLS configuration\")\n\t}\n\t// Make sure that allowed origins, if specified, can be parsed.\n\tfor _, ao := range wo.AllowedOrigins {\n\t\tif _, err := url.Parse(ao); err != nil {\n\t\t\treturn fmt.Errorf(\"unable to parse allowed origin: %v\", err)\n\t\t}\n\t}\n\t// If there is a NoAuthUser, we need to have Users defined and\n\t// the user to be present.\n\tif wo.NoAuthUser != _EMPTY_ {\n\t\tif err := validateNoAuthUser(o, wo.NoAuthUser); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\t// Token/Username not possible if there are users/nkeys\n\tif len(o.Users) > 0 || len(o.Nkeys) > 0 {\n\t\tif wo.Username != _EMPTY_ {\n\t\t\treturn fmt.Errorf(\"websocket authentication username not compatible with presence of users/nkeys\")\n\t\t}\n\t\tif wo.Token != _EMPTY_ {\n\t\t\treturn fmt.Errorf(\"websocket authentication token not compatible with presence of users/nkeys\")\n\t\t}\n\t}\n\t// Using JWT requires Trusted Keys\n\tif wo.JWTCookie != _EMPTY_ {\n\t\tif len(o.TrustedOperators) == 0 && len(o.TrustedKeys) == 0 {\n\t\t\treturn fmt.Errorf(\"trusted operators or trusted keys configuration is required for JWT authentication via cookie %q\", wo.JWTCookie)\n\t\t}\n\t}\n\tif err := validatePinnedCerts(wo.TLSPinnedCerts); err != nil {\n\t\treturn fmt.Errorf(\"websocket: %v\", err)\n\t}\n\n\t// Check for invalid headers here.\n\tfor key := range wo.Headers {\n\t\tk := strings.ToLower(key)\n\t\tswitch k {\n\t\tcase \"host\",\n\t\t\t\"content-length\",\n\t\t\t\"connection\",\n\t\t\t\"upgrade\",\n\t\t\t\"nats-no-masking\":\n\t\t\treturn fmt.Errorf(\"websocket: invalid header %q not allowed\", key)\n\t\t}\n\n\t\tif strings.HasPrefix(k, \"sec-websocket-\") {\n\t\t\treturn fmt.Errorf(\"websocket: invalid header %q, \\\"Sec-WebSocket-\\\" prefix not allowed\", key)\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// Creates or updates the existing map\nfunc (s *Server) wsSetOriginOptions(o *WebsocketOpts) {\n\tws := &s.websocket\n\tws.mu.Lock()\n\tdefer ws.mu.Unlock()\n\t// Copy over the option's same origin boolean\n\tws.sameOrigin = o.SameOrigin\n\t// Reset the map. Will help for config reload if/when we support it.\n\tws.allowedOrigins = nil\n\tif o.AllowedOrigins == nil {\n\t\treturn\n\t}\n\tfor _, ao := range o.AllowedOrigins {\n\t\t// We have previously checked (during options validation) that the urls\n\t\t// are parseable, but if we get an error, report and skip.\n\t\tu, err := url.ParseRequestURI(ao)\n\t\tif err != nil {\n\t\t\ts.Errorf(\"error parsing allowed origin: %v\", err)\n\t\t\tcontinue\n\t\t}\n\t\th, p, _ := wsGetHostAndPort(u.Scheme == \"https\", u.Host)\n\t\tif ws.allowedOrigins == nil {\n\t\t\tws.allowedOrigins = make(map[string]*allowedOrigin, len(o.AllowedOrigins))\n\t\t}\n\t\tws.allowedOrigins[h] = &allowedOrigin{scheme: u.Scheme, port: p}\n\t}\n}\n\n// Calculate the raw headers for websocket upgrade response.\nfunc (s *Server) wsSetHeadersOptions(o *WebsocketOpts) {\n\tvar sb strings.Builder\n\tfor k, v := range o.Headers {\n\t\tsb.WriteString(k)\n\t\tsb.WriteString(\": \")\n\t\tsb.WriteString(v)\n\t\tsb.WriteString(_CRLF_)\n\t}\n\tws := &s.websocket\n\tws.mu.Lock()\n\tdefer ws.mu.Unlock()\n\tws.rawHeaders = sb.String()\n}\n\n// Given the websocket options, we check if any auth configuration\n// has been provided. If so, possibly create users/nkey users and\n// store them in s.websocket.users/nkeys.\n// Also update a boolean that indicates if auth is required for\n// websocket clients.\n// Server lock is held on entry.\nfunc (s *Server) wsConfigAuth(opts *WebsocketOpts) {\n\tws := &s.websocket\n\t// If any of those is specified, we consider that there is an override.\n\tws.authOverride = opts.Username != _EMPTY_ || opts.Token != _EMPTY_ || opts.NoAuthUser != _EMPTY_\n}\n\nfunc (s *Server) startWebsocketServer() {\n\tif s.isShuttingDown() {\n\t\treturn\n\t}\n\n\tsopts := s.getOpts()\n\to := &sopts.Websocket\n\n\ts.wsSetOriginOptions(o)\n\ts.wsSetHeadersOptions(o)\n\n\tvar hl net.Listener\n\tvar proto string\n\tvar err error\n\n\tport := o.Port\n\tif port == -1 {\n\t\tport = 0\n\t}\n\thp := net.JoinHostPort(o.Host, strconv.Itoa(port))\n\n\t// We are enforcing (when validating the options) the use of TLS, but the\n\t// code was originally supporting both modes. The reason for TLS only is\n\t// that we expect users to send JWTs with bearer tokens and we want to\n\t// avoid the possibility of it being \"intercepted\".\n\n\ts.mu.Lock()\n\t// Do not check o.NoTLS here. If a TLS configuration is available, use it,\n\t// regardless of NoTLS. If we don't have a TLS config, it means that the\n\t// user has configured NoTLS because otherwise the server would have failed\n\t// to start due to options validation.\n\tif o.TLSConfig != nil {\n\t\tproto = wsSchemePrefixTLS\n\t\tconfig := o.TLSConfig.Clone()\n\t\tconfig.GetConfigForClient = s.wsGetTLSConfig\n\t\thl, err = tls.Listen(\"tcp\", hp, config)\n\t} else {\n\t\tproto = wsSchemePrefix\n\t\thl, err = net.Listen(\"tcp\", hp)\n\t}\n\ts.websocket.listenerErr = err\n\tif err != nil {\n\t\ts.mu.Unlock()\n\t\ts.Fatalf(\"Unable to listen for websocket connections: %v\", err)\n\t\treturn\n\t}\n\tif port == 0 {\n\t\to.Port = hl.Addr().(*net.TCPAddr).Port\n\t}\n\ts.Noticef(\"Listening for websocket clients on %s://%s:%d\", proto, o.Host, o.Port)\n\tif proto == wsSchemePrefix {\n\t\ts.Warnf(\"Websocket not configured with TLS. DO NOT USE IN PRODUCTION!\")\n\t}\n\n\t// These 3 are immutable and will be accessed without lock by the client\n\t// when generating/sending the INFO protocols.\n\ts.websocket.tls = proto == wsSchemePrefixTLS\n\ts.websocket.host, s.websocket.port = o.Host, o.Port\n\n\t// This will be updated when/if the cluster changes.\n\ts.websocket.connectURLs, err = s.getConnectURLs(o.Advertise, o.Host, o.Port)\n\tif err != nil {\n\t\ts.Fatalf(\"Unable to get websocket connect URLs: %v\", err)\n\t\thl.Close()\n\t\ts.mu.Unlock()\n\t\treturn\n\t}\n\thasLeaf := sopts.LeafNode.Port != 0\n\tmux := http.NewServeMux()\n\tmux.HandleFunc(\"/\", func(w http.ResponseWriter, r *http.Request) {\n\t\tres, err := s.wsUpgrade(w, r)\n\t\tif err != nil {\n\t\t\ts.Errorf(err.Error())\n\t\t\treturn\n\t\t}\n\t\tswitch res.kind {\n\t\tcase CLIENT:\n\t\t\ts.createWSClient(res.conn, res.ws)\n\t\tcase MQTT:\n\t\t\ts.createMQTTClient(res.conn, res.ws)\n\t\tcase LEAF:\n\t\t\tif !hasLeaf {\n\t\t\t\ts.Errorf(\"Not configured to accept leaf node connections\")\n\t\t\t\t// Silently close for now. If we want to send an error back, we would\n\t\t\t\t// need to create the leafnode client anyway, so that is handling websocket\n\t\t\t\t// frames, then send the error to the remote.\n\t\t\t\tres.conn.Close()\n\t\t\t\treturn\n\t\t\t}\n\t\t\ts.createLeafNode(res.conn, nil, nil, res.ws)\n\t\t}\n\t})\n\ths := &http.Server{\n\t\tAddr:        hp,\n\t\tHandler:     mux,\n\t\tReadTimeout: o.HandshakeTimeout,\n\t\tErrorLog:    log.New(&captureHTTPServerLog{s, \"websocket: \"}, _EMPTY_, 0),\n\t}\n\ts.websocket.mu.Lock()\n\ts.websocket.server = hs\n\ts.websocket.listener = hl\n\ts.websocket.mu.Unlock()\n\tgo func() {\n\t\tif err := hs.Serve(hl); err != http.ErrServerClosed {\n\t\t\ts.Fatalf(\"websocket listener error: %v\", err)\n\t\t}\n\t\tif s.isLameDuckMode() {\n\t\t\t// Signal that we are not accepting new clients\n\t\t\ts.ldmCh <- true\n\t\t\t// Now wait for the Shutdown...\n\t\t\t<-s.quitCh\n\t\t\treturn\n\t\t}\n\t\ts.done <- true\n\t}()\n\ts.mu.Unlock()\n}\n\n// The TLS configuration is passed to the listener when the websocket\n// \"server\" is setup. That prevents TLS configuration updates on reload\n// from being used. By setting this function in tls.Config.GetConfigForClient\n// we instruct the TLS handshake to ask for the tls configuration to be\n// used for a specific client. We don't care which client, we always use\n// the same TLS configuration.\nfunc (s *Server) wsGetTLSConfig(_ *tls.ClientHelloInfo) (*tls.Config, error) {\n\topts := s.getOpts()\n\treturn opts.Websocket.TLSConfig, nil\n}\n\n// This is similar to createClient() but has some modifications\n// specific to handle websocket clients.\n// The comments have been kept to minimum to reduce code size.\n// Check createClient() for more details.\nfunc (s *Server) createWSClient(conn net.Conn, ws *websocket) *client {\n\topts := s.getOpts()\n\n\tmaxPay := int32(opts.MaxPayload)\n\tmaxSubs := int32(opts.MaxSubs)\n\tif maxSubs == 0 {\n\t\tmaxSubs = -1\n\t}\n\tnow := time.Now().UTC()\n\n\tc := &client{srv: s, nc: conn, opts: defaultOpts, mpay: maxPay, msubs: maxSubs, start: now, last: now, ws: ws}\n\n\tc.registerWithAccount(s.globalAccount())\n\n\tvar info Info\n\tvar authRequired bool\n\n\ts.mu.Lock()\n\tinfo = s.copyInfo()\n\t// Check auth, override if applicable.\n\tif !info.AuthRequired {\n\t\t// Set info.AuthRequired since this is what is sent to the client.\n\t\tinfo.AuthRequired = s.websocket.authOverride\n\t}\n\tif s.nonceRequired() {\n\t\tvar raw [nonceLen]byte\n\t\tnonce := raw[:]\n\t\ts.generateNonce(nonce)\n\t\tinfo.Nonce = string(nonce)\n\t}\n\tc.nonce = []byte(info.Nonce)\n\tauthRequired = info.AuthRequired\n\n\ts.totalClients++\n\ts.mu.Unlock()\n\n\tc.mu.Lock()\n\tif authRequired {\n\t\tc.flags.set(expectConnect)\n\t}\n\tc.initClient()\n\tc.Debugf(\"Client connection created\")\n\tc.sendProtoNow(c.generateClientInfoJSON(info))\n\tc.mu.Unlock()\n\n\ts.mu.Lock()\n\tif !s.isRunning() || s.ldm {\n\t\tif s.isShuttingDown() {\n\t\t\tconn.Close()\n\t\t}\n\t\ts.mu.Unlock()\n\t\treturn c\n\t}\n\n\tif opts.MaxConn > 0 && len(s.clients) >= opts.MaxConn {\n\t\ts.mu.Unlock()\n\t\tc.maxConnExceeded()\n\t\treturn nil\n\t}\n\ts.clients[c.cid] = c\n\ts.mu.Unlock()\n\n\tc.mu.Lock()\n\t// Websocket clients do TLS in the websocket http server.\n\t// So no TLS initiation here...\n\tif _, ok := conn.(*tls.Conn); ok {\n\t\tc.flags.set(handshakeComplete)\n\t}\n\n\tif c.isClosed() {\n\t\tc.mu.Unlock()\n\t\tc.closeConnection(WriteError)\n\t\treturn nil\n\t}\n\n\tif authRequired {\n\t\ttimeout := opts.AuthTimeout\n\t\t// Possibly override with Websocket specific value.\n\t\tif opts.Websocket.AuthTimeout != 0 {\n\t\t\ttimeout = opts.Websocket.AuthTimeout\n\t\t}\n\t\tc.setAuthTimer(secondsToDuration(timeout))\n\t}\n\n\tc.setPingTimer()\n\n\ts.startGoRoutine(func() { c.readLoop(nil) })\n\ts.startGoRoutine(func() { c.writeLoop() })\n\n\tc.mu.Unlock()\n\n\treturn c\n}\n\nfunc (c *client) wsCollapsePtoNB() (net.Buffers, int64) {\n\tnb := c.out.nb\n\tvar mfs int\n\tvar usz int\n\tif c.ws.browser {\n\t\tmfs = wsFrameSizeForBrowsers\n\t}\n\tmask := c.ws.maskwrite\n\t// Start with possible already framed buffers (that we could have\n\t// got from partials or control messages such as ws pings or pongs).\n\tbufs := c.ws.frames\n\tcompress := c.ws.compress\n\tif compress && len(nb) > 0 {\n\t\t// First, make sure we don't compress for very small cumulative buffers.\n\t\tfor _, b := range nb {\n\t\t\tusz += len(b)\n\t\t}\n\t\tif usz <= wsCompressThreshold {\n\t\t\tcompress = false\n\t\t\tif cp := c.ws.compressor; cp != nil {\n\t\t\t\tcp.Reset(nil)\n\t\t\t}\n\t\t}\n\t}\n\tif compress && len(nb) > 0 {\n\t\t// Overwrite mfs if this connection does not support fragmented compressed frames.\n\t\tif mfs > 0 && c.ws.nocompfrag {\n\t\t\tmfs = 0\n\t\t}\n\t\tbuf := bytes.NewBuffer(nbPoolGet(usz))\n\t\tcp := c.ws.compressor\n\t\tif cp == nil {\n\t\t\tc.ws.compressor, _ = flate.NewWriter(buf, flate.BestSpeed)\n\t\t\tcp = c.ws.compressor\n\t\t} else {\n\t\t\tcp.Reset(buf)\n\t\t}\n\t\tvar csz int\n\t\tfor _, b := range nb {\n\t\t\tfor len(b) > 0 {\n\t\t\t\tn, err := cp.Write(b)\n\t\t\t\tif err != nil {\n\t\t\t\t\t// Whatever this error is, it'll be handled by the cp.Flush()\n\t\t\t\t\t// call below, as the same error will be returned there.\n\t\t\t\t\t// Let the outer loop return all the buffers back to the pool\n\t\t\t\t\t// and fall through naturally.\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t\tb = b[n:]\n\t\t\t}\n\t\t\tnbPoolPut(b) // No longer needed as contents written to compressor.\n\t\t}\n\t\tif err := cp.Flush(); err != nil {\n\t\t\tc.Errorf(\"Error during compression: %v\", err)\n\t\t\tc.markConnAsClosed(WriteError)\n\t\t\tcp.Reset(nil)\n\t\t\treturn nil, 0\n\t\t}\n\t\tb := buf.Bytes()\n\t\tp := b[:len(b)-4]\n\t\tif mfs > 0 && len(p) > mfs {\n\t\t\tfor first, final := true, false; len(p) > 0; first = false {\n\t\t\t\tlp := len(p)\n\t\t\t\tif lp > mfs {\n\t\t\t\t\tlp = mfs\n\t\t\t\t} else {\n\t\t\t\t\tfinal = true\n\t\t\t\t}\n\t\t\t\t// Only the first frame should be marked as compressed, so pass\n\t\t\t\t// `first` for the compressed boolean.\n\t\t\t\tfh := nbPoolGet(wsMaxFrameHeaderSize)[:wsMaxFrameHeaderSize]\n\t\t\t\tn, key := wsFillFrameHeader(fh, mask, first, final, first, wsBinaryMessage, lp)\n\t\t\t\tif mask {\n\t\t\t\t\twsMaskBuf(key, p[:lp])\n\t\t\t\t}\n\t\t\t\tbufs = append(bufs, fh[:n], p[:lp])\n\t\t\t\tcsz += n + lp\n\t\t\t\tp = p[lp:]\n\t\t\t}\n\t\t} else {\n\t\t\tol := len(p)\n\t\t\th, key := wsCreateFrameHeader(mask, true, wsBinaryMessage, ol)\n\t\t\tif mask {\n\t\t\t\twsMaskBuf(key, p)\n\t\t\t}\n\t\t\tif ol > 0 {\n\t\t\t\tbufs = append(bufs, h, p)\n\t\t\t}\n\t\t\tcsz = len(h) + ol\n\t\t}\n\t\t// Make sure that the compressor no longer holds a reference to\n\t\t// the bytes.Buffer, so that the underlying memory gets cleaned\n\t\t// up after flushOutbound/flushAndClose. For this to be safe, we\n\t\t// always cp.Reset(...) before reusing the compressor again.\n\t\tcp.Reset(nil)\n\t\t// Add to pb the compressed data size (including headers), but\n\t\t// remove the original uncompressed data size that was added\n\t\t// during the queueing.\n\t\tc.out.pb += int64(csz) - int64(usz)\n\t\tc.ws.fs += int64(csz)\n\t} else if len(nb) > 0 {\n\t\tvar total int\n\t\tif mfs > 0 {\n\t\t\t// We are limiting the frame size.\n\t\t\tstartFrame := func() int {\n\t\t\t\tbufs = append(bufs, nbPoolGet(wsMaxFrameHeaderSize))\n\t\t\t\treturn len(bufs) - 1\n\t\t\t}\n\t\t\tendFrame := func(idx, size int) {\n\t\t\t\tbufs[idx] = bufs[idx][:wsMaxFrameHeaderSize]\n\t\t\t\tn, key := wsFillFrameHeader(bufs[idx], mask, wsFirstFrame, wsFinalFrame, wsUncompressedFrame, wsBinaryMessage, size)\n\t\t\t\tbufs[idx] = bufs[idx][:n]\n\t\t\t\tc.out.pb += int64(n)\n\t\t\t\tc.ws.fs += int64(n + size)\n\t\t\t\tif mask {\n\t\t\t\t\twsMaskBufs(key, bufs[idx+1:])\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tfhIdx := startFrame()\n\t\t\tfor i := 0; i < len(nb); i++ {\n\t\t\t\tb := nb[i]\n\t\t\t\tif total+len(b) <= mfs {\n\t\t\t\t\tbuf := nbPoolGet(len(b))\n\t\t\t\t\tbufs = append(bufs, append(buf, b...))\n\t\t\t\t\ttotal += len(b)\n\t\t\t\t\tnbPoolPut(nb[i])\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tfor len(b) > 0 {\n\t\t\t\t\tendStart := total != 0\n\t\t\t\t\tif endStart {\n\t\t\t\t\t\tendFrame(fhIdx, total)\n\t\t\t\t\t}\n\t\t\t\t\ttotal = len(b)\n\t\t\t\t\tif total >= mfs {\n\t\t\t\t\t\ttotal = mfs\n\t\t\t\t\t}\n\t\t\t\t\tif endStart {\n\t\t\t\t\t\tfhIdx = startFrame()\n\t\t\t\t\t}\n\t\t\t\t\tbuf := nbPoolGet(total)\n\t\t\t\t\tbufs = append(bufs, append(buf, b[:total]...))\n\t\t\t\t\tb = b[total:]\n\t\t\t\t}\n\t\t\t\tnbPoolPut(nb[i]) // No longer needed as copied into smaller frames.\n\t\t\t}\n\t\t\tif total > 0 {\n\t\t\t\tendFrame(fhIdx, total)\n\t\t\t}\n\t\t} else {\n\t\t\t// If there is no limit on the frame size, create a single frame for\n\t\t\t// all pending buffers.\n\t\t\tfor _, b := range nb {\n\t\t\t\ttotal += len(b)\n\t\t\t}\n\t\t\twsfh, key := wsCreateFrameHeader(mask, false, wsBinaryMessage, total)\n\t\t\tc.out.pb += int64(len(wsfh))\n\t\t\tbufs = append(bufs, wsfh)\n\t\t\tidx := len(bufs)\n\t\t\tbufs = append(bufs, nb...)\n\t\t\tif mask {\n\t\t\t\twsMaskBufs(key, bufs[idx:])\n\t\t\t}\n\t\t\tc.ws.fs += int64(len(wsfh) + total)\n\t\t}\n\t}\n\tif len(c.ws.closeMsg) > 0 {\n\t\tbufs = append(bufs, c.ws.closeMsg)\n\t\tc.ws.fs += int64(len(c.ws.closeMsg))\n\t\tc.ws.closeMsg = nil\n\t\tc.ws.compressor = nil\n\t}\n\tc.ws.frames = nil\n\treturn bufs, c.ws.fs\n}\n\nfunc isWSURL(u *url.URL) bool {\n\treturn strings.HasPrefix(strings.ToLower(u.Scheme), wsSchemePrefix)\n}\n\nfunc isWSSURL(u *url.URL) bool {\n\treturn strings.HasPrefix(strings.ToLower(u.Scheme), wsSchemePrefixTLS)\n}\n",
    "source_file": "server/websocket.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2019-2025 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage server\n\nimport (\n\tcrand \"crypto/rand\"\n\t\"encoding/binary\"\n\t\"fmt\"\n\t\"math\"\n\t\"slices\"\n\t\"sort\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/nats-io/nats-server/v2/server/avl\"\n\t\"github.com/nats-io/nats-server/v2/server/stree\"\n\t\"github.com/nats-io/nats-server/v2/server/thw\"\n)\n\n// TODO(dlc) - This is a fairly simplistic approach but should do for now.\ntype memStore struct {\n\tmu          sync.RWMutex\n\tcfg         StreamConfig\n\tstate       StreamState\n\tmsgs        map[uint64]*StoreMsg\n\tfss         *stree.SubjectTree[SimpleState]\n\tdmap        avl.SequenceSet\n\tmaxp        int64\n\tscb         StorageUpdateHandler\n\trmcb        StorageRemoveMsgHandler\n\tsdmcb       SubjectDeleteMarkerUpdateHandler\n\tageChk      *time.Timer\n\tconsumers   int\n\treceivedAny bool\n\tttls        *thw.HashWheel\n\tsdm         *SDMMeta\n}\n\nfunc newMemStore(cfg *StreamConfig) (*memStore, error) {\n\tif cfg == nil {\n\t\treturn nil, fmt.Errorf(\"config required\")\n\t}\n\tif cfg.Storage != MemoryStorage {\n\t\treturn nil, fmt.Errorf(\"memStore requires memory storage type in config\")\n\t}\n\tms := &memStore{\n\t\tmsgs: make(map[uint64]*StoreMsg),\n\t\tfss:  stree.NewSubjectTree[SimpleState](),\n\t\tmaxp: cfg.MaxMsgsPer,\n\t\tcfg:  *cfg,\n\t}\n\t// Only create a THW if we're going to allow TTLs.\n\tif cfg.AllowMsgTTL {\n\t\tms.ttls = thw.NewHashWheel()\n\t}\n\tif cfg.FirstSeq > 0 {\n\t\tif _, err := ms.purge(cfg.FirstSeq); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\treturn ms, nil\n}\n\nfunc (ms *memStore) UpdateConfig(cfg *StreamConfig) error {\n\tif cfg == nil {\n\t\treturn fmt.Errorf(\"config required\")\n\t}\n\tif cfg.Storage != MemoryStorage {\n\t\treturn fmt.Errorf(\"memStore requires memory storage type in config\")\n\t}\n\n\tms.mu.Lock()\n\tms.cfg = *cfg\n\t// Create or delete the THW if needed.\n\tif cfg.AllowMsgTTL && ms.ttls == nil {\n\t\tms.ttls = thw.NewHashWheel()\n\t} else if !cfg.AllowMsgTTL && ms.ttls != nil {\n\t\tms.ttls = nil\n\t}\n\t// Limits checks and enforcement.\n\tms.enforceMsgLimit()\n\tms.enforceBytesLimit()\n\t// Do age timers.\n\tif ms.ageChk == nil && ms.cfg.MaxAge != 0 {\n\t\tms.startAgeChk()\n\t}\n\tif ms.ageChk != nil && ms.cfg.MaxAge == 0 {\n\t\tms.ageChk.Stop()\n\t\tms.ageChk = nil\n\t}\n\t// Make sure to update MaxMsgsPer\n\tif cfg.MaxMsgsPer < -1 {\n\t\tcfg.MaxMsgsPer = -1\n\t}\n\tmaxp := ms.maxp\n\tms.maxp = cfg.MaxMsgsPer\n\t// If the value is smaller, or was unset before, we need to enforce that.\n\tif ms.maxp > 0 && (maxp == 0 || ms.maxp < maxp) {\n\t\tlm := uint64(ms.maxp)\n\t\tms.fss.IterFast(func(subj []byte, ss *SimpleState) bool {\n\t\t\tif ss.Msgs > lm {\n\t\t\t\tms.enforcePerSubjectLimit(bytesToString(subj), ss)\n\t\t\t}\n\t\t\treturn true\n\t\t})\n\t}\n\tms.mu.Unlock()\n\n\tif cfg.MaxAge != 0 || cfg.AllowMsgTTL {\n\t\tms.expireMsgs()\n\t}\n\treturn nil\n}\n\n// Stores a raw message with expected sequence number and timestamp.\n// Lock should be held.\nfunc (ms *memStore) storeRawMsg(subj string, hdr, msg []byte, seq uint64, ts, ttl int64) error {\n\tif ms.msgs == nil {\n\t\treturn ErrStoreClosed\n\t}\n\n\t// Tracking by subject.\n\tvar ss *SimpleState\n\tvar asl bool\n\tif len(subj) > 0 {\n\t\tvar ok bool\n\t\tif ss, ok = ms.fss.Find(stringToBytes(subj)); ok {\n\t\t\tasl = ms.maxp > 0 && ss.Msgs >= uint64(ms.maxp)\n\t\t}\n\t}\n\n\t// Check if we are discarding new messages when we reach the limit.\n\tif ms.cfg.Discard == DiscardNew {\n\t\tif asl && ms.cfg.DiscardNewPer {\n\t\t\treturn ErrMaxMsgsPerSubject\n\t\t}\n\t\t// If we are discard new and limits policy and clustered, we do the enforcement\n\t\t// above and should not disqualify the message here since it could cause replicas to drift.\n\t\tif ms.cfg.Retention == LimitsPolicy || ms.cfg.Replicas == 1 {\n\t\t\tif ms.cfg.MaxMsgs > 0 && ms.state.Msgs >= uint64(ms.cfg.MaxMsgs) {\n\t\t\t\t// If we are tracking max messages per subject and are at the limit we will replace, so this is ok.\n\t\t\t\tif !asl {\n\t\t\t\t\treturn ErrMaxMsgs\n\t\t\t\t}\n\t\t\t}\n\t\t\tif ms.cfg.MaxBytes > 0 && ms.state.Bytes+memStoreMsgSize(subj, hdr, msg) >= uint64(ms.cfg.MaxBytes) {\n\t\t\t\tif !asl {\n\t\t\t\t\treturn ErrMaxBytes\n\t\t\t\t}\n\t\t\t\t// If we are here we are at a subject maximum, need to determine if dropping last message gives us enough room.\n\t\t\t\tif ss.firstNeedsUpdate || ss.lastNeedsUpdate {\n\t\t\t\t\tms.recalculateForSubj(subj, ss)\n\t\t\t\t}\n\t\t\t\tsm, ok := ms.msgs[ss.First]\n\t\t\t\tif !ok || memStoreMsgSize(sm.subj, sm.hdr, sm.msg) < memStoreMsgSize(subj, hdr, msg) {\n\t\t\t\t\treturn ErrMaxBytes\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tif seq != ms.state.LastSeq+1 {\n\t\tif seq > 0 {\n\t\t\treturn ErrSequenceMismatch\n\t\t}\n\t\tseq = ms.state.LastSeq + 1\n\t}\n\n\t// Adjust first if needed.\n\tnow := time.Unix(0, ts).UTC()\n\tif ms.state.Msgs == 0 {\n\t\tms.state.FirstSeq = seq\n\t\tms.state.FirstTime = now\n\t}\n\n\t// Make copies\n\t// TODO(dlc) - Maybe be smarter here.\n\tif len(msg) > 0 {\n\t\tmsg = copyBytes(msg)\n\t}\n\tif len(hdr) > 0 {\n\t\thdr = copyBytes(hdr)\n\t}\n\n\t// FIXME(dlc) - Could pool at this level?\n\tsm := &StoreMsg{subj, nil, nil, make([]byte, 0, len(hdr)+len(msg)), seq, ts}\n\tsm.buf = append(sm.buf, hdr...)\n\tsm.buf = append(sm.buf, msg...)\n\tif len(hdr) > 0 {\n\t\tsm.hdr = sm.buf[:len(hdr)]\n\t}\n\tsm.msg = sm.buf[len(hdr):]\n\tms.msgs[seq] = sm\n\tms.state.Msgs++\n\tms.state.Bytes += memStoreMsgSize(subj, hdr, msg)\n\tms.state.LastSeq = seq\n\tms.state.LastTime = now\n\n\t// Track per subject.\n\tif len(subj) > 0 {\n\t\tif ss != nil {\n\t\t\tss.Msgs++\n\t\t\tss.Last = seq\n\t\t\tss.lastNeedsUpdate = false\n\t\t\t// Check per subject limits.\n\t\t\tif ms.maxp > 0 && ss.Msgs > uint64(ms.maxp) {\n\t\t\t\tms.enforcePerSubjectLimit(subj, ss)\n\t\t\t}\n\t\t} else {\n\t\t\tms.fss.Insert([]byte(subj), SimpleState{Msgs: 1, First: seq, Last: seq})\n\t\t}\n\t}\n\n\t// Limits checks and enforcement.\n\tms.enforceMsgLimit()\n\tms.enforceBytesLimit()\n\n\t// Per-message TTL.\n\tif ms.ttls != nil && ttl > 0 {\n\t\texpires := time.Duration(ts) + (time.Second * time.Duration(ttl))\n\t\tms.ttls.Add(seq, int64(expires))\n\t}\n\n\t// Check if we have and need the age expiration timer running.\n\tswitch {\n\tcase ms.ttls != nil && ttl > 0:\n\t\tms.resetAgeChk(0)\n\tcase ms.ageChk == nil && (ms.cfg.MaxAge > 0 || ms.ttls != nil):\n\t\tms.startAgeChk()\n\t}\n\n\treturn nil\n}\n\n// StoreRawMsg stores a raw message with expected sequence number and timestamp.\nfunc (ms *memStore) StoreRawMsg(subj string, hdr, msg []byte, seq uint64, ts, ttl int64) error {\n\tms.mu.Lock()\n\terr := ms.storeRawMsg(subj, hdr, msg, seq, ts, ttl)\n\tcb := ms.scb\n\t// Check if first message timestamp requires expiry\n\t// sooner than initial replica expiry timer set to MaxAge when initializing.\n\tif !ms.receivedAny && ms.cfg.MaxAge != 0 && ts > 0 {\n\t\tms.receivedAny = true\n\t\t// Calculate duration when the next expireMsgs should be called.\n\t\tms.resetAgeChk(int64(time.Millisecond) * 50)\n\t}\n\tms.mu.Unlock()\n\n\tif err == nil && cb != nil {\n\t\tcb(1, int64(memStoreMsgSize(subj, hdr, msg)), seq, subj)\n\t}\n\n\treturn err\n}\n\n// Store stores a message.\nfunc (ms *memStore) StoreMsg(subj string, hdr, msg []byte, ttl int64) (uint64, int64, error) {\n\tms.mu.Lock()\n\tseq, ts := ms.state.LastSeq+1, time.Now().UnixNano()\n\terr := ms.storeRawMsg(subj, hdr, msg, seq, ts, ttl)\n\tcb := ms.scb\n\tms.mu.Unlock()\n\n\tif err != nil {\n\t\tseq, ts = 0, 0\n\t} else if cb != nil {\n\t\tcb(1, int64(memStoreMsgSize(subj, hdr, msg)), seq, subj)\n\t}\n\n\treturn seq, ts, err\n}\n\n// SkipMsg will use the next sequence number but not store anything.\nfunc (ms *memStore) SkipMsg() uint64 {\n\t// Grab time.\n\tnow := time.Now().UTC()\n\n\tms.mu.Lock()\n\tseq := ms.state.LastSeq + 1\n\tms.state.LastSeq = seq\n\tms.state.LastTime = now\n\tif ms.state.Msgs == 0 {\n\t\tms.state.FirstSeq = seq + 1\n\t\tms.state.FirstTime = now\n\t} else {\n\t\tms.dmap.Insert(seq)\n\t}\n\tms.mu.Unlock()\n\treturn seq\n}\n\n// Skip multiple msgs.\nfunc (ms *memStore) SkipMsgs(seq uint64, num uint64) error {\n\t// Grab time.\n\tnow := time.Now().UTC()\n\n\tms.mu.Lock()\n\tdefer ms.mu.Unlock()\n\n\t// Check sequence matches our last sequence.\n\tif seq != ms.state.LastSeq+1 {\n\t\tif seq > 0 {\n\t\t\treturn ErrSequenceMismatch\n\t\t}\n\t\tseq = ms.state.LastSeq + 1\n\t}\n\tlseq := seq + num - 1\n\n\tms.state.LastSeq = lseq\n\tms.state.LastTime = now\n\tif ms.state.Msgs == 0 {\n\t\tms.state.FirstSeq, ms.state.FirstTime = lseq+1, now\n\t} else {\n\t\tfor ; seq <= lseq; seq++ {\n\t\t\tms.dmap.Insert(seq)\n\t\t}\n\t}\n\treturn nil\n}\n\n// RegisterStorageUpdates registers a callback for updates to storage changes.\n// It will present number of messages and bytes as a signed integer and an\n// optional sequence number of the message if a single.\nfunc (ms *memStore) RegisterStorageUpdates(cb StorageUpdateHandler) {\n\tms.mu.Lock()\n\tms.scb = cb\n\tms.mu.Unlock()\n}\n\n// RegisterStorageRemoveMsg registers a callback to remove messages.\n// Replicated streams should propose removals, R1 can remove inline.\nfunc (ms *memStore) RegisterStorageRemoveMsg(cb StorageRemoveMsgHandler) {\n\tms.mu.Lock()\n\tms.rmcb = cb\n\tms.mu.Unlock()\n}\n\n// RegisterSubjectDeleteMarkerUpdates registers a callback for updates to new subject delete markers.\nfunc (ms *memStore) RegisterSubjectDeleteMarkerUpdates(cb SubjectDeleteMarkerUpdateHandler) {\n\tms.mu.Lock()\n\tms.sdmcb = cb\n\tms.mu.Unlock()\n}\n\n// GetSeqFromTime looks for the first sequence number that has the message\n// with >= timestamp.\n// FIXME(dlc) - inefficient.\nfunc (ms *memStore) GetSeqFromTime(t time.Time) uint64 {\n\tts := t.UnixNano()\n\tms.mu.RLock()\n\tdefer ms.mu.RUnlock()\n\tif len(ms.msgs) == 0 {\n\t\treturn ms.state.LastSeq + 1\n\t}\n\tif ts <= ms.msgs[ms.state.FirstSeq].ts {\n\t\treturn ms.state.FirstSeq\n\t}\n\t// LastSeq is not guaranteed to be present since last does not go backwards.\n\tvar lmsg *StoreMsg\n\tfor lseq := ms.state.LastSeq; lseq > ms.state.FirstSeq; lseq-- {\n\t\tif lmsg = ms.msgs[lseq]; lmsg != nil {\n\t\t\tbreak\n\t\t}\n\t}\n\tif lmsg == nil {\n\t\treturn ms.state.LastSeq + 1\n\t}\n\n\tlast := lmsg.ts\n\tif ts == last {\n\t\treturn ms.state.LastSeq\n\t}\n\tif ts > last {\n\t\treturn ms.state.LastSeq + 1\n\t}\n\tindex := sort.Search(len(ms.msgs), func(i int) bool {\n\t\tif msg := ms.msgs[ms.state.FirstSeq+uint64(i)]; msg != nil {\n\t\t\treturn msg.ts >= ts\n\t\t}\n\t\treturn false\n\t})\n\treturn uint64(index) + ms.state.FirstSeq\n}\n\n// FilteredState will return the SimpleState associated with the filtered subject and a proposed starting sequence.\nfunc (ms *memStore) FilteredState(sseq uint64, subj string) SimpleState {\n\t// This needs to be a write lock, as filteredStateLocked can\n\t// mutate the per-subject state.\n\tms.mu.Lock()\n\tdefer ms.mu.Unlock()\n\n\treturn ms.filteredStateLocked(sseq, subj, false)\n}\n\nfunc (ms *memStore) filteredStateLocked(sseq uint64, filter string, lastPerSubject bool) SimpleState {\n\tif sseq < ms.state.FirstSeq {\n\t\tsseq = ms.state.FirstSeq\n\t}\n\n\t// If past the end no results.\n\tif sseq > ms.state.LastSeq {\n\t\treturn SimpleState{}\n\t}\n\n\tif filter == _EMPTY_ {\n\t\tfilter = fwcs\n\t}\n\tisAll := filter == fwcs\n\n\t// First check if we can optimize this part.\n\t// This means we want all and the starting sequence was before this block.\n\tif isAll && sseq <= ms.state.FirstSeq {\n\t\ttotal := ms.state.Msgs\n\t\tif lastPerSubject {\n\t\t\ttotal = uint64(ms.fss.Size())\n\t\t}\n\t\treturn SimpleState{\n\t\t\tMsgs:  total,\n\t\t\tFirst: ms.state.FirstSeq,\n\t\t\tLast:  ms.state.LastSeq,\n\t\t}\n\t}\n\n\t_tsa, _fsa := [32]string{}, [32]string{}\n\ttsa, fsa := _tsa[:0], _fsa[:0]\n\twc := subjectHasWildcard(filter)\n\tif wc {\n\t\tfsa = tokenizeSubjectIntoSlice(fsa[:0], filter)\n\t}\n\t// 1. See if we match any subs from fss.\n\t// 2. If we match and the sseq is past ss.Last then we can use meta only.\n\t// 3. If we match we need to do a partial, break and clear any totals and do a full scan like num pending.\n\n\tisMatch := func(subj string) bool {\n\t\tif isAll {\n\t\t\treturn true\n\t\t}\n\t\tif !wc {\n\t\t\treturn subj == filter\n\t\t}\n\t\ttsa = tokenizeSubjectIntoSlice(tsa[:0], subj)\n\t\treturn isSubsetMatchTokenized(tsa, fsa)\n\t}\n\n\tvar ss SimpleState\n\tupdate := func(fss *SimpleState) {\n\t\tmsgs, first, last := fss.Msgs, fss.First, fss.Last\n\t\tif lastPerSubject {\n\t\t\tmsgs, first = 1, last\n\t\t}\n\t\tss.Msgs += msgs\n\t\tif ss.First == 0 || first < ss.First {\n\t\t\tss.First = first\n\t\t}\n\t\tif last > ss.Last {\n\t\t\tss.Last = last\n\t\t}\n\t}\n\n\tvar havePartial bool\n\tvar totalSkipped uint64\n\t// We will track start and end sequences as we go.\n\tms.fss.Match(stringToBytes(filter), func(subj []byte, fss *SimpleState) {\n\t\tif fss.firstNeedsUpdate || fss.lastNeedsUpdate {\n\t\t\tms.recalculateForSubj(bytesToString(subj), fss)\n\t\t}\n\t\tif sseq <= fss.First {\n\t\t\tupdate(fss)\n\t\t} else if sseq <= fss.Last {\n\t\t\t// We matched but it is a partial.\n\t\t\thavePartial = true\n\t\t\t// Don't break here, we will update to keep tracking last.\n\t\t\tupdate(fss)\n\t\t} else {\n\t\t\ttotalSkipped += fss.Msgs\n\t\t}\n\t})\n\n\t// If we did not encounter any partials we can return here.\n\tif !havePartial {\n\t\treturn ss\n\t}\n\n\t// If we are here we need to scan the msgs.\n\t// Capture first and last sequences for scan and then clear what we had.\n\tfirst, last := ss.First, ss.Last\n\t// To track if we decide to exclude we need to calculate first.\n\tvar needScanFirst bool\n\tif first < sseq {\n\t\tfirst = sseq\n\t\tneedScanFirst = true\n\t}\n\n\t// Now we want to check if it is better to scan inclusive and recalculate that way\n\t// or leave and scan exclusive and adjust our totals.\n\t// ss.Last is always correct here.\n\ttoScan, toExclude := last-first, first-ms.state.FirstSeq+ms.state.LastSeq-ss.Last\n\tvar seen map[string]bool\n\tif lastPerSubject {\n\t\tseen = make(map[string]bool)\n\t}\n\tif toScan < toExclude {\n\t\tss.Msgs, ss.First = 0, 0\n\n\t\tupdate := func(sm *StoreMsg) {\n\t\t\tss.Msgs++\n\t\t\tif ss.First == 0 {\n\t\t\t\tss.First = sm.seq\n\t\t\t}\n\t\t\tif seen != nil {\n\t\t\t\tseen[sm.subj] = true\n\t\t\t}\n\t\t}\n\t\t// Check if easier to just scan msgs vs the sequence range.\n\t\t// This can happen with lots of interior deletes.\n\t\tif last-first > uint64(len(ms.msgs)) {\n\t\t\tfor _, sm := range ms.msgs {\n\t\t\t\tif sm.seq >= first && sm.seq <= last && !seen[sm.subj] && isMatch(sm.subj) {\n\t\t\t\t\tupdate(sm)\n\t\t\t\t}\n\t\t\t}\n\t\t} else {\n\t\t\tfor seq := first; seq <= last; seq++ {\n\t\t\t\tif sm, ok := ms.msgs[seq]; ok && !seen[sm.subj] && isMatch(sm.subj) {\n\t\t\t\t\tupdate(sm)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t} else {\n\t\t// We will adjust from the totals above by scanning what we need to exclude.\n\t\tss.First = first\n\t\tss.Msgs += totalSkipped\n\t\tvar adjust uint64\n\t\tvar tss *SimpleState\n\n\t\tupdate := func(sm *StoreMsg) {\n\t\t\tif lastPerSubject {\n\t\t\t\ttss, _ = ms.fss.Find(stringToBytes(sm.subj))\n\t\t\t}\n\t\t\t// If we are last per subject, make sure to only adjust if all messages are before our first.\n\t\t\tif tss == nil || tss.Last < first {\n\t\t\t\tadjust++\n\t\t\t}\n\t\t\tif seen != nil {\n\t\t\t\tseen[sm.subj] = true\n\t\t\t}\n\t\t}\n\t\t// Check if easier to just scan msgs vs the sequence range.\n\t\tif first-ms.state.FirstSeq > uint64(len(ms.msgs)) {\n\t\t\tfor _, sm := range ms.msgs {\n\t\t\t\tif sm.seq < first && !seen[sm.subj] && isMatch(sm.subj) {\n\t\t\t\t\tupdate(sm)\n\t\t\t\t}\n\t\t\t}\n\t\t} else {\n\t\t\tfor seq := ms.state.FirstSeq; seq < first; seq++ {\n\t\t\t\tif sm, ok := ms.msgs[seq]; ok && !seen[sm.subj] && isMatch(sm.subj) {\n\t\t\t\t\tupdate(sm)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t// Now do range at end.\n\t\tfor seq := last + 1; seq < ms.state.LastSeq; seq++ {\n\t\t\tif sm, ok := ms.msgs[seq]; ok && !seen[sm.subj] && isMatch(sm.subj) {\n\t\t\t\tadjust++\n\t\t\t\tif seen != nil {\n\t\t\t\t\tseen[sm.subj] = true\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tss.Msgs -= adjust\n\t\tif needScanFirst {\n\t\t\t// Check if easier to just scan msgs vs the sequence range.\n\t\t\t// Since we will need to scan all of the msgs vs below where we break on the first match,\n\t\t\t// we will only do so if a few orders of magnitude lower.\n\t\t\tif last-first > 100*uint64(len(ms.msgs)) {\n\t\t\t\tlow := ms.state.LastSeq\n\t\t\t\tfor _, sm := range ms.msgs {\n\t\t\t\t\tif sm.seq >= first && sm.seq < last && isMatch(sm.subj) {\n\t\t\t\t\t\tif sm.seq < low {\n\t\t\t\t\t\t\tlow = sm.seq\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif low < ms.state.LastSeq {\n\t\t\t\t\tss.First = low\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tfor seq := first; seq < last; seq++ {\n\t\t\t\t\tif sm, ok := ms.msgs[seq]; ok && isMatch(sm.subj) {\n\t\t\t\t\t\tss.First = seq\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\treturn ss\n}\n\n// SubjectsState returns a map of SimpleState for all matching subjects.\nfunc (ms *memStore) SubjectsState(subject string) map[string]SimpleState {\n\t// This needs to be a write lock, as we can mutate the per-subject state.\n\tms.mu.Lock()\n\tdefer ms.mu.Unlock()\n\n\tif ms.fss.Size() == 0 {\n\t\treturn nil\n\t}\n\n\tif subject == _EMPTY_ {\n\t\tsubject = fwcs\n\t}\n\n\tfss := make(map[string]SimpleState)\n\tms.fss.Match(stringToBytes(subject), func(subj []byte, ss *SimpleState) {\n\t\tsubjs := string(subj)\n\t\tif ss.firstNeedsUpdate || ss.lastNeedsUpdate {\n\t\t\tms.recalculateForSubj(subjs, ss)\n\t\t}\n\t\toss := fss[subjs]\n\t\tif oss.First == 0 { // New\n\t\t\tfss[subjs] = *ss\n\t\t} else {\n\t\t\t// Merge here.\n\t\t\toss.Last, oss.Msgs = ss.Last, oss.Msgs+ss.Msgs\n\t\t\tfss[subjs] = oss\n\t\t}\n\t})\n\treturn fss\n}\n\n// AllLastSeqs will return a sorted list of last sequences for all subjects.\nfunc (ms *memStore) AllLastSeqs() ([]uint64, error) {\n\tms.mu.RLock()\n\tdefer ms.mu.RUnlock()\n\treturn ms.allLastSeqsLocked()\n}\n\n// allLastSeqsLocked will return a sorted list of last sequences for all\n// subjects, but won't take the lock to do it, to avoid the issue of compounding\n// read locks causing a deadlock with a write lock.\nfunc (ms *memStore) allLastSeqsLocked() ([]uint64, error) {\n\tif len(ms.msgs) == 0 {\n\t\treturn nil, nil\n\t}\n\n\tseqs := make([]uint64, 0, ms.fss.Size())\n\tms.fss.IterFast(func(subj []byte, ss *SimpleState) bool {\n\t\tseqs = append(seqs, ss.Last)\n\t\treturn true\n\t})\n\n\tslices.Sort(seqs)\n\treturn seqs, nil\n}\n\n// Helper to determine if the filter(s) represent all the subjects.\n// Most clients send in subjects even if they match the stream's ingest subjects.\n// Lock should be held.\nfunc (ms *memStore) filterIsAll(filters []string) bool {\n\tif len(filters) != len(ms.cfg.Subjects) {\n\t\treturn false\n\t}\n\t// Sort so we can compare.\n\tslices.Sort(filters)\n\tfor i, subj := range filters {\n\t\tif !subjectIsSubsetMatch(ms.cfg.Subjects[i], subj) {\n\t\t\treturn false\n\t\t}\n\t}\n\treturn true\n}\n\n// MultiLastSeqs will return a sorted list of sequences that match all subjects presented in filters.\n// We will not exceed the maxSeq, which if 0 becomes the store's last sequence.\nfunc (ms *memStore) MultiLastSeqs(filters []string, maxSeq uint64, maxAllowed int) ([]uint64, error) {\n\tms.mu.RLock()\n\tdefer ms.mu.RUnlock()\n\n\tif len(ms.msgs) == 0 {\n\t\treturn nil, nil\n\t}\n\n\t// See if we can short circuit if we think they are asking for all last sequences and have no maxSeq or maxAllowed set.\n\tif maxSeq == 0 && maxAllowed <= 0 && ms.filterIsAll(filters) {\n\t\treturn ms.allLastSeqsLocked()\n\t}\n\n\t// Implied last sequence.\n\tif maxSeq == 0 {\n\t\tmaxSeq = ms.state.LastSeq\n\t}\n\n\tseqs := make([]uint64, 0, 64)\n\tseen := make(map[uint64]struct{})\n\n\taddIfNotDupe := func(seq uint64) {\n\t\tif _, ok := seen[seq]; !ok {\n\t\t\tseqs = append(seqs, seq)\n\t\t\tseen[seq] = struct{}{}\n\t\t}\n\t}\n\n\tfor _, filter := range filters {\n\t\tms.fss.Match(stringToBytes(filter), func(subj []byte, ss *SimpleState) {\n\t\t\tif ss.Last <= maxSeq {\n\t\t\t\taddIfNotDupe(ss.Last)\n\t\t\t} else if ss.Msgs > 1 {\n\t\t\t\t// The last is greater than maxSeq.\n\t\t\t\ts := bytesToString(subj)\n\t\t\t\tfor seq := maxSeq; seq > 0; seq-- {\n\t\t\t\t\tif sm, ok := ms.msgs[seq]; ok && sm.subj == s {\n\t\t\t\t\t\taddIfNotDupe(seq)\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t})\n\t\t// If maxAllowed was sepcified check that we will not exceed that.\n\t\tif maxAllowed > 0 && len(seqs) > maxAllowed {\n\t\t\treturn nil, ErrTooManyResults\n\t\t}\n\t}\n\tslices.Sort(seqs)\n\treturn seqs, nil\n}\n\n// SubjectsTotals return message totals per subject.\nfunc (ms *memStore) SubjectsTotals(filterSubject string) map[string]uint64 {\n\tms.mu.RLock()\n\tdefer ms.mu.RUnlock()\n\treturn ms.subjectsTotalsLocked(filterSubject)\n}\n\n// Lock should be held.\nfunc (ms *memStore) subjectsTotalsLocked(filterSubject string) map[string]uint64 {\n\tif ms.fss.Size() == 0 {\n\t\treturn nil\n\t}\n\n\t_tsa, _fsa := [32]string{}, [32]string{}\n\ttsa, fsa := _tsa[:0], _fsa[:0]\n\tfsa = tokenizeSubjectIntoSlice(fsa[:0], filterSubject)\n\tisAll := filterSubject == _EMPTY_ || filterSubject == fwcs\n\n\tfst := make(map[string]uint64)\n\tms.fss.Match(stringToBytes(filterSubject), func(subj []byte, ss *SimpleState) {\n\t\tsubjs := string(subj)\n\t\tif isAll {\n\t\t\tfst[subjs] = ss.Msgs\n\t\t} else {\n\t\t\tif tsa = tokenizeSubjectIntoSlice(tsa[:0], subjs); isSubsetMatchTokenized(tsa, fsa) {\n\t\t\t\tfst[subjs] = ss.Msgs\n\t\t\t}\n\t\t}\n\t})\n\treturn fst\n}\n\n// NumPending will return the number of pending messages matching the filter subject starting at sequence.\nfunc (ms *memStore) NumPending(sseq uint64, filter string, lastPerSubject bool) (total, validThrough uint64) {\n\t// This needs to be a write lock, as filteredStateLocked can mutate the per-subject state.\n\tms.mu.Lock()\n\tdefer ms.mu.Unlock()\n\n\tss := ms.filteredStateLocked(sseq, filter, lastPerSubject)\n\treturn ss.Msgs, ms.state.LastSeq\n}\n\n// NumPending will return the number of pending messages matching any subject in the sublist starting at sequence.\nfunc (ms *memStore) NumPendingMulti(sseq uint64, sl *Sublist, lastPerSubject bool) (total, validThrough uint64) {\n\tif sl == nil {\n\t\treturn ms.NumPending(sseq, fwcs, lastPerSubject)\n\t}\n\n\t// This needs to be a write lock, as we can mutate the per-subject state.\n\tms.mu.Lock()\n\tdefer ms.mu.Unlock()\n\n\tvar ss SimpleState\n\tif sseq < ms.state.FirstSeq {\n\t\tsseq = ms.state.FirstSeq\n\t}\n\t// If past the end no results.\n\tif sseq > ms.state.LastSeq {\n\t\treturn 0, ms.state.LastSeq\n\t}\n\n\tupdate := func(fss *SimpleState) {\n\t\tmsgs, first, last := fss.Msgs, fss.First, fss.Last\n\t\tif lastPerSubject {\n\t\t\tmsgs, first = 1, last\n\t\t}\n\t\tss.Msgs += msgs\n\t\tif ss.First == 0 || first < ss.First {\n\t\t\tss.First = first\n\t\t}\n\t\tif last > ss.Last {\n\t\t\tss.Last = last\n\t\t}\n\t}\n\n\tvar havePartial bool\n\tvar totalSkipped uint64\n\t// We will track start and end sequences as we go.\n\tIntersectStree[SimpleState](ms.fss, sl, func(subj []byte, fss *SimpleState) {\n\t\tif fss.firstNeedsUpdate || fss.lastNeedsUpdate {\n\t\t\tms.recalculateForSubj(bytesToString(subj), fss)\n\t\t}\n\t\tif sseq <= fss.First {\n\t\t\tupdate(fss)\n\t\t} else if sseq <= fss.Last {\n\t\t\t// We matched but it is a partial.\n\t\t\thavePartial = true\n\t\t\t// Don't break here, we will update to keep tracking last.\n\t\t\tupdate(fss)\n\t\t} else {\n\t\t\ttotalSkipped += fss.Msgs\n\t\t}\n\t})\n\n\t// If we did not encounter any partials we can return here.\n\tif !havePartial {\n\t\treturn ss.Msgs, ms.state.LastSeq\n\t}\n\n\t// If we are here we need to scan the msgs.\n\t// Capture first and last sequences for scan and then clear what we had.\n\tfirst, last := ss.First, ss.Last\n\t// To track if we decide to exclude we need to calculate first.\n\tif first < sseq {\n\t\tfirst = sseq\n\t}\n\n\t// Now we want to check if it is better to scan inclusive and recalculate that way\n\t// or leave and scan exclusive and adjust our totals.\n\t// ss.Last is always correct here.\n\ttoScan, toExclude := last-first, first-ms.state.FirstSeq+ms.state.LastSeq-ss.Last\n\tvar seen map[string]bool\n\tif lastPerSubject {\n\t\tseen = make(map[string]bool)\n\t}\n\tif toScan < toExclude {\n\t\tss.Msgs, ss.First = 0, 0\n\n\t\tupdate := func(sm *StoreMsg) {\n\t\t\tss.Msgs++\n\t\t\tif ss.First == 0 {\n\t\t\t\tss.First = sm.seq\n\t\t\t}\n\t\t\tif seen != nil {\n\t\t\t\tseen[sm.subj] = true\n\t\t\t}\n\t\t}\n\t\t// Check if easier to just scan msgs vs the sequence range.\n\t\t// This can happen with lots of interior deletes.\n\t\tif last-first > uint64(len(ms.msgs)) {\n\t\t\tfor _, sm := range ms.msgs {\n\t\t\t\tif sm.seq >= first && sm.seq <= last && !seen[sm.subj] && sl.HasInterest(sm.subj) {\n\t\t\t\t\tupdate(sm)\n\t\t\t\t}\n\t\t\t}\n\t\t} else {\n\t\t\tfor seq := first; seq <= last; seq++ {\n\t\t\t\tif sm, ok := ms.msgs[seq]; ok && !seen[sm.subj] && sl.HasInterest(sm.subj) {\n\t\t\t\t\tupdate(sm)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t} else {\n\t\t// We will adjust from the totals above by scanning what we need to exclude.\n\t\tss.First = first\n\t\tss.Msgs += totalSkipped\n\t\tvar adjust uint64\n\t\tvar tss *SimpleState\n\n\t\tupdate := func(sm *StoreMsg) {\n\t\t\tif lastPerSubject {\n\t\t\t\ttss, _ = ms.fss.Find(stringToBytes(sm.subj))\n\t\t\t}\n\t\t\t// If we are last per subject, make sure to only adjust if all messages are before our first.\n\t\t\tif tss == nil || tss.Last < first {\n\t\t\t\tadjust++\n\t\t\t}\n\t\t\tif seen != nil {\n\t\t\t\tseen[sm.subj] = true\n\t\t\t}\n\t\t}\n\t\t// Check if easier to just scan msgs vs the sequence range.\n\t\tif first-ms.state.FirstSeq > uint64(len(ms.msgs)) {\n\t\t\tfor _, sm := range ms.msgs {\n\t\t\t\tif sm.seq < first && !seen[sm.subj] && sl.HasInterest(sm.subj) {\n\t\t\t\t\tupdate(sm)\n\t\t\t\t}\n\t\t\t}\n\t\t} else {\n\t\t\tfor seq := ms.state.FirstSeq; seq < first; seq++ {\n\t\t\t\tif sm, ok := ms.msgs[seq]; ok && !seen[sm.subj] && sl.HasInterest(sm.subj) {\n\t\t\t\t\tupdate(sm)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t// Now do range at end.\n\t\tfor seq := last + 1; seq < ms.state.LastSeq; seq++ {\n\t\t\tif sm, ok := ms.msgs[seq]; ok && !seen[sm.subj] && sl.HasInterest(sm.subj) {\n\t\t\t\tadjust++\n\t\t\t\tif seen != nil {\n\t\t\t\t\tseen[sm.subj] = true\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tss.Msgs -= adjust\n\t}\n\n\treturn ss.Msgs, ms.state.LastSeq\n}\n\n// Will check the msg limit for this tracked subject.\n// Lock should be held.\nfunc (ms *memStore) enforcePerSubjectLimit(subj string, ss *SimpleState) {\n\tif ms.maxp <= 0 {\n\t\treturn\n\t}\n\tfor nmsgs := ss.Msgs; nmsgs > uint64(ms.maxp); nmsgs = ss.Msgs {\n\t\tif ss.firstNeedsUpdate || ss.lastNeedsUpdate {\n\t\t\tms.recalculateForSubj(subj, ss)\n\t\t}\n\t\tif !ms.removeMsg(ss.First, false) {\n\t\t\tbreak\n\t\t}\n\t}\n}\n\n// Will check the msg limit and drop firstSeq msg if needed.\n// Lock should be held.\nfunc (ms *memStore) enforceMsgLimit() {\n\tif ms.cfg.Discard != DiscardOld {\n\t\treturn\n\t}\n\tif ms.cfg.MaxMsgs <= 0 || ms.state.Msgs <= uint64(ms.cfg.MaxMsgs) {\n\t\treturn\n\t}\n\tfor nmsgs := ms.state.Msgs; nmsgs > uint64(ms.cfg.MaxMsgs); nmsgs = ms.state.Msgs {\n\t\tms.deleteFirstMsgOrPanic()\n\t}\n}\n\n// Will check the bytes limit and drop msgs if needed.\n// Lock should be held.\nfunc (ms *memStore) enforceBytesLimit() {\n\tif ms.cfg.Discard != DiscardOld {\n\t\treturn\n\t}\n\tif ms.cfg.MaxBytes <= 0 || ms.state.Bytes <= uint64(ms.cfg.MaxBytes) {\n\t\treturn\n\t}\n\tfor bs := ms.state.Bytes; bs > uint64(ms.cfg.MaxBytes); bs = ms.state.Bytes {\n\t\tms.deleteFirstMsgOrPanic()\n\t}\n}\n\n// Will start the age check timer.\n// Lock should be held.\nfunc (ms *memStore) startAgeChk() {\n\tif ms.ageChk != nil {\n\t\treturn\n\t}\n\tif ms.cfg.MaxAge != 0 || ms.ttls != nil {\n\t\tms.ageChk = time.AfterFunc(ms.cfg.MaxAge, ms.expireMsgs)\n\t}\n}\n\n// Lock should be held.\nfunc (ms *memStore) resetAgeChk(delta int64) {\n\tvar next int64 = math.MaxInt64\n\tif ms.ttls != nil {\n\t\tnext = ms.ttls.GetNextExpiration(next)\n\t}\n\n\t// If there's no MaxAge and there's nothing waiting to be expired then\n\t// don't bother continuing. The next storeRawMsg() will wake us up if\n\t// needs be.\n\tif ms.cfg.MaxAge <= 0 && next == math.MaxInt64 {\n\t\tclearTimer(&ms.ageChk)\n\t\treturn\n\t}\n\n\t// Check to see if we should be firing sooner than MaxAge for an expiring TTL.\n\tfireIn := ms.cfg.MaxAge\n\n\t// If delta for next-to-expire message is unset, but we still have messages to remove.\n\t// Assume messages are removed through proposals, and we need to speed up subsequent age check.\n\tif delta == 0 && ms.state.Msgs > 0 {\n\t\tif until := 2 * time.Second; until < fireIn {\n\t\t\tfireIn = until\n\t\t}\n\t}\n\n\tif next < math.MaxInt64 {\n\t\t// Looks like there's a next expiration, use it either if there's no\n\t\t// MaxAge set or if it looks to be sooner than MaxAge is.\n\t\tif until := time.Until(time.Unix(0, next)); fireIn == 0 || until < fireIn {\n\t\t\tfireIn = until\n\t\t}\n\t}\n\n\t// If not then look at the delta provided (usually gap to next age expiry).\n\tif delta > 0 {\n\t\tif fireIn == 0 || time.Duration(delta) < fireIn {\n\t\t\tfireIn = time.Duration(delta)\n\t\t}\n\t}\n\n\t// Make sure we aren't firing too often either way, otherwise we can\n\t// negatively impact stream ingest performance.\n\tif fireIn < 250*time.Millisecond {\n\t\tfireIn = 250 * time.Millisecond\n\t}\n\n\tif ms.ageChk != nil {\n\t\tms.ageChk.Reset(fireIn)\n\t} else {\n\t\tms.ageChk = time.AfterFunc(fireIn, ms.expireMsgs)\n\t}\n}\n\n// Lock should be held.\nfunc (ms *memStore) cancelAgeChk() {\n\tif ms.ageChk != nil {\n\t\tms.ageChk.Stop()\n\t\tms.ageChk = nil\n\t}\n}\n\n// Will expire msgs that are too old.\nfunc (ms *memStore) expireMsgs() {\n\tvar smv StoreMsg\n\tvar sm *StoreMsg\n\tms.mu.RLock()\n\tmaxAge := int64(ms.cfg.MaxAge)\n\tminAge := time.Now().UnixNano() - maxAge\n\trmcb := ms.rmcb\n\tsdmcb := ms.sdmcb\n\tsdmTTL := int64(ms.cfg.SubjectDeleteMarkerTTL.Seconds())\n\tsdmEnabled := sdmTTL > 0\n\tms.mu.RUnlock()\n\tif sdmEnabled && (rmcb == nil || sdmcb == nil) {\n\t\treturn\n\t}\n\n\tif maxAge > 0 {\n\t\tvar seq uint64\n\t\tfor sm, seq, _ = ms.LoadNextMsg(fwcs, true, 0, &smv); sm != nil && sm.ts <= minAge; sm, seq, _ = ms.LoadNextMsg(fwcs, true, seq+1, &smv) {\n\t\t\tif len(sm.hdr) > 0 {\n\t\t\t\tif ttl, err := getMessageTTL(sm.hdr); err == nil && ttl < 0 {\n\t\t\t\t\t// The message has a negative TTL, therefore it must \"never expire\".\n\t\t\t\t\tminAge = time.Now().UnixNano() - maxAge\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t}\n\t\t\tif sdmEnabled {\n\t\t\t\tif last, ok := ms.shouldProcessSdm(seq, sm.subj); ok {\n\t\t\t\t\tsdm := last && len(getHeader(JSMarkerReason, sm.hdr)) == 0\n\t\t\t\t\tms.handleRemovalOrSdm(seq, sm.subj, sdm, sdmTTL)\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tms.mu.Lock()\n\t\t\t\tms.removeMsg(seq, false)\n\t\t\t\tms.mu.Unlock()\n\t\t\t}\n\t\t\t// Recalculate in case we are expiring a bunch.\n\t\t\tminAge = time.Now().UnixNano() - maxAge\n\t\t}\n\t}\n\n\tms.mu.Lock()\n\tdefer ms.mu.Unlock()\n\n\t// TODO: Not great that we're holding the lock here, but the timed hash wheel isn't thread-safe.\n\tnextTTL := int64(math.MaxInt64)\n\tvar rmSeqs []uint64\n\tvar ttlSdm map[string][]SDMBySubj\n\tif ms.ttls != nil {\n\t\tms.ttls.ExpireTasks(func(seq uint64, ts int64) bool {\n\t\t\tif sdmEnabled {\n\t\t\t\t// Need to grab subject for the specified sequence, and check\n\t\t\t\t// if the message hasn't been removed in the meantime.\n\t\t\t\tsm, _ = ms.loadMsgLocked(seq, &smv, false)\n\t\t\t\tif sm != nil {\n\t\t\t\t\tif ttlSdm == nil {\n\t\t\t\t\t\tttlSdm = make(map[string][]SDMBySubj, 1)\n\t\t\t\t\t}\n\t\t\t\t\tttlSdm[sm.subj] = append(ttlSdm[sm.subj], SDMBySubj{seq, len(getHeader(JSMarkerReason, sm.hdr)) != 0})\n\t\t\t\t\treturn false\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\t// Collect sequences to remove. Don't remove messages inline here,\n\t\t\t\t// as that releases the lock and THW is not thread-safe.\n\t\t\t\trmSeqs = append(rmSeqs, seq)\n\t\t\t}\n\t\t\treturn true\n\t\t})\n\t\tif maxAge > 0 {\n\t\t\t// Only check if we're expiring something in the next MaxAge interval, saves us a bit\n\t\t\t// of work if MaxAge will beat us to the next expiry anyway.\n\t\t\tnextTTL = ms.ttls.GetNextExpiration(time.Now().Add(time.Duration(maxAge)).UnixNano())\n\t\t} else {\n\t\t\tnextTTL = ms.ttls.GetNextExpiration(math.MaxInt64)\n\t\t}\n\t}\n\n\t// Remove messages collected by THW.\n\tfor _, seq := range rmSeqs {\n\t\tms.removeMsg(seq, false)\n\t}\n\n\t// THW is unordered, so must sort by sequence and must not be holding the lock.\n\tif len(ttlSdm) > 0 {\n\t\tms.mu.Unlock()\n\t\tfor subj, es := range ttlSdm {\n\t\t\tslices.SortFunc(es, func(a, b SDMBySubj) int {\n\t\t\t\tif a.seq == b.seq {\n\t\t\t\t\treturn 0\n\t\t\t\t} else if a.seq < b.seq {\n\t\t\t\t\treturn -1\n\t\t\t\t} else {\n\t\t\t\t\treturn 1\n\t\t\t\t}\n\t\t\t})\n\t\t\tfor _, e := range es {\n\t\t\t\tif last, ok := ms.shouldProcessSdm(e.seq, subj); ok {\n\t\t\t\t\tsdm := last && !e.sdm\n\t\t\t\t\tms.handleRemovalOrSdm(e.seq, subj, sdm, sdmTTL)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tms.mu.Lock()\n\t}\n\n\t// Only cancel if no message left, not on potential lookup error that would result in sm == nil.\n\tif ms.state.Msgs == 0 && nextTTL == math.MaxInt64 {\n\t\tms.cancelAgeChk()\n\t} else {\n\t\tif sm == nil {\n\t\t\tms.resetAgeChk(0)\n\t\t} else {\n\t\t\tms.resetAgeChk(sm.ts - minAge)\n\t\t}\n\t}\n}\n\nfunc (ms *memStore) shouldProcessSdm(seq uint64, subj string) (bool, bool) {\n\tms.mu.Lock()\n\tdefer ms.mu.Unlock()\n\n\tif ms.sdm == nil {\n\t\tms.sdm = newSDMMeta()\n\t}\n\n\tif p, ok := ms.sdm.pending[seq]; ok {\n\t\t// If we're about to use the cached value, and we knew it was last before,\n\t\t// quickly check that we don't have more remaining messages for the subject now.\n\t\t// Which means we are not the last anymore and must reset to not remove later data.\n\t\tif p.last {\n\t\t\tmsgs := ms.subjectsTotalsLocked(subj)[subj]\n\t\t\tnumPending := ms.sdm.totals[subj]\n\t\t\tif remaining := msgs - numPending; remaining > 0 {\n\t\t\t\tp.last = false\n\t\t\t}\n\t\t}\n\n\t\t// Don't allow more proposals for the same sequence if we already did recently.\n\t\tif time.Since(time.Unix(0, p.ts)) < 2*time.Second {\n\t\t\treturn p.last, false\n\t\t}\n\t\tms.sdm.pending[seq] = SDMBySeq{p.last, time.Now().UnixNano()}\n\t\treturn p.last, true\n\t}\n\n\tmsgs := ms.subjectsTotalsLocked(subj)[subj]\n\tif msgs == 0 {\n\t\treturn false, true\n\t}\n\tnumPending := ms.sdm.totals[subj]\n\tremaining := msgs - numPending\n\treturn ms.sdm.trackPending(seq, subj, remaining == 1), true\n}\n\nfunc (ms *memStore) handleRemovalOrSdm(seq uint64, subj string, sdm bool, sdmTTL int64) {\n\tif sdm {\n\t\tvar _hdr [128]byte\n\t\thdr := fmt.Appendf(\n\t\t\t_hdr[:0],\n\t\t\t\"NATS/1.0\\r\\n%s: %s\\r\\n%s: %s\\r\\n%s: %s\\r\\n\\r\\n\",\n\t\t\tJSMarkerReason, JSMarkerReasonMaxAge,\n\t\t\tJSMessageTTL, time.Duration(sdmTTL)*time.Second,\n\t\t\tJSMsgRollup, JSMsgRollupSubject,\n\t\t)\n\t\tmsg := &inMsg{\n\t\t\tsubj: subj,\n\t\t\thdr:  hdr,\n\t\t}\n\t\tms.sdmcb(msg)\n\t} else {\n\t\tms.rmcb(seq)\n\t}\n}\n\n// PurgeEx will remove messages based on subject filters, sequence and number of messages to keep.\n// Will return the number of purged messages.\nfunc (ms *memStore) PurgeEx(subject string, sequence, keep uint64) (purged uint64, err error) {\n\tif subject == _EMPTY_ || subject == fwcs {\n\t\tif keep == 0 && sequence == 0 {\n\t\t\treturn ms.purge(0)\n\t\t}\n\t\tif sequence > 1 {\n\t\t\treturn ms.compact(sequence)\n\t\t} else if keep > 0 {\n\t\t\tms.mu.RLock()\n\t\t\tmsgs, lseq := ms.state.Msgs, ms.state.LastSeq\n\t\t\tms.mu.RUnlock()\n\t\t\tif keep >= msgs {\n\t\t\t\treturn 0, nil\n\t\t\t}\n\t\t\treturn ms.compact(lseq - keep + 1)\n\t\t}\n\t\treturn 0, nil\n\n\t}\n\teq := compareFn(subject)\n\tif ss := ms.FilteredState(1, subject); ss.Msgs > 0 {\n\t\tif keep > 0 {\n\t\t\tif keep >= ss.Msgs {\n\t\t\t\treturn 0, nil\n\t\t\t}\n\t\t\tss.Msgs -= keep\n\t\t}\n\t\tlast := ss.Last\n\t\tif sequence > 1 {\n\t\t\tlast = sequence - 1\n\t\t}\n\t\tms.mu.Lock()\n\t\tfor seq := ss.First; seq <= last; seq++ {\n\t\t\tif sm, ok := ms.msgs[seq]; ok && eq(sm.subj, subject) {\n\t\t\t\tif ok := ms.removeMsg(sm.seq, false); ok {\n\t\t\t\t\tpurged++\n\t\t\t\t\tif purged >= ss.Msgs {\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tms.mu.Unlock()\n\t}\n\treturn purged, nil\n}\n\n// Purge will remove all messages from this store.\n// Will return the number of purged messages.\nfunc (ms *memStore) Purge() (uint64, error) {\n\treturn ms.purge(0)\n}\n\nfunc (ms *memStore) purge(fseq uint64) (uint64, error) {\n\tms.mu.Lock()\n\tpurged := uint64(len(ms.msgs))\n\tcb := ms.scb\n\tbytes := int64(ms.state.Bytes)\n\tif fseq == 0 {\n\t\tfseq = ms.state.LastSeq + 1\n\t} else if fseq < ms.state.LastSeq {\n\t\tms.mu.Unlock()\n\t\treturn 0, fmt.Errorf(\"partial purges not supported on memory store\")\n\t}\n\tms.state.FirstSeq = fseq\n\tms.state.LastSeq = fseq - 1\n\tms.state.FirstTime = time.Time{}\n\tms.state.Bytes = 0\n\tms.state.Msgs = 0\n\tms.msgs = make(map[uint64]*StoreMsg)\n\tms.fss = stree.NewSubjectTree[SimpleState]()\n\tms.dmap.Empty()\n\tms.sdm.empty()\n\tms.mu.Unlock()\n\n\tif cb != nil {\n\t\tcb(-int64(purged), -bytes, 0, _EMPTY_)\n\t}\n\n\treturn purged, nil\n}\n\n// Compact will remove all messages from this store up to\n// but not including the seq parameter.\n// Will return the number of purged messages.\nfunc (ms *memStore) Compact(seq uint64) (uint64, error) {\n\treturn ms.compact(seq)\n}\n\nfunc (ms *memStore) compact(seq uint64) (uint64, error) {\n\tif seq == 0 {\n\t\treturn ms.Purge()\n\t}\n\n\tvar purged, bytes uint64\n\n\tms.mu.Lock()\n\tcb := ms.scb\n\tif seq <= ms.state.LastSeq {\n\t\tfseq := ms.state.FirstSeq\n\t\t// Determine new first sequence.\n\t\tfor ; seq <= ms.state.LastSeq; seq++ {\n\t\t\tif sm, ok := ms.msgs[seq]; ok {\n\t\t\t\tms.state.FirstSeq = seq\n\t\t\t\tms.state.FirstTime = time.Unix(0, sm.ts).UTC()\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\tfor seq := seq - 1; seq >= fseq; seq-- {\n\t\t\tif sm := ms.msgs[seq]; sm != nil {\n\t\t\t\tbytes += memStoreMsgSize(sm.subj, sm.hdr, sm.msg)\n\t\t\t\tpurged++\n\t\t\t\tms.removeSeqPerSubject(sm.subj, seq)\n\t\t\t\t// Must delete message after updating per-subject info, to be consistent with file store.\n\t\t\t\tdelete(ms.msgs, seq)\n\t\t\t} else if !ms.dmap.IsEmpty() {\n\t\t\t\tms.dmap.Delete(seq)\n\t\t\t}\n\t\t}\n\t\tif purged > ms.state.Msgs {\n\t\t\tpurged = ms.state.Msgs\n\t\t}\n\t\tms.state.Msgs -= purged\n\t\tif bytes > ms.state.Bytes {\n\t\t\tbytes = ms.state.Bytes\n\t\t}\n\t\tms.state.Bytes -= bytes\n\t} else {\n\t\t// We are compacting past the end of our range. Do purge and set sequences correctly\n\t\t// such that the next message placed will have seq.\n\t\tpurged = uint64(len(ms.msgs))\n\t\tbytes = ms.state.Bytes\n\t\tms.state.Bytes = 0\n\t\tms.state.Msgs = 0\n\t\tms.state.FirstSeq = seq\n\t\tms.state.FirstTime = time.Time{}\n\t\tms.state.LastSeq = seq - 1\n\t\t// Reset msgs, fss and dmap.\n\t\tms.msgs = make(map[uint64]*StoreMsg)\n\t\tms.fss = stree.NewSubjectTree[SimpleState]()\n\t\tms.dmap.Empty()\n\t\tms.sdm.empty()\n\t}\n\tms.mu.Unlock()\n\n\tif cb != nil {\n\t\tcb(-int64(purged), -int64(bytes), 0, _EMPTY_)\n\t}\n\n\treturn purged, nil\n}\n\n// Will completely reset our store.\nfunc (ms *memStore) reset() error {\n\tms.mu.Lock()\n\tvar purged, bytes uint64\n\tcb := ms.scb\n\tif cb != nil {\n\t\tfor _, sm := range ms.msgs {\n\t\t\tpurged++\n\t\t\tbytes += memStoreMsgSize(sm.subj, sm.hdr, sm.msg)\n\t\t}\n\t}\n\n\t// Reset\n\tms.state.FirstSeq = 0\n\tms.state.FirstTime = time.Time{}\n\tms.state.LastSeq = 0\n\tms.state.LastTime = time.Now().UTC()\n\t// Update msgs and bytes.\n\tms.state.Msgs = 0\n\tms.state.Bytes = 0\n\t// Reset msgs, fss and dmap.\n\tms.msgs = make(map[uint64]*StoreMsg)\n\tms.fss = stree.NewSubjectTree[SimpleState]()\n\tms.dmap.Empty()\n\tms.sdm.empty()\n\n\tms.mu.Unlock()\n\n\tif cb != nil {\n\t\tcb(-int64(purged), -int64(bytes), 0, _EMPTY_)\n\t}\n\n\treturn nil\n}\n\n// Truncate will truncate a stream store up to seq. Sequence needs to be valid.\nfunc (ms *memStore) Truncate(seq uint64) error {\n\t// Check for request to reset.\n\tif seq == 0 {\n\t\treturn ms.reset()\n\t}\n\n\tvar purged, bytes uint64\n\n\tms.mu.Lock()\n\tlsm, ok := ms.msgs[seq]\n\tif !ok {\n\t\tms.mu.Unlock()\n\t\treturn ErrInvalidSequence\n\t}\n\n\tfor i := ms.state.LastSeq; i > seq; i-- {\n\t\tif sm := ms.msgs[i]; sm != nil {\n\t\t\tpurged++\n\t\t\tbytes += memStoreMsgSize(sm.subj, sm.hdr, sm.msg)\n\t\t\tms.removeSeqPerSubject(sm.subj, i)\n\t\t\t// Must delete message after updating per-subject info, to be consistent with file store.\n\t\t\tdelete(ms.msgs, i)\n\t\t} else if !ms.dmap.IsEmpty() {\n\t\t\tms.dmap.Delete(i)\n\t\t}\n\t}\n\t// Reset last.\n\tms.state.LastSeq = lsm.seq\n\tms.state.LastTime = time.Unix(0, lsm.ts).UTC()\n\t// Update msgs and bytes.\n\tif purged > ms.state.Msgs {\n\t\tpurged = ms.state.Msgs\n\t}\n\tms.state.Msgs -= purged\n\tif bytes > ms.state.Bytes {\n\t\tbytes = ms.state.Bytes\n\t}\n\tms.state.Bytes -= bytes\n\n\tcb := ms.scb\n\tms.mu.Unlock()\n\n\tif cb != nil {\n\t\tcb(-int64(purged), -int64(bytes), 0, _EMPTY_)\n\t}\n\n\treturn nil\n}\n\nfunc (ms *memStore) deleteFirstMsgOrPanic() {\n\tif !ms.deleteFirstMsg() {\n\t\tpanic(\"jetstream memstore has inconsistent state, can't find first seq msg\")\n\t}\n}\n\nfunc (ms *memStore) deleteFirstMsg() bool {\n\treturn ms.removeMsg(ms.state.FirstSeq, false)\n}\n\n// SubjectForSeq will return what the subject is for this sequence if found.\nfunc (ms *memStore) SubjectForSeq(seq uint64) (string, error) {\n\tms.mu.RLock()\n\tdefer ms.mu.RUnlock()\n\tif seq < ms.state.FirstSeq {\n\t\treturn _EMPTY_, ErrStoreMsgNotFound\n\t}\n\tif sm, ok := ms.msgs[seq]; ok {\n\t\treturn sm.subj, nil\n\t}\n\treturn _EMPTY_, ErrStoreMsgNotFound\n}\n\n// LoadMsg will lookup the message by sequence number and return it if found.\nfunc (ms *memStore) LoadMsg(seq uint64, smp *StoreMsg) (*StoreMsg, error) {\n\treturn ms.loadMsgLocked(seq, smp, true)\n}\n\n// loadMsgLocked will lookup the message by sequence number and return it if found.\nfunc (ms *memStore) loadMsgLocked(seq uint64, smp *StoreMsg, needMSLock bool) (*StoreMsg, error) {\n\tif needMSLock {\n\t\tms.mu.RLock()\n\t}\n\tsm, ok := ms.msgs[seq]\n\tlast := ms.state.LastSeq\n\tif needMSLock {\n\t\tms.mu.RUnlock()\n\t}\n\n\tif !ok || sm == nil {\n\t\tvar err = ErrStoreEOF\n\t\tif seq <= last {\n\t\t\terr = ErrStoreMsgNotFound\n\t\t}\n\t\treturn nil, err\n\t}\n\n\tif smp == nil {\n\t\tsmp = new(StoreMsg)\n\t}\n\tsm.copy(smp)\n\treturn smp, nil\n}\n\n// LoadLastMsg will return the last message we have that matches a given subject.\n// The subject can be a wildcard.\nfunc (ms *memStore) LoadLastMsg(subject string, smp *StoreMsg) (*StoreMsg, error) {\n\tvar sm *StoreMsg\n\tvar ok bool\n\n\t// This needs to be a write lock, as filteredStateLocked can\n\t// mutate the per-subject state.\n\tms.mu.Lock()\n\tdefer ms.mu.Unlock()\n\n\tif subject == _EMPTY_ || subject == fwcs {\n\t\tsm, ok = ms.msgs[ms.state.LastSeq]\n\t} else if subjectIsLiteral(subject) {\n\t\tvar ss *SimpleState\n\t\tif ss, ok = ms.fss.Find(stringToBytes(subject)); ok && ss.Msgs > 0 {\n\t\t\tsm, ok = ms.msgs[ss.Last]\n\t\t}\n\t} else if ss := ms.filteredStateLocked(1, subject, true); ss.Msgs > 0 {\n\t\tsm, ok = ms.msgs[ss.Last]\n\t}\n\tif !ok || sm == nil {\n\t\treturn nil, ErrStoreMsgNotFound\n\t}\n\n\tif smp == nil {\n\t\tsmp = new(StoreMsg)\n\t}\n\tsm.copy(smp)\n\treturn smp, nil\n}\n\n// LoadNextMsgMulti will find the next message matching any entry in the sublist.\nfunc (ms *memStore) LoadNextMsgMulti(sl *Sublist, start uint64, smp *StoreMsg) (sm *StoreMsg, skip uint64, err error) {\n\t// TODO(dlc) - for now simple linear walk to get started.\n\tms.mu.RLock()\n\tdefer ms.mu.RUnlock()\n\n\tif start < ms.state.FirstSeq {\n\t\tstart = ms.state.FirstSeq\n\t}\n\n\t// If past the end no results.\n\tif start > ms.state.LastSeq || ms.state.Msgs == 0 {\n\t\treturn nil, ms.state.LastSeq, ErrStoreEOF\n\t}\n\n\t// Initial setup.\n\tfseq, lseq := start, ms.state.LastSeq\n\n\tfor nseq := fseq; nseq <= lseq; nseq++ {\n\t\tsm, ok := ms.msgs[nseq]\n\t\tif !ok {\n\t\t\tcontinue\n\t\t}\n\t\tif sl.HasInterest(sm.subj) {\n\t\t\tif smp == nil {\n\t\t\t\tsmp = new(StoreMsg)\n\t\t\t}\n\t\t\tsm.copy(smp)\n\t\t\treturn smp, nseq, nil\n\t\t}\n\t}\n\treturn nil, ms.state.LastSeq, ErrStoreEOF\n}\n\n// LoadNextMsg will find the next message matching the filter subject starting at the start sequence.\n// The filter subject can be a wildcard.\nfunc (ms *memStore) LoadNextMsg(filter string, wc bool, start uint64, smp *StoreMsg) (*StoreMsg, uint64, error) {\n\tms.mu.Lock()\n\tdefer ms.mu.Unlock()\n\n\tif start < ms.state.FirstSeq {\n\t\tstart = ms.state.FirstSeq\n\t}\n\n\t// If past the end no results.\n\tif start > ms.state.LastSeq || ms.state.Msgs == 0 {\n\t\treturn nil, ms.state.LastSeq, ErrStoreEOF\n\t}\n\n\tif filter == _EMPTY_ {\n\t\tfilter = fwcs\n\t}\n\tisAll := filter == fwcs\n\n\t// Skip scan of ms.fss if number of messages in the block are less than\n\t// 1/2 the number of subjects in ms.fss. Or we have a wc and lots of fss entries.\n\tconst linearScanMaxFSS = 256\n\tdoLinearScan := isAll || 2*int(ms.state.LastSeq-start) < ms.fss.Size() || (wc && ms.fss.Size() > linearScanMaxFSS)\n\n\t// Initial setup.\n\tfseq, lseq := start, ms.state.LastSeq\n\n\tif !doLinearScan {\n\t\tsubs := []string{filter}\n\t\tif wc || isAll {\n\t\t\tsubs = subs[:0]\n\t\t\tms.fss.Match(stringToBytes(filter), func(subj []byte, val *SimpleState) {\n\t\t\t\tsubs = append(subs, string(subj))\n\t\t\t})\n\t\t}\n\t\tfseq, lseq = ms.state.LastSeq, uint64(0)\n\t\tfor _, subj := range subs {\n\t\t\tss, ok := ms.fss.Find(stringToBytes(subj))\n\t\t\tif !ok {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif ss.firstNeedsUpdate || ss.lastNeedsUpdate {\n\t\t\t\tms.recalculateForSubj(subj, ss)\n\t\t\t}\n\t\t\tif ss.First < fseq {\n\t\t\t\tfseq = ss.First\n\t\t\t}\n\t\t\tif ss.Last > lseq {\n\t\t\t\tlseq = ss.Last\n\t\t\t}\n\t\t}\n\t\tif fseq < start {\n\t\t\tfseq = start\n\t\t}\n\t}\n\n\teq := subjectsEqual\n\tif wc {\n\t\teq = subjectIsSubsetMatch\n\t}\n\n\tfor nseq := fseq; nseq <= lseq; nseq++ {\n\t\tif sm, ok := ms.msgs[nseq]; ok && (isAll || eq(sm.subj, filter)) {\n\t\t\tif smp == nil {\n\t\t\t\tsmp = new(StoreMsg)\n\t\t\t}\n\t\t\tsm.copy(smp)\n\t\t\treturn smp, nseq, nil\n\t\t}\n\t}\n\treturn nil, ms.state.LastSeq, ErrStoreEOF\n}\n\n// Will load the next non-deleted msg starting at the start sequence and walking backwards.\nfunc (ms *memStore) LoadPrevMsg(start uint64, smp *StoreMsg) (sm *StoreMsg, err error) {\n\tms.mu.RLock()\n\tdefer ms.mu.RUnlock()\n\n\tif ms.msgs == nil {\n\t\treturn nil, ErrStoreClosed\n\t}\n\tif ms.state.Msgs == 0 || start < ms.state.FirstSeq {\n\t\treturn nil, ErrStoreEOF\n\t}\n\tif start > ms.state.LastSeq {\n\t\tstart = ms.state.LastSeq\n\t}\n\n\tfor seq := start; seq >= ms.state.FirstSeq; seq-- {\n\t\tif sm, ok := ms.msgs[seq]; ok {\n\t\t\tif smp == nil {\n\t\t\t\tsmp = new(StoreMsg)\n\t\t\t}\n\t\t\tsm.copy(smp)\n\t\t\treturn smp, nil\n\t\t}\n\t}\n\treturn nil, ErrStoreEOF\n}\n\n// RemoveMsg will remove the message from this store.\n// Will return the number of bytes removed.\nfunc (ms *memStore) RemoveMsg(seq uint64) (bool, error) {\n\tms.mu.Lock()\n\tremoved := ms.removeMsg(seq, false)\n\tms.mu.Unlock()\n\treturn removed, nil\n}\n\n// EraseMsg will remove the message and rewrite its contents.\nfunc (ms *memStore) EraseMsg(seq uint64) (bool, error) {\n\tms.mu.Lock()\n\tremoved := ms.removeMsg(seq, true)\n\tms.mu.Unlock()\n\treturn removed, nil\n}\n\n// Performs logic to update first sequence number.\n// Lock should be held.\nfunc (ms *memStore) updateFirstSeq(seq uint64) {\n\tif seq != ms.state.FirstSeq {\n\t\t// Interior delete.\n\t\treturn\n\t}\n\tvar nsm *StoreMsg\n\tvar ok bool\n\tfor nseq := ms.state.FirstSeq + 1; nseq <= ms.state.LastSeq; nseq++ {\n\t\tif nsm, ok = ms.msgs[nseq]; ok {\n\t\t\tbreak\n\t\t}\n\t}\n\toldFirst := ms.state.FirstSeq\n\tif nsm != nil {\n\t\tms.state.FirstSeq = nsm.seq\n\t\tms.state.FirstTime = time.Unix(0, nsm.ts).UTC()\n\t} else {\n\t\t// Like purge.\n\t\tms.state.FirstSeq = ms.state.LastSeq + 1\n\t\tms.state.FirstTime = time.Time{}\n\t}\n\n\tif oldFirst == ms.state.FirstSeq-1 {\n\t\tms.dmap.Delete(oldFirst)\n\t} else {\n\t\tfor seq := oldFirst; seq < ms.state.FirstSeq; seq++ {\n\t\t\tms.dmap.Delete(seq)\n\t\t}\n\t}\n}\n\n// Remove a seq from the fss and select new first.\n// Lock should be held.\nfunc (ms *memStore) removeSeqPerSubject(subj string, seq uint64) {\n\tss, ok := ms.fss.Find(stringToBytes(subj))\n\tif !ok {\n\t\treturn\n\t}\n\tms.sdm.removeSeqAndSubject(seq, subj)\n\tif ss.Msgs == 1 {\n\t\tms.fss.Delete(stringToBytes(subj))\n\t\treturn\n\t}\n\tss.Msgs--\n\n\t// Only one left\n\tif ss.Msgs == 1 {\n\t\tif !ss.lastNeedsUpdate && seq != ss.Last {\n\t\t\tss.First = ss.Last\n\t\t\tss.firstNeedsUpdate = false\n\t\t\treturn\n\t\t}\n\t\tif !ss.firstNeedsUpdate && seq != ss.First {\n\t\t\tss.Last = ss.First\n\t\t\tss.lastNeedsUpdate = false\n\t\t\treturn\n\t\t}\n\t}\n\n\t// We can lazily calculate the first/last sequence when needed.\n\tss.firstNeedsUpdate = seq == ss.First || ss.firstNeedsUpdate\n\tss.lastNeedsUpdate = seq == ss.Last || ss.lastNeedsUpdate\n}\n\n// Will recalculate the first and/or last sequence for this subject.\n// Lock should be held.\nfunc (ms *memStore) recalculateForSubj(subj string, ss *SimpleState) {\n\tif ss.firstNeedsUpdate {\n\t\ttseq := ss.First + 1\n\t\tif tseq < ms.state.FirstSeq {\n\t\t\ttseq = ms.state.FirstSeq\n\t\t}\n\t\tfor ; tseq <= ss.Last; tseq++ {\n\t\t\tif sm := ms.msgs[tseq]; sm != nil && sm.subj == subj {\n\t\t\t\tss.First = tseq\n\t\t\t\tss.firstNeedsUpdate = false\n\t\t\t\tif ss.Msgs == 1 {\n\t\t\t\t\tss.Last = tseq\n\t\t\t\t\tss.lastNeedsUpdate = false\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t}\n\tif ss.lastNeedsUpdate {\n\t\ttseq := ss.Last - 1\n\t\tif tseq > ms.state.LastSeq {\n\t\t\ttseq = ms.state.LastSeq\n\t\t}\n\t\tfor ; tseq >= ss.First; tseq-- {\n\t\t\tif sm := ms.msgs[tseq]; sm != nil && sm.subj == subj {\n\t\t\t\tss.Last = tseq\n\t\t\t\tss.lastNeedsUpdate = false\n\t\t\t\tif ss.Msgs == 1 {\n\t\t\t\t\tss.First = tseq\n\t\t\t\t\tss.firstNeedsUpdate = false\n\t\t\t\t}\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}\n}\n\n// Removes the message referenced by seq.\n// Lock should be held.\nfunc (ms *memStore) removeMsg(seq uint64, secure bool) bool {\n\tvar ss uint64\n\tsm, ok := ms.msgs[seq]\n\tif !ok {\n\t\treturn false\n\t}\n\n\tss = memStoreMsgSize(sm.subj, sm.hdr, sm.msg)\n\n\tif ms.state.Msgs > 0 {\n\t\tms.state.Msgs--\n\t\tif ss > ms.state.Bytes {\n\t\t\tss = ms.state.Bytes\n\t\t}\n\t\tms.state.Bytes -= ss\n\t}\n\tms.dmap.Insert(seq)\n\tms.updateFirstSeq(seq)\n\n\tif secure {\n\t\tif len(sm.hdr) > 0 {\n\t\t\tsm.hdr = make([]byte, len(sm.hdr))\n\t\t\tcrand.Read(sm.hdr)\n\t\t}\n\t\tif len(sm.msg) > 0 {\n\t\t\tsm.msg = make([]byte, len(sm.msg))\n\t\t\tcrand.Read(sm.msg)\n\t\t}\n\t\tsm.seq, sm.ts = 0, 0\n\t}\n\n\t// Remove any per subject tracking.\n\tms.removeSeqPerSubject(sm.subj, seq)\n\n\t// Must delete message after updating per-subject info, to be consistent with file store.\n\tdelete(ms.msgs, seq)\n\n\tif ms.scb != nil {\n\t\t// We do not want to hold any locks here.\n\t\tms.mu.Unlock()\n\t\tif ms.scb != nil {\n\t\t\tdelta := int64(ss)\n\t\t\tms.scb(-1, -delta, seq, sm.subj)\n\t\t}\n\t\tms.mu.Lock()\n\t}\n\n\treturn ok\n}\n\n// Type returns the type of the underlying store.\nfunc (ms *memStore) Type() StorageType {\n\treturn MemoryStorage\n}\n\n// FastState will fill in state with only the following.\n// Msgs, Bytes, First and Last Sequence and Time and NumDeleted.\nfunc (ms *memStore) FastState(state *StreamState) {\n\tms.mu.RLock()\n\tstate.Msgs = ms.state.Msgs\n\tstate.Bytes = ms.state.Bytes\n\tstate.FirstSeq = ms.state.FirstSeq\n\tstate.FirstTime = ms.state.FirstTime\n\tstate.LastSeq = ms.state.LastSeq\n\tstate.LastTime = ms.state.LastTime\n\tif state.LastSeq > state.FirstSeq {\n\t\tstate.NumDeleted = int((state.LastSeq - state.FirstSeq + 1) - state.Msgs)\n\t\tif state.NumDeleted < 0 {\n\t\t\tstate.NumDeleted = 0\n\t\t}\n\t}\n\tstate.Consumers = ms.consumers\n\tstate.NumSubjects = ms.fss.Size()\n\tms.mu.RUnlock()\n}\n\nfunc (ms *memStore) State() StreamState {\n\tms.mu.RLock()\n\tdefer ms.mu.RUnlock()\n\n\tstate := ms.state\n\tstate.Consumers = ms.consumers\n\tstate.NumSubjects = ms.fss.Size()\n\tstate.Deleted = nil\n\n\t// Calculate interior delete details.\n\tif numDeleted := int((state.LastSeq - state.FirstSeq + 1) - state.Msgs); numDeleted > 0 {\n\t\tstate.Deleted = make([]uint64, 0, numDeleted)\n\t\tfseq, lseq := state.FirstSeq, state.LastSeq\n\t\tms.dmap.Range(func(seq uint64) bool {\n\t\t\tif seq < fseq || seq > lseq {\n\t\t\t\tms.dmap.Delete(seq)\n\t\t\t} else {\n\t\t\t\tstate.Deleted = append(state.Deleted, seq)\n\t\t\t}\n\t\t\treturn true\n\t\t})\n\t}\n\tif len(state.Deleted) > 0 {\n\t\tstate.NumDeleted = len(state.Deleted)\n\t}\n\n\treturn state\n}\n\nfunc (ms *memStore) Utilization() (total, reported uint64, err error) {\n\tms.mu.RLock()\n\tdefer ms.mu.RUnlock()\n\treturn ms.state.Bytes, ms.state.Bytes, nil\n}\n\nfunc memStoreMsgSize(subj string, hdr, msg []byte) uint64 {\n\treturn uint64(len(subj) + len(hdr) + len(msg) + 16) // 8*2 for seq + age\n}\n\n// Delete is same as Stop for memory store.\nfunc (ms *memStore) Delete() error {\n\treturn ms.Stop()\n}\n\nfunc (ms *memStore) Stop() error {\n\t// These can't come back, so stop is same as Delete.\n\tms.Purge()\n\tms.mu.Lock()\n\tif ms.ageChk != nil {\n\t\tms.ageChk.Stop()\n\t\tms.ageChk = nil\n\t}\n\tms.msgs = nil\n\tms.mu.Unlock()\n\treturn nil\n}\n\nfunc (ms *memStore) isClosed() bool {\n\tms.mu.RLock()\n\tdefer ms.mu.RUnlock()\n\treturn ms.msgs == nil\n}\n\ntype consumerMemStore struct {\n\tmu     sync.Mutex\n\tms     StreamStore\n\tcfg    ConsumerConfig\n\tstate  ConsumerState\n\tclosed bool\n}\n\nfunc (ms *memStore) ConsumerStore(name string, cfg *ConsumerConfig) (ConsumerStore, error) {\n\tif ms == nil {\n\t\treturn nil, fmt.Errorf(\"memstore is nil\")\n\t}\n\tif ms.isClosed() {\n\t\treturn nil, ErrStoreClosed\n\t}\n\tif cfg == nil || name == _EMPTY_ {\n\t\treturn nil, fmt.Errorf(\"bad consumer config\")\n\t}\n\to := &consumerMemStore{ms: ms, cfg: *cfg}\n\tms.AddConsumer(o)\n\treturn o, nil\n}\n\nfunc (ms *memStore) AddConsumer(o ConsumerStore) error {\n\tms.mu.Lock()\n\tms.consumers++\n\tms.mu.Unlock()\n\treturn nil\n}\n\nfunc (ms *memStore) RemoveConsumer(o ConsumerStore) error {\n\tms.mu.Lock()\n\tif ms.consumers > 0 {\n\t\tms.consumers--\n\t}\n\tms.mu.Unlock()\n\treturn nil\n}\n\nfunc (ms *memStore) Snapshot(_ time.Duration, _, _ bool) (*SnapshotResult, error) {\n\treturn nil, fmt.Errorf(\"no impl\")\n}\n\n// Binary encoded state snapshot, >= v2.10 server.\nfunc (ms *memStore) EncodedStreamState(failed uint64) ([]byte, error) {\n\tms.mu.RLock()\n\tdefer ms.mu.RUnlock()\n\n\t// Quick calculate num deleted.\n\tnumDeleted := int((ms.state.LastSeq - ms.state.FirstSeq + 1) - ms.state.Msgs)\n\tif numDeleted < 0 {\n\t\tnumDeleted = 0\n\t}\n\n\t// Encoded is Msgs, Bytes, FirstSeq, LastSeq, Failed, NumDeleted and optional DeletedBlocks\n\tvar buf [1024]byte\n\tbuf[0], buf[1] = streamStateMagic, streamStateVersion\n\tn := hdrLen\n\tn += binary.PutUvarint(buf[n:], ms.state.Msgs)\n\tn += binary.PutUvarint(buf[n:], ms.state.Bytes)\n\tn += binary.PutUvarint(buf[n:], ms.state.FirstSeq)\n\tn += binary.PutUvarint(buf[n:], ms.state.LastSeq)\n\tn += binary.PutUvarint(buf[n:], failed)\n\tn += binary.PutUvarint(buf[n:], uint64(numDeleted))\n\n\tb := buf[0:n]\n\n\tif numDeleted > 0 {\n\t\tbuf, err := ms.dmap.Encode(nil)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tb = append(b, buf...)\n\t}\n\n\treturn b, nil\n}\n\n// SyncDeleted will make sure this stream has same deleted state as dbs.\nfunc (ms *memStore) SyncDeleted(dbs DeleteBlocks) {\n\tms.mu.Lock()\n\tdefer ms.mu.Unlock()\n\n\t// For now we share one dmap, so if we have one entry here check if states are the same.\n\t// Note this will work for any DeleteBlock type, but we expect this to be a dmap too.\n\tif len(dbs) == 1 {\n\t\tmin, max, num := ms.dmap.State()\n\t\tif pmin, pmax, pnum := dbs[0].State(); pmin == min && pmax == max && pnum == num {\n\t\t\treturn\n\t\t}\n\t}\n\tlseq := ms.state.LastSeq\n\tfor _, db := range dbs {\n\t\t// Skip if beyond our current state.\n\t\tif first, _, _ := db.State(); first > lseq {\n\t\t\tcontinue\n\t\t}\n\t\tdb.Range(func(seq uint64) bool {\n\t\t\tms.removeMsg(seq, false)\n\t\t\treturn true\n\t\t})\n\t}\n}\n\nfunc (o *consumerMemStore) Update(state *ConsumerState) error {\n\t// Sanity checks.\n\tif state.AckFloor.Consumer > state.Delivered.Consumer {\n\t\treturn fmt.Errorf(\"bad ack floor for consumer\")\n\t}\n\tif state.AckFloor.Stream > state.Delivered.Stream {\n\t\treturn fmt.Errorf(\"bad ack floor for stream\")\n\t}\n\n\t// Copy to our state.\n\tvar pending map[uint64]*Pending\n\tvar redelivered map[uint64]uint64\n\tif len(state.Pending) > 0 {\n\t\tpending = make(map[uint64]*Pending, len(state.Pending))\n\t\tfor seq, p := range state.Pending {\n\t\t\tpending[seq] = &Pending{p.Sequence, p.Timestamp}\n\t\t\tif seq <= state.AckFloor.Stream || seq > state.Delivered.Stream {\n\t\t\t\treturn fmt.Errorf(\"bad pending entry, sequence [%d] out of range\", seq)\n\t\t\t}\n\t\t}\n\t}\n\tif len(state.Redelivered) > 0 {\n\t\tredelivered = make(map[uint64]uint64, len(state.Redelivered))\n\t\tfor seq, dc := range state.Redelivered {\n\t\t\tredelivered[seq] = dc\n\t\t}\n\t}\n\n\t// Replace our state.\n\to.mu.Lock()\n\tdefer o.mu.Unlock()\n\n\t// Check to see if this is an outdated update.\n\tif state.Delivered.Consumer < o.state.Delivered.Consumer || state.AckFloor.Stream < o.state.AckFloor.Stream {\n\t\treturn fmt.Errorf(\"old update ignored\")\n\t}\n\n\to.state.Delivered = state.Delivered\n\to.state.AckFloor = state.AckFloor\n\to.state.Pending = pending\n\to.state.Redelivered = redelivered\n\n\treturn nil\n}\n\n// SetStarting sets our starting stream sequence.\nfunc (o *consumerMemStore) SetStarting(sseq uint64) error {\n\to.mu.Lock()\n\to.state.Delivered.Stream = sseq\n\to.mu.Unlock()\n\treturn nil\n}\n\n// UpdateStarting updates our starting stream sequence.\nfunc (o *consumerMemStore) UpdateStarting(sseq uint64) {\n\to.mu.Lock()\n\tdefer o.mu.Unlock()\n\n\tif sseq > o.state.Delivered.Stream {\n\t\to.state.Delivered.Stream = sseq\n\t\t// For AckNone just update delivered and ackfloor at the same time.\n\t\tif o.cfg.AckPolicy == AckNone {\n\t\t\to.state.AckFloor.Stream = sseq\n\t\t}\n\t}\n}\n\n// HasState returns if this store has a recorded state.\nfunc (o *consumerMemStore) HasState() bool {\n\to.mu.Lock()\n\tdefer o.mu.Unlock()\n\t// We have a running state.\n\treturn o.state.Delivered.Consumer != 0 || o.state.Delivered.Stream != 0\n}\n\nfunc (o *consumerMemStore) UpdateDelivered(dseq, sseq, dc uint64, ts int64) error {\n\to.mu.Lock()\n\tdefer o.mu.Unlock()\n\n\tif dc != 1 && o.cfg.AckPolicy == AckNone {\n\t\treturn ErrNoAckPolicy\n\t}\n\n\t// On restarts the old leader may get a replay from the raft logs that are old.\n\tif dseq <= o.state.AckFloor.Consumer {\n\t\treturn nil\n\t}\n\n\t// See if we expect an ack for this.\n\tif o.cfg.AckPolicy != AckNone {\n\t\t// Need to create pending records here.\n\t\tif o.state.Pending == nil {\n\t\t\to.state.Pending = make(map[uint64]*Pending)\n\t\t}\n\t\tvar p *Pending\n\t\t// Check for an update to a message already delivered.\n\t\tif sseq <= o.state.Delivered.Stream {\n\t\t\tif p = o.state.Pending[sseq]; p != nil {\n\t\t\t\t// Do not update p.Sequence, that should be the original delivery sequence.\n\t\t\t\tp.Timestamp = ts\n\t\t\t}\n\t\t} else {\n\t\t\t// Add to pending.\n\t\t\to.state.Pending[sseq] = &Pending{dseq, ts}\n\t\t}\n\t\t// Update delivered as needed.\n\t\tif dseq > o.state.Delivered.Consumer {\n\t\t\to.state.Delivered.Consumer = dseq\n\t\t}\n\t\tif sseq > o.state.Delivered.Stream {\n\t\t\to.state.Delivered.Stream = sseq\n\t\t}\n\n\t\tif dc > 1 {\n\t\t\tif maxdc := uint64(o.cfg.MaxDeliver); maxdc > 0 && dc > maxdc {\n\t\t\t\t// Make sure to remove from pending.\n\t\t\t\tdelete(o.state.Pending, sseq)\n\t\t\t}\n\t\t\tif o.state.Redelivered == nil {\n\t\t\t\to.state.Redelivered = make(map[uint64]uint64)\n\t\t\t}\n\t\t\t// Only update if greater than what we already have.\n\t\t\tif o.state.Redelivered[sseq] < dc-1 {\n\t\t\t\to.state.Redelivered[sseq] = dc - 1\n\t\t\t}\n\t\t}\n\t} else {\n\t\t// For AckNone just update delivered and ackfloor at the same time.\n\t\tif dseq > o.state.Delivered.Consumer {\n\t\t\to.state.Delivered.Consumer = dseq\n\t\t\to.state.AckFloor.Consumer = dseq\n\t\t}\n\t\tif sseq > o.state.Delivered.Stream {\n\t\t\to.state.Delivered.Stream = sseq\n\t\t\to.state.AckFloor.Stream = sseq\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc (o *consumerMemStore) UpdateAcks(dseq, sseq uint64) error {\n\to.mu.Lock()\n\tdefer o.mu.Unlock()\n\n\tif o.cfg.AckPolicy == AckNone {\n\t\treturn ErrNoAckPolicy\n\t}\n\n\t// On restarts the old leader may get a replay from the raft logs that are old.\n\tif dseq <= o.state.AckFloor.Consumer {\n\t\treturn nil\n\t}\n\n\tif len(o.state.Pending) == 0 || o.state.Pending[sseq] == nil {\n\t\tdelete(o.state.Redelivered, sseq)\n\t\treturn ErrStoreMsgNotFound\n\t}\n\n\t// Check for AckAll here.\n\tif o.cfg.AckPolicy == AckAll {\n\t\tsgap := sseq - o.state.AckFloor.Stream\n\t\to.state.AckFloor.Consumer = dseq\n\t\to.state.AckFloor.Stream = sseq\n\t\tif sgap > uint64(len(o.state.Pending)) {\n\t\t\tfor seq := range o.state.Pending {\n\t\t\t\tif seq <= sseq {\n\t\t\t\t\tdelete(o.state.Pending, seq)\n\t\t\t\t\tdelete(o.state.Redelivered, seq)\n\t\t\t\t}\n\t\t\t}\n\t\t} else {\n\t\t\tfor seq := sseq; seq > sseq-sgap && len(o.state.Pending) > 0; seq-- {\n\t\t\t\tdelete(o.state.Pending, seq)\n\t\t\t\tdelete(o.state.Redelivered, seq)\n\t\t\t}\n\t\t}\n\t\treturn nil\n\t}\n\n\t// AckExplicit\n\n\t// First delete from our pending state.\n\tif p, ok := o.state.Pending[sseq]; ok {\n\t\tdelete(o.state.Pending, sseq)\n\t\tif dseq > p.Sequence && p.Sequence > 0 {\n\t\t\tdseq = p.Sequence // Use the original.\n\t\t}\n\t}\n\n\tif len(o.state.Pending) == 0 {\n\t\to.state.AckFloor.Consumer = o.state.Delivered.Consumer\n\t\to.state.AckFloor.Stream = o.state.Delivered.Stream\n\t} else if dseq == o.state.AckFloor.Consumer+1 {\n\t\to.state.AckFloor.Consumer = dseq\n\t\to.state.AckFloor.Stream = sseq\n\n\t\tif o.state.Delivered.Consumer > dseq {\n\t\t\tfor ss := sseq + 1; ss <= o.state.Delivered.Stream; ss++ {\n\t\t\t\tif p, ok := o.state.Pending[ss]; ok {\n\t\t\t\t\tif p.Sequence > 0 {\n\t\t\t\t\t\to.state.AckFloor.Consumer = p.Sequence - 1\n\t\t\t\t\t\to.state.AckFloor.Stream = ss - 1\n\t\t\t\t\t}\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\t// We do these regardless.\n\tdelete(o.state.Redelivered, sseq)\n\n\treturn nil\n}\n\nfunc (o *consumerMemStore) UpdateConfig(cfg *ConsumerConfig) error {\n\to.mu.Lock()\n\tdefer o.mu.Unlock()\n\n\t// This is mostly unchecked here. We are assuming the upper layers have done sanity checking.\n\to.cfg = *cfg\n\treturn nil\n}\n\nfunc (o *consumerMemStore) Stop() error {\n\to.mu.Lock()\n\to.closed = true\n\tms := o.ms\n\to.mu.Unlock()\n\tms.RemoveConsumer(o)\n\treturn nil\n}\n\nfunc (o *consumerMemStore) Delete() error {\n\treturn o.Stop()\n}\n\nfunc (o *consumerMemStore) StreamDelete() error {\n\treturn o.Stop()\n}\n\nfunc (o *consumerMemStore) State() (*ConsumerState, error) {\n\treturn o.stateWithCopy(true)\n}\n\n// This will not copy pending or redelivered, so should only be done under the\n// consumer owner's lock.\nfunc (o *consumerMemStore) BorrowState() (*ConsumerState, error) {\n\treturn o.stateWithCopy(false)\n}\n\nfunc (o *consumerMemStore) stateWithCopy(doCopy bool) (*ConsumerState, error) {\n\to.mu.Lock()\n\tdefer o.mu.Unlock()\n\n\tif o.closed {\n\t\treturn nil, ErrStoreClosed\n\t}\n\n\tstate := &ConsumerState{}\n\n\tstate.Delivered = o.state.Delivered\n\tstate.AckFloor = o.state.AckFloor\n\tif len(o.state.Pending) > 0 {\n\t\tif doCopy {\n\t\t\tstate.Pending = o.copyPending()\n\t\t} else {\n\t\t\tstate.Pending = o.state.Pending\n\t\t}\n\t}\n\tif len(o.state.Redelivered) > 0 {\n\t\tif doCopy {\n\t\t\tstate.Redelivered = o.copyRedelivered()\n\t\t} else {\n\t\t\tstate.Redelivered = o.state.Redelivered\n\t\t}\n\t}\n\treturn state, nil\n}\n\n// EncodedState for this consumer store.\nfunc (o *consumerMemStore) EncodedState() ([]byte, error) {\n\to.mu.Lock()\n\tdefer o.mu.Unlock()\n\n\tif o.closed {\n\t\treturn nil, ErrStoreClosed\n\t}\n\n\treturn encodeConsumerState(&o.state), nil\n}\n\nfunc (o *consumerMemStore) copyPending() map[uint64]*Pending {\n\tpending := make(map[uint64]*Pending, len(o.state.Pending))\n\tfor seq, p := range o.state.Pending {\n\t\tpending[seq] = &Pending{p.Sequence, p.Timestamp}\n\t}\n\treturn pending\n}\n\nfunc (o *consumerMemStore) copyRedelivered() map[uint64]uint64 {\n\tredelivered := make(map[uint64]uint64, len(o.state.Redelivered))\n\tfor seq, dc := range o.state.Redelivered {\n\t\tredelivered[seq] = dc\n\t}\n\treturn redelivered\n}\n\n// Type returns the type of the underlying store.\nfunc (o *consumerMemStore) Type() StorageType { return MemoryStorage }\n\n// Templates\ntype templateMemStore struct{}\n\nfunc newTemplateMemStore() *templateMemStore {\n\treturn &templateMemStore{}\n}\n\n// No-ops for memstore.\nfunc (ts *templateMemStore) Store(t *streamTemplate) error  { return nil }\nfunc (ts *templateMemStore) Delete(t *streamTemplate) error { return nil }\n",
    "source_file": "server/memstore.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2012-2025 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage server\n\nimport (\n\t\"regexp\"\n\t\"runtime/debug\"\n\t\"time\"\n)\n\n// Command is a signal used to control a running nats-server process.\ntype Command string\n\n// Valid Command values.\nconst (\n\tCommandStop   = Command(\"stop\")\n\tCommandQuit   = Command(\"quit\")\n\tCommandReopen = Command(\"reopen\")\n\tCommandReload = Command(\"reload\")\n\n\t// private for now\n\tcommandLDMode = Command(\"ldm\")\n\tcommandTerm   = Command(\"term\")\n)\n\nvar (\n\t// gitCommit and serverVersion injected at build.\n\tgitCommit, serverVersion string\n\t// trustedKeys is a whitespace separated array of trusted operator's public nkeys.\n\ttrustedKeys string\n\t// SemVer regexp to validate the VERSION.\n\tsemVerRe = regexp.MustCompile(`^(0|[1-9]\\d*)\\.(0|[1-9]\\d*)\\.(0|[1-9]\\d*)(?:-((?:0|[1-9]\\d*|\\d*[a-zA-Z-][0-9a-zA-Z-]*)(?:\\.(?:0|[1-9]\\d*|\\d*[a-zA-Z-][0-9a-zA-Z-]*))*))?(?:\\+([0-9a-zA-Z-]+(?:\\.[0-9a-zA-Z-]+)*))?$`)\n)\n\nfunc init() {\n\t// Use build info if present, it would be if building using 'go build .'\n\t// or when using a release.\n\tif info, ok := debug.ReadBuildInfo(); ok {\n\t\tfor _, setting := range info.Settings {\n\t\t\tswitch setting.Key {\n\t\t\tcase \"vcs.revision\":\n\t\t\t\tgitCommit = setting.Value[:7]\n\t\t\t}\n\t\t}\n\t}\n}\n\nconst (\n\t// VERSION is the current version for the server.\n\tVERSION = \"2.12.0-dev\"\n\n\t// PROTO is the currently supported protocol.\n\t// 0 was the original\n\t// 1 maintains proto 0, adds echo abilities for CONNECT from the client. Clients\n\t// should not send echo unless proto in INFO is >= 1.\n\tPROTO = 1\n\n\t// DEFAULT_PORT is the default port for client connections.\n\tDEFAULT_PORT = 4222\n\n\t// RANDOM_PORT is the value for port that, when supplied, will cause the\n\t// server to listen on a randomly-chosen available port. The resolved port\n\t// is available via the Addr() method.\n\tRANDOM_PORT = -1\n\n\t// DEFAULT_HOST defaults to all interfaces.\n\tDEFAULT_HOST = \"0.0.0.0\"\n\n\t// MAX_CONTROL_LINE_SIZE is the maximum allowed protocol control line size.\n\t// 4k should be plenty since payloads sans connect/info string are separate.\n\tMAX_CONTROL_LINE_SIZE = 4096\n\n\t// MAX_PAYLOAD_SIZE is the maximum allowed payload size. Should be using\n\t// something different if > 1MB payloads are needed.\n\tMAX_PAYLOAD_SIZE = (1024 * 1024)\n\n\t// MAX_PAYLOAD_MAX_SIZE is the size at which the server will warn about\n\t// max_payload being too high. In the future, the server may enforce/reject\n\t// max_payload above this value.\n\tMAX_PAYLOAD_MAX_SIZE = (8 * 1024 * 1024)\n\n\t// MAX_PENDING_SIZE is the maximum outbound pending bytes per client.\n\tMAX_PENDING_SIZE = (64 * 1024 * 1024)\n\n\t// DEFAULT_MAX_CONNECTIONS is the default maximum connections allowed.\n\tDEFAULT_MAX_CONNECTIONS = (64 * 1024)\n\n\t// TLS_TIMEOUT is the TLS wait time.\n\tTLS_TIMEOUT = 2 * time.Second\n\n\t// DEFAULT_TLS_HANDSHAKE_FIRST_FALLBACK_DELAY is the default amount of\n\t// time for the server to wait for the TLS handshake with a client to\n\t// be initiated before falling back to sending the INFO protocol first.\n\t// See TLSHandshakeFirst and TLSHandshakeFirstFallback options.\n\tDEFAULT_TLS_HANDSHAKE_FIRST_FALLBACK_DELAY = 50 * time.Millisecond\n\n\t// AUTH_TIMEOUT is the authorization wait time.\n\tAUTH_TIMEOUT = 2 * time.Second\n\n\t// DEFAULT_PING_INTERVAL is how often pings are sent to clients, etc...\n\tDEFAULT_PING_INTERVAL = 2 * time.Minute\n\n\t// DEFAULT_PING_MAX_OUT is maximum allowed pings outstanding before disconnect.\n\tDEFAULT_PING_MAX_OUT = 2\n\n\t// CR_LF string\n\tCR_LF = \"\\r\\n\"\n\n\t// LEN_CR_LF hold onto the computed size.\n\tLEN_CR_LF = len(CR_LF)\n\n\t// DEFAULT_FLUSH_DEADLINE is the write/flush deadlines.\n\tDEFAULT_FLUSH_DEADLINE = 10 * time.Second\n\n\t// DEFAULT_HTTP_PORT is the default monitoring port.\n\tDEFAULT_HTTP_PORT = 8222\n\n\t// DEFAULT_HTTP_BASE_PATH is the default base path for monitoring.\n\tDEFAULT_HTTP_BASE_PATH = \"/\"\n\n\t// ACCEPT_MIN_SLEEP is the minimum acceptable sleep times on temporary errors.\n\tACCEPT_MIN_SLEEP = 10 * time.Millisecond\n\n\t// ACCEPT_MAX_SLEEP is the maximum acceptable sleep times on temporary errors\n\tACCEPT_MAX_SLEEP = 1 * time.Second\n\n\t// DEFAULT_ROUTE_CONNECT Route solicitation intervals.\n\tDEFAULT_ROUTE_CONNECT = 1 * time.Second\n\n\t// DEFAULT_ROUTE_RECONNECT Route reconnect intervals.\n\tDEFAULT_ROUTE_RECONNECT = 1 * time.Second\n\n\t// DEFAULT_ROUTE_DIAL Route dial timeout.\n\tDEFAULT_ROUTE_DIAL = 1 * time.Second\n\n\t// DEFAULT_ROUTE_POOL_SIZE Route default pool size\n\tDEFAULT_ROUTE_POOL_SIZE = 3\n\n\t// DEFAULT_LEAF_NODE_RECONNECT LeafNode reconnect interval.\n\tDEFAULT_LEAF_NODE_RECONNECT = time.Second\n\n\t// DEFAULT_LEAF_TLS_TIMEOUT TLS timeout for LeafNodes\n\tDEFAULT_LEAF_TLS_TIMEOUT = 2 * time.Second\n\n\t// PROTO_SNIPPET_SIZE is the default size of proto to print on parse errors.\n\tPROTO_SNIPPET_SIZE = 32\n\n\t// MAX_CONTROL_LINE_SNIPPET_SIZE is the default size of proto to print on max control line errors.\n\tMAX_CONTROL_LINE_SNIPPET_SIZE = 128\n\n\t// MAX_MSG_ARGS Maximum possible number of arguments from MSG proto.\n\tMAX_MSG_ARGS = 4\n\n\t// MAX_RMSG_ARGS Maximum possible number of arguments from RMSG proto.\n\tMAX_RMSG_ARGS = 6\n\n\t// MAX_HMSG_ARGS Maximum possible number of arguments from HMSG proto.\n\tMAX_HMSG_ARGS = 7\n\n\t// MAX_PUB_ARGS Maximum possible number of arguments from PUB proto.\n\tMAX_PUB_ARGS = 3\n\n\t// MAX_HPUB_ARGS Maximum possible number of arguments from HPUB proto.\n\tMAX_HPUB_ARGS = 4\n\n\t// MAX_RSUB_ARGS Maximum possible number of arguments from a RS+/LS+ proto.\n\tMAX_RSUB_ARGS = 6\n\n\t// DEFAULT_MAX_CLOSED_CLIENTS is the maximum number of closed connections we hold onto.\n\tDEFAULT_MAX_CLOSED_CLIENTS = 10000\n\n\t// DEFAULT_LAME_DUCK_DURATION is the time in which the server spreads\n\t// the closing of clients when signaled to go in lame duck mode.\n\tDEFAULT_LAME_DUCK_DURATION = 2 * time.Minute\n\n\t// DEFAULT_LAME_DUCK_GRACE_PERIOD is the duration the server waits, after entering\n\t// lame duck mode, before starting closing client connections.\n\tDEFAULT_LAME_DUCK_GRACE_PERIOD = 10 * time.Second\n\n\t// DEFAULT_LEAFNODE_INFO_WAIT Route dial timeout.\n\tDEFAULT_LEAFNODE_INFO_WAIT = 1 * time.Second\n\n\t// DEFAULT_LEAFNODE_PORT is the default port for remote leafnode connections.\n\tDEFAULT_LEAFNODE_PORT = 7422\n\n\t// DEFAULT_CONNECT_ERROR_REPORTS is the number of attempts at which a\n\t// repeated failed route, gateway or leaf node connection is reported.\n\t// This is used for initial connection, that is, when the server has\n\t// never had a connection to the given endpoint. Once connected, and\n\t// if a disconnect occurs, DEFAULT_RECONNECT_ERROR_REPORTS is used\n\t// instead.\n\t// The default is to report every 3600 attempts (roughly every hour).\n\tDEFAULT_CONNECT_ERROR_REPORTS = 3600\n\n\t// DEFAULT_RECONNECT_ERROR_REPORTS is the default number of failed\n\t// attempt to reconnect a route, gateway or leaf node connection.\n\t// The default is to report every attempt.\n\tDEFAULT_RECONNECT_ERROR_REPORTS = 1\n\n\t// DEFAULT_RTT_MEASUREMENT_INTERVAL is how often we want to measure RTT from\n\t// this server to clients, routes, gateways or leafnode connections.\n\tDEFAULT_RTT_MEASUREMENT_INTERVAL = time.Hour\n\n\t// DEFAULT_ALLOW_RESPONSE_MAX_MSGS is the default number of responses allowed\n\t// for a reply subject.\n\tDEFAULT_ALLOW_RESPONSE_MAX_MSGS = 1\n\n\t// DEFAULT_ALLOW_RESPONSE_EXPIRATION is the default time allowed for a given\n\t// dynamic response permission.\n\tDEFAULT_ALLOW_RESPONSE_EXPIRATION = 2 * time.Minute\n\n\t// DEFAULT_SERVICE_EXPORT_RESPONSE_THRESHOLD is the default time that the system will\n\t// expect a service export response to be delivered. This is used in corner cases for\n\t// time based cleanup of reverse mapping structures.\n\tDEFAULT_SERVICE_EXPORT_RESPONSE_THRESHOLD = 2 * time.Minute\n\n\t// DEFAULT_SERVICE_LATENCY_SAMPLING is the default sampling rate for service\n\t// latency metrics\n\tDEFAULT_SERVICE_LATENCY_SAMPLING = 100\n\n\t// DEFAULT_SYSTEM_ACCOUNT\n\tDEFAULT_SYSTEM_ACCOUNT = \"$SYS\"\n\n\t// DEFAULT GLOBAL_ACCOUNT\n\tDEFAULT_GLOBAL_ACCOUNT = \"$G\"\n\n\t// DEFAULT_FETCH_TIMEOUT is the default time that the system will wait for an account fetch to return.\n\tDEFAULT_ACCOUNT_FETCH_TIMEOUT = 1900 * time.Millisecond\n)\n",
    "source_file": "server/const.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2018-2025 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage server\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\t\"net\"\n\t\"os\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/nats-io/jwt/v2\"\n\t\"github.com/nats-io/nkeys\"\n)\n\n// All JWTs once encoded start with this\nconst jwtPrefix = \"eyJ\"\n\n// ReadOperatorJWT will read a jwt file for an operator claim. This can be a decorated file.\nfunc ReadOperatorJWT(jwtfile string) (*jwt.OperatorClaims, error) {\n\t_, claim, err := readOperatorJWT(jwtfile)\n\treturn claim, err\n}\n\nfunc readOperatorJWT(jwtfile string) (string, *jwt.OperatorClaims, error) {\n\tcontents, err := os.ReadFile(jwtfile)\n\tif err != nil {\n\t\t// Check to see if the JWT has been inlined.\n\t\tif !strings.HasPrefix(jwtfile, jwtPrefix) {\n\t\t\treturn \"\", nil, err\n\t\t}\n\t\t// We may have an inline jwt here.\n\t\tcontents = []byte(jwtfile)\n\t}\n\tdefer wipeSlice(contents)\n\n\ttheJWT, err := jwt.ParseDecoratedJWT(contents)\n\tif err != nil {\n\t\treturn \"\", nil, err\n\t}\n\topc, err := jwt.DecodeOperatorClaims(theJWT)\n\tif err != nil {\n\t\treturn \"\", nil, err\n\t}\n\treturn theJWT, opc, nil\n}\n\n// Just wipe slice with 'x', for clearing contents of nkey seed file.\nfunc wipeSlice(buf []byte) {\n\tfor i := range buf {\n\t\tbuf[i] = 'x'\n\t}\n}\n\n// validateTrustedOperators will check that we do not have conflicts with\n// assigned trusted keys and trusted operators. If operators are defined we\n// will expand the trusted keys in options.\nfunc validateTrustedOperators(o *Options) error {\n\tif len(o.TrustedOperators) == 0 {\n\t\t// if we have no operator, default sentinel shouldn't be set\n\t\tif o.DefaultSentinel != \"\" {\n\t\t\treturn fmt.Errorf(\"default sentinel requires operators and accounts\")\n\t\t}\n\t\treturn nil\n\t}\n\tif o.AccountResolver == nil {\n\t\treturn fmt.Errorf(\"operators require an account resolver to be configured\")\n\t}\n\tif len(o.Accounts) > 0 {\n\t\treturn fmt.Errorf(\"operators do not allow Accounts to be configured directly\")\n\t}\n\tif len(o.Users) > 0 || len(o.Nkeys) > 0 {\n\t\treturn fmt.Errorf(\"operators do not allow users to be configured directly\")\n\t}\n\tif len(o.TrustedOperators) > 0 && len(o.TrustedKeys) > 0 {\n\t\treturn fmt.Errorf(\"conflicting options for 'TrustedKeys' and 'TrustedOperators'\")\n\t}\n\tif o.SystemAccount != _EMPTY_ {\n\t\tfoundSys := false\n\t\tfoundNonEmpty := false\n\t\tfor _, op := range o.TrustedOperators {\n\t\t\tif op.SystemAccount != _EMPTY_ {\n\t\t\t\tfoundNonEmpty = true\n\t\t\t}\n\t\t\tif op.SystemAccount == o.SystemAccount {\n\t\t\t\tfoundSys = true\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\tif foundNonEmpty && !foundSys {\n\t\t\treturn fmt.Errorf(\"system_account in config and operator JWT must be identical\")\n\t\t}\n\t} else if o.TrustedOperators[0].SystemAccount == _EMPTY_ {\n\t\t// In case the system account is neither defined in config nor in the first operator.\n\t\t// If it would be needed due to the nats account resolver, raise an error.\n\t\tswitch o.AccountResolver.(type) {\n\t\tcase *DirAccResolver, *CacheDirAccResolver:\n\t\t\treturn fmt.Errorf(\"using nats based account resolver - the system account needs to be specified in configuration or the operator jwt\")\n\t\t}\n\t}\n\n\tsrvMajor, srvMinor, srvUpdate, _ := versionComponents(VERSION)\n\tfor _, opc := range o.TrustedOperators {\n\t\tif major, minor, update, err := jwt.ParseServerVersion(opc.AssertServerVersion); err != nil {\n\t\t\treturn fmt.Errorf(\"operator %s expects version %s got error instead: %s\",\n\t\t\t\topc.Subject, opc.AssertServerVersion, err)\n\t\t} else if major > srvMajor {\n\t\t\treturn fmt.Errorf(\"operator %s expected major version %d > server major version %d\",\n\t\t\t\topc.Subject, major, srvMajor)\n\t\t} else if srvMajor > major {\n\t\t} else if minor > srvMinor {\n\t\t\treturn fmt.Errorf(\"operator %s expected minor version %d > server minor version %d\",\n\t\t\t\topc.Subject, minor, srvMinor)\n\t\t} else if srvMinor > minor {\n\t\t} else if update > srvUpdate {\n\t\t\treturn fmt.Errorf(\"operator %s expected update version %d > server update version %d\",\n\t\t\t\topc.Subject, update, srvUpdate)\n\t\t}\n\t}\n\t// If we have operators, fill in the trusted keys.\n\t// FIXME(dlc) - We had TrustedKeys before TrustedOperators. The jwt.OperatorClaims\n\t// has a DidSign(). Use that longer term. For now we can expand in place.\n\tfor _, opc := range o.TrustedOperators {\n\t\tif o.TrustedKeys == nil {\n\t\t\to.TrustedKeys = make([]string, 0, 4)\n\t\t}\n\t\tif !opc.StrictSigningKeyUsage {\n\t\t\to.TrustedKeys = append(o.TrustedKeys, opc.Subject)\n\t\t}\n\t\to.TrustedKeys = append(o.TrustedKeys, opc.SigningKeys...)\n\t}\n\tfor _, key := range o.TrustedKeys {\n\t\tif !nkeys.IsValidPublicOperatorKey(key) {\n\t\t\treturn fmt.Errorf(\"trusted Keys %q are required to be a valid public operator nkey\", key)\n\t\t}\n\t}\n\tif len(o.resolverPinnedAccounts) > 0 {\n\t\tfor key := range o.resolverPinnedAccounts {\n\t\t\tif !nkeys.IsValidPublicAccountKey(key) {\n\t\t\t\treturn fmt.Errorf(\"pinned account key %q is not a valid public account nkey\", key)\n\t\t\t}\n\t\t}\n\t\t// ensure the system account (belonging to the operator can always connect)\n\t\tif o.SystemAccount != _EMPTY_ {\n\t\t\to.resolverPinnedAccounts[o.SystemAccount] = struct{}{}\n\t\t}\n\t}\n\n\t// If we have an auth callout defined make sure we are not in operator mode.\n\tif o.AuthCallout != nil {\n\t\treturn errors.New(\"operators do not allow authorization callouts to be configured directly\")\n\t}\n\n\treturn nil\n}\n\nfunc validateSrc(claims *jwt.UserClaims, host string) bool {\n\tif claims == nil {\n\t\treturn false\n\t} else if len(claims.Src) == 0 {\n\t\treturn true\n\t} else if host == \"\" {\n\t\treturn false\n\t}\n\tip := net.ParseIP(host)\n\tif ip == nil {\n\t\treturn false\n\t}\n\tfor _, cidr := range claims.Src {\n\t\tif _, net, err := net.ParseCIDR(cidr); err != nil {\n\t\t\treturn false // should not happen as this jwt is invalid\n\t\t} else if net.Contains(ip) {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\nfunc validateTimes(claims *jwt.UserClaims) (bool, time.Duration) {\n\tif claims == nil {\n\t\treturn false, time.Duration(0)\n\t} else if len(claims.Times) == 0 {\n\t\treturn true, time.Duration(0)\n\t}\n\tnow := time.Now()\n\tloc := time.Local\n\tif claims.Locale != \"\" {\n\t\tvar err error\n\t\tif loc, err = time.LoadLocation(claims.Locale); err != nil {\n\t\t\treturn false, time.Duration(0) // parsing not expected to fail at this point\n\t\t}\n\t\tnow = now.In(loc)\n\t}\n\tfor _, timeRange := range claims.Times {\n\t\ty, m, d := now.Date()\n\t\tm = m - 1\n\t\td = d - 1\n\t\tstart, err := time.ParseInLocation(\"15:04:05\", timeRange.Start, loc)\n\t\tif err != nil {\n\t\t\treturn false, time.Duration(0) // parsing not expected to fail at this point\n\t\t}\n\t\tend, err := time.ParseInLocation(\"15:04:05\", timeRange.End, loc)\n\t\tif err != nil {\n\t\t\treturn false, time.Duration(0) // parsing not expected to fail at this point\n\t\t}\n\t\tif start.After(end) {\n\t\t\tstart = start.AddDate(y, int(m), d)\n\t\t\td++ // the intent is to be the next day\n\t\t} else {\n\t\t\tstart = start.AddDate(y, int(m), d)\n\t\t}\n\t\tif start.Before(now) {\n\t\t\tend = end.AddDate(y, int(m), d)\n\t\t\tif end.After(now) {\n\t\t\t\treturn true, end.Sub(now)\n\t\t\t}\n\t\t}\n\t}\n\treturn false, time.Duration(0)\n}\n",
    "source_file": "server/jwt.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2018-2025 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage server\n\n// We wrap to hold onto optional items for /connz.\ntype closedClient struct {\n\tConnInfo\n\tsubs []SubDetail\n\tuser string\n\tacc  string\n}\n\n// Fixed sized ringbuffer for closed connections.\ntype closedRingBuffer struct {\n\ttotal uint64\n\tconns []*closedClient\n}\n\n// Create a new ring buffer with at most max items.\nfunc newClosedRingBuffer(max int) *closedRingBuffer {\n\trb := &closedRingBuffer{}\n\trb.conns = make([]*closedClient, max)\n\treturn rb\n}\n\n// Adds in a new closed connection. If there is no more room,\n// remove the oldest.\nfunc (rb *closedRingBuffer) append(cc *closedClient) {\n\trb.conns[rb.next()] = cc\n\trb.total++\n}\n\nfunc (rb *closedRingBuffer) next() int {\n\treturn int(rb.total % uint64(cap(rb.conns)))\n}\n\nfunc (rb *closedRingBuffer) len() int {\n\tif rb.total > uint64(cap(rb.conns)) {\n\t\treturn cap(rb.conns)\n\t}\n\treturn int(rb.total)\n}\n\nfunc (rb *closedRingBuffer) totalConns() uint64 {\n\treturn rb.total\n}\n\n// This will return a sorted copy of the list which recipient can\n// modify. If the contents of the client itself need to be modified,\n// meaning swapping in any optional items, a copy should be made. We\n// could introduce a new lock and hold that but since we return this\n// list inside monitor which allows programatic access, we do not\n// know when it would be done.\nfunc (rb *closedRingBuffer) closedClients() []*closedClient {\n\tdup := make([]*closedClient, rb.len())\n\thead := rb.next()\n\tif rb.total <= uint64(cap(rb.conns)) || head == 0 {\n\t\tcopy(dup, rb.conns[:rb.len()])\n\t} else {\n\t\tfp := rb.conns[head:]\n\t\tsp := rb.conns[:head]\n\t\tcopy(dup, fp)\n\t\tcopy(dup[len(fp):], sp)\n\t}\n\treturn dup\n}\n",
    "source_file": "server/ring.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2025 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage server\n\nimport \"time\"\n\n// SDMMeta holds pending/proposed data for subject delete markers or message removals.\ntype SDMMeta struct {\n\ttotals  map[string]uint64\n\tpending map[uint64]SDMBySeq\n}\n\n// SDMBySeq holds data for a message with a specific sequence.\ntype SDMBySeq struct {\n\tlast bool  // Whether the message for this sequence was the last for this subject.\n\tts   int64 // Last timestamp we proposed a removal/sdm.\n}\n\n// SDMBySubj holds whether a message for a specific subject and sequence was a subject delete marker or not.\ntype SDMBySubj struct {\n\tseq uint64\n\tsdm bool\n}\n\nfunc newSDMMeta() *SDMMeta {\n\treturn &SDMMeta{\n\t\ttotals:  make(map[string]uint64, 1),\n\t\tpending: make(map[uint64]SDMBySeq, 1),\n\t}\n}\n\n// empty clears all data.\nfunc (sdm *SDMMeta) empty() {\n\tif sdm == nil {\n\t\treturn\n\t}\n\tclear(sdm.totals)\n\tclear(sdm.pending)\n}\n\n// trackPending caches the given seq and subj and whether it's the last message for that subject.\nfunc (sdm *SDMMeta) trackPending(seq uint64, subj string, last bool) bool {\n\tif p, ok := sdm.pending[seq]; ok {\n\t\treturn p.last\n\t}\n\tsdm.pending[seq] = SDMBySeq{last, time.Now().UnixNano()}\n\tsdm.totals[subj]++\n\treturn last\n}\n\n// removeSeqAndSubject clears the seq and subj from the cache.\nfunc (sdm *SDMMeta) removeSeqAndSubject(seq uint64, subj string) {\n\tif sdm == nil {\n\t\treturn\n\t}\n\tif _, ok := sdm.pending[seq]; ok {\n\t\tdelete(sdm.pending, seq)\n\t\tif msgs, ok := sdm.totals[subj]; ok {\n\t\t\tif msgs <= 1 {\n\t\t\t\tdelete(sdm.totals, subj)\n\t\t\t} else {\n\t\t\t\tsdm.totals[subj] = msgs - 1\n\t\t\t}\n\t\t}\n\t}\n}\n",
    "source_file": "server/sdm.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2024 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n// Inspired by https://github.com/protocolbuffers/protobuf-go/blob/master/encoding/protowire/wire.go\n\npackage server\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\t\"math\"\n)\n\nvar errProtoInsufficient = errors.New(\"insufficient data to read a value\")\nvar errProtoOverflow = errors.New(\"too much data for a value\")\nvar errProtoInvalidFieldNumber = errors.New(\"invalid field number\")\n\nfunc protoScanField(b []byte) (num, typ, size int, err error) {\n\tnum, typ, sizeTag, err := protoScanTag(b)\n\tif err != nil {\n\t\treturn 0, 0, 0, err\n\t}\n\tb = b[sizeTag:]\n\n\tsizeValue, err := protoScanFieldValue(typ, b)\n\tif err != nil {\n\t\treturn 0, 0, 0, err\n\t}\n\treturn num, typ, sizeTag + sizeValue, nil\n}\n\nfunc protoScanTag(b []byte) (num, typ, size int, err error) {\n\ttagint, size, err := protoScanVarint(b)\n\tif err != nil {\n\t\treturn 0, 0, 0, err\n\t}\n\n\t// NOTE: MessageSet allows for larger field numbers than normal.\n\tif (tagint >> 3) > uint64(math.MaxInt32) {\n\t\treturn 0, 0, 0, errProtoInvalidFieldNumber\n\t}\n\tnum = int(tagint >> 3)\n\tif num < 1 {\n\t\treturn 0, 0, 0, errProtoInvalidFieldNumber\n\t}\n\ttyp = int(tagint & 7)\n\n\treturn num, typ, size, nil\n}\n\nfunc protoScanFieldValue(typ int, b []byte) (size int, err error) {\n\tswitch typ {\n\tcase 0:\n\t\t_, size, err = protoScanVarint(b)\n\tcase 5: // fixed32\n\t\tsize = 4\n\tcase 1: // fixed64\n\t\tsize = 8\n\tcase 2: // length-delimited\n\t\tsize, err = protoScanBytes(b)\n\tdefault:\n\t\treturn 0, fmt.Errorf(\"unsupported type: %d\", typ)\n\t}\n\treturn size, err\n}\n\nfunc protoScanVarint(b []byte) (v uint64, size int, err error) {\n\tvar y uint64\n\tif len(b) <= 0 {\n\t\treturn 0, 0, errProtoInsufficient\n\t}\n\tv = uint64(b[0])\n\tif v < 0x80 {\n\t\treturn v, 1, nil\n\t}\n\tv -= 0x80\n\n\tif len(b) <= 1 {\n\t\treturn 0, 0, errProtoInsufficient\n\t}\n\ty = uint64(b[1])\n\tv += y << 7\n\tif y < 0x80 {\n\t\treturn v, 2, nil\n\t}\n\tv -= 0x80 << 7\n\n\tif len(b) <= 2 {\n\t\treturn 0, 0, errProtoInsufficient\n\t}\n\ty = uint64(b[2])\n\tv += y << 14\n\tif y < 0x80 {\n\t\treturn v, 3, nil\n\t}\n\tv -= 0x80 << 14\n\n\tif len(b) <= 3 {\n\t\treturn 0, 0, errProtoInsufficient\n\t}\n\ty = uint64(b[3])\n\tv += y << 21\n\tif y < 0x80 {\n\t\treturn v, 4, nil\n\t}\n\tv -= 0x80 << 21\n\n\tif len(b) <= 4 {\n\t\treturn 0, 0, errProtoInsufficient\n\t}\n\ty = uint64(b[4])\n\tv += y << 28\n\tif y < 0x80 {\n\t\treturn v, 5, nil\n\t}\n\tv -= 0x80 << 28\n\n\tif len(b) <= 5 {\n\t\treturn 0, 0, errProtoInsufficient\n\t}\n\ty = uint64(b[5])\n\tv += y << 35\n\tif y < 0x80 {\n\t\treturn v, 6, nil\n\t}\n\tv -= 0x80 << 35\n\n\tif len(b) <= 6 {\n\t\treturn 0, 0, errProtoInsufficient\n\t}\n\ty = uint64(b[6])\n\tv += y << 42\n\tif y < 0x80 {\n\t\treturn v, 7, nil\n\t}\n\tv -= 0x80 << 42\n\n\tif len(b) <= 7 {\n\t\treturn 0, 0, errProtoInsufficient\n\t}\n\ty = uint64(b[7])\n\tv += y << 49\n\tif y < 0x80 {\n\t\treturn v, 8, nil\n\t}\n\tv -= 0x80 << 49\n\n\tif len(b) <= 8 {\n\t\treturn 0, 0, errProtoInsufficient\n\t}\n\ty = uint64(b[8])\n\tv += y << 56\n\tif y < 0x80 {\n\t\treturn v, 9, nil\n\t}\n\tv -= 0x80 << 56\n\n\tif len(b) <= 9 {\n\t\treturn 0, 0, errProtoInsufficient\n\t}\n\ty = uint64(b[9])\n\tv += y << 63\n\tif y < 2 {\n\t\treturn v, 10, nil\n\t}\n\treturn 0, 0, errProtoOverflow\n}\n\nfunc protoScanBytes(b []byte) (size int, err error) {\n\tl, lenSize, err := protoScanVarint(b)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\tif l > uint64(len(b[lenSize:])) {\n\t\treturn 0, errProtoInsufficient\n\t}\n\treturn lenSize + int(l), nil\n}\n\nfunc protoEncodeVarint(v uint64) []byte {\n\tb := make([]byte, 0, 10)\n\tswitch {\n\tcase v < 1<<7:\n\t\tb = append(b, byte(v))\n\tcase v < 1<<14:\n\t\tb = append(b,\n\t\t\tbyte((v>>0)&0x7f|0x80),\n\t\t\tbyte(v>>7))\n\tcase v < 1<<21:\n\t\tb = append(b,\n\t\t\tbyte((v>>0)&0x7f|0x80),\n\t\t\tbyte((v>>7)&0x7f|0x80),\n\t\t\tbyte(v>>14))\n\tcase v < 1<<28:\n\t\tb = append(b,\n\t\t\tbyte((v>>0)&0x7f|0x80),\n\t\t\tbyte((v>>7)&0x7f|0x80),\n\t\t\tbyte((v>>14)&0x7f|0x80),\n\t\t\tbyte(v>>21))\n\tcase v < 1<<35:\n\t\tb = append(b,\n\t\t\tbyte((v>>0)&0x7f|0x80),\n\t\t\tbyte((v>>7)&0x7f|0x80),\n\t\t\tbyte((v>>14)&0x7f|0x80),\n\t\t\tbyte((v>>21)&0x7f|0x80),\n\t\t\tbyte(v>>28))\n\tcase v < 1<<42:\n\t\tb = append(b,\n\t\t\tbyte((v>>0)&0x7f|0x80),\n\t\t\tbyte((v>>7)&0x7f|0x80),\n\t\t\tbyte((v>>14)&0x7f|0x80),\n\t\t\tbyte((v>>21)&0x7f|0x80),\n\t\t\tbyte((v>>28)&0x7f|0x80),\n\t\t\tbyte(v>>35))\n\tcase v < 1<<49:\n\t\tb = append(b,\n\t\t\tbyte((v>>0)&0x7f|0x80),\n\t\t\tbyte((v>>7)&0x7f|0x80),\n\t\t\tbyte((v>>14)&0x7f|0x80),\n\t\t\tbyte((v>>21)&0x7f|0x80),\n\t\t\tbyte((v>>28)&0x7f|0x80),\n\t\t\tbyte((v>>35)&0x7f|0x80),\n\t\t\tbyte(v>>42))\n\tcase v < 1<<56:\n\t\tb = append(b,\n\t\t\tbyte((v>>0)&0x7f|0x80),\n\t\t\tbyte((v>>7)&0x7f|0x80),\n\t\t\tbyte((v>>14)&0x7f|0x80),\n\t\t\tbyte((v>>21)&0x7f|0x80),\n\t\t\tbyte((v>>28)&0x7f|0x80),\n\t\t\tbyte((v>>35)&0x7f|0x80),\n\t\t\tbyte((v>>42)&0x7f|0x80),\n\t\t\tbyte(v>>49))\n\tcase v < 1<<63:\n\t\tb = append(b,\n\t\t\tbyte((v>>0)&0x7f|0x80),\n\t\t\tbyte((v>>7)&0x7f|0x80),\n\t\t\tbyte((v>>14)&0x7f|0x80),\n\t\t\tbyte((v>>21)&0x7f|0x80),\n\t\t\tbyte((v>>28)&0x7f|0x80),\n\t\t\tbyte((v>>35)&0x7f|0x80),\n\t\t\tbyte((v>>42)&0x7f|0x80),\n\t\t\tbyte((v>>49)&0x7f|0x80),\n\t\t\tbyte(v>>56))\n\tdefault:\n\t\tb = append(b,\n\t\t\tbyte((v>>0)&0x7f|0x80),\n\t\t\tbyte((v>>7)&0x7f|0x80),\n\t\t\tbyte((v>>14)&0x7f|0x80),\n\t\t\tbyte((v>>21)&0x7f|0x80),\n\t\t\tbyte((v>>28)&0x7f|0x80),\n\t\t\tbyte((v>>35)&0x7f|0x80),\n\t\t\tbyte((v>>42)&0x7f|0x80),\n\t\t\tbyte((v>>49)&0x7f|0x80),\n\t\t\tbyte((v>>56)&0x7f|0x80),\n\t\t\t1)\n\t}\n\treturn b\n}\n",
    "source_file": "server/proto.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2020-2025 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n//go:build gofuzz\n\npackage server\n\nvar defaultFuzzServerOptions = Options{\n\tHost:                  \"127.0.0.1\",\n\tTrace:                 true,\n\tDebug:                 true,\n\tDisableShortFirstPing: true,\n\tNoLog:                 true,\n\tNoSigs:                true,\n}\n\nfunc dummyFuzzClient() *client {\n\treturn &client{srv: New(&defaultFuzzServerOptions), msubs: -1, mpay: MAX_PAYLOAD_SIZE, mcl: MAX_CONTROL_LINE_SIZE}\n}\n\nfunc FuzzClient(data []byte) int {\n\tif len(data) < 100 {\n\t\treturn -1\n\t}\n\tc := dummyFuzzClient()\n\n\terr := c.parse(data[:50])\n\tif err != nil {\n\t\treturn 0\n\t}\n\n\terr = c.parse(data[50:])\n\tif err != nil {\n\t\treturn 0\n\t}\n\treturn 1\n}\n",
    "source_file": "server/fuzz.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2020-2025 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage server\n\nimport (\n\t\"bytes\"\n\t\"crypto/sha256\"\n\t\"encoding/binary\"\n\t\"errors\"\n\t\"fmt\"\n\t\"hash\"\n\t\"math\"\n\t\"math/rand\"\n\t\"net\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"runtime\"\n\t\"strings\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t\"github.com/nats-io/nats-server/v2/internal/fastrand\"\n\n\t\"github.com/minio/highwayhash\"\n)\n\ntype RaftNode interface {\n\tPropose(entry []byte) error\n\tProposeMulti(entries []*Entry) error\n\tForwardProposal(entry []byte) error\n\tInstallSnapshot(snap []byte) error\n\tSendSnapshot(snap []byte) error\n\tNeedSnapshot() bool\n\tApplied(index uint64) (entries uint64, bytes uint64)\n\tState() RaftState\n\tSize() (entries, bytes uint64)\n\tProgress() (index, commit, applied uint64)\n\tLeader() bool\n\tQuorum() bool\n\tCurrent() bool\n\tHealthy() bool\n\tTerm() uint64\n\tLeaderless() bool\n\tGroupLeader() string\n\tHadPreviousLeader() bool\n\tStepDown(preferred ...string) error\n\tSetObserver(isObserver bool)\n\tIsObserver() bool\n\tCampaign() error\n\tCampaignImmediately() error\n\tID() string\n\tGroup() string\n\tPeers() []*Peer\n\tProposeKnownPeers(knownPeers []string)\n\tUpdateKnownPeers(knownPeers []string)\n\tProposeAddPeer(peer string) error\n\tProposeRemovePeer(peer string) error\n\tAdjustClusterSize(csz int) error\n\tAdjustBootClusterSize(csz int) error\n\tClusterSize() int\n\tApplyQ() *ipQueue[*CommittedEntry]\n\tPauseApply() error\n\tResumeApply()\n\tLeadChangeC() <-chan bool\n\tQuitC() <-chan struct{}\n\tCreated() time.Time\n\tStop()\n\tWaitForStop()\n\tDelete()\n\tRecreateInternalSubs() error\n\tIsSystemAccount() bool\n}\n\ntype WAL interface {\n\tType() StorageType\n\tStoreMsg(subj string, hdr, msg []byte, ttl int64) (uint64, int64, error)\n\tLoadMsg(index uint64, sm *StoreMsg) (*StoreMsg, error)\n\tRemoveMsg(index uint64) (bool, error)\n\tCompact(index uint64) (uint64, error)\n\tPurge() (uint64, error)\n\tPurgeEx(subject string, seq, keep uint64) (uint64, error)\n\tTruncate(seq uint64) error\n\tState() StreamState\n\tFastState(*StreamState)\n\tStop() error\n\tDelete() error\n}\n\ntype Peer struct {\n\tID      string\n\tCurrent bool\n\tLast    time.Time\n\tLag     uint64\n}\n\ntype RaftState uint8\n\n// Allowable states for a NATS Consensus Group.\nconst (\n\tFollower RaftState = iota\n\tLeader\n\tCandidate\n\tClosed\n)\n\nfunc (state RaftState) String() string {\n\tswitch state {\n\tcase Follower:\n\t\treturn \"FOLLOWER\"\n\tcase Candidate:\n\t\treturn \"CANDIDATE\"\n\tcase Leader:\n\t\treturn \"LEADER\"\n\tcase Closed:\n\t\treturn \"CLOSED\"\n\t}\n\treturn \"UNKNOWN\"\n}\n\ntype raft struct {\n\tsync.RWMutex\n\n\tcreated time.Time      // Time that the group was created\n\taccName string         // Account name of the asset this raft group is for\n\tacc     *Account       // Account that NRG traffic will be sent/received in\n\tgroup   string         // Raft group\n\tsd      string         // Store directory\n\tid      string         // Node ID\n\twg      sync.WaitGroup // Wait for running goroutines to exit on shutdown\n\n\twal   WAL         // WAL store (filestore or memstore)\n\twtype StorageType // WAL type, e.g. FileStorage or MemoryStorage\n\ttrack bool        //\n\twerr  error       // Last write error\n\n\tstate       atomic.Int32 // RaftState\n\tleaderState atomic.Bool  // Is in (complete) leader state.\n\thh          hash.Hash64  // Highwayhash, used for snapshots\n\tsnapfile    string       // Snapshot filename\n\n\tcsz   int             // Cluster size\n\tqn    int             // Number of nodes needed to establish quorum\n\tpeers map[string]*lps // Other peers in the Raft group\n\n\tremoved map[string]time.Time           // Peers that were removed from the group\n\tacks    map[uint64]map[string]struct{} // Append entry responses/acks, map of entry index -> peer ID\n\tpae     map[uint64]*appendEntry        // Pending append entries\n\n\telect  *time.Timer // Election timer, normally accessed via electTimer\n\tetlr   time.Time   // Election timer last reset time, for unit tests only\n\tactive time.Time   // Last activity time, i.e. for heartbeats\n\tllqrt  time.Time   // Last quorum lost time\n\tlsut   time.Time   // Last scale-up time\n\n\tterm    uint64 // The current vote term\n\tpterm   uint64 // Previous term from the last snapshot\n\tpindex  uint64 // Previous index from the last snapshot\n\tcommit  uint64 // Index of the most recent commit\n\tapplied uint64 // Index of the most recently applied commit\n\n\taflr uint64 // Index when to signal initial messages have been applied after becoming leader. 0 means signaling is disabled.\n\n\tleader string // The ID of the leader\n\tvote   string // Our current vote state\n\tlxfer  bool   // Are we doing a leadership transfer?\n\n\thcbehind bool // Were we falling behind at the last health check? (see: isCurrent)\n\n\ts  *Server    // Reference to top-level server\n\tc  *client    // Internal client for subscriptions\n\tjs *jetStream // JetStream, if running, to see if we are out of resources\n\n\tdflag       bool        // Debug flag\n\thasleader   atomic.Bool // Is there a group leader right now?\n\tpleader     atomic.Bool // Has the group ever had a leader?\n\tisSysAcc    atomic.Bool // Are we utilizing the system account?\n\tmaybeLeader bool        // The group had a preferred leader. And is maybe already acting as leader prior to scale up.\n\n\tobserver bool // The node is observing, i.e. not able to become leader\n\n\textSt extensionState // Extension state\n\n\tpsubj  string // Proposals subject\n\trpsubj string // Remove peers subject\n\tvsubj  string // Vote requests subject\n\tvreply string // Vote responses subject\n\tasubj  string // Append entries subject\n\tareply string // Append entries responses subject\n\n\tsq    *sendq        // Send queue for outbound RPC messages\n\taesub *subscription // Subscription for handleAppendEntry callbacks\n\n\twtv []byte // Term and vote to be written\n\twps []byte // Peer state to be written\n\n\tcatchup  *catchupState               // For when we need to catch up as a follower.\n\tprogress map[string]*ipQueue[uint64] // For leader or server catching up a follower.\n\n\tpaused    bool   // Whether or not applies are paused\n\thcommit   uint64 // The commit at the time that applies were paused\n\tpobserver bool   // Whether we were an observer at the time that applies were paused\n\n\tprop  *ipQueue[*proposedEntry]       // Proposals\n\tentry *ipQueue[*appendEntry]         // Append entries\n\tresp  *ipQueue[*appendEntryResponse] // Append entries responses\n\tapply *ipQueue[*CommittedEntry]      // Apply queue (committed entries to be passed to upper layer)\n\treqs  *ipQueue[*voteRequest]         // Vote requests\n\tvotes *ipQueue[*voteResponse]        // Vote responses\n\tleadc chan bool                      // Leader changes\n\tquit  chan struct{}                  // Raft group shutdown\n}\n\ntype proposedEntry struct {\n\t*Entry\n\treply string // Optional, to respond once proposal handled\n}\n\n// cacthupState structure that holds our subscription, and catchup term and index\n// as well as starting term and index and how many updates we have seen.\ntype catchupState struct {\n\tsub    *subscription // Subscription that catchup messages will arrive on\n\tcterm  uint64        // Catchup term\n\tcindex uint64        // Catchup index\n\tpterm  uint64        // Starting term\n\tpindex uint64        // Starting index\n\tactive time.Time     // Last time we received a message for this catchup\n}\n\n// lps holds peer state of last time and last index replicated.\ntype lps struct {\n\tts int64  // Last timestamp\n\tli uint64 // Last index replicated\n\tkp bool   // Known peer\n}\n\nconst (\n\tminElectionTimeoutDefault      = 4 * time.Second\n\tmaxElectionTimeoutDefault      = 9 * time.Second\n\tminCampaignTimeoutDefault      = 100 * time.Millisecond\n\tmaxCampaignTimeoutDefault      = 8 * minCampaignTimeoutDefault\n\thbIntervalDefault              = 1 * time.Second\n\tlostQuorumIntervalDefault      = hbIntervalDefault * 10 // 10 seconds\n\tlostQuorumCheckIntervalDefault = hbIntervalDefault * 10 // 10 seconds\n\tobserverModeIntervalDefault    = 48 * time.Hour\n\tpeerRemoveTimeoutDefault       = 5 * time.Minute\n)\n\nvar (\n\tminElectionTimeout   = minElectionTimeoutDefault\n\tmaxElectionTimeout   = maxElectionTimeoutDefault\n\tminCampaignTimeout   = minCampaignTimeoutDefault\n\tmaxCampaignTimeout   = maxCampaignTimeoutDefault\n\thbInterval           = hbIntervalDefault\n\tlostQuorumInterval   = lostQuorumIntervalDefault\n\tlostQuorumCheck      = lostQuorumCheckIntervalDefault\n\tobserverModeInterval = observerModeIntervalDefault\n\tpeerRemoveTimeout    = peerRemoveTimeoutDefault\n)\n\ntype RaftConfig struct {\n\tName     string\n\tStore    string\n\tLog      WAL\n\tTrack    bool\n\tObserver bool\n}\n\nvar (\n\terrNotLeader         = errors.New(\"raft: not leader\")\n\terrAlreadyLeader     = errors.New(\"raft: already leader\")\n\terrNilCfg            = errors.New(\"raft: no config given\")\n\terrCorruptPeers      = errors.New(\"raft: corrupt peer state\")\n\terrEntryLoadFailed   = errors.New(\"raft: could not load entry from WAL\")\n\terrEntryStoreFailed  = errors.New(\"raft: could not store entry to WAL\")\n\terrNodeClosed        = errors.New(\"raft: node is closed\")\n\terrBadSnapName       = errors.New(\"raft: snapshot name could not be parsed\")\n\terrNoSnapAvailable   = errors.New(\"raft: no snapshot available\")\n\terrCatchupsRunning   = errors.New(\"raft: snapshot can not be installed while catchups running\")\n\terrSnapshotCorrupt   = errors.New(\"raft: snapshot corrupt\")\n\terrTooManyPrefs      = errors.New(\"raft: stepdown requires at most one preferred new leader\")\n\terrNoPeerState       = errors.New(\"raft: no peerstate\")\n\terrAdjustBootCluster = errors.New(\"raft: can not adjust boot peer size on established group\")\n\terrLeaderLen         = fmt.Errorf(\"raft: leader should be exactly %d bytes\", idLen)\n\terrTooManyEntries    = errors.New(\"raft: append entry can contain a max of 64k entries\")\n\terrBadAppendEntry    = errors.New(\"raft: append entry corrupt\")\n\terrNoInternalClient  = errors.New(\"raft: no internal client\")\n)\n\n// This will bootstrap a raftNode by writing its config into the store directory.\nfunc (s *Server) bootstrapRaftNode(cfg *RaftConfig, knownPeers []string, allPeersKnown bool) error {\n\tif cfg == nil {\n\t\treturn errNilCfg\n\t}\n\t// Check validity of peers if presented.\n\tfor _, p := range knownPeers {\n\t\tif len(p) != idLen {\n\t\t\treturn fmt.Errorf(\"raft: illegal peer: %q\", p)\n\t\t}\n\t}\n\texpected := len(knownPeers)\n\t// We need to adjust this is all peers are not known.\n\tif !allPeersKnown {\n\t\ts.Debugf(\"Determining expected peer size for JetStream meta group\")\n\t\tif expected < 2 {\n\t\t\texpected = 2\n\t\t}\n\t\topts := s.getOpts()\n\t\tnrs := len(opts.Routes)\n\n\t\tcn := s.ClusterName()\n\t\tngwps := 0\n\t\tfor _, gw := range opts.Gateway.Gateways {\n\t\t\t// Ignore our own cluster if specified.\n\t\t\tif gw.Name == cn {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tfor _, u := range gw.URLs {\n\t\t\t\thost := u.Hostname()\n\t\t\t\t// If this is an IP just add one.\n\t\t\t\tif net.ParseIP(host) != nil {\n\t\t\t\t\tngwps++\n\t\t\t\t} else {\n\t\t\t\t\taddrs, _ := net.LookupHost(host)\n\t\t\t\t\tngwps += len(addrs)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tif expected < nrs+ngwps {\n\t\t\texpected = nrs + ngwps\n\t\t\ts.Debugf(\"Adjusting expected peer set size to %d with %d known\", expected, len(knownPeers))\n\t\t}\n\t}\n\n\t// Check the store directory. If we have a memory based WAL we need to make sure the directory is setup.\n\tif stat, err := os.Stat(cfg.Store); os.IsNotExist(err) {\n\t\tif err := os.MkdirAll(cfg.Store, defaultDirPerms); err != nil {\n\t\t\treturn fmt.Errorf(\"raft: could not create storage directory - %v\", err)\n\t\t}\n\t} else if stat == nil || !stat.IsDir() {\n\t\treturn fmt.Errorf(\"raft: storage directory is not a directory\")\n\t}\n\ttmpfile, err := os.CreateTemp(cfg.Store, \"_test_\")\n\tif err != nil {\n\t\treturn fmt.Errorf(\"raft: storage directory is not writable\")\n\t}\n\ttmpfile.Close()\n\tos.Remove(tmpfile.Name())\n\n\treturn writePeerState(cfg.Store, &peerState{knownPeers, expected, extUndetermined})\n}\n\n// initRaftNode will initialize the raft node, to be used by startRaftNode or when testing to not run the Go routine.\nfunc (s *Server) initRaftNode(accName string, cfg *RaftConfig, labels pprofLabels) (*raft, error) {\n\tif cfg == nil {\n\t\treturn nil, errNilCfg\n\t}\n\ts.mu.RLock()\n\tif s.sys == nil {\n\t\ts.mu.RUnlock()\n\t\treturn nil, ErrNoSysAccount\n\t}\n\thash := s.sys.shash\n\ts.mu.RUnlock()\n\n\t// Do this here to process error quicker.\n\tps, err := readPeerState(cfg.Store)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif ps == nil {\n\t\treturn nil, errNoPeerState\n\t}\n\n\tqpfx := fmt.Sprintf(\"[ACC:%s] RAFT '%s' \", accName, cfg.Name)\n\tn := &raft{\n\t\tcreated:  time.Now(),\n\t\tid:       hash[:idLen],\n\t\tgroup:    cfg.Name,\n\t\tsd:       cfg.Store,\n\t\twal:      cfg.Log,\n\t\twtype:    cfg.Log.Type(),\n\t\ttrack:    cfg.Track,\n\t\tcsz:      ps.clusterSize,\n\t\tqn:       ps.clusterSize/2 + 1,\n\t\tpeers:    make(map[string]*lps),\n\t\tacks:     make(map[uint64]map[string]struct{}),\n\t\tpae:      make(map[uint64]*appendEntry),\n\t\ts:        s,\n\t\tjs:       s.getJetStream(),\n\t\tquit:     make(chan struct{}),\n\t\treqs:     newIPQueue[*voteRequest](s, qpfx+\"vreq\"),\n\t\tvotes:    newIPQueue[*voteResponse](s, qpfx+\"vresp\"),\n\t\tprop:     newIPQueue[*proposedEntry](s, qpfx+\"entry\"),\n\t\tentry:    newIPQueue[*appendEntry](s, qpfx+\"appendEntry\"),\n\t\tresp:     newIPQueue[*appendEntryResponse](s, qpfx+\"appendEntryResponse\"),\n\t\tapply:    newIPQueue[*CommittedEntry](s, qpfx+\"committedEntry\"),\n\t\taccName:  accName,\n\t\tleadc:    make(chan bool, 32),\n\t\tobserver: cfg.Observer,\n\t\textSt:    ps.domainExt,\n\t}\n\n\t// Setup our internal subscriptions for proposals, votes and append entries.\n\t// If we fail to do this for some reason then this is fatal \u2014 we cannot\n\t// continue setting up or the Raft node may be partially/totally isolated.\n\tif err := n.RecreateInternalSubs(); err != nil {\n\t\tn.shutdown()\n\t\treturn nil, err\n\t}\n\n\tif atomic.LoadInt32(&s.logging.debug) > 0 {\n\t\tn.dflag = true\n\t}\n\n\t// Set up the highwayhash for the snapshots.\n\tkey := sha256.Sum256([]byte(n.group))\n\tn.hh, _ = highwayhash.New64(key[:])\n\n\t// If we have a term and vote file (tav.idx on the filesystem) then read in\n\t// what we think the term and vote was. It's possible these are out of date\n\t// so a catch-up may be required.\n\tif term, vote, err := n.readTermVote(); err == nil && term > 0 {\n\t\tn.term = term\n\t\tn.vote = vote\n\t}\n\n\t// Can't recover snapshots if memory based since wal will be reset.\n\t// We will inherit from the current leader.\n\tif _, ok := n.wal.(*memStore); ok {\n\t\t_ = os.RemoveAll(filepath.Join(n.sd, snapshotsDir))\n\t} else {\n\t\t// See if we have any snapshots and if so load and process on startup.\n\t\tn.setupLastSnapshot()\n\t}\n\n\t// Make sure that the snapshots directory exists.\n\tif err := os.MkdirAll(filepath.Join(n.sd, snapshotsDir), defaultDirPerms); err != nil {\n\t\treturn nil, fmt.Errorf(\"could not create snapshots directory - %v\", err)\n\t}\n\n\ttruncateAndErr := func(index uint64) {\n\t\tif err := n.wal.Truncate(index); err != nil {\n\t\t\tn.setWriteErr(err)\n\t\t}\n\t}\n\n\t// Retrieve the stream state from the WAL. If there are pending append\n\t// entries that were committed but not applied before we last shut down,\n\t// we will try to replay them and process them here.\n\tvar state StreamState\n\tn.wal.FastState(&state)\n\tif state.Msgs > 0 {\n\t\tn.debug(\"Replaying state of %d entries\", state.Msgs)\n\t\tif first, err := n.loadFirstEntry(); err == nil {\n\t\t\tn.pterm, n.pindex = first.pterm, first.pindex\n\t\t\tif first.commit > 0 && first.commit > n.commit {\n\t\t\t\tn.commit = first.commit\n\t\t\t}\n\t\t}\n\n\t\t// This process will queue up entries on our applied queue but prior to the upper\n\t\t// state machine running. So we will monitor how much we have queued and if we\n\t\t// reach a limit will pause the apply queue and resume inside of run() go routine.\n\t\tconst maxQsz = 32 * 1024 * 1024 // 32MB max\n\n\t\t// It looks like there are entries we have committed but not applied\n\t\t// yet. Replay them.\n\t\tfor index, qsz := state.FirstSeq, 0; index <= state.LastSeq; index++ {\n\t\t\tae, err := n.loadEntry(index)\n\t\t\tif err != nil {\n\t\t\t\tn.warn(\"Could not load %d from WAL [%+v]: %v\", index, state, err)\n\t\t\t\ttruncateAndErr(index)\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tif ae.pindex != index-1 {\n\t\t\t\tn.warn(\"Corrupt WAL, will truncate\")\n\t\t\t\ttruncateAndErr(index)\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tn.processAppendEntry(ae, nil)\n\t\t\t// Check how much we have queued up so far to determine if we should pause.\n\t\t\tfor _, e := range ae.entries {\n\t\t\t\tqsz += len(e.Data)\n\t\t\t\tif qsz > maxQsz && !n.paused {\n\t\t\t\t\tn.PauseApply()\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\t// Make sure to track ourselves.\n\tn.peers[n.id] = &lps{time.Now().UnixNano(), 0, true}\n\n\t// Track known peers\n\tfor _, peer := range ps.knownPeers {\n\t\tif peer != n.id {\n\t\t\t// Set these to 0 to start but mark as known peer.\n\t\t\tn.peers[peer] = &lps{0, 0, true}\n\t\t}\n\t}\n\n\tn.debug(\"Started\")\n\n\t// Check if we need to start in observer mode due to lame duck status.\n\t// This will stop us from taking on the leader role when we're about to\n\t// shutdown anyway.\n\tif s.isLameDuckMode() {\n\t\tn.debug(\"Will start in observer mode due to lame duck status\")\n\t\tn.SetObserver(true)\n\t}\n\n\t// Set the election timer and lost quorum timers to now, so that we\n\t// won't accidentally trigger either state without knowing the real state\n\t// of the other nodes.\n\tn.Lock()\n\tn.resetElectionTimeout()\n\tn.llqrt = time.Now()\n\tn.Unlock()\n\n\t// Register the Raft group.\n\tlabels[\"group\"] = n.group\n\ts.registerRaftNode(n.group, n)\n\n\treturn n, nil\n}\n\n// startRaftNode will start the raft node.\nfunc (s *Server) startRaftNode(accName string, cfg *RaftConfig, labels pprofLabels) (RaftNode, error) {\n\tn, err := s.initRaftNode(accName, cfg, labels)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Start the run goroutine for the Raft state machine.\n\tn.wg.Add(1)\n\ts.startGoRoutine(n.run, labels)\n\n\treturn n, nil\n}\n\n// Returns whether peers within this group claim to support\n// moving NRG traffic into the asset account.\n// Lock must be held.\nfunc (n *raft) checkAccountNRGStatus() bool {\n\tif !n.s.accountNRGAllowed.Load() {\n\t\treturn false\n\t}\n\tenabled := true\n\tfor pn := range n.peers {\n\t\tif si, ok := n.s.nodeToInfo.Load(pn); ok && si != nil {\n\t\t\tenabled = enabled && si.(nodeInfo).accountNRG\n\t\t}\n\t}\n\treturn enabled\n}\n\n// Whether we are using the system account or not.\nfunc (n *raft) IsSystemAccount() bool {\n\treturn n.isSysAcc.Load()\n}\n\nfunc (n *raft) RecreateInternalSubs() error {\n\tn.Lock()\n\tdefer n.Unlock()\n\treturn n.recreateInternalSubsLocked()\n}\n\nfunc (n *raft) recreateInternalSubsLocked() error {\n\t// Sanity check for system account, as it can disappear when\n\t// the system is shutting down.\n\tif n.s == nil {\n\t\treturn fmt.Errorf(\"server not found\")\n\t}\n\tn.s.mu.RLock()\n\tsys := n.s.sys\n\tn.s.mu.RUnlock()\n\tif sys == nil {\n\t\treturn fmt.Errorf(\"system account not found\")\n\t}\n\n\t// Default is the system account.\n\tnrgAcc := sys.account\n\tn.isSysAcc.Store(true)\n\n\t// Is account NRG enabled in this account and do all group\n\t// peers claim to also support account NRG?\n\tif n.checkAccountNRGStatus() {\n\t\t// Check whether the account that the asset belongs to\n\t\t// has volunteered a different NRG account.\n\t\ttarget := nrgAcc.Name\n\t\tif a, _ := n.s.lookupAccount(n.accName); a != nil {\n\t\t\ta.mu.RLock()\n\t\t\tif a.js != nil {\n\t\t\t\ttarget = a.nrgAccount\n\t\t\t}\n\t\t\ta.mu.RUnlock()\n\t\t}\n\n\t\t// If the target account exists, then we'll use that.\n\t\tif target != _EMPTY_ {\n\t\t\tif a, _ := n.s.lookupAccount(target); a != nil {\n\t\t\t\tnrgAcc = a\n\t\t\t\tif a != sys.account {\n\t\t\t\t\tn.isSysAcc.Store(false)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tif n.aesub != nil && n.acc == nrgAcc {\n\t\t// Subscriptions already exist and the account NRG state\n\t\t// hasn't changed.\n\t\treturn nil\n\t}\n\n\t// Need to cancel any in-progress catch-ups, otherwise the\n\t// inboxes are about to be pulled out from underneath it in\n\t// the next step...\n\tn.cancelCatchup()\n\n\t// If we have an existing client then tear down any existing\n\t// subscriptions and close the internal client.\n\tif c := n.c; c != nil {\n\t\tc.mu.Lock()\n\t\tsubs := make([]*subscription, 0, len(c.subs))\n\t\tfor _, sub := range c.subs {\n\t\t\tsubs = append(subs, sub)\n\t\t}\n\t\tc.mu.Unlock()\n\t\tfor _, sub := range subs {\n\t\t\tn.unsubscribe(sub)\n\t\t}\n\t\tc.closeConnection(InternalClient)\n\t}\n\n\tif n.acc != nrgAcc {\n\t\tn.debug(\"Subscribing in '%s'\", nrgAcc.GetName())\n\t}\n\n\tc := n.s.createInternalSystemClient()\n\tc.registerWithAccount(nrgAcc)\n\tif nrgAcc.sq == nil {\n\t\tnrgAcc.sq = n.s.newSendQ(nrgAcc)\n\t}\n\tn.c = c\n\tn.sq = nrgAcc.sq\n\tn.acc = nrgAcc\n\n\t// Recreate any internal subscriptions for voting, append\n\t// entries etc in the new account.\n\treturn n.createInternalSubs()\n}\n\n// outOfResources checks to see if we are out of resources.\nfunc (n *raft) outOfResources() bool {\n\tjs := n.js\n\tif !n.track || js == nil {\n\t\treturn false\n\t}\n\treturn js.limitsExceeded(n.wtype)\n}\n\n// Maps node names back to server names.\nfunc (s *Server) serverNameForNode(node string) string {\n\tif si, ok := s.nodeToInfo.Load(node); ok && si != nil {\n\t\treturn si.(nodeInfo).name\n\t}\n\treturn _EMPTY_\n}\n\n// Maps node names back to cluster names.\nfunc (s *Server) clusterNameForNode(node string) string {\n\tif si, ok := s.nodeToInfo.Load(node); ok && si != nil {\n\t\treturn si.(nodeInfo).cluster\n\t}\n\treturn _EMPTY_\n}\n\n// Registers the Raft node with the server, as it will track all of the Raft\n// nodes.\nfunc (s *Server) registerRaftNode(group string, n RaftNode) {\n\ts.rnMu.Lock()\n\tdefer s.rnMu.Unlock()\n\tif s.raftNodes == nil {\n\t\ts.raftNodes = make(map[string]RaftNode)\n\t}\n\ts.raftNodes[group] = n\n}\n\n// Unregisters the Raft node from the server, i.e. at shutdown.\nfunc (s *Server) unregisterRaftNode(group string) {\n\ts.rnMu.Lock()\n\tdefer s.rnMu.Unlock()\n\tif s.raftNodes != nil {\n\t\tdelete(s.raftNodes, group)\n\t}\n}\n\n// Returns how many Raft nodes are running in this server instance.\nfunc (s *Server) numRaftNodes() int {\n\ts.rnMu.RLock()\n\tdefer s.rnMu.RUnlock()\n\treturn len(s.raftNodes)\n}\n\n// Finds the Raft node for a given Raft group, if any. If there is no Raft node\n// running for this group then it can return nil.\nfunc (s *Server) lookupRaftNode(group string) RaftNode {\n\ts.rnMu.RLock()\n\tdefer s.rnMu.RUnlock()\n\tvar n RaftNode\n\tif s.raftNodes != nil {\n\t\tn = s.raftNodes[group]\n\t}\n\treturn n\n}\n\n// Reloads the debug state for all running Raft nodes. This is necessary when\n// the configuration has been reloaded and the debug log level has changed.\nfunc (s *Server) reloadDebugRaftNodes(debug bool) {\n\tif s == nil {\n\t\treturn\n\t}\n\ts.rnMu.RLock()\n\tfor _, ni := range s.raftNodes {\n\t\tn := ni.(*raft)\n\t\tn.Lock()\n\t\tn.dflag = debug\n\t\tn.Unlock()\n\t}\n\ts.rnMu.RUnlock()\n}\n\n// Requests that all Raft nodes on this server step down and place them into\n// observer mode. This is called when the server is shutting down.\nfunc (s *Server) stepdownRaftNodes() {\n\tif s == nil {\n\t\treturn\n\t}\n\ts.rnMu.RLock()\n\tif len(s.raftNodes) == 0 {\n\t\ts.rnMu.RUnlock()\n\t\treturn\n\t}\n\ts.Debugf(\"Stepping down all leader raft nodes\")\n\tnodes := make([]RaftNode, 0, len(s.raftNodes))\n\tfor _, n := range s.raftNodes {\n\t\tnodes = append(nodes, n)\n\t}\n\ts.rnMu.RUnlock()\n\n\tfor _, node := range nodes {\n\t\tnode.StepDown()\n\t\tnode.SetObserver(true)\n\t}\n}\n\n// Shuts down all Raft nodes on this server. This is called either when the\n// server is either entering lame duck mode, shutting down or when JetStream\n// has been disabled.\nfunc (s *Server) shutdownRaftNodes() {\n\tif s == nil {\n\t\treturn\n\t}\n\ts.rnMu.RLock()\n\tif len(s.raftNodes) == 0 {\n\t\ts.rnMu.RUnlock()\n\t\treturn\n\t}\n\tnodes := make([]RaftNode, 0, len(s.raftNodes))\n\ts.Debugf(\"Shutting down all raft nodes\")\n\tfor _, n := range s.raftNodes {\n\t\tnodes = append(nodes, n)\n\t}\n\ts.rnMu.RUnlock()\n\n\tfor _, node := range nodes {\n\t\tnode.Stop()\n\t}\n}\n\n// Used in lameduck mode to move off the leaders.\n// We also put all nodes in observer mode so new leaders\n// can not be placed on this server.\nfunc (s *Server) transferRaftLeaders() bool {\n\tif s == nil {\n\t\treturn false\n\t}\n\ts.rnMu.RLock()\n\tif len(s.raftNodes) == 0 {\n\t\ts.rnMu.RUnlock()\n\t\treturn false\n\t}\n\tnodes := make([]RaftNode, 0, len(s.raftNodes))\n\tfor _, n := range s.raftNodes {\n\t\tnodes = append(nodes, n)\n\t}\n\ts.rnMu.RUnlock()\n\n\tvar didTransfer bool\n\tfor _, node := range nodes {\n\t\tif err := node.StepDown(); err == nil {\n\t\t\tdidTransfer = true\n\t\t}\n\t\tnode.SetObserver(true)\n\t}\n\treturn didTransfer\n}\n\n// Formal API\n\n// Propose will propose a new entry to the group.\n// This should only be called on the leader.\nfunc (n *raft) Propose(data []byte) error {\n\tif state := n.State(); state != Leader {\n\t\tn.debug(\"Proposal ignored, not leader (state: %v)\", state)\n\t\treturn errNotLeader\n\t}\n\tn.Lock()\n\tdefer n.Unlock()\n\n\t// Error if we had a previous write error.\n\tif werr := n.werr; werr != nil {\n\t\treturn werr\n\t}\n\tn.prop.push(newProposedEntry(newEntry(EntryNormal, data), _EMPTY_))\n\treturn nil\n}\n\n// ProposeDirect will propose multiple entries at once.\n// This should only be called on the leader.\nfunc (n *raft) ProposeMulti(entries []*Entry) error {\n\tif state := n.State(); state != Leader {\n\t\tn.debug(\"Direct proposal ignored, not leader (state: %v)\", state)\n\t\treturn errNotLeader\n\t}\n\tn.Lock()\n\tdefer n.Unlock()\n\n\t// Error if we had a previous write error.\n\tif werr := n.werr; werr != nil {\n\t\treturn werr\n\t}\n\tfor _, e := range entries {\n\t\tn.prop.push(newProposedEntry(e, _EMPTY_))\n\t}\n\treturn nil\n}\n\n// ForwardProposal will forward the proposal to the leader if known.\n// If we are the leader this is the same as calling propose.\nfunc (n *raft) ForwardProposal(entry []byte) error {\n\tif n.State() == Leader {\n\t\treturn n.Propose(entry)\n\t}\n\n\t// TODO: Currently we do not set a reply subject, even though we are\n\t// now capable of responding. Do this once enough time has passed,\n\t// i.e. maybe in 2.12.\n\tn.sendRPC(n.psubj, _EMPTY_, entry)\n\treturn nil\n}\n\n// ProposeAddPeer is called to add a peer to the group.\nfunc (n *raft) ProposeAddPeer(peer string) error {\n\tif n.State() != Leader {\n\t\treturn errNotLeader\n\t}\n\tn.RLock()\n\t// Error if we had a previous write error.\n\tif werr := n.werr; werr != nil {\n\t\tn.RUnlock()\n\t\treturn werr\n\t}\n\tprop := n.prop\n\tn.RUnlock()\n\n\tprop.push(newProposedEntry(newEntry(EntryAddPeer, []byte(peer)), _EMPTY_))\n\treturn nil\n}\n\n// As a leader if we are proposing to remove a peer assume its already gone.\nfunc (n *raft) doRemovePeerAsLeader(peer string) {\n\tn.Lock()\n\tif n.removed == nil {\n\t\tn.removed = map[string]time.Time{}\n\t}\n\tn.removed[peer] = time.Now()\n\tif _, ok := n.peers[peer]; ok {\n\t\tdelete(n.peers, peer)\n\t\t// We should decrease our cluster size since we are tracking this peer and the peer is most likely already gone.\n\t\tn.adjustClusterSizeAndQuorum()\n\t}\n\tn.Unlock()\n}\n\n// ProposeRemovePeer is called to remove a peer from the group.\nfunc (n *raft) ProposeRemovePeer(peer string) error {\n\tn.RLock()\n\tprop, subj := n.prop, n.rpsubj\n\tisLeader := n.State() == Leader\n\twerr := n.werr\n\tn.RUnlock()\n\n\t// Error if we had a previous write error.\n\tif werr != nil {\n\t\treturn werr\n\t}\n\n\t// If we are the leader then we are responsible for processing the\n\t// peer remove and then notifying the rest of the group that the\n\t// peer was removed.\n\tif isLeader {\n\t\tprop.push(newProposedEntry(newEntry(EntryRemovePeer, []byte(peer)), _EMPTY_))\n\t\tn.doRemovePeerAsLeader(peer)\n\t\treturn nil\n\t}\n\n\t// Otherwise we need to forward the proposal to the leader.\n\tn.sendRPC(subj, _EMPTY_, []byte(peer))\n\treturn nil\n}\n\n// ClusterSize reports back the total cluster size.\n// This effects quorum etc.\nfunc (n *raft) ClusterSize() int {\n\tn.Lock()\n\tdefer n.Unlock()\n\treturn n.csz\n}\n\n// AdjustBootClusterSize can be called to adjust the boot cluster size.\n// Will error if called on a group with a leader or a previous leader.\n// This can be helpful in mixed mode.\nfunc (n *raft) AdjustBootClusterSize(csz int) error {\n\tn.Lock()\n\tdefer n.Unlock()\n\n\tif n.leader != noLeader || n.pleader.Load() {\n\t\treturn errAdjustBootCluster\n\t}\n\t// Same floor as bootstrap.\n\tif csz < 2 {\n\t\tcsz = 2\n\t}\n\t// Adjust the cluster size and the number of nodes needed to establish\n\t// a quorum.\n\tn.csz = csz\n\tn.qn = n.csz/2 + 1\n\n\treturn nil\n}\n\n// AdjustClusterSize will change the cluster set size.\n// Must be the leader.\nfunc (n *raft) AdjustClusterSize(csz int) error {\n\tif n.State() != Leader {\n\t\treturn errNotLeader\n\t}\n\tn.Lock()\n\t// Same floor as bootstrap.\n\tif csz < 2 {\n\t\tcsz = 2\n\t}\n\n\t// Adjust the cluster size and the number of nodes needed to establish\n\t// a quorum.\n\tn.csz = csz\n\tn.qn = n.csz/2 + 1\n\tn.Unlock()\n\n\tn.sendPeerState()\n\treturn nil\n}\n\n// PauseApply will allow us to pause processing of append entries onto our\n// external apply queue. In effect this means that the upper layer will no longer\n// receive any new entries from the Raft group.\nfunc (n *raft) PauseApply() error {\n\tif n.State() == Leader {\n\t\treturn errAlreadyLeader\n\t}\n\n\tn.Lock()\n\tdefer n.Unlock()\n\n\t// If we are currently a candidate make sure we step down.\n\tif n.State() == Candidate {\n\t\tn.stepdownLocked(noLeader)\n\t}\n\n\tn.debug(\"Pausing our apply channel\")\n\tn.paused = true\n\tn.hcommit = n.commit\n\t// Also prevent us from trying to become a leader while paused and catching up.\n\tn.pobserver, n.observer = n.observer, true\n\tn.resetElect(observerModeInterval)\n\n\treturn nil\n}\n\n// ResumeApply will resume sending applies to the external apply queue. This\n// means that we will start sending new entries to the upper layer.\nfunc (n *raft) ResumeApply() {\n\tn.Lock()\n\tdefer n.Unlock()\n\n\tif !n.paused {\n\t\treturn\n\t}\n\n\tn.debug(\"Resuming our apply channel\")\n\n\t// Reset before we start.\n\tn.resetElectionTimeout()\n\n\t// Run catchup..\n\tif n.hcommit > n.commit {\n\t\tn.debug(\"Resuming %d replays\", n.hcommit+1-n.commit)\n\t\tfor index := n.commit + 1; index <= n.hcommit; index++ {\n\t\t\tif err := n.applyCommit(index); err != nil {\n\t\t\t\tn.warn(\"Got error on apply commit during replay: %v\", err)\n\t\t\t\tbreak\n\t\t\t}\n\t\t\t// We want to unlock here to allow the upper layers to call Applied() without blocking.\n\t\t\tn.Unlock()\n\t\t\t// Give hint to let other Go routines run.\n\t\t\t// Might not be necessary but seems to make it more fine grained interleaving.\n\t\t\truntime.Gosched()\n\t\t\t// Simply re-acquire\n\t\t\tn.Lock()\n\t\t\t// Need to check if we got closed.\n\t\t\tif n.State() == Closed {\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}\n\n\t// Clear our observer and paused state after we apply.\n\tn.observer, n.pobserver = n.pobserver, false\n\tn.paused = false\n\tn.hcommit = 0\n\n\t// If we had been selected to be the next leader campaign here now that we have resumed.\n\tif n.lxfer {\n\t\tn.xferCampaign()\n\t} else {\n\t\tn.resetElectionTimeout()\n\t}\n}\n\n// Applied is a callback that must be called by the upper layer when it\n// has successfully applied the committed entries that it received from the\n// apply queue. It will return the number of entries and an estimation of the\n// byte size that could be removed with a snapshot/compact.\nfunc (n *raft) Applied(index uint64) (entries uint64, bytes uint64) {\n\tn.Lock()\n\tdefer n.Unlock()\n\n\t// Ignore if not applicable. This can happen during a reset.\n\tif index > n.commit {\n\t\treturn 0, 0\n\t}\n\n\t// Ignore if already applied.\n\tif index > n.applied {\n\t\tn.applied = index\n\t}\n\n\t// If it was set, and we reached the minimum applied index, reset and send signal to upper layer.\n\tif n.aflr > 0 && index >= n.aflr {\n\t\tn.aflr = 0\n\t\t// Quick sanity-check to confirm we're still leader.\n\t\t// In which case we must signal, since switchToLeader would not have done so already.\n\t\tif n.State() == Leader {\n\t\t\tn.leaderState.Store(true)\n\t\t\tn.updateLeadChange(true)\n\t\t}\n\t}\n\n\t// Calculate the number of entries and estimate the byte size that\n\t// we can now remove with a compaction/snapshot.\n\tvar state StreamState\n\tn.wal.FastState(&state)\n\tif n.applied > state.FirstSeq {\n\t\tentries = n.applied - state.FirstSeq\n\t}\n\tif state.Msgs > 0 {\n\t\tbytes = entries * state.Bytes / state.Msgs\n\t}\n\treturn entries, bytes\n}\n\n// For capturing data needed by snapshot.\ntype snapshot struct {\n\tlastTerm  uint64\n\tlastIndex uint64\n\tpeerstate []byte\n\tdata      []byte\n}\n\nconst minSnapshotLen = 28\n\n// Encodes a snapshot into a buffer for storage.\n// Lock should be held.\nfunc (n *raft) encodeSnapshot(snap *snapshot) []byte {\n\tif snap == nil {\n\t\treturn nil\n\t}\n\tvar le = binary.LittleEndian\n\tbuf := make([]byte, minSnapshotLen+len(snap.peerstate)+len(snap.data))\n\tle.PutUint64(buf[0:], snap.lastTerm)\n\tle.PutUint64(buf[8:], snap.lastIndex)\n\t// Peer state\n\tle.PutUint32(buf[16:], uint32(len(snap.peerstate)))\n\twi := 20\n\tcopy(buf[wi:], snap.peerstate)\n\twi += len(snap.peerstate)\n\t// data itself.\n\tcopy(buf[wi:], snap.data)\n\twi += len(snap.data)\n\n\t// Now do the hash for the end.\n\tn.hh.Reset()\n\tn.hh.Write(buf[:wi])\n\tchecksum := n.hh.Sum(nil)\n\tcopy(buf[wi:], checksum)\n\twi += len(checksum)\n\treturn buf[:wi]\n}\n\n// SendSnapshot will send the latest snapshot as a normal AE.\n// Should only be used when the upper layers know this is most recent.\n// Used when restoring streams, moving a stream from R1 to R>1, etc.\nfunc (n *raft) SendSnapshot(data []byte) error {\n\tn.sendAppendEntry([]*Entry{{EntrySnapshot, data}})\n\treturn nil\n}\n\n// Used to install a snapshot for the given term and applied index. This will release\n// all of the log entries up to and including index. This should not be called with\n// entries that have been applied to the FSM but have not been applied to the raft state.\nfunc (n *raft) InstallSnapshot(data []byte) error {\n\tif n.State() == Closed {\n\t\treturn errNodeClosed\n\t}\n\n\tn.Lock()\n\tdefer n.Unlock()\n\n\t// If a write error has occurred already then stop here.\n\tif werr := n.werr; werr != nil {\n\t\treturn werr\n\t}\n\n\t// Check that a catchup isn't already taking place. If it is then we won't\n\t// allow installing snapshots until it is done.\n\tif len(n.progress) > 0 || n.paused {\n\t\treturn errCatchupsRunning\n\t}\n\n\tif n.applied == 0 {\n\t\tn.debug(\"Not snapshotting as there are no applied entries\")\n\t\treturn errNoSnapAvailable\n\t}\n\n\tvar term uint64\n\tif ae, _ := n.loadEntry(n.applied); ae != nil {\n\t\tterm = ae.term\n\t} else {\n\t\tn.debug(\"Not snapshotting as entry %d is not available\", n.applied)\n\t\treturn errNoSnapAvailable\n\t}\n\n\tn.debug(\"Installing snapshot of %d bytes\", len(data))\n\n\treturn n.installSnapshot(&snapshot{\n\t\tlastTerm:  term,\n\t\tlastIndex: n.applied,\n\t\tpeerstate: encodePeerState(&peerState{n.peerNames(), n.csz, n.extSt}),\n\t\tdata:      data,\n\t})\n}\n\n// Install the snapshot.\n// Lock should be held.\nfunc (n *raft) installSnapshot(snap *snapshot) error {\n\tsnapDir := filepath.Join(n.sd, snapshotsDir)\n\tsn := fmt.Sprintf(snapFileT, snap.lastTerm, snap.lastIndex)\n\tsfile := filepath.Join(snapDir, sn)\n\n\tif err := writeFileWithSync(sfile, n.encodeSnapshot(snap), defaultFilePerms); err != nil {\n\t\t// We could set write err here, but if this is a temporary situation, too many open files etc.\n\t\t// we want to retry and snapshots are not fatal.\n\t\treturn err\n\t}\n\n\t// Delete our previous snapshot file if it exists.\n\tif n.snapfile != _EMPTY_ && n.snapfile != sfile {\n\t\tos.Remove(n.snapfile)\n\t}\n\t// Remember our latest snapshot file.\n\tn.snapfile = sfile\n\tif _, err := n.wal.Compact(snap.lastIndex + 1); err != nil {\n\t\tn.setWriteErrLocked(err)\n\t\treturn err\n\t}\n\n\treturn nil\n}\n\n// NeedSnapshot returns true if it is necessary to try to install a snapshot, i.e.\n// after we have finished recovering/replaying at startup, on a regular interval or\n// as a part of cleaning up when shutting down.\nfunc (n *raft) NeedSnapshot() bool {\n\tn.RLock()\n\tdefer n.RUnlock()\n\treturn n.snapfile == _EMPTY_ && n.applied > 1\n}\n\nconst (\n\tsnapshotsDir = \"snapshots\"\n\tsnapFileT    = \"snap.%d.%d\"\n)\n\n// termAndIndexFromSnapfile tries to load the snapshot file and returns the term\n// and index from that snapshot.\nfunc termAndIndexFromSnapFile(sn string) (term, index uint64, err error) {\n\tif sn == _EMPTY_ {\n\t\treturn 0, 0, errBadSnapName\n\t}\n\tfn := filepath.Base(sn)\n\tif n, err := fmt.Sscanf(fn, snapFileT, &term, &index); err != nil || n != 2 {\n\t\treturn 0, 0, errBadSnapName\n\t}\n\treturn term, index, nil\n}\n\n// setupLastSnapshot is called at startup to try and recover the last snapshot from\n// the disk if possible. We will try to recover the term, index and commit/applied\n// indices and then notify the upper layer what we found. Compacts the WAL if needed.\nfunc (n *raft) setupLastSnapshot() {\n\tsnapDir := filepath.Join(n.sd, snapshotsDir)\n\tpsnaps, err := os.ReadDir(snapDir)\n\tif err != nil {\n\t\treturn\n\t}\n\n\tvar lterm, lindex uint64\n\tvar latest string\n\tfor _, sf := range psnaps {\n\t\tsfile := filepath.Join(snapDir, sf.Name())\n\t\tvar term, index uint64\n\t\tterm, index, err := termAndIndexFromSnapFile(sf.Name())\n\t\tif err == nil {\n\t\t\tif term > lterm {\n\t\t\t\tlterm, lindex = term, index\n\t\t\t\tlatest = sfile\n\t\t\t} else if term == lterm && index > lindex {\n\t\t\t\tlindex = index\n\t\t\t\tlatest = sfile\n\t\t\t}\n\t\t} else {\n\t\t\t// Clean this up, can't parse the name.\n\t\t\t// TODO(dlc) - We could read in and check actual contents.\n\t\t\tn.debug(\"Removing snapshot, can't parse name: %q\", sf.Name())\n\t\t\tos.Remove(sfile)\n\t\t}\n\t}\n\n\t// Now cleanup any old entries\n\tfor _, sf := range psnaps {\n\t\tsfile := filepath.Join(snapDir, sf.Name())\n\t\tif sfile != latest {\n\t\t\tn.debug(\"Removing old snapshot: %q\", sfile)\n\t\t\tos.Remove(sfile)\n\t\t}\n\t}\n\n\tif latest == _EMPTY_ {\n\t\treturn\n\t}\n\n\t// Set latest snapshot we have.\n\tn.Lock()\n\tdefer n.Unlock()\n\n\tn.snapfile = latest\n\tsnap, err := n.loadLastSnapshot()\n\tif err != nil {\n\t\t// We failed to recover the last snapshot for some reason, so we will\n\t\t// assume it has been corrupted and will try to delete it.\n\t\tif n.snapfile != _EMPTY_ {\n\t\t\tos.Remove(n.snapfile)\n\t\t\tn.snapfile = _EMPTY_\n\t\t}\n\t\treturn\n\t}\n\n\t// We successfully recovered the last snapshot from the disk.\n\t// Recover state from the snapshot and then notify the upper layer.\n\t// Compact the WAL when we're done if needed.\n\tn.pindex = snap.lastIndex\n\tn.pterm = snap.lastTerm\n\tn.commit = snap.lastIndex\n\tn.applied = snap.lastIndex\n\tn.apply.push(newCommittedEntry(n.commit, []*Entry{{EntrySnapshot, snap.data}}))\n\tif _, err := n.wal.Compact(snap.lastIndex + 1); err != nil {\n\t\tn.setWriteErrLocked(err)\n\t}\n}\n\n// loadLastSnapshot will load and return our last snapshot.\n// Lock should be held.\nfunc (n *raft) loadLastSnapshot() (*snapshot, error) {\n\tif n.snapfile == _EMPTY_ {\n\t\treturn nil, errNoSnapAvailable\n\t}\n\n\t<-dios\n\tbuf, err := os.ReadFile(n.snapfile)\n\tdios <- struct{}{}\n\n\tif err != nil {\n\t\tn.warn(\"Error reading snapshot: %v\", err)\n\t\tos.Remove(n.snapfile)\n\t\tn.snapfile = _EMPTY_\n\t\treturn nil, err\n\t}\n\tif len(buf) < minSnapshotLen {\n\t\tn.warn(\"Snapshot corrupt, too short\")\n\t\tos.Remove(n.snapfile)\n\t\tn.snapfile = _EMPTY_\n\t\treturn nil, errSnapshotCorrupt\n\t}\n\n\t// Check to make sure hash is consistent.\n\thoff := len(buf) - 8\n\tlchk := buf[hoff:]\n\tn.hh.Reset()\n\tn.hh.Write(buf[:hoff])\n\tif !bytes.Equal(lchk[:], n.hh.Sum(nil)) {\n\t\tn.warn(\"Snapshot corrupt, checksums did not match\")\n\t\tos.Remove(n.snapfile)\n\t\tn.snapfile = _EMPTY_\n\t\treturn nil, errSnapshotCorrupt\n\t}\n\n\tvar le = binary.LittleEndian\n\tlps := le.Uint32(buf[16:])\n\tsnap := &snapshot{\n\t\tlastTerm:  le.Uint64(buf[0:]),\n\t\tlastIndex: le.Uint64(buf[8:]),\n\t\tpeerstate: buf[20 : 20+lps],\n\t\tdata:      buf[20+lps : hoff],\n\t}\n\n\t// We had a bug in 2.9.12 that would allow snapshots on last index of 0.\n\t// Detect that here and return err.\n\tif snap.lastIndex == 0 {\n\t\tn.warn(\"Snapshot with last index 0 is invalid, cleaning up\")\n\t\tos.Remove(n.snapfile)\n\t\tn.snapfile = _EMPTY_\n\t\treturn nil, errSnapshotCorrupt\n\t}\n\n\treturn snap, nil\n}\n\n// Leader returns if we are the leader for our group.\n// We use an atomic here now vs acquiring the read lock.\nfunc (n *raft) Leader() bool {\n\tif n == nil {\n\t\treturn false\n\t}\n\treturn n.leaderState.Load()\n}\n\n// stepdown immediately steps down the Raft node to the\n// follower state. This will take the lock itself.\nfunc (n *raft) stepdown(newLeader string) {\n\tn.Lock()\n\tdefer n.Unlock()\n\tn.stepdownLocked(newLeader)\n}\n\n// stepdownLocked immediately steps down the Raft node to the\n// follower state. This requires the lock is already held.\nfunc (n *raft) stepdownLocked(newLeader string) {\n\tn.debug(\"Stepping down\")\n\tn.switchToFollowerLocked(newLeader)\n}\n\n// isCatchingUp returns true if a catchup is currently taking place.\nfunc (n *raft) isCatchingUp() bool {\n\tn.RLock()\n\tdefer n.RUnlock()\n\treturn n.catchup != nil\n}\n\n// isCurrent is called from the healthchecks and returns true if we believe\n// that the upper layer is current with the Raft layer, i.e. that it has applied\n// all of the commits that we have given it.\n// Optionally we can also check whether or not we're making forward progress if we\n// aren't current, in which case this function may block for up to ~10ms to find out.\n// Lock should be held.\nfunc (n *raft) isCurrent(includeForwardProgress bool) bool {\n\t// Check if we are closed.\n\tif n.State() == Closed {\n\t\tn.debug(\"Not current, node is closed\")\n\t\treturn false\n\t}\n\n\t// Check whether we've made progress on any state, 0 is invalid so not healthy.\n\tif n.commit == 0 {\n\t\tn.debug(\"Not current, no commits\")\n\t\treturn false\n\t}\n\n\t// If we were previously logging about falling behind, also log when the problem\n\t// was cleared.\n\tclearBehindState := func() {\n\t\tif n.hcbehind {\n\t\t\tn.warn(\"Health check OK, no longer falling behind\")\n\t\t\tn.hcbehind = false\n\t\t}\n\t}\n\n\t// Make sure we are the leader or we know we have heard from the leader recently.\n\tif n.State() == Leader {\n\t\tclearBehindState()\n\t\treturn true\n\t}\n\n\t// Check to see that we have heard from the current leader lately.\n\tif n.leader != noLeader && n.leader != n.id && n.catchup == nil {\n\t\tokInterval := int64(hbInterval) * 2\n\t\tts := time.Now().UnixNano()\n\t\tif ps := n.peers[n.leader]; ps == nil || ps.ts == 0 && (ts-ps.ts) > okInterval {\n\t\t\tn.debug(\"Not current, no recent leader contact\")\n\t\t\treturn false\n\t\t}\n\t}\n\tif cs := n.catchup; cs != nil {\n\t\t// We're actively catching up, can't mark current even if commit==applied.\n\t\tn.debug(\"Not current, still catching up pindex=%d, cindex=%d\", n.pindex, cs.cindex)\n\t\treturn false\n\t}\n\n\tif n.paused && n.hcommit > n.commit {\n\t\t// We're currently paused, waiting to be resumed to apply pending commits.\n\t\tn.debug(\"Not current, waiting to resume applies commit=%d, hcommit=%d\", n.commit, n.hcommit)\n\t\treturn false\n\t}\n\n\tif n.commit == n.applied {\n\t\t// At this point if we are current, we can return saying so.\n\t\tclearBehindState()\n\t\treturn true\n\t} else if !includeForwardProgress {\n\t\t// Otherwise, if we aren't allowed to include forward progress\n\t\t// (i.e. we are checking \"current\" instead of \"healthy\") then\n\t\t// give up now.\n\t\treturn false\n\t}\n\n\t// Otherwise, wait for a short period of time and see if we are making any\n\t// forward progress.\n\tif startDelta := n.commit - n.applied; startDelta > 0 {\n\t\tfor i := 0; i < 10; i++ { // 10ms, in 1ms increments\n\t\t\tn.Unlock()\n\t\t\ttime.Sleep(time.Millisecond)\n\t\t\tn.Lock()\n\t\t\tif n.commit-n.applied < startDelta {\n\t\t\t\t// The gap is getting smaller, so we're making forward progress.\n\t\t\t\tclearBehindState()\n\t\t\t\treturn true\n\t\t\t}\n\t\t}\n\t}\n\n\tn.hcbehind = true\n\tn.warn(\"Falling behind in health check, commit %d != applied %d\", n.commit, n.applied)\n\treturn false\n}\n\n// Current returns if we are the leader for our group or an up to date follower.\nfunc (n *raft) Current() bool {\n\tif n == nil {\n\t\treturn false\n\t}\n\tn.Lock()\n\tdefer n.Unlock()\n\treturn n.isCurrent(false)\n}\n\n// Healthy returns if we are the leader for our group and nearly up-to-date.\nfunc (n *raft) Healthy() bool {\n\tif n == nil {\n\t\treturn false\n\t}\n\tn.Lock()\n\tdefer n.Unlock()\n\treturn n.isCurrent(true)\n}\n\n// HadPreviousLeader indicates if this group ever had a leader.\nfunc (n *raft) HadPreviousLeader() bool {\n\treturn n.pleader.Load()\n}\n\n// GroupLeader returns the current leader of the group.\nfunc (n *raft) GroupLeader() string {\n\tif n == nil {\n\t\treturn noLeader\n\t}\n\tn.RLock()\n\tdefer n.RUnlock()\n\treturn n.leader\n}\n\n// Leaderless is a lockless way of finding out if the group has a\n// leader or not. Use instead of GroupLeader in hot paths.\nfunc (n *raft) Leaderless() bool {\n\tif n == nil {\n\t\treturn true\n\t}\n\t// Negated because we want the default state of hasLeader to be\n\t// false until the first setLeader() call.\n\treturn !n.hasleader.Load()\n}\n\n// Guess the best next leader. Stepdown will check more thoroughly.\n// Lock should be held.\nfunc (n *raft) selectNextLeader() string {\n\tnextLeader, hli := noLeader, uint64(0)\n\tfor peer, ps := range n.peers {\n\t\tif peer == n.id || ps.li <= hli {\n\t\t\tcontinue\n\t\t}\n\t\thli = ps.li\n\t\tnextLeader = peer\n\t}\n\treturn nextLeader\n}\n\n// StepDown will have a leader stepdown and optionally do a leader transfer.\nfunc (n *raft) StepDown(preferred ...string) error {\n\tif n.State() != Leader {\n\t\treturn errNotLeader\n\t}\n\n\tn.Lock()\n\tif len(preferred) > 1 {\n\t\tn.Unlock()\n\t\treturn errTooManyPrefs\n\t}\n\n\tn.debug(\"Being asked to stepdown\")\n\n\t// See if we have up to date followers.\n\tmaybeLeader := noLeader\n\tif len(preferred) > 0 {\n\t\tif preferred[0] != _EMPTY_ {\n\t\t\tmaybeLeader = preferred[0]\n\t\t} else {\n\t\t\tpreferred = nil\n\t\t}\n\t}\n\n\t// Can't pick ourselves.\n\tif maybeLeader == n.id {\n\t\tmaybeLeader = noLeader\n\t\tpreferred = nil\n\t}\n\n\tnowts := time.Now().UnixNano()\n\n\t// If we have a preferred check it first.\n\tif maybeLeader != noLeader {\n\t\tvar isHealthy bool\n\t\tif ps, ok := n.peers[maybeLeader]; ok {\n\t\t\tsi, ok := n.s.nodeToInfo.Load(maybeLeader)\n\t\t\tisHealthy = ok && !si.(nodeInfo).offline && (nowts-ps.ts) < int64(hbInterval*3)\n\t\t}\n\t\tif !isHealthy {\n\t\t\tmaybeLeader = noLeader\n\t\t}\n\t}\n\n\t// If we do not have a preferred at this point pick the first healthy one.\n\t// Make sure not ourselves.\n\tif maybeLeader == noLeader {\n\t\tfor peer, ps := range n.peers {\n\t\t\tif peer == n.id {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tsi, ok := n.s.nodeToInfo.Load(peer)\n\t\t\tisHealthy := ok && !si.(nodeInfo).offline && (nowts-ps.ts) < int64(hbInterval*3)\n\t\t\tif isHealthy {\n\t\t\t\tmaybeLeader = peer\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t}\n\n\t// Clear our vote state.\n\tn.vote = noVote\n\tn.writeTermVote()\n\n\tn.Unlock()\n\n\tif len(preferred) > 0 && maybeLeader == noLeader {\n\t\tn.debug(\"Can not transfer to preferred peer %q\", preferred[0])\n\t}\n\n\t// If we have a new leader selected, transfer over to them.\n\t// Send the append entry directly rather than via the proposals queue,\n\t// as we will switch to follower state immediately and will blow away\n\t// the contents of the proposal queue in the process.\n\tif maybeLeader != noLeader {\n\t\tn.debug(\"Selected %q for new leader, stepping down due to leadership transfer\", maybeLeader)\n\t\tae := newEntry(EntryLeaderTransfer, []byte(maybeLeader))\n\t\tn.sendAppendEntry([]*Entry{ae})\n\t}\n\n\t// Force us to stepdown here.\n\tn.stepdown(noLeader)\n\n\treturn nil\n}\n\n// Campaign will have our node start a leadership vote.\nfunc (n *raft) Campaign() error {\n\tn.Lock()\n\tdefer n.Unlock()\n\treturn n.campaign(randCampaignTimeout())\n}\n\n// CampaignImmediately will have our node start a leadership vote after minimal delay.\nfunc (n *raft) CampaignImmediately() error {\n\tn.Lock()\n\tdefer n.Unlock()\n\tn.maybeLeader = true\n\treturn n.campaign(minCampaignTimeout / 2)\n}\n\nfunc randCampaignTimeout() time.Duration {\n\tdelta := rand.Int63n(int64(maxCampaignTimeout - minCampaignTimeout))\n\treturn (minCampaignTimeout + time.Duration(delta))\n}\n\n// Campaign will have our node start a leadership vote.\n// Lock should be held.\nfunc (n *raft) campaign(et time.Duration) error {\n\tn.debug(\"Starting campaign\")\n\tif n.State() == Leader {\n\t\treturn errAlreadyLeader\n\t}\n\tn.resetElect(et)\n\treturn nil\n}\n\n// xferCampaign will have our node start an immediate leadership vote.\n// Lock should be held.\nfunc (n *raft) xferCampaign() error {\n\tn.debug(\"Starting transfer campaign\")\n\tif n.State() == Leader {\n\t\tn.lxfer = false\n\t\treturn errAlreadyLeader\n\t}\n\tn.resetElect(10 * time.Millisecond)\n\treturn nil\n}\n\n// State returns the current state for this node.\n// Upper layers should not check State to check if we're Leader, use n.Leader() instead.\nfunc (n *raft) State() RaftState {\n\treturn RaftState(n.state.Load())\n}\n\n// Progress returns the current index, commit and applied values.\nfunc (n *raft) Progress() (index, commit, applied uint64) {\n\tn.RLock()\n\tdefer n.RUnlock()\n\treturn n.pindex, n.commit, n.applied\n}\n\n// Size returns number of entries and total bytes for our WAL.\nfunc (n *raft) Size() (uint64, uint64) {\n\tn.RLock()\n\tvar state StreamState\n\tn.wal.FastState(&state)\n\tn.RUnlock()\n\treturn state.Msgs, state.Bytes\n}\n\nfunc (n *raft) ID() string {\n\tif n == nil {\n\t\treturn _EMPTY_\n\t}\n\t// Lock not needed as n.id is never changed after creation.\n\treturn n.id\n}\n\nfunc (n *raft) Group() string {\n\t// Lock not needed as n.group is never changed after creation.\n\treturn n.group\n}\n\nfunc (n *raft) Peers() []*Peer {\n\tn.RLock()\n\tdefer n.RUnlock()\n\n\tvar peers []*Peer\n\tfor id, ps := range n.peers {\n\t\tvar lag uint64\n\t\tif n.commit > ps.li {\n\t\t\tlag = n.commit - ps.li\n\t\t}\n\t\tp := &Peer{\n\t\t\tID:      id,\n\t\t\tCurrent: id == n.leader || ps.li >= n.applied,\n\t\t\tLast:    time.Unix(0, ps.ts),\n\t\t\tLag:     lag,\n\t\t}\n\t\tpeers = append(peers, p)\n\t}\n\treturn peers\n}\n\n// Update and propose our known set of peers.\nfunc (n *raft) ProposeKnownPeers(knownPeers []string) {\n\t// If we are the leader update and send this update out.\n\tif n.State() != Leader {\n\t\treturn\n\t}\n\tn.UpdateKnownPeers(knownPeers)\n\tn.sendPeerState()\n}\n\n// Update our known set of peers.\nfunc (n *raft) UpdateKnownPeers(knownPeers []string) {\n\tn.Lock()\n\t// Process like peer state update.\n\tps := &peerState{knownPeers, len(knownPeers), n.extSt}\n\tn.processPeerState(ps)\n\tn.Unlock()\n}\n\n// ApplyQ returns the apply queue that new commits will be sent to for the\n// upper layer to apply.\nfunc (n *raft) ApplyQ() *ipQueue[*CommittedEntry] { return n.apply }\n\n// LeadChangeC returns the leader change channel, notifying when the Raft\n// leader role has moved.\nfunc (n *raft) LeadChangeC() <-chan bool { return n.leadc }\n\n// QuitC returns the quit channel, notifying when the Raft group has shut down.\nfunc (n *raft) QuitC() <-chan struct{} { return n.quit }\n\nfunc (n *raft) Created() time.Time {\n\t// Lock not needed as n.created is never changed after creation.\n\treturn n.created\n}\n\nfunc (n *raft) Stop() {\n\tn.shutdown()\n}\n\nfunc (n *raft) WaitForStop() {\n\tif n.state.Load() == int32(Closed) {\n\t\tn.wg.Wait()\n\t}\n}\n\nfunc (n *raft) Delete() {\n\tn.shutdown()\n\tn.wg.Wait()\n\n\tn.Lock()\n\tdefer n.Unlock()\n\n\tif wal := n.wal; wal != nil {\n\t\twal.Delete()\n\t}\n\tos.RemoveAll(n.sd)\n\tn.debug(\"Deleted\")\n}\n\nfunc (n *raft) shutdown() {\n\t// First call to Stop or Delete should close the quit chan\n\t// to notify the runAs goroutines to stop what they're doing.\n\tif n.state.Swap(int32(Closed)) != int32(Closed) {\n\t\tn.leaderState.Store(false)\n\t\tclose(n.quit)\n\t}\n}\n\nconst (\n\traftAllSubj        = \"$NRG.>\"\n\traftVoteSubj       = \"$NRG.V.%s\"\n\traftAppendSubj     = \"$NRG.AE.%s\"\n\traftPropSubj       = \"$NRG.P.%s\"\n\traftRemovePeerSubj = \"$NRG.RP.%s\"\n\traftReply          = \"$NRG.R.%s\"\n\traftCatchupReply   = \"$NRG.CR.%s\"\n)\n\n// Lock should be held (due to use of random generator)\nfunc (n *raft) newCatchupInbox() string {\n\tvar b [replySuffixLen]byte\n\trn := fastrand.Uint64()\n\tfor i, l := 0, rn; i < len(b); i++ {\n\t\tb[i] = digits[l%base]\n\t\tl /= base\n\t}\n\treturn fmt.Sprintf(raftCatchupReply, b[:])\n}\n\nfunc (n *raft) newInbox() string {\n\tvar b [replySuffixLen]byte\n\trn := fastrand.Uint64()\n\tfor i, l := 0, rn; i < len(b); i++ {\n\t\tb[i] = digits[l%base]\n\t\tl /= base\n\t}\n\treturn fmt.Sprintf(raftReply, b[:])\n}\n\n// Our internal subscribe.\n// Lock should be held.\nfunc (n *raft) subscribe(subject string, cb msgHandler) (*subscription, error) {\n\tif n.c == nil {\n\t\treturn nil, errNoInternalClient\n\t}\n\treturn n.s.systemSubscribe(subject, _EMPTY_, false, n.c, cb)\n}\n\n// Lock should be held.\nfunc (n *raft) unsubscribe(sub *subscription) {\n\tif n.c != nil && sub != nil {\n\t\tn.c.processUnsub(sub.sid)\n\t}\n}\n\n// Lock should be held.\nfunc (n *raft) createInternalSubs() error {\n\tn.vsubj, n.vreply = fmt.Sprintf(raftVoteSubj, n.group), n.newInbox()\n\tn.asubj, n.areply = fmt.Sprintf(raftAppendSubj, n.group), n.newInbox()\n\tn.psubj = fmt.Sprintf(raftPropSubj, n.group)\n\tn.rpsubj = fmt.Sprintf(raftRemovePeerSubj, n.group)\n\n\t// Votes\n\tif _, err := n.subscribe(n.vreply, n.handleVoteResponse); err != nil {\n\t\treturn err\n\t}\n\tif _, err := n.subscribe(n.vsubj, n.handleVoteRequest); err != nil {\n\t\treturn err\n\t}\n\t// AppendEntry\n\tif _, err := n.subscribe(n.areply, n.handleAppendEntryResponse); err != nil {\n\t\treturn err\n\t}\n\tif sub, err := n.subscribe(n.asubj, n.handleAppendEntry); err != nil {\n\t\treturn err\n\t} else {\n\t\tn.aesub = sub\n\t}\n\n\treturn nil\n}\n\nfunc randElectionTimeout() time.Duration {\n\tdelta := rand.Int63n(int64(maxElectionTimeout - minElectionTimeout))\n\treturn (minElectionTimeout + time.Duration(delta))\n}\n\n// Lock should be held.\nfunc (n *raft) resetElectionTimeout() {\n\tn.resetElect(randElectionTimeout())\n}\n\nfunc (n *raft) resetElectionTimeoutWithLock() {\n\tn.resetElectWithLock(randElectionTimeout())\n}\n\n// Lock should be held.\nfunc (n *raft) resetElect(et time.Duration) {\n\tn.etlr = time.Now()\n\tif n.elect == nil {\n\t\tn.elect = time.NewTimer(et)\n\t} else {\n\t\tif !n.elect.Stop() {\n\t\t\tselect {\n\t\t\tcase <-n.elect.C:\n\t\t\tdefault:\n\t\t\t}\n\t\t}\n\t\tn.elect.Reset(et)\n\t}\n}\n\nfunc (n *raft) resetElectWithLock(et time.Duration) {\n\tn.Lock()\n\tn.resetElect(et)\n\tn.Unlock()\n}\n\n// run is the top-level runner for the Raft state machine. Depending on the\n// state of the node (leader, follower, candidate, observer), this will call\n// through to other functions. It is expected that this function will run for\n// the entire life of the Raft node once started.\nfunc (n *raft) run() {\n\ts := n.s\n\tdefer s.grWG.Done()\n\tdefer n.wg.Done()\n\n\t// We want to wait for some routing to be enabled, so we will wait for\n\t// at least a route, leaf or gateway connection to be established before\n\t// starting the run loop.\n\tfor gw := s.gateway; ; {\n\t\ts.mu.RLock()\n\t\tready, gwEnabled := s.numRemotes()+len(s.leafs) > 0, gw.enabled\n\t\ts.mu.RUnlock()\n\t\tif !ready && gwEnabled {\n\t\t\tgw.RLock()\n\t\t\tready = len(gw.out)+len(gw.in) > 0\n\t\t\tgw.RUnlock()\n\t\t}\n\t\tif !ready {\n\t\t\tselect {\n\t\t\tcase <-s.quitCh:\n\t\t\t\treturn\n\t\t\tcase <-time.After(100 * time.Millisecond):\n\t\t\t\ts.RateLimitWarnf(\"Waiting for routing to be established...\")\n\t\t\t}\n\t\t} else {\n\t\t\tbreak\n\t\t}\n\t}\n\n\t// We may have paused adding entries to apply queue, resume here.\n\t// No-op if not paused.\n\tn.ResumeApply()\n\n\t// Send nil entry to signal the upper layers we are done doing replay/restore.\n\tn.apply.push(nil)\n\nrunner:\n\tfor s.isRunning() {\n\t\tswitch n.State() {\n\t\tcase Follower:\n\t\t\tn.runAsFollower()\n\t\tcase Candidate:\n\t\t\tn.runAsCandidate()\n\t\tcase Leader:\n\t\t\tn.runAsLeader()\n\t\tcase Closed:\n\t\t\tbreak runner\n\t\t}\n\t}\n\n\t// If we've reached this point then we're shutting down, either because\n\t// the server is stopping or because the Raft group is closing/closed.\n\tn.Lock()\n\tdefer n.Unlock()\n\n\tif c := n.c; c != nil {\n\t\tvar subs []*subscription\n\t\tc.mu.Lock()\n\t\tfor _, sub := range c.subs {\n\t\t\tsubs = append(subs, sub)\n\t\t}\n\t\tc.mu.Unlock()\n\t\tfor _, sub := range subs {\n\t\t\tn.unsubscribe(sub)\n\t\t}\n\t\tc.closeConnection(InternalClient)\n\t\tn.c = nil\n\t}\n\n\t// Unregistering ipQueues do not prevent them from push/pop\n\t// just will remove them from the central monitoring map\n\tqueues := []interface {\n\t\tunregister()\n\t\tdrain() int\n\t}{n.reqs, n.votes, n.prop, n.entry, n.resp, n.apply}\n\tfor _, q := range queues {\n\t\tq.drain()\n\t\tq.unregister()\n\t}\n\n\tn.s.unregisterRaftNode(n.group)\n\n\tif wal := n.wal; wal != nil {\n\t\twal.Stop()\n\t}\n\n\tn.debug(\"Shutdown\")\n}\n\nfunc (n *raft) debug(format string, args ...any) {\n\tif n.dflag {\n\t\tnf := fmt.Sprintf(\"RAFT [%s - %s] %s\", n.id, n.group, format)\n\t\tn.s.Debugf(nf, args...)\n\t}\n}\n\nfunc (n *raft) warn(format string, args ...any) {\n\tnf := fmt.Sprintf(\"RAFT [%s - %s] %s\", n.id, n.group, format)\n\tn.s.RateLimitWarnf(nf, args...)\n}\n\nfunc (n *raft) error(format string, args ...any) {\n\tnf := fmt.Sprintf(\"RAFT [%s - %s] %s\", n.id, n.group, format)\n\tn.s.Errorf(nf, args...)\n}\n\nfunc (n *raft) electTimer() *time.Timer {\n\tn.RLock()\n\tdefer n.RUnlock()\n\treturn n.elect\n}\n\nfunc (n *raft) IsObserver() bool {\n\tn.RLock()\n\tdefer n.RUnlock()\n\treturn n.observer\n}\n\n// Sets the state to observer only.\nfunc (n *raft) SetObserver(isObserver bool) {\n\tn.setObserver(isObserver, extUndetermined)\n}\n\nfunc (n *raft) setObserver(isObserver bool, extSt extensionState) {\n\tn.Lock()\n\tdefer n.Unlock()\n\n\tif n.paused {\n\t\t// Applies are paused so we're already in observer state.\n\t\t// Resuming the applies will set the state back to whatever\n\t\t// is in \"pobserver\", so update that instead.\n\t\tn.pobserver = isObserver\n\t\treturn\n\t}\n\n\twasObserver := n.observer\n\tn.observer = isObserver\n\tn.extSt = extSt\n\n\t// If we're leaving observer state then reset the election timer or\n\t// we might end up waiting for up to the observerModeInterval.\n\tif wasObserver && !isObserver {\n\t\tn.resetElect(randCampaignTimeout())\n\t}\n}\n\n// processAppendEntries is called by the Raft state machine when there are\n// new append entries to be committed and sent to the upper state machine.\nfunc (n *raft) processAppendEntries() {\n\tcanProcess := true\n\tif n.isClosed() {\n\t\tn.debug(\"AppendEntry not processing inbound, closed\")\n\t\tcanProcess = false\n\t}\n\tif n.outOfResources() {\n\t\tn.debug(\"AppendEntry not processing inbound, no resources\")\n\t\tcanProcess = false\n\t}\n\t// Always pop the entries, but check if we can process them. If we can't\n\t// then the entries are effectively dropped.\n\taes := n.entry.pop()\n\tif canProcess {\n\t\tfor _, ae := range aes {\n\t\t\tn.processAppendEntry(ae, ae.sub)\n\t\t}\n\t}\n\tn.entry.recycle(&aes)\n}\n\n// runAsFollower is called by run and will block for as long as the node is\n// running in the follower state.\nfunc (n *raft) runAsFollower() {\n\tfor n.State() == Follower {\n\t\telect := n.electTimer()\n\n\t\tselect {\n\t\tcase <-n.entry.ch:\n\t\t\t// New append entries have arrived over the network.\n\t\t\tn.processAppendEntries()\n\t\tcase <-n.s.quitCh:\n\t\t\t// The server is shutting down.\n\t\t\treturn\n\t\tcase <-n.quit:\n\t\t\t// The Raft node is shutting down.\n\t\t\treturn\n\t\tcase <-elect.C:\n\t\t\t// The election timer has fired so we think it's time to call an election.\n\t\t\t// If we are out of resources we just want to stay in this state for the moment.\n\t\t\tif n.outOfResources() {\n\t\t\t\tn.resetElectionTimeoutWithLock()\n\t\t\t\tn.debug(\"Not switching to candidate, no resources\")\n\t\t\t} else if n.IsObserver() {\n\t\t\t\tn.resetElectWithLock(observerModeInterval)\n\t\t\t\tn.debug(\"Not switching to candidate, observer only\")\n\t\t\t} else if n.isCatchingUp() {\n\t\t\t\tn.debug(\"Not switching to candidate, catching up\")\n\t\t\t\t// Check to see if our catchup has stalled.\n\t\t\t\tn.Lock()\n\t\t\t\tif n.catchupStalled() {\n\t\t\t\t\tn.cancelCatchup()\n\t\t\t\t}\n\t\t\t\tn.resetElectionTimeout()\n\t\t\t\tn.Unlock()\n\t\t\t} else {\n\t\t\t\tn.switchToCandidate()\n\t\t\t\treturn\n\t\t\t}\n\t\tcase <-n.votes.ch:\n\t\t\t// We're receiving votes from the network, probably because we have only\n\t\t\t// just stepped down and they were already in flight. Ignore them.\n\t\t\tn.debug(\"Ignoring old vote response, we have stepped down\")\n\t\t\tn.votes.popOne()\n\t\tcase <-n.resp.ch:\n\t\t\t// Ignore append entry responses received from before the state change.\n\t\t\tn.resp.drain()\n\t\tcase <-n.prop.ch:\n\t\t\t// Ignore proposals received from before the state change.\n\t\t\tn.prop.drain()\n\t\tcase <-n.reqs.ch:\n\t\t\t// We've just received a vote request from the network.\n\t\t\t// Because of drain() it is possible that we get nil from popOne().\n\t\t\tif voteReq, ok := n.reqs.popOne(); ok {\n\t\t\t\tn.processVoteRequest(voteReq)\n\t\t\t}\n\t\t}\n\t}\n}\n\n// Pool for CommittedEntry re-use.\nvar cePool = sync.Pool{\n\tNew: func() any {\n\t\treturn &CommittedEntry{}\n\t},\n}\n\n// CommittedEntry is handed back to the user to apply a commit to their upper layer.\ntype CommittedEntry struct {\n\tIndex   uint64\n\tEntries []*Entry\n}\n\n// Create a new CommittedEntry. When the returned entry is no longer needed, it\n// should be returned to the pool by calling ReturnToPool.\nfunc newCommittedEntry(index uint64, entries []*Entry) *CommittedEntry {\n\tce := cePool.Get().(*CommittedEntry)\n\tce.Index, ce.Entries = index, entries\n\treturn ce\n}\n\n// ReturnToPool returns the CommittedEntry to the pool, after which point it is\n// no longer safe to reuse.\nfunc (ce *CommittedEntry) ReturnToPool() {\n\tif ce == nil {\n\t\treturn\n\t}\n\tif len(ce.Entries) > 0 {\n\t\tfor _, e := range ce.Entries {\n\t\t\tentryPool.Put(e)\n\t\t}\n\t}\n\tce.Index, ce.Entries = 0, nil\n\tcePool.Put(ce)\n}\n\n// Pool for Entry re-use.\nvar entryPool = sync.Pool{\n\tNew: func() any {\n\t\treturn &Entry{}\n\t},\n}\n\n// Helper to create new entries. When the returned entry is no longer needed, it\n// should be returned to the entryPool pool.\nfunc newEntry(t EntryType, data []byte) *Entry {\n\tentry := entryPool.Get().(*Entry)\n\tentry.Type, entry.Data = t, data\n\treturn entry\n}\n\n// Pool for appendEntry re-use.\nvar aePool = sync.Pool{\n\tNew: func() any {\n\t\treturn &appendEntry{}\n\t},\n}\n\n// appendEntry is the main struct that is used to sync raft peers.\ntype appendEntry struct {\n\tleader  string   // The leader that this append entry came from.\n\tterm    uint64   // The term when this entry was stored.\n\tcommit  uint64   // The commit index of the leader when this append entry was sent.\n\tpterm   uint64   // The previous term, for checking consistency.\n\tpindex  uint64   // The previous commit index, for checking consistency.\n\tentries []*Entry // Entries to process.\n\t// Below fields are for internal use only:\n\tlterm uint64        // The highest term for catchups only, as the leader understands it. (If lterm=0, use term instead)\n\treply string        // Reply subject to respond to once committed.\n\tsub   *subscription // The subscription that the append entry came in on.\n\tbuf   []byte\n}\n\n// Create a new appendEntry.\nfunc newAppendEntry(leader string, term, commit, pterm, pindex uint64, entries []*Entry) *appendEntry {\n\tae := aePool.Get().(*appendEntry)\n\tae.leader, ae.term, ae.commit, ae.pterm, ae.pindex, ae.entries = leader, term, commit, pterm, pindex, entries\n\tae.lterm, ae.reply, ae.sub, ae.buf = 0, _EMPTY_, nil, nil\n\treturn ae\n}\n\n// Will return this append entry, and its interior entries to their respective pools.\nfunc (ae *appendEntry) returnToPool() {\n\tae.entries, ae.buf, ae.sub, ae.reply = nil, nil, nil, _EMPTY_\n\taePool.Put(ae)\n}\n\n// Pool for proposedEntry re-use.\nvar pePool = sync.Pool{\n\tNew: func() any {\n\t\treturn &proposedEntry{}\n\t},\n}\n\n// Create a new proposedEntry.\nfunc newProposedEntry(entry *Entry, reply string) *proposedEntry {\n\tpe := pePool.Get().(*proposedEntry)\n\tpe.Entry, pe.reply = entry, reply\n\treturn pe\n}\n\n// Will return this proosed entry.\nfunc (pe *proposedEntry) returnToPool() {\n\tpe.Entry, pe.reply = nil, _EMPTY_\n\tpePool.Put(pe)\n}\n\ntype EntryType uint8\n\nconst (\n\tEntryNormal EntryType = iota\n\tEntryOldSnapshot\n\tEntryPeerState\n\tEntryAddPeer\n\tEntryRemovePeer\n\tEntryLeaderTransfer\n\tEntrySnapshot\n)\n\nfunc (t EntryType) String() string {\n\tswitch t {\n\tcase EntryNormal:\n\t\treturn \"Normal\"\n\tcase EntryOldSnapshot:\n\t\treturn \"OldSnapshot\"\n\tcase EntryPeerState:\n\t\treturn \"PeerState\"\n\tcase EntryAddPeer:\n\t\treturn \"AddPeer\"\n\tcase EntryRemovePeer:\n\t\treturn \"RemovePeer\"\n\tcase EntryLeaderTransfer:\n\t\treturn \"LeaderTransfer\"\n\tcase EntrySnapshot:\n\t\treturn \"Snapshot\"\n\t}\n\treturn fmt.Sprintf(\"Unknown [%d]\", uint8(t))\n}\n\ntype Entry struct {\n\tType EntryType\n\tData []byte\n}\n\nfunc (ae *appendEntry) String() string {\n\treturn fmt.Sprintf(\"&{leader:%s term:%d commit:%d pterm:%d pindex:%d entries: %d}\",\n\t\tae.leader, ae.term, ae.commit, ae.pterm, ae.pindex, len(ae.entries))\n}\n\nconst appendEntryBaseLen = idLen + 4*8 + 2\n\nfunc (ae *appendEntry) encode(b []byte) ([]byte, error) {\n\tif ll := len(ae.leader); ll != idLen && ll != 0 {\n\t\treturn nil, errLeaderLen\n\t}\n\tif len(ae.entries) > math.MaxUint16 {\n\t\treturn nil, errTooManyEntries\n\t}\n\n\tvar elen uint64\n\tfor _, e := range ae.entries {\n\t\t// MaxInt32 instead of MaxUint32 deliberate here to stop int\n\t\t// overflow on 32-bit platforms, still gives us ~2GB limit.\n\t\tulen := uint64(len(e.Data))\n\t\tif ulen > math.MaxInt32 {\n\t\t\treturn nil, errBadAppendEntry\n\t\t}\n\t\telen += ulen + 1 + 4 // 1 is type, 4 is for size.\n\t}\n\t// Uvarint for lterm can be a maximum 10 bytes for a uint64.\n\tvar _lterm [10]byte\n\tlterm := _lterm[:binary.PutUvarint(_lterm[:], ae.lterm)]\n\ttlen := appendEntryBaseLen + elen + uint64(len(lterm))\n\n\tvar buf []byte\n\tif uint64(cap(b)) >= tlen {\n\t\tbuf = b[:idLen]\n\t} else {\n\t\tbuf = make([]byte, idLen, tlen)\n\t}\n\n\tvar le = binary.LittleEndian\n\tcopy(buf[:idLen], ae.leader)\n\tbuf = le.AppendUint64(buf, ae.term)\n\tbuf = le.AppendUint64(buf, ae.commit)\n\tbuf = le.AppendUint64(buf, ae.pterm)\n\tbuf = le.AppendUint64(buf, ae.pindex)\n\tbuf = le.AppendUint16(buf, uint16(len(ae.entries)))\n\tfor _, e := range ae.entries {\n\t\t// The +1 is safe here as we've already checked len(e.Data)\n\t\t// is not greater than MaxInt32, which is less than MaxUint32.\n\t\tbuf = le.AppendUint32(buf, uint32(1+len(e.Data)))\n\t\tbuf = append(buf, byte(e.Type))\n\t\tbuf = append(buf, e.Data...)\n\t}\n\t// This is safe because old nodes will ignore bytes after the\n\t// encoded messages. Nodes that are aware of this will decode\n\t// it correctly.\n\tbuf = append(buf, lterm...)\n\treturn buf, nil\n}\n\n// This can not be used post the wire level callback since we do not copy.\nfunc (n *raft) decodeAppendEntry(msg []byte, sub *subscription, reply string) (*appendEntry, error) {\n\tif len(msg) < appendEntryBaseLen {\n\t\treturn nil, errBadAppendEntry\n\t}\n\n\tvar le = binary.LittleEndian\n\n\tae := newAppendEntry(string(msg[:idLen]), le.Uint64(msg[8:]), le.Uint64(msg[16:]), le.Uint64(msg[24:]), le.Uint64(msg[32:]), nil)\n\tae.reply, ae.sub = reply, sub\n\n\t// Decode Entries.\n\tne, ri := int(le.Uint16(msg[40:])), uint64(42)\n\tfor i, max := 0, uint64(len(msg)); i < ne; i++ {\n\t\tif ri >= max-1 {\n\t\t\treturn nil, errBadAppendEntry\n\t\t}\n\t\tml := uint64(le.Uint32(msg[ri:]))\n\t\tri += 4\n\t\tif ml <= 0 || ri+ml > max {\n\t\t\treturn nil, errBadAppendEntry\n\t\t}\n\t\tentry := newEntry(EntryType(msg[ri]), msg[ri+1:ri+ml])\n\t\tae.entries = append(ae.entries, entry)\n\t\tri += ml\n\t}\n\tif len(msg[ri:]) > 0 {\n\t\tif lterm, n := binary.Uvarint(msg[ri:]); n > 0 {\n\t\t\tae.lterm = lterm\n\t\t}\n\t}\n\tae.buf = msg\n\treturn ae, nil\n}\n\n// Pool for appendEntryResponse re-use.\nvar arPool = sync.Pool{\n\tNew: func() any {\n\t\treturn &appendEntryResponse{}\n\t},\n}\n\n// We want to make sure this does not change from system changing length of syshash.\nconst idLen = 8\nconst appendEntryResponseLen = 24 + 1\n\n// appendEntryResponse is our response to a received appendEntry.\ntype appendEntryResponse struct {\n\tterm    uint64\n\tindex   uint64\n\tpeer    string\n\treply   string // internal usage.\n\tsuccess bool\n}\n\n// Create a new appendEntryResponse.\nfunc newAppendEntryResponse(term, index uint64, peer string, success bool) *appendEntryResponse {\n\tar := arPool.Get().(*appendEntryResponse)\n\tar.term, ar.index, ar.peer, ar.success = term, index, peer, success\n\t// Always empty out.\n\tar.reply = _EMPTY_\n\treturn ar\n}\n\nfunc (ar *appendEntryResponse) encode(b []byte) []byte {\n\tvar buf []byte\n\tif cap(b) >= appendEntryResponseLen {\n\t\tbuf = b[:appendEntryResponseLen]\n\t} else {\n\t\tbuf = make([]byte, appendEntryResponseLen)\n\t}\n\tvar le = binary.LittleEndian\n\tle.PutUint64(buf[0:], ar.term)\n\tle.PutUint64(buf[8:], ar.index)\n\tcopy(buf[16:16+idLen], ar.peer)\n\tif ar.success {\n\t\tbuf[24] = 1\n\t} else {\n\t\tbuf[24] = 0\n\t}\n\treturn buf[:appendEntryResponseLen]\n}\n\n// Track all peers we may have ever seen to use an string interns for appendEntryResponse decoding.\nvar peers sync.Map\n\nfunc (n *raft) decodeAppendEntryResponse(msg []byte) *appendEntryResponse {\n\tif len(msg) != appendEntryResponseLen {\n\t\treturn nil\n\t}\n\tvar le = binary.LittleEndian\n\tar := arPool.Get().(*appendEntryResponse)\n\tar.term = le.Uint64(msg[0:])\n\tar.index = le.Uint64(msg[8:])\n\n\tpeer, ok := peers.Load(string(msg[16 : 16+idLen]))\n\tif !ok {\n\t\t// We missed so store inline here.\n\t\tpeer = string(msg[16 : 16+idLen])\n\t\tpeers.Store(peer, peer)\n\t}\n\tar.peer = peer.(string)\n\tar.success = msg[24] == 1\n\treturn ar\n}\n\n// Called when a remove peer proposal has been forwarded\nfunc (n *raft) handleForwardedRemovePeerProposal(sub *subscription, c *client, _ *Account, _, reply string, msg []byte) {\n\tn.debug(\"Received forwarded remove peer proposal: %q\", msg)\n\n\tif n.State() != Leader {\n\t\tn.debug(\"Ignoring forwarded peer removal proposal, not leader\")\n\t\treturn\n\t}\n\tif len(msg) != idLen {\n\t\tn.warn(\"Received invalid peer name for remove proposal: %q\", msg)\n\t\treturn\n\t}\n\n\tn.RLock()\n\tprop, werr := n.prop, n.werr\n\tn.RUnlock()\n\n\t// Ignore if we have had a write error previous.\n\tif werr != nil {\n\t\treturn\n\t}\n\n\t// Need to copy since this is underlying client/route buffer.\n\tpeer := copyBytes(msg)\n\tprop.push(newProposedEntry(newEntry(EntryRemovePeer, peer), reply))\n}\n\n// Called when a peer has forwarded a proposal.\nfunc (n *raft) handleForwardedProposal(sub *subscription, c *client, _ *Account, _, reply string, msg []byte) {\n\tif n.State() != Leader {\n\t\tn.debug(\"Ignoring forwarded proposal, not leader\")\n\t\treturn\n\t}\n\t// Need to copy since this is underlying client/route buffer.\n\tmsg = copyBytes(msg)\n\n\tn.RLock()\n\tprop, werr := n.prop, n.werr\n\tn.RUnlock()\n\n\t// Ignore if we have had a write error previous.\n\tif werr != nil {\n\t\treturn\n\t}\n\n\tprop.push(newProposedEntry(newEntry(EntryNormal, msg), reply))\n}\n\nfunc (n *raft) runAsLeader() {\n\tif n.State() == Closed {\n\t\treturn\n\t}\n\n\tn.Lock()\n\tpsubj, rpsubj := n.psubj, n.rpsubj\n\n\t// For forwarded proposals, both normal and remove peer proposals.\n\tfsub, err := n.subscribe(psubj, n.handleForwardedProposal)\n\tif err != nil {\n\t\tn.warn(\"Error subscribing to forwarded proposals: %v\", err)\n\t\tn.stepdownLocked(noLeader)\n\t\tn.Unlock()\n\t\treturn\n\t}\n\trpsub, err := n.subscribe(rpsubj, n.handleForwardedRemovePeerProposal)\n\tif err != nil {\n\t\tn.warn(\"Error subscribing to forwarded remove peer proposals: %v\", err)\n\t\tn.unsubscribe(fsub)\n\t\tn.stepdownLocked(noLeader)\n\t\tn.Unlock()\n\t\treturn\n\t}\n\tn.Unlock()\n\n\t// Cleanup our subscription when we leave.\n\tdefer func() {\n\t\tn.Lock()\n\t\tn.unsubscribe(fsub)\n\t\tn.unsubscribe(rpsub)\n\t\tn.Unlock()\n\t}()\n\n\t// To send out our initial peer state.\n\tn.sendPeerState()\n\n\thb := time.NewTicker(hbInterval)\n\tdefer hb.Stop()\n\n\tlq := time.NewTicker(lostQuorumCheck)\n\tdefer lq.Stop()\n\n\tfor n.State() == Leader {\n\t\tselect {\n\t\tcase <-n.s.quitCh:\n\t\t\treturn\n\t\tcase <-n.quit:\n\t\t\treturn\n\t\tcase <-n.resp.ch:\n\t\t\tars := n.resp.pop()\n\t\t\tfor _, ar := range ars {\n\t\t\t\tn.processAppendEntryResponse(ar)\n\t\t\t}\n\t\t\tn.resp.recycle(&ars)\n\t\tcase <-n.prop.ch:\n\t\t\tconst maxBatch = 256 * 1024\n\t\t\tconst maxEntries = 512\n\t\t\tvar entries []*Entry\n\n\t\t\tes, sz := n.prop.pop(), 0\n\t\t\tfor _, b := range es {\n\t\t\t\tif b.Type == EntryRemovePeer {\n\t\t\t\t\tn.doRemovePeerAsLeader(string(b.Data))\n\t\t\t\t}\n\t\t\t\tentries = append(entries, b.Entry)\n\t\t\t\t// Increment size.\n\t\t\t\tsz += len(b.Data) + 1\n\t\t\t\t// If below thresholds go ahead and send.\n\t\t\t\tif sz < maxBatch && len(entries) < maxEntries {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tn.sendAppendEntry(entries)\n\t\t\t\t// Reset our sz and entries.\n\t\t\t\t// We need to re-create `entries` because there is a reference\n\t\t\t\t// to it in the node's pae map.\n\t\t\t\tsz, entries = 0, nil\n\t\t\t}\n\t\t\tif len(entries) > 0 {\n\t\t\t\tn.sendAppendEntry(entries)\n\t\t\t}\n\t\t\t// Respond to any proposals waiting for a confirmation.\n\t\t\tfor _, pe := range es {\n\t\t\t\tif pe.reply != _EMPTY_ {\n\t\t\t\t\tn.sendReply(pe.reply, nil)\n\t\t\t\t}\n\t\t\t\tpe.returnToPool()\n\t\t\t}\n\t\t\tn.prop.recycle(&es)\n\n\t\tcase <-hb.C:\n\t\t\tif n.notActive() {\n\t\t\t\tn.sendHeartbeat()\n\t\t\t}\n\t\tcase <-lq.C:\n\t\t\tif n.lostQuorum() {\n\t\t\t\tn.stepdown(noLeader)\n\t\t\t\treturn\n\t\t\t}\n\t\tcase <-n.votes.ch:\n\t\t\t// Because of drain() it is possible that we get nil from popOne().\n\t\t\tvresp, ok := n.votes.popOne()\n\t\t\tif !ok {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif vresp.term > n.Term() {\n\t\t\t\tn.stepdown(noLeader)\n\t\t\t\treturn\n\t\t\t}\n\t\t\tn.trackPeer(vresp.peer)\n\t\tcase <-n.reqs.ch:\n\t\t\t// Because of drain() it is possible that we get nil from popOne().\n\t\t\tif voteReq, ok := n.reqs.popOne(); ok {\n\t\t\t\tn.processVoteRequest(voteReq)\n\t\t\t}\n\t\tcase <-n.entry.ch:\n\t\t\tn.processAppendEntries()\n\t\t}\n\t}\n}\n\n// Quorum reports the quorum status. Will be called on former leaders.\nfunc (n *raft) Quorum() bool {\n\tn.RLock()\n\tdefer n.RUnlock()\n\n\tnow, nc := time.Now().UnixNano(), 0\n\tfor id, peer := range n.peers {\n\t\tif id == n.id || time.Duration(now-peer.ts) < lostQuorumInterval {\n\t\t\tnc++\n\t\t\tif nc >= n.qn {\n\t\t\t\treturn true\n\t\t\t}\n\t\t}\n\t}\n\treturn false\n}\n\nfunc (n *raft) lostQuorum() bool {\n\tn.RLock()\n\tdefer n.RUnlock()\n\treturn n.lostQuorumLocked()\n}\n\nfunc (n *raft) lostQuorumLocked() bool {\n\t// In order to avoid false positives that can happen in heavily loaded systems\n\t// make sure nothing is queued up that we have not processed yet.\n\t// Also make sure we let any scale up actions settle before deciding.\n\tif n.resp.len() != 0 || (!n.lsut.IsZero() && time.Since(n.lsut) < lostQuorumInterval) {\n\t\treturn false\n\t}\n\n\tnow, nc := time.Now().UnixNano(), 0\n\tfor id, peer := range n.peers {\n\t\tif id == n.id || time.Duration(now-peer.ts) < lostQuorumInterval {\n\t\t\tnc++\n\t\t\tif nc >= n.qn {\n\t\t\t\treturn false\n\t\t\t}\n\t\t}\n\t}\n\treturn true\n}\n\n// Check for being not active in terms of sending entries.\n// Used in determining if we need to send a heartbeat.\nfunc (n *raft) notActive() bool {\n\tn.RLock()\n\tdefer n.RUnlock()\n\treturn time.Since(n.active) > hbInterval\n}\n\n// Return our current term.\nfunc (n *raft) Term() uint64 {\n\tn.RLock()\n\tdefer n.RUnlock()\n\treturn n.term\n}\n\n// Lock should be held.\nfunc (n *raft) loadFirstEntry() (ae *appendEntry, err error) {\n\tvar state StreamState\n\tn.wal.FastState(&state)\n\treturn n.loadEntry(state.FirstSeq)\n}\n\nfunc (n *raft) runCatchup(ar *appendEntryResponse, indexUpdatesQ *ipQueue[uint64]) {\n\tn.RLock()\n\ts, reply := n.s, n.areply\n\tpeer, subj, term, last := ar.peer, ar.reply, n.term, n.pindex\n\tn.RUnlock()\n\n\tdefer s.grWG.Done()\n\tdefer arPool.Put(ar)\n\n\tdefer func() {\n\t\tn.Lock()\n\t\tdelete(n.progress, peer)\n\t\tif len(n.progress) == 0 {\n\t\t\tn.progress = nil\n\t\t}\n\t\t// Check if this is a new peer and if so go ahead and propose adding them.\n\t\t_, exists := n.peers[peer]\n\t\tn.Unlock()\n\t\tif !exists {\n\t\t\tn.debug(\"Catchup done for %q, will add into peers\", peer)\n\t\t\tn.ProposeAddPeer(peer)\n\t\t}\n\t\tindexUpdatesQ.unregister()\n\t}()\n\n\tn.debug(\"Running catchup for %q\", peer)\n\n\tconst maxOutstanding = 2 * 1024 * 1024 // 2MB for now.\n\tnext, total, om := uint64(0), 0, make(map[uint64]int)\n\n\tsendNext := func() bool {\n\t\tfor total <= maxOutstanding {\n\t\t\tnext++\n\t\t\tif next > last {\n\t\t\t\treturn true\n\t\t\t}\n\t\t\tae, err := n.loadEntry(next)\n\t\t\tif err != nil {\n\t\t\t\tif err != ErrStoreEOF {\n\t\t\t\t\tn.warn(\"Got an error loading %d index: %v\", next, err)\n\t\t\t\t}\n\t\t\t\treturn true\n\t\t\t}\n\t\t\t// Re-encode with the lterm if needed\n\t\t\tif ae.lterm != term {\n\t\t\t\tae.lterm = term\n\t\t\t\tif ae.buf, err = ae.encode(ae.buf[:0]); err != nil {\n\t\t\t\t\tn.warn(\"Got an error re-encoding append entry: %v\", err)\n\t\t\t\t\treturn true\n\t\t\t\t}\n\t\t\t}\n\t\t\t// Update our tracking total.\n\t\t\tom[next] = len(ae.buf)\n\t\t\ttotal += len(ae.buf)\n\t\t\tn.sendRPC(subj, reply, ae.buf)\n\t\t}\n\t\treturn false\n\t}\n\n\tconst activityInterval = 2 * time.Second\n\ttimeout := time.NewTimer(activityInterval)\n\tdefer timeout.Stop()\n\n\tstepCheck := time.NewTicker(100 * time.Millisecond)\n\tdefer stepCheck.Stop()\n\n\t// Run as long as we are leader and still not caught up.\n\tfor n.State() == Leader {\n\t\tselect {\n\t\tcase <-n.s.quitCh:\n\t\t\treturn\n\t\tcase <-n.quit:\n\t\t\treturn\n\t\tcase <-stepCheck.C:\n\t\t\tif n.State() != Leader {\n\t\t\t\tn.debug(\"Catching up canceled, no longer leader\")\n\t\t\t\treturn\n\t\t\t}\n\t\tcase <-timeout.C:\n\t\t\tn.debug(\"Catching up for %q stalled\", peer)\n\t\t\treturn\n\t\tcase <-indexUpdatesQ.ch:\n\t\t\tif index, ok := indexUpdatesQ.popOne(); ok {\n\t\t\t\t// Update our activity timer.\n\t\t\t\ttimeout.Reset(activityInterval)\n\t\t\t\t// Update outstanding total.\n\t\t\t\ttotal -= om[index]\n\t\t\t\tdelete(om, index)\n\t\t\t\tif next == 0 {\n\t\t\t\t\tnext = index\n\t\t\t\t}\n\t\t\t\t// Check if we are done.\n\t\t\t\tif index > last || sendNext() {\n\t\t\t\t\tn.debug(\"Finished catching up\")\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n\n// Lock should be held.\nfunc (n *raft) sendSnapshotToFollower(subject string) (uint64, error) {\n\tsnap, err := n.loadLastSnapshot()\n\tif err != nil {\n\t\t// We need to stepdown here when this happens.\n\t\tn.stepdownLocked(noLeader)\n\t\t// We need to reset our state here as well.\n\t\tn.resetWAL()\n\t\treturn 0, err\n\t}\n\t// Go ahead and send the snapshot and peerstate here as first append entry to the catchup follower.\n\tae := n.buildAppendEntry([]*Entry{{EntrySnapshot, snap.data}, {EntryPeerState, snap.peerstate}})\n\tae.pterm, ae.pindex = snap.lastTerm, snap.lastIndex\n\tvar state StreamState\n\tn.wal.FastState(&state)\n\n\tfpIndex := state.FirstSeq - 1\n\tif snap.lastIndex < fpIndex && state.FirstSeq != 0 {\n\t\tsnap.lastIndex = fpIndex\n\t\tae.pindex = fpIndex\n\t}\n\n\tencoding, err := ae.encode(nil)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\tn.sendRPC(subject, n.areply, encoding)\n\treturn snap.lastIndex, nil\n}\n\nfunc (n *raft) catchupFollower(ar *appendEntryResponse) {\n\tn.debug(\"Being asked to catch up follower: %q\", ar.peer)\n\tn.Lock()\n\tif n.progress == nil {\n\t\tn.progress = make(map[string]*ipQueue[uint64])\n\t} else if q, ok := n.progress[ar.peer]; ok {\n\t\tn.debug(\"Will cancel existing entry for catching up %q\", ar.peer)\n\t\tdelete(n.progress, ar.peer)\n\t\tq.push(n.pindex)\n\t}\n\n\t// Check to make sure we have this entry.\n\tstart := ar.index + 1\n\tvar state StreamState\n\tn.wal.FastState(&state)\n\n\tif start < state.FirstSeq || (state.Msgs == 0 && start <= state.LastSeq) {\n\t\tn.debug(\"Need to send snapshot to follower\")\n\t\tif lastIndex, err := n.sendSnapshotToFollower(ar.reply); err != nil {\n\t\t\tn.error(\"Error sending snapshot to follower [%s]: %v\", ar.peer, err)\n\t\t\tn.Unlock()\n\t\t\tarPool.Put(ar)\n\t\t\treturn\n\t\t} else {\n\t\t\tstart = lastIndex + 1\n\t\t\t// If no other entries, we can just return here.\n\t\t\tif state.Msgs == 0 || start > state.LastSeq {\n\t\t\t\tn.debug(\"Finished catching up\")\n\t\t\t\tn.Unlock()\n\t\t\t\tarPool.Put(ar)\n\t\t\t\treturn\n\t\t\t}\n\t\t\tn.debug(\"Snapshot sent, reset first catchup entry to %d\", lastIndex)\n\t\t}\n\t}\n\n\tae, err := n.loadEntry(start)\n\tif err != nil {\n\t\tn.warn(\"Request from follower for entry at index [%d] errored for state %+v - %v\", start, state, err)\n\t\tif err == ErrStoreEOF {\n\t\t\t// If we are here we are seeing a request for an item beyond our state, meaning we should stepdown.\n\t\t\tn.stepdownLocked(noLeader)\n\t\t\tn.Unlock()\n\t\t\tarPool.Put(ar)\n\t\t\treturn\n\t\t}\n\t\tae, err = n.loadFirstEntry()\n\t}\n\tif err != nil || ae == nil {\n\t\tn.warn(\"Could not find a starting entry for catchup request: %v\", err)\n\t\t// If we are here we are seeing a request for an item we do not have, meaning we should stepdown.\n\t\t// This is possible on a reset of our WAL but the other side has a snapshot already.\n\t\t// If we do not stepdown this can cycle.\n\t\tn.stepdownLocked(noLeader)\n\t\tn.Unlock()\n\t\tarPool.Put(ar)\n\t\treturn\n\t}\n\tif ae.pindex != ar.index || ae.pterm != ar.term {\n\t\tn.debug(\"Our first entry [%d:%d] does not match request from follower [%d:%d]\", ae.pterm, ae.pindex, ar.term, ar.index)\n\t}\n\t// Create a queue for delivering updates from responses.\n\tindexUpdates := newIPQueue[uint64](n.s, fmt.Sprintf(\"[ACC:%s] RAFT '%s' indexUpdates\", n.accName, n.group))\n\tindexUpdates.push(ae.pindex)\n\tn.progress[ar.peer] = indexUpdates\n\tn.Unlock()\n\n\tn.wg.Add(1)\n\tn.s.startGoRoutine(func() {\n\t\tdefer n.wg.Done()\n\t\tn.runCatchup(ar, indexUpdates)\n\t})\n}\n\nfunc (n *raft) loadEntry(index uint64) (*appendEntry, error) {\n\tvar smp StoreMsg\n\tsm, err := n.wal.LoadMsg(index, &smp)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn n.decodeAppendEntry(sm.msg, nil, _EMPTY_)\n}\n\n// applyCommit will update our commit index and apply the entry to the apply queue.\n// lock should be held.\nfunc (n *raft) applyCommit(index uint64) error {\n\tif n.State() == Closed {\n\t\treturn errNodeClosed\n\t}\n\tif index <= n.commit {\n\t\tn.debug(\"Ignoring apply commit for %d, already processed\", index)\n\t\treturn nil\n\t}\n\n\tif n.State() == Leader {\n\t\tdelete(n.acks, index)\n\t}\n\n\tae := n.pae[index]\n\tif ae == nil {\n\t\tvar state StreamState\n\t\tn.wal.FastState(&state)\n\t\tif index < state.FirstSeq {\n\t\t\treturn nil\n\t\t}\n\t\tvar err error\n\t\tif ae, err = n.loadEntry(index); err != nil {\n\t\t\tif err != ErrStoreClosed && err != ErrStoreEOF {\n\t\t\t\tn.warn(\"Got an error loading %d index: %v - will reset\", index, err)\n\t\t\t\tif n.State() == Leader {\n\t\t\t\t\tn.stepdownLocked(n.selectNextLeader())\n\t\t\t\t}\n\t\t\t\t// Reset and cancel any catchup.\n\t\t\t\tn.resetWAL()\n\t\t\t\tn.cancelCatchup()\n\t\t\t}\n\t\t\treturn errEntryLoadFailed\n\t\t}\n\t} else {\n\t\tdefer delete(n.pae, index)\n\t}\n\n\tn.commit = index\n\tae.buf = nil\n\n\tvar committed []*Entry\n\tfor _, e := range ae.entries {\n\t\tswitch e.Type {\n\t\tcase EntryNormal:\n\t\t\tcommitted = append(committed, e)\n\t\tcase EntryOldSnapshot:\n\t\t\t// For old snapshots in our WAL.\n\t\t\tcommitted = append(committed, newEntry(EntrySnapshot, e.Data))\n\t\tcase EntrySnapshot:\n\t\t\tcommitted = append(committed, e)\n\t\tcase EntryPeerState:\n\t\t\tif n.State() != Leader {\n\t\t\t\tif ps, err := decodePeerState(e.Data); err == nil {\n\t\t\t\t\tn.processPeerState(ps)\n\t\t\t\t}\n\t\t\t}\n\t\tcase EntryAddPeer:\n\t\t\tnewPeer := string(e.Data)\n\t\t\tn.debug(\"Added peer %q\", newPeer)\n\n\t\t\t// Store our peer in our global peer map for all peers.\n\t\t\tpeers.LoadOrStore(newPeer, newPeer)\n\n\t\t\t// If we were on the removed list reverse that here.\n\t\t\tif n.removed != nil {\n\t\t\t\tdelete(n.removed, newPeer)\n\t\t\t}\n\n\t\t\tif lp, ok := n.peers[newPeer]; !ok {\n\t\t\t\t// We are not tracking this one automatically so we need to bump cluster size.\n\t\t\t\tn.peers[newPeer] = &lps{time.Now().UnixNano(), 0, true}\n\t\t\t} else {\n\t\t\t\t// Mark as added.\n\t\t\t\tlp.kp = true\n\t\t\t}\n\t\t\t// Adjust cluster size and quorum if needed.\n\t\t\tn.adjustClusterSizeAndQuorum()\n\t\t\t// Write out our new state.\n\t\t\tn.writePeerState(&peerState{n.peerNames(), n.csz, n.extSt})\n\t\t\t// We pass these up as well.\n\t\t\tcommitted = append(committed, e)\n\n\t\tcase EntryRemovePeer:\n\t\t\tpeer := string(e.Data)\n\t\t\tn.debug(\"Removing peer %q\", peer)\n\n\t\t\t// Make sure we have our removed map.\n\t\t\tif n.removed == nil {\n\t\t\t\tn.removed = make(map[string]time.Time)\n\t\t\t}\n\t\t\tn.removed[peer] = time.Now()\n\n\t\t\tif _, ok := n.peers[peer]; ok {\n\t\t\t\tdelete(n.peers, peer)\n\t\t\t\t// We should decrease our cluster size since we are tracking this peer.\n\t\t\t\tn.adjustClusterSizeAndQuorum()\n\t\t\t\t// Write out our new state.\n\t\t\t\tn.writePeerState(&peerState{n.peerNames(), n.csz, n.extSt})\n\t\t\t}\n\n\t\t\t// If this is us and we are the leader we should attempt to stepdown.\n\t\t\tif peer == n.id && n.State() == Leader {\n\t\t\t\tn.stepdownLocked(n.selectNextLeader())\n\t\t\t}\n\n\t\t\t// Remove from string intern map.\n\t\t\tpeers.Delete(peer)\n\n\t\t\t// We pass these up as well.\n\t\t\tcommitted = append(committed, e)\n\t\t}\n\t}\n\t// Pass to the upper layers if we have normal entries. It is\n\t// entirely possible that 'committed' might be an empty slice here,\n\t// which will happen if we've processed updates inline (like peer\n\t// states). In which case the upper layer will just call down with\n\t// Applied() with no further action.\n\tn.apply.push(newCommittedEntry(index, committed))\n\t// Place back in the pool.\n\tae.returnToPool()\n\treturn nil\n}\n\n// Used to track a success response and apply entries.\nfunc (n *raft) trackResponse(ar *appendEntryResponse) {\n\tif n.State() == Closed {\n\t\treturn\n\t}\n\n\tn.Lock()\n\n\t// Check state under lock, we might not be leader anymore.\n\tif n.State() != Leader {\n\t\tn.Unlock()\n\t\treturn\n\t}\n\n\t// Update peer's last index.\n\tif ps := n.peers[ar.peer]; ps != nil && ar.index > ps.li {\n\t\tps.li = ar.index\n\t}\n\n\t// If we are tracking this peer as a catchup follower, update that here.\n\tif indexUpdateQ := n.progress[ar.peer]; indexUpdateQ != nil {\n\t\tindexUpdateQ.push(ar.index)\n\t}\n\n\t// Ignore items already committed.\n\tif ar.index <= n.commit {\n\t\tn.Unlock()\n\t\treturn\n\t}\n\n\t// See if we have items to apply.\n\tvar sendHB bool\n\n\tresults := n.acks[ar.index]\n\tif results == nil {\n\t\tresults = make(map[string]struct{})\n\t\tn.acks[ar.index] = results\n\t}\n\tresults[ar.peer] = struct{}{}\n\n\t// We don't count ourselves to account for leader changes, so add 1.\n\tif nr := len(results); nr+1 >= n.qn {\n\t\t// We have a quorum.\n\t\tfor index := n.commit + 1; index <= ar.index; index++ {\n\t\t\tif err := n.applyCommit(index); err != nil && err != errNodeClosed {\n\t\t\t\tn.error(\"Got an error applying commit for %d: %v\", index, err)\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\tsendHB = n.prop.len() == 0\n\t}\n\tn.Unlock()\n\n\tif sendHB {\n\t\tn.sendHeartbeat()\n\t}\n}\n\n// Used to adjust cluster size and peer count based on added official peers.\n// lock should be held.\nfunc (n *raft) adjustClusterSizeAndQuorum() {\n\tpcsz, ncsz := n.csz, 0\n\tfor _, peer := range n.peers {\n\t\tif peer.kp {\n\t\t\tncsz++\n\t\t}\n\t}\n\tn.csz = ncsz\n\tn.qn = n.csz/2 + 1\n\n\tif ncsz > pcsz {\n\t\tn.debug(\"Expanding our clustersize: %d -> %d\", pcsz, ncsz)\n\t\tn.lsut = time.Now()\n\t} else if ncsz < pcsz {\n\t\tn.debug(\"Decreasing our clustersize: %d -> %d\", pcsz, ncsz)\n\t\tif n.State() == Leader {\n\t\t\tgo n.sendHeartbeat()\n\t\t}\n\t}\n\tif ncsz != pcsz {\n\t\tn.recreateInternalSubsLocked()\n\t}\n}\n\n// Track interactions with this peer.\nfunc (n *raft) trackPeer(peer string) error {\n\tn.Lock()\n\tvar needPeerAdd, isRemoved bool\n\tvar rts time.Time\n\tif n.removed != nil {\n\t\trts, isRemoved = n.removed[peer]\n\t\t// Removed peers can rejoin after timeout.\n\t\tif isRemoved && time.Since(rts) >= peerRemoveTimeout {\n\t\t\tisRemoved = false\n\t\t}\n\t}\n\tif n.State() == Leader {\n\t\tif lp, ok := n.peers[peer]; !ok || !lp.kp {\n\t\t\t// Check if this peer had been removed previously.\n\t\t\tneedPeerAdd = !isRemoved\n\t\t}\n\t}\n\tif ps := n.peers[peer]; ps != nil {\n\t\tps.ts = time.Now().UnixNano()\n\t} else if !isRemoved {\n\t\tn.peers[peer] = &lps{time.Now().UnixNano(), 0, false}\n\t}\n\tn.Unlock()\n\n\tif needPeerAdd {\n\t\tn.ProposeAddPeer(peer)\n\t}\n\treturn nil\n}\n\nfunc (n *raft) runAsCandidate() {\n\tn.Lock()\n\t// Drain old responses.\n\tn.votes.drain()\n\tn.Unlock()\n\n\t// Send out our request for votes.\n\tn.requestVote()\n\n\t// We vote for ourselves.\n\tvotes := map[string]struct{}{\n\t\tn.ID(): {},\n\t}\n\n\tfor n.State() == Candidate {\n\t\telect := n.electTimer()\n\t\tselect {\n\t\tcase <-n.entry.ch:\n\t\t\tn.processAppendEntries()\n\t\tcase <-n.resp.ch:\n\t\t\t// Ignore append entry responses received from before the state change.\n\t\t\tn.resp.drain()\n\t\tcase <-n.prop.ch:\n\t\t\t// Ignore proposals received from before the state change.\n\t\t\tn.prop.drain()\n\t\tcase <-n.s.quitCh:\n\t\t\treturn\n\t\tcase <-n.quit:\n\t\t\treturn\n\t\tcase <-elect.C:\n\t\t\tn.switchToCandidate()\n\t\t\treturn\n\t\tcase <-n.votes.ch:\n\t\t\t// Because of drain() it is possible that we get nil from popOne().\n\t\t\tvresp, ok := n.votes.popOne()\n\t\t\tif !ok {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tn.RLock()\n\t\t\tnterm := n.term\n\t\t\tn.RUnlock()\n\n\t\t\tif vresp.granted && nterm == vresp.term {\n\t\t\t\t// only track peers that would be our followers\n\t\t\t\tn.trackPeer(vresp.peer)\n\t\t\t\tvotes[vresp.peer] = struct{}{}\n\t\t\t\tif n.wonElection(len(votes)) {\n\t\t\t\t\t// Become LEADER if we have won and gotten a quorum with everyone we should hear from.\n\t\t\t\t\tn.switchToLeader()\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t} else if vresp.term > nterm {\n\t\t\t\t// if we observe a bigger term, we should start over again or risk forming a quorum fully knowing\n\t\t\t\t// someone with a better term exists. This is even the right thing to do if won == true.\n\t\t\t\tn.Lock()\n\t\t\t\tn.debug(\"Stepping down from candidate, detected higher term: %d vs %d\", vresp.term, n.term)\n\t\t\t\tn.term = vresp.term\n\t\t\t\tn.vote = noVote\n\t\t\t\tn.writeTermVote()\n\t\t\t\tn.lxfer = false\n\t\t\t\tn.stepdownLocked(noLeader)\n\t\t\t\tn.Unlock()\n\t\t\t}\n\t\tcase <-n.reqs.ch:\n\t\t\t// Because of drain() it is possible that we get nil from popOne().\n\t\t\tif voteReq, ok := n.reqs.popOne(); ok {\n\t\t\t\tn.processVoteRequest(voteReq)\n\t\t\t}\n\t\t}\n\t}\n}\n\n// handleAppendEntry handles an append entry from the wire. This function\n// is an internal callback from the \"asubj\" append entry subscription.\nfunc (n *raft) handleAppendEntry(sub *subscription, c *client, _ *Account, _, reply string, msg []byte) {\n\tmsg = copyBytes(msg)\n\tif ae, err := n.decodeAppendEntry(msg, sub, reply); err == nil {\n\t\t// Push to the new entry channel. From here one of the worker\n\t\t// goroutines (runAsLeader, runAsFollower, runAsCandidate) will\n\t\t// pick it up.\n\t\tn.entry.push(ae)\n\t} else {\n\t\tn.warn(\"AppendEntry failed to be placed on internal channel: corrupt entry\")\n\t}\n}\n\n// cancelCatchup will stop an in-flight catchup by unsubscribing from the\n// catchup subscription.\n// Lock should be held.\nfunc (n *raft) cancelCatchup() {\n\tn.debug(\"Canceling catchup subscription since we are now up to date\")\n\n\tif n.catchup != nil && n.catchup.sub != nil {\n\t\tn.unsubscribe(n.catchup.sub)\n\t}\n\tn.catchup = nil\n}\n\n// catchupStalled will try to determine if we are stalled. This is called\n// on a new entry from our leader.\n// Lock should be held.\nfunc (n *raft) catchupStalled() bool {\n\tif n.catchup == nil {\n\t\treturn false\n\t}\n\tif n.catchup.pindex == n.pindex {\n\t\treturn time.Since(n.catchup.active) > 2*time.Second\n\t}\n\tn.catchup.pindex = n.pindex\n\tn.catchup.active = time.Now()\n\treturn false\n}\n\n// createCatchup will create the state needed to track a catchup as it\n// runs. It then creates a unique inbox for this catchup and subscribes\n// to it. The remote side will stream entries to that subject.\n// Lock should be held.\nfunc (n *raft) createCatchup(ae *appendEntry) string {\n\t// Cleanup any old ones.\n\tif n.catchup != nil && n.catchup.sub != nil {\n\t\tn.unsubscribe(n.catchup.sub)\n\t}\n\t// Snapshot term and index.\n\tn.catchup = &catchupState{\n\t\tcterm:  ae.pterm,\n\t\tcindex: ae.pindex,\n\t\tpterm:  n.pterm,\n\t\tpindex: n.pindex,\n\t\tactive: time.Now(),\n\t}\n\tinbox := n.newCatchupInbox()\n\tsub, _ := n.subscribe(inbox, n.handleAppendEntry)\n\tn.catchup.sub = sub\n\n\treturn inbox\n}\n\n// Truncate our WAL and reset.\n// Lock should be held.\nfunc (n *raft) truncateWAL(term, index uint64) {\n\tn.debug(\"Truncating and repairing WAL to Term %d Index %d\", term, index)\n\n\tif term == 0 && index == 0 {\n\t\tif n.commit > 0 {\n\t\t\tn.warn(\"Resetting WAL state\")\n\t\t} else {\n\t\t\tn.debug(\"Clearing WAL state (no commits)\")\n\t\t}\n\t}\n\n\tdefer func() {\n\t\t// Check to see if we invalidated any snapshots that might have held state\n\t\t// from the entries we are truncating.\n\t\tif snap, _ := n.loadLastSnapshot(); snap != nil && snap.lastIndex > index {\n\t\t\tos.Remove(n.snapfile)\n\t\t\tn.snapfile = _EMPTY_\n\t\t}\n\t\t// Make sure to reset commit and applied if above\n\t\tif n.commit > n.pindex {\n\t\t\tn.commit = n.pindex\n\t\t}\n\t\tif n.applied > n.commit {\n\t\t\tn.applied = n.commit\n\t\t}\n\t}()\n\n\tif err := n.wal.Truncate(index); err != nil {\n\t\t// If we get an invalid sequence, reset our wal all together.\n\t\t// We will not have holes, so this means we do not have this message stored anymore.\n\t\t// This is normal when truncating back to applied/snapshot.\n\t\tif err == ErrInvalidSequence {\n\t\t\tn.debug(\"Clearing WAL\")\n\t\t\tn.wal.Truncate(0)\n\t\t\t// If our index is non-zero use PurgeEx to set us to the correct next index.\n\t\t\tif index > 0 {\n\t\t\t\tn.wal.PurgeEx(fwcs, index+1, 0)\n\t\t\t}\n\t\t} else {\n\t\t\tn.warn(\"Error truncating WAL: %v\", err)\n\t\t\tn.setWriteErrLocked(err)\n\t\t\treturn\n\t\t}\n\t}\n\t// Set after we know we have truncated properly.\n\tn.pterm, n.pindex = term, index\n}\n\n// Reset our WAL. This is equivalent to truncating all data from the log.\n// Lock should be held.\nfunc (n *raft) resetWAL() {\n\tn.truncateWAL(0, 0)\n}\n\n// Lock should be held\nfunc (n *raft) updateLeader(newLeader string) {\n\tn.leader = newLeader\n\tn.hasleader.Store(newLeader != _EMPTY_)\n\tif !n.pleader.Load() && newLeader != noLeader {\n\t\tn.pleader.Store(true)\n\t\t// If we were preferred to become the first leader, but didn't end up successful.\n\t\t// Ensure to call lead change. When scaling from R1 to R3 we've optimized for a scale up\n\t\t// not flipping leader/non-leader/leader status if the leader remains the same. But we need to\n\t\t// correct that if the first leader turns out to be different.\n\t\tif n.maybeLeader {\n\t\t\tn.maybeLeader = false\n\t\t\tif n.id != newLeader {\n\t\t\t\tn.updateLeadChange(false)\n\t\t\t}\n\t\t}\n\t}\n}\n\n// processAppendEntry will process an appendEntry. This is called either\n// during recovery or from processAppendEntries when there are new entries\n// to be committed.\nfunc (n *raft) processAppendEntry(ae *appendEntry, sub *subscription) {\n\tn.Lock()\n\t// Don't reset here if we have been asked to assume leader position.\n\tif !n.lxfer {\n\t\tn.resetElectionTimeout()\n\t}\n\n\t// Just return if closed or we had previous write error.\n\tif n.State() == Closed || n.werr != nil {\n\t\tn.Unlock()\n\t\treturn\n\t}\n\n\t// Scratch buffer for responses.\n\tvar scratch [appendEntryResponseLen]byte\n\tarbuf := scratch[:]\n\n\t// Are we receiving from another leader.\n\tif n.State() == Leader {\n\t\t// If we are the same we should step down to break the tie.\n\t\tif ae.term >= n.term {\n\t\t\t// If the append entry term is newer than the current term, erase our\n\t\t\t// vote.\n\t\t\tif ae.term > n.term {\n\t\t\t\tn.term = ae.term\n\t\t\t\tn.vote = noVote\n\t\t\t\tn.writeTermVote()\n\t\t\t}\n\t\t\tn.debug(\"Received append entry from another leader, stepping down to %q\", ae.leader)\n\t\t\tn.stepdownLocked(ae.leader)\n\t\t} else {\n\t\t\t// Let them know we are the leader.\n\t\t\tar := newAppendEntryResponse(n.term, n.pindex, n.id, false)\n\t\t\tn.debug(\"AppendEntry ignoring old term from another leader\")\n\t\t\tn.sendRPC(ae.reply, _EMPTY_, ar.encode(arbuf))\n\t\t\tarPool.Put(ar)\n\t\t}\n\t\t// Always return here from processing.\n\t\tn.Unlock()\n\t\treturn\n\t}\n\n\t// If we received an append entry as a candidate then it would appear that\n\t// another node has taken on the leader role already, so we should convert\n\t// to a follower of that node instead.\n\tif n.State() == Candidate {\n\t\t// If we have a leader in the current term or higher, we should stepdown,\n\t\t// write the term and vote if the term of the request is higher.\n\t\tif ae.term >= n.term {\n\t\t\t// If the append entry term is newer than the current term, erase our\n\t\t\t// vote.\n\t\t\tif ae.term > n.term {\n\t\t\t\tn.term = ae.term\n\t\t\t\tn.vote = noVote\n\t\t\t\tn.writeTermVote()\n\t\t\t}\n\t\t\tn.debug(\"Received append entry in candidate state from %q, converting to follower\", ae.leader)\n\t\t\tn.stepdownLocked(ae.leader)\n\t\t}\n\t}\n\n\t// Catching up state.\n\tcatchingUp := n.catchup != nil\n\t// Is this a new entry? New entries will be delivered on the append entry\n\t// sub, rather than a catch-up sub.\n\tisNew := sub != nil && sub == n.aesub\n\n\t// Track leader directly\n\tif isNew && ae.leader != noLeader {\n\t\tif ps := n.peers[ae.leader]; ps != nil {\n\t\t\tps.ts = time.Now().UnixNano()\n\t\t} else {\n\t\t\tn.peers[ae.leader] = &lps{time.Now().UnixNano(), 0, true}\n\t\t}\n\t}\n\n\t// If we are catching up ignore old catchup subs.\n\t// This could happen when we stall or cancel a catchup.\n\tif !isNew && catchingUp && sub != n.catchup.sub {\n\t\tn.Unlock()\n\t\tn.debug(\"AppendEntry ignoring old entry from previous catchup\")\n\t\treturn\n\t}\n\n\t// Check state if we are catching up.\n\tvar resetCatchingUp bool\n\tif catchingUp {\n\t\tif cs := n.catchup; cs != nil && n.pterm >= cs.cterm && n.pindex >= cs.cindex {\n\t\t\t// If we are here we are good, so if we have a catchup pending we can cancel.\n\t\t\tn.cancelCatchup()\n\t\t\t// Reset our notion of catching up.\n\t\t\tresetCatchingUp = true\n\t\t} else if isNew {\n\t\t\tvar ar *appendEntryResponse\n\t\t\tvar inbox string\n\t\t\t// Check to see if we are stalled. If so recreate our catchup state and resend response.\n\t\t\tif n.catchupStalled() {\n\t\t\t\tn.debug(\"Catchup may be stalled, will request again\")\n\t\t\t\tinbox = n.createCatchup(ae)\n\t\t\t\tar = newAppendEntryResponse(n.pterm, n.pindex, n.id, false)\n\t\t\t}\n\t\t\tn.Unlock()\n\t\t\tif ar != nil {\n\t\t\t\tn.sendRPC(ae.reply, inbox, ar.encode(arbuf))\n\t\t\t\tarPool.Put(ar)\n\t\t\t}\n\t\t\t// Ignore new while catching up or replaying.\n\t\t\treturn\n\t\t}\n\t}\n\n\t// Grab term from append entry. But if leader explicitly defined its term, use that instead.\n\t// This is required during catchup if the leader catches us up on older items from previous terms.\n\t// While still allowing us to confirm they're matching our highest known term.\n\tlterm := ae.term\n\tif ae.lterm != 0 {\n\t\tlterm = ae.lterm\n\t}\n\n\t// If this term is greater than ours.\n\tif lterm > n.term {\n\t\tn.term = lterm\n\t\tn.vote = noVote\n\t\tif isNew {\n\t\t\tn.writeTermVote()\n\t\t}\n\t\tif n.State() != Follower {\n\t\t\tn.debug(\"Term higher than ours and we are not a follower: %v, stepping down to %q\", n.State(), ae.leader)\n\t\t\tn.stepdownLocked(ae.leader)\n\t\t}\n\t} else if lterm < n.term && sub != nil && !(catchingUp && ae.lterm == 0) {\n\t\t// Anything that's below our expected highest term needs to be rejected.\n\t\t// Unless we're replaying (sub=nil), in which case we'll always continue.\n\t\t// For backward-compatibility we shouldn't reject if we're being caught up by an old server.\n\t\tn.debug(\"Rejected AppendEntry from a leader (%s) with term %d which is less than ours\", ae.leader, lterm)\n\t\tar := newAppendEntryResponse(n.term, n.pindex, n.id, false)\n\t\tn.Unlock()\n\t\tn.sendRPC(ae.reply, _EMPTY_, ar.encode(arbuf))\n\t\tarPool.Put(ar)\n\t\treturn\n\t}\n\n\t// Reset after checking the term is correct, because we use catchingUp in a condition above.\n\tif resetCatchingUp {\n\t\tcatchingUp = false\n\t}\n\n\tif isNew && n.leader != ae.leader && n.State() == Follower {\n\t\tn.debug(\"AppendEntry updating leader to %q\", ae.leader)\n\t\tn.updateLeader(ae.leader)\n\t\tn.writeTermVote()\n\t\tn.resetElectionTimeout()\n\t\tn.updateLeadChange(false)\n\t}\n\n\tif ae.pterm != n.pterm || ae.pindex != n.pindex {\n\t\t// Check if this is a lower or equal index than what we were expecting.\n\t\tif ae.pindex <= n.pindex {\n\t\t\tn.debug(\"AppendEntry detected pindex less than/equal to ours: %d:%d vs %d:%d\", ae.pterm, ae.pindex, n.pterm, n.pindex)\n\t\t\tvar ar *appendEntryResponse\n\t\t\tvar success bool\n\n\t\t\tif ae.pindex < n.commit {\n\t\t\t\t// If we have already committed this entry, just mark success.\n\t\t\t\tsuccess = true\n\t\t\t} else if eae, _ := n.loadEntry(ae.pindex); eae == nil {\n\t\t\t\t// If terms are equal, and we are not catching up, we have simply already processed this message.\n\t\t\t\t// So we will ACK back to the leader. This can happen on server restarts based on timings of snapshots.\n\t\t\t\tif ae.pterm == n.pterm && !catchingUp {\n\t\t\t\t\tsuccess = true\n\t\t\t\t} else if ae.pindex == n.pindex {\n\t\t\t\t\t// Check if only our terms do not match here.\n\t\t\t\t\t// Make sure pterms match and we take on the leader's.\n\t\t\t\t\t// This prevents constant spinning.\n\t\t\t\t\tn.truncateWAL(ae.pterm, ae.pindex)\n\t\t\t\t} else if ae.pindex == n.applied {\n\t\t\t\t\t// Entry can't be found, this is normal because we have a snapshot at this index.\n\t\t\t\t\t// Truncate back to where we've created the snapshot.\n\t\t\t\t\tn.truncateWAL(ae.pterm, ae.pindex)\n\t\t\t\t} else {\n\t\t\t\t\tn.resetWAL()\n\t\t\t\t}\n\t\t\t} else if eae.term == ae.pterm {\n\t\t\t\t// If terms match we can delete all entries past this one, and then continue storing the current entry.\n\t\t\t\tn.truncateWAL(ae.pterm, ae.pindex)\n\t\t\t\t// Only continue if truncation was successful, and we ended up such that we can safely continue.\n\t\t\t\tif ae.pterm == n.pterm && ae.pindex == n.pindex {\n\t\t\t\t\tgoto CONTINUE\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\t// If terms mismatched, delete that entry and all others past it.\n\t\t\t\t// Make sure to cancel any catchups in progress.\n\t\t\t\t// Truncate will reset our pterm and pindex. Only do so if we have an entry.\n\t\t\t\tn.truncateWAL(eae.pterm, eae.pindex)\n\t\t\t}\n\t\t\t// Cancel regardless if unsuccessful.\n\t\t\tif !success {\n\t\t\t\tn.cancelCatchup()\n\t\t\t}\n\n\t\t\t// Create response.\n\t\t\tar = newAppendEntryResponse(ae.pterm, ae.pindex, n.id, success)\n\t\t\tn.Unlock()\n\t\t\tn.sendRPC(ae.reply, _EMPTY_, ar.encode(arbuf))\n\t\t\tarPool.Put(ar)\n\t\t\treturn\n\t\t}\n\n\t\t// Check if we are catching up. If we are here we know the leader did not have all of the entries\n\t\t// so make sure this is a snapshot entry. If it is not start the catchup process again since it\n\t\t// means we may have missed additional messages.\n\t\tif catchingUp {\n\t\t\t// This means we already entered into a catchup state but what the leader sent us did not match what we expected.\n\t\t\t// Snapshots and peerstate will always be together when a leader is catching us up in this fashion.\n\t\t\tif len(ae.entries) != 2 || ae.entries[0].Type != EntrySnapshot || ae.entries[1].Type != EntryPeerState {\n\t\t\t\tn.warn(\"Expected first catchup entry to be a snapshot and peerstate, will retry\")\n\t\t\t\tn.cancelCatchup()\n\t\t\t\tn.Unlock()\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\tif ps, err := decodePeerState(ae.entries[1].Data); err == nil {\n\t\t\t\tn.processPeerState(ps)\n\t\t\t\t// Also need to copy from client's buffer.\n\t\t\t\tae.entries[0].Data = copyBytes(ae.entries[0].Data)\n\t\t\t} else {\n\t\t\t\tn.warn(\"Could not parse snapshot peerstate correctly\")\n\t\t\t\tn.cancelCatchup()\n\t\t\t\tn.Unlock()\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\t// Inherit state from appendEntry with the leader's snapshot.\n\t\t\tn.pindex = ae.pindex\n\t\t\tn.pterm = ae.pterm\n\t\t\tn.commit = ae.pindex\n\n\t\t\tif _, err := n.wal.Compact(n.pindex + 1); err != nil {\n\t\t\t\tn.setWriteErrLocked(err)\n\t\t\t\tn.Unlock()\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\tsnap := &snapshot{\n\t\t\t\tlastTerm:  n.pterm,\n\t\t\t\tlastIndex: n.pindex,\n\t\t\t\tpeerstate: encodePeerState(&peerState{n.peerNames(), n.csz, n.extSt}),\n\t\t\t\tdata:      ae.entries[0].Data,\n\t\t\t}\n\t\t\t// Install the leader's snapshot as our own.\n\t\t\tif err := n.installSnapshot(snap); err != nil {\n\t\t\t\tn.setWriteErrLocked(err)\n\t\t\t\tn.Unlock()\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\t// Now send snapshot to upper levels. Only send the snapshot, not the peerstate entry.\n\t\t\tn.apply.push(newCommittedEntry(n.commit, ae.entries[:1]))\n\t\t\tn.Unlock()\n\t\t\treturn\n\t\t}\n\n\t\t// Setup our state for catching up.\n\t\tn.debug(\"AppendEntry did not match %d %d with %d %d\", ae.pterm, ae.pindex, n.pterm, n.pindex)\n\t\tinbox := n.createCatchup(ae)\n\t\tar := newAppendEntryResponse(n.pterm, n.pindex, n.id, false)\n\t\tn.Unlock()\n\t\tn.sendRPC(ae.reply, inbox, ar.encode(arbuf))\n\t\tarPool.Put(ar)\n\t\treturn\n\t}\n\nCONTINUE:\n\t// Save to our WAL if we have entries.\n\tif ae.shouldStore() {\n\t\t// Only store if an original which will have sub != nil\n\t\tif sub != nil {\n\t\t\tif err := n.storeToWAL(ae); err != nil {\n\t\t\t\tif err != ErrStoreClosed {\n\t\t\t\t\tn.warn(\"Error storing entry to WAL: %v\", err)\n\t\t\t\t}\n\t\t\t\tn.Unlock()\n\t\t\t\treturn\n\t\t\t}\n\t\t\t// Save in memory for faster processing during applyCommit.\n\t\t\t// Only save so many however to avoid memory bloat.\n\t\t\tif l := len(n.pae); l <= paeDropThreshold {\n\t\t\t\tn.pae[n.pindex], l = ae, l+1\n\t\t\t\tif l > paeWarnThreshold && l%paeWarnModulo == 0 {\n\t\t\t\t\tn.warn(\"%d append entries pending\", len(n.pae))\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\t// Invalidate cache entry at this index, we might have\n\t\t\t\t// stored it previously with a different value.\n\t\t\t\tdelete(n.pae, n.pindex)\n\t\t\t\tif l%paeWarnModulo == 0 {\n\t\t\t\t\tn.debug(\"Not saving to append entries pending\")\n\t\t\t\t}\n\t\t\t}\n\t\t} else {\n\t\t\t// This is a replay on startup so just take the appendEntry version.\n\t\t\tn.pterm = ae.term\n\t\t\tn.pindex = ae.pindex + 1\n\t\t}\n\t}\n\n\t// Check to see if we have any related entries to process here.\n\tfor _, e := range ae.entries {\n\t\tswitch e.Type {\n\t\tcase EntryLeaderTransfer:\n\t\t\t// Only process these if they are new, so no replays or catchups.\n\t\t\tif isNew {\n\t\t\t\tmaybeLeader := string(e.Data)\n\t\t\t\t// This is us. We need to check if we can become the leader.\n\t\t\t\tif maybeLeader == n.id {\n\t\t\t\t\t// If not an observer and not paused we are good to go.\n\t\t\t\t\tif !n.observer && !n.paused {\n\t\t\t\t\t\tn.lxfer = true\n\t\t\t\t\t\tn.xferCampaign()\n\t\t\t\t\t} else if n.paused && !n.pobserver {\n\t\t\t\t\t\t// Here we can become a leader but need to wait for resume of the apply queue.\n\t\t\t\t\t\tn.lxfer = true\n\t\t\t\t\t}\n\t\t\t\t} else if n.vote != noVote {\n\t\t\t\t\t// Since we are here we are not the chosen one but we should clear any vote preference.\n\t\t\t\t\tn.vote = noVote\n\t\t\t\t\tn.writeTermVote()\n\t\t\t\t}\n\t\t\t}\n\t\tcase EntryAddPeer:\n\t\t\tif newPeer := string(e.Data); len(newPeer) == idLen {\n\t\t\t\t// Track directly, but wait for commit to be official\n\t\t\t\tif ps := n.peers[newPeer]; ps != nil {\n\t\t\t\t\tps.ts = time.Now().UnixNano()\n\t\t\t\t} else {\n\t\t\t\t\tn.peers[newPeer] = &lps{time.Now().UnixNano(), 0, false}\n\t\t\t\t}\n\t\t\t\t// Store our peer in our global peer map for all peers.\n\t\t\t\tpeers.LoadOrStore(newPeer, newPeer)\n\t\t\t}\n\t\t}\n\t}\n\n\t// Make a copy of these values, as the AppendEntry might be cached and returned to the pool in applyCommit.\n\taeCommit := ae.commit\n\taeReply := ae.reply\n\n\t// Apply anything we need here.\n\tif aeCommit > n.commit {\n\t\tif n.paused {\n\t\t\tn.hcommit = aeCommit\n\t\t\tn.debug(\"Paused, not applying %d\", aeCommit)\n\t\t} else {\n\t\t\tfor index := n.commit + 1; index <= aeCommit; index++ {\n\t\t\t\tif err := n.applyCommit(index); err != nil {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\t// Only ever respond to new entries.\n\t// Never respond to catchup messages, because providing quorum based on this is unsafe.\n\tvar ar *appendEntryResponse\n\tif sub != nil && isNew {\n\t\tar = newAppendEntryResponse(n.pterm, n.pindex, n.id, true)\n\t}\n\tn.Unlock()\n\n\t// Success. Send our response.\n\tif ar != nil {\n\t\tn.sendRPC(aeReply, _EMPTY_, ar.encode(arbuf))\n\t\tarPool.Put(ar)\n\t}\n}\n\n// processPeerState is called when a peer state entry is received\n// over the wire or when we're updating known peers.\n// Lock should be held.\nfunc (n *raft) processPeerState(ps *peerState) {\n\t// Update our version of peers to that of the leader. Calculate\n\t// the number of nodes needed to establish a quorum.\n\tn.csz = ps.clusterSize\n\tn.qn = n.csz/2 + 1\n\n\told := n.peers\n\tn.peers = make(map[string]*lps)\n\tfor _, peer := range ps.knownPeers {\n\t\tif lp := old[peer]; lp != nil {\n\t\t\tlp.kp = true\n\t\t\tn.peers[peer] = lp\n\t\t} else {\n\t\t\tn.peers[peer] = &lps{0, 0, true}\n\t\t}\n\t}\n\tn.debug(\"Update peers from leader to %+v\", n.peers)\n\tn.writePeerState(ps)\n}\n\n// processAppendEntryResponse is called when we receive an append entry\n// response from another node. They will send a confirmation to tell us\n// whether they successfully committed the entry or not.\nfunc (n *raft) processAppendEntryResponse(ar *appendEntryResponse) {\n\tn.trackPeer(ar.peer)\n\n\tif ar.success {\n\t\t// The remote node successfully committed the append entry.\n\t\t// They agree with our leadership and are happy with the state of the log.\n\t\t// In this case ar.term doesn't matter.\n\t\tn.trackResponse(ar)\n\t\tarPool.Put(ar)\n\t} else if ar.reply != _EMPTY_ {\n\t\t// The remote node didn't commit the append entry, and they believe they\n\t\t// are behind and have specified a reply subject, so let's try to catch them up.\n\t\t// In this case ar.term was populated with the remote's pterm.\n\t\tn.catchupFollower(ar)\n\t} else if ar.term > n.term {\n\t\t// The remote node didn't commit the append entry, it looks like\n\t\t// they are on a newer term than we are. Step down.\n\t\t// In this case ar.term was populated with the remote's term.\n\t\tn.Lock()\n\t\tn.term = ar.term\n\t\tn.vote = noVote\n\t\tn.writeTermVote()\n\t\tn.warn(\"Detected another leader with higher term, will stepdown\")\n\t\tn.stepdownLocked(noLeader)\n\t\tn.Unlock()\n\t\tarPool.Put(ar)\n\t} else {\n\t\t// Ignore, but return back to pool.\n\t\tarPool.Put(ar)\n\t}\n}\n\n// handleAppendEntryResponse processes responses to append entries.\nfunc (n *raft) handleAppendEntryResponse(sub *subscription, c *client, _ *Account, subject, reply string, msg []byte) {\n\tar := n.decodeAppendEntryResponse(msg)\n\tar.reply = reply\n\tn.resp.push(ar)\n}\n\nfunc (n *raft) buildAppendEntry(entries []*Entry) *appendEntry {\n\treturn newAppendEntry(n.id, n.term, n.commit, n.pterm, n.pindex, entries)\n}\n\n// Determine if we should store an entry. This stops us from storing\n// heartbeat messages.\nfunc (ae *appendEntry) shouldStore() bool {\n\treturn ae != nil && len(ae.entries) > 0\n}\n\n// Store our append entry to our WAL.\n// lock should be held.\nfunc (n *raft) storeToWAL(ae *appendEntry) error {\n\tif ae == nil {\n\t\treturn fmt.Errorf(\"raft: Missing append entry for storage\")\n\t}\n\tif n.werr != nil {\n\t\treturn n.werr\n\t}\n\n\tseq, _, err := n.wal.StoreMsg(_EMPTY_, nil, ae.buf, 0)\n\tif err != nil {\n\t\tn.setWriteErrLocked(err)\n\t\treturn err\n\t}\n\n\t// Sanity checking for now.\n\tif index := ae.pindex + 1; index != seq {\n\t\tn.warn(\"Wrong index, ae is %+v, index stored was %d, n.pindex is %d, will reset\", ae, seq, n.pindex)\n\t\tif n.State() == Leader {\n\t\t\tn.stepdownLocked(n.selectNextLeader())\n\t\t}\n\t\t// Reset and cancel any catchup.\n\t\tn.resetWAL()\n\t\tn.cancelCatchup()\n\t\treturn errEntryStoreFailed\n\t}\n\n\tn.pterm = ae.term\n\tn.pindex = seq\n\treturn nil\n}\n\nconst (\n\tpaeDropThreshold = 20_000\n\tpaeWarnThreshold = 10_000\n\tpaeWarnModulo    = 5_000\n)\n\nfunc (n *raft) sendAppendEntry(entries []*Entry) {\n\tn.Lock()\n\tdefer n.Unlock()\n\tae := n.buildAppendEntry(entries)\n\n\tvar err error\n\tvar scratch [1024]byte\n\tae.buf, err = ae.encode(scratch[:])\n\tif err != nil {\n\t\treturn\n\t}\n\n\t// If we have entries store this in our wal.\n\tshouldStore := ae.shouldStore()\n\tif shouldStore {\n\t\tif err := n.storeToWAL(ae); err != nil {\n\t\t\treturn\n\t\t}\n\t\tn.active = time.Now()\n\n\t\t// Save in memory for faster processing during applyCommit.\n\t\tn.pae[n.pindex] = ae\n\t\tif l := len(n.pae); l > paeWarnThreshold && l%paeWarnModulo == 0 {\n\t\t\tn.warn(\"%d append entries pending\", len(n.pae))\n\t\t}\n\t}\n\tn.sendRPC(n.asubj, n.areply, ae.buf)\n\tif !shouldStore {\n\t\tae.returnToPool()\n\t}\n}\n\ntype extensionState uint16\n\nconst (\n\textUndetermined = extensionState(iota)\n\textExtended\n\textNotExtended\n)\n\ntype peerState struct {\n\tknownPeers  []string\n\tclusterSize int\n\tdomainExt   extensionState\n}\n\nfunc peerStateBufSize(ps *peerState) int {\n\treturn 4 + 4 + (idLen * len(ps.knownPeers)) + 2\n}\n\nfunc encodePeerState(ps *peerState) []byte {\n\tvar le = binary.LittleEndian\n\tbuf := make([]byte, peerStateBufSize(ps))\n\tle.PutUint32(buf[0:], uint32(ps.clusterSize))\n\tle.PutUint32(buf[4:], uint32(len(ps.knownPeers)))\n\twi := 8\n\tfor _, peer := range ps.knownPeers {\n\t\tcopy(buf[wi:], peer)\n\t\twi += idLen\n\t}\n\tle.PutUint16(buf[wi:], uint16(ps.domainExt))\n\treturn buf\n}\n\nfunc decodePeerState(buf []byte) (*peerState, error) {\n\tif len(buf) < 8 {\n\t\treturn nil, errCorruptPeers\n\t}\n\tvar le = binary.LittleEndian\n\tps := &peerState{clusterSize: int(le.Uint32(buf[0:]))}\n\texpectedPeers := int(le.Uint32(buf[4:]))\n\tbuf = buf[8:]\n\tri := 0\n\tfor i, n := 0, expectedPeers; i < n && ri < len(buf); i++ {\n\t\tps.knownPeers = append(ps.knownPeers, string(buf[ri:ri+idLen]))\n\t\tri += idLen\n\t}\n\tif len(ps.knownPeers) != expectedPeers {\n\t\treturn nil, errCorruptPeers\n\t}\n\tif len(buf[ri:]) >= 2 {\n\t\tps.domainExt = extensionState(le.Uint16(buf[ri:]))\n\t}\n\treturn ps, nil\n}\n\n// Lock should be held.\nfunc (n *raft) peerNames() []string {\n\tvar peers []string\n\tfor name, peer := range n.peers {\n\t\tif peer.kp {\n\t\t\tpeers = append(peers, name)\n\t\t}\n\t}\n\treturn peers\n}\n\nfunc (n *raft) currentPeerState() *peerState {\n\tn.RLock()\n\tps := &peerState{n.peerNames(), n.csz, n.extSt}\n\tn.RUnlock()\n\treturn ps\n}\n\n// sendPeerState will send our current peer state to the cluster.\nfunc (n *raft) sendPeerState() {\n\tn.sendAppendEntry([]*Entry{{EntryPeerState, encodePeerState(n.currentPeerState())}})\n}\n\n// Send a heartbeat.\nfunc (n *raft) sendHeartbeat() {\n\tn.sendAppendEntry(nil)\n}\n\ntype voteRequest struct {\n\tterm      uint64\n\tlastTerm  uint64\n\tlastIndex uint64\n\tcandidate string\n\t// internal only.\n\treply string\n}\n\nconst voteRequestLen = 24 + idLen\n\nfunc (vr *voteRequest) encode() []byte {\n\tvar buf [voteRequestLen]byte\n\tvar le = binary.LittleEndian\n\tle.PutUint64(buf[0:], vr.term)\n\tle.PutUint64(buf[8:], vr.lastTerm)\n\tle.PutUint64(buf[16:], vr.lastIndex)\n\tcopy(buf[24:24+idLen], vr.candidate)\n\n\treturn buf[:voteRequestLen]\n}\n\nfunc decodeVoteRequest(msg []byte, reply string) *voteRequest {\n\tif len(msg) != voteRequestLen {\n\t\treturn nil\n\t}\n\n\tvar le = binary.LittleEndian\n\treturn &voteRequest{\n\t\tterm:      le.Uint64(msg[0:]),\n\t\tlastTerm:  le.Uint64(msg[8:]),\n\t\tlastIndex: le.Uint64(msg[16:]),\n\t\tcandidate: string(copyBytes(msg[24 : 24+idLen])),\n\t\treply:     reply,\n\t}\n}\n\nconst peerStateFile = \"peers.idx\"\n\n// Lock should be held.\nfunc (n *raft) writePeerState(ps *peerState) {\n\tpse := encodePeerState(ps)\n\tif bytes.Equal(n.wps, pse) {\n\t\treturn\n\t}\n\t// Stamp latest and write the peer state file.\n\tn.wps = pse\n\tif err := writePeerState(n.sd, ps); err != nil && !n.isClosed() {\n\t\tn.setWriteErrLocked(err)\n\t\tn.warn(\"Error writing peer state file for %q: %v\", n.group, err)\n\t}\n}\n\n// Writes out our peer state outside of a specific raft context.\nfunc writePeerState(sd string, ps *peerState) error {\n\tpsf := filepath.Join(sd, peerStateFile)\n\tif _, err := os.Stat(psf); err != nil && !os.IsNotExist(err) {\n\t\treturn err\n\t}\n\treturn writeFileWithSync(psf, encodePeerState(ps), defaultFilePerms)\n}\n\nfunc readPeerState(sd string) (ps *peerState, err error) {\n\t<-dios\n\tbuf, err := os.ReadFile(filepath.Join(sd, peerStateFile))\n\tdios <- struct{}{}\n\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn decodePeerState(buf)\n}\n\nconst termVoteFile = \"tav.idx\"\nconst termLen = 8 // uint64\nconst termVoteLen = idLen + termLen\n\n// Writes out our term & vote outside of a specific raft context.\nfunc writeTermVote(sd string, wtv []byte) error {\n\tpsf := filepath.Join(sd, termVoteFile)\n\tif _, err := os.Stat(psf); err != nil && !os.IsNotExist(err) {\n\t\treturn err\n\t}\n\treturn writeFileWithSync(psf, wtv, defaultFilePerms)\n}\n\n// readTermVote will read the largest term and who we voted from to stable storage.\n// Lock should be held.\nfunc (n *raft) readTermVote() (term uint64, voted string, err error) {\n\t<-dios\n\tbuf, err := os.ReadFile(filepath.Join(n.sd, termVoteFile))\n\tdios <- struct{}{}\n\n\tif err != nil {\n\t\treturn 0, noVote, err\n\t}\n\tif len(buf) < termLen {\n\t\t// Not enough bytes for the uint64 below, so avoid a panic.\n\t\treturn 0, noVote, nil\n\t}\n\tvar le = binary.LittleEndian\n\tterm = le.Uint64(buf[0:])\n\tif len(buf) < termVoteLen {\n\t\treturn term, noVote, nil\n\t}\n\tvoted = string(buf[8:])\n\treturn term, voted, nil\n}\n\n// Lock should be held.\nfunc (n *raft) setWriteErrLocked(err error) {\n\t// Check if we are closed already.\n\tif n.State() == Closed {\n\t\treturn\n\t}\n\t// Ignore if already set.\n\tif n.werr == err || err == nil {\n\t\treturn\n\t}\n\t// Ignore non-write errors.\n\tif err == ErrStoreClosed ||\n\t\terr == ErrStoreEOF ||\n\t\terr == ErrInvalidSequence ||\n\t\terr == ErrStoreMsgNotFound ||\n\t\terr == errNoPending ||\n\t\terr == errPartialCache {\n\t\treturn\n\t}\n\t// If this is a not found report but do not disable.\n\tif os.IsNotExist(err) {\n\t\tn.error(\"Resource not found: %v\", err)\n\t\treturn\n\t}\n\tn.error(\"Critical write error: %v\", err)\n\tn.werr = err\n\n\tif isPermissionError(err) {\n\t\tgo n.s.handleWritePermissionError()\n\t}\n\n\tif isOutOfSpaceErr(err) {\n\t\t// For now since this can be happening all under the covers, we will call up and disable JetStream.\n\t\tgo n.s.handleOutOfSpace(nil)\n\t}\n}\n\n// Helper to check if we are closed when we do not hold a lock already.\nfunc (n *raft) isClosed() bool {\n\treturn n.State() == Closed\n}\n\n// Capture our write error if any and hold.\nfunc (n *raft) setWriteErr(err error) {\n\tn.Lock()\n\tdefer n.Unlock()\n\tn.setWriteErrLocked(err)\n}\n\n// writeTermVote will record the largest term and who we voted for to stable storage.\n// Lock should be held.\nfunc (n *raft) writeTermVote() {\n\tvar buf [termVoteLen]byte\n\tvar le = binary.LittleEndian\n\tle.PutUint64(buf[0:], n.term)\n\tcopy(buf[8:], n.vote)\n\tb := buf[:8+len(n.vote)]\n\n\t// If the term and vote hasn't changed then don't rewrite to disk.\n\tif bytes.Equal(n.wtv, b) {\n\t\treturn\n\t}\n\t// Stamp latest and write the term & vote file.\n\tn.wtv = b\n\tif err := writeTermVote(n.sd, n.wtv); err != nil && !n.isClosed() {\n\t\t// Clear wtv since we failed.\n\t\tn.wtv = nil\n\t\tn.setWriteErrLocked(err)\n\t\tn.warn(\"Error writing term and vote file for %q: %v\", n.group, err)\n\t}\n}\n\n// voteResponse is a response to a vote request.\ntype voteResponse struct {\n\tterm    uint64\n\tpeer    string\n\tgranted bool\n}\n\nconst voteResponseLen = 8 + 8 + 1\n\nfunc (vr *voteResponse) encode() []byte {\n\tvar buf [voteResponseLen]byte\n\tvar le = binary.LittleEndian\n\tle.PutUint64(buf[0:], vr.term)\n\tcopy(buf[8:], vr.peer)\n\tif vr.granted {\n\t\tbuf[16] = 1\n\t} else {\n\t\tbuf[16] = 0\n\t}\n\treturn buf[:voteResponseLen]\n}\n\nfunc decodeVoteResponse(msg []byte) *voteResponse {\n\tif len(msg) != voteResponseLen {\n\t\treturn nil\n\t}\n\tvar le = binary.LittleEndian\n\tvr := &voteResponse{term: le.Uint64(msg[0:]), peer: string(msg[8:16])}\n\tvr.granted = msg[16] == 1\n\treturn vr\n}\n\nfunc (n *raft) handleVoteResponse(sub *subscription, c *client, _ *Account, _, reply string, msg []byte) {\n\tvr := decodeVoteResponse(msg)\n\tn.debug(\"Received a voteResponse %+v\", vr)\n\tif vr == nil {\n\t\tn.error(\"Received malformed vote response for %q\", n.group)\n\t\treturn\n\t}\n\n\tif state := n.State(); state != Candidate && state != Leader {\n\t\tn.debug(\"Ignoring old vote response, we have stepped down\")\n\t\treturn\n\t}\n\n\tn.votes.push(vr)\n}\n\nfunc (n *raft) processVoteRequest(vr *voteRequest) error {\n\t// To simplify calling code, we can possibly pass `nil` to this function.\n\t// If that is the case, does not consider it an error.\n\tif vr == nil {\n\t\treturn nil\n\t}\n\tn.debug(\"Received a voteRequest %+v\", vr)\n\n\tif err := n.trackPeer(vr.candidate); err != nil {\n\t\treturn err\n\t}\n\n\tn.Lock()\n\n\tvresp := &voteResponse{n.term, n.id, false}\n\tdefer n.debug(\"Sending a voteResponse %+v -> %q\", vresp, vr.reply)\n\n\t// Ignore if we are newer. This is important so that we don't accidentally process\n\t// votes from a previous term if they were still in flight somewhere.\n\tif vr.term < n.term {\n\t\tn.Unlock()\n\t\tn.sendReply(vr.reply, vresp.encode())\n\t\treturn nil\n\t}\n\n\t// If this is a higher term go ahead and stepdown.\n\tif vr.term > n.term {\n\t\tif n.State() != Follower {\n\t\t\tn.debug(\"Stepping down from %s, detected higher term: %d vs %d\",\n\t\t\t\tstrings.ToLower(n.State().String()), vr.term, n.term)\n\t\t\tn.stepdownLocked(noLeader)\n\t\t}\n\t\tn.cancelCatchup()\n\t\tn.term = vr.term\n\t\tn.vote = noVote\n\t\tn.writeTermVote()\n\t}\n\n\t// Only way we get to yes is through here.\n\tvoteOk := n.vote == noVote || n.vote == vr.candidate\n\tif voteOk && (vr.lastTerm > n.pterm || vr.lastTerm == n.pterm && vr.lastIndex >= n.pindex) {\n\t\tvresp.granted = true\n\t\tn.term = vr.term\n\t\tn.vote = vr.candidate\n\t\tn.writeTermVote()\n\t\tn.resetElectionTimeout()\n\t} else if n.vote == noVote && n.State() != Candidate {\n\t\t// We have a more up-to-date log, and haven't voted yet.\n\t\t// Start campaigning earlier, but only if not candidate already, as that would short-circuit us.\n\t\tn.resetElect(randCampaignTimeout())\n\t}\n\n\t// Term might have changed, make sure response has the most current\n\tvresp.term = n.term\n\n\tn.Unlock()\n\n\tn.sendReply(vr.reply, vresp.encode())\n\n\treturn nil\n}\n\nfunc (n *raft) handleVoteRequest(sub *subscription, c *client, _ *Account, subject, reply string, msg []byte) {\n\tvr := decodeVoteRequest(msg, reply)\n\tif vr == nil {\n\t\tn.error(\"Received malformed vote request for %q\", n.group)\n\t\treturn\n\t}\n\tn.reqs.push(vr)\n}\n\nfunc (n *raft) requestVote() {\n\tn.Lock()\n\tif n.State() != Candidate {\n\t\tn.Unlock()\n\t\treturn\n\t}\n\tn.vote = n.id\n\tn.writeTermVote()\n\tvr := voteRequest{n.term, n.pterm, n.pindex, n.id, _EMPTY_}\n\tsubj, reply := n.vsubj, n.vreply\n\tn.Unlock()\n\n\tn.debug(\"Sending out voteRequest %+v\", vr)\n\n\t// Now send it out.\n\tn.sendRPC(subj, reply, vr.encode())\n}\n\nfunc (n *raft) sendRPC(subject, reply string, msg []byte) {\n\tif n.sq != nil {\n\t\tn.sq.send(subject, reply, nil, msg)\n\t}\n}\n\nfunc (n *raft) sendReply(subject string, msg []byte) {\n\tif n.sq != nil {\n\t\tn.sq.send(subject, _EMPTY_, nil, msg)\n\t}\n}\n\nfunc (n *raft) wonElection(votes int) bool {\n\treturn votes >= n.quorumNeeded()\n}\n\n// Return the quorum size for a given cluster config.\nfunc (n *raft) quorumNeeded() int {\n\tn.RLock()\n\tqn := n.qn\n\tn.RUnlock()\n\treturn qn\n}\n\n// Lock should be held.\nfunc (n *raft) updateLeadChange(isLeader bool) {\n\t// We don't care about values that have not been consumed (transitory states),\n\t// so we dequeue any state that is pending and push the new one.\n\tfor {\n\t\tselect {\n\t\tcase n.leadc <- isLeader:\n\t\t\treturn\n\t\tdefault:\n\t\t\tselect {\n\t\t\tcase <-n.leadc:\n\t\t\tdefault:\n\t\t\t\t// May have been consumed by the \"reader\" go routine, so go back\n\t\t\t\t// to the top of the loop and try to send again.\n\t\t\t}\n\t\t}\n\t}\n}\n\n// Lock should be held.\nfunc (n *raft) switchState(state RaftState) bool {\nretry:\n\tpstate := n.State()\n\tif pstate == Closed {\n\t\treturn false\n\t}\n\n\t// Set our state. If something else has changed our state\n\t// then retry, this will either be a Stop or Delete call.\n\tif !n.state.CompareAndSwap(int32(pstate), int32(state)) {\n\t\tgoto retry\n\t}\n\n\t// Reset the election timer.\n\tn.resetElectionTimeout()\n\n\tvar leadChange bool\n\tif pstate == Leader && state != Leader {\n\t\tleadChange = true\n\t\tn.updateLeadChange(false)\n\t\t// Drain the append entry response and proposal queues.\n\t\tn.resp.drain()\n\t\tn.prop.drain()\n\t} else if state == Leader && pstate != Leader {\n\t\t// Don't updateLeadChange here, it will be done in switchToLeader or after initial messages are applied.\n\t\tleadChange = true\n\t\tif len(n.pae) > 0 {\n\t\t\tn.pae = make(map[uint64]*appendEntry)\n\t\t}\n\t}\n\n\tn.writeTermVote()\n\treturn leadChange\n}\n\nconst (\n\tnoLeader = _EMPTY_\n\tnoVote   = _EMPTY_\n)\n\nfunc (n *raft) switchToFollower(leader string) {\n\tn.Lock()\n\tdefer n.Unlock()\n\n\tn.switchToFollowerLocked(leader)\n}\n\nfunc (n *raft) switchToFollowerLocked(leader string) {\n\tif n.State() == Closed {\n\t\treturn\n\t}\n\n\tn.debug(\"Switching to follower\")\n\n\tn.aflr = 0\n\tn.leaderState.Store(false)\n\tn.lxfer = false\n\t// Reset acks, we can't assume acks from a previous term are still valid in another term.\n\tif len(n.acks) > 0 {\n\t\tn.acks = make(map[uint64]map[string]struct{})\n\t}\n\tn.updateLeader(leader)\n\tn.switchState(Follower)\n}\n\nfunc (n *raft) switchToCandidate() {\n\tif n.State() == Closed {\n\t\treturn\n\t}\n\n\tn.Lock()\n\tdefer n.Unlock()\n\n\t// If we are catching up or are in observer mode we can not switch.\n\t// Avoid petitioning to become leader if we're behind on applies.\n\tif n.observer || n.paused || n.applied < n.commit {\n\t\tn.resetElect(minElectionTimeout / 4)\n\t\treturn\n\t}\n\n\tif n.State() != Candidate {\n\t\tn.debug(\"Switching to candidate\")\n\t} else {\n\t\tif n.lostQuorumLocked() && time.Since(n.llqrt) > 20*time.Second {\n\t\t\t// We signal to the upper layers such that can alert on quorum lost.\n\t\t\tn.updateLeadChange(false)\n\t\t\tn.llqrt = time.Now()\n\t\t}\n\t}\n\t// Increment the term.\n\tn.term++\n\t// Clear current Leader.\n\tn.updateLeader(noLeader)\n\tn.switchState(Candidate)\n}\n\nfunc (n *raft) switchToLeader() {\n\tif n.State() == Closed {\n\t\treturn\n\t}\n\n\tn.Lock()\n\n\tn.debug(\"Switching to leader\")\n\n\tvar state StreamState\n\tn.wal.FastState(&state)\n\n\t// Check if we have items pending as we are taking over.\n\tsendHB := state.LastSeq > n.commit\n\n\tn.lxfer = false\n\tn.updateLeader(n.id)\n\tleadChange := n.switchState(Leader)\n\n\tif leadChange {\n\t\t// Wait for messages to be applied if we've stored more, otherwise signal immediately.\n\t\t// It's important to wait signaling we're leader if we're not up-to-date yet, as that\n\t\t// would mean we're in a consistent state compared with the previous leader.\n\t\tif n.pindex > n.applied {\n\t\t\tn.aflr = n.pindex\n\t\t} else {\n\t\t\t// We know we have applied all entries in our log and can signal immediately.\n\t\t\t// For sanity reset applied floor back down to 0, so we aren't able to signal twice.\n\t\t\tn.aflr = 0\n\t\t\tn.leaderState.Store(true)\n\t\t\tn.updateLeadChange(true)\n\t\t}\n\t}\n\tn.Unlock()\n\n\tif sendHB {\n\t\tn.sendHeartbeat()\n\t}\n}\n",
    "source_file": "server/raft.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2022-2025 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage server\n\nimport (\n\t\"bytes\"\n\t\"crypto/tls\"\n\t\"encoding/pem\"\n\t\"errors\"\n\t\"fmt\"\n\t\"time\"\n\t\"unicode\"\n\n\t\"github.com/nats-io/jwt/v2\"\n\t\"github.com/nats-io/nkeys\"\n)\n\nconst (\n\tAuthCalloutSubject    = \"$SYS.REQ.USER.AUTH\"\n\tAuthRequestSubject    = \"nats-authorization-request\"\n\tAuthRequestXKeyHeader = \"Nats-Server-Xkey\"\n)\n\n// Process a callout on this client's behalf.\nfunc (s *Server) processClientOrLeafCallout(c *client, opts *Options) (authorized bool, errStr string) {\n\tisOperatorMode := len(opts.TrustedKeys) > 0\n\n\t// this is the account the user connected in, or the one running the callout\n\tvar acc *Account\n\tif !isOperatorMode && opts.AuthCallout != nil && opts.AuthCallout.Account != _EMPTY_ {\n\t\taname := opts.AuthCallout.Account\n\t\tvar err error\n\t\tacc, err = s.LookupAccount(aname)\n\t\tif err != nil {\n\t\t\terrStr = fmt.Sprintf(\"No valid account %q for auth callout request: %v\", aname, err)\n\t\t\ts.Warnf(errStr)\n\t\t\treturn false, errStr\n\t\t}\n\t} else {\n\t\tacc = c.acc\n\t}\n\n\t// Check if we have been requested to encrypt.\n\tvar xkp nkeys.KeyPair\n\tvar xkey string\n\tvar pubAccXKey string\n\tif !isOperatorMode && opts.AuthCallout != nil && opts.AuthCallout.XKey != _EMPTY_ {\n\t\tpubAccXKey = opts.AuthCallout.XKey\n\t} else if isOperatorMode {\n\t\tpubAccXKey = acc.externalAuthXKey()\n\t}\n\t// If set grab server's xkey keypair and public key.\n\tif pubAccXKey != _EMPTY_ {\n\t\t// These are only set on creation, so lock not needed.\n\t\txkp, xkey = s.xkp, s.info.XKey\n\t}\n\n\t// FIXME: so things like the server ID that get assigned, are used as a sort of nonce - but\n\t//  reality is that the keypair here, is generated, so the response generated a JWT has to be\n\t//  this user - no replay possible\n\t// Create a keypair for the user. We will expect this public user to be in the signed response.\n\t// This prevents replay attacks.\n\tukp, _ := nkeys.CreateUser()\n\tpub, _ := ukp.PublicKey()\n\n\treply := s.newRespInbox()\n\trespCh := make(chan string, 1)\n\n\tdecodeResponse := func(rc *client, rmsg []byte, acc *Account) (*jwt.UserClaims, error) {\n\t\taccount := acc.Name\n\t\t_, msg := rc.msgParts(rmsg)\n\n\t\t// This signals not authorized.\n\t\t// Since this is an account subscription will always have \"\\r\\n\".\n\t\tif len(msg) <= LEN_CR_LF {\n\t\t\treturn nil, fmt.Errorf(\"auth callout violation: %q on account %q\", \"no reason supplied\", account)\n\t\t}\n\t\t// Strip trailing CRLF.\n\t\tmsg = msg[:len(msg)-LEN_CR_LF]\n\t\tencrypted := false\n\t\t// If we sent an encrypted request the response could be encrypted as well.\n\t\t// we are expecting the input to be `eyJ` if it is a JWT\n\t\tif xkp != nil && len(msg) > 0 && !bytes.HasPrefix(msg, []byte(jwtPrefix)) {\n\t\t\tvar err error\n\t\t\tmsg, err = xkp.Open(msg, pubAccXKey)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, fmt.Errorf(\"error decrypting auth callout response on account %q: %v\", account, err)\n\t\t\t}\n\t\t\tencrypted = true\n\t\t}\n\n\t\tcr, err := jwt.DecodeAuthorizationResponseClaims(string(msg))\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tvr := jwt.CreateValidationResults()\n\t\tcr.Validate(vr)\n\t\tif len(vr.Issues) > 0 {\n\t\t\treturn nil, fmt.Errorf(\"authorization response had validation errors: %v\", vr.Issues[0])\n\t\t}\n\n\t\t// the subject is the user id\n\t\tif cr.Subject != pub {\n\t\t\treturn nil, errors.New(\"auth callout violation: auth callout response is not for expected user\")\n\t\t}\n\n\t\t// check the audience to be the server ID\n\t\tif cr.Audience != s.info.ID {\n\t\t\treturn nil, errors.New(\"auth callout violation: auth callout response is not for server\")\n\t\t}\n\n\t\t// check if had an error message from the auth account\n\t\tif cr.Error != _EMPTY_ {\n\t\t\treturn nil, fmt.Errorf(\"auth callout service returned an error: %v\", cr.Error)\n\t\t}\n\n\t\t// if response is encrypted none of this is needed\n\t\tif isOperatorMode && !encrypted {\n\t\t\tpkStr := cr.Issuer\n\t\t\tif cr.IssuerAccount != _EMPTY_ {\n\t\t\t\tpkStr = cr.IssuerAccount\n\t\t\t}\n\t\t\tif pkStr != account {\n\t\t\t\tif _, ok := acc.signingKeys[pkStr]; !ok {\n\t\t\t\t\treturn nil, errors.New(\"auth callout signing key is unknown\")\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\treturn jwt.DecodeUserClaims(cr.Jwt)\n\t}\n\n\t// getIssuerAccount returns the issuer (as per JWT) - it also asserts that\n\t// only in operator mode we expect to receive `issuer_account`.\n\tgetIssuerAccount := func(arc *jwt.UserClaims, account string) (string, error) {\n\t\t// Make sure correct issuer.\n\t\tvar issuer string\n\t\tif opts.AuthCallout != nil {\n\t\t\tissuer = opts.AuthCallout.Issuer\n\t\t} else {\n\t\t\t// Operator mode is who we send the request on unless switching accounts.\n\t\t\tissuer = acc.Name\n\t\t}\n\n\t\t// the jwt issuer can be a signing key\n\t\tjwtIssuer := arc.Issuer\n\t\tif arc.IssuerAccount != _EMPTY_ {\n\t\t\tif !isOperatorMode {\n\t\t\t\t// this should be invalid - effectively it would allow the auth callout\n\t\t\t\t// to issue on another account which may be allowed given the configuration\n\t\t\t\t// where the auth callout account can handle multiple different ones..\n\t\t\t\treturn _EMPTY_, fmt.Errorf(\"error non operator mode account %q: attempted to use issuer_account\", account)\n\t\t\t}\n\t\t\tjwtIssuer = arc.IssuerAccount\n\t\t}\n\n\t\tif jwtIssuer != issuer {\n\t\t\tif !isOperatorMode {\n\t\t\t\treturn _EMPTY_, fmt.Errorf(\"wrong issuer for auth callout response on account %q, expected %q got %q\", account, issuer, jwtIssuer)\n\t\t\t} else if !acc.isAllowedAcount(jwtIssuer) {\n\t\t\t\treturn _EMPTY_, fmt.Errorf(\"account %q not permitted as valid account option for auth callout for account %q\",\n\t\t\t\t\tarc.Issuer, account)\n\t\t\t}\n\t\t}\n\t\treturn jwtIssuer, nil\n\t}\n\n\tgetExpirationAndAllowedConnections := func(arc *jwt.UserClaims, account string) (time.Duration, map[string]struct{}, error) {\n\t\tallowNow, expiration := validateTimes(arc)\n\t\tif !allowNow {\n\t\t\tc.Errorf(\"Outside connect times\")\n\t\t\treturn 0, nil, fmt.Errorf(\"authorized user on account %q outside of valid connect times\", account)\n\t\t}\n\n\t\tallowedConnTypes, err := convertAllowedConnectionTypes(arc.User.AllowedConnectionTypes)\n\t\tif err != nil {\n\t\t\tc.Debugf(\"%v\", err)\n\t\t\tif len(allowedConnTypes) == 0 {\n\t\t\t\treturn 0, nil, fmt.Errorf(\"authorized user on account %q using invalid connection type\", account)\n\t\t\t}\n\t\t}\n\t\treturn expiration, allowedConnTypes, nil\n\t}\n\n\tassignAccountAndPermissions := func(arc *jwt.UserClaims, account string) (*Account, error) {\n\t\t// Apply to this client.\n\t\tvar err error\n\t\tissuerAccount, err := getIssuerAccount(arc, account)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\t// if we are not in operator mode, they can specify placement as a tag\n\t\tvar placement string\n\t\tif !isOperatorMode {\n\t\t\t// only allow placement if we are not in operator mode\n\t\t\tplacement = arc.Audience\n\t\t} else {\n\t\t\tplacement = issuerAccount\n\t\t}\n\n\t\ttargetAcc, err := s.LookupAccount(placement)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"no valid account %q for auth callout response on account %q: %v\", placement, account, err)\n\t\t}\n\t\tif isOperatorMode {\n\t\t\t// this will validate the signing key that emitted the user, and if it is a signing\n\t\t\t// key it assigns the permissions from the target account\n\t\t\tif scope, ok := targetAcc.hasIssuer(arc.Issuer); !ok {\n\t\t\t\treturn nil, fmt.Errorf(\"user JWT issuer %q is not known\", arc.Issuer)\n\t\t\t} else if scope != nil {\n\t\t\t\t// this possibly has to be different because it could just be a plain issued by a non-scoped signing key\n\t\t\t\tif err := scope.ValidateScopedSigner(arc); err != nil {\n\t\t\t\t\treturn nil, fmt.Errorf(\"user JWT is not valid: %v\", err)\n\t\t\t\t} else if uSc, ok := scope.(*jwt.UserScope); !ok {\n\t\t\t\t\treturn nil, fmt.Errorf(\"user JWT is not a valid scoped user\")\n\t\t\t\t} else if arc.User.UserPermissionLimits, err = processUserPermissionsTemplate(uSc.Template, arc, targetAcc); err != nil {\n\t\t\t\t\treturn nil, fmt.Errorf(\"user JWT generated invalid permissions: %v\", err)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\treturn targetAcc, nil\n\t}\n\n\tprocessReply := func(_ *subscription, rc *client, racc *Account, subject, reply string, rmsg []byte) {\n\t\ttitleCase := func(m string) string {\n\t\t\tr := []rune(m)\n\t\t\treturn string(append([]rune{unicode.ToUpper(r[0])}, r[1:]...))\n\t\t}\n\n\t\tarc, err := decodeResponse(rc, rmsg, racc)\n\t\tif err != nil {\n\t\t\tc.authViolation()\n\t\t\trespCh <- titleCase(err.Error())\n\t\t\treturn\n\t\t}\n\t\tvr := jwt.CreateValidationResults()\n\t\tarc.Validate(vr)\n\t\tif len(vr.Issues) > 0 {\n\t\t\tc.authViolation()\n\t\t\trespCh <- fmt.Sprintf(\"Error validating user JWT: %v\", vr.Issues[0])\n\t\t\treturn\n\t\t}\n\n\t\t// Make sure that the user is what we requested.\n\t\tif arc.Subject != pub {\n\t\t\tc.authViolation()\n\t\t\trespCh <- fmt.Sprintf(\"Expected authorized user of %q but got %q on account %q\", pub, arc.Subject, racc.Name)\n\t\t\treturn\n\t\t}\n\n\t\texpiration, allowedConnTypes, err := getExpirationAndAllowedConnections(arc, racc.Name)\n\t\tif err != nil {\n\t\t\tc.authViolation()\n\t\t\trespCh <- titleCase(err.Error())\n\t\t\treturn\n\t\t}\n\n\t\ttargetAcc, err := assignAccountAndPermissions(arc, racc.Name)\n\t\tif err != nil {\n\t\t\tc.authViolation()\n\t\t\trespCh <- titleCase(err.Error())\n\t\t\treturn\n\t\t}\n\n\t\t// the JWT is cleared, because if in operator mode it may hold the JWT\n\t\t// for the bearer token that connected to the callout if in operator mode\n\t\t// the permissions are already set on the client, this prevents a decode\n\t\t// on c.RegisterNKeyUser which would have wrong values\n\t\tc.mu.Lock()\n\t\tc.opts.JWT = _EMPTY_\n\t\tc.mu.Unlock()\n\n\t\t// Build internal user and bind to the targeted account.\n\t\tnkuser := buildInternalNkeyUser(arc, allowedConnTypes, targetAcc)\n\t\tif err := c.RegisterNkeyUser(nkuser); err != nil {\n\t\t\tc.authViolation()\n\t\t\trespCh <- fmt.Sprintf(\"Could not register auth callout user: %v\", err)\n\t\t\treturn\n\t\t}\n\n\t\t// See if the response wants to override the username.\n\t\tif arc.Name != _EMPTY_ {\n\t\t\tc.mu.Lock()\n\t\t\tc.opts.Username = arc.Name\n\t\t\t// Clear any others.\n\t\t\tc.opts.Nkey = _EMPTY_\n\t\t\tc.pubKey = _EMPTY_\n\t\t\tc.opts.Token = _EMPTY_\n\t\t\tc.mu.Unlock()\n\t\t}\n\n\t\t// Check if we need to set an auth timer if the user jwt expires.\n\t\tc.setExpiration(arc.Claims(), expiration)\n\n\t\trespCh <- _EMPTY_\n\t}\n\n\t// create a subscription to receive a response from the authcallout\n\tsub, err := acc.subscribeInternal(reply, processReply)\n\tif err != nil {\n\t\terrStr = fmt.Sprintf(\"Error setting up reply subscription for auth request: %v\", err)\n\t\ts.Warnf(errStr)\n\t\treturn false, errStr\n\t}\n\tdefer acc.unsubscribeInternal(sub)\n\n\t// Build our request claims - jwt subject should be nkey\n\tjwtSub := acc.Name\n\tif opts.AuthCallout != nil {\n\t\tjwtSub = opts.AuthCallout.Issuer\n\t}\n\n\t// The public key of the server, if set is available on Varz.Key\n\t// This means that when a service connects, it can now peer\n\t// authenticate if it wants to - but that also means that it needs to be\n\t// listening to cluster changes\n\tclaim := jwt.NewAuthorizationRequestClaims(jwtSub)\n\tclaim.Audience = AuthRequestSubject\n\t// Set expected public user nkey.\n\tclaim.UserNkey = pub\n\n\ts.mu.RLock()\n\tclaim.Server = jwt.ServerID{\n\t\tName:    s.info.Name,\n\t\tHost:    s.info.Host,\n\t\tID:      s.info.ID,\n\t\tVersion: s.info.Version,\n\t\tCluster: s.info.Cluster,\n\t}\n\ts.mu.RUnlock()\n\n\t// Tags\n\tclaim.Server.Tags = s.getOpts().Tags\n\n\t// Check if we have been requested to encrypt.\n\t// FIXME: possibly this public key also needs to be on the\n\t//  Varz, because then it can be peer verified?\n\tif xkp != nil {\n\t\tclaim.Server.XKey = xkey\n\t}\n\n\tauthTimeout := secondsToDuration(s.getOpts().AuthTimeout)\n\tclaim.Expires = time.Now().Add(time.Duration(authTimeout)).UTC().Unix()\n\n\t// Grab client info for the request.\n\tc.mu.Lock()\n\tc.fillClientInfo(&claim.ClientInformation)\n\tc.fillConnectOpts(&claim.ConnectOptions)\n\t// If we have a sig in the client opts, fill in nonce.\n\tif claim.ConnectOptions.SignedNonce != _EMPTY_ {\n\t\tclaim.ClientInformation.Nonce = string(c.nonce)\n\t}\n\n\t// TLS\n\tif c.flags.isSet(handshakeComplete) && c.nc != nil {\n\t\tvar ct jwt.ClientTLS\n\t\tconn := c.nc.(*tls.Conn)\n\t\tcs := conn.ConnectionState()\n\t\tct.Version = tlsVersion(cs.Version)\n\t\tct.Cipher = tlsCipher(cs.CipherSuite)\n\t\t// Check verified chains.\n\t\tfor _, vs := range cs.VerifiedChains {\n\t\t\tvar certs []string\n\t\t\tfor _, c := range vs {\n\t\t\t\tblk := &pem.Block{\n\t\t\t\t\tType:  \"CERTIFICATE\",\n\t\t\t\t\tBytes: c.Raw,\n\t\t\t\t}\n\t\t\t\tcerts = append(certs, string(pem.EncodeToMemory(blk)))\n\t\t\t}\n\t\t\tct.VerifiedChains = append(ct.VerifiedChains, certs)\n\t\t}\n\t\t// If we do not have verified chains put in peer certs.\n\t\tif len(ct.VerifiedChains) == 0 {\n\t\t\tfor _, c := range cs.PeerCertificates {\n\t\t\t\tblk := &pem.Block{\n\t\t\t\t\tType:  \"CERTIFICATE\",\n\t\t\t\t\tBytes: c.Raw,\n\t\t\t\t}\n\t\t\t\tct.Certs = append(ct.Certs, string(pem.EncodeToMemory(blk)))\n\t\t\t}\n\t\t}\n\t\tclaim.TLS = &ct\n\t}\n\tc.mu.Unlock()\n\n\tb, err := claim.Encode(s.kp)\n\tif err != nil {\n\t\terrStr = fmt.Sprintf(\"Error encoding auth request claim on account %q: %v\", acc.Name, err)\n\t\ts.Warnf(errStr)\n\t\treturn false, errStr\n\t}\n\treq := []byte(b)\n\tvar hdr map[string]string\n\n\t// Check if we have been asked to encrypt.\n\tif xkp != nil {\n\t\treq, err = xkp.Seal([]byte(req), pubAccXKey)\n\t\tif err != nil {\n\t\t\terrStr = fmt.Sprintf(\"Error encrypting auth request claim on account %q: %v\", acc.Name, err)\n\t\t\ts.Warnf(errStr)\n\t\t\treturn false, errStr\n\t\t}\n\t\thdr = map[string]string{AuthRequestXKeyHeader: xkey}\n\t}\n\n\t// Send out our request.\n\tif err := s.sendInternalAccountMsgWithReply(acc, AuthCalloutSubject, reply, hdr, req, false); err != nil {\n\t\terrStr = fmt.Sprintf(\"Error sending authorization request: %v\", err)\n\t\ts.Debugf(errStr)\n\t\treturn false, errStr\n\t}\n\tselect {\n\tcase errStr = <-respCh:\n\t\tif authorized = errStr == _EMPTY_; !authorized {\n\t\t\ts.Warnf(errStr)\n\t\t}\n\tcase <-time.After(authTimeout):\n\t\ts.Debugf(fmt.Sprintf(\"Authorization callout response not received in time on account %q\", acc.Name))\n\t}\n\n\treturn authorized, errStr\n}\n\n// Fill in client information for the request.\n// Lock should be held.\nfunc (c *client) fillClientInfo(ci *jwt.ClientInformation) {\n\tif c == nil || (c.kind != CLIENT && c.kind != LEAF && c.kind != JETSTREAM && c.kind != ACCOUNT) {\n\t\treturn\n\t}\n\n\t// Do it this way to fail to compile if fields are added to jwt.ClientInformation.\n\t*ci = jwt.ClientInformation{\n\t\tHost:    c.host,\n\t\tID:      c.cid,\n\t\tUser:    c.getRawAuthUser(),\n\t\tName:    c.opts.Name,\n\t\tTags:    c.tags,\n\t\tNameTag: c.nameTag,\n\t\tKind:    c.kindString(),\n\t\tType:    c.clientTypeString(),\n\t\tMQTT:    c.getMQTTClientID(),\n\t}\n}\n\n// Fill in client options.\n// Lock should be held.\nfunc (c *client) fillConnectOpts(opts *jwt.ConnectOptions) {\n\tif c == nil || (c.kind != CLIENT && c.kind != LEAF && c.kind != JETSTREAM && c.kind != ACCOUNT) {\n\t\treturn\n\t}\n\n\to := c.opts\n\n\t// Do it this way to fail to compile if fields are added to jwt.ClientInformation.\n\t*opts = jwt.ConnectOptions{\n\t\tJWT:         o.JWT,\n\t\tNkey:        o.Nkey,\n\t\tSignedNonce: o.Sig,\n\t\tToken:       o.Token,\n\t\tUsername:    o.Username,\n\t\tPassword:    o.Password,\n\t\tName:        o.Name,\n\t\tLang:        o.Lang,\n\t\tVersion:     o.Version,\n\t\tProtocol:    o.Protocol,\n\t}\n}\n",
    "source_file": "server/auth_callout.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2019-2025 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage server\n\nimport (\n\t\"archive/tar\"\n\t\"bytes\"\n\t\"crypto/aes\"\n\t\"crypto/cipher\"\n\t\"crypto/rand\"\n\t\"crypto/sha256\"\n\t\"encoding/binary\"\n\t\"encoding/hex\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"fmt\"\n\t\"hash\"\n\t\"io\"\n\t\"io/fs\"\n\t\"math\"\n\tmrand \"math/rand\"\n\t\"net\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"runtime\"\n\t\"slices\"\n\t\"sort\"\n\t\"strings\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t\"github.com/klauspost/compress/s2\"\n\t\"github.com/minio/highwayhash\"\n\t\"github.com/nats-io/nats-server/v2/server/ats\"\n\t\"github.com/nats-io/nats-server/v2/server/avl\"\n\t\"github.com/nats-io/nats-server/v2/server/stree\"\n\t\"github.com/nats-io/nats-server/v2/server/thw\"\n\t\"golang.org/x/crypto/chacha20\"\n\t\"golang.org/x/crypto/chacha20poly1305\"\n)\n\ntype FileStoreConfig struct {\n\t// Where the parent directory for all storage will be located.\n\tStoreDir string\n\t// BlockSize is the file block size. This also represents the maximum overhead size.\n\tBlockSize uint64\n\t// CacheExpire is how long with no activity until we expire the cache.\n\tCacheExpire time.Duration\n\t// SubjectStateExpire is how long with no activity until we expire a msg block's subject state.\n\tSubjectStateExpire time.Duration\n\t// SyncInterval is how often we sync to disk in the background.\n\tSyncInterval time.Duration\n\t// SyncAlways is when the stream should sync all data writes.\n\tSyncAlways bool\n\t// AsyncFlush allows async flush to batch write operations.\n\tAsyncFlush bool\n\t// Cipher is the cipher to use when encrypting.\n\tCipher StoreCipher\n\t// Compression is the algorithm to use when compressing.\n\tCompression StoreCompression\n\n\t// Internal reference to our server.\n\tsrv *Server\n}\n\n// FileStreamInfo allows us to remember created time.\ntype FileStreamInfo struct {\n\tCreated time.Time\n\tStreamConfig\n}\n\ntype StoreCipher int\n\nconst (\n\tChaCha StoreCipher = iota\n\tAES\n\tNoCipher\n)\n\nfunc (cipher StoreCipher) String() string {\n\tswitch cipher {\n\tcase ChaCha:\n\t\treturn \"ChaCha20-Poly1305\"\n\tcase AES:\n\t\treturn \"AES-GCM\"\n\tcase NoCipher:\n\t\treturn \"None\"\n\tdefault:\n\t\treturn \"Unknown StoreCipher\"\n\t}\n}\n\ntype StoreCompression uint8\n\nconst (\n\tNoCompression StoreCompression = iota\n\tS2Compression\n)\n\nfunc (alg StoreCompression) String() string {\n\tswitch alg {\n\tcase NoCompression:\n\t\treturn \"None\"\n\tcase S2Compression:\n\t\treturn \"S2\"\n\tdefault:\n\t\treturn \"Unknown StoreCompression\"\n\t}\n}\n\nfunc (alg StoreCompression) MarshalJSON() ([]byte, error) {\n\tvar str string\n\tswitch alg {\n\tcase S2Compression:\n\t\tstr = \"s2\"\n\tcase NoCompression:\n\t\tstr = \"none\"\n\tdefault:\n\t\treturn nil, fmt.Errorf(\"unknown compression algorithm\")\n\t}\n\treturn json.Marshal(str)\n}\n\nfunc (alg *StoreCompression) UnmarshalJSON(b []byte) error {\n\tvar str string\n\tif err := json.Unmarshal(b, &str); err != nil {\n\t\treturn err\n\t}\n\tswitch str {\n\tcase \"s2\":\n\t\t*alg = S2Compression\n\tcase \"none\":\n\t\t*alg = NoCompression\n\tdefault:\n\t\treturn fmt.Errorf(\"unknown compression algorithm\")\n\t}\n\treturn nil\n}\n\n// File ConsumerInfo is used for creating consumer stores.\ntype FileConsumerInfo struct {\n\tCreated time.Time\n\tName    string\n\tConsumerConfig\n}\n\n// Default file and directory permissions.\nconst (\n\tdefaultDirPerms  = os.FileMode(0700)\n\tdefaultFilePerms = os.FileMode(0600)\n)\n\ntype psi struct {\n\ttotal uint64\n\tfblk  uint32\n\tlblk  uint32\n}\n\ntype fileStore struct {\n\tsrv         *Server\n\tmu          sync.RWMutex\n\tstate       StreamState\n\ttombs       []uint64\n\tld          *LostStreamData\n\tscb         StorageUpdateHandler\n\trmcb        StorageRemoveMsgHandler\n\tsdmcb       SubjectDeleteMarkerUpdateHandler\n\tageChk      *time.Timer\n\tsyncTmr     *time.Timer\n\tcfg         FileStreamInfo\n\tfcfg        FileStoreConfig\n\tprf         keyGen\n\toldprf      keyGen\n\taek         cipher.AEAD\n\tlmb         *msgBlock\n\tblks        []*msgBlock\n\tbim         map[uint32]*msgBlock\n\tpsim        *stree.SubjectTree[psi]\n\ttsl         int\n\tadml        int\n\thh          hash.Hash64\n\tqch         chan struct{}\n\tfsld        chan struct{}\n\tcmu         sync.RWMutex\n\tcfs         []ConsumerStore\n\tsips        int\n\tdirty       int\n\tclosing     bool\n\tclosed      bool\n\tfip         bool\n\treceivedAny bool\n\tfirstMoved  bool\n\tttls        *thw.HashWheel\n\tsdm         *SDMMeta\n\tlpex        time.Time // Last PurgeEx call.\n}\n\n// Represents a message store block and its data.\ntype msgBlock struct {\n\t// Here for 32bit systems and atomic.\n\tfirst      msgId\n\tlast       msgId\n\tmu         sync.RWMutex\n\tfs         *fileStore\n\taek        cipher.AEAD\n\tbek        cipher.Stream\n\tseed       []byte\n\tnonce      []byte\n\tmfn        string\n\tmfd        *os.File\n\tcmp        StoreCompression // Effective compression at the time of loading the block\n\tliwsz      int64\n\tindex      uint32\n\tbytes      uint64 // User visible bytes count.\n\trbytes     uint64 // Total bytes (raw) including deleted. Used for rolling to new blk.\n\tcbytes     uint64 // Bytes count after last compaction. 0 if no compaction happened yet.\n\tmsgs       uint64 // User visible message count.\n\tfss        *stree.SubjectTree[SimpleState]\n\tkfn        string\n\tlwts       int64\n\tllts       int64\n\tlrts       int64\n\tlsts       int64\n\tllseq      uint64\n\thh         hash.Hash64\n\tcache      *cache\n\tcloads     uint64\n\tcexp       time.Duration\n\tfexp       time.Duration\n\tctmr       *time.Timer\n\twerr       error\n\tdmap       avl.SequenceSet\n\tfch        chan struct{}\n\tqch        chan struct{}\n\tlchk       [8]byte\n\tloading    bool\n\tflusher    bool\n\tnoTrack    bool\n\tneedSync   bool\n\tsyncAlways bool\n\tnoCompact  bool\n\tclosed     bool\n\tttls       uint64 // How many msgs have TTLs?\n\n\t// Used to mock write failures.\n\tmockWriteErr bool\n}\n\n// Write through caching layer that is also used on loading messages.\ntype cache struct {\n\tbuf  []byte\n\toff  int\n\twp   int\n\tidx  []uint32\n\tlrl  uint32\n\tfseq uint64\n\tnra  bool\n}\n\ntype msgId struct {\n\tseq uint64\n\tts  int64\n}\n\nconst (\n\t// Magic is used to identify the file store files.\n\tmagic = uint8(22)\n\t// Version\n\tversion = uint8(1)\n\t// New IndexInfo Version\n\tnewVersion = uint8(2)\n\t// hdrLen\n\thdrLen = 2\n\t// This is where we keep the streams.\n\tstreamsDir = \"streams\"\n\t// This is where we keep the message store blocks.\n\tmsgDir = \"msgs\"\n\t// This is where we temporarily move the messages dir.\n\tpurgeDir = \"__msgs__\"\n\t// used to scan blk file names.\n\tblkScan = \"%d.blk\"\n\t// suffix of a block file\n\tblkSuffix = \".blk\"\n\t// used for compacted blocks that are staged.\n\tnewScan = \"%d.new\"\n\t// used to scan index file names.\n\tindexScan = \"%d.idx\"\n\t// used to store our block encryption key.\n\tkeyScan = \"%d.key\"\n\t// to look for orphans\n\tkeyScanAll = \"*.key\"\n\t// This is where we keep state on consumers.\n\tconsumerDir = \"obs\"\n\t// Index file for a consumer.\n\tconsumerState = \"o.dat\"\n\t// The suffix that will be given to a new temporary block during compression.\n\tcompressTmpSuffix = \".tmp\"\n\t// This is where we keep state on templates.\n\ttmplsDir = \"templates\"\n\t// Maximum size of a write buffer we may consider for re-use.\n\tmaxBufReuse = 2 * 1024 * 1024\n\t// default cache buffer expiration\n\tdefaultCacheBufferExpiration = 10 * time.Second\n\t// default sync interval\n\tdefaultSyncInterval = 2 * time.Minute\n\t// default idle timeout to close FDs.\n\tcloseFDsIdle = 30 * time.Second\n\t// default expiration time for mb.fss when idle.\n\tdefaultFssExpiration = 2 * time.Minute\n\t// coalesceMinimum\n\tcoalesceMinimum = 16 * 1024\n\t// maxFlushWait is maximum we will wait to gather messages to flush.\n\tmaxFlushWait = 8 * time.Millisecond\n\n\t// Metafiles for streams and consumers.\n\tJetStreamMetaFile    = \"meta.inf\"\n\tJetStreamMetaFileSum = \"meta.sum\"\n\tJetStreamMetaFileKey = \"meta.key\"\n\n\t// This is the full snapshotted state for the stream.\n\tstreamStreamStateFile = \"index.db\"\n\n\t// This is the encoded time hash wheel for TTLs.\n\tttlStreamStateFile = \"thw.db\"\n\n\t// AEK key sizes\n\tminMetaKeySize = 64\n\tminBlkKeySize  = 64\n\n\t// Default stream block size.\n\tdefaultLargeBlockSize = 8 * 1024 * 1024 // 8MB\n\t// Default for workqueue or interest based.\n\tdefaultMediumBlockSize = 4 * 1024 * 1024 // 4MB\n\t// For smaller reuse buffers. Usually being generated during contention on the lead write buffer.\n\t// E.g. mirrors/sources etc.\n\tdefaultSmallBlockSize = 1 * 1024 * 1024 // 1MB\n\t// Maximum size for the encrypted head block.\n\tmaximumEncryptedBlockSize = 2 * 1024 * 1024 // 2MB\n\t// Default for KV based\n\tdefaultKVBlockSize = defaultMediumBlockSize\n\t// max block size for now.\n\tmaxBlockSize = defaultLargeBlockSize\n\t// Compact minimum threshold.\n\tcompactMinimum = 2 * 1024 * 1024 // 2MB\n\t// FileStoreMinBlkSize is minimum size we will do for a blk size.\n\tFileStoreMinBlkSize = 32 * 1000 // 32kib\n\t// FileStoreMaxBlkSize is maximum size we will do for a blk size.\n\tFileStoreMaxBlkSize = maxBlockSize\n\t// Check for bad record length value due to corrupt data.\n\trlBadThresh = 32 * 1024 * 1024\n\t// Checksum size for hash for msg records.\n\trecordHashSize = 8\n)\n\nfunc newFileStore(fcfg FileStoreConfig, cfg StreamConfig) (*fileStore, error) {\n\treturn newFileStoreWithCreated(fcfg, cfg, time.Now().UTC(), nil, nil)\n}\n\nfunc newFileStoreWithCreated(fcfg FileStoreConfig, cfg StreamConfig, created time.Time, prf, oldprf keyGen) (fs *fileStore, err error) {\n\tif cfg.Name == _EMPTY_ {\n\t\treturn nil, fmt.Errorf(\"name required\")\n\t}\n\tif cfg.Storage != FileStorage {\n\t\treturn nil, fmt.Errorf(\"fileStore requires file storage type in config\")\n\t}\n\t// Default values.\n\tif fcfg.BlockSize == 0 {\n\t\tfcfg.BlockSize = dynBlkSize(cfg.Retention, cfg.MaxBytes, prf != nil)\n\t}\n\tif fcfg.BlockSize > maxBlockSize {\n\t\treturn nil, fmt.Errorf(\"filestore max block size is %s\", friendlyBytes(maxBlockSize))\n\t}\n\tif fcfg.CacheExpire == 0 {\n\t\tfcfg.CacheExpire = defaultCacheBufferExpiration\n\t}\n\tif fcfg.SubjectStateExpire == 0 {\n\t\tfcfg.SubjectStateExpire = defaultFssExpiration\n\t}\n\tif fcfg.SyncInterval == 0 {\n\t\tfcfg.SyncInterval = defaultSyncInterval\n\t}\n\n\t// Check the directory\n\tif stat, err := os.Stat(fcfg.StoreDir); os.IsNotExist(err) {\n\t\tif err := os.MkdirAll(fcfg.StoreDir, defaultDirPerms); err != nil {\n\t\t\treturn nil, fmt.Errorf(\"could not create storage directory - %v\", err)\n\t\t}\n\t} else if stat == nil || !stat.IsDir() {\n\t\treturn nil, fmt.Errorf(\"storage directory is not a directory\")\n\t}\n\ttmpfile, err := os.CreateTemp(fcfg.StoreDir, \"_test_\")\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"storage directory is not writable\")\n\t}\n\n\ttmpfile.Close()\n\t<-dios\n\tos.Remove(tmpfile.Name())\n\tdios <- struct{}{}\n\n\tfs = &fileStore{\n\t\tfcfg:   fcfg,\n\t\tpsim:   stree.NewSubjectTree[psi](),\n\t\tbim:    make(map[uint32]*msgBlock),\n\t\tcfg:    FileStreamInfo{Created: created, StreamConfig: cfg},\n\t\tprf:    prf,\n\t\toldprf: oldprf,\n\t\tqch:    make(chan struct{}),\n\t\tfsld:   make(chan struct{}),\n\t\tsrv:    fcfg.srv,\n\t}\n\n\t// Register with access time service.\n\tats.Register()\n\n\t// If we error before completion make sure to cleanup.\n\tdefer func() {\n\t\tif err != nil {\n\t\t\tats.Unregister()\n\t\t}\n\t}()\n\n\t// Only create a THW if we're going to allow TTLs.\n\tif cfg.AllowMsgTTL {\n\t\tfs.ttls = thw.NewHashWheel()\n\t}\n\n\t// Set flush in place to AsyncFlush which by default is false.\n\tfs.fip = !fcfg.AsyncFlush\n\n\t// Check if this is a new setup.\n\tmdir := filepath.Join(fcfg.StoreDir, msgDir)\n\todir := filepath.Join(fcfg.StoreDir, consumerDir)\n\tif err := os.MkdirAll(mdir, defaultDirPerms); err != nil {\n\t\treturn nil, fmt.Errorf(\"could not create message storage directory - %v\", err)\n\t}\n\tif err := os.MkdirAll(odir, defaultDirPerms); err != nil {\n\t\treturn nil, fmt.Errorf(\"could not create consumer storage directory - %v\", err)\n\t}\n\n\t// Create highway hash for message blocks. Use sha256 of directory as key.\n\tkey := sha256.Sum256([]byte(cfg.Name))\n\tfs.hh, err = highwayhash.New64(key[:])\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"could not create hash: %v\", err)\n\t}\n\n\tkeyFile := filepath.Join(fs.fcfg.StoreDir, JetStreamMetaFileKey)\n\t// Make sure we do not have an encrypted store underneath of us but no main key.\n\tif fs.prf == nil {\n\t\tif _, err := os.Stat(keyFile); err == nil {\n\t\t\treturn nil, errNoMainKey\n\t\t}\n\t}\n\n\t// Attempt to recover our state.\n\terr = fs.recoverFullState()\n\tif err != nil {\n\t\tif !os.IsNotExist(err) {\n\t\t\tfs.warn(\"Recovering stream state from index errored: %v\", err)\n\t\t}\n\t\t// Hold onto state\n\t\tprior := fs.state\n\t\t// Reset anything that could have been set from above.\n\t\tfs.state = StreamState{}\n\t\tfs.psim, fs.tsl = fs.psim.Empty(), 0\n\t\tfs.bim = make(map[uint32]*msgBlock)\n\t\tfs.blks = nil\n\t\tfs.tombs = nil\n\n\t\t// Recover our message state the old way\n\t\tif err := fs.recoverMsgs(); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\t// Check if our prior state remembers a last sequence past where we can see.\n\t\tif prior.LastSeq > fs.state.LastSeq {\n\t\t\tfs.state.LastSeq, fs.state.LastTime = prior.LastSeq, prior.LastTime\n\t\t\tif fs.state.Msgs == 0 {\n\t\t\t\tfs.state.FirstSeq = fs.state.LastSeq + 1\n\t\t\t\tfs.state.FirstTime = time.Time{}\n\t\t\t}\n\t\t\tif fs.ld != nil {\n\t\t\t\tif _, err := fs.newMsgBlockForWrite(); err == nil {\n\t\t\t\t\tif err = fs.writeTombstone(prior.LastSeq, prior.LastTime.UnixNano()); err != nil {\n\t\t\t\t\t\treturn nil, err\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\treturn nil, err\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t// Since we recovered here, make sure to kick ourselves to write out our stream state.\n\t\tfs.dirty++\n\t}\n\n\t// See if we can bring back our TTL timed hash wheel state from disk.\n\tif cfg.AllowMsgTTL {\n\t\tif err = fs.recoverTTLState(); err != nil && !os.IsNotExist(err) {\n\t\t\tfs.warn(\"Recovering TTL state from index errored: %v\", err)\n\t\t}\n\t}\n\n\t// Also make sure we get rid of old idx and fss files on return.\n\t// Do this in separate go routine vs inline and at end of processing.\n\tdefer func() {\n\t\tif fs != nil {\n\t\t\tgo fs.cleanupOldMeta()\n\t\t}\n\t}()\n\n\t// Lock while we do enforcements and removals.\n\tfs.mu.Lock()\n\n\t// Check if we have any left over tombstones to process.\n\tif len(fs.tombs) > 0 {\n\t\tfor _, seq := range fs.tombs {\n\t\t\tfs.removeMsg(seq, false, true, false)\n\t\t\tfs.removeFromLostData(seq)\n\t\t}\n\t\t// Not needed after this phase.\n\t\tfs.tombs = nil\n\t}\n\n\t// Limits checks and enforcement.\n\tfs.enforceMsgLimit()\n\tfs.enforceBytesLimit()\n\n\t// Do age checks too, make sure to call in place.\n\tif fs.cfg.MaxAge != 0 {\n\t\terr := fs.expireMsgsOnRecover()\n\t\tif isPermissionError(err) {\n\t\t\treturn nil, err\n\t\t}\n\t\tfs.startAgeChk()\n\t}\n\n\t// If we have max msgs per subject make sure the is also enforced.\n\tif fs.cfg.MaxMsgsPer > 0 {\n\t\tfs.enforceMsgPerSubjectLimit(false)\n\t}\n\n\t// Grab first sequence for check below while we have lock.\n\tfirstSeq := fs.state.FirstSeq\n\tfs.mu.Unlock()\n\n\t// If the stream has an initial sequence number then make sure we\n\t// have purged up until that point. We will do this only if the\n\t// recovered first sequence number is before our configured first\n\t// sequence. Need to do this locked as by now the age check timer\n\t// has started.\n\tif cfg.FirstSeq > 0 && firstSeq < cfg.FirstSeq {\n\t\tif _, err := fs.purge(cfg.FirstSeq); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\t// Write our meta data if it does not exist or is zero'd out.\n\tmeta := filepath.Join(fcfg.StoreDir, JetStreamMetaFile)\n\tfi, err := os.Stat(meta)\n\tif err != nil && os.IsNotExist(err) || fi != nil && fi.Size() == 0 {\n\t\tif err := fs.writeStreamMeta(); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\t// If we expect to be encrypted check that what we are restoring is not plaintext.\n\t// This can happen on snapshot restores or conversions.\n\tif fs.prf != nil {\n\t\tif _, err := os.Stat(keyFile); err != nil && os.IsNotExist(err) {\n\t\t\tif err := fs.writeStreamMeta(); err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t}\n\t}\n\n\t// Setup our sync timer.\n\tfs.setSyncTimer()\n\n\t// Spin up the go routine that will write out our full state stream index.\n\tgo fs.flushStreamStateLoop(fs.qch, fs.fsld)\n\n\treturn fs, nil\n}\n\n// Lock all existing message blocks.\n// Lock held on entry.\nfunc (fs *fileStore) lockAllMsgBlocks() {\n\tfor _, mb := range fs.blks {\n\t\tmb.mu.Lock()\n\t}\n}\n\n// Unlock all existing message blocks.\n// Lock held on entry.\nfunc (fs *fileStore) unlockAllMsgBlocks() {\n\tfor _, mb := range fs.blks {\n\t\tmb.mu.Unlock()\n\t}\n}\n\nfunc (fs *fileStore) UpdateConfig(cfg *StreamConfig) error {\n\tstart := time.Now()\n\tdefer func() {\n\t\tif took := time.Since(start); took > time.Minute {\n\t\t\tfs.warn(\"UpdateConfig took %v\", took.Round(time.Millisecond))\n\t\t}\n\t}()\n\n\tif fs.isClosed() {\n\t\treturn ErrStoreClosed\n\t}\n\tif cfg.Name == _EMPTY_ {\n\t\treturn fmt.Errorf(\"name required\")\n\t}\n\tif cfg.Storage != FileStorage {\n\t\treturn fmt.Errorf(\"fileStore requires file storage type in config\")\n\t}\n\tif cfg.MaxMsgsPer < -1 {\n\t\tcfg.MaxMsgsPer = -1\n\t}\n\n\tfs.mu.Lock()\n\tnew_cfg := FileStreamInfo{Created: fs.cfg.Created, StreamConfig: *cfg}\n\told_cfg := fs.cfg\n\t// The reference story has changed here, so this full msg block lock\n\t// may not be needed.\n\tfs.lockAllMsgBlocks()\n\tfs.cfg = new_cfg\n\tfs.unlockAllMsgBlocks()\n\tif err := fs.writeStreamMeta(); err != nil {\n\t\tfs.lockAllMsgBlocks()\n\t\tfs.cfg = old_cfg\n\t\tfs.unlockAllMsgBlocks()\n\t\tfs.mu.Unlock()\n\t\treturn err\n\t}\n\n\t// Create or delete the THW if needed.\n\tif cfg.AllowMsgTTL && fs.ttls == nil {\n\t\tfs.ttls = thw.NewHashWheel()\n\t} else if !cfg.AllowMsgTTL && fs.ttls != nil {\n\t\tfs.ttls = nil\n\t}\n\n\t// Limits checks and enforcement.\n\tfs.enforceMsgLimit()\n\tfs.enforceBytesLimit()\n\n\t// Do age timers.\n\tif fs.ageChk == nil && fs.cfg.MaxAge != 0 {\n\t\tfs.startAgeChk()\n\t}\n\tif fs.ageChk != nil && fs.cfg.MaxAge == 0 {\n\t\tfs.ageChk.Stop()\n\t\tfs.ageChk = nil\n\t}\n\n\tif fs.cfg.MaxMsgsPer > 0 && (old_cfg.MaxMsgsPer == 0 || fs.cfg.MaxMsgsPer < old_cfg.MaxMsgsPer) {\n\t\tfs.enforceMsgPerSubjectLimit(true)\n\t}\n\tfs.mu.Unlock()\n\n\tif cfg.MaxAge != 0 || cfg.AllowMsgTTL {\n\t\tfs.expireMsgs()\n\t}\n\treturn nil\n}\n\nfunc dynBlkSize(retention RetentionPolicy, maxBytes int64, encrypted bool) uint64 {\n\tif maxBytes > 0 {\n\t\tblkSize := (maxBytes / 4) + 1 // (25% overhead)\n\t\t// Round up to nearest 100\n\t\tif m := blkSize % 100; m != 0 {\n\t\t\tblkSize += 100 - m\n\t\t}\n\t\tif blkSize <= FileStoreMinBlkSize {\n\t\t\tblkSize = FileStoreMinBlkSize\n\t\t} else if blkSize >= FileStoreMaxBlkSize {\n\t\t\tblkSize = FileStoreMaxBlkSize\n\t\t} else {\n\t\t\tblkSize = defaultMediumBlockSize\n\t\t}\n\t\tif encrypted && blkSize > maximumEncryptedBlockSize {\n\t\t\t// Notes on this below.\n\t\t\tblkSize = maximumEncryptedBlockSize\n\t\t}\n\t\treturn uint64(blkSize)\n\t}\n\n\tswitch {\n\tcase encrypted:\n\t\t// In the case of encrypted stores, large blocks can result in worsened perf\n\t\t// since many writes on disk involve re-encrypting the entire block. For now,\n\t\t// we will enforce a cap on the block size when encryption is enabled to avoid\n\t\t// this.\n\t\treturn maximumEncryptedBlockSize\n\tcase retention == LimitsPolicy:\n\t\t// TODO(dlc) - Make the blocksize relative to this if set.\n\t\treturn defaultLargeBlockSize\n\tdefault:\n\t\t// TODO(dlc) - Make the blocksize relative to this if set.\n\t\treturn defaultMediumBlockSize\n\t}\n}\n\nfunc genEncryptionKey(sc StoreCipher, seed []byte) (ek cipher.AEAD, err error) {\n\tif sc == ChaCha {\n\t\tek, err = chacha20poly1305.NewX(seed)\n\t} else if sc == AES {\n\t\tblock, e := aes.NewCipher(seed)\n\t\tif e != nil {\n\t\t\treturn nil, e\n\t\t}\n\t\tek, err = cipher.NewGCMWithNonceSize(block, block.BlockSize())\n\t} else {\n\t\terr = errUnknownCipher\n\t}\n\treturn ek, err\n}\n\n// Generate an asset encryption key from the context and server PRF.\nfunc (fs *fileStore) genEncryptionKeys(context string) (aek cipher.AEAD, bek cipher.Stream, seed, encrypted []byte, err error) {\n\tif fs.prf == nil {\n\t\treturn nil, nil, nil, nil, errNoEncryption\n\t}\n\t// Generate key encryption key.\n\trb, err := fs.prf([]byte(context))\n\tif err != nil {\n\t\treturn nil, nil, nil, nil, err\n\t}\n\n\tsc := fs.fcfg.Cipher\n\n\tkek, err := genEncryptionKey(sc, rb)\n\tif err != nil {\n\t\treturn nil, nil, nil, nil, err\n\t}\n\t// Generate random asset encryption key seed.\n\n\tconst seedSize = 32\n\tseed = make([]byte, seedSize)\n\tif n, err := rand.Read(seed); err != nil {\n\t\treturn nil, nil, nil, nil, err\n\t} else if n != seedSize {\n\t\treturn nil, nil, nil, nil, fmt.Errorf(\"not enough seed bytes read (%d != %d\", n, seedSize)\n\t}\n\n\taek, err = genEncryptionKey(sc, seed)\n\tif err != nil {\n\t\treturn nil, nil, nil, nil, err\n\t}\n\n\t// Generate our nonce. Use same buffer to hold encrypted seed.\n\tnonce := make([]byte, kek.NonceSize(), kek.NonceSize()+len(seed)+kek.Overhead())\n\tif n, err := rand.Read(nonce); err != nil {\n\t\treturn nil, nil, nil, nil, err\n\t} else if n != len(nonce) {\n\t\treturn nil, nil, nil, nil, fmt.Errorf(\"not enough nonce bytes read (%d != %d)\", n, len(nonce))\n\t}\n\n\tbek, err = genBlockEncryptionKey(sc, seed[:], nonce)\n\tif err != nil {\n\t\treturn nil, nil, nil, nil, err\n\t}\n\n\treturn aek, bek, seed, kek.Seal(nonce, nonce, seed, nil), nil\n}\n\n// Will generate the block encryption key.\nfunc genBlockEncryptionKey(sc StoreCipher, seed, nonce []byte) (cipher.Stream, error) {\n\tif sc == ChaCha {\n\t\treturn chacha20.NewUnauthenticatedCipher(seed, nonce)\n\t} else if sc == AES {\n\t\tblock, err := aes.NewCipher(seed)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn cipher.NewCTR(block, nonce), nil\n\t}\n\treturn nil, errUnknownCipher\n}\n\n// Lock should be held.\nfunc (fs *fileStore) recoverAEK() error {\n\tif fs.prf != nil && fs.aek == nil {\n\t\tekey, err := os.ReadFile(filepath.Join(fs.fcfg.StoreDir, JetStreamMetaFileKey))\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\trb, err := fs.prf([]byte(fs.cfg.Name))\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tkek, err := genEncryptionKey(fs.fcfg.Cipher, rb)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tns := kek.NonceSize()\n\t\tseed, err := kek.Open(nil, ekey[:ns], ekey[ns:], nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\taek, err := genEncryptionKey(fs.fcfg.Cipher, seed)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tfs.aek = aek\n\t}\n\treturn nil\n}\n\n// Lock should be held.\nfunc (fs *fileStore) setupAEK() error {\n\tif fs.prf != nil && fs.aek == nil {\n\t\tkey, _, _, encrypted, err := fs.genEncryptionKeys(fs.cfg.Name)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tkeyFile := filepath.Join(fs.fcfg.StoreDir, JetStreamMetaFileKey)\n\t\tif _, err := os.Stat(keyFile); err != nil && !os.IsNotExist(err) {\n\t\t\treturn err\n\t\t}\n\t\terr = fs.writeFileWithOptionalSync(keyFile, encrypted, defaultFilePerms)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t// Set our aek.\n\t\tfs.aek = key\n\t}\n\treturn nil\n}\n\n// Write out meta and the checksum.\n// Lock should be held.\nfunc (fs *fileStore) writeStreamMeta() error {\n\tif err := fs.setupAEK(); err != nil {\n\t\treturn err\n\t}\n\n\tmeta := filepath.Join(fs.fcfg.StoreDir, JetStreamMetaFile)\n\tif _, err := os.Stat(meta); err != nil && !os.IsNotExist(err) {\n\t\treturn err\n\t}\n\tb, err := json.Marshal(fs.cfg)\n\tif err != nil {\n\t\treturn err\n\t}\n\t// Encrypt if needed.\n\tif fs.aek != nil {\n\t\tnonce := make([]byte, fs.aek.NonceSize(), fs.aek.NonceSize()+len(b)+fs.aek.Overhead())\n\t\tif n, err := rand.Read(nonce); err != nil {\n\t\t\treturn err\n\t\t} else if n != len(nonce) {\n\t\t\treturn fmt.Errorf(\"not enough nonce bytes read (%d != %d)\", n, len(nonce))\n\t\t}\n\t\tb = fs.aek.Seal(nonce, nonce, b, nil)\n\t}\n\n\terr = fs.writeFileWithOptionalSync(meta, b, defaultFilePerms)\n\tif err != nil {\n\t\treturn err\n\t}\n\tfs.hh.Reset()\n\tfs.hh.Write(b)\n\tchecksum := hex.EncodeToString(fs.hh.Sum(nil))\n\tsum := filepath.Join(fs.fcfg.StoreDir, JetStreamMetaFileSum)\n\terr = fs.writeFileWithOptionalSync(sum, []byte(checksum), defaultFilePerms)\n\tif err != nil {\n\t\treturn err\n\t}\n\treturn nil\n}\n\n// Pools to recycle the blocks to help with memory pressure.\nvar blkPoolBig sync.Pool    // 16MB\nvar blkPoolMedium sync.Pool // 8MB\nvar blkPoolSmall sync.Pool  // 2MB\n\n// Get a new msg block based on sz estimate.\nfunc getMsgBlockBuf(sz int) (buf []byte) {\n\tvar pb any\n\tif sz <= defaultSmallBlockSize {\n\t\tpb = blkPoolSmall.Get()\n\t} else if sz <= defaultMediumBlockSize {\n\t\tpb = blkPoolMedium.Get()\n\t} else {\n\t\tpb = blkPoolBig.Get()\n\t}\n\tif pb != nil {\n\t\tbuf = *(pb.(*[]byte))\n\t} else {\n\t\t// Here we need to make a new blk.\n\t\t// If small leave as is..\n\t\tif sz > defaultSmallBlockSize && sz <= defaultMediumBlockSize {\n\t\t\tsz = defaultMediumBlockSize\n\t\t} else if sz > defaultMediumBlockSize {\n\t\t\tsz = defaultLargeBlockSize\n\t\t}\n\t\tbuf = make([]byte, sz)\n\t}\n\treturn buf[:0]\n}\n\n// Recycle the msg block.\nfunc recycleMsgBlockBuf(buf []byte) {\n\tif buf == nil || cap(buf) < defaultSmallBlockSize {\n\t\treturn\n\t}\n\t// Make sure to reset before placing back into pool.\n\tbuf = buf[:0]\n\n\t// We need to make sure the load code gets a block that can fit the maximum for a size block.\n\t// E.g. 8, 16 etc. otherwise we thrash and actually make things worse by pulling it out, and putting\n\t// it right back in and making a new []byte.\n\t// From above we know its already >= defaultSmallBlockSize\n\tif sz := cap(buf); sz < defaultMediumBlockSize {\n\t\tblkPoolSmall.Put(&buf)\n\t} else if sz < defaultLargeBlockSize {\n\t\tblkPoolMedium.Put(&buf)\n\t} else {\n\t\tblkPoolBig.Put(&buf)\n\t}\n}\n\nconst (\n\tmsgHdrSize     = 22\n\tchecksumSize   = 8\n\temptyRecordLen = msgHdrSize + checksumSize\n)\n\n// Lock should be held.\nfunc (fs *fileStore) noTrackSubjects() bool {\n\treturn !(fs.psim.Size() > 0 || len(fs.cfg.Subjects) > 0 || fs.cfg.Mirror != nil || len(fs.cfg.Sources) > 0)\n}\n\n// Will init the basics for a message block.\nfunc (fs *fileStore) initMsgBlock(index uint32) *msgBlock {\n\tmb := &msgBlock{\n\t\tfs:         fs,\n\t\tindex:      index,\n\t\tcexp:       fs.fcfg.CacheExpire,\n\t\tfexp:       fs.fcfg.SubjectStateExpire,\n\t\tnoTrack:    fs.noTrackSubjects(),\n\t\tsyncAlways: fs.fcfg.SyncAlways,\n\t}\n\n\tmdir := filepath.Join(fs.fcfg.StoreDir, msgDir)\n\tmb.mfn = filepath.Join(mdir, fmt.Sprintf(blkScan, index))\n\n\tif mb.hh == nil {\n\t\tkey := sha256.Sum256(fs.hashKeyForBlock(index))\n\t\tmb.hh, _ = highwayhash.New64(key[:])\n\t}\n\treturn mb\n}\n\n// Lock for fs should be held.\nfunc (fs *fileStore) loadEncryptionForMsgBlock(mb *msgBlock) error {\n\tif fs.prf == nil {\n\t\treturn nil\n\t}\n\n\tvar createdKeys bool\n\tmdir := filepath.Join(fs.fcfg.StoreDir, msgDir)\n\tekey, err := os.ReadFile(filepath.Join(mdir, fmt.Sprintf(keyScan, mb.index)))\n\tif err != nil {\n\t\t// We do not seem to have keys even though we should. Could be a plaintext conversion.\n\t\t// Create the keys and we will double check below.\n\t\tif err := fs.genEncryptionKeysForBlock(mb); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tcreatedKeys = true\n\t} else {\n\t\tif len(ekey) < minBlkKeySize {\n\t\t\treturn errBadKeySize\n\t\t}\n\t\t// Recover key encryption key.\n\t\trb, err := fs.prf([]byte(fmt.Sprintf(\"%s:%d\", fs.cfg.Name, mb.index)))\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tsc := fs.fcfg.Cipher\n\t\tkek, err := genEncryptionKey(sc, rb)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tns := kek.NonceSize()\n\t\tseed, err := kek.Open(nil, ekey[:ns], ekey[ns:], nil)\n\t\tif err != nil {\n\t\t\t// We may be here on a cipher conversion, so attempt to convert.\n\t\t\tif err = mb.convertCipher(); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t} else {\n\t\t\tmb.seed, mb.nonce = seed, ekey[:ns]\n\t\t}\n\t\tmb.aek, err = genEncryptionKey(sc, mb.seed)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif mb.bek, err = genBlockEncryptionKey(sc, mb.seed, mb.nonce); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\t// If we created keys here, let's check the data and if it is plaintext convert here.\n\tif createdKeys {\n\t\tif err := mb.convertToEncrypted(); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// Load a last checksum if needed from the block file.\n// Lock should be held.\nfunc (mb *msgBlock) ensureLastChecksumLoaded() {\n\tvar empty [8]byte\n\tif mb.lchk != empty {\n\t\treturn\n\t}\n\tcopy(mb.lchk[0:], mb.lastChecksum())\n}\n\n// Lock held on entry\nfunc (fs *fileStore) recoverMsgBlock(index uint32) (*msgBlock, error) {\n\tmb := fs.initMsgBlock(index)\n\t// Open up the message file, but we will try to recover from the index file.\n\t// We will check that the last checksums match.\n\tfile, err := mb.openBlock()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer file.Close()\n\n\tif fi, err := file.Stat(); fi != nil {\n\t\tmb.rbytes = uint64(fi.Size())\n\t} else {\n\t\treturn nil, err\n\t}\n\n\t// Make sure encryption loaded if needed.\n\tfs.loadEncryptionForMsgBlock(mb)\n\n\t// Grab last checksum from main block file.\n\tvar lchk [8]byte\n\tif mb.rbytes >= checksumSize {\n\t\tif mb.bek != nil {\n\t\t\tif buf, _ := mb.loadBlock(nil); len(buf) >= checksumSize {\n\t\t\t\tmb.bek.XORKeyStream(buf, buf)\n\t\t\t\tcopy(lchk[0:], buf[len(buf)-checksumSize:])\n\t\t\t}\n\t\t} else {\n\t\t\tfile.ReadAt(lchk[:], int64(mb.rbytes)-checksumSize)\n\t\t}\n\t}\n\n\tfile.Close()\n\n\t// Read our index file. Use this as source of truth if possible.\n\t// This not applicable in >= 2.10 servers. Here for upgrade paths from < 2.10.\n\tif err := mb.readIndexInfo(); err == nil {\n\t\t// Quick sanity check here.\n\t\t// Note this only checks that the message blk file is not newer then this file, or is empty and we expect empty.\n\t\tif (mb.rbytes == 0 && mb.msgs == 0) || bytes.Equal(lchk[:], mb.lchk[:]) {\n\t\t\tif mb.msgs > 0 && !mb.noTrack && fs.psim != nil {\n\t\t\t\tfs.populateGlobalPerSubjectInfo(mb)\n\t\t\t\t// Try to dump any state we needed on recovery.\n\t\t\t\tmb.tryForceExpireCacheLocked()\n\t\t\t}\n\t\t\tfs.addMsgBlock(mb)\n\t\t\treturn mb, nil\n\t\t}\n\t}\n\n\t// If we get data loss rebuilding the message block state record that with the fs itself.\n\tld, tombs, _ := mb.rebuildState()\n\tif ld != nil {\n\t\tfs.addLostData(ld)\n\t}\n\t// Collect all tombstones.\n\tif len(tombs) > 0 {\n\t\tfs.tombs = append(fs.tombs, tombs...)\n\t}\n\n\tif mb.msgs > 0 && !mb.noTrack && fs.psim != nil {\n\t\tfs.populateGlobalPerSubjectInfo(mb)\n\t\t// Try to dump any state we needed on recovery.\n\t\tmb.tryForceExpireCacheLocked()\n\t}\n\n\tmb.closeFDs()\n\tfs.addMsgBlock(mb)\n\n\treturn mb, nil\n}\n\nfunc (fs *fileStore) lostData() *LostStreamData {\n\tfs.mu.RLock()\n\tdefer fs.mu.RUnlock()\n\tif fs.ld == nil {\n\t\treturn nil\n\t}\n\tnld := *fs.ld\n\treturn &nld\n}\n\n// Lock should be held.\nfunc (fs *fileStore) addLostData(ld *LostStreamData) {\n\tif ld == nil {\n\t\treturn\n\t}\n\tif fs.ld != nil {\n\t\tvar added bool\n\t\tfor _, seq := range ld.Msgs {\n\t\t\tif _, found := fs.ld.exists(seq); !found {\n\t\t\t\tfs.ld.Msgs = append(fs.ld.Msgs, seq)\n\t\t\t\tadded = true\n\t\t\t}\n\t\t}\n\t\tif added {\n\t\t\tmsgs := fs.ld.Msgs\n\t\t\tslices.Sort(msgs)\n\t\t\tfs.ld.Bytes += ld.Bytes\n\t\t}\n\t} else {\n\t\tfs.ld = ld\n\t}\n}\n\n// Helper to see if we already have this sequence reported in our lost data.\nfunc (ld *LostStreamData) exists(seq uint64) (int, bool) {\n\ti := slices.IndexFunc(ld.Msgs, func(i uint64) bool {\n\t\treturn i == seq\n\t})\n\treturn i, i > -1\n}\n\nfunc (fs *fileStore) removeFromLostData(seq uint64) {\n\tif fs.ld == nil {\n\t\treturn\n\t}\n\tif i, found := fs.ld.exists(seq); found {\n\t\tfs.ld.Msgs = append(fs.ld.Msgs[:i], fs.ld.Msgs[i+1:]...)\n\t\tif len(fs.ld.Msgs) == 0 {\n\t\t\tfs.ld = nil\n\t\t}\n\t}\n}\n\nfunc (fs *fileStore) rebuildState(ld *LostStreamData) {\n\tfs.mu.Lock()\n\tdefer fs.mu.Unlock()\n\tfs.rebuildStateLocked(ld)\n}\n\n// Lock should be held.\nfunc (fs *fileStore) rebuildStateLocked(ld *LostStreamData) {\n\tfs.addLostData(ld)\n\n\tfs.state.Msgs, fs.state.Bytes = 0, 0\n\tfs.state.FirstSeq, fs.state.LastSeq = 0, 0\n\n\tfor _, mb := range fs.blks {\n\t\tmb.mu.RLock()\n\t\tfs.state.Msgs += mb.msgs\n\t\tfs.state.Bytes += mb.bytes\n\t\tfseq := atomic.LoadUint64(&mb.first.seq)\n\t\tif fs.state.FirstSeq == 0 || (fseq < fs.state.FirstSeq && mb.first.ts != 0) {\n\t\t\tfs.state.FirstSeq = fseq\n\t\t\tif mb.first.ts == 0 {\n\t\t\t\tfs.state.FirstTime = time.Time{}\n\t\t\t} else {\n\t\t\t\tfs.state.FirstTime = time.Unix(0, mb.first.ts).UTC()\n\t\t\t}\n\t\t}\n\t\tif lseq := atomic.LoadUint64(&mb.last.seq); lseq > fs.state.LastSeq {\n\t\t\tfs.state.LastSeq = lseq\n\t\t\tif mb.last.ts == 0 {\n\t\t\t\tfs.state.LastTime = time.Time{}\n\t\t\t} else {\n\t\t\t\tfs.state.LastTime = time.Unix(0, mb.last.ts).UTC()\n\t\t\t}\n\t\t}\n\t\tmb.mu.RUnlock()\n\t}\n}\n\n// Attempt to convert the cipher used for this message block.\nfunc (mb *msgBlock) convertCipher() error {\n\tfs := mb.fs\n\tsc := fs.fcfg.Cipher\n\n\tvar osc StoreCipher\n\tswitch sc {\n\tcase ChaCha:\n\t\tosc = AES\n\tcase AES:\n\t\tosc = ChaCha\n\t}\n\n\tmdir := filepath.Join(fs.fcfg.StoreDir, msgDir)\n\tekey, err := os.ReadFile(filepath.Join(mdir, fmt.Sprintf(keyScan, mb.index)))\n\tif err != nil {\n\t\treturn err\n\t}\n\tif len(ekey) < minBlkKeySize {\n\t\treturn errBadKeySize\n\t}\n\ttype prfWithCipher struct {\n\t\tkeyGen\n\t\tStoreCipher\n\t}\n\tvar prfs []prfWithCipher\n\tif fs.prf != nil {\n\t\tprfs = append(prfs, prfWithCipher{fs.prf, sc})\n\t\tprfs = append(prfs, prfWithCipher{fs.prf, osc})\n\t}\n\tif fs.oldprf != nil {\n\t\tprfs = append(prfs, prfWithCipher{fs.oldprf, sc})\n\t\tprfs = append(prfs, prfWithCipher{fs.oldprf, osc})\n\t}\n\n\tfor _, prf := range prfs {\n\t\t// Recover key encryption key.\n\t\trb, err := prf.keyGen([]byte(fmt.Sprintf(\"%s:%d\", fs.cfg.Name, mb.index)))\n\t\tif err != nil {\n\t\t\tcontinue\n\t\t}\n\t\tkek, err := genEncryptionKey(prf.StoreCipher, rb)\n\t\tif err != nil {\n\t\t\tcontinue\n\t\t}\n\t\tns := kek.NonceSize()\n\t\tseed, err := kek.Open(nil, ekey[:ns], ekey[ns:], nil)\n\t\tif err != nil {\n\t\t\tcontinue\n\t\t}\n\t\tnonce := ekey[:ns]\n\t\tbek, err := genBlockEncryptionKey(prf.StoreCipher, seed, nonce)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tbuf, _ := mb.loadBlock(nil)\n\t\tbek.XORKeyStream(buf, buf)\n\t\t// Make sure we can parse with old cipher and key file.\n\t\tif err = mb.indexCacheBuf(buf); err != nil {\n\t\t\treturn err\n\t\t}\n\t\t// Reset the cache since we just read everything in.\n\t\tmb.cache = nil\n\n\t\t// Generate new keys. If we error for some reason then we will put\n\t\t// the old keyfile back.\n\t\tif err := fs.genEncryptionKeysForBlock(mb); err != nil {\n\t\t\tkeyFile := filepath.Join(mdir, fmt.Sprintf(keyScan, mb.index))\n\t\t\tfs.writeFileWithOptionalSync(keyFile, ekey, defaultFilePerms)\n\t\t\treturn err\n\t\t}\n\t\tmb.bek.XORKeyStream(buf, buf)\n\t\t<-dios\n\t\terr = os.WriteFile(mb.mfn, buf, defaultFilePerms)\n\t\tdios <- struct{}{}\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t}\n\treturn fmt.Errorf(\"unable to recover keys\")\n}\n\n// Convert a plaintext block to encrypted.\nfunc (mb *msgBlock) convertToEncrypted() error {\n\tif mb.bek == nil {\n\t\treturn nil\n\t}\n\tbuf, err := mb.loadBlock(nil)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif err := mb.indexCacheBuf(buf); err != nil {\n\t\t// This likely indicates this was already encrypted or corrupt.\n\t\tmb.cache = nil\n\t\treturn err\n\t}\n\t// Undo cache from above for later.\n\tmb.cache = nil\n\tmb.bek.XORKeyStream(buf, buf)\n\t<-dios\n\terr = os.WriteFile(mb.mfn, buf, defaultFilePerms)\n\tdios <- struct{}{}\n\tif err != nil {\n\t\treturn err\n\t}\n\treturn nil\n}\n\n// Return the mb's index.\nfunc (mb *msgBlock) getIndex() uint32 {\n\tmb.mu.RLock()\n\tdefer mb.mu.RUnlock()\n\treturn mb.index\n}\n\n// Rebuild the state of the blk based on what we have on disk in the N.blk file.\n// We will return any lost data, and we will return any delete tombstones we encountered.\nfunc (mb *msgBlock) rebuildState() (*LostStreamData, []uint64, error) {\n\tmb.mu.Lock()\n\tdefer mb.mu.Unlock()\n\treturn mb.rebuildStateLocked()\n}\n\n// Rebuild the state of the blk based on what we have on disk in the N.blk file.\n// Lock should be held.\nfunc (mb *msgBlock) rebuildStateLocked() (*LostStreamData, []uint64, error) {\n\tstartLastSeq := atomic.LoadUint64(&mb.last.seq)\n\n\t// Remove the .fss file and clear any cache we have set.\n\tmb.clearCacheAndOffset()\n\n\tbuf, err := mb.loadBlock(nil)\n\tdefer recycleMsgBlockBuf(buf)\n\n\tif err != nil || len(buf) == 0 {\n\t\tvar ld *LostStreamData\n\t\t// No data to rebuild from here.\n\t\tif mb.msgs > 0 {\n\t\t\t// We need to declare lost data here.\n\t\t\tld = &LostStreamData{Msgs: make([]uint64, 0, mb.msgs), Bytes: mb.bytes}\n\t\t\tfirstSeq, lastSeq := atomic.LoadUint64(&mb.first.seq), atomic.LoadUint64(&mb.last.seq)\n\t\t\tfor seq := firstSeq; seq <= lastSeq; seq++ {\n\t\t\t\tif !mb.dmap.Exists(seq) {\n\t\t\t\t\tld.Msgs = append(ld.Msgs, seq)\n\t\t\t\t}\n\t\t\t}\n\t\t\t// Clear invalid state. We will let this blk be added in here.\n\t\t\tmb.msgs, mb.bytes, mb.rbytes, mb.fss = 0, 0, 0, nil\n\t\t\tmb.dmap.Empty()\n\t\t\tatomic.StoreUint64(&mb.first.seq, atomic.LoadUint64(&mb.last.seq)+1)\n\t\t}\n\t\treturn ld, nil, err\n\t}\n\n\t// Clear state we need to rebuild.\n\tmb.msgs, mb.bytes, mb.rbytes, mb.fss = 0, 0, 0, nil\n\tatomic.StoreUint64(&mb.last.seq, 0)\n\tmb.last.ts = 0\n\tfirstNeedsSet := true\n\n\t// Check if we need to decrypt.\n\tif mb.bek != nil && len(buf) > 0 {\n\t\t// Recreate to reset counter.\n\t\tmb.bek, err = genBlockEncryptionKey(mb.fs.fcfg.Cipher, mb.seed, mb.nonce)\n\t\tif err != nil {\n\t\t\treturn nil, nil, err\n\t\t}\n\t\tmb.bek.XORKeyStream(buf, buf)\n\t}\n\n\t// Check for compression.\n\tif buf, err = mb.decompressIfNeeded(buf); err != nil {\n\t\treturn nil, nil, err\n\t}\n\n\tmb.rbytes = uint64(len(buf))\n\n\taddToDmap := func(seq uint64) {\n\t\tif seq == 0 {\n\t\t\treturn\n\t\t}\n\t\tmb.dmap.Insert(seq)\n\t}\n\n\tvar le = binary.LittleEndian\n\n\ttruncate := func(index uint32) {\n\t\tvar fd *os.File\n\t\tif mb.mfd != nil {\n\t\t\tfd = mb.mfd\n\t\t} else {\n\t\t\t<-dios\n\t\t\tfd, err = os.OpenFile(mb.mfn, os.O_RDWR, defaultFilePerms)\n\t\t\tdios <- struct{}{}\n\t\t\tif err == nil {\n\t\t\t\tdefer fd.Close()\n\t\t\t}\n\t\t}\n\t\tif fd == nil {\n\t\t\treturn\n\t\t}\n\t\tif err := fd.Truncate(int64(index)); err == nil {\n\t\t\t// Update our checksum.\n\t\t\tif index >= 8 {\n\t\t\t\tvar lchk [8]byte\n\t\t\t\tfd.ReadAt(lchk[:], int64(index-8))\n\t\t\t\tcopy(mb.lchk[0:], lchk[:])\n\t\t\t}\n\t\t\tfd.Sync()\n\t\t}\n\t}\n\n\tgatherLost := func(lb uint32) *LostStreamData {\n\t\tvar ld LostStreamData\n\t\tfor seq := atomic.LoadUint64(&mb.last.seq) + 1; seq <= startLastSeq; seq++ {\n\t\t\tld.Msgs = append(ld.Msgs, seq)\n\t\t}\n\t\tld.Bytes = uint64(lb)\n\t\treturn &ld\n\t}\n\n\t// For tombstones that we find and collect.\n\tvar (\n\t\ttombstones      []uint64\n\t\tminTombstoneSeq uint64\n\t\tminTombstoneTs  int64\n\t)\n\n\t// To detect gaps from compaction.\n\tvar last uint64\n\n\tfor index, lbuf := uint32(0), uint32(len(buf)); index < lbuf; {\n\t\tif index+msgHdrSize > lbuf {\n\t\t\ttruncate(index)\n\t\t\treturn gatherLost(lbuf - index), tombstones, nil\n\t\t}\n\n\t\thdr := buf[index : index+msgHdrSize]\n\t\trl, slen := le.Uint32(hdr[0:]), int(le.Uint16(hdr[20:]))\n\n\t\thasHeaders := rl&hbit != 0\n\t\tvar ttl int64\n\t\tif mb.fs.ttls != nil && len(hdr) > 0 {\n\t\t\tttl, _ = getMessageTTL(hdr)\n\t\t}\n\t\t// Clear any headers bit that could be set.\n\t\trl &^= hbit\n\t\tdlen := int(rl) - msgHdrSize\n\t\t// Do some quick sanity checks here.\n\t\tif dlen < 0 || slen > (dlen-recordHashSize) || dlen > int(rl) || index+rl > lbuf || rl > rlBadThresh {\n\t\t\ttruncate(index)\n\t\t\treturn gatherLost(lbuf - index), tombstones, errBadMsg\n\t\t}\n\n\t\t// Check for checksum failures before additional processing.\n\t\tdata := buf[index+msgHdrSize : index+rl]\n\t\tif hh := mb.hh; hh != nil {\n\t\t\thh.Reset()\n\t\t\thh.Write(hdr[4:20])\n\t\t\thh.Write(data[:slen])\n\t\t\tif hasHeaders {\n\t\t\t\thh.Write(data[slen+4 : dlen-recordHashSize])\n\t\t\t} else {\n\t\t\t\thh.Write(data[slen : dlen-recordHashSize])\n\t\t\t}\n\t\t\tchecksum := hh.Sum(nil)\n\t\t\tif !bytes.Equal(checksum, data[len(data)-recordHashSize:]) {\n\t\t\t\ttruncate(index)\n\t\t\t\treturn gatherLost(lbuf - index), tombstones, errBadMsg\n\t\t\t}\n\t\t\tcopy(mb.lchk[0:], checksum)\n\t\t}\n\n\t\t// Grab our sequence and timestamp.\n\t\tseq := le.Uint64(hdr[4:])\n\t\tts := int64(le.Uint64(hdr[12:]))\n\n\t\t// Check if this is a delete tombstone.\n\t\tif seq&tbit != 0 {\n\t\t\tseq = seq &^ tbit\n\t\t\t// Need to process this here and make sure we have accounted for this properly.\n\t\t\ttombstones = append(tombstones, seq)\n\t\t\tif minTombstoneSeq == 0 || seq < minTombstoneSeq {\n\t\t\t\tminTombstoneSeq, minTombstoneTs = seq, ts\n\t\t\t}\n\t\t\tindex += rl\n\t\t\tcontinue\n\t\t}\n\n\t\tfseq := atomic.LoadUint64(&mb.first.seq)\n\t\t// This is an old erased message, or a new one that we can track.\n\t\tif seq == 0 || seq&ebit != 0 || seq < fseq {\n\t\t\tseq = seq &^ ebit\n\t\t\tif seq >= fseq {\n\t\t\t\tatomic.StoreUint64(&mb.last.seq, seq)\n\t\t\t\tmb.last.ts = ts\n\t\t\t\tif mb.msgs == 0 {\n\t\t\t\t\tatomic.StoreUint64(&mb.first.seq, seq+1)\n\t\t\t\t\tmb.first.ts = 0\n\t\t\t\t} else if seq != 0 {\n\t\t\t\t\t// Only add to dmap if past recorded first seq and non-zero.\n\t\t\t\t\taddToDmap(seq)\n\t\t\t\t}\n\t\t\t}\n\t\t\tindex += rl\n\t\t\tcontinue\n\t\t}\n\n\t\t// This is for when we have index info that adjusts for deleted messages\n\t\t// at the head. So the first.seq will be already set here. If this is larger\n\t\t// replace what we have with this seq.\n\t\tif firstNeedsSet && seq >= fseq {\n\t\t\tatomic.StoreUint64(&mb.first.seq, seq)\n\t\t\tfirstNeedsSet, mb.first.ts = false, ts\n\t\t}\n\n\t\tif !mb.dmap.Exists(seq) {\n\t\t\tmb.msgs++\n\t\t\tmb.bytes += uint64(rl)\n\t\t\tif mb.fs.ttls != nil && ttl > 0 {\n\t\t\t\texpires := time.Duration(ts) + (time.Second * time.Duration(ttl))\n\t\t\t\tmb.fs.ttls.Add(seq, int64(expires))\n\t\t\t\tmb.ttls++\n\t\t\t}\n\t\t}\n\n\t\t// Check for any gaps from compaction, meaning no ebit entry.\n\t\tif last > 0 && seq != last+1 {\n\t\t\tfor dseq := last + 1; dseq < seq; dseq++ {\n\t\t\t\taddToDmap(dseq)\n\t\t\t}\n\t\t}\n\n\t\t// Always set last\n\t\tlast = seq\n\t\tatomic.StoreUint64(&mb.last.seq, last)\n\t\tmb.last.ts = ts\n\n\t\t// Advance to next record.\n\t\tindex += rl\n\t}\n\n\t// For empty msg blocks make sure we recover last seq correctly based off of first.\n\t// Or if we seem to have no messages but had a tombstone, which we use to remember\n\t// sequences and timestamps now, use that to properly setup the first and last.\n\tif mb.msgs == 0 {\n\t\tfseq := atomic.LoadUint64(&mb.first.seq)\n\t\tif fseq > 0 {\n\t\t\tatomic.StoreUint64(&mb.last.seq, fseq-1)\n\t\t} else if fseq == 0 && minTombstoneSeq > 0 {\n\t\t\tatomic.StoreUint64(&mb.first.seq, minTombstoneSeq+1)\n\t\t\tmb.first.ts = 0\n\t\t\tif mb.last.seq == 0 {\n\t\t\t\tatomic.StoreUint64(&mb.last.seq, minTombstoneSeq)\n\t\t\t\tmb.last.ts = minTombstoneTs\n\t\t\t}\n\t\t}\n\t}\n\n\treturn nil, tombstones, nil\n}\n\n// For doing warn logging.\n// Lock should be held.\nfunc (fs *fileStore) warn(format string, args ...any) {\n\t// No-op if no server configured.\n\tif fs.srv == nil {\n\t\treturn\n\t}\n\tfs.srv.Warnf(fmt.Sprintf(\"Filestore [%s] %s\", fs.cfg.Name, format), args...)\n}\n\n// For doing debug logging.\n// Lock should be held.\nfunc (fs *fileStore) debug(format string, args ...any) {\n\t// No-op if no server configured.\n\tif fs.srv == nil {\n\t\treturn\n\t}\n\tfs.srv.Debugf(fmt.Sprintf(\"Filestore [%s] %s\", fs.cfg.Name, format), args...)\n}\n\n// Track local state but ignore timestamps here.\nfunc updateTrackingState(state *StreamState, mb *msgBlock) {\n\tif state.FirstSeq == 0 {\n\t\tstate.FirstSeq = mb.first.seq\n\t} else if mb.first.seq < state.FirstSeq && mb.first.ts != 0 {\n\t\tstate.FirstSeq = mb.first.seq\n\t}\n\tif mb.last.seq > state.LastSeq {\n\t\tstate.LastSeq = mb.last.seq\n\t}\n\tstate.Msgs += mb.msgs\n\tstate.Bytes += mb.bytes\n}\n\n// Determine if our tracking states are the same.\nfunc trackingStatesEqual(fs, mb *StreamState) bool {\n\t// When a fs is brand new the fs state will have first seq of 0, but tracking mb may have 1.\n\t// If either has a first sequence that is not 0 or 1 we will check if they are the same, otherwise skip.\n\tif (fs.FirstSeq > 1 && mb.FirstSeq > 1) || mb.FirstSeq > 1 {\n\t\treturn fs.Msgs == mb.Msgs && fs.FirstSeq == mb.FirstSeq && fs.LastSeq == mb.LastSeq && fs.Bytes == mb.Bytes\n\t}\n\treturn fs.Msgs == mb.Msgs && fs.LastSeq == mb.LastSeq && fs.Bytes == mb.Bytes\n}\n\n// recoverFullState will attempt to receover our last full state and re-process any state changes\n// that happened afterwards.\nfunc (fs *fileStore) recoverFullState() (rerr error) {\n\tfs.mu.Lock()\n\tdefer fs.mu.Unlock()\n\n\t// Check for any left over purged messages.\n\t<-dios\n\tpdir := filepath.Join(fs.fcfg.StoreDir, purgeDir)\n\tif _, err := os.Stat(pdir); err == nil {\n\t\tos.RemoveAll(pdir)\n\t}\n\t// Grab our stream state file and load it in.\n\tfn := filepath.Join(fs.fcfg.StoreDir, msgDir, streamStreamStateFile)\n\tbuf, err := os.ReadFile(fn)\n\tdios <- struct{}{}\n\n\tif err != nil {\n\t\tif !os.IsNotExist(err) {\n\t\t\tfs.warn(\"Could not read stream state file: %v\", err)\n\t\t}\n\t\treturn err\n\t}\n\n\tconst minLen = 32\n\tif len(buf) < minLen {\n\t\tos.Remove(fn)\n\t\tfs.warn(\"Stream state too short (%d bytes)\", len(buf))\n\t\treturn errCorruptState\n\t}\n\n\t// The highwayhash will be on the end. Check that it still matches.\n\th := buf[len(buf)-highwayhash.Size64:]\n\tbuf = buf[:len(buf)-highwayhash.Size64]\n\tfs.hh.Reset()\n\tfs.hh.Write(buf)\n\tif !bytes.Equal(h, fs.hh.Sum(nil)) {\n\t\tos.Remove(fn)\n\t\tfs.warn(\"Stream state checksum did not match\")\n\t\treturn errCorruptState\n\t}\n\n\t// Decrypt if needed.\n\tif fs.prf != nil {\n\t\t// We can be setup for encryption but if this is a snapshot restore we will be missing the keyfile\n\t\t// since snapshots strip encryption.\n\t\tif err := fs.recoverAEK(); err == nil {\n\t\t\tns := fs.aek.NonceSize()\n\t\t\tbuf, err = fs.aek.Open(nil, buf[:ns], buf[ns:], nil)\n\t\t\tif err != nil {\n\t\t\t\tfs.warn(\"Stream state error reading encryption key: %v\", err)\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t}\n\n\tversion := buf[1]\n\tif buf[0] != fullStateMagic || version < fullStateMinVersion || version > fullStateVersion {\n\t\tos.Remove(fn)\n\t\tfs.warn(\"Stream state magic and version mismatch\")\n\t\treturn errCorruptState\n\t}\n\n\tbi := hdrLen\n\n\treadU64 := func() uint64 {\n\t\tif bi < 0 {\n\t\t\treturn 0\n\t\t}\n\t\tv, n := binary.Uvarint(buf[bi:])\n\t\tif n <= 0 {\n\t\t\tbi = -1\n\t\t\treturn 0\n\t\t}\n\t\tbi += n\n\t\treturn v\n\t}\n\treadI64 := func() int64 {\n\t\tif bi < 0 {\n\t\t\treturn 0\n\t\t}\n\t\tv, n := binary.Varint(buf[bi:])\n\t\tif n <= 0 {\n\t\t\tbi = -1\n\t\t\treturn -1\n\t\t}\n\t\tbi += n\n\t\treturn v\n\t}\n\n\tsetTime := func(t *time.Time, ts int64) {\n\t\tif ts == 0 {\n\t\t\t*t = time.Time{}\n\t\t} else {\n\t\t\t*t = time.Unix(0, ts).UTC()\n\t\t}\n\t}\n\n\tvar state StreamState\n\tstate.Msgs = readU64()\n\tstate.Bytes = readU64()\n\tstate.FirstSeq = readU64()\n\tbaseTime := readI64()\n\tsetTime(&state.FirstTime, baseTime)\n\tstate.LastSeq = readU64()\n\tsetTime(&state.LastTime, readI64())\n\n\t// Check for per subject info.\n\tif numSubjects := int(readU64()); numSubjects > 0 {\n\t\tfs.psim, fs.tsl = fs.psim.Empty(), 0\n\t\tfor i := 0; i < numSubjects; i++ {\n\t\t\tif lsubj := int(readU64()); lsubj > 0 {\n\t\t\t\tif bi+lsubj > len(buf) {\n\t\t\t\t\tos.Remove(fn)\n\t\t\t\t\tfs.warn(\"Stream state bad subject len (%d)\", lsubj)\n\t\t\t\t\treturn errCorruptState\n\t\t\t\t}\n\t\t\t\t// If we have lots of subjects this will alloc for each one.\n\t\t\t\t// We could reference the underlying buffer, but we could guess wrong if\n\t\t\t\t// number of blocks is large and subjects is low, since we would reference buf.\n\t\t\t\tsubj := buf[bi : bi+lsubj]\n\t\t\t\t// We had a bug that could cause memory corruption in the PSIM that could have gotten stored to disk.\n\t\t\t\t// Only would affect subjects, so do quick check.\n\t\t\t\tif !isValidSubject(bytesToString(subj), true) {\n\t\t\t\t\tos.Remove(fn)\n\t\t\t\t\tfs.warn(\"Stream state corrupt subject detected\")\n\t\t\t\t\treturn errCorruptState\n\t\t\t\t}\n\t\t\t\tbi += lsubj\n\t\t\t\tpsi := psi{total: readU64(), fblk: uint32(readU64())}\n\t\t\t\tif psi.total > 1 {\n\t\t\t\t\tpsi.lblk = uint32(readU64())\n\t\t\t\t} else {\n\t\t\t\t\tpsi.lblk = psi.fblk\n\t\t\t\t}\n\t\t\t\tfs.psim.Insert(subj, psi)\n\t\t\t\tfs.tsl += lsubj\n\t\t\t}\n\t\t}\n\t}\n\n\t// Track the state as represented by the blocks themselves.\n\tvar mstate StreamState\n\n\tif numBlocks := readU64(); numBlocks > 0 {\n\t\tlastIndex := int(numBlocks - 1)\n\t\tfs.blks = make([]*msgBlock, 0, numBlocks)\n\t\tfor i := 0; i < int(numBlocks); i++ {\n\t\t\tindex, nbytes, fseq, fts, lseq, lts, numDeleted := uint32(readU64()), readU64(), readU64(), readI64(), readU64(), readI64(), readU64()\n\t\t\tvar ttls uint64\n\t\t\tif version >= 2 {\n\t\t\t\tttls = readU64()\n\t\t\t}\n\t\t\tif bi < 0 {\n\t\t\t\tos.Remove(fn)\n\t\t\t\treturn errCorruptState\n\t\t\t}\n\t\t\tmb := fs.initMsgBlock(index)\n\t\t\tatomic.StoreUint64(&mb.first.seq, fseq)\n\t\t\tatomic.StoreUint64(&mb.last.seq, lseq)\n\t\t\tmb.msgs, mb.bytes = lseq-fseq+1, nbytes\n\t\t\tmb.first.ts, mb.last.ts = fts+baseTime, lts+baseTime\n\t\t\tmb.ttls = ttls\n\t\t\tif numDeleted > 0 {\n\t\t\t\tdmap, n, err := avl.Decode(buf[bi:])\n\t\t\t\tif err != nil {\n\t\t\t\t\tos.Remove(fn)\n\t\t\t\t\tfs.warn(\"Stream state error decoding avl dmap: %v\", err)\n\t\t\t\t\treturn errCorruptState\n\t\t\t\t}\n\t\t\t\tmb.dmap = *dmap\n\t\t\t\tif mb.msgs > numDeleted {\n\t\t\t\t\tmb.msgs -= numDeleted\n\t\t\t\t} else {\n\t\t\t\t\tmb.msgs = 0\n\t\t\t\t}\n\t\t\t\tbi += n\n\t\t\t}\n\t\t\t// Only add in if not empty or the lmb.\n\t\t\tif mb.msgs > 0 || i == lastIndex {\n\t\t\t\tfs.addMsgBlock(mb)\n\t\t\t\tupdateTrackingState(&mstate, mb)\n\t\t\t} else {\n\t\t\t\t// Mark dirty to cleanup.\n\t\t\t\tfs.dirty++\n\t\t\t}\n\t\t}\n\t}\n\n\t// Pull in last block index for the block that had last checksum when we wrote the full state.\n\tblkIndex := uint32(readU64())\n\tvar lchk [8]byte\n\tif bi+len(lchk) > len(buf) {\n\t\tbi = -1\n\t} else {\n\t\tcopy(lchk[0:], buf[bi:bi+len(lchk)])\n\t}\n\n\t// Check if we had any errors.\n\tif bi < 0 {\n\t\tos.Remove(fn)\n\t\tfs.warn(\"Stream state has no checksum present\")\n\t\treturn errCorruptState\n\t}\n\n\t// Move into place our state, msgBlks and subject info.\n\tfs.state = state\n\n\t// First let's check the happy path, open the blk file that was the lmb when we created the full state.\n\t// See if we have the last block available.\n\tvar matched bool\n\tmb := fs.lmb\n\tif mb == nil || mb.index != blkIndex {\n\t\tos.Remove(fn)\n\t\tfs.warn(\"Stream state block does not exist or index mismatch\")\n\t\treturn errCorruptState\n\t}\n\tif _, err := os.Stat(mb.mfn); err != nil && os.IsNotExist(err) {\n\t\t// If our saved state is past what we see on disk, fallback and rebuild.\n\t\tif ld, _, _ := mb.rebuildState(); ld != nil {\n\t\t\tfs.addLostData(ld)\n\t\t}\n\t\tfs.warn(\"Stream state detected prior state, could not locate msg block %d\", blkIndex)\n\t\treturn errPriorState\n\t}\n\tif matched = bytes.Equal(mb.lastChecksum(), lchk[:]); !matched {\n\t\t// Detected a stale index.db, we didn't write it upon shutdown so can't rely on it being correct.\n\t\tfs.warn(\"Stream state outdated, last block has additional entries, will rebuild\")\n\t\treturn errPriorState\n\t}\n\n\t// We need to see if any blocks exist after our last one even though we matched the last record exactly.\n\tmdir := filepath.Join(fs.fcfg.StoreDir, msgDir)\n\tvar dirs []os.DirEntry\n\n\t<-dios\n\tif f, err := os.Open(mdir); err == nil {\n\t\tdirs, _ = f.ReadDir(-1)\n\t\tf.Close()\n\t}\n\tdios <- struct{}{}\n\n\tvar index uint32\n\tfor _, fi := range dirs {\n\t\t// Ensure it's actually a block file, otherwise fmt.Sscanf also matches %d.blk.tmp\n\t\tif !strings.HasSuffix(fi.Name(), blkSuffix) {\n\t\t\tcontinue\n\t\t}\n\t\tif n, err := fmt.Sscanf(fi.Name(), blkScan, &index); err == nil && n == 1 {\n\t\t\tif index > blkIndex {\n\t\t\t\tfs.warn(\"Stream state outdated, found extra blocks, will rebuild\")\n\t\t\t\treturn errPriorState\n\t\t\t}\n\t\t}\n\t}\n\n\t// We check first and last seq and number of msgs and bytes. If there is a difference,\n\t// return and error so we rebuild from the message block state on disk.\n\tif !trackingStatesEqual(&fs.state, &mstate) {\n\t\tos.Remove(fn)\n\t\tfs.warn(\"Stream state encountered internal inconsistency on recover\")\n\t\treturn errCorruptState\n\t}\n\n\treturn nil\n}\n\nfunc (fs *fileStore) recoverTTLState() error {\n\t// See if we have a timed hash wheel for TTLs.\n\t<-dios\n\tfn := filepath.Join(fs.fcfg.StoreDir, msgDir, ttlStreamStateFile)\n\tbuf, err := os.ReadFile(fn)\n\tdios <- struct{}{}\n\n\tif err != nil && !os.IsNotExist(err) {\n\t\treturn err\n\t}\n\n\tfs.ttls = thw.NewHashWheel()\n\n\tvar ttlseq uint64\n\tif err == nil {\n\t\tttlseq, err = fs.ttls.Decode(buf)\n\t\tif err != nil {\n\t\t\tfs.warn(\"Error decoding TTL state: %s\", err)\n\t\t\tos.Remove(fn)\n\t\t}\n\t}\n\n\tif ttlseq < fs.state.FirstSeq {\n\t\tttlseq = fs.state.FirstSeq\n\t}\n\n\tdefer fs.resetAgeChk(0)\n\tif fs.state.Msgs > 0 && ttlseq <= fs.state.LastSeq {\n\t\tfs.warn(\"TTL state is outdated; attempting to recover using linear scan (seq %d to %d)\", ttlseq, fs.state.LastSeq)\n\t\tvar sm StoreMsg\n\t\tmb := fs.selectMsgBlock(ttlseq)\n\t\tif mb == nil {\n\t\t\treturn nil\n\t\t}\n\t\tmblseq := atomic.LoadUint64(&mb.last.seq)\n\t\tfor seq := ttlseq; seq <= fs.state.LastSeq; seq++ {\n\t\tretry:\n\t\t\tif mb.ttls == 0 {\n\t\t\t\t// None of the messages in the block have message TTLs so don't\n\t\t\t\t// bother doing anything further with this block, skip to the end.\n\t\t\t\tseq = atomic.LoadUint64(&mb.last.seq) + 1\n\t\t\t}\n\t\t\tif seq > mblseq {\n\t\t\t\t// We've reached the end of the loaded block, see if we can continue\n\t\t\t\t// by loading the next one.\n\t\t\t\tmb.tryForceExpireCache()\n\t\t\t\tif mb = fs.selectMsgBlock(seq); mb == nil {\n\t\t\t\t\t// TODO(nat): Deal with gaps properly. Right now this will be\n\t\t\t\t\t// probably expensive on CPU.\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tmblseq = atomic.LoadUint64(&mb.last.seq)\n\t\t\t\t// At this point we've loaded another block, so let's go back to the\n\t\t\t\t// beginning and see if we need to skip this one too.\n\t\t\t\tgoto retry\n\t\t\t}\n\t\t\tmsg, _, err := mb.fetchMsgNoCopy(seq, &sm)\n\t\t\tif err != nil {\n\t\t\t\tfs.warn(\"Error loading msg seq %d for recovering TTL: %s\", seq, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif len(msg.hdr) == 0 {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif ttl, _ := getMessageTTL(msg.hdr); ttl > 0 {\n\t\t\t\texpires := time.Duration(msg.ts) + (time.Second * time.Duration(ttl))\n\t\t\t\tfs.ttls.Add(seq, int64(expires))\n\t\t\t}\n\t\t}\n\t}\n\treturn nil\n}\n\n// Grabs last checksum for the named block file.\n// Takes into account encryption etc.\nfunc (mb *msgBlock) lastChecksum() []byte {\n\tf, err := mb.openBlock()\n\tif err != nil {\n\t\treturn nil\n\t}\n\tdefer f.Close()\n\n\tvar lchk [8]byte\n\tif fi, _ := f.Stat(); fi != nil {\n\t\tmb.rbytes = uint64(fi.Size())\n\t}\n\tif mb.rbytes < checksumSize {\n\t\treturn lchk[:]\n\t}\n\t// Encrypted?\n\t// Check for encryption, we do not load keys on startup anymore so might need to load them here.\n\tif mb.fs != nil && mb.fs.prf != nil && (mb.aek == nil || mb.bek == nil) {\n\t\tif err := mb.fs.loadEncryptionForMsgBlock(mb); err != nil {\n\t\t\treturn nil\n\t\t}\n\t}\n\tif mb.bek != nil {\n\t\tif buf, _ := mb.loadBlock(nil); len(buf) >= checksumSize {\n\t\t\tbek, err := genBlockEncryptionKey(mb.fs.fcfg.Cipher, mb.seed, mb.nonce)\n\t\t\tif err != nil {\n\t\t\t\treturn nil\n\t\t\t}\n\t\t\tmb.bek = bek\n\t\t\tmb.bek.XORKeyStream(buf, buf)\n\t\t\tcopy(lchk[0:], buf[len(buf)-checksumSize:])\n\t\t}\n\t} else {\n\t\tf.ReadAt(lchk[:], int64(mb.rbytes)-checksumSize)\n\t}\n\treturn lchk[:]\n}\n\n// This will make sure we clean up old idx and fss files.\nfunc (fs *fileStore) cleanupOldMeta() {\n\tfs.mu.RLock()\n\tmdir := filepath.Join(fs.fcfg.StoreDir, msgDir)\n\tfs.mu.RUnlock()\n\n\t<-dios\n\tf, err := os.Open(mdir)\n\tdios <- struct{}{}\n\tif err != nil {\n\t\treturn\n\t}\n\n\tdirs, _ := f.ReadDir(-1)\n\tf.Close()\n\n\tconst (\n\t\tminLen    = 4\n\t\tidxSuffix = \".idx\"\n\t\tfssSuffix = \".fss\"\n\t)\n\tfor _, fi := range dirs {\n\t\tif name := fi.Name(); strings.HasSuffix(name, idxSuffix) || strings.HasSuffix(name, fssSuffix) {\n\t\t\tos.Remove(filepath.Join(mdir, name))\n\t\t}\n\t}\n}\n\nfunc (fs *fileStore) recoverMsgs() error {\n\tfs.mu.Lock()\n\tdefer fs.mu.Unlock()\n\n\t// Check for any left over purged messages.\n\t<-dios\n\tpdir := filepath.Join(fs.fcfg.StoreDir, purgeDir)\n\tif _, err := os.Stat(pdir); err == nil {\n\t\tos.RemoveAll(pdir)\n\t}\n\tmdir := filepath.Join(fs.fcfg.StoreDir, msgDir)\n\tf, err := os.Open(mdir)\n\tif err != nil {\n\t\tdios <- struct{}{}\n\t\treturn errNotReadable\n\t}\n\tdirs, err := f.ReadDir(-1)\n\tf.Close()\n\tdios <- struct{}{}\n\n\tif err != nil {\n\t\treturn errNotReadable\n\t}\n\n\tindices := make(sort.IntSlice, 0, len(dirs))\n\tvar index int\n\tfor _, fi := range dirs {\n\t\t// Ensure it's actually a block file, otherwise fmt.Sscanf also matches %d.blk.tmp\n\t\tif !strings.HasSuffix(fi.Name(), blkSuffix) {\n\t\t\tcontinue\n\t\t}\n\t\tif n, err := fmt.Sscanf(fi.Name(), blkScan, &index); err == nil && n == 1 {\n\t\t\tindices = append(indices, index)\n\t\t}\n\t}\n\tindices.Sort()\n\n\t// Recover all of the msg blocks.\n\t// We now guarantee they are coming in order.\n\tfor _, index := range indices {\n\t\tif mb, err := fs.recoverMsgBlock(uint32(index)); err == nil && mb != nil {\n\t\t\t// This is a truncate block with possibly no index. If the OS got shutdown\n\t\t\t// out from underneath of us this is possible.\n\t\t\tif mb.first.seq == 0 {\n\t\t\t\tmb.dirtyCloseWithRemove(true)\n\t\t\t\tfs.removeMsgBlockFromList(mb)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tfseq := atomic.LoadUint64(&mb.first.seq)\n\t\t\tif fs.state.FirstSeq == 0 || (fseq < fs.state.FirstSeq && mb.first.ts != 0) {\n\t\t\t\tfs.state.FirstSeq = fseq\n\t\t\t\tif mb.first.ts == 0 {\n\t\t\t\t\tfs.state.FirstTime = time.Time{}\n\t\t\t\t} else {\n\t\t\t\t\tfs.state.FirstTime = time.Unix(0, mb.first.ts).UTC()\n\t\t\t\t}\n\t\t\t}\n\t\t\tif lseq := atomic.LoadUint64(&mb.last.seq); lseq > fs.state.LastSeq {\n\t\t\t\tfs.state.LastSeq = lseq\n\t\t\t\tif mb.last.ts == 0 {\n\t\t\t\t\tfs.state.LastTime = time.Time{}\n\t\t\t\t} else {\n\t\t\t\t\tfs.state.LastTime = time.Unix(0, mb.last.ts).UTC()\n\t\t\t\t}\n\t\t\t}\n\t\t\tfs.state.Msgs += mb.msgs\n\t\t\tfs.state.Bytes += mb.bytes\n\t\t} else {\n\t\t\treturn err\n\t\t}\n\t}\n\n\tif len(fs.blks) > 0 {\n\t\tfs.lmb = fs.blks[len(fs.blks)-1]\n\t} else {\n\t\t_, err = fs.newMsgBlockForWrite()\n\t}\n\n\t// Check if we encountered any lost data.\n\tif fs.ld != nil {\n\t\tvar emptyBlks []*msgBlock\n\t\tfor _, mb := range fs.blks {\n\t\t\tif mb.msgs == 0 && mb.rbytes == 0 {\n\t\t\t\temptyBlks = append(emptyBlks, mb)\n\t\t\t}\n\t\t}\n\t\tfor _, mb := range emptyBlks {\n\t\t\t// Need the mb lock here.\n\t\t\tmb.mu.Lock()\n\t\t\tfs.removeMsgBlock(mb)\n\t\t\tmb.mu.Unlock()\n\t\t}\n\t}\n\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Check for keyfiles orphans.\n\tif kms, err := filepath.Glob(filepath.Join(mdir, keyScanAll)); err == nil && len(kms) > 0 {\n\t\tvalid := make(map[uint32]bool)\n\t\tfor _, mb := range fs.blks {\n\t\t\tvalid[mb.index] = true\n\t\t}\n\t\tfor _, fn := range kms {\n\t\t\tvar index uint32\n\t\t\tshouldRemove := true\n\t\t\tif n, err := fmt.Sscanf(filepath.Base(fn), keyScan, &index); err == nil && n == 1 && valid[index] {\n\t\t\t\tshouldRemove = false\n\t\t\t}\n\t\t\tif shouldRemove {\n\t\t\t\tos.Remove(fn)\n\t\t\t}\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// Will expire msgs that have aged out on restart.\n// We will treat this differently in case we have a recovery\n// that will expire alot of messages on startup.\n// Should only be called on startup.\nfunc (fs *fileStore) expireMsgsOnRecover() error {\n\tif fs.state.Msgs == 0 {\n\t\treturn nil\n\t}\n\n\t// If subject delete markers is configured, can't expire on recover.\n\t// When clustered we need to go through proposals.\n\tif fs.cfg.SubjectDeleteMarkerTTL > 0 {\n\t\treturn nil\n\t}\n\n\tvar minAge = time.Now().UnixNano() - int64(fs.cfg.MaxAge)\n\tvar purged, bytes uint64\n\tvar deleted int\n\tvar nts int64\n\n\t// If we expire all make sure to write out a tombstone. Need to be done by hand here,\n\t// usually taken care of by fs.removeMsgBlock() but we do not call that here.\n\tvar last msgId\n\n\tdeleteEmptyBlock := func(mb *msgBlock) error {\n\t\t// If we are the last keep state to remember first/last sequence.\n\t\t// Do this part by hand since not deleting one by one.\n\t\tif mb == fs.lmb {\n\t\t\tlast.seq = atomic.LoadUint64(&mb.last.seq)\n\t\t\tlast.ts = mb.last.ts\n\t\t}\n\t\t// Make sure we do subject cleanup as well.\n\t\tmb.ensurePerSubjectInfoLoaded()\n\t\tmb.fss.IterOrdered(func(bsubj []byte, ss *SimpleState) bool {\n\t\t\tsubj := bytesToString(bsubj)\n\t\t\tfor i := uint64(0); i < ss.Msgs; i++ {\n\t\t\t\tfs.removePerSubject(subj)\n\t\t\t}\n\t\t\treturn true\n\t\t})\n\t\terr := mb.dirtyCloseWithRemove(true)\n\t\tif isPermissionError(err) {\n\t\t\treturn err\n\t\t}\n\t\tdeleted++\n\t\treturn nil\n\t}\n\n\tfor _, mb := range fs.blks {\n\t\tmb.mu.Lock()\n\t\tif minAge < mb.first.ts {\n\t\t\tnts = mb.first.ts\n\t\t\tmb.mu.Unlock()\n\t\t\tbreak\n\t\t}\n\t\t// Can we remove whole block here?\n\t\tif mb.last.ts <= minAge {\n\t\t\tpurged += mb.msgs\n\t\t\tbytes += mb.bytes\n\t\t\terr := deleteEmptyBlock(mb)\n\t\t\tmb.mu.Unlock()\n\t\t\tif isPermissionError(err) {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\n\t\t// If we are here we have to process the interior messages of this blk.\n\t\t// This will load fss as well.\n\t\tif err := mb.loadMsgsWithLock(); err != nil {\n\t\t\tmb.mu.Unlock()\n\t\t\tbreak\n\t\t}\n\n\t\tvar smv StoreMsg\n\t\tvar needNextFirst bool\n\n\t\t// Walk messages and remove if expired.\n\t\tfseq, lseq := atomic.LoadUint64(&mb.first.seq), atomic.LoadUint64(&mb.last.seq)\n\t\tfor seq := fseq; seq <= lseq; seq++ {\n\t\t\tsm, err := mb.cacheLookupNoCopy(seq, &smv)\n\t\t\t// Process interior deleted msgs.\n\t\t\tif err == errDeletedMsg {\n\t\t\t\t// Update dmap.\n\t\t\t\tif mb.dmap.Exists(seq) {\n\t\t\t\t\tmb.dmap.Delete(seq)\n\t\t\t\t}\n\t\t\t\t// Keep this updated just in case since we are removing dmap entries.\n\t\t\t\tatomic.StoreUint64(&mb.first.seq, seq)\n\t\t\t\tneedNextFirst = true\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\t// Break on other errors.\n\t\t\tif err != nil || sm == nil {\n\t\t\t\tatomic.StoreUint64(&mb.first.seq, seq)\n\t\t\t\tneedNextFirst = true\n\t\t\t\tbreak\n\t\t\t}\n\n\t\t\t// No error and sm != nil from here onward.\n\n\t\t\t// Check for done.\n\t\t\tif minAge < sm.ts {\n\t\t\t\tatomic.StoreUint64(&mb.first.seq, sm.seq)\n\t\t\t\tmb.first.ts = sm.ts\n\t\t\t\tneedNextFirst = false\n\t\t\t\tnts = sm.ts\n\t\t\t\tbreak\n\t\t\t}\n\n\t\t\t// Delete the message here.\n\t\t\tif mb.msgs > 0 {\n\t\t\t\tatomic.StoreUint64(&mb.first.seq, seq)\n\t\t\t\tneedNextFirst = true\n\t\t\t\tsz := fileStoreMsgSize(sm.subj, sm.hdr, sm.msg)\n\t\t\t\tif sz > mb.bytes {\n\t\t\t\t\tsz = mb.bytes\n\t\t\t\t}\n\t\t\t\tmb.bytes -= sz\n\t\t\t\tbytes += sz\n\t\t\t\tmb.msgs--\n\t\t\t\tpurged++\n\t\t\t}\n\t\t\t// Update fss\n\t\t\t// Make sure we have fss loaded.\n\t\t\tmb.removeSeqPerSubject(sm.subj, seq)\n\t\t\tfs.removePerSubject(sm.subj)\n\t\t}\n\t\t// Make sure we have a proper next first sequence.\n\t\tif needNextFirst {\n\t\t\tmb.selectNextFirst()\n\t\t}\n\t\t// Check if empty after processing, could happen if tail of messages are all deleted.\n\t\tif mb.msgs == 0 {\n\t\t\tdeleteEmptyBlock(mb)\n\t\t}\n\t\tmb.mu.Unlock()\n\t\tbreak\n\t}\n\n\tif nts > 0 {\n\t\t// Make sure to set age check based on this value.\n\t\tfs.resetAgeChk(nts - minAge)\n\t}\n\n\tif deleted > 0 {\n\t\t// Update block map.\n\t\tif fs.bim != nil {\n\t\t\tfor _, mb := range fs.blks[:deleted] {\n\t\t\t\tdelete(fs.bim, mb.index)\n\t\t\t}\n\t\t}\n\t\t// Update blks slice.\n\t\tfs.blks = copyMsgBlocks(fs.blks[deleted:])\n\t\tif lb := len(fs.blks); lb == 0 {\n\t\t\tfs.lmb = nil\n\t\t} else {\n\t\t\tfs.lmb = fs.blks[lb-1]\n\t\t}\n\t}\n\t// Update top level accounting.\n\tif purged < fs.state.Msgs {\n\t\tfs.state.Msgs -= purged\n\t} else {\n\t\tfs.state.Msgs = 0\n\t}\n\tif bytes < fs.state.Bytes {\n\t\tfs.state.Bytes -= bytes\n\t} else {\n\t\tfs.state.Bytes = 0\n\t}\n\t// Make sure to we properly set the fs first sequence and timestamp.\n\tfs.selectNextFirst()\n\n\t// Check if we have no messages and blocks left.\n\tif fs.lmb == nil && last.seq != 0 {\n\t\tif lmb, _ := fs.newMsgBlockForWrite(); lmb != nil {\n\t\t\tfs.writeTombstone(last.seq, last.ts)\n\t\t}\n\t\t// Clear any global subject state.\n\t\tfs.psim, fs.tsl = fs.psim.Empty(), 0\n\t}\n\n\t// If we purged anything, make sure we kick flush state loop.\n\tif purged > 0 {\n\t\tfs.dirty++\n\t}\n\treturn nil\n}\n\nfunc copyMsgBlocks(src []*msgBlock) []*msgBlock {\n\tif src == nil {\n\t\treturn nil\n\t}\n\tdst := make([]*msgBlock, len(src))\n\tcopy(dst, src)\n\treturn dst\n}\n\n// GetSeqFromTime looks for the first sequence number that has\n// the message with >= timestamp.\n// FIXME(dlc) - inefficient, and dumb really. Make this better.\nfunc (fs *fileStore) GetSeqFromTime(t time.Time) uint64 {\n\tfs.mu.RLock()\n\tlastSeq := fs.state.LastSeq\n\tclosed := fs.closed\n\tfs.mu.RUnlock()\n\n\tif closed {\n\t\treturn 0\n\t}\n\n\tmb := fs.selectMsgBlockForStart(t)\n\tif mb == nil {\n\t\treturn lastSeq + 1\n\t}\n\n\tfseq := atomic.LoadUint64(&mb.first.seq)\n\tlseq := atomic.LoadUint64(&mb.last.seq)\n\n\tvar smv StoreMsg\n\n\t// Linear search, hence the dumb part..\n\tts := t.UnixNano()\n\tfor seq := fseq; seq <= lseq; seq++ {\n\t\tsm, _, _ := mb.fetchMsgNoCopy(seq, &smv)\n\t\tif sm != nil && sm.ts >= ts {\n\t\t\treturn sm.seq\n\t\t}\n\t}\n\treturn 0\n}\n\n// Find the first matching message against a sublist.\nfunc (mb *msgBlock) firstMatchingMulti(sl *Sublist, start uint64, sm *StoreMsg) (*StoreMsg, bool, error) {\n\tmb.mu.Lock()\n\tvar didLoad bool\n\tvar updateLLTS bool\n\tdefer func() {\n\t\tif updateLLTS {\n\t\t\tmb.llts = ats.AccessTime()\n\t\t}\n\t\tmb.mu.Unlock()\n\t}()\n\n\t// Need messages loaded from here on out.\n\tif mb.cacheNotLoaded() {\n\t\tif err := mb.loadMsgsWithLock(); err != nil {\n\t\t\treturn nil, false, err\n\t\t}\n\t\tdidLoad = true\n\t}\n\n\t// Make sure to start at mb.first.seq if fseq < mb.first.seq\n\tif seq := atomic.LoadUint64(&mb.first.seq); seq > start {\n\t\tstart = seq\n\t}\n\tlseq := atomic.LoadUint64(&mb.last.seq)\n\n\tif sm == nil {\n\t\tsm = new(StoreMsg)\n\t}\n\n\t// If the FSS state has fewer entries than sequences in the linear scan,\n\t// then use intersection instead as likely going to be cheaper. This will\n\t// often be the case with high numbers of deletes, as well as a smaller\n\t// number of subjects in the block.\n\tif uint64(mb.fss.Size()) < lseq-start {\n\t\t// If there are no subject matches then this is effectively no-op.\n\t\thseq := uint64(math.MaxUint64)\n\t\tIntersectStree(mb.fss, sl, func(subj []byte, ss *SimpleState) {\n\t\t\tif ss.firstNeedsUpdate || ss.lastNeedsUpdate {\n\t\t\t\t// mb is already loaded into the cache so should be fast-ish.\n\t\t\t\tmb.recalculateForSubj(bytesToString(subj), ss)\n\t\t\t}\n\t\t\tfirst := ss.First\n\t\t\tif start > first {\n\t\t\t\tfirst = start\n\t\t\t}\n\t\t\tif first > ss.Last || first >= hseq {\n\t\t\t\t// The start cutoff is after the last sequence for this subject,\n\t\t\t\t// or we think we already know of a subject with an earlier msg\n\t\t\t\t// than our first seq for this subject.\n\t\t\t\treturn\n\t\t\t}\n\t\t\tif first == ss.First {\n\t\t\t\t// If the start floor is below where this subject starts then we can\n\t\t\t\t// short-circuit, avoiding needing to scan for the next message.\n\t\t\t\tif fsm, err := mb.cacheLookup(ss.First, sm); err == nil {\n\t\t\t\t\tsm = fsm\n\t\t\t\t\thseq = ss.First\n\t\t\t\t}\n\t\t\t\treturn\n\t\t\t}\n\t\t\tfor seq := first; seq <= ss.Last; seq++ {\n\t\t\t\t// Otherwise we have a start floor that intersects where this subject\n\t\t\t\t// has messages in the block, so we need to walk up until we find a\n\t\t\t\t// message matching the subject.\n\t\t\t\tif mb.dmap.Exists(seq) {\n\t\t\t\t\t// Optimisation to avoid calling cacheLookup which hits time.Now().\n\t\t\t\t\t// Instead we will update it only once in a defer.\n\t\t\t\t\tupdateLLTS = true\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tllseq := mb.llseq\n\t\t\t\tfsm, err := mb.cacheLookup(seq, sm)\n\t\t\t\tif err != nil {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tupdateLLTS = false // cacheLookup already updated it.\n\t\t\t\tif sl.HasInterest(fsm.subj) {\n\t\t\t\t\thseq = seq\n\t\t\t\t\tsm = fsm\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t\t// If we are here we did not match, so put the llseq back.\n\t\t\t\tmb.llseq = llseq\n\t\t\t}\n\t\t})\n\t\tif hseq < uint64(math.MaxUint64) && sm != nil {\n\t\t\treturn sm, didLoad, nil\n\t\t}\n\t} else {\n\t\tfor seq := start; seq <= lseq; seq++ {\n\t\t\tif mb.dmap.Exists(seq) {\n\t\t\t\t// Optimisation to avoid calling cacheLookup which hits time.Now().\n\t\t\t\t// Instead we will update it only once in a defer.\n\t\t\t\tupdateLLTS = true\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tllseq := mb.llseq\n\t\t\tfsm, err := mb.cacheLookup(seq, sm)\n\t\t\tif err != nil {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\texpireOk := seq == lseq && mb.llseq == seq\n\t\t\tupdateLLTS = false // cacheLookup already updated it.\n\t\t\tif sl.HasInterest(fsm.subj) {\n\t\t\t\treturn fsm, expireOk, nil\n\t\t\t}\n\t\t\t// If we are here we did not match, so put the llseq back.\n\t\t\tmb.llseq = llseq\n\t\t}\n\t}\n\n\treturn nil, didLoad, ErrStoreMsgNotFound\n}\n\n// Find the first matching message.\n// fs lock should be held.\nfunc (mb *msgBlock) firstMatching(filter string, wc bool, start uint64, sm *StoreMsg) (*StoreMsg, bool, error) {\n\tmb.mu.Lock()\n\tvar updateLLTS bool\n\tdefer func() {\n\t\tif updateLLTS {\n\t\t\tmb.llts = ats.AccessTime()\n\t\t}\n\t\tmb.mu.Unlock()\n\t}()\n\n\tfseq, isAll := start, filter == _EMPTY_ || filter == fwcs\n\n\tvar didLoad bool\n\tif mb.fssNotLoaded() {\n\t\t// Make sure we have fss loaded.\n\t\tmb.loadMsgsWithLock()\n\t\tdidLoad = true\n\t}\n\t// Mark fss activity.\n\tmb.lsts = ats.AccessTime()\n\n\tif filter == _EMPTY_ {\n\t\tfilter = fwcs\n\t\twc = true\n\t}\n\n\t// If we only have 1 subject currently and it matches our filter we can also set isAll.\n\tif !isAll && mb.fss.Size() == 1 {\n\t\tif !wc {\n\t\t\t_, isAll = mb.fss.Find(stringToBytes(filter))\n\t\t} else {\n\t\t\t// Since mb.fss.Find won't work if filter is a wildcard, need to use Match instead.\n\t\t\tmb.fss.Match(stringToBytes(filter), func(subject []byte, _ *SimpleState) {\n\t\t\t\tisAll = true\n\t\t\t})\n\t\t}\n\t}\n\t// Make sure to start at mb.first.seq if fseq < mb.first.seq\n\tfseq = max(fseq, atomic.LoadUint64(&mb.first.seq))\n\tlseq := atomic.LoadUint64(&mb.last.seq)\n\n\t// Optionally build the isMatch for wildcard filters.\n\tvar isMatch func(subj string) bool\n\t// Decide to build.\n\tif wc {\n\t\t_tsa, _fsa := [32]string{}, [32]string{}\n\t\ttsa, fsa := _tsa[:0], tokenizeSubjectIntoSlice(_fsa[:0], filter)\n\t\tisMatch = func(subj string) bool {\n\t\t\ttsa = tokenizeSubjectIntoSlice(tsa[:0], subj)\n\t\t\treturn isSubsetMatchTokenized(tsa, fsa)\n\t\t}\n\t}\n\n\tsubjs := mb.fs.cfg.Subjects\n\t// If isAll or our single filter matches the filter arg do linear scan.\n\tdoLinearScan := isAll || (wc && len(subjs) == 1 && subjs[0] == filter)\n\t// If we do not think we should do a linear scan check how many fss we\n\t// would need to scan vs the full range of the linear walk. Optimize for\n\t// 25th quantile of a match in a linear walk. Filter should be a wildcard.\n\t// We should consult fss if our cache is not loaded and we only have fss loaded.\n\tif !doLinearScan && wc && mb.cacheAlreadyLoaded() {\n\t\tdoLinearScan = mb.fss.Size()*4 > int(lseq-fseq)\n\t}\n\n\tif !doLinearScan {\n\t\t// If we have a wildcard match against all tracked subjects we know about.\n\t\tfseq = lseq + 1\n\t\tif bfilter := stringToBytes(filter); wc {\n\t\t\tmb.fss.Match(bfilter, func(bsubj []byte, ss *SimpleState) {\n\t\t\t\tif ss.firstNeedsUpdate || ss.lastNeedsUpdate {\n\t\t\t\t\tmb.recalculateForSubj(bytesToString(bsubj), ss)\n\t\t\t\t}\n\t\t\t\tif start <= ss.Last {\n\t\t\t\t\tfseq = min(fseq, max(start, ss.First))\n\t\t\t\t}\n\t\t\t})\n\t\t} else if ss, _ := mb.fss.Find(bfilter); ss != nil {\n\t\t\tif ss.firstNeedsUpdate || ss.lastNeedsUpdate {\n\t\t\t\tmb.recalculateForSubj(filter, ss)\n\t\t\t}\n\t\t\tif start <= ss.Last {\n\t\t\t\tfseq = min(fseq, max(start, ss.First))\n\t\t\t}\n\t\t}\n\t}\n\n\tif fseq > lseq {\n\t\treturn nil, didLoad, ErrStoreMsgNotFound\n\t}\n\n\t// Need messages loaded from here on out.\n\tif mb.cacheNotLoaded() {\n\t\tif err := mb.loadMsgsWithLock(); err != nil {\n\t\t\treturn nil, false, err\n\t\t}\n\t\tdidLoad = true\n\t}\n\n\tif sm == nil {\n\t\tsm = new(StoreMsg)\n\t}\n\n\tfor seq := fseq; seq <= lseq; seq++ {\n\t\tif mb.dmap.Exists(seq) {\n\t\t\t// Optimisation to avoid calling cacheLookup which hits time.Now().\n\t\t\t// Instead we will update it only once in a defer.\n\t\t\tupdateLLTS = true\n\t\t\tcontinue\n\t\t}\n\t\tllseq := mb.llseq\n\t\tfsm, err := mb.cacheLookup(seq, sm)\n\t\tif err != nil {\n\t\t\tif err == errPartialCache || err == errNoCache {\n\t\t\t\treturn nil, false, err\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\t\tupdateLLTS = false // cacheLookup already updated it.\n\t\texpireOk := seq == lseq && mb.llseq == seq\n\t\tif isAll {\n\t\t\treturn fsm, expireOk, nil\n\t\t}\n\t\tif wc && isMatch(sm.subj) {\n\t\t\treturn fsm, expireOk, nil\n\t\t} else if !wc && fsm.subj == filter {\n\t\t\treturn fsm, expireOk, nil\n\t\t}\n\t\t// If we are here we did not match, so put the llseq back.\n\t\tmb.llseq = llseq\n\t}\n\n\treturn nil, didLoad, ErrStoreMsgNotFound\n}\n\n// This will traverse a message block and generate the filtered pending.\nfunc (mb *msgBlock) filteredPending(subj string, wc bool, seq uint64) (total, first, last uint64) {\n\tmb.mu.Lock()\n\tdefer mb.mu.Unlock()\n\treturn mb.filteredPendingLocked(subj, wc, seq)\n}\n\n// This will traverse a message block and generate the filtered pending.\n// Lock should be held.\nfunc (mb *msgBlock) filteredPendingLocked(filter string, wc bool, sseq uint64) (total, first, last uint64) {\n\tisAll := filter == _EMPTY_ || filter == fwcs\n\n\t// First check if we can optimize this part.\n\t// This means we want all and the starting sequence was before this block.\n\tif isAll {\n\t\tif fseq := atomic.LoadUint64(&mb.first.seq); sseq <= fseq {\n\t\t\treturn mb.msgs, fseq, atomic.LoadUint64(&mb.last.seq)\n\t\t}\n\t}\n\n\tif filter == _EMPTY_ {\n\t\tfilter, wc = fwcs, true\n\t}\n\n\tupdate := func(ss *SimpleState) {\n\t\ttotal += ss.Msgs\n\t\tif first == 0 || ss.First < first {\n\t\t\tfirst = ss.First\n\t\t}\n\t\tif ss.Last > last {\n\t\t\tlast = ss.Last\n\t\t}\n\t}\n\n\t// Make sure we have fss loaded.\n\tmb.ensurePerSubjectInfoLoaded()\n\n\tvar havePartial bool\n\n\t// If we are not a wildcard just use Find() here. Avoids allocations.\n\tif !wc {\n\t\tif ss, ok := mb.fss.Find(stringToBytes(filter)); ok && ss != nil {\n\t\t\tif ss.firstNeedsUpdate || ss.lastNeedsUpdate {\n\t\t\t\tmb.recalculateForSubj(filter, ss)\n\t\t\t}\n\t\t\tif sseq <= ss.First {\n\t\t\t\tupdate(ss)\n\t\t\t} else if sseq <= ss.Last {\n\t\t\t\t// We matched but its a partial.\n\t\t\t\thavePartial = true\n\t\t\t}\n\t\t}\n\t} else {\n\t\tmb.fss.Match(stringToBytes(filter), func(bsubj []byte, ss *SimpleState) {\n\t\t\tif havePartial {\n\t\t\t\t// If we already found a partial then don't do anything else.\n\t\t\t\treturn\n\t\t\t}\n\t\t\tif ss.firstNeedsUpdate || ss.lastNeedsUpdate {\n\t\t\t\tmb.recalculateForSubj(bytesToString(bsubj), ss)\n\t\t\t}\n\t\t\tif sseq <= ss.First {\n\t\t\t\tupdate(ss)\n\t\t\t} else if sseq <= ss.Last {\n\t\t\t\t// We matched but its a partial.\n\t\t\t\thavePartial = true\n\t\t\t}\n\t\t})\n\t}\n\n\t// If we did not encounter any partials we can return here.\n\tif !havePartial {\n\t\treturn total, first, last\n\t}\n\n\t// If we are here we need to scan the msgs.\n\t// Clear what we had.\n\ttotal, first, last = 0, 0, 0\n\n\t// If we load the cache for a linear scan we want to expire that cache upon exit.\n\tvar shouldExpire bool\n\tif mb.cacheNotLoaded() {\n\t\tmb.loadMsgsWithLock()\n\t\tshouldExpire = true\n\t}\n\n\t_tsa, _fsa := [32]string{}, [32]string{}\n\ttsa, fsa := _tsa[:0], _fsa[:0]\n\tvar isMatch func(subj string) bool\n\n\tif !wc {\n\t\tisMatch = func(subj string) bool { return subj == filter }\n\t} else {\n\t\tfsa = tokenizeSubjectIntoSlice(fsa[:0], filter)\n\t\tisMatch = func(subj string) bool {\n\t\t\ttsa = tokenizeSubjectIntoSlice(tsa[:0], subj)\n\t\t\treturn isSubsetMatchTokenized(tsa, fsa)\n\t\t}\n\t}\n\n\t// 1. See if we match any subs from fss.\n\t// 2. If we match and the sseq is past ss.Last then we can use meta only.\n\t// 3. If we match and we need to do a partial, break and clear any totals and do a full scan like num pending.\n\n\tvar smv StoreMsg\n\tfor seq, lseq := sseq, atomic.LoadUint64(&mb.last.seq); seq <= lseq; seq++ {\n\t\tsm, _ := mb.cacheLookupNoCopy(seq, &smv)\n\t\tif sm == nil {\n\t\t\tcontinue\n\t\t}\n\t\tif isAll || isMatch(sm.subj) {\n\t\t\ttotal++\n\t\t\tif first == 0 || seq < first {\n\t\t\t\tfirst = seq\n\t\t\t}\n\t\t\tif seq > last {\n\t\t\t\tlast = seq\n\t\t\t}\n\t\t}\n\t}\n\t// If we loaded this block for this operation go ahead and expire it here.\n\tif shouldExpire {\n\t\tmb.tryForceExpireCacheLocked()\n\t}\n\n\treturn total, first, last\n}\n\n// FilteredState will return the SimpleState associated with the filtered subject and a proposed starting sequence.\nfunc (fs *fileStore) FilteredState(sseq uint64, subj string) SimpleState {\n\tfs.mu.RLock()\n\tdefer fs.mu.RUnlock()\n\n\tlseq := fs.state.LastSeq\n\tif sseq < fs.state.FirstSeq {\n\t\tsseq = fs.state.FirstSeq\n\t}\n\n\t// Returned state.\n\tvar ss SimpleState\n\n\t// If past the end no results.\n\tif sseq > lseq {\n\t\t// Make sure we track sequences\n\t\tss.First = fs.state.FirstSeq\n\t\tss.Last = fs.state.LastSeq\n\t\treturn ss\n\t}\n\n\t// If we want all msgs that match we can shortcircuit.\n\t// TODO(dlc) - This can be extended for all cases but would\n\t// need to be careful on total msgs calculations etc.\n\tif sseq == fs.state.FirstSeq {\n\t\tfs.numFilteredPending(subj, &ss)\n\t} else {\n\t\twc := subjectHasWildcard(subj)\n\t\t// Tracking subject state.\n\t\t// TODO(dlc) - Optimize for 2.10 with avl tree and no atomics per block.\n\t\tfor _, mb := range fs.blks {\n\t\t\t// Skip blocks that are less than our starting sequence.\n\t\t\tif sseq > atomic.LoadUint64(&mb.last.seq) {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tt, f, l := mb.filteredPending(subj, wc, sseq)\n\t\t\tss.Msgs += t\n\t\t\tif ss.First == 0 || (f > 0 && f < ss.First) {\n\t\t\t\tss.First = f\n\t\t\t}\n\t\t\tif l > ss.Last {\n\t\t\t\tss.Last = l\n\t\t\t}\n\t\t}\n\t}\n\n\treturn ss\n}\n\n// This is used to see if we can selectively jump start blocks based on filter subject and a starting block index.\n// Will return -1 and ErrStoreEOF if no matches at all or no more from where we are.\nfunc (fs *fileStore) checkSkipFirstBlock(filter string, wc bool, bi int) (int, error) {\n\t// If we match everything, just move to next blk.\n\tif filter == _EMPTY_ || filter == fwcs {\n\t\treturn bi + 1, nil\n\t}\n\t// Move through psim to gather start and stop bounds.\n\tstart, stop := uint32(math.MaxUint32), uint32(0)\n\tif wc {\n\t\tfs.psim.Match(stringToBytes(filter), func(_ []byte, psi *psi) {\n\t\t\tif psi.fblk < start {\n\t\t\t\tstart = psi.fblk\n\t\t\t}\n\t\t\tif psi.lblk > stop {\n\t\t\t\tstop = psi.lblk\n\t\t\t}\n\t\t})\n\t} else if psi, ok := fs.psim.Find(stringToBytes(filter)); ok {\n\t\tstart, stop = psi.fblk, psi.lblk\n\t}\n\t// Nothing was found.\n\tif start == uint32(math.MaxUint32) {\n\t\treturn -1, ErrStoreEOF\n\t}\n\t// Can not be nil so ok to inline dereference.\n\tmbi := fs.blks[bi].getIndex()\n\t// All matching msgs are behind us.\n\t// Less than AND equal is important because we were called because we missed searching bi.\n\tif stop <= mbi {\n\t\treturn -1, ErrStoreEOF\n\t}\n\t// If start is > index return dereference of fs.blks index.\n\tif start > mbi {\n\t\tif mb := fs.bim[start]; mb != nil {\n\t\t\tni, _ := fs.selectMsgBlockWithIndex(atomic.LoadUint64(&mb.last.seq))\n\t\t\treturn ni, nil\n\t\t}\n\t}\n\t// Otherwise just bump to the next one.\n\treturn bi + 1, nil\n}\n\n// Optimized way for getting all num pending matching a filter subject.\n// Lock should be held.\nfunc (fs *fileStore) numFilteredPending(filter string, ss *SimpleState) {\n\tfs.numFilteredPendingWithLast(filter, true, ss)\n}\n\n// Optimized way for getting all num pending matching a filter subject and first sequence only.\n// Lock should be held.\nfunc (fs *fileStore) numFilteredPendingNoLast(filter string, ss *SimpleState) {\n\tfs.numFilteredPendingWithLast(filter, false, ss)\n}\n\n// Optimized way for getting all num pending matching a filter subject.\n// Optionally look up last sequence. Sometimes do not need last and this avoids cost.\n// Read lock should be held.\nfunc (fs *fileStore) numFilteredPendingWithLast(filter string, last bool, ss *SimpleState) {\n\tisAll := filter == _EMPTY_ || filter == fwcs\n\n\t// If isAll we do not need to do anything special to calculate the first and last and total.\n\tif isAll {\n\t\tss.First = fs.state.FirstSeq\n\t\tss.Last = fs.state.LastSeq\n\t\tss.Msgs = fs.state.Msgs\n\t\treturn\n\t}\n\t// Always reset.\n\tss.First, ss.Last, ss.Msgs = 0, 0, 0\n\n\t// We do need to figure out the first and last sequences.\n\twc := subjectHasWildcard(filter)\n\tstart, stop := uint32(math.MaxUint32), uint32(0)\n\n\tif wc {\n\t\tfs.psim.Match(stringToBytes(filter), func(_ []byte, psi *psi) {\n\t\t\tss.Msgs += psi.total\n\t\t\t// Keep track of start and stop indexes for this subject.\n\t\t\tif psi.fblk < start {\n\t\t\t\tstart = psi.fblk\n\t\t\t}\n\t\t\tif psi.lblk > stop {\n\t\t\t\tstop = psi.lblk\n\t\t\t}\n\t\t})\n\t} else if psi, ok := fs.psim.Find(stringToBytes(filter)); ok {\n\t\tss.Msgs += psi.total\n\t\tstart, stop = psi.fblk, psi.lblk\n\t}\n\n\t// Did not find anything.\n\tif stop == 0 {\n\t\treturn\n\t}\n\n\t// Do start\n\tmb := fs.bim[start]\n\tif mb != nil {\n\t\t_, f, _ := mb.filteredPending(filter, wc, 0)\n\t\tss.First = f\n\t}\n\n\tif ss.First == 0 {\n\t\t// This is a miss. This can happen since psi.fblk is lazy.\n\t\t// We will make sure to update fblk.\n\n\t\t// Hold this outside loop for psim fblk updates when done.\n\t\ti := start + 1\n\t\tfor ; i <= stop; i++ {\n\t\t\tmb := fs.bim[i]\n\t\t\tif mb == nil {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif _, f, _ := mb.filteredPending(filter, wc, 0); f > 0 {\n\t\t\t\tss.First = f\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\t// Update fblk since fblk was outdated.\n\t\t// We only require read lock here as that is desirable,\n\t\t// so we need to do this in a go routine to acquire write lock.\n\t\tgo func() {\n\t\t\tfs.mu.Lock()\n\t\t\tdefer fs.mu.Unlock()\n\t\t\tif !wc {\n\t\t\t\tif info, ok := fs.psim.Find(stringToBytes(filter)); ok {\n\t\t\t\t\tif i > info.fblk {\n\t\t\t\t\t\tinfo.fblk = i\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tfs.psim.Match(stringToBytes(filter), func(subj []byte, psi *psi) {\n\t\t\t\t\tif i > psi.fblk {\n\t\t\t\t\t\tpsi.fblk = i\n\t\t\t\t\t}\n\t\t\t\t})\n\t\t\t}\n\t\t}()\n\t}\n\t// Now gather last sequence if asked to do so.\n\tif last {\n\t\tif mb = fs.bim[stop]; mb != nil {\n\t\t\t_, _, l := mb.filteredPending(filter, wc, 0)\n\t\t\tss.Last = l\n\t\t}\n\t}\n}\n\n// SubjectsState returns a map of SimpleState for all matching subjects.\nfunc (fs *fileStore) SubjectsState(subject string) map[string]SimpleState {\n\tfs.mu.RLock()\n\tdefer fs.mu.RUnlock()\n\n\tif fs.state.Msgs == 0 || fs.noTrackSubjects() {\n\t\treturn nil\n\t}\n\n\tif subject == _EMPTY_ {\n\t\tsubject = fwcs\n\t}\n\n\tstart, stop := fs.blks[0], fs.lmb\n\t// We can short circuit if not a wildcard using psim for start and stop.\n\tif !subjectHasWildcard(subject) {\n\t\tinfo, ok := fs.psim.Find(stringToBytes(subject))\n\t\tif !ok {\n\t\t\treturn nil\n\t\t}\n\t\tif f := fs.bim[info.fblk]; f != nil {\n\t\t\tstart = f\n\t\t}\n\t\tif l := fs.bim[info.lblk]; l != nil {\n\t\t\tstop = l\n\t\t}\n\t}\n\n\t// Aggregate fss.\n\tfss := make(map[string]SimpleState)\n\tvar startFound bool\n\n\tfor _, mb := range fs.blks {\n\t\tif !startFound {\n\t\t\tif mb != start {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tstartFound = true\n\t\t}\n\n\t\tmb.mu.Lock()\n\t\tvar shouldExpire bool\n\t\tif mb.fssNotLoaded() {\n\t\t\t// Make sure we have fss loaded.\n\t\t\tmb.loadMsgsWithLock()\n\t\t\tshouldExpire = true\n\t\t}\n\t\t// Mark fss activity.\n\t\tmb.lsts = ats.AccessTime()\n\t\tmb.fss.Match(stringToBytes(subject), func(bsubj []byte, ss *SimpleState) {\n\t\t\tsubj := string(bsubj)\n\t\t\tif ss.firstNeedsUpdate || ss.lastNeedsUpdate {\n\t\t\t\tmb.recalculateForSubj(subj, ss)\n\t\t\t}\n\t\t\toss := fss[subj]\n\t\t\tif oss.First == 0 { // New\n\t\t\t\tfss[subj] = *ss\n\t\t\t} else {\n\t\t\t\t// Merge here.\n\t\t\t\toss.Last, oss.Msgs = ss.Last, oss.Msgs+ss.Msgs\n\t\t\t\tfss[subj] = oss\n\t\t\t}\n\t\t})\n\t\tif shouldExpire {\n\t\t\t// Expire this cache before moving on.\n\t\t\tmb.tryForceExpireCacheLocked()\n\t\t}\n\t\tmb.mu.Unlock()\n\n\t\tif mb == stop {\n\t\t\tbreak\n\t\t}\n\t}\n\n\treturn fss\n}\n\n// AllLastSeqs will return a sorted list of last sequences for all subjects.\nfunc (fs *fileStore) AllLastSeqs() ([]uint64, error) {\n\tfs.mu.RLock()\n\tdefer fs.mu.RUnlock()\n\treturn fs.allLastSeqsLocked()\n}\n\n// allLastSeqsLocked will return a sorted list of last sequences for all\n// subjects, but won't take the lock to do it, to avoid the issue of compounding\n// read locks causing a deadlock with a write lock.\nfunc (fs *fileStore) allLastSeqsLocked() ([]uint64, error) {\n\tif fs.state.Msgs == 0 || fs.noTrackSubjects() {\n\t\treturn nil, nil\n\t}\n\n\tnumSubjects := fs.psim.Size()\n\tseqs := make([]uint64, 0, numSubjects)\n\tsubs := make(map[string]struct{}, numSubjects)\n\n\tfor i := len(fs.blks) - 1; i >= 0; i-- {\n\t\tif len(subs) == numSubjects {\n\t\t\tbreak\n\t\t}\n\t\tmb := fs.blks[i]\n\t\tmb.mu.Lock()\n\n\t\tvar shouldExpire bool\n\t\tif mb.fssNotLoaded() {\n\t\t\t// Make sure we have fss loaded.\n\t\t\tmb.loadMsgsWithLock()\n\t\t\tshouldExpire = true\n\t\t}\n\n\t\tmb.fss.IterFast(func(bsubj []byte, ss *SimpleState) bool {\n\t\t\t// Check if already been processed and accounted.\n\t\t\tif _, ok := subs[string(bsubj)]; !ok {\n\t\t\t\tseqs = append(seqs, ss.Last)\n\t\t\t\tsubs[string(bsubj)] = struct{}{}\n\t\t\t}\n\t\t\treturn true\n\t\t})\n\t\tif shouldExpire {\n\t\t\t// Expire this cache before moving on.\n\t\t\tmb.tryForceExpireCacheLocked()\n\t\t}\n\t\tmb.mu.Unlock()\n\t}\n\n\tslices.Sort(seqs)\n\treturn seqs, nil\n}\n\n// Helper to determine if the filter(s) represent all the subjects.\n// Most clients send in subjects even if they match the stream's ingest subjects.\n// Lock should be held.\nfunc (fs *fileStore) filterIsAll(filters []string) bool {\n\tif len(filters) != len(fs.cfg.Subjects) {\n\t\treturn false\n\t}\n\t// Sort so we can compare.\n\tslices.Sort(filters)\n\tfor i, subj := range filters {\n\t\tif !subjectIsSubsetMatch(fs.cfg.Subjects[i], subj) {\n\t\t\treturn false\n\t\t}\n\t}\n\treturn true\n}\n\n// MultiLastSeqs will return a sorted list of sequences that match all subjects presented in filters.\n// We will not exceed the maxSeq, which if 0 becomes the store's last sequence.\nfunc (fs *fileStore) MultiLastSeqs(filters []string, maxSeq uint64, maxAllowed int) ([]uint64, error) {\n\tfs.mu.RLock()\n\tdefer fs.mu.RUnlock()\n\n\tif fs.state.Msgs == 0 || fs.noTrackSubjects() {\n\t\treturn nil, nil\n\t}\n\n\t// See if we can short circuit if we think they are asking for all last sequences and have no maxSeq or maxAllowed set.\n\tif maxSeq == 0 && maxAllowed <= 0 && fs.filterIsAll(filters) {\n\t\treturn fs.allLastSeqsLocked()\n\t}\n\n\tlastBlkIndex := len(fs.blks) - 1\n\tlastMB := fs.blks[lastBlkIndex]\n\n\t// Implied last sequence.\n\tif maxSeq == 0 {\n\t\tmaxSeq = fs.state.LastSeq\n\t} else {\n\t\t// Udate last mb index if not last seq.\n\t\tlastBlkIndex, lastMB = fs.selectMsgBlockWithIndex(maxSeq)\n\t}\n\t// Make sure non-nil\n\tif lastMB == nil {\n\t\treturn nil, nil\n\t}\n\n\t// Grab our last mb index (not same as blk index).\n\tlastMB.mu.RLock()\n\tlastMBIndex := lastMB.index\n\tlastMB.mu.RUnlock()\n\n\tsubs := make(map[string]*psi)\n\tvar numLess int\n\tvar maxBlk uint32\n\n\tfor _, filter := range filters {\n\t\tfs.psim.Match(stringToBytes(filter), func(subj []byte, psi *psi) {\n\t\t\tsubs[string(subj)] = psi\n\t\t\tif psi.lblk < lastMBIndex {\n\t\t\t\tnumLess++\n\t\t\t\tif psi.lblk > maxBlk {\n\t\t\t\t\tmaxBlk = psi.lblk\n\t\t\t\t}\n\t\t\t}\n\t\t})\n\t}\n\n\t// If all subjects have a lower last index, select the largest for our walk backwards.\n\tif numLess == len(subs) {\n\t\tlastMB = fs.bim[maxBlk]\n\t}\n\n\t// Collect all sequences needed.\n\tseqs := make([]uint64, 0, len(subs))\n\tfor i, lnf := lastBlkIndex, false; i >= 0; i-- {\n\t\tif len(subs) == 0 {\n\t\t\tbreak\n\t\t}\n\t\tmb := fs.blks[i]\n\t\tif !lnf {\n\t\t\tif mb != lastMB {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tlnf = true\n\t\t}\n\t\t// We can start properly looking here.\n\t\tmb.mu.Lock()\n\t\tmb.ensurePerSubjectInfoLoaded()\n\n\t\t// Iterate the fss and check against our subs. We will delete from subs as we add.\n\t\t// Once len(subs) == 0 we are done.\n\t\tmb.fss.IterFast(func(bsubj []byte, ss *SimpleState) bool {\n\t\t\t// Already been processed and accounted for was not matched in the first place.\n\t\t\tif subs[string(bsubj)] == nil {\n\t\t\t\treturn true\n\t\t\t}\n\t\t\t// Check if we need to recalculate. We only care about the last sequence.\n\t\t\tif ss.lastNeedsUpdate {\n\t\t\t\t// mb is already loaded into the cache so should be fast-ish.\n\t\t\t\tmb.recalculateForSubj(bytesToString(bsubj), ss)\n\t\t\t}\n\t\t\t// If we are equal or below just add to seqs slice.\n\t\t\tif ss.Last <= maxSeq {\n\t\t\t\tseqs = append(seqs, ss.Last)\n\t\t\t\tdelete(subs, bytesToString(bsubj))\n\t\t\t} else {\n\t\t\t\t// Need to search for the real last since recorded last is > maxSeq.\n\t\t\t\tif mb.cacheNotLoaded() {\n\t\t\t\t\tmb.loadMsgsWithLock()\n\t\t\t\t}\n\t\t\t\tvar smv StoreMsg\n\t\t\t\tfseq := atomic.LoadUint64(&mb.first.seq)\n\t\t\t\tlseq := min(atomic.LoadUint64(&mb.last.seq), maxSeq)\n\t\t\t\tssubj := bytesToString(bsubj)\n\t\t\t\tfor seq := lseq; seq >= fseq; seq-- {\n\t\t\t\t\tsm, _ := mb.cacheLookupNoCopy(seq, &smv)\n\t\t\t\t\tif sm == nil || sm.subj != ssubj {\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\t\t\t\t\tseqs = append(seqs, sm.seq)\n\t\t\t\t\tdelete(subs, ssubj)\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn true\n\t\t})\n\t\tmb.mu.Unlock()\n\n\t\t// If maxAllowed was sepcified check that we will not exceed that.\n\t\tif maxAllowed > 0 && len(seqs) > maxAllowed {\n\t\t\treturn nil, ErrTooManyResults\n\t\t}\n\t}\n\tif len(seqs) == 0 {\n\t\treturn nil, nil\n\t}\n\tslices.Sort(seqs)\n\treturn seqs, nil\n}\n\n// NumPending will return the number of pending messages matching the filter subject starting at sequence.\n// Optimized for stream num pending calculations for consumers.\nfunc (fs *fileStore) NumPending(sseq uint64, filter string, lastPerSubject bool) (total, validThrough uint64) {\n\tfs.mu.RLock()\n\tdefer fs.mu.RUnlock()\n\n\t// This can always be last for these purposes.\n\tvalidThrough = fs.state.LastSeq\n\n\tif fs.state.Msgs == 0 || sseq > fs.state.LastSeq {\n\t\treturn 0, validThrough\n\t}\n\n\t// If sseq is less then our first set to first.\n\tif sseq < fs.state.FirstSeq {\n\t\tsseq = fs.state.FirstSeq\n\t}\n\t// Track starting for both block for the sseq and staring block that matches any subject.\n\tvar seqStart int\n\t// See if we need to figure out starting block per sseq.\n\tif sseq > fs.state.FirstSeq {\n\t\t// This should not, but can return -1, so make sure we check to avoid panic below.\n\t\tif seqStart, _ = fs.selectMsgBlockWithIndex(sseq); seqStart < 0 {\n\t\t\tseqStart = 0\n\t\t}\n\t}\n\n\tisAll := filter == _EMPTY_ || filter == fwcs\n\tif isAll && filter == _EMPTY_ {\n\t\tfilter = fwcs\n\t}\n\twc := subjectHasWildcard(filter)\n\n\t// See if filter was provided but its the only subject.\n\tif !isAll && !wc && fs.psim.Size() == 1 {\n\t\t_, isAll = fs.psim.Find(stringToBytes(filter))\n\t}\n\t// If we are isAll and have no deleted we can do a simpler calculation.\n\tif !lastPerSubject && isAll && (fs.state.LastSeq-fs.state.FirstSeq+1) == fs.state.Msgs {\n\t\tif sseq == 0 {\n\t\t\treturn fs.state.Msgs, validThrough\n\t\t}\n\t\treturn fs.state.LastSeq - sseq + 1, validThrough\n\t}\n\n\t_tsa, _fsa := [32]string{}, [32]string{}\n\ttsa, fsa := _tsa[:0], _fsa[:0]\n\tif wc {\n\t\tfsa = tokenizeSubjectIntoSlice(fsa[:0], filter)\n\t}\n\n\tisMatch := func(subj string) bool {\n\t\tif isAll {\n\t\t\treturn true\n\t\t}\n\t\tif !wc {\n\t\t\treturn subj == filter\n\t\t}\n\t\ttsa = tokenizeSubjectIntoSlice(tsa[:0], subj)\n\t\treturn isSubsetMatchTokenized(tsa, fsa)\n\t}\n\n\t// Handle last by subject a bit differently.\n\t// We will scan PSIM since we accurately track the last block we have seen the subject in. This\n\t// allows us to only need to load at most one block now.\n\t// For the last block, we need to track the subjects that we know are in that block, and track seen\n\t// while in the block itself, but complexity there worth it.\n\tif lastPerSubject {\n\t\t// If we want all and our start sequence is equal or less than first return number of subjects.\n\t\tif isAll && sseq <= fs.state.FirstSeq {\n\t\t\treturn uint64(fs.psim.Size()), validThrough\n\t\t}\n\t\t// If we are here we need to scan. We are going to scan the PSIM looking for lblks that are >= seqStart.\n\t\t// This will build up a list of all subjects from the selected block onward.\n\t\tlbm := make(map[string]bool)\n\t\tmb := fs.blks[seqStart]\n\t\tbi := mb.index\n\n\t\tfs.psim.Match(stringToBytes(filter), func(subj []byte, psi *psi) {\n\t\t\t// If the select blk start is greater than entry's last blk skip.\n\t\t\tif bi > psi.lblk {\n\t\t\t\treturn\n\t\t\t}\n\t\t\ttotal++\n\t\t\t// We will track the subjects that are an exact match to the last block.\n\t\t\t// This is needed for last block processing.\n\t\t\tif psi.lblk == bi {\n\t\t\t\tlbm[string(subj)] = true\n\t\t\t}\n\t\t})\n\n\t\t// Now check if we need to inspect the seqStart block.\n\t\t// Grab write lock in case we need to load in msgs.\n\t\tmb.mu.Lock()\n\t\tvar updateLLTS bool\n\t\tvar shouldExpire bool\n\t\t// We need to walk this block to correct accounting from above.\n\t\tif sseq > mb.first.seq {\n\t\t\t// Track the ones we add back in case more than one.\n\t\t\tseen := make(map[string]bool)\n\t\t\t// We need to discount the total by subjects seen before sseq, but also add them right back in if they are >= sseq for this blk.\n\t\t\t// This only should be subjects we know have the last blk in this block.\n\t\t\tif mb.cacheNotLoaded() {\n\t\t\t\tmb.loadMsgsWithLock()\n\t\t\t\tshouldExpire = true\n\t\t\t}\n\t\t\tvar smv StoreMsg\n\t\t\tfor seq, lseq := atomic.LoadUint64(&mb.first.seq), atomic.LoadUint64(&mb.last.seq); seq <= lseq; seq++ {\n\t\t\t\tif mb.dmap.Exists(seq) {\n\t\t\t\t\t// Optimisation to avoid calling cacheLookup which hits time.Now().\n\t\t\t\t\tupdateLLTS = true\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tsm, _ := mb.cacheLookupNoCopy(seq, &smv)\n\t\t\t\tif sm == nil || sm.subj == _EMPTY_ || !lbm[sm.subj] {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tupdateLLTS = false // cacheLookup already updated it.\n\t\t\t\tif isMatch(sm.subj) {\n\t\t\t\t\t// If less than sseq adjust off of total as long as this subject matched the last block.\n\t\t\t\t\tif seq < sseq {\n\t\t\t\t\t\tif !seen[sm.subj] {\n\t\t\t\t\t\t\ttotal--\n\t\t\t\t\t\t\tseen[sm.subj] = true\n\t\t\t\t\t\t}\n\t\t\t\t\t} else if seen[sm.subj] {\n\t\t\t\t\t\t// This is equal or more than sseq, so add back in.\n\t\t\t\t\t\ttotal++\n\t\t\t\t\t\t// Make sure to not process anymore.\n\t\t\t\t\t\tdelete(seen, sm.subj)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t// If we loaded the block try to force expire.\n\t\tif shouldExpire {\n\t\t\tmb.tryForceExpireCacheLocked()\n\t\t}\n\t\tif updateLLTS {\n\t\t\tmb.llts = ats.AccessTime()\n\t\t}\n\t\tmb.mu.Unlock()\n\t\treturn total, validThrough\n\t}\n\n\t// If we would need to scan more from the beginning, revert back to calculating directly here.\n\t// TODO(dlc) - Redo properly with sublists etc for subject-based filtering.\n\tif seqStart >= (len(fs.blks) / 2) {\n\t\tfor i := seqStart; i < len(fs.blks); i++ {\n\t\t\tvar shouldExpire bool\n\t\t\tmb := fs.blks[i]\n\t\t\t// Hold write lock in case we need to load cache.\n\t\t\tmb.mu.Lock()\n\t\t\tif isAll && sseq <= atomic.LoadUint64(&mb.first.seq) {\n\t\t\t\ttotal += mb.msgs\n\t\t\t\tmb.mu.Unlock()\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\t// If we are here we need to at least scan the subject fss.\n\t\t\t// Make sure we have fss loaded.\n\t\t\tif mb.fssNotLoaded() {\n\t\t\t\tmb.loadMsgsWithLock()\n\t\t\t\tshouldExpire = true\n\t\t\t}\n\t\t\t// Mark fss activity.\n\t\t\tmb.lsts = ats.AccessTime()\n\n\t\t\tvar t uint64\n\t\t\tvar havePartial bool\n\t\t\tmb.fss.Match(stringToBytes(filter), func(bsubj []byte, ss *SimpleState) {\n\t\t\t\tif havePartial {\n\t\t\t\t\t// If we already found a partial then don't do anything else.\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t\tsubj := bytesToString(bsubj)\n\t\t\t\tif ss.firstNeedsUpdate || ss.lastNeedsUpdate {\n\t\t\t\t\tmb.recalculateForSubj(subj, ss)\n\t\t\t\t}\n\t\t\t\tif sseq <= ss.First {\n\t\t\t\t\tt += ss.Msgs\n\t\t\t\t} else if sseq <= ss.Last {\n\t\t\t\t\t// We matched but its a partial.\n\t\t\t\t\thavePartial = true\n\t\t\t\t}\n\t\t\t})\n\n\t\t\t// See if we need to scan msgs here.\n\t\t\tif havePartial {\n\t\t\t\t// Make sure we have the cache loaded.\n\t\t\t\tif mb.cacheNotLoaded() {\n\t\t\t\t\tmb.loadMsgsWithLock()\n\t\t\t\t\tshouldExpire = true\n\t\t\t\t}\n\t\t\t\t// Clear on partial.\n\t\t\t\tt = 0\n\t\t\t\tstart := sseq\n\t\t\t\tif fseq := atomic.LoadUint64(&mb.first.seq); fseq > start {\n\t\t\t\t\tstart = fseq\n\t\t\t\t}\n\t\t\t\tvar smv StoreMsg\n\t\t\t\tfor seq, lseq := start, atomic.LoadUint64(&mb.last.seq); seq <= lseq; seq++ {\n\t\t\t\t\tif sm, _ := mb.cacheLookupNoCopy(seq, &smv); sm != nil && isMatch(sm.subj) {\n\t\t\t\t\t\tt++\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\t// If we loaded this block for this operation go ahead and expire it here.\n\t\t\tif shouldExpire {\n\t\t\t\tmb.tryForceExpireCacheLocked()\n\t\t\t}\n\t\t\tmb.mu.Unlock()\n\t\t\ttotal += t\n\t\t}\n\t\treturn total, validThrough\n\t}\n\n\t// If we are here it's better to calculate totals from psim and adjust downward by scanning less blocks.\n\t// TODO(dlc) - Eventually when sublist uses generics, make this sublist driven instead.\n\tstart := uint32(math.MaxUint32)\n\tfs.psim.Match(stringToBytes(filter), func(_ []byte, psi *psi) {\n\t\ttotal += psi.total\n\t\t// Keep track of start index for this subject.\n\t\tif psi.fblk < start {\n\t\t\tstart = psi.fblk\n\t\t}\n\t})\n\t// See if we were asked for all, if so we are done.\n\tif sseq <= fs.state.FirstSeq {\n\t\treturn total, validThrough\n\t}\n\n\t// If we are here we need to calculate partials for the first blocks.\n\tfirstSubjBlk := fs.bim[start]\n\tvar firstSubjBlkFound bool\n\t// Adjust in case not found.\n\tif firstSubjBlk == nil {\n\t\tfirstSubjBlkFound = true\n\t}\n\n\t// Track how many we need to adjust against the total.\n\tvar adjust uint64\n\tfor i := 0; i <= seqStart; i++ {\n\t\tmb := fs.blks[i]\n\t\t// We can skip blks if we know they are below the first one that has any subject matches.\n\t\tif !firstSubjBlkFound {\n\t\t\tif firstSubjBlkFound = (mb == firstSubjBlk); !firstSubjBlkFound {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t}\n\t\t// We need to scan this block.\n\t\tvar shouldExpire bool\n\t\tvar updateLLTS bool\n\t\tmb.mu.Lock()\n\t\t// Check if we should include all of this block in adjusting. If so work with metadata.\n\t\tif sseq > atomic.LoadUint64(&mb.last.seq) {\n\t\t\tif isAll {\n\t\t\t\tadjust += mb.msgs\n\t\t\t} else {\n\t\t\t\t// We need to adjust for all matches in this block.\n\t\t\t\t// Make sure we have fss loaded. This loads whole block now.\n\t\t\t\tif mb.fssNotLoaded() {\n\t\t\t\t\tmb.loadMsgsWithLock()\n\t\t\t\t\tshouldExpire = true\n\t\t\t\t}\n\t\t\t\t// Mark fss activity.\n\t\t\t\tmb.lsts = ats.AccessTime()\n\n\t\t\t\tmb.fss.Match(stringToBytes(filter), func(bsubj []byte, ss *SimpleState) {\n\t\t\t\t\tadjust += ss.Msgs\n\t\t\t\t})\n\t\t\t}\n\t\t} else {\n\t\t\t// This is the last block. We need to scan per message here.\n\t\t\tif mb.cacheNotLoaded() {\n\t\t\t\tmb.loadMsgsWithLock()\n\t\t\t\tshouldExpire = true\n\t\t\t}\n\t\t\tvar last = atomic.LoadUint64(&mb.last.seq)\n\t\t\tif sseq < last {\n\t\t\t\tlast = sseq\n\t\t\t}\n\t\t\t// We need to walk all messages in this block\n\t\t\tvar smv StoreMsg\n\t\t\tfor seq := atomic.LoadUint64(&mb.first.seq); seq < last; seq++ {\n\t\t\t\tif mb.dmap.Exists(seq) {\n\t\t\t\t\t// Optimisation to avoid calling cacheLookup which hits time.Now().\n\t\t\t\t\tupdateLLTS = true\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tsm, _ := mb.cacheLookupNoCopy(seq, &smv)\n\t\t\t\tif sm == nil || sm.subj == _EMPTY_ {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tupdateLLTS = false // cacheLookup already updated it.\n\t\t\t\t// Check if it matches our filter.\n\t\t\t\tif sm.seq < sseq && isMatch(sm.subj) {\n\t\t\t\t\tadjust++\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t// If we loaded the block try to force expire.\n\t\tif shouldExpire {\n\t\t\tmb.tryForceExpireCacheLocked()\n\t\t}\n\t\tif updateLLTS {\n\t\t\tmb.llts = ats.AccessTime()\n\t\t}\n\t\tmb.mu.Unlock()\n\t}\n\t// Make final adjustment.\n\ttotal -= adjust\n\n\treturn total, validThrough\n}\n\n// NumPending will return the number of pending messages matching any subject in the sublist starting at sequence.\n// Optimized for stream num pending calculations for consumers with lots of filtered subjects.\n// Subjects should not overlap, this property is held when doing multi-filtered consumers.\nfunc (fs *fileStore) NumPendingMulti(sseq uint64, sl *Sublist, lastPerSubject bool) (total, validThrough uint64) {\n\tfs.mu.RLock()\n\tdefer fs.mu.RUnlock()\n\n\t// This can always be last for these purposes.\n\tvalidThrough = fs.state.LastSeq\n\n\tif fs.state.Msgs == 0 || sseq > fs.state.LastSeq {\n\t\treturn 0, validThrough\n\t}\n\n\t// If sseq is less then our first set to first.\n\tif sseq < fs.state.FirstSeq {\n\t\tsseq = fs.state.FirstSeq\n\t}\n\t// Track starting for both block for the sseq and staring block that matches any subject.\n\tvar seqStart int\n\t// See if we need to figure out starting block per sseq.\n\tif sseq > fs.state.FirstSeq {\n\t\t// This should not, but can return -1, so make sure we check to avoid panic below.\n\t\tif seqStart, _ = fs.selectMsgBlockWithIndex(sseq); seqStart < 0 {\n\t\t\tseqStart = 0\n\t\t}\n\t}\n\n\tisAll := sl == nil\n\n\t// See if filter was provided but its the only subject.\n\tif !isAll && fs.psim.Size() == 1 {\n\t\tfs.psim.IterFast(func(subject []byte, _ *psi) bool {\n\t\t\tisAll = sl.HasInterest(bytesToString(subject))\n\t\t\treturn true\n\t\t})\n\t}\n\t// If we are isAll and have no deleted we can do a simpler calculation.\n\tif !lastPerSubject && isAll && (fs.state.LastSeq-fs.state.FirstSeq+1) == fs.state.Msgs {\n\t\tif sseq == 0 {\n\t\t\treturn fs.state.Msgs, validThrough\n\t\t}\n\t\treturn fs.state.LastSeq - sseq + 1, validThrough\n\t}\n\t// Setup the isMatch function.\n\tisMatch := func(subj string) bool {\n\t\tif isAll {\n\t\t\treturn true\n\t\t}\n\t\treturn sl.HasInterest(subj)\n\t}\n\n\t// Handle last by subject a bit differently.\n\t// We will scan PSIM since we accurately track the last block we have seen the subject in. This\n\t// allows us to only need to load at most one block now.\n\t// For the last block, we need to track the subjects that we know are in that block, and track seen\n\t// while in the block itself, but complexity there worth it.\n\tif lastPerSubject {\n\t\t// If we want all and our start sequence is equal or less than first return number of subjects.\n\t\tif isAll && sseq <= fs.state.FirstSeq {\n\t\t\treturn uint64(fs.psim.Size()), validThrough\n\t\t}\n\t\t// If we are here we need to scan. We are going to scan the PSIM looking for lblks that are >= seqStart.\n\t\t// This will build up a list of all subjects from the selected block onward.\n\t\tlbm := make(map[string]bool)\n\t\tmb := fs.blks[seqStart]\n\t\tbi := mb.index\n\n\t\tsubs := make([]*subscription, 0, sl.Count())\n\t\tsl.All(&subs)\n\t\tfor _, sub := range subs {\n\t\t\tfs.psim.Match(sub.subject, func(subj []byte, psi *psi) {\n\t\t\t\t// If the select blk start is greater than entry's last blk skip.\n\t\t\t\tif bi > psi.lblk {\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t\ttotal++\n\t\t\t\t// We will track the subjects that are an exact match to the last block.\n\t\t\t\t// This is needed for last block processing.\n\t\t\t\tif psi.lblk == bi {\n\t\t\t\t\tlbm[string(subj)] = true\n\t\t\t\t}\n\t\t\t})\n\t\t}\n\n\t\t// Now check if we need to inspect the seqStart block.\n\t\t// Grab write lock in case we need to load in msgs.\n\t\tmb.mu.Lock()\n\t\tvar shouldExpire bool\n\t\tvar updateLLTS bool\n\t\t// We need to walk this block to correct accounting from above.\n\t\tif sseq > mb.first.seq {\n\t\t\t// Track the ones we add back in case more than one.\n\t\t\tseen := make(map[string]bool)\n\t\t\t// We need to discount the total by subjects seen before sseq, but also add them right back in if they are >= sseq for this blk.\n\t\t\t// This only should be subjects we know have the last blk in this block.\n\t\t\tif mb.cacheNotLoaded() {\n\t\t\t\tmb.loadMsgsWithLock()\n\t\t\t\tshouldExpire = true\n\t\t\t}\n\t\t\tvar smv StoreMsg\n\t\t\tfor seq, lseq := atomic.LoadUint64(&mb.first.seq), atomic.LoadUint64(&mb.last.seq); seq <= lseq; seq++ {\n\t\t\t\tif mb.dmap.Exists(seq) {\n\t\t\t\t\t// Optimisation to avoid calling cacheLookup which hits time.Now().\n\t\t\t\t\tupdateLLTS = true\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tsm, _ := mb.cacheLookupNoCopy(seq, &smv)\n\t\t\t\tif sm == nil || sm.subj == _EMPTY_ || !lbm[sm.subj] {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tupdateLLTS = false // cacheLookup already updated it.\n\t\t\t\tif isMatch(sm.subj) {\n\t\t\t\t\t// If less than sseq adjust off of total as long as this subject matched the last block.\n\t\t\t\t\tif seq < sseq {\n\t\t\t\t\t\tif !seen[sm.subj] {\n\t\t\t\t\t\t\ttotal--\n\t\t\t\t\t\t\tseen[sm.subj] = true\n\t\t\t\t\t\t}\n\t\t\t\t\t} else if seen[sm.subj] {\n\t\t\t\t\t\t// This is equal or more than sseq, so add back in.\n\t\t\t\t\t\ttotal++\n\t\t\t\t\t\t// Make sure to not process anymore.\n\t\t\t\t\t\tdelete(seen, sm.subj)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t// If we loaded the block try to force expire.\n\t\tif shouldExpire {\n\t\t\tmb.tryForceExpireCacheLocked()\n\t\t}\n\t\tif updateLLTS {\n\t\t\tmb.llts = ats.AccessTime()\n\t\t}\n\t\tmb.mu.Unlock()\n\t\treturn total, validThrough\n\t}\n\n\t// If we would need to scan more from the beginning, revert back to calculating directly here.\n\tif seqStart >= (len(fs.blks) / 2) {\n\t\tfor i := seqStart; i < len(fs.blks); i++ {\n\t\t\tvar shouldExpire bool\n\t\t\tmb := fs.blks[i]\n\t\t\t// Hold write lock in case we need to load cache.\n\t\t\tmb.mu.Lock()\n\t\t\tif isAll && sseq <= atomic.LoadUint64(&mb.first.seq) {\n\t\t\t\ttotal += mb.msgs\n\t\t\t\tmb.mu.Unlock()\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\t// If we are here we need to at least scan the subject fss.\n\t\t\t// Make sure we have fss loaded.\n\t\t\tif mb.fssNotLoaded() {\n\t\t\t\tmb.loadMsgsWithLock()\n\t\t\t\tshouldExpire = true\n\t\t\t}\n\t\t\t// Mark fss activity.\n\t\t\tmb.lsts = ats.AccessTime()\n\n\t\t\tvar t uint64\n\t\t\tvar havePartial bool\n\t\t\tvar updateLLTS bool\n\t\t\tIntersectStree[SimpleState](mb.fss, sl, func(bsubj []byte, ss *SimpleState) {\n\t\t\t\tsubj := bytesToString(bsubj)\n\t\t\t\tif havePartial {\n\t\t\t\t\t// If we already found a partial then don't do anything else.\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t\tif ss.firstNeedsUpdate || ss.lastNeedsUpdate {\n\t\t\t\t\tmb.recalculateForSubj(subj, ss)\n\t\t\t\t}\n\t\t\t\tif sseq <= ss.First {\n\t\t\t\t\tt += ss.Msgs\n\t\t\t\t} else if sseq <= ss.Last {\n\t\t\t\t\t// We matched but its a partial.\n\t\t\t\t\thavePartial = true\n\t\t\t\t}\n\t\t\t})\n\n\t\t\t// See if we need to scan msgs here.\n\t\t\tif havePartial {\n\t\t\t\t// Make sure we have the cache loaded.\n\t\t\t\tif mb.cacheNotLoaded() {\n\t\t\t\t\tmb.loadMsgsWithLock()\n\t\t\t\t\tshouldExpire = true\n\t\t\t\t}\n\t\t\t\t// Clear on partial.\n\t\t\t\tt = 0\n\t\t\t\tstart := sseq\n\t\t\t\tif fseq := atomic.LoadUint64(&mb.first.seq); fseq > start {\n\t\t\t\t\tstart = fseq\n\t\t\t\t}\n\t\t\t\tvar smv StoreMsg\n\t\t\t\tfor seq, lseq := start, atomic.LoadUint64(&mb.last.seq); seq <= lseq; seq++ {\n\t\t\t\t\tif mb.dmap.Exists(seq) {\n\t\t\t\t\t\t// Optimisation to avoid calling cacheLookup which hits time.Now().\n\t\t\t\t\t\tupdateLLTS = true\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\t\t\t\t\tif sm, _ := mb.cacheLookupNoCopy(seq, &smv); sm != nil && isMatch(sm.subj) {\n\t\t\t\t\t\tt++\n\t\t\t\t\t\tupdateLLTS = false // cacheLookup already updated it.\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\t// If we loaded this block for this operation go ahead and expire it here.\n\t\t\tif shouldExpire {\n\t\t\t\tmb.tryForceExpireCacheLocked()\n\t\t\t}\n\t\t\tif updateLLTS {\n\t\t\t\tmb.llts = ats.AccessTime()\n\t\t\t}\n\t\t\tmb.mu.Unlock()\n\t\t\ttotal += t\n\t\t}\n\t\treturn total, validThrough\n\t}\n\n\t// If we are here it's better to calculate totals from psim and adjust downward by scanning less blocks.\n\tstart := uint32(math.MaxUint32)\n\tsubs := make([]*subscription, 0, sl.Count())\n\tsl.All(&subs)\n\tfor _, sub := range subs {\n\t\tfs.psim.Match(sub.subject, func(_ []byte, psi *psi) {\n\t\t\ttotal += psi.total\n\t\t\t// Keep track of start index for this subject.\n\t\t\tif psi.fblk < start {\n\t\t\t\tstart = psi.fblk\n\t\t\t}\n\t\t})\n\t}\n\t// See if we were asked for all, if so we are done.\n\tif sseq <= fs.state.FirstSeq {\n\t\treturn total, validThrough\n\t}\n\n\t// If we are here we need to calculate partials for the first blocks.\n\tfirstSubjBlk := fs.bim[start]\n\tvar firstSubjBlkFound bool\n\t// Adjust in case not found.\n\tif firstSubjBlk == nil {\n\t\tfirstSubjBlkFound = true\n\t}\n\n\t// Track how many we need to adjust against the total.\n\tvar adjust uint64\n\tfor i := 0; i <= seqStart; i++ {\n\t\tmb := fs.blks[i]\n\t\t// We can skip blks if we know they are below the first one that has any subject matches.\n\t\tif !firstSubjBlkFound {\n\t\t\tif firstSubjBlkFound = (mb == firstSubjBlk); !firstSubjBlkFound {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t}\n\t\t// We need to scan this block.\n\t\tvar shouldExpire bool\n\t\tvar updateLLTS bool\n\t\tmb.mu.Lock()\n\t\t// Check if we should include all of this block in adjusting. If so work with metadata.\n\t\tif sseq > atomic.LoadUint64(&mb.last.seq) {\n\t\t\tif isAll {\n\t\t\t\tadjust += mb.msgs\n\t\t\t} else {\n\t\t\t\t// We need to adjust for all matches in this block.\n\t\t\t\t// Make sure we have fss loaded. This loads whole block now.\n\t\t\t\tif mb.fssNotLoaded() {\n\t\t\t\t\tmb.loadMsgsWithLock()\n\t\t\t\t\tshouldExpire = true\n\t\t\t\t}\n\t\t\t\t// Mark fss activity.\n\t\t\t\tmb.lsts = ats.AccessTime()\n\t\t\t\tIntersectStree(mb.fss, sl, func(bsubj []byte, ss *SimpleState) {\n\t\t\t\t\tadjust += ss.Msgs\n\t\t\t\t})\n\t\t\t}\n\t\t} else {\n\t\t\t// This is the last block. We need to scan per message here.\n\t\t\tif mb.cacheNotLoaded() {\n\t\t\t\tmb.loadMsgsWithLock()\n\t\t\t\tshouldExpire = true\n\t\t\t}\n\t\t\tvar last = atomic.LoadUint64(&mb.last.seq)\n\t\t\tif sseq < last {\n\t\t\t\tlast = sseq\n\t\t\t}\n\t\t\t// We need to walk all messages in this block\n\t\t\tvar smv StoreMsg\n\t\t\tfor seq := atomic.LoadUint64(&mb.first.seq); seq < last; seq++ {\n\t\t\t\tif mb.dmap.Exists(seq) {\n\t\t\t\t\t// Optimisation to avoid calling cacheLookup which hits time.Now().\n\t\t\t\t\tupdateLLTS = true\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tsm, _ := mb.cacheLookupNoCopy(seq, &smv)\n\t\t\t\tif sm == nil || sm.subj == _EMPTY_ {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tupdateLLTS = false // cacheLookup already updated it.\n\t\t\t\t// Check if it matches our filter.\n\t\t\t\tif sm.seq < sseq && isMatch(sm.subj) {\n\t\t\t\t\tadjust++\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t// If we loaded the block try to force expire.\n\t\tif shouldExpire {\n\t\t\tmb.tryForceExpireCacheLocked()\n\t\t}\n\t\tif updateLLTS {\n\t\t\tmb.llts = ats.AccessTime()\n\t\t}\n\t\tmb.mu.Unlock()\n\t}\n\t// Make final adjustment.\n\ttotal -= adjust\n\n\treturn total, validThrough\n}\n\n// SubjectsTotals return message totals per subject.\nfunc (fs *fileStore) SubjectsTotals(filter string) map[string]uint64 {\n\tfs.mu.RLock()\n\tdefer fs.mu.RUnlock()\n\treturn fs.subjectsTotalsLocked(filter)\n}\n\n// Lock should be held.\nfunc (fs *fileStore) subjectsTotalsLocked(filter string) map[string]uint64 {\n\tif fs.psim.Size() == 0 {\n\t\treturn nil\n\t}\n\t// Match all if no filter given.\n\tif filter == _EMPTY_ {\n\t\tfilter = fwcs\n\t}\n\tfst := make(map[string]uint64)\n\tfs.psim.Match(stringToBytes(filter), func(subj []byte, psi *psi) {\n\t\tfst[string(subj)] = psi.total\n\t})\n\treturn fst\n}\n\n// RegisterStorageUpdates registers a callback for updates to storage changes.\n// It will present number of messages and bytes as a signed integer and an\n// optional sequence number of the message if a single.\nfunc (fs *fileStore) RegisterStorageUpdates(cb StorageUpdateHandler) {\n\tfs.mu.Lock()\n\tfs.scb = cb\n\tbsz := fs.state.Bytes\n\tfs.mu.Unlock()\n\tif cb != nil && bsz > 0 {\n\t\tcb(0, int64(bsz), 0, _EMPTY_)\n\t}\n}\n\n// RegisterStorageRemoveMsg registers a callback to remove messages.\n// Replicated streams should propose removals, R1 can remove inline.\nfunc (fs *fileStore) RegisterStorageRemoveMsg(cb StorageRemoveMsgHandler) {\n\tfs.mu.Lock()\n\tfs.rmcb = cb\n\tfs.mu.Unlock()\n}\n\n// RegisterSubjectDeleteMarkerUpdates registers a callback for updates to new tombstones.\nfunc (fs *fileStore) RegisterSubjectDeleteMarkerUpdates(cb SubjectDeleteMarkerUpdateHandler) {\n\tfs.mu.Lock()\n\tfs.sdmcb = cb\n\tfs.mu.Unlock()\n}\n\n// Helper to get hash key for specific message block.\n// Lock should be held\nfunc (fs *fileStore) hashKeyForBlock(index uint32) []byte {\n\treturn []byte(fmt.Sprintf(\"%s-%d\", fs.cfg.Name, index))\n}\n\nfunc (mb *msgBlock) setupWriteCache(buf []byte) {\n\t// Make sure we have a cache setup.\n\tif mb.cache != nil {\n\t\treturn\n\t}\n\n\t// Setup simple cache.\n\tmb.cache = &cache{buf: buf}\n\t// Make sure we set the proper cache offset if we have existing data.\n\tvar fi os.FileInfo\n\tif mb.mfd != nil {\n\t\tfi, _ = mb.mfd.Stat()\n\t} else if mb.mfn != _EMPTY_ {\n\t\tfi, _ = os.Stat(mb.mfn)\n\t}\n\tif fi != nil {\n\t\tmb.cache.off = int(fi.Size())\n\t}\n\tmb.llts = ats.AccessTime()\n\tmb.startCacheExpireTimer()\n}\n\n// This rolls to a new append msg block.\n// Lock should be held.\nfunc (fs *fileStore) newMsgBlockForWrite() (*msgBlock, error) {\n\tindex := uint32(1)\n\tvar rbuf []byte\n\n\tif lmb := fs.lmb; lmb != nil {\n\t\tindex = lmb.index + 1\n\t\t// Determine if we can reclaim any resources here.\n\t\tif fs.fip {\n\t\t\tlmb.mu.Lock()\n\t\t\tlmb.closeFDsLocked()\n\t\t\tif lmb.cache != nil {\n\t\t\t\t// Reset write timestamp and see if we can expire this cache.\n\t\t\t\trbuf = lmb.tryExpireWriteCache()\n\t\t\t}\n\t\t\tlmb.mu.Unlock()\n\t\t}\n\t}\n\n\tmb := fs.initMsgBlock(index)\n\t// Lock should be held to quiet race detector.\n\tmb.mu.Lock()\n\tmb.setupWriteCache(rbuf)\n\tmb.fss = stree.NewSubjectTree[SimpleState]()\n\n\t// Set cache time to creation time to start.\n\tmb.llts, mb.lwts = 0, ats.AccessTime()\n\t// Remember our last sequence number.\n\tatomic.StoreUint64(&mb.first.seq, fs.state.LastSeq+1)\n\tatomic.StoreUint64(&mb.last.seq, fs.state.LastSeq)\n\tmb.mu.Unlock()\n\n\t// Now do local hash.\n\tkey := sha256.Sum256(fs.hashKeyForBlock(index))\n\thh, err := highwayhash.New64(key[:])\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"could not create hash: %v\", err)\n\t}\n\tmb.hh = hh\n\n\t<-dios\n\tmfd, err := os.OpenFile(mb.mfn, os.O_CREATE|os.O_RDWR, defaultFilePerms)\n\tdios <- struct{}{}\n\n\tif err != nil {\n\t\tif isPermissionError(err) {\n\t\t\treturn nil, err\n\t\t}\n\t\tmb.dirtyCloseWithRemove(true)\n\t\treturn nil, fmt.Errorf(\"Error creating msg block file: %v\", err)\n\t}\n\tmb.mfd = mfd\n\n\t// Check if encryption is enabled.\n\tif fs.prf != nil {\n\t\tif err := fs.genEncryptionKeysForBlock(mb); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\t// If we know we will need this so go ahead and spin up.\n\tif !fs.fip {\n\t\tmb.spinUpFlushLoop()\n\t}\n\n\t// Add to our list of blocks and mark as last.\n\tfs.addMsgBlock(mb)\n\n\treturn mb, nil\n}\n\n// Generate the keys for this message block and write them out.\nfunc (fs *fileStore) genEncryptionKeysForBlock(mb *msgBlock) error {\n\tif mb == nil {\n\t\treturn nil\n\t}\n\tkey, bek, seed, encrypted, err := fs.genEncryptionKeys(fmt.Sprintf(\"%s:%d\", fs.cfg.Name, mb.index))\n\tif err != nil {\n\t\treturn err\n\t}\n\tmb.aek, mb.bek, mb.seed, mb.nonce = key, bek, seed, encrypted[:key.NonceSize()]\n\tmdir := filepath.Join(fs.fcfg.StoreDir, msgDir)\n\tkeyFile := filepath.Join(mdir, fmt.Sprintf(keyScan, mb.index))\n\tif _, err := os.Stat(keyFile); err != nil && !os.IsNotExist(err) {\n\t\treturn err\n\t}\n\terr = fs.writeFileWithOptionalSync(keyFile, encrypted, defaultFilePerms)\n\tif err != nil {\n\t\treturn err\n\t}\n\tmb.kfn = keyFile\n\treturn nil\n}\n\n// Stores a raw message with expected sequence number and timestamp.\n// Lock should be held.\nfunc (fs *fileStore) storeRawMsg(subj string, hdr, msg []byte, seq uint64, ts, ttl int64) (err error) {\n\tif fs.closed {\n\t\treturn ErrStoreClosed\n\t}\n\n\t// Per subject max check needed.\n\tmmp := uint64(fs.cfg.MaxMsgsPer)\n\tvar psmc uint64\n\tpsmax := mmp > 0 && len(subj) > 0\n\tif psmax {\n\t\tif info, ok := fs.psim.Find(stringToBytes(subj)); ok {\n\t\t\tpsmc = info.total\n\t\t}\n\t}\n\n\tvar fseq uint64\n\t// Check if we are discarding new messages when we reach the limit.\n\tif fs.cfg.Discard == DiscardNew {\n\t\tvar asl bool\n\t\tif psmax && psmc >= mmp {\n\t\t\t// If we are instructed to discard new per subject, this is an error.\n\t\t\tif fs.cfg.DiscardNewPer {\n\t\t\t\treturn ErrMaxMsgsPerSubject\n\t\t\t}\n\t\t\tif fseq, err = fs.firstSeqForSubj(subj); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tasl = true\n\t\t}\n\t\t// If we are discard new and limits policy and clustered, we do the enforcement\n\t\t// above and should not disqualify the message here since it could cause replicas to drift.\n\t\tif fs.cfg.Retention == LimitsPolicy || fs.cfg.Replicas == 1 {\n\t\t\tif fs.cfg.MaxMsgs > 0 && fs.state.Msgs >= uint64(fs.cfg.MaxMsgs) && !asl {\n\t\t\t\treturn ErrMaxMsgs\n\t\t\t}\n\t\t\tif fs.cfg.MaxBytes > 0 && fs.state.Bytes+fileStoreMsgSize(subj, hdr, msg) >= uint64(fs.cfg.MaxBytes) {\n\t\t\t\tif !asl || fs.sizeForSeq(fseq) <= int(fileStoreMsgSize(subj, hdr, msg)) {\n\t\t\t\t\treturn ErrMaxBytes\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\t// Check sequence.\n\tif seq != fs.state.LastSeq+1 {\n\t\tif seq > 0 {\n\t\t\treturn ErrSequenceMismatch\n\t\t}\n\t\tseq = fs.state.LastSeq + 1\n\t}\n\n\t// Write msg record.\n\t// Add expiry bit to sequence if needed. This is so that if we need to\n\t// rebuild, we know which messages to look at more quickly.\n\tn, err := fs.writeMsgRecord(seq, ts, subj, hdr, msg)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Adjust top level tracking of per subject msg counts.\n\tif len(subj) > 0 && fs.psim != nil {\n\t\tindex := fs.lmb.index\n\t\tif info, ok := fs.psim.Find(stringToBytes(subj)); ok {\n\t\t\tinfo.total++\n\t\t\tif index > info.lblk {\n\t\t\t\tinfo.lblk = index\n\t\t\t}\n\t\t} else {\n\t\t\tfs.psim.Insert(stringToBytes(subj), psi{total: 1, fblk: index, lblk: index})\n\t\t\tfs.tsl += len(subj)\n\t\t}\n\t}\n\n\t// Adjust first if needed.\n\tnow := time.Unix(0, ts).UTC()\n\tif fs.state.Msgs == 0 {\n\t\tfs.state.FirstSeq = seq\n\t\tfs.state.FirstTime = now\n\t}\n\n\tfs.state.Msgs++\n\tfs.state.Bytes += n\n\tfs.state.LastSeq = seq\n\tfs.state.LastTime = now\n\n\t// Enforce per message limits.\n\t// We snapshotted psmc before our actual write, so >= comparison needed.\n\tif psmax && psmc >= mmp {\n\t\t// We may have done this above.\n\t\tif fseq == 0 {\n\t\t\tfseq, _ = fs.firstSeqForSubj(subj)\n\t\t}\n\t\tif ok, _ := fs.removeMsgViaLimits(fseq); ok {\n\t\t\t// Make sure we are below the limit.\n\t\t\tif psmc--; psmc >= mmp {\n\t\t\t\tbsubj := stringToBytes(subj)\n\t\t\t\tfor info, ok := fs.psim.Find(bsubj); ok && info.total > mmp; info, ok = fs.psim.Find(bsubj) {\n\t\t\t\t\tif seq, _ := fs.firstSeqForSubj(subj); seq > 0 {\n\t\t\t\t\t\tif ok, _ := fs.removeMsgViaLimits(seq); !ok {\n\t\t\t\t\t\t\tbreak\n\t\t\t\t\t\t}\n\t\t\t\t\t} else {\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t} else if mb := fs.selectMsgBlock(fseq); mb != nil {\n\t\t\t// If we are here we could not remove fseq from above, so rebuild.\n\t\t\tvar ld *LostStreamData\n\t\t\tif ld, _, _ = mb.rebuildState(); ld != nil {\n\t\t\t\tfs.rebuildStateLocked(ld)\n\t\t\t}\n\t\t}\n\t}\n\n\t// Limits checks and enforcement.\n\t// If they do any deletions they will update the\n\t// byte count on their own, so no need to compensate.\n\tfs.enforceMsgLimit()\n\tfs.enforceBytesLimit()\n\n\t// Per-message TTL.\n\tif fs.ttls != nil && ttl > 0 {\n\t\texpires := time.Duration(ts) + (time.Second * time.Duration(ttl))\n\t\tfs.ttls.Add(seq, int64(expires))\n\t\tfs.lmb.ttls++\n\t}\n\n\t// Check if we have and need the age expiration timer running.\n\tswitch {\n\tcase fs.ttls != nil && ttl > 0:\n\t\tfs.resetAgeChk(0)\n\tcase fs.ageChk == nil && (fs.cfg.MaxAge > 0 || fs.ttls != nil):\n\t\tfs.startAgeChk()\n\t}\n\n\treturn nil\n}\n\n// StoreRawMsg stores a raw message with expected sequence number and timestamp.\nfunc (fs *fileStore) StoreRawMsg(subj string, hdr, msg []byte, seq uint64, ts, ttl int64) error {\n\tfs.mu.Lock()\n\terr := fs.storeRawMsg(subj, hdr, msg, seq, ts, ttl)\n\tcb := fs.scb\n\t// Check if first message timestamp requires expiry\n\t// sooner than initial replica expiry timer set to MaxAge when initializing.\n\tif !fs.receivedAny && fs.cfg.MaxAge != 0 && ts > 0 {\n\t\tfs.receivedAny = true\n\t\t// don't block here by calling expireMsgs directly.\n\t\t// Instead, set short timeout.\n\t\tfs.resetAgeChk(int64(time.Millisecond * 50))\n\t}\n\tfs.mu.Unlock()\n\n\tif err == nil && cb != nil {\n\t\tcb(1, int64(fileStoreMsgSize(subj, hdr, msg)), seq, subj)\n\t}\n\n\treturn err\n}\n\n// Store stores a message. We hold the main filestore lock for any write operation.\nfunc (fs *fileStore) StoreMsg(subj string, hdr, msg []byte, ttl int64) (uint64, int64, error) {\n\tfs.mu.Lock()\n\tseq, ts := fs.state.LastSeq+1, time.Now().UnixNano()\n\terr := fs.storeRawMsg(subj, hdr, msg, seq, ts, ttl)\n\tcb := fs.scb\n\tfs.mu.Unlock()\n\n\tif err != nil {\n\t\tseq, ts = 0, 0\n\t} else if cb != nil {\n\t\tcb(1, int64(fileStoreMsgSize(subj, hdr, msg)), seq, subj)\n\t}\n\n\treturn seq, ts, err\n}\n\n// skipMsg will update this message block for a skipped message.\n// If we do not have any messages, just update the metadata, otherwise\n// we will place an empty record marking the sequence as used. The\n// sequence will be marked erased.\n// fs lock should be held.\nfunc (mb *msgBlock) skipMsg(seq uint64, now time.Time) {\n\tif mb == nil {\n\t\treturn\n\t}\n\tvar needsRecord bool\n\tnowts := ats.AccessTime()\n\n\tmb.mu.Lock()\n\t// If we are empty can just do meta.\n\tif mb.msgs == 0 {\n\t\tatomic.StoreUint64(&mb.last.seq, seq)\n\t\tmb.last.ts = nowts\n\t\tatomic.StoreUint64(&mb.first.seq, seq+1)\n\t\tmb.first.ts = nowts\n\t\tneedsRecord = mb == mb.fs.lmb\n\t\tif needsRecord && mb.rbytes > 0 {\n\t\t\t// We want to make sure since we have no messages\n\t\t\t// that we write to the beginning since we only need last one.\n\t\t\tmb.rbytes, mb.cache = 0, &cache{}\n\t\t\t// If encrypted we need to reset counter since we just keep one.\n\t\t\tif mb.bek != nil {\n\t\t\t\t// Recreate to reset counter.\n\t\t\t\tmb.bek, _ = genBlockEncryptionKey(mb.fs.fcfg.Cipher, mb.seed, mb.nonce)\n\t\t\t}\n\t\t}\n\t} else {\n\t\tneedsRecord = true\n\t\tmb.dmap.Insert(seq)\n\t}\n\tmb.mu.Unlock()\n\n\tif needsRecord {\n\t\tmb.writeMsgRecord(emptyRecordLen, seq|ebit, _EMPTY_, nil, nil, nowts, true)\n\t} else {\n\t\tmb.kickFlusher()\n\t}\n}\n\n// SkipMsg will use the next sequence number but not store anything.\nfunc (fs *fileStore) SkipMsg() uint64 {\n\tfs.mu.Lock()\n\tdefer fs.mu.Unlock()\n\n\t// Grab our current last message block.\n\tmb, err := fs.checkLastBlock(emptyRecordLen)\n\tif err != nil {\n\t\treturn 0\n\t}\n\n\t// Grab time and last seq.\n\tnow, seq := time.Now(), fs.state.LastSeq+1\n\n\t// Write skip msg.\n\tmb.skipMsg(seq, now)\n\n\t// Update fs state.\n\tfs.state.LastSeq, fs.state.LastTime = seq, now\n\tif fs.state.Msgs == 0 {\n\t\tfs.state.FirstSeq, fs.state.FirstTime = seq, now\n\t}\n\tif seq == fs.state.FirstSeq {\n\t\tfs.state.FirstSeq, fs.state.FirstTime = seq+1, now\n\t}\n\t// Mark as dirty for stream state.\n\tfs.dirty++\n\n\treturn seq\n}\n\n// Skip multiple msgs. We will determine if we can fit into current lmb or we need to create a new block.\nfunc (fs *fileStore) SkipMsgs(seq uint64, num uint64) error {\n\tfs.mu.Lock()\n\tdefer fs.mu.Unlock()\n\n\t// Check sequence matches our last sequence.\n\tif seq != fs.state.LastSeq+1 {\n\t\tif seq > 0 {\n\t\t\treturn ErrSequenceMismatch\n\t\t}\n\t\tseq = fs.state.LastSeq + 1\n\t}\n\n\t// Limit number of dmap entries\n\tconst maxDeletes = 64 * 1024\n\tmb := fs.lmb\n\n\tnumDeletes := int(num)\n\tif mb != nil {\n\t\tnumDeletes += mb.dmap.Size()\n\t}\n\tif mb == nil || numDeletes > maxDeletes && mb.msgs > 0 || mb.msgs > 0 && mb.blkSize()+emptyRecordLen > fs.fcfg.BlockSize {\n\t\tif mb != nil && fs.fcfg.Compression != NoCompression {\n\t\t\t// We've now reached the end of this message block, if we want\n\t\t\t// to compress blocks then now's the time to do it.\n\t\t\tgo mb.recompressOnDiskIfNeeded()\n\t\t}\n\t\tvar err error\n\t\tif mb, err = fs.newMsgBlockForWrite(); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\t// Insert into dmap all entries and place last as marker.\n\tnow := time.Now()\n\tnowts := now.UnixNano()\n\tlseq := seq + num - 1\n\n\tmb.mu.Lock()\n\t// If we are empty update meta directly.\n\tif mb.msgs == 0 {\n\t\tatomic.StoreUint64(&mb.last.seq, lseq)\n\t\tmb.last.ts = nowts\n\t\tatomic.StoreUint64(&mb.first.seq, lseq+1)\n\t\tmb.first.ts = nowts\n\t} else {\n\t\tfor ; seq <= lseq; seq++ {\n\t\t\tmb.dmap.Insert(seq)\n\t\t}\n\t}\n\tmb.mu.Unlock()\n\n\t// Write out our placeholder.\n\tmb.writeMsgRecord(emptyRecordLen, lseq|ebit, _EMPTY_, nil, nil, nowts, true)\n\n\t// Now update FS accounting.\n\t// Update fs state.\n\tfs.state.LastSeq, fs.state.LastTime = lseq, now\n\tif fs.state.Msgs == 0 {\n\t\tfs.state.FirstSeq, fs.state.FirstTime = lseq+1, now\n\t}\n\n\t// Mark as dirty for stream state.\n\tfs.dirty++\n\n\treturn nil\n}\n\n// Lock should be held.\nfunc (fs *fileStore) rebuildFirst() {\n\tif len(fs.blks) == 0 {\n\t\treturn\n\t}\n\tfmb := fs.blks[0]\n\tif fmb == nil {\n\t\treturn\n\t}\n\n\tld, _, _ := fmb.rebuildState()\n\tfmb.mu.RLock()\n\tisEmpty := fmb.msgs == 0\n\tfmb.mu.RUnlock()\n\tif isEmpty {\n\t\tfmb.mu.Lock()\n\t\tfs.removeMsgBlock(fmb)\n\t\tfmb.mu.Unlock()\n\t}\n\tfs.selectNextFirst()\n\tfs.rebuildStateLocked(ld)\n}\n\n// Optimized helper function to return first sequence.\n// subj will always be publish subject here, meaning non-wildcard.\n// We assume a fast check that this subj even exists already happened.\n// Write lock should be held.\nfunc (fs *fileStore) firstSeqForSubj(subj string) (uint64, error) {\n\tif len(fs.blks) == 0 {\n\t\treturn 0, nil\n\t}\n\n\t// See if we can optimize where we start.\n\tstart, stop := fs.blks[0].index, fs.lmb.index\n\tif info, ok := fs.psim.Find(stringToBytes(subj)); ok {\n\t\tstart, stop = info.fblk, info.lblk\n\t}\n\n\tfor i := start; i <= stop; i++ {\n\t\tmb := fs.bim[i]\n\t\tif mb == nil {\n\t\t\tcontinue\n\t\t}\n\t\t// If we need to load msgs here and we need to walk multiple blocks this\n\t\t// could tie up the upper fs lock, so release while dealing with the block.\n\t\tfs.mu.Unlock()\n\n\t\tmb.mu.Lock()\n\t\tvar shouldExpire bool\n\t\tif mb.fssNotLoaded() {\n\t\t\t// Make sure we have fss loaded.\n\t\t\tif err := mb.loadMsgsWithLock(); err != nil {\n\t\t\t\tmb.mu.Unlock()\n\t\t\t\t// Re-acquire fs lock\n\t\t\t\tfs.mu.Lock()\n\t\t\t\treturn 0, err\n\t\t\t}\n\t\t\tshouldExpire = true\n\t\t}\n\t\t// Mark fss activity.\n\t\tmb.lsts = ats.AccessTime()\n\n\t\tbsubj := stringToBytes(subj)\n\t\tif ss, ok := mb.fss.Find(bsubj); ok && ss != nil {\n\t\t\t// Adjust first if it was not where we thought it should be.\n\t\t\tif i != start {\n\t\t\t\tif info, ok := fs.psim.Find(bsubj); ok {\n\t\t\t\t\tinfo.fblk = i\n\t\t\t\t}\n\t\t\t}\n\t\t\tif ss.firstNeedsUpdate || ss.lastNeedsUpdate {\n\t\t\t\tmb.recalculateForSubj(subj, ss)\n\t\t\t}\n\t\t\tmb.mu.Unlock()\n\t\t\t// Re-acquire fs lock\n\t\t\tfs.mu.Lock()\n\t\t\treturn ss.First, nil\n\t\t}\n\t\t// If we did not find it and we loaded this msgBlock try to expire as long as not the last.\n\t\tif shouldExpire {\n\t\t\t// Expire this cache before moving on.\n\t\t\tmb.tryForceExpireCacheLocked()\n\t\t}\n\t\tmb.mu.Unlock()\n\t\t// Re-acquire fs lock\n\t\tfs.mu.Lock()\n\t}\n\treturn 0, nil\n}\n\n// Will check the msg limit and drop firstSeq msg if needed.\n// Lock should be held.\nfunc (fs *fileStore) enforceMsgLimit() {\n\tif fs.cfg.Discard != DiscardOld {\n\t\treturn\n\t}\n\tif fs.cfg.MaxMsgs <= 0 || fs.state.Msgs <= uint64(fs.cfg.MaxMsgs) {\n\t\treturn\n\t}\n\tfor nmsgs := fs.state.Msgs; nmsgs > uint64(fs.cfg.MaxMsgs); nmsgs = fs.state.Msgs {\n\t\tif removed, err := fs.deleteFirstMsg(); err != nil || !removed {\n\t\t\tfs.rebuildFirst()\n\t\t\treturn\n\t\t}\n\t}\n}\n\n// Will check the bytes limit and drop msgs if needed.\n// Lock should be held.\nfunc (fs *fileStore) enforceBytesLimit() {\n\tif fs.cfg.Discard != DiscardOld {\n\t\treturn\n\t}\n\tif fs.cfg.MaxBytes <= 0 || fs.state.Bytes <= uint64(fs.cfg.MaxBytes) {\n\t\treturn\n\t}\n\tfor bs := fs.state.Bytes; bs > uint64(fs.cfg.MaxBytes); bs = fs.state.Bytes {\n\t\tif removed, err := fs.deleteFirstMsg(); err != nil || !removed {\n\t\t\tfs.rebuildFirst()\n\t\t\treturn\n\t\t}\n\t}\n}\n\n// Will make sure we have limits honored for max msgs per subject on recovery or config update.\n// We will make sure to go through all msg blocks etc. but in practice this\n// will most likely only be the last one, so can take a more conservative approach.\n// Lock should be held.\nfunc (fs *fileStore) enforceMsgPerSubjectLimit(fireCallback bool) {\n\tstart := time.Now()\n\tdefer func() {\n\t\tif took := time.Since(start); took > time.Minute {\n\t\t\tfs.warn(\"enforceMsgPerSubjectLimit took %v\", took.Round(time.Millisecond))\n\t\t}\n\t}()\n\n\tmaxMsgsPer := uint64(fs.cfg.MaxMsgsPer)\n\n\t// We may want to suppress callbacks from remove during this process\n\t// since these should have already been deleted and accounted for.\n\tif !fireCallback {\n\t\tcb := fs.scb\n\t\tfs.scb = nil\n\t\tdefer func() { fs.scb = cb }()\n\t}\n\n\tvar numMsgs uint64\n\n\t// collect all that are not correct.\n\tneedAttention := stree.NewSubjectTree[uint64]()\n\tfblk, lblk := uint32(math.MaxUint32), uint32(0)\n\tfs.psim.IterFast(func(subj []byte, psi *psi) bool {\n\t\tnumMsgs += psi.total\n\t\tif psi.total > maxMsgsPer {\n\t\t\tneedAttention.Insert(subj, psi.total)\n\t\t\tif psi.fblk < fblk {\n\t\t\t\tfblk = psi.fblk\n\t\t\t}\n\t\t\tif psi.lblk > lblk {\n\t\t\t\tlblk = psi.lblk\n\t\t\t}\n\t\t}\n\t\treturn true\n\t})\n\n\t// We had an issue with a use case where psim (and hence fss) were correct but idx was not and was not properly being caught.\n\t// So do a quick sanity check here. If we detect a skew do a rebuild then re-check.\n\tif numMsgs != fs.state.Msgs {\n\t\tfs.warn(\"Detected skew in subject-based total (%d) vs raw total (%d), rebuilding\", numMsgs, fs.state.Msgs)\n\t\t// Clear any global subject state.\n\t\tfs.psim, fs.tsl = fs.psim.Empty(), 0\n\t\tfor _, mb := range fs.blks {\n\t\t\tld, _, err := mb.rebuildState()\n\t\t\tif err != nil && ld != nil {\n\t\t\t\tfs.addLostData(ld)\n\t\t\t}\n\t\t\tfs.populateGlobalPerSubjectInfo(mb)\n\t\t}\n\t\t// Rebuild fs state too.\n\t\tfs.rebuildStateLocked(nil)\n\t\t// Need to redo blocks that need attention.\n\t\tneedAttention.Empty()\n\t\tfblk, lblk = uint32(math.MaxUint32), uint32(0)\n\t\tfs.psim.IterFast(func(subj []byte, psi *psi) bool {\n\t\t\tif psi.total > maxMsgsPer {\n\t\t\t\tneedAttention.Insert(subj, psi.total)\n\t\t\t\tif psi.fblk < fblk {\n\t\t\t\t\tfblk = psi.fblk\n\t\t\t\t}\n\t\t\t\tif psi.lblk > lblk {\n\t\t\t\t\tlblk = psi.lblk\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn true\n\t\t})\n\t}\n\n\t// If nothing to do then stop.\n\tif fblk == math.MaxUint32 {\n\t\treturn\n\t}\n\n\t// Collect all the msgBlks we alter.\n\tblks := make(map[*msgBlock]struct{})\n\n\t// For re-use below.\n\tvar sm StoreMsg\n\tvar fss *stree.SubjectTree[*SimpleState]\n\tfor i := fblk; i <= lblk; i++ {\n\t\tmb := fs.bim[i]\n\t\tif mb == nil {\n\t\t\tcontinue\n\t\t}\n\t\tmb.mu.Lock()\n\t\tmb.ensurePerSubjectInfoLoaded()\n\t\t// It isn't safe to intersect mb.fss directly, because removeMsgViaLimits modifies it\n\t\t// during the iteration, which can cause us to miss keys. We won't copy the entire\n\t\t// SimpleState structs though but rather just take pointers for speed.\n\t\tfss = fss.Empty()\n\t\tmb.fss.IterFast(func(subject []byte, val *SimpleState) bool {\n\t\t\tfss.Insert(subject, val)\n\t\t\treturn true\n\t\t})\n\t\tmb.mu.Unlock()\n\t\tstree.LazyIntersect(needAttention, fss, func(subj []byte, total *uint64, ssptr **SimpleState) {\n\t\t\tif ssptr == nil || total == nil {\n\t\t\t\treturn\n\t\t\t}\n\t\t\tss := *ssptr\n\t\t\tif ss.firstNeedsUpdate || ss.lastNeedsUpdate {\n\t\t\t\tmb.mu.Lock()\n\t\t\t\tmb.recalculateForSubj(bytesToString(subj), ss)\n\t\t\t\tmb.mu.Unlock()\n\t\t\t}\n\t\t\tfor first := ss.First; *total > maxMsgsPer && first <= ss.Last; {\n\t\t\t\tm, _, err := mb.firstMatching(bytesToString(subj), false, first, &sm)\n\t\t\t\tif err != nil {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t\tfirst = m.seq + 1\n\t\t\t\tif removed, _ := fs.removeMsgViaLimits(m.seq); removed {\n\t\t\t\t\tblks[mb] = struct{}{}\n\t\t\t\t\t*total--\n\t\t\t\t}\n\t\t\t}\n\t\t})\n\t}\n\n\t// Expire the cache if we can.\n\tfor mb := range blks {\n\t\tmb.mu.Lock()\n\t\tif mb.msgs > 0 {\n\t\t\tmb.tryForceExpireCacheLocked()\n\t\t}\n\t\tmb.mu.Unlock()\n\t}\n}\n\n// Lock should be held.\nfunc (fs *fileStore) deleteFirstMsg() (bool, error) {\n\treturn fs.removeMsgViaLimits(fs.state.FirstSeq)\n}\n\n// If we remove via limits that can always be recovered on a restart we\n// do not force the system to update the index file.\n// Lock should be held.\nfunc (fs *fileStore) removeMsgViaLimits(seq uint64) (bool, error) {\n\treturn fs.removeMsg(seq, false, true, false)\n}\n\n// RemoveMsg will remove the message from this store.\n// Will return the number of bytes removed.\nfunc (fs *fileStore) RemoveMsg(seq uint64) (bool, error) {\n\treturn fs.removeMsg(seq, false, false, true)\n}\n\nfunc (fs *fileStore) EraseMsg(seq uint64) (bool, error) {\n\treturn fs.removeMsg(seq, true, false, true)\n}\n\n// Convenience function to remove per subject tracking at the filestore level.\n// Lock should be held.\nfunc (fs *fileStore) removePerSubject(subj string) uint64 {\n\tif len(subj) == 0 || fs.psim == nil {\n\t\treturn 0\n\t}\n\t// We do not update sense of fblk here but will do so when we resolve during lookup.\n\tbsubj := stringToBytes(subj)\n\tif info, ok := fs.psim.Find(bsubj); ok {\n\t\tinfo.total--\n\t\tif info.total == 1 {\n\t\t\tinfo.fblk = info.lblk\n\t\t} else if info.total == 0 {\n\t\t\tif _, ok = fs.psim.Delete(bsubj); ok {\n\t\t\t\tfs.tsl -= len(subj)\n\t\t\t\treturn 0\n\t\t\t}\n\t\t}\n\t\treturn info.total\n\t}\n\treturn 0\n}\n\n// Remove a message, optionally rewriting the mb file.\nfunc (fs *fileStore) removeMsg(seq uint64, secure, viaLimits, needFSLock bool) (bool, error) {\n\tif seq == 0 {\n\t\treturn false, ErrStoreMsgNotFound\n\t}\n\tfsLock := func() {\n\t\tif needFSLock {\n\t\t\tfs.mu.Lock()\n\t\t}\n\t}\n\tfsUnlock := func() {\n\t\tif needFSLock {\n\t\t\tfs.mu.Unlock()\n\t\t}\n\t}\n\n\tfsLock()\n\n\tif fs.closed {\n\t\tfsUnlock()\n\t\treturn false, ErrStoreClosed\n\t}\n\tif !viaLimits && fs.sips > 0 {\n\t\tfsUnlock()\n\t\treturn false, ErrStoreSnapshotInProgress\n\t}\n\t// If in encrypted mode negate secure rewrite here.\n\tif secure && fs.prf != nil {\n\t\tsecure = false\n\t}\n\n\tmb := fs.selectMsgBlock(seq)\n\tif mb == nil {\n\t\tvar err = ErrStoreEOF\n\t\tif seq <= fs.state.LastSeq {\n\t\t\terr = ErrStoreMsgNotFound\n\t\t}\n\t\tfsUnlock()\n\t\treturn false, err\n\t}\n\n\tmb.mu.Lock()\n\n\t// See if we are closed or the sequence number is still relevant or if we know its deleted.\n\tif mb.closed || seq < atomic.LoadUint64(&mb.first.seq) || mb.dmap.Exists(seq) {\n\t\tmb.mu.Unlock()\n\t\tfsUnlock()\n\t\treturn false, nil\n\t}\n\n\t// We used to not have to load in the messages except with callbacks or the filtered subject state (which is now always on).\n\t// Now just load regardless.\n\t// TODO(dlc) - Figure out a way not to have to load it in, we need subject tracking outside main data block.\n\tif mb.cacheNotLoaded() {\n\t\tif err := mb.loadMsgsWithLock(); err != nil {\n\t\t\tmb.mu.Unlock()\n\t\t\tfsUnlock()\n\t\t\treturn false, err\n\t\t}\n\t}\n\n\tvar smv StoreMsg\n\tsm, err := mb.cacheLookupNoCopy(seq, &smv)\n\tif err != nil {\n\t\tmb.mu.Unlock()\n\t\tfsUnlock()\n\t\t// Mimic err behavior from above check to dmap. No error returned if already removed.\n\t\tif err == errDeletedMsg {\n\t\t\terr = nil\n\t\t}\n\t\treturn false, err\n\t}\n\t// Grab size\n\tmsz := fileStoreMsgSize(sm.subj, sm.hdr, sm.msg)\n\n\t// Set cache timestamp for last remove.\n\tmb.lrts = ats.AccessTime()\n\n\t// Global stats\n\tif fs.state.Msgs > 0 {\n\t\tfs.state.Msgs--\n\t}\n\tif msz < fs.state.Bytes {\n\t\tfs.state.Bytes -= msz\n\t} else {\n\t\tfs.state.Bytes = 0\n\t}\n\n\t// Now local mb updates.\n\tif mb.msgs > 0 {\n\t\tmb.msgs--\n\t}\n\tif msz < mb.bytes {\n\t\tmb.bytes -= msz\n\t} else {\n\t\tmb.bytes = 0\n\t}\n\n\t// Allow us to check compaction again.\n\tmb.noCompact = false\n\n\t// Mark as dirty for stream state.\n\tfs.dirty++\n\n\t// If we are tracking subjects here make sure we update that accounting.\n\tmb.ensurePerSubjectInfoLoaded()\n\n\t// If we are tracking multiple subjects here make sure we update that accounting.\n\tmb.removeSeqPerSubject(sm.subj, seq)\n\tfs.removePerSubject(sm.subj)\n\n\tif secure {\n\t\t// Grab record info.\n\t\tri, rl, _, _ := mb.slotInfo(int(seq - mb.cache.fseq))\n\t\tif err := mb.eraseMsg(seq, int(ri), int(rl)); err != nil {\n\t\t\treturn false, err\n\t\t}\n\t}\n\n\tfifo := seq == atomic.LoadUint64(&mb.first.seq)\n\tisLastBlock := mb == fs.lmb\n\tisEmpty := mb.msgs == 0\n\n\tif fifo {\n\t\tmb.selectNextFirst()\n\t\tif !isEmpty {\n\t\t\t// Can update this one in place.\n\t\t\tif seq == fs.state.FirstSeq {\n\t\t\t\tfs.state.FirstSeq = atomic.LoadUint64(&mb.first.seq) // new one.\n\t\t\t\tif mb.first.ts == 0 {\n\t\t\t\t\tfs.state.FirstTime = time.Time{}\n\t\t\t\t} else {\n\t\t\t\t\tfs.state.FirstTime = time.Unix(0, mb.first.ts).UTC()\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t} else if !isEmpty {\n\t\t// Out of order delete.\n\t\tmb.dmap.Insert(seq)\n\t\t// Make simple check here similar to Compact(). If we can save 50% and over a certain threshold do inline.\n\t\t// All other more thorough cleanup will happen in syncBlocks logic.\n\t\t// Note that we do not have to store empty records for the deleted, so don't use to calculate.\n\t\t// TODO(dlc) - This should not be inline, should kick the sync routine.\n\t\tif !isLastBlock && mb.shouldCompactInline() {\n\t\t\tmb.compact()\n\t\t}\n\t}\n\n\tif secure {\n\t\tif ld, _ := mb.flushPendingMsgsLocked(); ld != nil {\n\t\t\t// We have the mb lock here, this needs the mb locks so do in its own go routine.\n\t\t\tgo fs.rebuildState(ld)\n\t\t}\n\t}\n\n\t// If empty remove this block and check if we need to update first sequence.\n\t// We will write a tombstone at the end.\n\tvar firstSeqNeedsUpdate bool\n\tif isEmpty {\n\t\t// This writes tombstone iff mb == lmb, so no need to do below.\n\t\tfs.removeMsgBlock(mb)\n\t\tfirstSeqNeedsUpdate = seq == fs.state.FirstSeq\n\t}\n\tmb.mu.Unlock()\n\n\t// If we emptied the current message block and the seq was state.FirstSeq\n\t// then we need to jump message blocks. We will also write the index so\n\t// we don't lose track of the first sequence.\n\tif firstSeqNeedsUpdate {\n\t\tfs.selectNextFirst()\n\t}\n\n\t// Check if we need to write a deleted record tombstone.\n\t// This is for user initiated removes or to hold the first seq\n\t// when the last block is empty.\n\n\t// If not via limits and not empty (empty writes tombstone above if last) write tombstone.\n\tif !viaLimits && !isEmpty && sm != nil {\n\t\tfs.writeTombstone(sm.seq, sm.ts)\n\t}\n\n\tif cb := fs.scb; cb != nil {\n\t\t// If we have a callback registered we need to release lock regardless since cb might need it to lookup msg, etc.\n\t\tfs.mu.Unlock()\n\t\t// Storage updates.\n\t\tif cb != nil {\n\t\t\tvar subj string\n\t\t\tif sm != nil {\n\t\t\t\tsubj = sm.subj\n\t\t\t}\n\t\t\tdelta := int64(msz)\n\t\t\tcb(-1, -delta, seq, subj)\n\t\t}\n\n\t\tif !needFSLock {\n\t\t\tfs.mu.Lock()\n\t\t}\n\t} else if needFSLock {\n\t\t// We acquired it so release it.\n\t\tfs.mu.Unlock()\n\t}\n\n\treturn true, nil\n}\n\n// Tests whether we should try to compact this block while inline removing msgs.\n// We will want rbytes to be over the minimum and have a 2x potential savings.\n// If we compacted before but rbytes didn't improve much, guard against constantly compacting.\n// Lock should be held.\nfunc (mb *msgBlock) shouldCompactInline() bool {\n\treturn mb.rbytes > compactMinimum && mb.bytes*2 < mb.rbytes && (mb.cbytes == 0 || mb.bytes*2 < mb.cbytes)\n}\n\n// Tests whether we should try to compact this block while running periodic sync.\n// We will want rbytes to be over the minimum and have a 2x potential savings.\n// Ignores 2MB minimum.\n// Lock should be held.\nfunc (mb *msgBlock) shouldCompactSync() bool {\n\treturn mb.bytes*2 < mb.rbytes && !mb.noCompact\n}\n\n// This will compact and rewrite this block. This version will not process any tombstone cleanup.\n// Write lock needs to be held.\nfunc (mb *msgBlock) compact() {\n\tmb.compactWithFloor(0)\n}\n\n// This will compact and rewrite this block. This should only be called when we know we want to rewrite this block.\n// This should not be called on the lmb since we will prune tail deleted messages which could cause issues with\n// writing new messages. We will silently bail on any issues with the underlying block and let someone else detect.\n// if fseq > 0 we will attempt to cleanup stale tombstones.\n// Write lock needs to be held.\nfunc (mb *msgBlock) compactWithFloor(floor uint64) {\n\twasLoaded := mb.cacheAlreadyLoaded()\n\tif !wasLoaded {\n\t\tif err := mb.loadMsgsWithLock(); err != nil {\n\t\t\treturn\n\t\t}\n\t}\n\n\tbuf := mb.cache.buf\n\tnbuf := getMsgBlockBuf(len(buf))\n\t// Recycle our nbuf when we are done.\n\tdefer recycleMsgBlockBuf(nbuf)\n\n\tvar le = binary.LittleEndian\n\tvar firstSet bool\n\n\tfseq := atomic.LoadUint64(&mb.first.seq)\n\tisDeleted := func(seq uint64) bool {\n\t\treturn seq == 0 || seq&ebit != 0 || mb.dmap.Exists(seq) || seq < fseq\n\t}\n\n\tfor index, lbuf := uint32(0), uint32(len(buf)); index < lbuf; {\n\t\tif index+msgHdrSize > lbuf {\n\t\t\treturn\n\t\t}\n\t\thdr := buf[index : index+msgHdrSize]\n\t\trl, slen := le.Uint32(hdr[0:]), int(le.Uint16(hdr[20:]))\n\t\t// Clear any headers bit that could be set.\n\t\trl &^= hbit\n\t\tdlen := int(rl) - msgHdrSize\n\t\t// Do some quick sanity checks here.\n\t\tif dlen < 0 || slen > (dlen-recordHashSize) || dlen > int(rl) || index+rl > lbuf || rl > rlBadThresh {\n\t\t\treturn\n\t\t}\n\t\t// Only need to process non-deleted messages.\n\t\tseq := le.Uint64(hdr[4:])\n\n\t\tif !isDeleted(seq) {\n\t\t\t// Check for tombstones.\n\t\t\tif seq&tbit != 0 {\n\t\t\t\tseq = seq &^ tbit\n\t\t\t\t// If this entry is for a lower seq than ours then keep around.\n\t\t\t\t// We also check that it is greater than our floor. Floor is zero on normal\n\t\t\t\t// calls to compact.\n\t\t\t\tif seq < fseq && seq >= floor {\n\t\t\t\t\tnbuf = append(nbuf, buf[index:index+rl]...)\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\t// Normal message here.\n\t\t\t\tnbuf = append(nbuf, buf[index:index+rl]...)\n\t\t\t\tif !firstSet {\n\t\t\t\t\tfirstSet = true\n\t\t\t\t\tatomic.StoreUint64(&mb.first.seq, seq)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t// Advance to next record.\n\t\tindex += rl\n\t}\n\n\t// Handle compression\n\tif mb.cmp != NoCompression && len(nbuf) > 0 {\n\t\tcbuf, err := mb.cmp.Compress(nbuf)\n\t\tif err != nil {\n\t\t\treturn\n\t\t}\n\t\tmeta := &CompressionInfo{\n\t\t\tAlgorithm:    mb.cmp,\n\t\t\tOriginalSize: uint64(len(nbuf)),\n\t\t}\n\t\tnbuf = append(meta.MarshalMetadata(), cbuf...)\n\t}\n\n\t// Check for encryption.\n\tif mb.bek != nil && len(nbuf) > 0 {\n\t\t// Recreate to reset counter.\n\t\trbek, err := genBlockEncryptionKey(mb.fs.fcfg.Cipher, mb.seed, mb.nonce)\n\t\tif err != nil {\n\t\t\treturn\n\t\t}\n\t\trbek.XORKeyStream(nbuf, nbuf)\n\t}\n\n\t// Close FDs first.\n\tmb.closeFDsLocked()\n\n\t// We will write to a new file and mv/rename it in case of failure.\n\tmfn := filepath.Join(mb.fs.fcfg.StoreDir, msgDir, fmt.Sprintf(newScan, mb.index))\n\t<-dios\n\terr := os.WriteFile(mfn, nbuf, defaultFilePerms)\n\tdios <- struct{}{}\n\tif err != nil {\n\t\tos.Remove(mfn)\n\t\treturn\n\t}\n\tif err := os.Rename(mfn, mb.mfn); err != nil {\n\t\tos.Remove(mfn)\n\t\treturn\n\t}\n\n\t// Make sure to sync\n\tmb.needSync = true\n\n\t// Capture the updated rbytes.\n\tif rbytes := uint64(len(nbuf)); rbytes == mb.rbytes {\n\t\t// No change, so set our noCompact bool here to avoid attempting to continually compress in syncBlocks.\n\t\tmb.noCompact = true\n\t} else {\n\t\tmb.rbytes = rbytes\n\t}\n\tmb.cbytes = mb.bytes\n\n\t// Remove any seqs from the beginning of the blk.\n\tfor seq, nfseq := fseq, atomic.LoadUint64(&mb.first.seq); seq < nfseq; seq++ {\n\t\tmb.dmap.Delete(seq)\n\t}\n\t// Make sure we clear the cache since no longer valid.\n\tmb.clearCacheAndOffset()\n\t// If we entered with the msgs loaded make sure to reload them.\n\tif wasLoaded {\n\t\tmb.loadMsgsWithLock()\n\t}\n}\n\n// Grab info from a slot.\n// Lock should be held.\nfunc (mb *msgBlock) slotInfo(slot int) (uint32, uint32, bool, error) {\n\tif mb.cache == nil || slot >= len(mb.cache.idx) {\n\t\treturn 0, 0, false, errPartialCache\n\t}\n\n\tbi := mb.cache.idx[slot]\n\tri, hashChecked := (bi &^ cbit), (bi&cbit) != 0\n\n\t// If this is a deleted slot return here.\n\tif bi == dbit {\n\t\treturn 0, 0, false, errDeletedMsg\n\t}\n\n\t// Determine record length\n\tvar rl uint32\n\tif slot >= len(mb.cache.idx) {\n\t\trl = mb.cache.lrl\n\t} else {\n\t\t// Need to account for dbit markers in idx.\n\t\t// So we will walk until we find valid idx slot to calculate rl.\n\t\tfor i := 1; slot+i < len(mb.cache.idx); i++ {\n\t\t\tni := mb.cache.idx[slot+i] &^ cbit\n\t\t\tif ni == dbit {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\trl = ni - ri\n\t\t\tbreak\n\t\t}\n\t\t// check if we had all trailing dbits.\n\t\t// If so use len of cache buf minus ri.\n\t\tif rl == 0 {\n\t\t\trl = uint32(len(mb.cache.buf)) - ri\n\t\t}\n\t}\n\tif rl < msgHdrSize {\n\t\treturn 0, 0, false, errBadMsg\n\t}\n\treturn uint32(ri), rl, hashChecked, nil\n}\n\nfunc (fs *fileStore) isClosed() bool {\n\tfs.mu.RLock()\n\tclosed := fs.closed\n\tfs.mu.RUnlock()\n\treturn closed\n}\n\n// Will spin up our flush loop.\nfunc (mb *msgBlock) spinUpFlushLoop() {\n\tmb.mu.Lock()\n\tdefer mb.mu.Unlock()\n\tmb.spinUpFlushLoopLocked()\n}\n\n// Will spin up our flush loop.\n// Lock should be held.\nfunc (mb *msgBlock) spinUpFlushLoopLocked() {\n\t// Are we already running or closed?\n\tif mb.flusher || mb.closed {\n\t\treturn\n\t}\n\tmb.flusher = true\n\tmb.fch = make(chan struct{}, 1)\n\tmb.qch = make(chan struct{})\n\tfch, qch := mb.fch, mb.qch\n\n\tgo mb.flushLoop(fch, qch)\n}\n\n// Raw low level kicker for flush loops.\nfunc kickFlusher(fch chan struct{}) {\n\tif fch != nil {\n\t\tselect {\n\t\tcase fch <- struct{}{}:\n\t\tdefault:\n\t\t}\n\t}\n}\n\n// Kick flusher for this message block.\nfunc (mb *msgBlock) kickFlusher() {\n\tmb.mu.RLock()\n\tdefer mb.mu.RUnlock()\n\tkickFlusher(mb.fch)\n}\n\nfunc (mb *msgBlock) setInFlusher() {\n\tmb.mu.Lock()\n\tmb.flusher = true\n\tmb.mu.Unlock()\n}\n\nfunc (mb *msgBlock) clearInFlusher() {\n\tmb.mu.Lock()\n\tmb.flusher = false\n\tmb.mu.Unlock()\n}\n\n// flushLoop watches for messages, index info, or recently closed msg block updates.\nfunc (mb *msgBlock) flushLoop(fch, qch chan struct{}) {\n\tmb.setInFlusher()\n\tdefer mb.clearInFlusher()\n\n\tfor {\n\t\tselect {\n\t\tcase <-fch:\n\t\t\t// If we have pending messages process them first.\n\t\t\tif waiting := mb.pendingWriteSize(); waiting != 0 {\n\t\t\t\tts := 1 * time.Millisecond\n\t\t\t\tvar waited time.Duration\n\n\t\t\t\tfor waiting < coalesceMinimum {\n\t\t\t\t\ttime.Sleep(ts)\n\t\t\t\t\tselect {\n\t\t\t\t\tcase <-qch:\n\t\t\t\t\t\treturn\n\t\t\t\t\tdefault:\n\t\t\t\t\t}\n\t\t\t\t\tnewWaiting := mb.pendingWriteSize()\n\t\t\t\t\tif waited = waited + ts; waited > maxFlushWait || newWaiting <= waiting {\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\t\t\t\t\twaiting = newWaiting\n\t\t\t\t\tts *= 2\n\t\t\t\t}\n\t\t\t\tmb.flushPendingMsgs()\n\t\t\t\t// Check if we are no longer the last message block. If we are\n\t\t\t\t// not we can close FDs and exit.\n\t\t\t\tmb.fs.mu.RLock()\n\t\t\t\tnotLast := mb != mb.fs.lmb\n\t\t\t\tmb.fs.mu.RUnlock()\n\t\t\t\tif notLast {\n\t\t\t\t\tif err := mb.closeFDs(); err == nil {\n\t\t\t\t\t\treturn\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\tcase <-qch:\n\t\t\treturn\n\t\t}\n\t}\n}\n\n// Lock should be held.\nfunc (mb *msgBlock) eraseMsg(seq uint64, ri, rl int) error {\n\tvar le = binary.LittleEndian\n\tvar hdr [msgHdrSize]byte\n\n\tle.PutUint32(hdr[0:], uint32(rl))\n\tle.PutUint64(hdr[4:], seq|ebit)\n\tle.PutUint64(hdr[12:], 0)\n\tle.PutUint16(hdr[20:], 0)\n\n\t// Randomize record\n\tdata := make([]byte, rl-emptyRecordLen)\n\tif n, err := rand.Read(data); err != nil {\n\t\treturn err\n\t} else if n != len(data) {\n\t\treturn fmt.Errorf(\"not enough overwrite bytes read (%d != %d)\", n, len(data))\n\t}\n\n\t// Now write to underlying buffer.\n\tvar b bytes.Buffer\n\tb.Write(hdr[:])\n\tb.Write(data)\n\n\t// Calculate hash.\n\tmb.hh.Reset()\n\tmb.hh.Write(hdr[4:20])\n\tmb.hh.Write(data)\n\tchecksum := mb.hh.Sum(nil)\n\t// Write to msg record.\n\tb.Write(checksum)\n\n\t// Update both cache and disk.\n\tnbytes := b.Bytes()\n\n\t// Cache\n\tif ri >= mb.cache.off {\n\t\tli := ri - mb.cache.off\n\t\tbuf := mb.cache.buf[li : li+rl]\n\t\tcopy(buf, nbytes)\n\t}\n\n\t// Disk\n\tif mb.cache.off+mb.cache.wp > ri {\n\t\t<-dios\n\t\tmfd, err := os.OpenFile(mb.mfn, os.O_RDWR, defaultFilePerms)\n\t\tdios <- struct{}{}\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tdefer mfd.Close()\n\t\tif _, err = mfd.WriteAt(nbytes, int64(ri)); err == nil {\n\t\t\tmfd.Sync()\n\t\t}\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn nil\n}\n\n// Truncate this message block to the storedMsg.\nfunc (mb *msgBlock) truncate(sm *StoreMsg) (nmsgs, nbytes uint64, err error) {\n\tmb.mu.Lock()\n\tdefer mb.mu.Unlock()\n\n\t// Make sure we are loaded to process messages etc.\n\tif err := mb.loadMsgsWithLock(); err != nil {\n\t\treturn 0, 0, err\n\t}\n\n\t// Calculate new eof using slot info from our new last sm.\n\tri, rl, _, err := mb.slotInfo(int(sm.seq - mb.cache.fseq))\n\tif err != nil {\n\t\treturn 0, 0, err\n\t}\n\t// Calculate new eof.\n\teof := int64(ri + rl)\n\n\tvar purged, bytes uint64\n\n\tcheckDmap := mb.dmap.Size() > 0\n\tvar smv StoreMsg\n\n\tfor seq := atomic.LoadUint64(&mb.last.seq); seq > sm.seq; seq-- {\n\t\tif checkDmap {\n\t\t\tif mb.dmap.Exists(seq) {\n\t\t\t\t// Delete and skip to next.\n\t\t\t\tmb.dmap.Delete(seq)\n\t\t\t\tcheckDmap = !mb.dmap.IsEmpty()\n\t\t\t\tcontinue\n\t\t\t}\n\t\t}\n\t\t// We should have a valid msg to calculate removal stats.\n\t\tif m, err := mb.cacheLookupNoCopy(seq, &smv); err == nil {\n\t\t\tif mb.msgs > 0 {\n\t\t\t\trl := fileStoreMsgSize(m.subj, m.hdr, m.msg)\n\t\t\t\tmb.msgs--\n\t\t\t\tif rl > mb.bytes {\n\t\t\t\t\trl = mb.bytes\n\t\t\t\t}\n\t\t\t\tmb.bytes -= rl\n\t\t\t\tmb.rbytes -= rl\n\t\t\t\t// For return accounting.\n\t\t\t\tpurged++\n\t\t\t\tbytes += uint64(rl)\n\t\t\t}\n\t\t}\n\t}\n\n\t// If the block is compressed then we have to load it into memory\n\t// and decompress it, truncate it and then write it back out.\n\t// Otherwise, truncate the file itself and close the descriptor.\n\tif mb.cmp != NoCompression {\n\t\tbuf, err := mb.loadBlock(nil)\n\t\tif err != nil {\n\t\t\treturn 0, 0, fmt.Errorf(\"failed to load block from disk: %w\", err)\n\t\t}\n\t\tif mb.bek != nil && len(buf) > 0 {\n\t\t\tbek, err := genBlockEncryptionKey(mb.fs.fcfg.Cipher, mb.seed, mb.nonce)\n\t\t\tif err != nil {\n\t\t\t\treturn 0, 0, err\n\t\t\t}\n\t\t\tmb.bek = bek\n\t\t\tmb.bek.XORKeyStream(buf, buf)\n\t\t}\n\t\tbuf, err = mb.decompressIfNeeded(buf)\n\t\tif err != nil {\n\t\t\treturn 0, 0, fmt.Errorf(\"failed to decompress block: %w\", err)\n\t\t}\n\t\tbuf = buf[:eof]\n\t\tcopy(mb.lchk[0:], buf[:len(buf)-checksumSize])\n\t\tbuf, err = mb.cmp.Compress(buf)\n\t\tif err != nil {\n\t\t\treturn 0, 0, fmt.Errorf(\"failed to recompress block: %w\", err)\n\t\t}\n\t\tmeta := &CompressionInfo{\n\t\t\tAlgorithm:    mb.cmp,\n\t\t\tOriginalSize: uint64(eof),\n\t\t}\n\t\tbuf = append(meta.MarshalMetadata(), buf...)\n\t\tif mb.bek != nil && len(buf) > 0 {\n\t\t\tbek, err := genBlockEncryptionKey(mb.fs.fcfg.Cipher, mb.seed, mb.nonce)\n\t\t\tif err != nil {\n\t\t\t\treturn 0, 0, err\n\t\t\t}\n\t\t\tmb.bek = bek\n\t\t\tmb.bek.XORKeyStream(buf, buf)\n\t\t}\n\t\tn, err := mb.writeAt(buf, 0)\n\t\tif err != nil {\n\t\t\treturn 0, 0, fmt.Errorf(\"failed to rewrite compressed block: %w\", err)\n\t\t}\n\t\tif n != len(buf) {\n\t\t\treturn 0, 0, fmt.Errorf(\"short write (%d != %d)\", n, len(buf))\n\t\t}\n\t\tmb.mfd.Truncate(int64(len(buf)))\n\t\tmb.mfd.Sync()\n\t} else if mb.mfd != nil {\n\t\tmb.mfd.Truncate(eof)\n\t\tmb.mfd.Sync()\n\t\t// Update our checksum.\n\t\tvar lchk [8]byte\n\t\tmb.mfd.ReadAt(lchk[:], eof-8)\n\t\tcopy(mb.lchk[0:], lchk[:])\n\t} else {\n\t\treturn 0, 0, fmt.Errorf(\"failed to truncate msg block %d, file not open\", mb.index)\n\t}\n\n\t// Update our last msg.\n\tatomic.StoreUint64(&mb.last.seq, sm.seq)\n\tmb.last.ts = sm.ts\n\n\t// Clear our cache.\n\tmb.clearCacheAndOffset()\n\n\t// Redo per subject info for this block.\n\tmb.resetPerSubjectInfo()\n\n\t// Load msgs again.\n\tmb.loadMsgsWithLock()\n\n\treturn purged, bytes, nil\n}\n\n// Helper to determine if the mb is empty.\nfunc (mb *msgBlock) isEmpty() bool {\n\treturn atomic.LoadUint64(&mb.first.seq) > atomic.LoadUint64(&mb.last.seq)\n}\n\n// Lock should be held.\nfunc (mb *msgBlock) selectNextFirst() {\n\tvar seq uint64\n\tfseq, lseq := atomic.LoadUint64(&mb.first.seq), atomic.LoadUint64(&mb.last.seq)\n\tfor seq = fseq + 1; seq <= lseq; seq++ {\n\t\tif mb.dmap.Exists(seq) {\n\t\t\t// We will move past this so we can delete the entry.\n\t\t\tmb.dmap.Delete(seq)\n\t\t} else {\n\t\t\tbreak\n\t\t}\n\t}\n\t// Set new first sequence.\n\tatomic.StoreUint64(&mb.first.seq, seq)\n\n\t// Check if we are empty..\n\tif seq > lseq {\n\t\tmb.first.ts = 0\n\t\treturn\n\t}\n\n\t// Need to get the timestamp.\n\t// We will try the cache direct and fallback if needed.\n\tvar smv StoreMsg\n\tsm, _ := mb.cacheLookupNoCopy(seq, &smv)\n\tif sm == nil {\n\t\t// Slow path, need to unlock.\n\t\tmb.mu.Unlock()\n\t\tsm, _, _ = mb.fetchMsgNoCopy(seq, &smv)\n\t\tmb.mu.Lock()\n\t}\n\tif sm != nil {\n\t\tmb.first.ts = sm.ts\n\t} else {\n\t\tmb.first.ts = 0\n\t}\n}\n\n// Select the next FirstSeq\n// Lock should be held.\nfunc (fs *fileStore) selectNextFirst() {\n\tif len(fs.blks) > 0 {\n\t\tmb := fs.blks[0]\n\t\tmb.mu.RLock()\n\t\tfs.state.FirstSeq = atomic.LoadUint64(&mb.first.seq)\n\t\tfs.state.FirstTime = time.Unix(0, mb.first.ts).UTC()\n\t\tmb.mu.RUnlock()\n\t} else {\n\t\t// Could not find anything, so treat like purge\n\t\tfs.state.FirstSeq = fs.state.LastSeq + 1\n\t\tfs.state.FirstTime = time.Time{}\n\t}\n\t// Mark first as moved. Plays into tombstone cleanup for syncBlocks.\n\tfs.firstMoved = true\n}\n\n// Lock should be held.\nfunc (mb *msgBlock) resetCacheExpireTimer(td time.Duration) {\n\tif td == 0 {\n\t\ttd = mb.cexp + 100*time.Millisecond\n\t}\n\tif mb.ctmr == nil {\n\t\tmb.ctmr = time.AfterFunc(td, mb.expireCache)\n\t} else {\n\t\tmb.ctmr.Reset(td)\n\t}\n}\n\n// Lock should be held.\nfunc (mb *msgBlock) startCacheExpireTimer() {\n\tmb.resetCacheExpireTimer(0)\n}\n\n// Used when we load in a message block.\n// Lock should be held.\nfunc (mb *msgBlock) clearCacheAndOffset() {\n\t// Reset linear scan tracker.\n\tmb.llseq = 0\n\tif mb.cache != nil {\n\t\tmb.cache.off = 0\n\t\tmb.cache.wp = 0\n\t}\n\tmb.clearCache()\n}\n\n// Lock should be held.\nfunc (mb *msgBlock) clearCache() {\n\tif mb.ctmr != nil {\n\t\ttsla := mb.sinceLastActivity()\n\t\tif mb.fss == nil || tsla > mb.fexp {\n\t\t\t// Force\n\t\t\tmb.fss = nil\n\t\t\tmb.ctmr.Stop()\n\t\t\tmb.ctmr = nil\n\t\t} else {\n\t\t\tmb.resetCacheExpireTimer(mb.fexp - tsla)\n\t\t}\n\t}\n\n\tif mb.cache == nil {\n\t\treturn\n\t}\n\n\tbuf := mb.cache.buf\n\tif mb.cache.off == 0 {\n\t\tmb.cache = nil\n\t} else {\n\t\t// Clear msgs and index.\n\t\tmb.cache.buf = nil\n\t\tmb.cache.idx = nil\n\t\tmb.cache.wp = 0\n\t}\n\trecycleMsgBlockBuf(buf)\n}\n\n// Called to possibly expire a message block cache.\nfunc (mb *msgBlock) expireCache() {\n\tmb.mu.Lock()\n\tdefer mb.mu.Unlock()\n\tmb.expireCacheLocked()\n}\n\nfunc (mb *msgBlock) tryForceExpireCache() {\n\tmb.mu.Lock()\n\tdefer mb.mu.Unlock()\n\tmb.tryForceExpireCacheLocked()\n}\n\n// We will attempt to force expire this by temporarily clearing the last load time.\nfunc (mb *msgBlock) tryForceExpireCacheLocked() {\n\tllts := mb.llts\n\tmb.llts = 0\n\tmb.expireCacheLocked()\n\tmb.llts = llts\n}\n\n// This is for expiration of the write cache, which will be partial with fip.\n// So we want to bypass the Pools here.\n// Lock should be held.\nfunc (mb *msgBlock) tryExpireWriteCache() []byte {\n\tif mb.cache == nil {\n\t\treturn nil\n\t}\n\tlwts, buf, llts, nra := mb.lwts, mb.cache.buf, mb.llts, mb.cache.nra\n\tmb.lwts, mb.cache.nra = 0, true\n\tmb.expireCacheLocked()\n\tmb.lwts = lwts\n\tif mb.cache != nil {\n\t\tmb.cache.nra = nra\n\t}\n\t// We could check for a certain time since last load, but to be safe just reuse if no loads at all.\n\tif llts == 0 && (mb.cache == nil || mb.cache.buf == nil) {\n\t\t// Clear last write time since we now are about to move on to a new lmb.\n\t\tmb.lwts = 0\n\t\treturn buf[:0]\n\t}\n\treturn nil\n}\n\n// Lock should be held.\nfunc (mb *msgBlock) expireCacheLocked() {\n\tif mb.cache == nil && mb.fss == nil {\n\t\tif mb.ctmr != nil {\n\t\t\tmb.ctmr.Stop()\n\t\t\tmb.ctmr = nil\n\t\t}\n\t\treturn\n\t}\n\n\t// Can't expire if we still have pending.\n\tif mb.cache != nil && len(mb.cache.buf)-int(mb.cache.wp) > 0 {\n\t\tmb.resetCacheExpireTimer(mb.cexp)\n\t\treturn\n\t}\n\n\t// Grab timestamp to compare.\n\ttns := ats.AccessTime()\n\n\t// For the core buffer of messages, we care about reads and writes, but not removes.\n\tbufts := mb.llts\n\tif mb.lwts > bufts {\n\t\tbufts = mb.lwts\n\t}\n\n\t// Check for activity on the cache that would prevent us from expiring.\n\tif tns-bufts <= int64(mb.cexp) {\n\t\tmb.resetCacheExpireTimer(mb.cexp - time.Duration(tns-bufts))\n\t\treturn\n\t}\n\n\t// If we are here we will at least expire the core msg buffer.\n\t// We need to capture offset in case we do a write next before a full load.\n\tif mb.cache != nil {\n\t\tmb.cache.off += len(mb.cache.buf)\n\t\tif !mb.cache.nra {\n\t\t\trecycleMsgBlockBuf(mb.cache.buf)\n\t\t}\n\t\tmb.cache.buf = nil\n\t\tmb.cache.wp = 0\n\t}\n\n\t// Check if we can clear out our idx unless under force expire.\n\t// fss we keep longer and expire under sync timer checks.\n\tmb.clearCache()\n}\n\nfunc (fs *fileStore) startAgeChk() {\n\tif fs.ageChk != nil {\n\t\treturn\n\t}\n\tif fs.cfg.MaxAge != 0 || fs.ttls != nil {\n\t\tfs.ageChk = time.AfterFunc(fs.cfg.MaxAge, fs.expireMsgs)\n\t}\n}\n\n// Lock should be held.\nfunc (fs *fileStore) resetAgeChk(delta int64) {\n\tvar next int64 = math.MaxInt64\n\tif fs.ttls != nil {\n\t\tnext = fs.ttls.GetNextExpiration(next)\n\t}\n\n\t// If there's no MaxAge and there's nothing waiting to be expired then\n\t// don't bother continuing. The next storeRawMsg() will wake us up if\n\t// needs be.\n\tif fs.cfg.MaxAge <= 0 && next == math.MaxInt64 {\n\t\tclearTimer(&fs.ageChk)\n\t\treturn\n\t}\n\n\t// Check to see if we should be firing sooner than MaxAge for an expiring TTL.\n\tfireIn := fs.cfg.MaxAge\n\n\t// If delta for next-to-expire message is unset, but we still have messages to remove.\n\t// Assume messages are removed through proposals, and we need to speed up subsequent age check.\n\tif delta == 0 && fs.state.Msgs > 0 {\n\t\tif until := 2 * time.Second; until < fireIn {\n\t\t\tfireIn = until\n\t\t}\n\t}\n\n\tif next < math.MaxInt64 {\n\t\t// Looks like there's a next expiration, use it either if there's no\n\t\t// MaxAge set or if it looks to be sooner than MaxAge is.\n\t\tif until := time.Until(time.Unix(0, next)); fireIn == 0 || until < fireIn {\n\t\t\tfireIn = until\n\t\t}\n\t}\n\n\t// If not then look at the delta provided (usually gap to next age expiry).\n\tif delta > 0 {\n\t\tif fireIn == 0 || time.Duration(delta) < fireIn {\n\t\t\tfireIn = time.Duration(delta)\n\t\t}\n\t}\n\n\t// Make sure we aren't firing too often either way, otherwise we can\n\t// negatively impact stream ingest performance.\n\tif fireIn < 250*time.Millisecond {\n\t\tfireIn = 250 * time.Millisecond\n\t}\n\n\tif fs.ageChk != nil {\n\t\tfs.ageChk.Reset(fireIn)\n\t} else {\n\t\tfs.ageChk = time.AfterFunc(fireIn, fs.expireMsgs)\n\t}\n}\n\n// Lock should be held.\nfunc (fs *fileStore) cancelAgeChk() {\n\tif fs.ageChk != nil {\n\t\tfs.ageChk.Stop()\n\t\tfs.ageChk = nil\n\t}\n}\n\n// Will expire msgs that are too old.\nfunc (fs *fileStore) expireMsgs() {\n\t// We need to delete one by one here and can not optimize for the time being.\n\t// Reason is that we need more information to adjust ack pending in consumers.\n\tvar smv StoreMsg\n\tvar sm *StoreMsg\n\n\tfs.mu.RLock()\n\tmaxAge := int64(fs.cfg.MaxAge)\n\tminAge := ats.AccessTime() - maxAge\n\trmcb := fs.rmcb\n\tsdmcb := fs.sdmcb\n\tsdmTTL := int64(fs.cfg.SubjectDeleteMarkerTTL.Seconds())\n\tsdmEnabled := sdmTTL > 0\n\tfs.mu.RUnlock()\n\n\tif sdmEnabled && (rmcb == nil || sdmcb == nil) {\n\t\treturn\n\t}\n\n\tif maxAge > 0 {\n\t\tvar seq uint64\n\t\tfor sm, seq, _ = fs.LoadNextMsg(fwcs, true, 0, &smv); sm != nil && sm.ts <= minAge; sm, seq, _ = fs.LoadNextMsg(fwcs, true, seq+1, &smv) {\n\t\t\tif len(sm.hdr) > 0 {\n\t\t\t\tif ttl, err := getMessageTTL(sm.hdr); err == nil && ttl < 0 {\n\t\t\t\t\t// The message has a negative TTL, therefore it must \"never expire\".\n\t\t\t\t\tminAge = ats.AccessTime() - maxAge\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t}\n\t\t\t// Remove the message and then, if LimitsTTL is enabled, try and work out\n\t\t\t// if it was the last message of that particular subject that we just deleted.\n\t\t\tif sdmEnabled {\n\t\t\t\tif last, ok := fs.shouldProcessSdm(seq, sm.subj); ok {\n\t\t\t\t\tsdm := last && len(getHeader(JSMarkerReason, sm.hdr)) == 0\n\t\t\t\t\tfs.handleRemovalOrSdm(seq, sm.subj, sdm, sdmTTL)\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tfs.mu.Lock()\n\t\t\t\tfs.removeMsgViaLimits(sm.seq)\n\t\t\t\tfs.mu.Unlock()\n\t\t\t}\n\t\t\t// Recalculate in case we are expiring a bunch.\n\t\t\tminAge = ats.AccessTime() - maxAge\n\t\t}\n\t}\n\n\tfs.mu.Lock()\n\tdefer fs.mu.Unlock()\n\n\t// TODO: Not great that we're holding the lock here, but the timed hash wheel isn't thread-safe.\n\tnextTTL := int64(math.MaxInt64)\n\tvar rmSeqs []uint64\n\tvar ttlSdm map[string][]SDMBySubj\n\tif fs.ttls != nil {\n\t\tfs.ttls.ExpireTasks(func(seq uint64, ts int64) bool {\n\t\t\t// Need to grab subject for the specified sequence if for SDM, and check\n\t\t\t// if the message hasn't been removed in the meantime.\n\t\t\tsm, _ = fs.msgForSeqLocked(seq, &smv, false)\n\t\t\tif sm == nil {\n\t\t\t\treturn true\n\t\t\t}\n\n\t\t\tif sdmEnabled {\n\t\t\t\tif ttlSdm == nil {\n\t\t\t\t\tttlSdm = make(map[string][]SDMBySubj, 1)\n\t\t\t\t}\n\t\t\t\tttlSdm[sm.subj] = append(ttlSdm[sm.subj], SDMBySubj{seq, len(getHeader(JSMarkerReason, sm.hdr)) != 0})\n\t\t\t} else {\n\t\t\t\t// Collect sequences to remove. Don't remove messages inline here,\n\t\t\t\t// as that releases the lock and THW is not thread-safe.\n\t\t\t\trmSeqs = append(rmSeqs, seq)\n\t\t\t}\n\t\t\t// Removing messages out of band, those can fail, and we can be shutdown halfway\n\t\t\t// through so don't remove from THW just yet.\n\t\t\treturn false\n\t\t})\n\t\tif maxAge > 0 {\n\t\t\t// Only check if we're expiring something in the next MaxAge interval, saves us a bit\n\t\t\t// of work if MaxAge will beat us to the next expiry anyway.\n\t\t\tnextTTL = fs.ttls.GetNextExpiration(time.Now().Add(time.Duration(maxAge)).UnixNano())\n\t\t} else {\n\t\t\tnextTTL = fs.ttls.GetNextExpiration(math.MaxInt64)\n\t\t}\n\t}\n\n\t// Remove messages collected by THW.\n\tfor _, seq := range rmSeqs {\n\t\tfs.removeMsg(seq, false, false, false)\n\t}\n\n\t// THW is unordered, so must sort by sequence and must not be holding the lock.\n\tif len(ttlSdm) > 0 {\n\t\tfs.mu.Unlock()\n\t\tfor subj, es := range ttlSdm {\n\t\t\tslices.SortFunc(es, func(a, b SDMBySubj) int {\n\t\t\t\tif a.seq == b.seq {\n\t\t\t\t\treturn 0\n\t\t\t\t} else if a.seq < b.seq {\n\t\t\t\t\treturn -1\n\t\t\t\t} else {\n\t\t\t\t\treturn 1\n\t\t\t\t}\n\t\t\t})\n\t\t\tfor _, e := range es {\n\t\t\t\tif last, ok := fs.shouldProcessSdm(e.seq, subj); ok {\n\t\t\t\t\tsdm := last && !e.sdm\n\t\t\t\t\tfs.handleRemovalOrSdm(e.seq, subj, sdm, sdmTTL)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tfs.mu.Lock()\n\t}\n\n\t// Only cancel if no message left, not on potential lookup error that would result in sm == nil.\n\tif fs.state.Msgs == 0 && nextTTL == math.MaxInt64 {\n\t\tfs.cancelAgeChk()\n\t} else {\n\t\tif sm == nil {\n\t\t\tfs.resetAgeChk(0)\n\t\t} else {\n\t\t\tfs.resetAgeChk(sm.ts - minAge)\n\t\t}\n\t}\n}\n\nfunc (fs *fileStore) shouldProcessSdm(seq uint64, subj string) (bool, bool) {\n\tfs.mu.Lock()\n\tdefer fs.mu.Unlock()\n\n\tif fs.sdm == nil {\n\t\tfs.sdm = newSDMMeta()\n\t}\n\n\tif p, ok := fs.sdm.pending[seq]; ok {\n\t\t// If we're about to use the cached value, and we knew it was last before,\n\t\t// quickly check that we don't have more remaining messages for the subject now.\n\t\t// Which means we are not the last anymore and must reset to not remove later data.\n\t\tif p.last {\n\t\t\tmsgs := fs.subjectsTotalsLocked(subj)[subj]\n\t\t\tnumPending := fs.sdm.totals[subj]\n\t\t\tif remaining := msgs - numPending; remaining > 0 {\n\t\t\t\tp.last = false\n\t\t\t}\n\t\t}\n\n\t\t// Don't allow more proposals for the same sequence if we already did recently.\n\t\tif time.Since(time.Unix(0, p.ts)) < 2*time.Second {\n\t\t\treturn p.last, false\n\t\t}\n\t\tfs.sdm.pending[seq] = SDMBySeq{p.last, time.Now().UnixNano()}\n\t\treturn p.last, true\n\t}\n\n\tmsgs := fs.subjectsTotalsLocked(subj)[subj]\n\tif msgs == 0 {\n\t\treturn false, true\n\t}\n\tnumPending := fs.sdm.totals[subj]\n\tremaining := msgs - numPending\n\treturn fs.sdm.trackPending(seq, subj, remaining == 1), true\n}\n\nfunc (fs *fileStore) handleRemovalOrSdm(seq uint64, subj string, sdm bool, sdmTTL int64) {\n\tif sdm {\n\t\tvar _hdr [128]byte\n\t\thdr := fmt.Appendf(\n\t\t\t_hdr[:0],\n\t\t\t\"NATS/1.0\\r\\n%s: %s\\r\\n%s: %s\\r\\n%s: %s\\r\\n\\r\\n\",\n\t\t\tJSMarkerReason, JSMarkerReasonMaxAge,\n\t\t\tJSMessageTTL, time.Duration(sdmTTL)*time.Second,\n\t\t\tJSMsgRollup, JSMsgRollupSubject,\n\t\t)\n\t\tmsg := &inMsg{\n\t\t\tsubj: subj,\n\t\t\thdr:  hdr,\n\t\t}\n\t\tfs.sdmcb(msg)\n\t} else {\n\t\tfs.rmcb(seq)\n\t}\n}\n\n// Lock should be held.\nfunc (fs *fileStore) checkAndFlushAllBlocks() {\n\tfor _, mb := range fs.blks {\n\t\tif mb.pendingWriteSize() > 0 {\n\t\t\t// Since fs lock is held need to pull this apart in case we need to rebuild state.\n\t\t\tmb.mu.Lock()\n\t\t\tld, _ := mb.flushPendingMsgsLocked()\n\t\t\tmb.mu.Unlock()\n\t\t\tif ld != nil {\n\t\t\t\tfs.rebuildStateLocked(ld)\n\t\t\t}\n\t\t}\n\t}\n}\n\n// This will check all the checksums on messages and report back any sequence numbers with errors.\nfunc (fs *fileStore) checkMsgs() *LostStreamData {\n\tfs.mu.Lock()\n\tdefer fs.mu.Unlock()\n\n\tfs.checkAndFlushAllBlocks()\n\n\t// Clear any global subject state.\n\tfs.psim, fs.tsl = fs.psim.Empty(), 0\n\n\tfor _, mb := range fs.blks {\n\t\t// Make sure encryption loaded if needed for the block.\n\t\tfs.loadEncryptionForMsgBlock(mb)\n\t\t// FIXME(dlc) - check tombstones here too?\n\t\tif ld, _, err := mb.rebuildState(); err != nil && ld != nil {\n\t\t\t// Rebuild fs state too.\n\t\t\tfs.rebuildStateLocked(ld)\n\t\t}\n\t\tfs.populateGlobalPerSubjectInfo(mb)\n\t}\n\n\treturn fs.ld\n}\n\n// Lock should be held.\nfunc (mb *msgBlock) enableForWriting(fip bool) error {\n\tif mb == nil {\n\t\treturn errNoMsgBlk\n\t}\n\tif mb.mfd != nil {\n\t\treturn nil\n\t}\n\t<-dios\n\tmfd, err := os.OpenFile(mb.mfn, os.O_CREATE|os.O_RDWR, defaultFilePerms)\n\tdios <- struct{}{}\n\tif err != nil {\n\t\treturn fmt.Errorf(\"error opening msg block file [%q]: %v\", mb.mfn, err)\n\t}\n\tmb.mfd = mfd\n\n\t// Spin up our flusher loop if needed.\n\tif !fip {\n\t\tmb.spinUpFlushLoopLocked()\n\t}\n\n\treturn nil\n}\n\n// Helper function to place a delete tombstone.\nfunc (mb *msgBlock) writeTombstone(seq uint64, ts int64) error {\n\treturn mb.writeMsgRecord(emptyRecordLen, seq|tbit, _EMPTY_, nil, nil, ts, true)\n}\n\n// Helper function to place a delete tombstone without flush.\n// Lock should not be held.\nfunc (mb *msgBlock) writeTombstoneNoFlush(seq uint64, ts int64) error {\n\tmb.mu.Lock()\n\tdefer mb.mu.Unlock()\n\treturn mb.writeMsgRecordLocked(emptyRecordLen, seq|tbit, _EMPTY_, nil, nil, ts, false, false)\n}\n\n// Will write the message record to the underlying message block.\n// filestore lock will be held.\nfunc (mb *msgBlock) writeMsgRecord(rl, seq uint64, subj string, mhdr, msg []byte, ts int64, flush bool) error {\n\tmb.mu.Lock()\n\tdefer mb.mu.Unlock()\n\treturn mb.writeMsgRecordLocked(rl, seq, subj, mhdr, msg, ts, flush, true)\n}\n\n// Will write the message record to the underlying message block.\n// filestore lock will be held.\n// mb lock should be held.\nfunc (mb *msgBlock) writeMsgRecordLocked(rl, seq uint64, subj string, mhdr, msg []byte, ts int64, flush, kick bool) error {\n\t// Enable for writing if our mfd is not open.\n\tif mb.mfd == nil {\n\t\tif err := mb.enableForWriting(flush && kick); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\t// Make sure we have a cache setup.\n\tif mb.cache == nil {\n\t\tmb.setupWriteCache(nil)\n\t}\n\n\t// Check if we are tracking per subject for our simple state.\n\t// Do this before changing the cache that would trigger a flush pending msgs call\n\t// if we needed to regenerate the per subject info.\n\t// Note that tombstones have no subject so will not trigger here.\n\tif len(subj) > 0 && !mb.noTrack {\n\t\tif err := mb.ensurePerSubjectInfoLoaded(); err != nil {\n\t\t\treturn err\n\t\t}\n\t\t// Mark fss activity.\n\t\tmb.lsts = ats.AccessTime()\n\t\tif ss, ok := mb.fss.Find(stringToBytes(subj)); ok && ss != nil {\n\t\t\tss.Msgs++\n\t\t\tss.Last = seq\n\t\t\tss.lastNeedsUpdate = false\n\t\t} else {\n\t\t\tmb.fss.Insert(stringToBytes(subj), SimpleState{Msgs: 1, First: seq, Last: seq})\n\t\t}\n\t}\n\n\t// Indexing\n\tindex := len(mb.cache.buf) + int(mb.cache.off)\n\n\t// Formats\n\t// Format with no header\n\t// total_len(4) sequence(8) timestamp(8) subj_len(2) subj msg hash(8)\n\t// With headers, high bit on total length will be set.\n\t// total_len(4) sequence(8) timestamp(8) subj_len(2) subj hdr_len(4) hdr msg hash(8)\n\n\tvar le = binary.LittleEndian\n\n\tl := uint32(rl)\n\thasHeaders := len(mhdr) > 0\n\tif hasHeaders {\n\t\tl |= hbit\n\t}\n\n\t// Reserve space for the header on the underlying buffer.\n\tmb.cache.buf = append(mb.cache.buf, make([]byte, msgHdrSize)...)\n\thdr := mb.cache.buf[len(mb.cache.buf)-msgHdrSize : len(mb.cache.buf)]\n\tle.PutUint32(hdr[0:], l)\n\tle.PutUint64(hdr[4:], seq)\n\tle.PutUint64(hdr[12:], uint64(ts))\n\tle.PutUint16(hdr[20:], uint16(len(subj)))\n\n\t// Now write to underlying buffer.\n\tmb.cache.buf = append(mb.cache.buf, subj...)\n\n\tif hasHeaders {\n\t\tvar hlen [4]byte\n\t\tle.PutUint32(hlen[0:], uint32(len(mhdr)))\n\t\tmb.cache.buf = append(mb.cache.buf, hlen[:]...)\n\t\tmb.cache.buf = append(mb.cache.buf, mhdr...)\n\t}\n\tmb.cache.buf = append(mb.cache.buf, msg...)\n\n\t// Calculate hash.\n\tmb.hh.Reset()\n\tmb.hh.Write(hdr[4:20])\n\tmb.hh.Write(stringToBytes(subj))\n\tif hasHeaders {\n\t\tmb.hh.Write(mhdr)\n\t}\n\tmb.hh.Write(msg)\n\tchecksum := mb.hh.Sum(mb.lchk[:0:highwayhash.Size64])\n\tcopy(mb.lchk[0:], checksum)\n\n\t// Update write through cache.\n\t// Write to msg record.\n\tmb.cache.buf = append(mb.cache.buf, checksum...)\n\tmb.cache.lrl = uint32(rl)\n\n\t// Set cache timestamp for last store.\n\tmb.lwts = ts\n\n\t// Only update index and do accounting if not a delete tombstone.\n\tif seq&tbit == 0 {\n\t\t// Accounting, do this before stripping ebit, it is ebit aware.\n\t\tmb.updateAccounting(seq, ts, rl)\n\t\t// Strip ebit if set.\n\t\tseq = seq &^ ebit\n\t\tif mb.cache.fseq == 0 {\n\t\t\tmb.cache.fseq = seq\n\t\t}\n\t\t// Write index\n\t\tmb.cache.idx = append(mb.cache.idx, uint32(index)|cbit)\n\t} else {\n\t\t// Make sure to account for tombstones in rbytes.\n\t\tmb.rbytes += rl\n\t}\n\n\tfch, werr := mb.fch, mb.werr\n\n\t// If we should be flushing, or had a write error, do so here.\n\tif flush || werr != nil {\n\t\tld, err := mb.flushPendingMsgsLocked()\n\t\tif ld != nil && mb.fs != nil {\n\t\t\t// We have the mb lock here, this needs the mb locks so do in its own go routine.\n\t\t\tgo mb.fs.rebuildState(ld)\n\t\t}\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t} else if kick {\n\t\t// Kick the flusher here.\n\t\tkickFlusher(fch)\n\t}\n\n\treturn nil\n}\n\n// How many bytes pending to be written for this message block.\nfunc (mb *msgBlock) pendingWriteSize() int {\n\tif mb == nil {\n\t\treturn 0\n\t}\n\tmb.mu.RLock()\n\tdefer mb.mu.RUnlock()\n\treturn mb.pendingWriteSizeLocked()\n}\n\n// How many bytes pending to be written for this message block.\nfunc (mb *msgBlock) pendingWriteSizeLocked() int {\n\tif mb == nil {\n\t\treturn 0\n\t}\n\tvar pending int\n\tif !mb.closed && mb.mfd != nil && mb.cache != nil {\n\t\tpending = len(mb.cache.buf) - int(mb.cache.wp)\n\t}\n\treturn pending\n}\n\n// Try to close our FDs if we can.\nfunc (mb *msgBlock) closeFDs() error {\n\tmb.mu.Lock()\n\tdefer mb.mu.Unlock()\n\treturn mb.closeFDsLocked()\n}\n\nfunc (mb *msgBlock) closeFDsLocked() error {\n\tif buf, _ := mb.bytesPending(); len(buf) > 0 {\n\t\treturn errPendingData\n\t}\n\tmb.closeFDsLockedNoCheck()\n\treturn nil\n}\n\nfunc (mb *msgBlock) closeFDsLockedNoCheck() {\n\tif mb.mfd != nil {\n\t\tmb.mfd.Close()\n\t\tmb.mfd = nil\n\t}\n}\n\n// bytesPending returns the buffer to be used for writing to the underlying file.\n// This marks we are in flush and will return nil if asked again until cleared.\n// Lock should be held.\nfunc (mb *msgBlock) bytesPending() ([]byte, error) {\n\tif mb == nil || mb.mfd == nil {\n\t\treturn nil, errNoPending\n\t}\n\tif mb.cache == nil {\n\t\treturn nil, errNoCache\n\t}\n\tif len(mb.cache.buf) <= mb.cache.wp {\n\t\treturn nil, errNoPending\n\t}\n\tbuf := mb.cache.buf[mb.cache.wp:]\n\tif len(buf) == 0 {\n\t\treturn nil, errNoPending\n\t}\n\treturn buf, nil\n}\n\n// Returns the current blkSize including deleted msgs etc.\nfunc (mb *msgBlock) blkSize() uint64 {\n\tif mb == nil {\n\t\treturn 0\n\t}\n\tmb.mu.RLock()\n\tnb := mb.rbytes\n\tmb.mu.RUnlock()\n\treturn nb\n}\n\n// Update accounting on a write msg.\n// Lock should be held.\nfunc (mb *msgBlock) updateAccounting(seq uint64, ts int64, rl uint64) {\n\tisDeleted := seq&ebit != 0\n\tif isDeleted {\n\t\tseq = seq &^ ebit\n\t}\n\n\tfseq := atomic.LoadUint64(&mb.first.seq)\n\tif (fseq == 0 || mb.first.ts == 0) && seq >= fseq {\n\t\tatomic.StoreUint64(&mb.first.seq, seq)\n\t\tmb.first.ts = ts\n\t}\n\t// Need atomics here for selectMsgBlock speed.\n\tatomic.StoreUint64(&mb.last.seq, seq)\n\tmb.last.ts = ts\n\tmb.rbytes += rl\n\tif !isDeleted {\n\t\tmb.bytes += rl\n\t\tmb.msgs++\n\t}\n}\n\n// Helper to check last msg block and create new one if too big.\n// Lock should be held.\nfunc (fs *fileStore) checkLastBlock(rl uint64) (lmb *msgBlock, err error) {\n\t// Grab our current last message block.\n\tlmb = fs.lmb\n\trbytes := lmb.blkSize()\n\tif lmb == nil || (rbytes > 0 && rbytes+rl > fs.fcfg.BlockSize) {\n\t\tif lmb != nil {\n\t\t\tlmb.flushPendingMsgs()\n\t\t\tif fs.fcfg.Compression != NoCompression {\n\t\t\t\t// We've now reached the end of this message block, if we want\n\t\t\t\t// to compress blocks then now's the time to do it.\n\t\t\t\tgo lmb.recompressOnDiskIfNeeded()\n\t\t\t}\n\t\t}\n\t\tif lmb, err = fs.newMsgBlockForWrite(); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\treturn lmb, nil\n}\n\n// Lock should be held.\nfunc (fs *fileStore) writeMsgRecord(seq uint64, ts int64, subj string, hdr, msg []byte) (uint64, error) {\n\t// Get size for this message.\n\trl := fileStoreMsgSize(subj, hdr, msg)\n\tif rl&hbit != 0 || rl > rlBadThresh {\n\t\treturn 0, ErrMsgTooLarge\n\t}\n\t// Grab our current last message block.\n\tmb, err := fs.checkLastBlock(rl)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\n\t// Mark as dirty for stream state.\n\tfs.dirty++\n\n\t// Ask msg block to store in write through cache.\n\terr = mb.writeMsgRecord(rl, seq, subj, hdr, msg, ts, fs.fip)\n\n\treturn rl, err\n}\n\n// For writing tombstones to our lmb. This version will enforce maximum block sizes.\n// Lock should be held.\nfunc (fs *fileStore) writeTombstone(seq uint64, ts int64) error {\n\t// Grab our current last message block.\n\tlmb, err := fs.checkLastBlock(emptyRecordLen)\n\tif err != nil {\n\t\treturn err\n\t}\n\treturn lmb.writeTombstone(seq, ts)\n}\n\n// For writing tombstones to our lmb. This version will enforce maximum block sizes.\n// This version does not flush contents.\n// Lock should be held.\nfunc (fs *fileStore) writeTombstoneNoFlush(seq uint64, ts int64) error {\n\tlmb, err := fs.checkLastBlock(emptyRecordLen)\n\tif err != nil {\n\t\treturn err\n\t}\n\t// Write tombstone without flush or kick.\n\treturn lmb.writeTombstoneNoFlush(seq, ts)\n}\n\nfunc (mb *msgBlock) recompressOnDiskIfNeeded() error {\n\talg := mb.fs.fcfg.Compression\n\tmb.mu.Lock()\n\tdefer mb.mu.Unlock()\n\n\torigFN := mb.mfn                    // The original message block on disk.\n\ttmpFN := mb.mfn + compressTmpSuffix // The compressed block will be written here.\n\n\t// Open up the file block and read in the entire contents into memory.\n\t// One of two things will happen:\n\t// 1. The block will be compressed already and have a valid metadata\n\t//    header, in which case we do nothing.\n\t// 2. The block will be uncompressed, in which case we will compress it\n\t//    and then write it back out to disk, re-encrypting if necessary.\n\t<-dios\n\torigBuf, err := os.ReadFile(origFN)\n\tdios <- struct{}{}\n\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to read original block from disk: %w\", err)\n\t}\n\n\t// If the block is encrypted then we will need to decrypt it before\n\t// doing anything. We always encrypt after compressing because then the\n\t// compression can be as efficient as possible on the raw data, whereas\n\t// the encrypted ciphertext will not compress anywhere near as well.\n\t// The block encryption also covers the optional compression metadata.\n\tif mb.bek != nil && len(origBuf) > 0 {\n\t\tbek, err := genBlockEncryptionKey(mb.fs.fcfg.Cipher, mb.seed, mb.nonce)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tmb.bek = bek\n\t\tmb.bek.XORKeyStream(origBuf, origBuf)\n\t}\n\n\tmeta := &CompressionInfo{}\n\tif _, err := meta.UnmarshalMetadata(origBuf); err != nil {\n\t\t// An error is only returned here if there's a problem with parsing\n\t\t// the metadata. If the file has no metadata at all, no error is\n\t\t// returned and the algorithm defaults to no compression.\n\t\treturn fmt.Errorf(\"failed to read existing metadata header: %w\", err)\n\t}\n\tif meta.Algorithm == alg {\n\t\t// The block is already compressed with the chosen algorithm so there\n\t\t// is nothing else to do. This is not a common case, it is here only\n\t\t// to ensure we don't do unnecessary work in case something asked us\n\t\t// to recompress an already compressed block with the same algorithm.\n\t\treturn nil\n\t} else if alg != NoCompression {\n\t\t// The block is already compressed using some algorithm, so we need\n\t\t// to decompress the block using the existing algorithm before we can\n\t\t// recompress it with the new one.\n\t\tif origBuf, err = meta.Algorithm.Decompress(origBuf); err != nil {\n\t\t\treturn fmt.Errorf(\"failed to decompress original block: %w\", err)\n\t\t}\n\t}\n\n\t// Rather than modifying the existing block on disk (which is a dangerous\n\t// operation if something goes wrong), create a new temporary file. We will\n\t// write out the new block here and then swap the files around afterwards\n\t// once everything else has succeeded correctly.\n\t<-dios\n\ttmpFD, err := os.OpenFile(tmpFN, os.O_CREATE|os.O_TRUNC|os.O_WRONLY, defaultFilePerms)\n\tdios <- struct{}{}\n\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to create temporary file: %w\", err)\n\t}\n\n\terrorCleanup := func(err error) error {\n\t\ttmpFD.Close()\n\t\tos.Remove(tmpFN)\n\t\treturn err\n\t}\n\n\t// The original buffer at this point is uncompressed, so we will now compress\n\t// it if needed. Note that if the selected algorithm is NoCompression, the\n\t// Compress function will just return the input buffer unmodified.\n\tcmpBuf, err := alg.Compress(origBuf)\n\tif err != nil {\n\t\treturn errorCleanup(fmt.Errorf(\"failed to compress block: %w\", err))\n\t}\n\n\t// We only need to write out the metadata header if compression is enabled.\n\t// If we're trying to uncompress the file on disk at this point, don't bother\n\t// writing metadata.\n\tif alg != NoCompression {\n\t\tmeta := &CompressionInfo{\n\t\t\tAlgorithm:    alg,\n\t\t\tOriginalSize: uint64(len(origBuf)),\n\t\t}\n\t\tcmpBuf = append(meta.MarshalMetadata(), cmpBuf...)\n\t}\n\n\t// Re-encrypt the block if necessary.\n\tif mb.bek != nil && len(cmpBuf) > 0 {\n\t\tbek, err := genBlockEncryptionKey(mb.fs.fcfg.Cipher, mb.seed, mb.nonce)\n\t\tif err != nil {\n\t\t\treturn errorCleanup(err)\n\t\t}\n\t\tmb.bek = bek\n\t\tmb.bek.XORKeyStream(cmpBuf, cmpBuf)\n\t}\n\n\t// Write the new block data (which might be compressed or encrypted) to the\n\t// temporary file.\n\tif n, err := tmpFD.Write(cmpBuf); err != nil {\n\t\treturn errorCleanup(fmt.Errorf(\"failed to write to temporary file: %w\", err))\n\t} else if n != len(cmpBuf) {\n\t\treturn errorCleanup(fmt.Errorf(\"short write to temporary file (%d != %d)\", n, len(cmpBuf)))\n\t}\n\tif err := tmpFD.Sync(); err != nil {\n\t\treturn errorCleanup(fmt.Errorf(\"failed to sync temporary file: %w\", err))\n\t}\n\tif err := tmpFD.Close(); err != nil {\n\t\treturn errorCleanup(fmt.Errorf(\"failed to close temporary file: %w\", err))\n\t}\n\n\t// Now replace the original file with the newly updated temp file.\n\tif err := os.Rename(tmpFN, origFN); err != nil {\n\t\treturn fmt.Errorf(\"failed to move temporary file into place: %w\", err)\n\t}\n\n\t// Since the message block might be retained in memory, make sure the\n\t// compression algorithm is up-to-date, since this will be needed when\n\t// compacting or truncating.\n\tmb.cmp = alg\n\n\t// Also update rbytes\n\tmb.rbytes = uint64(len(cmpBuf))\n\n\treturn nil\n}\n\nfunc (mb *msgBlock) decompressIfNeeded(buf []byte) ([]byte, error) {\n\tvar meta CompressionInfo\n\tif n, err := meta.UnmarshalMetadata(buf); err != nil {\n\t\t// There was a problem parsing the metadata header of the block.\n\t\t// If there's no metadata header, an error isn't returned here,\n\t\t// we will instead just use default values of no compression.\n\t\treturn nil, err\n\t} else if n == 0 {\n\t\t// There were no metadata bytes, so we assume the block is not\n\t\t// compressed and return it as-is.\n\t\treturn buf, nil\n\t} else {\n\t\t// Metadata was present so it's quite likely the block contents\n\t\t// are compressed. If by any chance the metadata claims that the\n\t\t// block is uncompressed, then the input slice is just returned\n\t\t// unmodified.\n\t\treturn meta.Algorithm.Decompress(buf[n:])\n\t}\n}\n\n// Lock should be held.\nfunc (mb *msgBlock) ensureRawBytesLoaded() error {\n\tif mb.rbytes > 0 {\n\t\treturn nil\n\t}\n\tf, err := mb.openBlock()\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer f.Close()\n\tif fi, err := f.Stat(); fi != nil && err == nil {\n\t\tmb.rbytes = uint64(fi.Size())\n\t} else {\n\t\treturn err\n\t}\n\treturn nil\n}\n\n// Sync msg and index files as needed. This is called from a timer.\nfunc (fs *fileStore) syncBlocks() {\n\tfs.mu.Lock()\n\t// If closed or a snapshot is in progress bail.\n\tif fs.closed || fs.sips > 0 {\n\t\tfs.mu.Unlock()\n\t\treturn\n\t}\n\tblks := append([]*msgBlock(nil), fs.blks...)\n\tlmb, firstMoved, firstSeq := fs.lmb, fs.firstMoved, fs.state.FirstSeq\n\t// Clear first moved.\n\tfs.firstMoved = false\n\tfs.mu.Unlock()\n\n\tvar markDirty bool\n\tfor _, mb := range blks {\n\t\t// Do actual sync. Hold lock for consistency.\n\t\tmb.mu.Lock()\n\t\tif mb.closed {\n\t\t\tmb.mu.Unlock()\n\t\t\tcontinue\n\t\t}\n\t\t// See if we can close FDs due to being idle.\n\t\tif mb.mfd != nil && mb.sinceLastWriteActivity() > closeFDsIdle {\n\t\t\tmb.dirtyCloseWithRemove(false)\n\t\t}\n\n\t\t// If our first has moved and we are set to noCompact (which is from tombstones),\n\t\t// clear so that we might cleanup tombstones.\n\t\tif firstMoved && mb.noCompact {\n\t\t\tmb.noCompact = false\n\t\t}\n\t\t// Check if we should compact here as well.\n\t\t// Do not compact last mb.\n\t\tvar needsCompact bool\n\t\tif mb != lmb && mb.ensureRawBytesLoaded() == nil && mb.shouldCompactSync() {\n\t\t\tneedsCompact = true\n\t\t\tmarkDirty = true\n\t\t}\n\n\t\t// Check if we need to sync. We will not hold lock during actual sync.\n\t\tneedSync := mb.needSync\n\t\tif needSync {\n\t\t\t// Flush anything that may be pending.\n\t\t\tmb.flushPendingMsgsLocked()\n\t\t}\n\t\tmb.mu.Unlock()\n\n\t\t// Check if we should compact here.\n\t\t// Need to hold fs lock in case we reference psim when loading in the mb and we may remove this block if truly empty.\n\t\tif needsCompact {\n\t\t\tfs.mu.RLock()\n\t\t\tmb.mu.Lock()\n\t\t\tmb.compactWithFloor(firstSeq)\n\t\t\t// If this compact removed all raw bytes due to tombstone cleanup, schedule to remove.\n\t\t\tshouldRemove := mb.rbytes == 0\n\t\t\tmb.mu.Unlock()\n\t\t\tfs.mu.RUnlock()\n\n\t\t\t// Check if we should remove. This will not be common, so we will re-take fs write lock here vs changing\n\t\t\t//  it above which we would prefer to be a readlock such that other lookups can occur while compacting this block.\n\t\t\tif shouldRemove {\n\t\t\t\tfs.mu.Lock()\n\t\t\t\tmb.mu.Lock()\n\t\t\t\tfs.removeMsgBlock(mb)\n\t\t\t\tmb.mu.Unlock()\n\t\t\t\tfs.mu.Unlock()\n\t\t\t\tneedSync = false\n\t\t\t}\n\t\t}\n\n\t\t// Check if we need to sync this block.\n\t\tif needSync {\n\t\t\tmb.mu.Lock()\n\t\t\tvar fd *os.File\n\t\t\tvar didOpen bool\n\t\t\tif mb.mfd != nil {\n\t\t\t\tfd = mb.mfd\n\t\t\t} else {\n\t\t\t\t<-dios\n\t\t\t\tfd, _ = os.OpenFile(mb.mfn, os.O_RDWR, defaultFilePerms)\n\t\t\t\tdios <- struct{}{}\n\t\t\t\tdidOpen = true\n\t\t\t}\n\t\t\t// If we have an fd.\n\t\t\tif fd != nil {\n\t\t\t\tcanClear := fd.Sync() == nil\n\t\t\t\t// If we opened the file close the fd.\n\t\t\t\tif didOpen {\n\t\t\t\t\tfd.Close()\n\t\t\t\t}\n\t\t\t\t// Only clear sync flag on success.\n\t\t\t\tif canClear {\n\t\t\t\t\tmb.needSync = false\n\t\t\t\t}\n\t\t\t}\n\t\t\tmb.mu.Unlock()\n\t\t}\n\t}\n\n\tfs.mu.Lock()\n\tif fs.closed {\n\t\tfs.mu.Unlock()\n\t\treturn\n\t}\n\tfs.setSyncTimer()\n\tif markDirty {\n\t\tfs.dirty++\n\t}\n\n\t// Sync state file if we are not running with sync always.\n\tif !fs.fcfg.SyncAlways {\n\t\tfn := filepath.Join(fs.fcfg.StoreDir, msgDir, streamStreamStateFile)\n\t\t<-dios\n\t\tfd, _ := os.OpenFile(fn, os.O_RDWR, defaultFilePerms)\n\t\tdios <- struct{}{}\n\t\tif fd != nil {\n\t\t\tfd.Sync()\n\t\t\tfd.Close()\n\t\t}\n\t}\n\tfs.mu.Unlock()\n}\n\n// Select the message block where this message should be found.\n// Return nil if not in the set.\n// Read lock should be held.\nfunc (fs *fileStore) selectMsgBlock(seq uint64) *msgBlock {\n\t_, mb := fs.selectMsgBlockWithIndex(seq)\n\treturn mb\n}\n\n// Lock should be held.\nfunc (fs *fileStore) selectMsgBlockWithIndex(seq uint64) (int, *msgBlock) {\n\t// Check for out of range.\n\tif seq < fs.state.FirstSeq || seq > fs.state.LastSeq || fs.state.Msgs == 0 {\n\t\treturn -1, nil\n\t}\n\n\tconst linearThresh = 32\n\tnb := len(fs.blks) - 1\n\n\tif nb < linearThresh {\n\t\tfor i, mb := range fs.blks {\n\t\t\tif seq <= atomic.LoadUint64(&mb.last.seq) {\n\t\t\t\treturn i, mb\n\t\t\t}\n\t\t}\n\t\treturn -1, nil\n\t}\n\n\t// Do traditional binary search here since we know the blocks are sorted by sequence first and last.\n\tfor low, high, mid := 0, nb, nb/2; low <= high; mid = (low + high) / 2 {\n\t\tmb := fs.blks[mid]\n\t\t// Right now these atomic loads do not factor in, so fine to leave. Was considering\n\t\t// uplifting these to fs scope to avoid atomic load but not needed.\n\t\tfirst, last := atomic.LoadUint64(&mb.first.seq), atomic.LoadUint64(&mb.last.seq)\n\t\tif seq > last {\n\t\t\tlow = mid + 1\n\t\t} else if seq < first {\n\t\t\t// A message block's first sequence can change here meaning we could find a gap.\n\t\t\t// We want to behave like above, which if inclusive (we check at start) should\n\t\t\t// always return an index and a valid mb.\n\t\t\t// If we have a gap then our seq would be > fs.blks[mid-1].last.seq\n\t\t\tif mid == 0 || seq > atomic.LoadUint64(&fs.blks[mid-1].last.seq) {\n\t\t\t\treturn mid, mb\n\t\t\t}\n\t\t\thigh = mid - 1\n\t\t} else {\n\t\t\treturn mid, mb\n\t\t}\n\t}\n\n\treturn -1, nil\n}\n\n// Select the message block where this message should be found.\n// Return nil if not in the set.\nfunc (fs *fileStore) selectMsgBlockForStart(minTime time.Time) *msgBlock {\n\tfs.mu.RLock()\n\tdefer fs.mu.RUnlock()\n\n\tt := minTime.UnixNano()\n\tfor _, mb := range fs.blks {\n\t\tmb.mu.RLock()\n\t\tfound := t <= mb.last.ts\n\t\tmb.mu.RUnlock()\n\t\tif found {\n\t\t\treturn mb\n\t\t}\n\t}\n\treturn nil\n}\n\n// Index a raw msg buffer.\n// Lock should be held.\nfunc (mb *msgBlock) indexCacheBuf(buf []byte) error {\n\tvar le = binary.LittleEndian\n\n\tvar fseq uint64\n\tvar idx []uint32\n\tvar index uint32\n\n\tmbFirstSeq := atomic.LoadUint64(&mb.first.seq)\n\tmbLastSeq := atomic.LoadUint64(&mb.last.seq)\n\n\t// Sanity check here since we calculate size to allocate based on this.\n\tif mbFirstSeq > (mbLastSeq + 1) { // Purged state first == last + 1\n\t\tmb.fs.warn(\"indexCacheBuf corrupt state: mb.first %d mb.last %d\", mbFirstSeq, mbLastSeq)\n\t\t// This would cause idxSz to wrap.\n\t\treturn errCorruptState\n\t}\n\n\t// Capture beginning size of dmap.\n\tdms := uint64(mb.dmap.Size())\n\tidxSz := mbLastSeq - mbFirstSeq + 1\n\n\tif mb.cache == nil {\n\t\t// Approximation, may adjust below.\n\t\tfseq = mbFirstSeq\n\t\tidx = make([]uint32, 0, idxSz)\n\t\tmb.cache = &cache{}\n\t} else {\n\t\tfseq = mb.cache.fseq\n\t\tidx = mb.cache.idx\n\t\tif len(idx) == 0 {\n\t\t\tidx = make([]uint32, 0, idxSz)\n\t\t}\n\t\tindex = uint32(len(mb.cache.buf))\n\t\tbuf = append(mb.cache.buf, buf...)\n\t}\n\n\t// Create FSS if we should track.\n\tvar popFss bool\n\tif mb.fssNotLoaded() {\n\t\tmb.fss = stree.NewSubjectTree[SimpleState]()\n\t\tpopFss = true\n\t}\n\t// Mark fss activity.\n\tmb.lsts = ats.AccessTime()\n\tmb.ttls = 0\n\n\tlbuf := uint32(len(buf))\n\tvar seq, ttls uint64\n\tvar sm StoreMsg // Used for finding TTL headers\n\n\tfor index < lbuf {\n\t\tif index+msgHdrSize > lbuf {\n\t\t\treturn errCorruptState\n\t\t}\n\t\thdr := buf[index : index+msgHdrSize]\n\t\trl, slen := le.Uint32(hdr[0:]), int(le.Uint16(hdr[20:]))\n\t\tseq = le.Uint64(hdr[4:])\n\n\t\t// Clear any headers bit that could be set.\n\t\thasHeaders := rl&hbit != 0\n\t\trl &^= hbit\n\t\tdlen := int(rl) - msgHdrSize\n\n\t\t// Do some quick sanity checks here.\n\t\tif dlen < 0 || slen > (dlen-recordHashSize) || dlen > int(rl) || index+rl > lbuf || rl > rlBadThresh {\n\t\t\tmb.fs.warn(\"indexCacheBuf corrupt record state: dlen %d slen %d index %d rl %d lbuf %d\", dlen, slen, index, rl, lbuf)\n\t\t\t// This means something is off.\n\t\t\t// TODO(dlc) - Add into bad list?\n\t\t\treturn errCorruptState\n\t\t}\n\n\t\t// Check for tombstones which we can skip in terms of indexing.\n\t\tif seq&tbit != 0 {\n\t\t\tindex += rl\n\t\t\tcontinue\n\t\t}\n\n\t\t// Clear any erase bits.\n\t\terased := seq&ebit != 0\n\t\tseq = seq &^ ebit\n\n\t\t// We defer checksum checks to individual msg cache lookups to amortorize costs and\n\t\t// not introduce latency for first message from a newly loaded block.\n\t\tif seq >= mbFirstSeq {\n\t\t\t// Track that we do not have holes.\n\t\t\tif slot := int(seq - mbFirstSeq); slot != len(idx) {\n\t\t\t\t// If we have a hole fill it.\n\t\t\t\tfor dseq := mbFirstSeq + uint64(len(idx)); dseq < seq; dseq++ {\n\t\t\t\t\tidx = append(idx, dbit)\n\t\t\t\t\tif dms == 0 {\n\t\t\t\t\t\tmb.dmap.Insert(dseq)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\t// Add to our index.\n\t\t\tidx = append(idx, index)\n\t\t\tmb.cache.lrl = uint32(rl)\n\t\t\t// Adjust if we guessed wrong.\n\t\t\tif seq != 0 && seq < fseq {\n\t\t\t\tfseq = seq\n\t\t\t}\n\n\t\t\t// Make sure our dmap has this entry if it was erased.\n\t\t\tif erased && dms == 0 {\n\t\t\t\tmb.dmap.Insert(seq)\n\t\t\t}\n\n\t\t\t// Handle FSS inline here.\n\t\t\tif popFss && slen > 0 && !mb.noTrack && !erased && !mb.dmap.Exists(seq) {\n\t\t\t\tbsubj := buf[index+msgHdrSize : index+msgHdrSize+uint32(slen)]\n\t\t\t\tif ss, ok := mb.fss.Find(bsubj); ok && ss != nil {\n\t\t\t\t\tss.Msgs++\n\t\t\t\t\tss.Last = seq\n\t\t\t\t\tss.lastNeedsUpdate = false\n\t\t\t\t} else {\n\t\t\t\t\tmb.fss.Insert(bsubj, SimpleState{\n\t\t\t\t\t\tMsgs:  1,\n\t\t\t\t\t\tFirst: seq,\n\t\t\t\t\t\tLast:  seq,\n\t\t\t\t\t})\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// Count how many TTLs we think are in this message block.\n\t\t\t// TODO(nat): Not terribly optimal...\n\t\t\tif hasHeaders {\n\t\t\t\tif fsm, err := mb.msgFromBufNoCopy(buf[index:], &sm, nil); err == nil && fsm != nil {\n\t\t\t\t\tif ttl := sliceHeader(JSMessageTTL, fsm.hdr); len(ttl) > 0 {\n\t\t\t\t\t\tttls++\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tindex += rl\n\t}\n\n\t// Track holes at the end of the block, these would be missed in the\n\t// earlier loop if we've ran out of block file to look at, but should\n\t// be easily noticed because the seq will be below the last seq from\n\t// the index.\n\tif seq > 0 && seq < mbLastSeq {\n\t\tfor dseq := seq; dseq < mbLastSeq; dseq++ {\n\t\t\tidx = append(idx, dbit)\n\t\t\tif dms == 0 {\n\t\t\t\tmb.dmap.Insert(dseq)\n\t\t\t}\n\t\t}\n\t}\n\n\tmb.cache.buf = buf\n\tmb.cache.idx = idx\n\tmb.cache.fseq = fseq\n\tmb.cache.wp += int(lbuf)\n\tmb.ttls = ttls\n\n\treturn nil\n}\n\n// flushPendingMsgs writes out any messages for this message block.\nfunc (mb *msgBlock) flushPendingMsgs() error {\n\tmb.mu.Lock()\n\tfsLostData, err := mb.flushPendingMsgsLocked()\n\tfs := mb.fs\n\tmb.mu.Unlock()\n\n\t// Signals us that we need to rebuild filestore state.\n\tif fsLostData != nil && fs != nil {\n\t\t// Rebuild fs state too.\n\t\tfs.rebuildState(fsLostData)\n\t}\n\treturn err\n}\n\n// Write function for actual data.\n// mb.mfd should not be nil.\n// Lock should held.\nfunc (mb *msgBlock) writeAt(buf []byte, woff int64) (int, error) {\n\t// Used to mock write failures.\n\tif mb.mockWriteErr {\n\t\t// Reset on trip.\n\t\tmb.mockWriteErr = false\n\t\treturn 0, errors.New(\"mock write error\")\n\t}\n\t<-dios\n\tn, err := mb.mfd.WriteAt(buf, woff)\n\tdios <- struct{}{}\n\treturn n, err\n}\n\n// flushPendingMsgsLocked writes out any messages for this message block.\n// Lock should be held.\nfunc (mb *msgBlock) flushPendingMsgsLocked() (*LostStreamData, error) {\n\t// Signals us that we need to rebuild filestore state.\n\tvar fsLostData *LostStreamData\n\n\tif mb.cache == nil || mb.mfd == nil {\n\t\treturn nil, nil\n\t}\n\n\tbuf, err := mb.bytesPending()\n\t// If we got an error back return here.\n\tif err != nil {\n\t\t// No pending data to be written is not an error.\n\t\tif err == errNoPending || err == errNoCache {\n\t\t\terr = nil\n\t\t}\n\t\treturn nil, err\n\t}\n\n\twoff := int64(mb.cache.off + mb.cache.wp)\n\tlob := len(buf)\n\n\t// TODO(dlc) - Normally we would not hold the lock across I/O so we can improve performance.\n\t// We will hold to stabilize the code base, as we have had a few anomalies with partial cache errors\n\t// under heavy load.\n\n\t// Check if we need to encrypt.\n\tif mb.bek != nil && lob > 0 {\n\t\t// Need to leave original alone.\n\t\tvar dst []byte\n\t\tif lob <= defaultLargeBlockSize {\n\t\t\tdst = getMsgBlockBuf(lob)[:lob]\n\t\t} else {\n\t\t\tdst = make([]byte, lob)\n\t\t}\n\t\tmb.bek.XORKeyStream(dst, buf)\n\t\tbuf = dst\n\t}\n\n\t// Append new data to the message block file.\n\tfor lbb := lob; lbb > 0; lbb = len(buf) {\n\t\tn, err := mb.writeAt(buf, woff)\n\t\tif err != nil {\n\t\t\tmb.dirtyCloseWithRemove(false)\n\t\t\tld, _, _ := mb.rebuildStateLocked()\n\t\t\tmb.werr = err\n\t\t\treturn ld, err\n\t\t}\n\t\t// Update our write offset.\n\t\twoff += int64(n)\n\t\t// Partial write.\n\t\tif n != lbb {\n\t\t\tbuf = buf[n:]\n\t\t} else {\n\t\t\t// Done.\n\t\t\tbreak\n\t\t}\n\t}\n\n\t// Clear any error.\n\tmb.werr = nil\n\n\t// Cache may be gone.\n\tif mb.cache == nil || mb.mfd == nil {\n\t\treturn fsLostData, mb.werr\n\t}\n\n\t// Check if we are in sync always mode.\n\tif mb.syncAlways {\n\t\tmb.mfd.Sync()\n\t} else {\n\t\tmb.needSync = true\n\t}\n\n\t// Check for additional writes while we were writing to the disk.\n\tmoreBytes := len(mb.cache.buf) - mb.cache.wp - lob\n\n\t// Decide what we want to do with the buffer in hand. If we have load interest\n\t// we will hold onto the whole thing, otherwise empty the buffer, possibly reusing it.\n\tif ts := ats.AccessTime(); ts < mb.llts || (ts-mb.llts) <= int64(mb.cexp) {\n\t\tmb.cache.wp += lob\n\t} else {\n\t\tif cap(mb.cache.buf) <= maxBufReuse {\n\t\t\tbuf = mb.cache.buf[:0]\n\t\t} else {\n\t\t\trecycleMsgBlockBuf(mb.cache.buf)\n\t\t\tbuf = nil\n\t\t}\n\t\tif moreBytes > 0 {\n\t\t\tnbuf := mb.cache.buf[len(mb.cache.buf)-moreBytes:]\n\t\t\tif moreBytes > (len(mb.cache.buf)/4*3) && cap(nbuf) <= maxBufReuse {\n\t\t\t\tbuf = nbuf\n\t\t\t} else {\n\t\t\t\tbuf = append(buf, nbuf...)\n\t\t\t}\n\t\t}\n\t\t// Update our cache offset.\n\t\tmb.cache.off = int(woff)\n\t\t// Reset write pointer.\n\t\tmb.cache.wp = 0\n\t\t// Place buffer back in the cache structure.\n\t\tmb.cache.buf = buf\n\t\t// Mark fseq to 0\n\t\tmb.cache.fseq = 0\n\t}\n\n\treturn fsLostData, mb.werr\n}\n\n// Lock should be held.\nfunc (mb *msgBlock) clearLoading() {\n\tmb.loading = false\n}\n\n// Will load msgs from disk.\nfunc (mb *msgBlock) loadMsgs() error {\n\t// We hold the lock here the whole time by design.\n\tmb.mu.Lock()\n\tdefer mb.mu.Unlock()\n\treturn mb.loadMsgsWithLock()\n}\n\n// Lock should be held.\nfunc (mb *msgBlock) cacheAlreadyLoaded() bool {\n\tif mb.cache == nil || mb.cache.off != 0 || mb.cache.fseq == 0 || len(mb.cache.buf) == 0 {\n\t\treturn false\n\t}\n\tnumEntries := mb.msgs + uint64(mb.dmap.Size()) + (atomic.LoadUint64(&mb.first.seq) - mb.cache.fseq)\n\treturn numEntries == uint64(len(mb.cache.idx))\n}\n\n// Lock should be held.\nfunc (mb *msgBlock) cacheNotLoaded() bool {\n\treturn !mb.cacheAlreadyLoaded()\n}\n\n// Report if our fss is not loaded.\n// Lock should be held.\nfunc (mb *msgBlock) fssNotLoaded() bool {\n\treturn mb.fss == nil && !mb.noTrack\n}\n\n// Wrap openBlock for the gated semaphore processing.\n// Lock should be held\nfunc (mb *msgBlock) openBlock() (*os.File, error) {\n\t// Gate with concurrent IO semaphore.\n\t<-dios\n\tf, err := os.Open(mb.mfn)\n\tdios <- struct{}{}\n\treturn f, err\n}\n\n// Used to load in the block contents.\n// Lock should be held and all conditionals satisfied prior.\nfunc (mb *msgBlock) loadBlock(buf []byte) ([]byte, error) {\n\tvar f *os.File\n\t// Re-use if we have mfd open.\n\tif mb.mfd != nil {\n\t\tf = mb.mfd\n\t\tif n, err := f.Seek(0, 0); n != 0 || err != nil {\n\t\t\tf = nil\n\t\t\tmb.closeFDsLockedNoCheck()\n\t\t}\n\t}\n\tif f == nil {\n\t\tvar err error\n\t\tf, err = mb.openBlock()\n\t\tif err != nil {\n\t\t\tif os.IsNotExist(err) {\n\t\t\t\terr = errNoBlkData\n\t\t\t}\n\t\t\treturn nil, err\n\t\t}\n\t\tdefer f.Close()\n\t}\n\n\tvar sz int\n\tif info, err := f.Stat(); err == nil {\n\t\tsz64 := info.Size()\n\t\tif int64(int(sz64)) == sz64 {\n\t\t\tsz = int(sz64)\n\t\t} else {\n\t\t\treturn nil, errMsgBlkTooBig\n\t\t}\n\t}\n\n\tif buf == nil {\n\t\tbuf = getMsgBlockBuf(sz)\n\t\tif sz > cap(buf) {\n\t\t\t// We know we will make a new one so just recycle for now.\n\t\t\trecycleMsgBlockBuf(buf)\n\t\t\tbuf = nil\n\t\t}\n\t}\n\n\tif sz > cap(buf) {\n\t\tbuf = make([]byte, sz)\n\t} else {\n\t\tbuf = buf[:sz]\n\t}\n\n\t<-dios\n\tn, err := io.ReadFull(f, buf)\n\tdios <- struct{}{}\n\t// On success capture raw bytes size.\n\tif err == nil {\n\t\tmb.rbytes = uint64(n)\n\t}\n\treturn buf[:n], err\n}\n\n// Lock should be held.\nfunc (mb *msgBlock) loadMsgsWithLock() error {\n\t// Check for encryption, we do not load keys on startup anymore so might need to load them here.\n\tif mb.fs != nil && mb.fs.prf != nil && (mb.aek == nil || mb.bek == nil) {\n\t\tif err := mb.fs.loadEncryptionForMsgBlock(mb); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\t// Check to see if we are loading already.\n\tif mb.loading {\n\t\treturn nil\n\t}\n\n\t// Set loading status.\n\tmb.loading = true\n\tdefer mb.clearLoading()\n\n\tvar nchecks int\n\ncheckCache:\n\tnchecks++\n\tif nchecks > 8 {\n\t\treturn errCorruptState\n\t}\n\n\t// Check to see if we have a full cache.\n\tif mb.cacheAlreadyLoaded() {\n\t\treturn nil\n\t}\n\n\tmb.llts = ats.AccessTime()\n\n\t// FIXME(dlc) - We could be smarter here.\n\tif buf, _ := mb.bytesPending(); len(buf) > 0 {\n\t\tld, err := mb.flushPendingMsgsLocked()\n\t\tif ld != nil && mb.fs != nil {\n\t\t\t// We do not know if fs is locked or not at this point.\n\t\t\t// This should be an exceptional condition so do so in Go routine.\n\t\t\tgo mb.fs.rebuildState(ld)\n\t\t}\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tgoto checkCache\n\t}\n\n\t// Load in the whole block.\n\t// We want to hold the mb lock here to avoid any changes to state.\n\tbuf, err := mb.loadBlock(nil)\n\tif err != nil {\n\t\tmb.fs.warn(\"loadBlock error: %v\", err)\n\t\tif err == errNoBlkData {\n\t\t\tif ld, _, err := mb.rebuildStateLocked(); err != nil && ld != nil {\n\t\t\t\t// Rebuild fs state too.\n\t\t\t\tgo mb.fs.rebuildState(ld)\n\t\t\t}\n\t\t}\n\t\treturn err\n\t}\n\n\t// Reset the cache since we just read everything in.\n\t// Make sure this is cleared in case we had a partial when we started.\n\tmb.clearCacheAndOffset()\n\n\t// Check if we need to decrypt.\n\tif mb.bek != nil && len(buf) > 0 {\n\t\tbek, err := genBlockEncryptionKey(mb.fs.fcfg.Cipher, mb.seed, mb.nonce)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tmb.bek = bek\n\t\tmb.bek.XORKeyStream(buf, buf)\n\t}\n\n\t// Check for compression.\n\tif buf, err = mb.decompressIfNeeded(buf); err != nil {\n\t\treturn err\n\t}\n\n\tif err := mb.indexCacheBuf(buf); err != nil {\n\t\tif err == errCorruptState {\n\t\t\tvar ld *LostStreamData\n\t\t\tif ld, _, err = mb.rebuildStateLocked(); ld != nil {\n\t\t\t\t// We do not know if fs is locked or not at this point.\n\t\t\t\t// This should be an exceptional condition so do so in Go routine.\n\t\t\t\tgo mb.fs.rebuildState(ld)\n\t\t\t}\n\t\t}\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tgoto checkCache\n\t}\n\n\tif len(buf) > 0 {\n\t\tmb.cloads++\n\t\tmb.startCacheExpireTimer()\n\t}\n\n\treturn nil\n}\n\n// Fetch a message from this block, possibly reading in and caching the messages.\n// We assume the block was selected and is correct, so we do not do range checks.\n// Lock should not be held.\nfunc (mb *msgBlock) fetchMsg(seq uint64, sm *StoreMsg) (*StoreMsg, bool, error) {\n\treturn mb.fetchMsgEx(seq, sm, true)\n}\n\n// Fetch a message from this block, possibly reading in and caching the messages.\n// We assume the block was selected and is correct, so we do not do range checks.\n// We will not copy the msg data.\n// Lock should not be held.\nfunc (mb *msgBlock) fetchMsgNoCopy(seq uint64, sm *StoreMsg) (*StoreMsg, bool, error) {\n\treturn mb.fetchMsgEx(seq, sm, false)\n}\n\n// Fetch a message from this block, possibly reading in and caching the messages.\n// We assume the block was selected and is correct, so we do not do range checks.\n// We will copy the msg data based on doCopy boolean.\n// Lock should not be held.\nfunc (mb *msgBlock) fetchMsgEx(seq uint64, sm *StoreMsg, doCopy bool) (*StoreMsg, bool, error) {\n\tmb.mu.Lock()\n\tdefer mb.mu.Unlock()\n\n\tfseq, lseq := atomic.LoadUint64(&mb.first.seq), atomic.LoadUint64(&mb.last.seq)\n\tif seq < fseq || seq > lseq {\n\t\treturn nil, false, ErrStoreMsgNotFound\n\t}\n\n\t// See if we can short circuit if we already know msg deleted.\n\tif mb.dmap.Exists(seq) {\n\t\t// Update for scanning like cacheLookup would have.\n\t\tllseq := mb.llseq\n\t\tif mb.llseq == 0 || seq < mb.llseq || seq == mb.llseq+1 || seq == mb.llseq-1 {\n\t\t\tmb.llseq = seq\n\t\t}\n\t\texpireOk := (seq == lseq && llseq == seq-1) || (seq == fseq && llseq == seq+1)\n\t\treturn nil, expireOk, errDeletedMsg\n\t}\n\n\tif mb.cacheNotLoaded() {\n\t\tif err := mb.loadMsgsWithLock(); err != nil {\n\t\t\treturn nil, false, err\n\t\t}\n\t}\n\tllseq := mb.llseq\n\n\tfsm, err := mb.cacheLookupEx(seq, sm, doCopy)\n\tif err != nil {\n\t\treturn nil, false, err\n\t}\n\texpireOk := (seq == lseq && llseq == seq-1) || (seq == fseq && llseq == seq+1)\n\treturn fsm, expireOk, err\n}\n\nvar (\n\terrNoCache       = errors.New(\"no message cache\")\n\terrBadMsg        = errors.New(\"malformed or corrupt message\")\n\terrDeletedMsg    = errors.New(\"deleted message\")\n\terrPartialCache  = errors.New(\"partial cache\")\n\terrNoPending     = errors.New(\"message block does not have pending data\")\n\terrNotReadable   = errors.New(\"storage directory not readable\")\n\terrCorruptState  = errors.New(\"corrupt state file\")\n\terrPriorState    = errors.New(\"prior state file\")\n\terrPendingData   = errors.New(\"pending data still present\")\n\terrNoEncryption  = errors.New(\"encryption not enabled\")\n\terrBadKeySize    = errors.New(\"encryption bad key size\")\n\terrNoMsgBlk      = errors.New(\"no message block\")\n\terrMsgBlkTooBig  = errors.New(\"message block size exceeded int capacity\")\n\terrUnknownCipher = errors.New(\"unknown cipher\")\n\terrNoMainKey     = errors.New(\"encrypted store encountered with no main key\")\n\terrNoBlkData     = errors.New(\"message block data missing\")\n\terrStateTooBig   = errors.New(\"store state too big for optional write\")\n)\n\nconst (\n\t// \"Checksum bit\" is used in \"mb.cache.idx\" for marking messages that have had their checksums checked.\n\tcbit = 1 << 31\n\t// \"Delete bit\" is used in \"mb.cache.idx\" to mark an index as deleted and non-existent.\n\tdbit = 1 << 30\n\t// \"Header bit\" is used in \"rl\" to signal a message record with headers.\n\thbit = 1 << 31\n\t// \"Erase bit\" is used in \"seq\" for marking erased messages sequences.\n\tebit = 1 << 63\n\t// \"Tombstone bit\" is used in \"seq\" for marking tombstone sequences.\n\ttbit = 1 << 62\n)\n\n// Will do a lookup from cache.\n// This will copy the msg from the cache.\n// Lock should be held.\nfunc (mb *msgBlock) cacheLookup(seq uint64, sm *StoreMsg) (*StoreMsg, error) {\n\treturn mb.cacheLookupEx(seq, sm, true)\n}\n\n// Will do a lookup from cache.\n// This will NOT copy the msg from the cache.\n// Lock should be held.\nfunc (mb *msgBlock) cacheLookupNoCopy(seq uint64, sm *StoreMsg) (*StoreMsg, error) {\n\treturn mb.cacheLookupEx(seq, sm, false)\n}\n\n// Will do a lookup from cache.\n// Lock should be held.\nfunc (mb *msgBlock) cacheLookupEx(seq uint64, sm *StoreMsg, doCopy bool) (*StoreMsg, error) {\n\tif seq < atomic.LoadUint64(&mb.first.seq) || seq > atomic.LoadUint64(&mb.last.seq) {\n\t\treturn nil, ErrStoreMsgNotFound\n\t}\n\n\t// The llseq signals us when we can expire a cache at the end of a linear scan.\n\t// We want to only update when we know the last reads (multiple consumers) are sequential.\n\t// We want to account for forwards and backwards linear scans.\n\tif mb.llseq == 0 || seq < mb.llseq || seq == mb.llseq+1 || seq == mb.llseq-1 {\n\t\tmb.llseq = seq\n\t}\n\n\t// If we have a delete map check it.\n\tif mb.dmap.Exists(seq) {\n\t\tmb.llts = ats.AccessTime()\n\t\treturn nil, errDeletedMsg\n\t}\n\n\t// Detect no cache loaded.\n\tif mb.cache == nil || mb.cache.fseq == 0 || len(mb.cache.idx) == 0 || len(mb.cache.buf) == 0 {\n\t\tvar reason string\n\t\tif mb.cache == nil {\n\t\t\treason = \"no cache\"\n\t\t} else if mb.cache.fseq == 0 {\n\t\t\treason = \"fseq is 0\"\n\t\t} else if len(mb.cache.idx) == 0 {\n\t\t\treason = \"no idx present\"\n\t\t} else {\n\t\t\treason = \"cache buf empty\"\n\t\t}\n\t\tmb.fs.warn(\"Cache lookup detected no cache: %s\", reason)\n\t\treturn nil, errNoCache\n\t}\n\t// Check partial cache status.\n\tif seq < mb.cache.fseq {\n\t\tmb.fs.warn(\"Cache lookup detected partial cache: seq %d vs cache fseq %d\", seq, mb.cache.fseq)\n\t\treturn nil, errPartialCache\n\t}\n\n\tbi, _, hashChecked, err := mb.slotInfo(int(seq - mb.cache.fseq))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Update cache activity.\n\tmb.llts = ats.AccessTime()\n\n\tli := int(bi) - mb.cache.off\n\tif li >= len(mb.cache.buf) {\n\t\treturn nil, errPartialCache\n\t}\n\tbuf := mb.cache.buf[li:]\n\n\t// We use the high bit to denote we have already checked the checksum.\n\tvar hh hash.Hash64\n\tif !hashChecked {\n\t\thh = mb.hh // This will force the hash check in msgFromBuf.\n\t}\n\n\t// Parse from the raw buffer.\n\tfsm, err := mb.msgFromBufEx(buf, sm, hh, doCopy)\n\tif err != nil || fsm == nil {\n\t\treturn nil, err\n\t}\n\n\t// Deleted messages that are decoded return a 0 for sequence.\n\tif fsm.seq == 0 {\n\t\treturn nil, errDeletedMsg\n\t}\n\n\tif seq != fsm.seq {\n\t\trecycleMsgBlockBuf(mb.cache.buf)\n\t\tmb.cache.buf = nil\n\t\treturn nil, fmt.Errorf(\"sequence numbers for cache load did not match, %d vs %d\", seq, fsm.seq)\n\t}\n\n\t// Clear the check bit here after we know all is good.\n\tif !hashChecked {\n\t\tmb.cache.idx[seq-mb.cache.fseq] = (bi | cbit)\n\t}\n\n\treturn fsm, nil\n}\n\n// Used when we are checking if discarding a message due to max msgs per subject will give us\n// enough room for a max bytes condition.\n// Lock should be already held.\nfunc (fs *fileStore) sizeForSeq(seq uint64) int {\n\tif seq == 0 {\n\t\treturn 0\n\t}\n\tvar smv StoreMsg\n\tif mb := fs.selectMsgBlock(seq); mb != nil {\n\t\tif sm, _, _ := mb.fetchMsgNoCopy(seq, &smv); sm != nil {\n\t\t\treturn int(fileStoreMsgSize(sm.subj, sm.hdr, sm.msg))\n\t\t}\n\t}\n\treturn 0\n}\n\n// Will return message for the given sequence number.\n// This will be returned to external callers.\nfunc (fs *fileStore) msgForSeq(seq uint64, sm *StoreMsg) (*StoreMsg, error) {\n\treturn fs.msgForSeqLocked(seq, sm, true)\n}\n\n// Will return message for the given sequence number.\nfunc (fs *fileStore) msgForSeqLocked(seq uint64, sm *StoreMsg, needFSLock bool) (*StoreMsg, error) {\n\t// TODO(dlc) - Since Store, Remove, Skip all hold the write lock on fs this will\n\t// be stalled. Need another lock if want to happen in parallel.\n\tif needFSLock {\n\t\tfs.mu.RLock()\n\t}\n\tif fs.closed {\n\t\tif needFSLock {\n\t\t\tfs.mu.RUnlock()\n\t\t}\n\t\treturn nil, ErrStoreClosed\n\t}\n\t// Indicates we want first msg.\n\tif seq == 0 {\n\t\tseq = fs.state.FirstSeq\n\t}\n\t// Make sure to snapshot here.\n\tmb, lseq := fs.selectMsgBlock(seq), fs.state.LastSeq\n\tif needFSLock {\n\t\tfs.mu.RUnlock()\n\t}\n\n\tif mb == nil {\n\t\tvar err = ErrStoreEOF\n\t\tif seq <= lseq {\n\t\t\terr = ErrStoreMsgNotFound\n\t\t}\n\t\treturn nil, err\n\t}\n\n\tfsm, expireOk, err := mb.fetchMsg(seq, sm)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// We detected a linear scan and access to the last message.\n\t// If we are not the last message block we can try to expire the cache.\n\tif expireOk {\n\t\tmb.tryForceExpireCache()\n\t}\n\n\treturn fsm, nil\n}\n\n// Internal function to return msg parts from a raw buffer.\n// Raw buffer will be copied into sm.\n// Lock should be held.\nfunc (mb *msgBlock) msgFromBuf(buf []byte, sm *StoreMsg, hh hash.Hash64) (*StoreMsg, error) {\n\treturn mb.msgFromBufEx(buf, sm, hh, true)\n}\n\n// Internal function to return msg parts from a raw buffer.\n// Raw buffer will NOT be copied into sm.\n// Only use for internal use, any message that is passed to upper layers should use mb.msgFromBuf.\n// Lock should be held.\nfunc (mb *msgBlock) msgFromBufNoCopy(buf []byte, sm *StoreMsg, hh hash.Hash64) (*StoreMsg, error) {\n\treturn mb.msgFromBufEx(buf, sm, hh, false)\n}\n\n// Internal function to return msg parts from a raw buffer.\n// copy boolean will determine if we make a copy or not.\n// Lock should be held.\nfunc (mb *msgBlock) msgFromBufEx(buf []byte, sm *StoreMsg, hh hash.Hash64, doCopy bool) (*StoreMsg, error) {\n\tif len(buf) < emptyRecordLen {\n\t\treturn nil, errBadMsg\n\t}\n\tvar le = binary.LittleEndian\n\n\thdr := buf[:msgHdrSize]\n\trl := le.Uint32(hdr[0:])\n\thasHeaders := rl&hbit != 0\n\trl &^= hbit // clear header bit\n\tdlen := int(rl) - msgHdrSize\n\tslen := int(le.Uint16(hdr[20:]))\n\t// Simple sanity check.\n\tif dlen < 0 || slen > (dlen-recordHashSize) || dlen > int(rl) || int(rl) > len(buf) || rl > rlBadThresh {\n\t\treturn nil, errBadMsg\n\t}\n\tdata := buf[msgHdrSize : msgHdrSize+dlen]\n\t// Do checksum tests here if requested.\n\tif hh != nil {\n\t\thh.Reset()\n\t\thh.Write(hdr[4:20])\n\t\thh.Write(data[:slen])\n\t\tif hasHeaders {\n\t\t\thh.Write(data[slen+4 : dlen-recordHashSize])\n\t\t} else {\n\t\t\thh.Write(data[slen : dlen-recordHashSize])\n\t\t}\n\t\tif !bytes.Equal(hh.Sum(nil), data[len(data)-8:]) {\n\t\t\treturn nil, errBadMsg\n\t\t}\n\t}\n\tseq := le.Uint64(hdr[4:])\n\tif seq&ebit != 0 {\n\t\tseq = 0\n\t}\n\tts := int64(le.Uint64(hdr[12:]))\n\n\t// Create a StoreMsg if needed.\n\tif sm == nil {\n\t\tsm = new(StoreMsg)\n\t} else {\n\t\tsm.clear()\n\t}\n\t// To recycle the large blocks we can never pass back a reference, so need to copy for the upper\n\t// layers and for us to be safe to expire, and recycle, the large msgBlocks.\n\tend := dlen - 8\n\n\tif hasHeaders {\n\t\thl := le.Uint32(data[slen:])\n\t\tbi := slen + 4\n\t\tli := bi + int(hl)\n\t\tif doCopy {\n\t\t\tsm.buf = append(sm.buf, data[bi:end]...)\n\t\t} else {\n\t\t\tsm.buf = data[bi:end]\n\t\t}\n\t\tli, end = li-bi, end-bi\n\t\tsm.hdr = sm.buf[0:li:li]\n\t\tsm.msg = sm.buf[li:end]\n\t} else {\n\t\tif doCopy {\n\t\t\tsm.buf = append(sm.buf, data[slen:end]...)\n\t\t} else {\n\t\t\tsm.buf = data[slen:end]\n\t\t}\n\t\tsm.msg = sm.buf[0 : end-slen]\n\t}\n\tsm.seq, sm.ts = seq, ts\n\tif slen > 0 {\n\t\tif doCopy {\n\t\t\t// Make a copy since sm.subj lifetime may last longer.\n\t\t\tsm.subj = string(data[:slen])\n\t\t} else {\n\t\t\tsm.subj = bytesToString(data[:slen])\n\t\t}\n\t}\n\n\treturn sm, nil\n}\n\n// SubjectForSeq will return what the subject is for this sequence if found.\nfunc (fs *fileStore) SubjectForSeq(seq uint64) (string, error) {\n\tfs.mu.RLock()\n\tif seq < fs.state.FirstSeq {\n\t\tfs.mu.RUnlock()\n\t\treturn _EMPTY_, ErrStoreMsgNotFound\n\t}\n\tvar smv StoreMsg\n\tmb := fs.selectMsgBlock(seq)\n\tfs.mu.RUnlock()\n\tif mb != nil {\n\t\tif sm, _, _ := mb.fetchMsgNoCopy(seq, &smv); sm != nil {\n\t\t\treturn sm.subj, nil\n\t\t}\n\t}\n\treturn _EMPTY_, ErrStoreMsgNotFound\n}\n\n// LoadMsg will lookup the message by sequence number and return it if found.\nfunc (fs *fileStore) LoadMsg(seq uint64, sm *StoreMsg) (*StoreMsg, error) {\n\treturn fs.msgForSeq(seq, sm)\n}\n\n// loadLast will load the last message for a subject. Subject should be non empty and not \">\".\nfunc (fs *fileStore) loadLast(subj string, sm *StoreMsg) (lsm *StoreMsg, err error) {\n\tfs.mu.RLock()\n\tdefer fs.mu.RUnlock()\n\n\tif fs.closed || fs.lmb == nil {\n\t\treturn nil, ErrStoreClosed\n\t}\n\n\tif len(fs.blks) == 0 {\n\t\treturn nil, ErrStoreMsgNotFound\n\t}\n\n\twc := subjectHasWildcard(subj)\n\tvar start, stop uint32\n\n\t// If literal subject check for presence.\n\tif wc {\n\t\tstart = fs.lmb.index\n\t\tfs.psim.Match(stringToBytes(subj), func(_ []byte, psi *psi) {\n\t\t\t// Keep track of start and stop indexes for this subject.\n\t\t\tif psi.fblk < start {\n\t\t\t\tstart = psi.fblk\n\t\t\t}\n\t\t\tif psi.lblk > stop {\n\t\t\t\tstop = psi.lblk\n\t\t\t}\n\t\t})\n\t\t// None matched.\n\t\tif stop == 0 {\n\t\t\treturn nil, ErrStoreMsgNotFound\n\t\t}\n\t\t// These need to be swapped.\n\t\tstart, stop = stop, start\n\t} else if info, ok := fs.psim.Find(stringToBytes(subj)); ok {\n\t\tstart, stop = info.lblk, info.fblk\n\t} else {\n\t\treturn nil, ErrStoreMsgNotFound\n\t}\n\n\t// Walk blocks backwards.\n\tfor i := start; i >= stop; i-- {\n\t\tmb := fs.bim[i]\n\t\tif mb == nil {\n\t\t\tcontinue\n\t\t}\n\t\tmb.mu.Lock()\n\t\tif err := mb.ensurePerSubjectInfoLoaded(); err != nil {\n\t\t\tmb.mu.Unlock()\n\t\t\treturn nil, err\n\t\t}\n\t\t// Mark fss activity.\n\t\tmb.lsts = ats.AccessTime()\n\n\t\tvar l uint64\n\t\t// Optimize if subject is not a wildcard.\n\t\tif !wc {\n\t\t\tif ss, ok := mb.fss.Find(stringToBytes(subj)); ok && ss != nil {\n\t\t\t\tl = ss.Last\n\t\t\t}\n\t\t}\n\t\tif l == 0 {\n\t\t\t_, _, l = mb.filteredPendingLocked(subj, wc, atomic.LoadUint64(&mb.first.seq))\n\t\t}\n\t\tif l > 0 {\n\t\t\tif mb.cacheNotLoaded() {\n\t\t\t\tif err := mb.loadMsgsWithLock(); err != nil {\n\t\t\t\t\tmb.mu.Unlock()\n\t\t\t\t\treturn nil, err\n\t\t\t\t}\n\t\t\t}\n\t\t\tlsm, err = mb.cacheLookup(l, sm)\n\t\t}\n\t\tmb.mu.Unlock()\n\t\tif l > 0 {\n\t\t\tbreak\n\t\t}\n\t}\n\treturn lsm, err\n}\n\n// LoadLastMsg will return the last message we have that matches a given subject.\n// The subject can be a wildcard.\nfunc (fs *fileStore) LoadLastMsg(subject string, smv *StoreMsg) (sm *StoreMsg, err error) {\n\tif subject == _EMPTY_ || subject == fwcs {\n\t\tsm, err = fs.msgForSeq(fs.lastSeq(), smv)\n\t} else {\n\t\tsm, err = fs.loadLast(subject, smv)\n\t}\n\tif sm == nil || (err != nil && err != ErrStoreClosed) {\n\t\terr = ErrStoreMsgNotFound\n\t}\n\treturn sm, err\n}\n\n// LoadNextMsgMulti will find the next message matching any entry in the sublist.\nfunc (fs *fileStore) LoadNextMsgMulti(sl *Sublist, start uint64, smp *StoreMsg) (sm *StoreMsg, skip uint64, err error) {\n\tif sl == nil {\n\t\treturn fs.LoadNextMsg(_EMPTY_, false, start, smp)\n\t}\n\tfs.mu.RLock()\n\tdefer fs.mu.RUnlock()\n\n\tif fs.closed {\n\t\treturn nil, 0, ErrStoreClosed\n\t}\n\tif fs.state.Msgs == 0 || start > fs.state.LastSeq {\n\t\treturn nil, fs.state.LastSeq, ErrStoreEOF\n\t}\n\tif start < fs.state.FirstSeq {\n\t\tstart = fs.state.FirstSeq\n\t}\n\n\tif bi, _ := fs.selectMsgBlockWithIndex(start); bi >= 0 {\n\t\tfor i := bi; i < len(fs.blks); i++ {\n\t\t\tmb := fs.blks[i]\n\t\t\tif sm, expireOk, err := mb.firstMatchingMulti(sl, start, smp); err == nil {\n\t\t\t\tif expireOk {\n\t\t\t\t\tmb.tryForceExpireCache()\n\t\t\t\t}\n\t\t\t\treturn sm, sm.seq, nil\n\t\t\t} else if err != ErrStoreMsgNotFound {\n\t\t\t\treturn nil, 0, err\n\t\t\t} else if expireOk {\n\t\t\t\tmb.tryForceExpireCache()\n\t\t\t}\n\t\t}\n\t}\n\n\treturn nil, fs.state.LastSeq, ErrStoreEOF\n\n}\n\nfunc (fs *fileStore) LoadNextMsg(filter string, wc bool, start uint64, sm *StoreMsg) (*StoreMsg, uint64, error) {\n\tfs.mu.RLock()\n\tdefer fs.mu.RUnlock()\n\n\tif fs.closed {\n\t\treturn nil, 0, ErrStoreClosed\n\t}\n\tif fs.state.Msgs == 0 || start > fs.state.LastSeq {\n\t\treturn nil, fs.state.LastSeq, ErrStoreEOF\n\t}\n\tif start < fs.state.FirstSeq {\n\t\tstart = fs.state.FirstSeq\n\t}\n\n\t// If start is less than or equal to beginning of our stream, meaning our first call,\n\t// let's check the psim to see if we can skip ahead.\n\tif start <= fs.state.FirstSeq {\n\t\tvar ss SimpleState\n\t\tfs.numFilteredPendingNoLast(filter, &ss)\n\t\t// Nothing available.\n\t\tif ss.Msgs == 0 {\n\t\t\treturn nil, fs.state.LastSeq, ErrStoreEOF\n\t\t}\n\t\t// We can skip ahead.\n\t\tif ss.First > start {\n\t\t\tstart = ss.First\n\t\t}\n\t}\n\n\tif bi, _ := fs.selectMsgBlockWithIndex(start); bi >= 0 {\n\t\tfor i := bi; i < len(fs.blks); i++ {\n\t\t\tmb := fs.blks[i]\n\t\t\tif sm, expireOk, err := mb.firstMatching(filter, wc, start, sm); err == nil {\n\t\t\t\tif expireOk {\n\t\t\t\t\tmb.tryForceExpireCache()\n\t\t\t\t}\n\t\t\t\treturn sm, sm.seq, nil\n\t\t\t} else if err != ErrStoreMsgNotFound {\n\t\t\t\treturn nil, 0, err\n\t\t\t} else {\n\t\t\t\t// Nothing found in this block. We missed, if first block (bi) check psim.\n\t\t\t\t// Similar to above if start <= first seq.\n\t\t\t\t// TODO(dlc) - For v2 track these by filter subject since they will represent filtered consumers.\n\t\t\t\t// We should not do this at all if we are already on the last block.\n\t\t\t\t// Also if we are a wildcard do not check if large subject space.\n\t\t\t\tconst wcMaxSizeToCheck = 64 * 1024\n\t\t\t\tif i == bi && i < len(fs.blks)-1 && (!wc || fs.psim.Size() < wcMaxSizeToCheck) {\n\t\t\t\t\tnbi, err := fs.checkSkipFirstBlock(filter, wc, bi)\n\t\t\t\t\t// Nothing available.\n\t\t\t\t\tif err == ErrStoreEOF {\n\t\t\t\t\t\treturn nil, fs.state.LastSeq, ErrStoreEOF\n\t\t\t\t\t}\n\t\t\t\t\t// See if we can jump ahead here.\n\t\t\t\t\t// Right now we can only spin on first, so if we have interior sparseness need to favor checking per block fss if loaded.\n\t\t\t\t\t// For v2 will track all blocks that have matches for psim.\n\t\t\t\t\tif nbi > i {\n\t\t\t\t\t\ti = nbi - 1 // For the iterator condition i++\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\t// Check is we can expire.\n\t\t\t\tif expireOk {\n\t\t\t\t\tmb.tryForceExpireCache()\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\treturn nil, fs.state.LastSeq, ErrStoreEOF\n}\n\n// Will load the next non-deleted msg starting at the start sequence and walking backwards.\nfunc (fs *fileStore) LoadPrevMsg(start uint64, smp *StoreMsg) (sm *StoreMsg, err error) {\n\tfs.mu.RLock()\n\tdefer fs.mu.RUnlock()\n\n\tif fs.closed {\n\t\treturn nil, ErrStoreClosed\n\t}\n\tif fs.state.Msgs == 0 || start < fs.state.FirstSeq {\n\t\treturn nil, ErrStoreEOF\n\t}\n\n\tif start > fs.state.LastSeq {\n\t\tstart = fs.state.LastSeq\n\t}\n\tif smp == nil {\n\t\tsmp = new(StoreMsg)\n\t}\n\n\tif bi, _ := fs.selectMsgBlockWithIndex(start); bi >= 0 {\n\t\tfor i := bi; i >= 0; i-- {\n\t\t\tmb := fs.blks[i]\n\t\t\tmb.mu.Lock()\n\t\t\t// Need messages loaded from here on out.\n\t\t\tif mb.cacheNotLoaded() {\n\t\t\t\tif err := mb.loadMsgsWithLock(); err != nil {\n\t\t\t\t\tmb.mu.Unlock()\n\t\t\t\t\treturn nil, err\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tlseq, fseq := atomic.LoadUint64(&mb.last.seq), atomic.LoadUint64(&mb.first.seq)\n\t\t\tif start > lseq {\n\t\t\t\tstart = lseq\n\t\t\t}\n\t\t\tfor seq := start; seq >= fseq; seq-- {\n\t\t\t\tif mb.dmap.Exists(seq) {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tif sm, err := mb.cacheLookup(seq, smp); err == nil {\n\t\t\t\t\tmb.mu.Unlock()\n\t\t\t\t\treturn sm, nil\n\t\t\t\t}\n\t\t\t}\n\t\t\tmb.mu.Unlock()\n\t\t}\n\t}\n\n\treturn nil, ErrStoreEOF\n}\n\n// Type returns the type of the underlying store.\nfunc (fs *fileStore) Type() StorageType {\n\treturn FileStorage\n}\n\n// Returns number of subjects in this store.\n// Lock should be held.\nfunc (fs *fileStore) numSubjects() int {\n\treturn fs.psim.Size()\n}\n\n// numConsumers uses new lock.\nfunc (fs *fileStore) numConsumers() int {\n\tfs.cmu.RLock()\n\tdefer fs.cmu.RUnlock()\n\treturn len(fs.cfs)\n}\n\n// FastState will fill in state with only the following.\n// Msgs, Bytes, First and Last Sequence and Time and NumDeleted.\nfunc (fs *fileStore) FastState(state *StreamState) {\n\tfs.mu.RLock()\n\tstate.Msgs = fs.state.Msgs\n\tstate.Bytes = fs.state.Bytes\n\tstate.FirstSeq = fs.state.FirstSeq\n\tstate.FirstTime = fs.state.FirstTime\n\tstate.LastSeq = fs.state.LastSeq\n\tstate.LastTime = fs.state.LastTime\n\t// Make sure to reset if being re-used.\n\tstate.Deleted, state.NumDeleted = nil, 0\n\tif state.LastSeq > state.FirstSeq {\n\t\tstate.NumDeleted = int((state.LastSeq - state.FirstSeq + 1) - state.Msgs)\n\t\tif state.NumDeleted < 0 {\n\t\t\tstate.NumDeleted = 0\n\t\t}\n\t}\n\tstate.Consumers = fs.numConsumers()\n\tstate.NumSubjects = fs.numSubjects()\n\tfs.mu.RUnlock()\n}\n\n// State returns the current state of the stream.\nfunc (fs *fileStore) State() StreamState {\n\tfs.mu.RLock()\n\tstate := fs.state\n\tstate.Consumers = fs.numConsumers()\n\tstate.NumSubjects = fs.numSubjects()\n\tstate.Deleted = nil // make sure.\n\n\tif numDeleted := int((state.LastSeq - state.FirstSeq + 1) - state.Msgs); numDeleted > 0 {\n\t\tstate.Deleted = make([]uint64, 0, numDeleted)\n\t\tcur := fs.state.FirstSeq\n\n\t\tfor _, mb := range fs.blks {\n\t\t\tmb.mu.Lock()\n\t\t\tfseq := atomic.LoadUint64(&mb.first.seq)\n\t\t\t// Account for messages missing from the head.\n\t\t\tif fseq > cur {\n\t\t\t\tfor seq := cur; seq < fseq; seq++ {\n\t\t\t\t\tstate.Deleted = append(state.Deleted, seq)\n\t\t\t\t}\n\t\t\t}\n\t\t\t// Only advance cur if we are increasing. We could have marker blocks with just tombstones.\n\t\t\tif last := atomic.LoadUint64(&mb.last.seq); last >= cur {\n\t\t\t\tcur = last + 1 // Expected next first.\n\t\t\t}\n\t\t\t// Add in deleted.\n\t\t\tmb.dmap.Range(func(seq uint64) bool {\n\t\t\t\tstate.Deleted = append(state.Deleted, seq)\n\t\t\t\treturn true\n\t\t\t})\n\t\t\tmb.mu.Unlock()\n\t\t}\n\t}\n\tfs.mu.RUnlock()\n\n\tstate.Lost = fs.lostData()\n\n\t// Can not be guaranteed to be sorted.\n\tif len(state.Deleted) > 0 {\n\t\tslices.Sort(state.Deleted)\n\t\tstate.NumDeleted = len(state.Deleted)\n\t}\n\treturn state\n}\n\nfunc (fs *fileStore) Utilization() (total, reported uint64, err error) {\n\tfs.mu.RLock()\n\tdefer fs.mu.RUnlock()\n\tfor _, mb := range fs.blks {\n\t\tmb.mu.RLock()\n\t\treported += mb.bytes\n\t\ttotal += mb.rbytes\n\t\tmb.mu.RUnlock()\n\t}\n\treturn total, reported, nil\n}\n\nfunc fileStoreMsgSize(subj string, hdr, msg []byte) uint64 {\n\tif len(hdr) == 0 {\n\t\t// length of the message record (4bytes) + seq(8) + ts(8) + subj_len(2) + subj + msg + hash(8)\n\t\treturn uint64(22 + len(subj) + len(msg) + 8)\n\t}\n\t// length of the message record (4bytes) + seq(8) + ts(8) + subj_len(2) + subj + hdr_len(4) + hdr + msg + hash(8)\n\treturn uint64(22 + len(subj) + 4 + len(hdr) + len(msg) + 8)\n}\n\nfunc fileStoreMsgSizeEstimate(slen, maxPayload int) uint64 {\n\treturn uint64(emptyRecordLen + slen + 4 + maxPayload)\n}\n\n// Determine time since any last activity, read/load, write or remove.\nfunc (mb *msgBlock) sinceLastActivity() time.Duration {\n\tif mb.closed {\n\t\treturn 0\n\t}\n\tlast := mb.lwts\n\tif mb.lrts > last {\n\t\tlast = mb.lrts\n\t}\n\tif mb.llts > last {\n\t\tlast = mb.llts\n\t}\n\tif mb.lsts > last {\n\t\tlast = mb.lsts\n\t}\n\treturn time.Since(time.Unix(0, last).UTC())\n}\n\n// Determine time since last write or remove of a message.\n// Read lock should be held.\nfunc (mb *msgBlock) sinceLastWriteActivity() time.Duration {\n\tif mb.closed {\n\t\treturn 0\n\t}\n\tlast := mb.lwts\n\tif mb.lrts > last {\n\t\tlast = mb.lrts\n\t}\n\treturn time.Since(time.Unix(0, last).UTC())\n}\n\nfunc checkNewHeader(hdr []byte) error {\n\tif len(hdr) < 2 || hdr[0] != magic ||\n\t\t(hdr[1] != version && hdr[1] != newVersion) {\n\t\treturn errCorruptState\n\t}\n\treturn nil\n}\n\n// readIndexInfo will read in the index information for the message block.\nfunc (mb *msgBlock) readIndexInfo() error {\n\tifn := filepath.Join(mb.fs.fcfg.StoreDir, msgDir, fmt.Sprintf(indexScan, mb.index))\n\tbuf, err := os.ReadFile(ifn)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Set if first time.\n\tif mb.liwsz == 0 {\n\t\tmb.liwsz = int64(len(buf))\n\t}\n\n\t// Decrypt if needed.\n\tif mb.aek != nil {\n\t\tbuf, err = mb.aek.Open(buf[:0], mb.nonce, buf, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\tif err := checkNewHeader(buf); err != nil {\n\t\tdefer os.Remove(ifn)\n\t\treturn fmt.Errorf(\"bad index file\")\n\t}\n\n\tbi := hdrLen\n\n\t// Helpers, will set i to -1 on error.\n\treadSeq := func() uint64 {\n\t\tif bi < 0 {\n\t\t\treturn 0\n\t\t}\n\t\tseq, n := binary.Uvarint(buf[bi:])\n\t\tif n <= 0 {\n\t\t\tbi = -1\n\t\t\treturn 0\n\t\t}\n\t\tbi += n\n\t\treturn seq &^ ebit\n\t}\n\treadCount := readSeq\n\treadTimeStamp := func() int64 {\n\t\tif bi < 0 {\n\t\t\treturn 0\n\t\t}\n\t\tts, n := binary.Varint(buf[bi:])\n\t\tif n <= 0 {\n\t\t\tbi = -1\n\t\t\treturn -1\n\t\t}\n\t\tbi += n\n\t\treturn ts\n\t}\n\tmb.msgs = readCount()\n\tmb.bytes = readCount()\n\tatomic.StoreUint64(&mb.first.seq, readSeq())\n\tmb.first.ts = readTimeStamp()\n\tatomic.StoreUint64(&mb.last.seq, readSeq())\n\tmb.last.ts = readTimeStamp()\n\tdmapLen := readCount()\n\n\t// Check if this is a short write index file.\n\tif bi < 0 || bi+checksumSize > len(buf) {\n\t\tos.Remove(ifn)\n\t\treturn fmt.Errorf(\"short index file\")\n\t}\n\n\t// Check for consistency if accounting. If something is off bail and we will rebuild.\n\tif mb.msgs != (atomic.LoadUint64(&mb.last.seq)-atomic.LoadUint64(&mb.first.seq)+1)-dmapLen {\n\t\tos.Remove(ifn)\n\t\treturn fmt.Errorf(\"accounting inconsistent\")\n\t}\n\n\t// Checksum\n\tcopy(mb.lchk[0:], buf[bi:bi+checksumSize])\n\tbi += checksumSize\n\n\t// Now check for presence of a delete map\n\tif dmapLen > 0 {\n\t\t// New version is encoded avl seqset.\n\t\tif buf[1] == newVersion {\n\t\t\tdmap, _, err := avl.Decode(buf[bi:])\n\t\t\tif err != nil {\n\t\t\t\treturn fmt.Errorf(\"could not decode avl dmap: %v\", err)\n\t\t\t}\n\t\t\tmb.dmap = *dmap\n\t\t} else {\n\t\t\t// This is the old version.\n\t\t\tfor i, fseq := 0, atomic.LoadUint64(&mb.first.seq); i < int(dmapLen); i++ {\n\t\t\t\tseq := readSeq()\n\t\t\t\tif seq == 0 {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t\tmb.dmap.Insert(seq + fseq)\n\t\t\t}\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// Will return total number of cache loads.\nfunc (fs *fileStore) cacheLoads() uint64 {\n\tvar tl uint64\n\tfs.mu.RLock()\n\tfor _, mb := range fs.blks {\n\t\ttl += mb.cloads\n\t}\n\tfs.mu.RUnlock()\n\treturn tl\n}\n\n// Will return total number of cached bytes.\nfunc (fs *fileStore) cacheSize() uint64 {\n\tvar sz uint64\n\tfs.mu.RLock()\n\tfor _, mb := range fs.blks {\n\t\tmb.mu.RLock()\n\t\tif mb.cache != nil {\n\t\t\tsz += uint64(len(mb.cache.buf))\n\t\t}\n\t\tmb.mu.RUnlock()\n\t}\n\tfs.mu.RUnlock()\n\treturn sz\n}\n\n// Will return total number of dmapEntries for all msg blocks.\nfunc (fs *fileStore) dmapEntries() int {\n\tvar total int\n\tfs.mu.RLock()\n\tfor _, mb := range fs.blks {\n\t\ttotal += mb.dmap.Size()\n\t}\n\tfs.mu.RUnlock()\n\treturn total\n}\n\n// Fixed helper for iterating.\nfunc subjectsEqual(a, b string) bool {\n\treturn a == b\n}\n\nfunc subjectsAll(a, b string) bool {\n\treturn true\n}\n\nfunc compareFn(subject string) func(string, string) bool {\n\tif subject == _EMPTY_ || subject == fwcs {\n\t\treturn subjectsAll\n\t}\n\tif subjectHasWildcard(subject) {\n\t\treturn subjectIsSubsetMatch\n\t}\n\treturn subjectsEqual\n}\n\n// PurgeEx will remove messages based on subject filters, sequence and number of messages to keep.\n// Will return the number of purged messages.\nfunc (fs *fileStore) PurgeEx(subject string, sequence, keep uint64) (purged uint64, err error) {\n\tif subject == _EMPTY_ || subject == fwcs {\n\t\tif keep == 0 && sequence == 0 {\n\t\t\treturn fs.purge(0)\n\t\t}\n\t\tif sequence > 1 {\n\t\t\treturn fs.compact(sequence)\n\t\t}\n\t}\n\n\t// Make sure to not leave subject if empty and we reach this spot.\n\tif subject == _EMPTY_ {\n\t\tsubject = fwcs\n\t}\n\n\teq, wc := compareFn(subject), subjectHasWildcard(subject)\n\tvar firstSeqNeedsUpdate bool\n\tvar bytes uint64\n\n\t// If we have a \"keep\" designation need to get full filtered state so we know how many to purge.\n\tvar maxp uint64\n\tif keep > 0 {\n\t\tss := fs.FilteredState(1, subject)\n\t\tif keep >= ss.Msgs {\n\t\t\treturn 0, nil\n\t\t}\n\t\tmaxp = ss.Msgs - keep\n\t}\n\n\tvar smv StoreMsg\n\tvar tombs []msgId\n\tvar lowSeq uint64\n\n\tfs.mu.Lock()\n\t// We may remove blocks as we purge, so don't range directly on fs.blks\n\t// otherwise we may jump over some (see https://github.com/nats-io/nats-server/issues/3528)\n\tfor i := 0; i < len(fs.blks); i++ {\n\t\tmb := fs.blks[i]\n\t\tmb.mu.Lock()\n\n\t\t// If we do not have our fss, try to expire the cache if we have no items in this block.\n\t\tshouldExpire := mb.fssNotLoaded()\n\n\t\tt, f, l := mb.filteredPendingLocked(subject, wc, atomic.LoadUint64(&mb.first.seq))\n\t\tif t == 0 {\n\t\t\t// Expire if we were responsible for loading.\n\t\t\tif shouldExpire {\n\t\t\t\t// Expire this cache before moving on.\n\t\t\t\tmb.tryForceExpireCacheLocked()\n\t\t\t}\n\t\t\tmb.mu.Unlock()\n\t\t\tcontinue\n\t\t}\n\n\t\tif sequence > 1 && sequence <= l {\n\t\t\tl = sequence - 1\n\t\t}\n\n\t\tif mb.cacheNotLoaded() {\n\t\t\tmb.loadMsgsWithLock()\n\t\t\tshouldExpire = true\n\t\t}\n\n\t\tvar nrg uint64 // Number of remaining messages globally after removal from psim.\n\n\t\tfor seq, te := f, len(tombs); seq <= l; seq++ {\n\t\t\tif sm, _ := mb.cacheLookupNoCopy(seq, &smv); sm != nil && eq(sm.subj, subject) {\n\t\t\t\trl := fileStoreMsgSize(sm.subj, sm.hdr, sm.msg)\n\t\t\t\t// Do fast in place remove.\n\t\t\t\t// Stats\n\t\t\t\tif mb.msgs > 0 {\n\t\t\t\t\t// Msgs\n\t\t\t\t\tfs.state.Msgs--\n\t\t\t\t\tmb.msgs--\n\t\t\t\t\t// Bytes, make sure to not go negative.\n\t\t\t\t\tif rl > fs.state.Bytes {\n\t\t\t\t\t\trl = fs.state.Bytes\n\t\t\t\t\t}\n\t\t\t\t\tif rl > mb.bytes {\n\t\t\t\t\t\trl = mb.bytes\n\t\t\t\t\t}\n\t\t\t\t\tfs.state.Bytes -= rl\n\t\t\t\t\tmb.bytes -= rl\n\t\t\t\t\t// Totals\n\t\t\t\t\tpurged++\n\t\t\t\t\tbytes += rl\n\t\t\t\t}\n\t\t\t\t// PSIM and FSS updates.\n\t\t\t\tnr := mb.removeSeqPerSubject(sm.subj, seq)\n\t\t\t\tnrg = fs.removePerSubject(sm.subj)\n\n\t\t\t\t// Track tombstones we need to write.\n\t\t\t\ttombs = append(tombs, msgId{sm.seq, sm.ts})\n\t\t\t\tif sm.seq < lowSeq || lowSeq == 0 {\n\t\t\t\t\tlowSeq = sm.seq\n\t\t\t\t}\n\n\t\t\t\t// Check for first message.\n\t\t\t\tif seq == atomic.LoadUint64(&mb.first.seq) {\n\t\t\t\t\tmb.selectNextFirst()\n\t\t\t\t\tif mb.isEmpty() {\n\t\t\t\t\t\t// Since we are removing this block don't need to write tombstones.\n\t\t\t\t\t\ttombs = tombs[:te]\n\t\t\t\t\t\tfs.removeMsgBlock(mb)\n\t\t\t\t\t\ti--\n\t\t\t\t\t\t// keep flag set, if set previously\n\t\t\t\t\t\tfirstSeqNeedsUpdate = firstSeqNeedsUpdate || seq == fs.state.FirstSeq\n\t\t\t\t\t} else if seq == fs.state.FirstSeq {\n\t\t\t\t\t\tfs.state.FirstSeq = atomic.LoadUint64(&mb.first.seq) // new one.\n\t\t\t\t\t\tfs.state.FirstTime = time.Unix(0, mb.first.ts).UTC()\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\t// Out of order delete.\n\t\t\t\t\tmb.dmap.Insert(seq)\n\t\t\t\t}\n\t\t\t\t// Break if we have emptied this block or if we set a maximum purge count.\n\t\t\t\tif mb.isEmpty() || (maxp > 0 && purged >= maxp) {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t\t// Also break if we know we have no more messages matching here.\n\t\t\t\t// This is only applicable for non-wildcarded filters.\n\t\t\t\tif !wc && nr == 0 {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t// Expire if we were responsible for loading and we do not seem to be doing successive purgeEx calls.\n\t\t// On successive calls - most likely from KV purge deletes, we want to keep the data loaded.\n\t\tif shouldExpire && time.Since(fs.lpex) > time.Second {\n\t\t\t// Expire this cache before moving on.\n\t\t\tmb.tryForceExpireCacheLocked()\n\t\t}\n\t\tmb.mu.Unlock()\n\n\t\t// Check if we should break out of top level too.\n\t\tif maxp > 0 && purged >= maxp {\n\t\t\tbreak\n\t\t}\n\t\t// Also check if not wildcarded and we have no remaining matches.\n\t\tif !wc && nrg == 0 {\n\t\t\tbreak\n\t\t}\n\t}\n\tif firstSeqNeedsUpdate {\n\t\tfs.selectNextFirst()\n\t}\n\n\t// Update the last purgeEx call time.\n\tdefer func() { fs.lpex = time.Now() }()\n\n\t// Write any tombstones as needed.\n\t// When writing multiple tombstones we will flush at the end.\n\tif len(tombs) > 0 {\n\t\tfor _, tomb := range tombs {\n\t\t\tif err := fs.writeTombstoneNoFlush(tomb.seq, tomb.ts); err != nil {\n\t\t\t\treturn purged, err\n\t\t\t}\n\t\t}\n\t\t// Flush any pending. If we change blocks the checkLastBlock() will flush any pending for us.\n\t\tif lmb := fs.lmb; lmb != nil {\n\t\t\tlmb.flushPendingMsgs()\n\t\t}\n\t}\n\n\tfs.dirty++\n\tcb := fs.scb\n\tfs.mu.Unlock()\n\n\tif cb != nil {\n\t\tif purged == 1 {\n\t\t\tcb(-int64(purged), -int64(bytes), lowSeq, subject)\n\t\t} else {\n\t\t\t// FIXME(dlc) - Since we track lowSeq we could send to upper layer if they dealt with the condition properly.\n\t\t\tcb(-int64(purged), -int64(bytes), 0, _EMPTY_)\n\t\t}\n\t}\n\n\treturn purged, nil\n}\n\n// Purge will remove all messages from this store.\n// Will return the number of purged messages.\nfunc (fs *fileStore) Purge() (uint64, error) {\n\treturn fs.purge(0)\n}\n\nfunc (fs *fileStore) purge(fseq uint64) (uint64, error) {\n\tfs.mu.Lock()\n\tif fs.closed {\n\t\tfs.mu.Unlock()\n\t\treturn 0, ErrStoreClosed\n\t}\n\n\tpurged := fs.state.Msgs\n\trbytes := int64(fs.state.Bytes)\n\n\tfs.state.FirstSeq = fs.state.LastSeq + 1\n\tfs.state.FirstTime = time.Time{}\n\n\tfs.state.Bytes = 0\n\tfs.state.Msgs = 0\n\n\tfor _, mb := range fs.blks {\n\t\tmb.dirtyClose()\n\t}\n\n\tfs.blks = nil\n\tfs.lmb = nil\n\tfs.bim = make(map[uint32]*msgBlock)\n\t// Clear any per subject tracking.\n\tfs.psim, fs.tsl = fs.psim.Empty(), 0\n\tfs.sdm.empty()\n\t// Mark dirty.\n\tfs.dirty++\n\n\t// Move the msgs directory out of the way, will delete out of band.\n\t// FIXME(dlc) - These can error and we need to change api above to propagate?\n\tmdir := filepath.Join(fs.fcfg.StoreDir, msgDir)\n\tpdir := filepath.Join(fs.fcfg.StoreDir, purgeDir)\n\t// If purge directory still exists then we need to wait\n\t// in place and remove since rename would fail.\n\tif _, err := os.Stat(pdir); err == nil {\n\t\t<-dios\n\t\tos.RemoveAll(pdir)\n\t\tdios <- struct{}{}\n\t}\n\n\t<-dios\n\tos.Rename(mdir, pdir)\n\tdios <- struct{}{}\n\n\tgo func() {\n\t\t<-dios\n\t\tos.RemoveAll(pdir)\n\t\tdios <- struct{}{}\n\t}()\n\n\t// Create new one.\n\t<-dios\n\tos.MkdirAll(mdir, defaultDirPerms)\n\tdios <- struct{}{}\n\n\t// Make sure we have a lmb to write to.\n\tif _, err := fs.newMsgBlockForWrite(); err != nil {\n\t\tfs.mu.Unlock()\n\t\treturn purged, err\n\t}\n\n\t// Check if we need to set the first seq to a new number.\n\tif fseq > fs.state.FirstSeq {\n\t\tfs.state.FirstSeq = fseq\n\t\tfs.state.LastSeq = fseq - 1\n\t}\n\n\tlmb := fs.lmb\n\tatomic.StoreUint64(&lmb.first.seq, fs.state.FirstSeq)\n\tatomic.StoreUint64(&lmb.last.seq, fs.state.LastSeq)\n\tlmb.last.ts = fs.state.LastTime.UnixNano()\n\n\tif lseq := atomic.LoadUint64(&lmb.last.seq); lseq > 1 {\n\t\t// Leave a tombstone so we can remember our starting sequence in case\n\t\t// full state becomes corrupted.\n\t\tfs.writeTombstone(lseq, lmb.last.ts)\n\t}\n\n\tcb := fs.scb\n\tfs.mu.Unlock()\n\n\t// Force a new index.db to be written.\n\tif purged > 0 {\n\t\tfs.forceWriteFullState()\n\t}\n\n\tif cb != nil {\n\t\tcb(-int64(purged), -rbytes, 0, _EMPTY_)\n\t}\n\n\treturn purged, nil\n}\n\n// Compact will remove all messages from this store up to\n// but not including the seq parameter.\n// Will return the number of purged messages.\nfunc (fs *fileStore) Compact(seq uint64) (uint64, error) {\n\treturn fs.compact(seq)\n}\n\nfunc (fs *fileStore) compact(seq uint64) (uint64, error) {\n\tif seq == 0 {\n\t\treturn fs.purge(seq)\n\t}\n\n\tvar purged, bytes uint64\n\n\tfs.mu.Lock()\n\t// Same as purge all.\n\tif lseq := fs.state.LastSeq; seq > lseq {\n\t\tfs.mu.Unlock()\n\t\treturn fs.purge(seq)\n\t}\n\t// We have to delete interior messages.\n\tsmb := fs.selectMsgBlock(seq)\n\tif smb == nil {\n\t\tfs.mu.Unlock()\n\t\treturn 0, nil\n\t}\n\n\t// All msgblocks up to this one can be thrown away.\n\tvar deleted int\n\tfor _, mb := range fs.blks {\n\t\tif mb == smb {\n\t\t\tbreak\n\t\t}\n\t\tmb.mu.Lock()\n\t\tpurged += mb.msgs\n\t\tbytes += mb.bytes\n\t\t// Make sure we do subject cleanup as well.\n\t\tmb.ensurePerSubjectInfoLoaded()\n\t\tmb.fss.IterOrdered(func(bsubj []byte, ss *SimpleState) bool {\n\t\t\tsubj := bytesToString(bsubj)\n\t\t\tfor i := uint64(0); i < ss.Msgs; i++ {\n\t\t\t\tfs.removePerSubject(subj)\n\t\t\t}\n\t\t\treturn true\n\t\t})\n\t\t// Now close.\n\t\tmb.dirtyCloseWithRemove(true)\n\t\tmb.mu.Unlock()\n\t\tdeleted++\n\t}\n\n\tvar smv StoreMsg\n\tvar err error\n\tvar tombs []msgId\n\n\tsmb.mu.Lock()\n\tif atomic.LoadUint64(&smb.first.seq) == seq {\n\t\tfs.state.FirstSeq = atomic.LoadUint64(&smb.first.seq)\n\t\tfs.state.FirstTime = time.Unix(0, smb.first.ts).UTC()\n\t\tgoto SKIP\n\t}\n\n\t// Make sure we have the messages loaded.\n\tif smb.cacheNotLoaded() {\n\t\tif err = smb.loadMsgsWithLock(); err != nil {\n\t\t\tgoto SKIP\n\t\t}\n\t}\n\tfor mseq := atomic.LoadUint64(&smb.first.seq); mseq < seq; mseq++ {\n\t\tsm, err := smb.cacheLookupNoCopy(mseq, &smv)\n\t\tif err == errDeletedMsg {\n\t\t\t// Update dmap.\n\t\t\tif !smb.dmap.IsEmpty() {\n\t\t\t\tsmb.dmap.Delete(mseq)\n\t\t\t}\n\t\t} else if sm != nil {\n\t\t\tsz := fileStoreMsgSize(sm.subj, sm.hdr, sm.msg)\n\t\t\tif smb.msgs > 0 {\n\t\t\t\tsmb.msgs--\n\t\t\t\tif sz > smb.bytes {\n\t\t\t\t\tsz = smb.bytes\n\t\t\t\t}\n\t\t\t\tsmb.bytes -= sz\n\t\t\t\tbytes += sz\n\t\t\t\tpurged++\n\t\t\t}\n\t\t\t// Update fss\n\t\t\tsmb.removeSeqPerSubject(sm.subj, mseq)\n\t\t\tfs.removePerSubject(sm.subj)\n\t\t\ttombs = append(tombs, msgId{sm.seq, sm.ts})\n\t\t}\n\t}\n\n\t// Check if empty after processing, could happen if tail of messages are all deleted.\n\tif isEmpty := smb.msgs == 0; isEmpty {\n\t\t// Only remove if not the last block.\n\t\tif smb != fs.lmb {\n\t\t\tsmb.dirtyCloseWithRemove(true)\n\t\t\tdeleted++\n\t\t} else {\n\t\t\t// Make sure to sync changes.\n\t\t\tsmb.needSync = true\n\t\t}\n\t\t// Update fs first here as well.\n\t\tfs.state.FirstSeq = atomic.LoadUint64(&smb.last.seq) + 1\n\t\tfs.state.FirstTime = time.Time{}\n\n\t} else {\n\t\t// Make sure to sync changes.\n\t\tsmb.needSync = true\n\t\t// Just for start condition for selectNextFirst.\n\t\tif smb.first.seq < seq {\n\t\t\tatomic.StoreUint64(&smb.first.seq, seq-1)\n\t\t} else {\n\t\t\t// selectNextFirst always adds 1, so need to subtract 1 here.\n\t\t\tatomic.StoreUint64(&smb.first.seq, smb.first.seq-1)\n\t\t}\n\t\tsmb.selectNextFirst()\n\n\t\tfs.state.FirstSeq = atomic.LoadUint64(&smb.first.seq)\n\t\tfs.state.FirstTime = time.Unix(0, smb.first.ts).UTC()\n\n\t\t// Check if we should reclaim the head space from this block.\n\t\t// This will be optimistic only, so don't continue if we encounter any errors here.\n\t\tif smb.rbytes > compactMinimum && smb.bytes*2 < smb.rbytes {\n\t\t\tvar moff uint32\n\t\t\tmoff, _, _, err = smb.slotInfo(int(atomic.LoadUint64(&smb.first.seq) - smb.cache.fseq))\n\t\t\tif err != nil || moff >= uint32(len(smb.cache.buf)) {\n\t\t\t\tgoto SKIP\n\t\t\t}\n\t\t\tbuf := smb.cache.buf[moff:]\n\t\t\t// Don't reuse, copy to new recycled buf.\n\t\t\tnbuf := getMsgBlockBuf(len(buf))\n\t\t\tnbuf = append(nbuf, buf...)\n\t\t\tsmb.closeFDsLockedNoCheck()\n\t\t\t// Check for encryption.\n\t\t\tif smb.bek != nil && len(nbuf) > 0 {\n\t\t\t\t// Recreate to reset counter.\n\t\t\t\tbek, err := genBlockEncryptionKey(smb.fs.fcfg.Cipher, smb.seed, smb.nonce)\n\t\t\t\tif err != nil {\n\t\t\t\t\tgoto SKIP\n\t\t\t\t}\n\t\t\t\t// For future writes make sure to set smb.bek to keep counter correct.\n\t\t\t\tsmb.bek = bek\n\t\t\t\tsmb.bek.XORKeyStream(nbuf, nbuf)\n\t\t\t}\n\t\t\t// Recompress if necessary (smb.cmp contains the algorithm used when\n\t\t\t// the block was loaded from disk, or defaults to NoCompression if not)\n\t\t\tif nbuf, err = smb.cmp.Compress(nbuf); err != nil {\n\t\t\t\tgoto SKIP\n\t\t\t}\n\t\t\t<-dios\n\t\t\terr = os.WriteFile(smb.mfn, nbuf, defaultFilePerms)\n\t\t\tdios <- struct{}{}\n\t\t\tif err != nil {\n\t\t\t\tgoto SKIP\n\t\t\t}\n\t\t\t// Make sure to remove fss state.\n\t\t\tsmb.fss = nil\n\t\t\tsmb.clearCacheAndOffset()\n\t\t\tsmb.rbytes = uint64(len(nbuf))\n\t\t\t// Make sure we don't write any additional tombstones.\n\t\t\ttombs = nil\n\t\t}\n\t}\n\nSKIP:\n\tsmb.mu.Unlock()\n\n\t// Write any tombstones as needed.\n\tfor _, tomb := range tombs {\n\t\tfs.writeTombstone(tomb.seq, tomb.ts)\n\t}\n\n\tif deleted > 0 {\n\t\t// Update block map.\n\t\tif fs.bim != nil {\n\t\t\tfor _, mb := range fs.blks[:deleted] {\n\t\t\t\tdelete(fs.bim, mb.index)\n\t\t\t}\n\t\t}\n\t\t// Update blks slice.\n\t\tfs.blks = copyMsgBlocks(fs.blks[deleted:])\n\t\tif lb := len(fs.blks); lb == 0 {\n\t\t\tfs.lmb = nil\n\t\t} else {\n\t\t\tfs.lmb = fs.blks[lb-1]\n\t\t}\n\t}\n\n\t// Update top level accounting.\n\tif purged > fs.state.Msgs {\n\t\tpurged = fs.state.Msgs\n\t}\n\tfs.state.Msgs -= purged\n\tif fs.state.Msgs == 0 {\n\t\tfs.state.FirstSeq = fs.state.LastSeq + 1\n\t\tfs.state.FirstTime = time.Time{}\n\t}\n\n\tif bytes > fs.state.Bytes {\n\t\tbytes = fs.state.Bytes\n\t}\n\tfs.state.Bytes -= bytes\n\n\t// Any existing state file no longer applicable. We will force write a new one\n\t// after we release the lock.\n\tos.Remove(filepath.Join(fs.fcfg.StoreDir, msgDir, streamStreamStateFile))\n\tfs.dirty++\n\tcb := fs.scb\n\tfs.mu.Unlock()\n\n\t// Force a new index.db to be written.\n\tif purged > 0 {\n\t\tfs.forceWriteFullState()\n\t}\n\n\tif cb != nil && purged > 0 {\n\t\tcb(-int64(purged), -int64(bytes), 0, _EMPTY_)\n\t}\n\n\treturn purged, err\n}\n\n// Will completely reset our store.\nfunc (fs *fileStore) reset() error {\n\tfs.mu.Lock()\n\tif fs.closed {\n\t\tfs.mu.Unlock()\n\t\treturn ErrStoreClosed\n\t}\n\tif fs.sips > 0 {\n\t\tfs.mu.Unlock()\n\t\treturn ErrStoreSnapshotInProgress\n\t}\n\n\tvar purged, bytes uint64\n\tcb := fs.scb\n\n\tfor _, mb := range fs.blks {\n\t\tmb.mu.Lock()\n\t\tpurged += mb.msgs\n\t\tbytes += mb.bytes\n\t\tmb.dirtyCloseWithRemove(true)\n\t\tmb.mu.Unlock()\n\t}\n\n\t// Reset\n\tfs.state.FirstSeq = 0\n\tfs.state.FirstTime = time.Time{}\n\tfs.state.LastSeq = 0\n\tfs.state.LastTime = time.Now().UTC()\n\t// Update msgs and bytes.\n\tfs.state.Msgs = 0\n\tfs.state.Bytes = 0\n\n\t// Reset blocks.\n\tfs.blks, fs.lmb = nil, nil\n\n\t// Reset subject mappings.\n\tfs.psim, fs.tsl = fs.psim.Empty(), 0\n\tfs.sdm.empty()\n\tfs.bim = make(map[uint32]*msgBlock)\n\n\t// If we purged anything, make sure we kick flush state loop.\n\tif purged > 0 {\n\t\tfs.dirty++\n\t}\n\n\tfs.mu.Unlock()\n\n\tif cb != nil {\n\t\tcb(-int64(purged), -int64(bytes), 0, _EMPTY_)\n\t}\n\n\treturn nil\n}\n\n// Return all active tombstones in this msgBlock.\nfunc (mb *msgBlock) tombs() []msgId {\n\tmb.mu.Lock()\n\tdefer mb.mu.Unlock()\n\treturn mb.tombsLocked()\n}\n\n// Return all active tombstones in this msgBlock.\n// Write lock should be held.\nfunc (mb *msgBlock) tombsLocked() []msgId {\n\tif mb.cacheNotLoaded() {\n\t\tif err := mb.loadMsgsWithLock(); err != nil {\n\t\t\treturn nil\n\t\t}\n\t}\n\n\tvar tombs []msgId\n\tvar le = binary.LittleEndian\n\tbuf := mb.cache.buf\n\n\tfor index, lbuf := uint32(0), uint32(len(buf)); index < lbuf; {\n\t\tif index+msgHdrSize > lbuf {\n\t\t\treturn tombs\n\t\t}\n\t\thdr := buf[index : index+msgHdrSize]\n\t\trl, seq := le.Uint32(hdr[0:]), le.Uint64(hdr[4:])\n\t\t// Clear any headers bit that could be set.\n\t\trl &^= hbit\n\t\t// Check for tombstones.\n\t\tif seq&tbit != 0 {\n\t\t\tts := int64(le.Uint64(hdr[12:]))\n\t\t\ttombs = append(tombs, msgId{seq &^ tbit, ts})\n\t\t}\n\t\t// Advance to next record.\n\t\tindex += rl\n\t}\n\n\treturn tombs\n}\n\n// Truncate will truncate a stream store up to seq. Sequence needs to be valid.\nfunc (fs *fileStore) Truncate(seq uint64) error {\n\t// Check for request to reset.\n\tif seq == 0 {\n\t\treturn fs.reset()\n\t}\n\n\tfs.mu.Lock()\n\n\tif fs.closed {\n\t\tfs.mu.Unlock()\n\t\treturn ErrStoreClosed\n\t}\n\tif fs.sips > 0 {\n\t\tfs.mu.Unlock()\n\t\treturn ErrStoreSnapshotInProgress\n\t}\n\n\tnlmb := fs.selectMsgBlock(seq)\n\tif nlmb == nil {\n\t\tfs.mu.Unlock()\n\t\treturn ErrInvalidSequence\n\t}\n\tlsm, _, _ := nlmb.fetchMsgNoCopy(seq, nil)\n\tif lsm == nil {\n\t\tfs.mu.Unlock()\n\t\treturn ErrInvalidSequence\n\t}\n\n\t// Set lmb to nlmb and make sure writeable.\n\tfs.lmb = nlmb\n\tif err := nlmb.enableForWriting(fs.fip); err != nil {\n\t\tfs.mu.Unlock()\n\t\treturn err\n\t}\n\t// Collect all tombstones, we want to put these back so we can survive\n\t// a restore without index.db properly.\n\tvar tombs []msgId\n\ttombs = append(tombs, nlmb.tombs()...)\n\n\tvar purged, bytes uint64\n\n\t// Truncate our new last message block.\n\tnmsgs, nbytes, err := nlmb.truncate(lsm)\n\tif err != nil {\n\t\tfs.mu.Unlock()\n\t\treturn fmt.Errorf(\"nlmb.truncate: %w\", err)\n\t}\n\t// Account for the truncated msgs and bytes.\n\tpurged += nmsgs\n\tbytes += nbytes\n\n\t// Remove any left over msg blocks.\n\tgetLastMsgBlock := func() *msgBlock { return fs.blks[len(fs.blks)-1] }\n\tfor mb := getLastMsgBlock(); mb != nlmb; mb = getLastMsgBlock() {\n\t\tmb.mu.Lock()\n\t\t// We do this to load tombs.\n\t\ttombs = append(tombs, mb.tombsLocked()...)\n\t\tpurged += mb.msgs\n\t\tbytes += mb.bytes\n\t\tfs.removeMsgBlock(mb)\n\t\tmb.mu.Unlock()\n\t}\n\n\t// Reset last.\n\tfs.state.LastSeq = lsm.seq\n\tfs.state.LastTime = time.Unix(0, lsm.ts).UTC()\n\t// Update msgs and bytes.\n\tif purged > fs.state.Msgs {\n\t\tpurged = fs.state.Msgs\n\t}\n\tfs.state.Msgs -= purged\n\tif bytes > fs.state.Bytes {\n\t\tbytes = fs.state.Bytes\n\t}\n\tfs.state.Bytes -= bytes\n\n\t// Reset our subject lookup info.\n\tfs.resetGlobalPerSubjectInfo()\n\n\t// Always create new write block.\n\tfs.newMsgBlockForWrite()\n\n\t// Write any tombstones as needed.\n\tfor _, tomb := range tombs {\n\t\tif tomb.seq <= lsm.seq {\n\t\t\tfs.writeTombstone(tomb.seq, tomb.ts)\n\t\t}\n\t}\n\n\t// Any existing state file no longer applicable. We will force write a new one\n\t// after we release the lock.\n\tos.Remove(filepath.Join(fs.fcfg.StoreDir, msgDir, streamStreamStateFile))\n\tfs.dirty++\n\n\tcb := fs.scb\n\tfs.mu.Unlock()\n\n\t// Force a new index.db to be written.\n\tif purged > 0 {\n\t\tfs.forceWriteFullState()\n\t}\n\n\tif cb != nil {\n\t\tcb(-int64(purged), -int64(bytes), 0, _EMPTY_)\n\t}\n\n\treturn nil\n}\n\nfunc (fs *fileStore) lastSeq() uint64 {\n\tfs.mu.RLock()\n\tseq := fs.state.LastSeq\n\tfs.mu.RUnlock()\n\treturn seq\n}\n\n// Returns number of msg blks.\nfunc (fs *fileStore) numMsgBlocks() int {\n\tfs.mu.RLock()\n\tdefer fs.mu.RUnlock()\n\treturn len(fs.blks)\n}\n\n// Will add a new msgBlock.\n// Lock should be held.\nfunc (fs *fileStore) addMsgBlock(mb *msgBlock) {\n\tfs.blks = append(fs.blks, mb)\n\tfs.lmb = mb\n\tfs.bim[mb.index] = mb\n}\n\n// Remove from our list of blks.\n// Both locks should be held.\nfunc (fs *fileStore) removeMsgBlockFromList(mb *msgBlock) {\n\t// Remove from list.\n\tfor i, omb := range fs.blks {\n\t\tif mb == omb {\n\t\t\tfs.dirty++\n\t\t\tblks := append(fs.blks[:i], fs.blks[i+1:]...)\n\t\t\tfs.blks = copyMsgBlocks(blks)\n\t\t\tif fs.bim != nil {\n\t\t\t\tdelete(fs.bim, mb.index)\n\t\t\t}\n\t\t\tbreak\n\t\t}\n\t}\n}\n\n// Removes the msgBlock\n// Both locks should be held.\nfunc (fs *fileStore) removeMsgBlock(mb *msgBlock) {\n\t// Check for us being last message block\n\tif mb == fs.lmb {\n\t\tlseq, lts := atomic.LoadUint64(&mb.last.seq), mb.last.ts\n\t\t// Creating a new message write block requires that the lmb lock is not held.\n\t\tmb.mu.Unlock()\n\t\t// Write the tombstone to remember since this was last block.\n\t\tif lmb, _ := fs.newMsgBlockForWrite(); lmb != nil {\n\t\t\tfs.writeTombstone(lseq, lts)\n\t\t}\n\t\tmb.mu.Lock()\n\t}\n\t// Only delete message block after (potentially) writing a new lmb.\n\tmb.dirtyCloseWithRemove(true)\n\tfs.removeMsgBlockFromList(mb)\n}\n\n// Called by purge to simply get rid of the cache and close our fds.\n// Lock should not be held.\nfunc (mb *msgBlock) dirtyClose() {\n\tmb.mu.Lock()\n\tdefer mb.mu.Unlock()\n\tmb.dirtyCloseWithRemove(false)\n}\n\n// Should be called with lock held.\nfunc (mb *msgBlock) dirtyCloseWithRemove(remove bool) error {\n\tif mb == nil {\n\t\treturn nil\n\t}\n\t// Stop cache expiration timer.\n\tif mb.ctmr != nil {\n\t\tmb.ctmr.Stop()\n\t\tmb.ctmr = nil\n\t}\n\t// Close cache\n\tmb.clearCacheAndOffset()\n\t// Quit our loops.\n\tif mb.qch != nil {\n\t\tclose(mb.qch)\n\t\tmb.qch = nil\n\t}\n\tif mb.mfd != nil {\n\t\tmb.mfd.Close()\n\t\tmb.mfd = nil\n\t}\n\tif remove {\n\t\t// Clear any tracking by subject if we are removing.\n\t\tmb.fss = nil\n\t\tif mb.mfn != _EMPTY_ {\n\t\t\terr := os.Remove(mb.mfn)\n\t\t\tif isPermissionError(err) {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tmb.mfn = _EMPTY_\n\t\t}\n\t\tif mb.kfn != _EMPTY_ {\n\t\t\terr := os.Remove(mb.kfn)\n\t\t\tif isPermissionError(err) {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t}\n\treturn nil\n}\n\n// Remove a seq from the fss and select new first.\n// Lock should be held.\nfunc (mb *msgBlock) removeSeqPerSubject(subj string, seq uint64) uint64 {\n\tmb.ensurePerSubjectInfoLoaded()\n\tif mb.fss == nil {\n\t\treturn 0\n\t}\n\tbsubj := stringToBytes(subj)\n\tss, ok := mb.fss.Find(bsubj)\n\tif !ok || ss == nil {\n\t\treturn 0\n\t}\n\n\tmb.fs.sdm.removeSeqAndSubject(seq, subj)\n\tif ss.Msgs == 1 {\n\t\tmb.fss.Delete(bsubj)\n\t\treturn 0\n\t}\n\n\tss.Msgs--\n\n\t// Only one left.\n\tif ss.Msgs == 1 {\n\t\tif !ss.lastNeedsUpdate && seq != ss.Last {\n\t\t\tss.First = ss.Last\n\t\t\tss.firstNeedsUpdate = false\n\t\t\treturn 1\n\t\t}\n\t\tif !ss.firstNeedsUpdate && seq != ss.First {\n\t\t\tss.Last = ss.First\n\t\t\tss.lastNeedsUpdate = false\n\t\t\treturn 1\n\t\t}\n\t}\n\n\t// We can lazily calculate the first/last sequence when needed.\n\tss.firstNeedsUpdate = seq == ss.First || ss.firstNeedsUpdate\n\tss.lastNeedsUpdate = seq == ss.Last || ss.lastNeedsUpdate\n\n\treturn ss.Msgs\n}\n\n// Will recalculate the first and/or last sequence for this subject in this block.\n// Will avoid slower path message lookups and scan the cache directly instead.\nfunc (mb *msgBlock) recalculateForSubj(subj string, ss *SimpleState) {\n\t// Need to make sure messages are loaded.\n\tif mb.cacheNotLoaded() {\n\t\tif err := mb.loadMsgsWithLock(); err != nil {\n\t\t\treturn\n\t\t}\n\t}\n\n\tstartSlot := int(ss.First - mb.cache.fseq)\n\tif startSlot < 0 {\n\t\tstartSlot = 0\n\t}\n\tif startSlot >= len(mb.cache.idx) {\n\t\tss.First = ss.Last\n\t\tss.firstNeedsUpdate = false\n\t\tss.lastNeedsUpdate = false\n\t\treturn\n\t}\n\n\tendSlot := int(ss.Last - mb.cache.fseq)\n\tif endSlot < 0 {\n\t\tendSlot = 0\n\t}\n\tif endSlot >= len(mb.cache.idx) || startSlot > endSlot {\n\t\treturn\n\t}\n\n\tvar le = binary.LittleEndian\n\tif ss.firstNeedsUpdate {\n\t\t// Mark first as updated.\n\t\tss.firstNeedsUpdate = false\n\n\t\tfseq := ss.First + 1\n\t\tif mbFseq := atomic.LoadUint64(&mb.first.seq); fseq < mbFseq {\n\t\t\tfseq = mbFseq\n\t\t}\n\t\tfor slot := startSlot; slot < len(mb.cache.idx); slot++ {\n\t\t\tbi := mb.cache.idx[slot] &^ cbit\n\t\t\tif bi == dbit {\n\t\t\t\t// delete marker so skip.\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tli := int(bi) - mb.cache.off\n\t\t\tif li >= len(mb.cache.buf) {\n\t\t\t\tss.First = ss.Last\n\t\t\t\t// Only need to reset ss.lastNeedsUpdate, ss.firstNeedsUpdate is already reset above.\n\t\t\t\tss.lastNeedsUpdate = false\n\t\t\t\treturn\n\t\t\t}\n\t\t\tbuf := mb.cache.buf[li:]\n\t\t\thdr := buf[:msgHdrSize]\n\t\t\tslen := int(le.Uint16(hdr[20:]))\n\t\t\tif subj == bytesToString(buf[msgHdrSize:msgHdrSize+slen]) {\n\t\t\t\tseq := le.Uint64(hdr[4:])\n\t\t\t\tif seq < fseq || seq&ebit != 0 || mb.dmap.Exists(seq) {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tss.First = seq\n\t\t\t\tif ss.Msgs == 1 {\n\t\t\t\t\tss.Last = seq\n\t\t\t\t\tss.lastNeedsUpdate = false\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t\t// Skip the start slot ahead, if we need to recalculate last we can stop early.\n\t\t\t\tstartSlot = slot\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t}\n\tif ss.lastNeedsUpdate {\n\t\t// Mark last as updated.\n\t\tss.lastNeedsUpdate = false\n\n\t\tlseq := ss.Last - 1\n\t\tif mbLseq := atomic.LoadUint64(&mb.last.seq); lseq > mbLseq {\n\t\t\tlseq = mbLseq\n\t\t}\n\t\tfor slot := endSlot; slot >= startSlot; slot-- {\n\t\t\tbi := mb.cache.idx[slot] &^ cbit\n\t\t\tif bi == dbit {\n\t\t\t\t// delete marker so skip.\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tli := int(bi) - mb.cache.off\n\t\t\tif li >= len(mb.cache.buf) {\n\t\t\t\t// Can't overwrite ss.Last, just skip.\n\t\t\t\treturn\n\t\t\t}\n\t\t\tbuf := mb.cache.buf[li:]\n\t\t\thdr := buf[:msgHdrSize]\n\t\t\tslen := int(le.Uint16(hdr[20:]))\n\t\t\tif subj == bytesToString(buf[msgHdrSize:msgHdrSize+slen]) {\n\t\t\t\tseq := le.Uint64(hdr[4:])\n\t\t\t\tif seq > lseq || seq&ebit != 0 || mb.dmap.Exists(seq) {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\t// Sequence should never be lower, but guard against it nonetheless.\n\t\t\t\tif seq < ss.First {\n\t\t\t\t\tseq = ss.First\n\t\t\t\t}\n\t\t\t\tss.Last = seq\n\t\t\t\tif ss.Msgs == 1 {\n\t\t\t\t\tss.First = seq\n\t\t\t\t\tss.firstNeedsUpdate = false\n\t\t\t\t}\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}\n}\n\n// Lock should be held.\nfunc (fs *fileStore) resetGlobalPerSubjectInfo() {\n\t// Clear any global subject state.\n\tfs.psim, fs.tsl = fs.psim.Empty(), 0\n\tfor _, mb := range fs.blks {\n\t\tfs.populateGlobalPerSubjectInfo(mb)\n\t}\n}\n\n// Lock should be held.\nfunc (mb *msgBlock) resetPerSubjectInfo() error {\n\tmb.fss = nil\n\treturn mb.generatePerSubjectInfo()\n}\n\n// generatePerSubjectInfo will generate the per subject info via the raw msg block.\n// Lock should be held.\nfunc (mb *msgBlock) generatePerSubjectInfo() error {\n\t// Check if this mb is empty. This can happen when its the last one and we are holding onto it for seq and timestamp info.\n\tif mb.msgs == 0 {\n\t\treturn nil\n\t}\n\n\tif mb.cacheNotLoaded() {\n\t\tif err := mb.loadMsgsWithLock(); err != nil {\n\t\t\treturn err\n\t\t}\n\t\t// indexCacheBuf can produce fss now, so if non-nil we are good.\n\t\tif mb.fss != nil {\n\t\t\treturn nil\n\t\t}\n\t}\n\n\t// Create new one regardless.\n\tmb.fss = mb.fss.Empty()\n\n\tvar smv StoreMsg\n\tfseq, lseq := atomic.LoadUint64(&mb.first.seq), atomic.LoadUint64(&mb.last.seq)\n\tfor seq := fseq; seq <= lseq; seq++ {\n\t\tif mb.dmap.Exists(seq) {\n\t\t\t// Optimisation to avoid calling cacheLookup which hits time.Now().\n\t\t\t// It gets set later on if the fss is non-empty anyway.\n\t\t\tcontinue\n\t\t}\n\t\tsm, err := mb.cacheLookupNoCopy(seq, &smv)\n\t\tif err != nil {\n\t\t\t// Since we are walking by sequence we can ignore some errors that are benign to rebuilding our state.\n\t\t\tif err == ErrStoreMsgNotFound || err == errDeletedMsg {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif err == errNoCache {\n\t\t\t\treturn nil\n\t\t\t}\n\t\t\treturn err\n\t\t}\n\t\tif sm != nil && len(sm.subj) > 0 {\n\t\t\tif ss, ok := mb.fss.Find(stringToBytes(sm.subj)); ok && ss != nil {\n\t\t\t\tss.Msgs++\n\t\t\t\tss.Last = seq\n\t\t\t\tss.lastNeedsUpdate = false\n\t\t\t} else {\n\t\t\t\tmb.fss.Insert(stringToBytes(sm.subj), SimpleState{Msgs: 1, First: seq, Last: seq})\n\t\t\t}\n\t\t}\n\t}\n\n\tif mb.fss.Size() > 0 {\n\t\t// Make sure we run the cache expire timer.\n\t\tmb.llts = ats.AccessTime()\n\t\t// Mark fss activity same as load time.\n\t\tmb.lsts = mb.llts\n\t\tmb.startCacheExpireTimer()\n\t}\n\treturn nil\n}\n\n// Helper to make sure fss loaded if we are tracking.\n// Lock should be held\nfunc (mb *msgBlock) ensurePerSubjectInfoLoaded() error {\n\tif mb.fss != nil || mb.noTrack {\n\t\tif mb.fss != nil {\n\t\t\t// Mark fss activity.\n\t\t\tmb.lsts = ats.AccessTime()\n\t\t}\n\t\treturn nil\n\t}\n\tif mb.msgs == 0 {\n\t\tmb.fss = stree.NewSubjectTree[SimpleState]()\n\t\treturn nil\n\t}\n\treturn mb.generatePerSubjectInfo()\n}\n\n// Called on recovery to populate the global psim state.\n// Lock should be held.\nfunc (fs *fileStore) populateGlobalPerSubjectInfo(mb *msgBlock) {\n\tmb.mu.Lock()\n\tdefer mb.mu.Unlock()\n\n\tif err := mb.ensurePerSubjectInfoLoaded(); err != nil {\n\t\treturn\n\t}\n\n\t// Now populate psim.\n\tmb.fss.IterFast(func(bsubj []byte, ss *SimpleState) bool {\n\t\tif len(bsubj) > 0 {\n\t\t\tif info, ok := fs.psim.Find(bsubj); ok {\n\t\t\t\tinfo.total += ss.Msgs\n\t\t\t\tif mb.index > info.lblk {\n\t\t\t\t\tinfo.lblk = mb.index\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tfs.psim.Insert(bsubj, psi{total: ss.Msgs, fblk: mb.index, lblk: mb.index})\n\t\t\t\tfs.tsl += len(bsubj)\n\t\t\t}\n\t\t}\n\t\treturn true\n\t})\n}\n\n// Close the message block.\nfunc (mb *msgBlock) close(sync bool) {\n\tif mb == nil {\n\t\treturn\n\t}\n\tmb.mu.Lock()\n\tdefer mb.mu.Unlock()\n\n\tif mb.closed {\n\t\treturn\n\t}\n\n\t// Stop cache expiration timer.\n\tif mb.ctmr != nil {\n\t\tmb.ctmr.Stop()\n\t\tmb.ctmr = nil\n\t}\n\n\t// Clear fss.\n\tmb.fss = nil\n\n\t// Close cache\n\tmb.clearCacheAndOffset()\n\t// Quit our loops.\n\tif mb.qch != nil {\n\t\tclose(mb.qch)\n\t\tmb.qch = nil\n\t}\n\tif mb.mfd != nil {\n\t\tif sync {\n\t\t\tmb.mfd.Sync()\n\t\t}\n\t\tmb.mfd.Close()\n\t}\n\tmb.mfd = nil\n\t// Mark as closed.\n\tmb.closed = true\n}\n\nfunc (fs *fileStore) closeAllMsgBlocks(sync bool) {\n\tfor _, mb := range fs.blks {\n\t\tmb.close(sync)\n\t}\n}\n\nfunc (fs *fileStore) Delete() error {\n\tif fs.isClosed() {\n\t\t// Always attempt to remove since we could have been closed beforehand.\n\t\tos.RemoveAll(fs.fcfg.StoreDir)\n\t\t// Since we did remove, if we did have anything remaining make sure to\n\t\t// call into any storage updates that had been registered.\n\t\tfs.mu.Lock()\n\t\tcb, msgs, bytes := fs.scb, int64(fs.state.Msgs), int64(fs.state.Bytes)\n\t\t// Guard against double accounting if called twice.\n\t\tfs.state.Msgs, fs.state.Bytes = 0, 0\n\t\tfs.mu.Unlock()\n\t\tif msgs > 0 && cb != nil {\n\t\t\tcb(-msgs, -bytes, 0, _EMPTY_)\n\t\t}\n\t\treturn ErrStoreClosed\n\t}\n\n\tpdir := filepath.Join(fs.fcfg.StoreDir, purgeDir)\n\t// If purge directory still exists then we need to wait\n\t// in place and remove since rename would fail.\n\tif _, err := os.Stat(pdir); err == nil {\n\t\tos.RemoveAll(pdir)\n\t}\n\n\t// Quickly close all blocks and simulate a purge w/o overhead an new write block.\n\tfs.mu.Lock()\n\tfor _, mb := range fs.blks {\n\t\tmb.dirtyClose()\n\t}\n\tdmsgs := fs.state.Msgs\n\tdbytes := int64(fs.state.Bytes)\n\tfs.state.Msgs, fs.state.Bytes = 0, 0\n\tfs.blks = nil\n\tcb := fs.scb\n\tfs.mu.Unlock()\n\n\tif cb != nil {\n\t\tcb(-int64(dmsgs), -dbytes, 0, _EMPTY_)\n\t}\n\n\tif err := fs.stop(true, false); err != nil {\n\t\treturn err\n\t}\n\n\t// Make sure we will not try to recover if killed before removal below completes.\n\tif err := os.Remove(filepath.Join(fs.fcfg.StoreDir, JetStreamMetaFile)); err != nil {\n\t\treturn err\n\t}\n\t// Now move into different directory with \".\" prefix.\n\tndir := filepath.Join(filepath.Dir(fs.fcfg.StoreDir), tsep+filepath.Base(fs.fcfg.StoreDir))\n\tif err := os.Rename(fs.fcfg.StoreDir, ndir); err != nil {\n\t\treturn err\n\t}\n\t// Do this in separate Go routine in case lots of blocks.\n\t// Purge above protects us as does the removal of meta artifacts above.\n\tgo func() {\n\t\t<-dios\n\t\terr := os.RemoveAll(ndir)\n\t\tdios <- struct{}{}\n\t\tif err == nil {\n\t\t\treturn\n\t\t}\n\t\tttl := time.Now().Add(time.Second)\n\t\tfor time.Now().Before(ttl) {\n\t\t\ttime.Sleep(10 * time.Millisecond)\n\t\t\t<-dios\n\t\t\terr = os.RemoveAll(ndir)\n\t\t\tdios <- struct{}{}\n\t\t\tif err == nil {\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}()\n\n\treturn nil\n}\n\n// Lock should be held.\nfunc (fs *fileStore) setSyncTimer() {\n\tif fs.syncTmr != nil {\n\t\tfs.syncTmr.Reset(fs.fcfg.SyncInterval)\n\t} else {\n\t\t// First time this fires will be between SyncInterval/2 and SyncInterval,\n\t\t// so that different stores are spread out, rather than having many of\n\t\t// them trying to all sync at once, causing blips and contending dios.\n\t\tstart := (fs.fcfg.SyncInterval / 2) + (time.Duration(mrand.Int63n(int64(fs.fcfg.SyncInterval / 2))))\n\t\tfs.syncTmr = time.AfterFunc(start, fs.syncBlocks)\n\t}\n}\n\n// Lock should be held.\nfunc (fs *fileStore) cancelSyncTimer() {\n\tif fs.syncTmr != nil {\n\t\tfs.syncTmr.Stop()\n\t\tfs.syncTmr = nil\n\t}\n}\n\n// The full state file is versioned.\n// - 0x1: original binary index.db format\n// - 0x2: adds support for TTL count field after num deleted\nconst (\n\tfullStateMagic      = uint8(11)\n\tfullStateMinVersion = uint8(1) // What is the minimum version we know how to parse?\n\tfullStateVersion    = uint8(2) // What is the current version written out to index.db?\n)\n\n// This go routine periodically writes out our full stream state index.\nfunc (fs *fileStore) flushStreamStateLoop(qch, done chan struct{}) {\n\t// Signal we are done on exit.\n\tdefer close(done)\n\n\t// Make sure we do not try to write these out too fast.\n\t// Spread these out for large numbers on a server restart.\n\tconst writeThreshold = 2 * time.Minute\n\twriteJitter := time.Duration(mrand.Int63n(int64(30 * time.Second)))\n\tt := time.NewTicker(writeThreshold + writeJitter)\n\tdefer t.Stop()\n\n\tfor {\n\t\tselect {\n\t\tcase <-t.C:\n\t\t\terr := fs.writeFullState()\n\t\t\tif isPermissionError(err) && fs.srv != nil {\n\t\t\t\tfs.warn(\"File system permission denied when flushing stream state, disabling JetStream: %v\", err)\n\t\t\t\t// messages in block cache could be lost in the worst case.\n\t\t\t\t// In the clustered mode it is very highly unlikely as a result of replication.\n\t\t\t\tfs.srv.DisableJetStream()\n\t\t\t\treturn\n\t\t\t}\n\n\t\tcase <-qch:\n\t\t\treturn\n\t\t}\n\t}\n}\n\n// Helper since unixnano of zero time undefined.\nfunc timestampNormalized(t time.Time) int64 {\n\tif t.IsZero() {\n\t\treturn 0\n\t}\n\treturn t.UnixNano()\n}\n\n// writeFullState will proceed to write the full meta state iff not complex and time consuming.\n// Since this is for quick recovery it is optional and should not block/stall normal operations.\nfunc (fs *fileStore) writeFullState() error {\n\treturn fs._writeFullState(false)\n}\n\n// forceWriteFullState will proceed to write the full meta state. This should only be called by stop()\nfunc (fs *fileStore) forceWriteFullState() error {\n\treturn fs._writeFullState(true)\n}\n\n// This will write the full binary state for the stream.\n// This plus everything new since last hash will be the total recovered state.\n// This state dump will have the following.\n// 1. Stream summary - Msgs, Bytes, First and Last (Sequence and Timestamp)\n// 2. PSIM - Per Subject Index Map - Tracks first and last blocks with subjects present.\n// 3. MBs - Index, Bytes, First and Last Sequence and Timestamps, and the deleted map (avl.seqset).\n// 4. Last block index and hash of record inclusive to this stream state.\nfunc (fs *fileStore) _writeFullState(force bool) error {\n\tstart := time.Now()\n\tfs.mu.Lock()\n\tif fs.closed || fs.dirty == 0 {\n\t\tfs.mu.Unlock()\n\t\treturn nil\n\t}\n\n\t// For calculating size and checking time costs for non forced calls.\n\tnumSubjects := fs.numSubjects()\n\n\t// If we are not being forced to write out our state, check the complexity for time costs as to not\n\t// block or stall normal operations.\n\t// We will base off of number of subjects and interior deletes. A very large number of msg blocks could also\n\t// be used, but for next server version will redo all meta handling to be disk based. So this is temporary.\n\tif !force {\n\t\tconst numThreshold = 1_000_000\n\t\t// Calculate interior deletes.\n\t\tvar numDeleted int\n\t\tif fs.state.LastSeq > fs.state.FirstSeq {\n\t\t\tnumDeleted = int((fs.state.LastSeq - fs.state.FirstSeq + 1) - fs.state.Msgs)\n\t\t}\n\t\tif numSubjects > numThreshold || numDeleted > numThreshold {\n\t\t\tfs.mu.Unlock()\n\t\t\treturn errStateTooBig\n\t\t}\n\t}\n\n\t// We track this through subsequent runs to get an avg per blk used for subsequent runs.\n\tavgDmapLen := fs.adml\n\t// If first time through could be 0\n\tif avgDmapLen == 0 && ((fs.state.LastSeq-fs.state.FirstSeq+1)-fs.state.Msgs) > 0 {\n\t\tavgDmapLen = 1024\n\t}\n\n\t// Calculate and estimate of the uper bound on the  size to avoid multiple allocations.\n\tsz := hdrLen + // Magic and Version\n\t\t(binary.MaxVarintLen64 * 6) + // FS data\n\t\tbinary.MaxVarintLen64 + fs.tsl + // NumSubjects + total subject length\n\t\tnumSubjects*(binary.MaxVarintLen64*4) + // psi record\n\t\tbinary.MaxVarintLen64 + // Num blocks.\n\t\tlen(fs.blks)*((binary.MaxVarintLen64*8)+avgDmapLen) + // msg blocks, avgDmapLen is est for dmaps\n\t\tbinary.MaxVarintLen64 + 8 + 8 // last index + record checksum + full state checksum\n\n\t// Do 4k on stack if possible.\n\tconst ssz = 4 * 1024\n\tvar buf []byte\n\n\tif sz <= ssz {\n\t\tvar _buf [ssz]byte\n\t\tbuf, sz = _buf[0:hdrLen:ssz], ssz\n\t} else {\n\t\tbuf = make([]byte, hdrLen, sz)\n\t}\n\n\tbuf[0], buf[1] = fullStateMagic, fullStateVersion\n\tbuf = binary.AppendUvarint(buf, fs.state.Msgs)\n\tbuf = binary.AppendUvarint(buf, fs.state.Bytes)\n\tbuf = binary.AppendUvarint(buf, fs.state.FirstSeq)\n\tbuf = binary.AppendVarint(buf, timestampNormalized(fs.state.FirstTime))\n\tbuf = binary.AppendUvarint(buf, fs.state.LastSeq)\n\tbuf = binary.AppendVarint(buf, timestampNormalized(fs.state.LastTime))\n\n\t// Do per subject information map if applicable.\n\tbuf = binary.AppendUvarint(buf, uint64(numSubjects))\n\tif numSubjects > 0 {\n\t\tfs.psim.Match([]byte(fwcs), func(subj []byte, psi *psi) {\n\t\t\tbuf = binary.AppendUvarint(buf, uint64(len(subj)))\n\t\t\tbuf = append(buf, subj...)\n\t\t\tbuf = binary.AppendUvarint(buf, psi.total)\n\t\t\tbuf = binary.AppendUvarint(buf, uint64(psi.fblk))\n\t\t\tif psi.total > 1 {\n\t\t\t\tbuf = binary.AppendUvarint(buf, uint64(psi.lblk))\n\t\t\t}\n\t\t})\n\t}\n\n\t// Now walk all blocks and write out first and last and optional dmap encoding.\n\tvar lbi uint32\n\tvar lchk [8]byte\n\n\tnb := len(fs.blks)\n\tbuf = binary.AppendUvarint(buf, uint64(nb))\n\n\t// Use basetime to save some space.\n\tbaseTime := timestampNormalized(fs.state.FirstTime)\n\tvar scratch [8 * 1024]byte\n\n\t// Track the state as represented by the mbs.\n\tvar mstate StreamState\n\n\tvar dmapTotalLen int\n\tfor _, mb := range fs.blks {\n\t\tmb.mu.RLock()\n\t\tbuf = binary.AppendUvarint(buf, uint64(mb.index))\n\t\tbuf = binary.AppendUvarint(buf, mb.bytes)\n\t\tbuf = binary.AppendUvarint(buf, atomic.LoadUint64(&mb.first.seq))\n\t\tbuf = binary.AppendVarint(buf, mb.first.ts-baseTime)\n\t\tbuf = binary.AppendUvarint(buf, atomic.LoadUint64(&mb.last.seq))\n\t\tbuf = binary.AppendVarint(buf, mb.last.ts-baseTime)\n\n\t\tnumDeleted := mb.dmap.Size()\n\t\tbuf = binary.AppendUvarint(buf, uint64(numDeleted))\n\t\tbuf = binary.AppendUvarint(buf, mb.ttls) // Field is new in version 2\n\t\tif numDeleted > 0 {\n\t\t\tdmap, _ := mb.dmap.Encode(scratch[:0])\n\t\t\tdmapTotalLen += len(dmap)\n\t\t\tbuf = append(buf, dmap...)\n\t\t}\n\t\t// If this is the last one grab the last checksum and the block index, e.g. 22.blk, 22 is the block index.\n\t\t// We use this to quickly open this file on recovery.\n\t\tif mb == fs.lmb {\n\t\t\tlbi = mb.index\n\t\t\tmb.ensureLastChecksumLoaded()\n\t\t\tcopy(lchk[0:], mb.lchk[:])\n\t\t}\n\t\tupdateTrackingState(&mstate, mb)\n\t\tmb.mu.RUnlock()\n\t}\n\tif dmapTotalLen > 0 {\n\t\tfs.adml = dmapTotalLen / len(fs.blks)\n\t}\n\n\t// Place block index and hash onto the end.\n\tbuf = binary.AppendUvarint(buf, uint64(lbi))\n\tbuf = append(buf, lchk[:]...)\n\n\t// Encrypt if needed.\n\tif fs.prf != nil {\n\t\tif err := fs.setupAEK(); err != nil {\n\t\t\tfs.mu.Unlock()\n\t\t\treturn err\n\t\t}\n\t\tnonce := make([]byte, fs.aek.NonceSize(), fs.aek.NonceSize()+len(buf)+fs.aek.Overhead())\n\t\tif n, err := rand.Read(nonce); err != nil {\n\t\t\treturn err\n\t\t} else if n != len(nonce) {\n\t\t\treturn fmt.Errorf(\"not enough nonce bytes read (%d != %d)\", n, len(nonce))\n\t\t}\n\t\tbuf = fs.aek.Seal(nonce, nonce, buf, nil)\n\t}\n\n\tfn := filepath.Join(fs.fcfg.StoreDir, msgDir, streamStreamStateFile)\n\n\tfs.hh.Reset()\n\tfs.hh.Write(buf)\n\tbuf = fs.hh.Sum(buf)\n\n\t// Snapshot prior dirty count.\n\tpriorDirty := fs.dirty\n\n\tstatesEqual := trackingStatesEqual(&fs.state, &mstate)\n\t// Release lock.\n\tfs.mu.Unlock()\n\n\t// Check consistency here.\n\tif !statesEqual {\n\t\tfs.warn(\"Stream state encountered internal inconsistency on write\")\n\t\t// Rebuild our fs state from the mb state.\n\t\tfs.rebuildState(nil)\n\t\treturn errCorruptState\n\t}\n\n\tif cap(buf) > sz {\n\t\tfs.debug(\"WriteFullState reallocated from %d to %d\", sz, cap(buf))\n\t}\n\n\t// Only warn about construction time since file write not holding any locks.\n\tif took := time.Since(start); took > time.Minute {\n\t\tfs.warn(\"WriteFullState took %v (%d bytes)\", took.Round(time.Millisecond), len(buf))\n\t}\n\n\t// Write our update index.db\n\t// Protect with dios.\n\t<-dios\n\terr := os.WriteFile(fn, buf, defaultFilePerms)\n\t// if file system is not writable isPermissionError is set to true\n\tdios <- struct{}{}\n\tif isPermissionError(err) {\n\t\treturn err\n\t}\n\n\t// Update dirty if successful.\n\tif err == nil {\n\t\tfs.mu.Lock()\n\t\tfs.dirty -= priorDirty\n\t\tfs.mu.Unlock()\n\t}\n\n\treturn fs.writeTTLState()\n}\n\nfunc (fs *fileStore) writeTTLState() error {\n\tif fs.ttls == nil {\n\t\treturn nil\n\t}\n\n\tfs.mu.RLock()\n\tfn := filepath.Join(fs.fcfg.StoreDir, msgDir, ttlStreamStateFile)\n\t// Must be lseq+1 to identify up to which sequence the TTLs are valid.\n\tbuf := fs.ttls.Encode(fs.state.LastSeq + 1)\n\tfs.mu.RUnlock()\n\n\t<-dios\n\terr := os.WriteFile(fn, buf, defaultFilePerms)\n\tdios <- struct{}{}\n\n\treturn err\n}\n\n// Stop the current filestore.\nfunc (fs *fileStore) Stop() error {\n\treturn fs.stop(false, true)\n}\n\n// Stop the current filestore.\nfunc (fs *fileStore) stop(delete, writeState bool) error {\n\tfs.mu.Lock()\n\tif fs.closed || fs.closing {\n\t\tfs.mu.Unlock()\n\t\treturn ErrStoreClosed\n\t}\n\n\t// Mark as closing. Do before releasing the lock to writeFullState\n\t// so we don't end up with this function running more than once.\n\tfs.closing = true\n\n\tif writeState {\n\t\tfs.checkAndFlushAllBlocks()\n\t}\n\tfs.closeAllMsgBlocks(false)\n\n\tfs.cancelSyncTimer()\n\tfs.cancelAgeChk()\n\n\t// Release the state flusher loop.\n\tif fs.qch != nil {\n\t\tclose(fs.qch)\n\t\tfs.qch = nil\n\t}\n\n\tif writeState {\n\t\t// Wait for the state flush loop to exit.\n\t\tfsld := fs.fsld\n\t\tfs.mu.Unlock()\n\t\t<-fsld\n\t\t// Write full state if needed. If not dirty this is a no-op.\n\t\tfs.forceWriteFullState()\n\t\tfs.mu.Lock()\n\t}\n\n\t// Mark as closed. Last message block needs to be cleared after\n\t// writeFullState has completed.\n\tfs.closed = true\n\tfs.lmb = nil\n\n\t// We should update the upper usage layer on a stop.\n\tcb, bytes := fs.scb, int64(fs.state.Bytes)\n\tfs.mu.Unlock()\n\n\tfs.cmu.Lock()\n\tvar _cfs [256]ConsumerStore\n\tcfs := append(_cfs[:0], fs.cfs...)\n\tfs.cfs = nil\n\tfs.cmu.Unlock()\n\n\tfor _, o := range cfs {\n\t\tif delete {\n\t\t\to.StreamDelete()\n\t\t} else {\n\t\t\to.Stop()\n\t\t}\n\t}\n\n\tif bytes > 0 && cb != nil {\n\t\tcb(0, -bytes, 0, _EMPTY_)\n\t}\n\n\t// Unregister from the access time service.\n\tats.Unregister()\n\n\treturn nil\n}\n\nconst errFile = \"errors.txt\"\n\n// Stream our snapshot through S2 compression and tar.\nfunc (fs *fileStore) streamSnapshot(w io.WriteCloser, includeConsumers bool, errCh chan string) {\n\tdefer close(errCh)\n\tdefer w.Close()\n\n\tenc := s2.NewWriter(w)\n\tdefer enc.Close()\n\n\ttw := tar.NewWriter(enc)\n\tdefer tw.Close()\n\n\tdefer func() {\n\t\tfs.mu.Lock()\n\t\tfs.sips--\n\t\tfs.mu.Unlock()\n\t}()\n\n\tmodTime := time.Now().UTC()\n\n\twriteFile := func(name string, buf []byte) error {\n\t\thdr := &tar.Header{\n\t\t\tName:    name,\n\t\t\tMode:    0600,\n\t\t\tModTime: modTime,\n\t\t\tUname:   \"nats\",\n\t\t\tGname:   \"nats\",\n\t\t\tSize:    int64(len(buf)),\n\t\t\tFormat:  tar.FormatPAX,\n\t\t}\n\t\tif err := tw.WriteHeader(hdr); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif _, err := tw.Write(buf); err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t}\n\n\twriteErr := func(err string) {\n\t\twriteFile(errFile, []byte(err))\n\t\terrCh <- err\n\t}\n\n\tfs.mu.Lock()\n\tblks := fs.blks\n\t// Grab our general meta data.\n\t// We do this now instead of pulling from files since they could be encrypted.\n\tmeta, err := json.Marshal(fs.cfg)\n\tif err != nil {\n\t\tfs.mu.Unlock()\n\t\twriteErr(fmt.Sprintf(\"Could not gather stream meta file: %v\", err))\n\t\treturn\n\t}\n\thh := fs.hh\n\thh.Reset()\n\thh.Write(meta)\n\tsum := []byte(hex.EncodeToString(fs.hh.Sum(nil)))\n\tfs.mu.Unlock()\n\n\t// Meta first.\n\tif writeFile(JetStreamMetaFile, meta) != nil {\n\t\treturn\n\t}\n\tif writeFile(JetStreamMetaFileSum, sum) != nil {\n\t\treturn\n\t}\n\n\t// Can't use join path here, tar only recognizes relative paths with forward slashes.\n\tmsgPre := msgDir + \"/\"\n\tvar bbuf []byte\n\n\t// Now do messages themselves.\n\tfor _, mb := range blks {\n\t\tif mb.pendingWriteSize() > 0 {\n\t\t\tmb.flushPendingMsgs()\n\t\t}\n\t\tmb.mu.Lock()\n\t\t// We could stream but don't want to hold the lock and prevent changes, so just read in and\n\t\t// release the lock for now.\n\t\tbbuf, err = mb.loadBlock(bbuf)\n\t\tif err != nil {\n\t\t\tmb.mu.Unlock()\n\t\t\twriteErr(fmt.Sprintf(\"Could not read message block [%d]: %v\", mb.index, err))\n\t\t\treturn\n\t\t}\n\t\t// Check for encryption.\n\t\tif mb.bek != nil && len(bbuf) > 0 {\n\t\t\trbek, err := genBlockEncryptionKey(fs.fcfg.Cipher, mb.seed, mb.nonce)\n\t\t\tif err != nil {\n\t\t\t\tmb.mu.Unlock()\n\t\t\t\twriteErr(fmt.Sprintf(\"Could not create encryption key for message block [%d]: %v\", mb.index, err))\n\t\t\t\treturn\n\t\t\t}\n\t\t\trbek.XORKeyStream(bbuf, bbuf)\n\t\t}\n\t\t// Check for compression.\n\t\tif bbuf, err = mb.decompressIfNeeded(bbuf); err != nil {\n\t\t\tmb.mu.Unlock()\n\t\t\twriteErr(fmt.Sprintf(\"Could not decompress message block [%d]: %v\", mb.index, err))\n\t\t\treturn\n\t\t}\n\t\tmb.mu.Unlock()\n\n\t\t// Do this one unlocked.\n\t\tif writeFile(msgPre+fmt.Sprintf(blkScan, mb.index), bbuf) != nil {\n\t\t\treturn\n\t\t}\n\t}\n\n\t// Do index.db last. We will force a write as well.\n\t// Write out full state as well before proceeding.\n\tif err := fs.forceWriteFullState(); err == nil {\n\t\tconst minLen = 32\n\t\tsfn := filepath.Join(fs.fcfg.StoreDir, msgDir, streamStreamStateFile)\n\t\tif buf, err := os.ReadFile(sfn); err == nil && len(buf) >= minLen {\n\t\t\tif fs.aek != nil {\n\t\t\t\tns := fs.aek.NonceSize()\n\t\t\t\tbuf, err = fs.aek.Open(nil, buf[:ns], buf[ns:len(buf)-highwayhash.Size64], nil)\n\t\t\t\tif err == nil {\n\t\t\t\t\t// Redo hash checksum at end on plaintext.\n\t\t\t\t\tfs.mu.Lock()\n\t\t\t\t\thh.Reset()\n\t\t\t\t\thh.Write(buf)\n\t\t\t\t\tbuf = fs.hh.Sum(buf)\n\t\t\t\t\tfs.mu.Unlock()\n\t\t\t\t}\n\t\t\t}\n\t\t\tif err == nil && writeFile(msgPre+streamStreamStateFile, buf) != nil {\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}\n\n\t// Bail if no consumers requested.\n\tif !includeConsumers {\n\t\treturn\n\t}\n\n\t// Do consumers' state last.\n\tfs.cmu.RLock()\n\tcfs := fs.cfs\n\tfs.cmu.RUnlock()\n\n\tfor _, cs := range cfs {\n\t\to, ok := cs.(*consumerFileStore)\n\t\tif !ok {\n\t\t\tcontinue\n\t\t}\n\t\to.mu.Lock()\n\t\t// Grab our general meta data.\n\t\t// We do this now instead of pulling from files since they could be encrypted.\n\t\tmeta, err := json.Marshal(o.cfg)\n\t\tif err != nil {\n\t\t\to.mu.Unlock()\n\t\t\twriteErr(fmt.Sprintf(\"Could not gather consumer meta file for %q: %v\", o.name, err))\n\t\t\treturn\n\t\t}\n\t\to.hh.Reset()\n\t\to.hh.Write(meta)\n\t\tsum := []byte(hex.EncodeToString(o.hh.Sum(nil)))\n\n\t\t// We can have the running state directly encoded now.\n\t\tstate, err := o.encodeState()\n\t\tif err != nil {\n\t\t\to.mu.Unlock()\n\t\t\twriteErr(fmt.Sprintf(\"Could not encode consumer state for %q: %v\", o.name, err))\n\t\t\treturn\n\t\t}\n\t\todirPre := filepath.Join(consumerDir, o.name)\n\t\to.mu.Unlock()\n\n\t\t// Write all the consumer files.\n\t\tif writeFile(filepath.Join(odirPre, JetStreamMetaFile), meta) != nil {\n\t\t\treturn\n\t\t}\n\t\tif writeFile(filepath.Join(odirPre, JetStreamMetaFileSum), sum) != nil {\n\t\t\treturn\n\t\t}\n\t\twriteFile(filepath.Join(odirPre, consumerState), state)\n\t}\n}\n\n// Create a snapshot of this stream and its consumer's state along with messages.\nfunc (fs *fileStore) Snapshot(deadline time.Duration, checkMsgs, includeConsumers bool) (*SnapshotResult, error) {\n\tfs.mu.Lock()\n\tif fs.closed {\n\t\tfs.mu.Unlock()\n\t\treturn nil, ErrStoreClosed\n\t}\n\t// Only allow one at a time.\n\tif fs.sips > 0 {\n\t\tfs.mu.Unlock()\n\t\treturn nil, ErrStoreSnapshotInProgress\n\t}\n\t// Mark us as snapshotting\n\tfs.sips += 1\n\tfs.mu.Unlock()\n\n\tif checkMsgs {\n\t\tld := fs.checkMsgs()\n\t\tif ld != nil && len(ld.Msgs) > 0 {\n\t\t\treturn nil, fmt.Errorf(\"snapshot check detected %d bad messages\", len(ld.Msgs))\n\t\t}\n\t}\n\n\tpr, pw := net.Pipe()\n\n\t// Set a write deadline here to protect ourselves.\n\tif deadline > 0 {\n\t\tpw.SetWriteDeadline(time.Now().Add(deadline))\n\t}\n\n\t// We can add to our stream while snapshotting but not \"user\" delete anything.\n\tvar state StreamState\n\tfs.FastState(&state)\n\n\t// Stream in separate Go routine.\n\terrCh := make(chan string, 1)\n\tgo fs.streamSnapshot(pw, includeConsumers, errCh)\n\n\treturn &SnapshotResult{pr, state, errCh}, nil\n}\n\n// Helper to return the config.\nfunc (fs *fileStore) fileStoreConfig() FileStoreConfig {\n\tfs.mu.RLock()\n\tdefer fs.mu.RUnlock()\n\treturn fs.fcfg\n}\n\n// Read lock all existing message blocks.\n// Lock held on entry.\nfunc (fs *fileStore) readLockAllMsgBlocks() {\n\tfor _, mb := range fs.blks {\n\t\tmb.mu.RLock()\n\t}\n}\n\n// Read unlock all existing message blocks.\n// Lock held on entry.\nfunc (fs *fileStore) readUnlockAllMsgBlocks() {\n\tfor _, mb := range fs.blks {\n\t\tmb.mu.RUnlock()\n\t}\n}\n\n// Binary encoded state snapshot, >= v2.10 server.\nfunc (fs *fileStore) EncodedStreamState(failed uint64) ([]byte, error) {\n\tfs.mu.RLock()\n\tdefer fs.mu.RUnlock()\n\n\t// Calculate deleted.\n\tvar numDeleted int64\n\tif fs.state.LastSeq > fs.state.FirstSeq {\n\t\tnumDeleted = int64(fs.state.LastSeq-fs.state.FirstSeq+1) - int64(fs.state.Msgs)\n\t\tif numDeleted < 0 {\n\t\t\tnumDeleted = 0\n\t\t}\n\t}\n\n\t// Encoded is Msgs, Bytes, FirstSeq, LastSeq, Failed, NumDeleted and optional DeletedBlocks\n\tvar buf [1024]byte\n\tbuf[0], buf[1] = streamStateMagic, streamStateVersion\n\tn := hdrLen\n\tn += binary.PutUvarint(buf[n:], fs.state.Msgs)\n\tn += binary.PutUvarint(buf[n:], fs.state.Bytes)\n\tn += binary.PutUvarint(buf[n:], fs.state.FirstSeq)\n\tn += binary.PutUvarint(buf[n:], fs.state.LastSeq)\n\tn += binary.PutUvarint(buf[n:], failed)\n\tn += binary.PutUvarint(buf[n:], uint64(numDeleted))\n\n\tb := buf[0:n]\n\n\tif numDeleted > 0 {\n\t\tvar scratch [4 * 1024]byte\n\n\t\tfs.readLockAllMsgBlocks()\n\t\tdefer fs.readUnlockAllMsgBlocks()\n\n\t\tfor _, db := range fs.deleteBlocks() {\n\t\t\tswitch db := db.(type) {\n\t\t\tcase *DeleteRange:\n\t\t\t\tfirst, _, num := db.State()\n\t\t\t\tscratch[0] = runLengthMagic\n\t\t\t\ti := 1\n\t\t\t\ti += binary.PutUvarint(scratch[i:], first)\n\t\t\t\ti += binary.PutUvarint(scratch[i:], num)\n\t\t\t\tb = append(b, scratch[0:i]...)\n\t\t\tcase *avl.SequenceSet:\n\t\t\t\tbuf, err := db.Encode(scratch[:0])\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn nil, err\n\t\t\t\t}\n\t\t\t\tb = append(b, buf...)\n\t\t\tdefault:\n\t\t\t\treturn nil, errors.New(\"no impl\")\n\t\t\t}\n\t\t}\n\t}\n\n\treturn b, nil\n}\n\n// We used to be more sophisticated to save memory, but speed is more important.\n// All blocks should be at least read locked.\nfunc (fs *fileStore) deleteBlocks() DeleteBlocks {\n\tvar dbs DeleteBlocks\n\tvar prevLast uint64\n\n\tfor _, mb := range fs.blks {\n\t\t// Detect if we have a gap between these blocks.\n\t\tfseq := atomic.LoadUint64(&mb.first.seq)\n\t\tif prevLast > 0 && prevLast+1 != fseq {\n\t\t\tdbs = append(dbs, &DeleteRange{First: prevLast + 1, Num: fseq - prevLast - 1})\n\t\t}\n\t\tif mb.dmap.Size() > 0 {\n\t\t\tdbs = append(dbs, &mb.dmap)\n\t\t}\n\t\tprevLast = atomic.LoadUint64(&mb.last.seq)\n\t}\n\treturn dbs\n}\n\n// SyncDeleted will make sure this stream has same deleted state as dbs.\n// This will only process deleted state within our current state.\nfunc (fs *fileStore) SyncDeleted(dbs DeleteBlocks) {\n\tif len(dbs) == 0 {\n\t\treturn\n\t}\n\n\tfs.mu.Lock()\n\tdefer fs.mu.Unlock()\n\n\tlseq := fs.state.LastSeq\n\tvar needsCheck DeleteBlocks\n\n\tfs.readLockAllMsgBlocks()\n\tmdbs := fs.deleteBlocks()\n\tfor i, db := range dbs {\n\t\tfirst, last, num := db.State()\n\t\t// If the block is same as what we have we can skip.\n\t\tif i < len(mdbs) {\n\t\t\teFirst, eLast, eNum := mdbs[i].State()\n\t\t\tif first == eFirst && last == eLast && num == eNum {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t} else if first > lseq {\n\t\t\t// Skip blocks not applicable to our current state.\n\t\t\tcontinue\n\t\t}\n\t\t// Need to insert these.\n\t\tneedsCheck = append(needsCheck, db)\n\t}\n\tfs.readUnlockAllMsgBlocks()\n\n\tfor _, db := range needsCheck {\n\t\tdb.Range(func(dseq uint64) bool {\n\t\t\tfs.removeMsg(dseq, false, true, false)\n\t\t\treturn true\n\t\t})\n\t}\n}\n\n////////////////////////////////////////////////////////////////////////////////\n// Consumers\n////////////////////////////////////////////////////////////////////////////////\n\ntype consumerFileStore struct {\n\tmu      sync.Mutex\n\tfs      *fileStore\n\tcfg     *FileConsumerInfo\n\tprf     keyGen\n\taek     cipher.AEAD\n\tname    string\n\todir    string\n\tifn     string\n\thh      hash.Hash64\n\tstate   ConsumerState\n\tfch     chan struct{}\n\tqch     chan struct{}\n\tflusher bool\n\twriting bool\n\tdirty   bool\n\tclosed  bool\n}\n\nfunc (fs *fileStore) ConsumerStore(name string, cfg *ConsumerConfig) (ConsumerStore, error) {\n\tif fs == nil {\n\t\treturn nil, fmt.Errorf(\"filestore is nil\")\n\t}\n\tif fs.isClosed() {\n\t\treturn nil, ErrStoreClosed\n\t}\n\tif cfg == nil || name == _EMPTY_ {\n\t\treturn nil, fmt.Errorf(\"bad consumer config\")\n\t}\n\n\t// We now allow overrides from a stream being a filestore type and forcing a consumer to be memory store.\n\tif cfg.MemoryStorage {\n\t\t// Create directly here.\n\t\to := &consumerMemStore{ms: fs, cfg: *cfg}\n\t\tfs.AddConsumer(o)\n\t\treturn o, nil\n\t}\n\n\todir := filepath.Join(fs.fcfg.StoreDir, consumerDir, name)\n\tif err := os.MkdirAll(odir, defaultDirPerms); err != nil {\n\t\treturn nil, fmt.Errorf(\"could not create consumer directory - %v\", err)\n\t}\n\tcsi := &FileConsumerInfo{Name: name, Created: time.Now().UTC(), ConsumerConfig: *cfg}\n\to := &consumerFileStore{\n\t\tfs:   fs,\n\t\tcfg:  csi,\n\t\tprf:  fs.prf,\n\t\tname: name,\n\t\todir: odir,\n\t\tifn:  filepath.Join(odir, consumerState),\n\t}\n\tkey := sha256.Sum256([]byte(fs.cfg.Name + \"/\" + name))\n\thh, err := highwayhash.New64(key[:])\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"could not create hash: %v\", err)\n\t}\n\to.hh = hh\n\n\t// Check for encryption.\n\tif o.prf != nil {\n\t\tif ekey, err := os.ReadFile(filepath.Join(odir, JetStreamMetaFileKey)); err == nil {\n\t\t\tif len(ekey) < minBlkKeySize {\n\t\t\t\treturn nil, errBadKeySize\n\t\t\t}\n\t\t\t// Recover key encryption key.\n\t\t\trb, err := fs.prf([]byte(fs.cfg.Name + tsep + o.name))\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\n\t\t\tsc := fs.fcfg.Cipher\n\t\t\tkek, err := genEncryptionKey(sc, rb)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tns := kek.NonceSize()\n\t\t\tnonce := ekey[:ns]\n\t\t\tseed, err := kek.Open(nil, nonce, ekey[ns:], nil)\n\t\t\tif err != nil {\n\t\t\t\t// We may be here on a cipher conversion, so attempt to convert.\n\t\t\t\tif err = o.convertCipher(); err != nil {\n\t\t\t\t\treturn nil, err\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\to.aek, err = genEncryptionKey(sc, seed)\n\t\t\t}\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t}\n\t}\n\n\t// Track if we are creating the directory so that we can clean up if we encounter an error.\n\tvar didCreate bool\n\n\t// Write our meta data iff does not exist.\n\tmeta := filepath.Join(odir, JetStreamMetaFile)\n\tif _, err := os.Stat(meta); err != nil && os.IsNotExist(err) {\n\t\tdidCreate = true\n\t\tcsi.Created = time.Now().UTC()\n\t\tif err := o.writeConsumerMeta(); err != nil {\n\t\t\tos.RemoveAll(odir)\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\t// If we expect to be encrypted check that what we are restoring is not plaintext.\n\t// This can happen on snapshot restores or conversions.\n\tif o.prf != nil {\n\t\tkeyFile := filepath.Join(odir, JetStreamMetaFileKey)\n\t\tif _, err := os.Stat(keyFile); err != nil && os.IsNotExist(err) {\n\t\t\tif err := o.writeConsumerMeta(); err != nil {\n\t\t\t\tif didCreate {\n\t\t\t\t\tos.RemoveAll(odir)\n\t\t\t\t}\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\t// Redo the state file as well here if we have one and we can tell it was plaintext.\n\t\t\tif buf, err := os.ReadFile(o.ifn); err == nil {\n\t\t\t\tif _, err := decodeConsumerState(buf); err == nil {\n\t\t\t\t\tstate, err := o.encryptState(buf)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn nil, err\n\t\t\t\t\t}\n\t\t\t\t\terr = fs.writeFileWithOptionalSync(o.ifn, state, defaultFilePerms)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\tif didCreate {\n\t\t\t\t\t\t\tos.RemoveAll(odir)\n\t\t\t\t\t\t}\n\t\t\t\t\t\treturn nil, err\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\t// Create channels to control our flush go routine.\n\to.fch = make(chan struct{}, 1)\n\to.qch = make(chan struct{})\n\tgo o.flushLoop(o.fch, o.qch)\n\n\t// Make sure to load in our state from disk if needed.\n\to.loadState()\n\n\t// Assign to filestore.\n\tfs.AddConsumer(o)\n\n\treturn o, nil\n}\n\nfunc (o *consumerFileStore) convertCipher() error {\n\tfs := o.fs\n\todir := filepath.Join(fs.fcfg.StoreDir, consumerDir, o.name)\n\n\tekey, err := os.ReadFile(filepath.Join(odir, JetStreamMetaFileKey))\n\tif err != nil {\n\t\treturn err\n\t}\n\tif len(ekey) < minBlkKeySize {\n\t\treturn errBadKeySize\n\t}\n\t// Recover key encryption key.\n\trb, err := fs.prf([]byte(fs.cfg.Name + tsep + o.name))\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Do these in reverse since converting.\n\tsc := fs.fcfg.Cipher\n\tosc := AES\n\tif sc == AES {\n\t\tosc = ChaCha\n\t}\n\tkek, err := genEncryptionKey(osc, rb)\n\tif err != nil {\n\t\treturn err\n\t}\n\tns := kek.NonceSize()\n\tnonce := ekey[:ns]\n\tseed, err := kek.Open(nil, nonce, ekey[ns:], nil)\n\tif err != nil {\n\t\treturn err\n\t}\n\taek, err := genEncryptionKey(osc, seed)\n\tif err != nil {\n\t\treturn err\n\t}\n\t// Now read in and decode our state using the old cipher.\n\tbuf, err := os.ReadFile(o.ifn)\n\tif err != nil {\n\t\treturn err\n\t}\n\tbuf, err = aek.Open(nil, buf[:ns], buf[ns:], nil)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Since we are here we recovered our old state.\n\t// Now write our meta, which will generate the new keys with the new cipher.\n\tif err := o.writeConsumerMeta(); err != nil {\n\t\treturn err\n\t}\n\n\t// Now write out or state with the new cipher.\n\treturn o.writeState(buf)\n}\n\n// Kick flusher for this consumer.\n// Lock should be held.\nfunc (o *consumerFileStore) kickFlusher() {\n\tif o.fch != nil {\n\t\tselect {\n\t\tcase o.fch <- struct{}{}:\n\t\tdefault:\n\t\t}\n\t}\n\to.dirty = true\n}\n\n// Set in flusher status\nfunc (o *consumerFileStore) setInFlusher() {\n\to.mu.Lock()\n\to.flusher = true\n\to.mu.Unlock()\n}\n\n// Clear in flusher status\nfunc (o *consumerFileStore) clearInFlusher() {\n\to.mu.Lock()\n\to.flusher = false\n\to.mu.Unlock()\n}\n\n// Report in flusher status\nfunc (o *consumerFileStore) inFlusher() bool {\n\to.mu.Lock()\n\tdefer o.mu.Unlock()\n\treturn o.flusher\n}\n\n// flushLoop watches for consumer updates and the quit channel.\nfunc (o *consumerFileStore) flushLoop(fch, qch chan struct{}) {\n\n\to.setInFlusher()\n\tdefer o.clearInFlusher()\n\n\t// Maintain approximately 10 updates per second per consumer under load.\n\tconst minTime = 100 * time.Millisecond\n\tvar lastWrite time.Time\n\tvar dt *time.Timer\n\n\tsetDelayTimer := func(addWait time.Duration) {\n\t\tif dt == nil {\n\t\t\tdt = time.NewTimer(addWait)\n\t\t\treturn\n\t\t}\n\t\tif !dt.Stop() {\n\t\t\tselect {\n\t\t\tcase <-dt.C:\n\t\t\tdefault:\n\t\t\t}\n\t\t}\n\t\tdt.Reset(addWait)\n\t}\n\n\tfor {\n\t\tselect {\n\t\tcase <-fch:\n\t\t\tif ts := time.Since(lastWrite); ts < minTime {\n\t\t\t\tsetDelayTimer(minTime - ts)\n\t\t\t\tselect {\n\t\t\t\tcase <-dt.C:\n\t\t\t\tcase <-qch:\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}\n\t\t\to.mu.Lock()\n\t\t\tif o.closed {\n\t\t\t\to.mu.Unlock()\n\t\t\t\treturn\n\t\t\t}\n\t\t\tbuf, err := o.encodeState()\n\t\t\to.mu.Unlock()\n\t\t\tif err != nil {\n\t\t\t\treturn\n\t\t\t}\n\t\t\t// TODO(dlc) - if we error should start failing upwards.\n\t\t\tif err := o.writeState(buf); err == nil {\n\t\t\t\tlastWrite = time.Now()\n\t\t\t}\n\t\tcase <-qch:\n\t\t\treturn\n\t\t}\n\t}\n}\n\n// SetStarting sets our starting stream sequence.\nfunc (o *consumerFileStore) SetStarting(sseq uint64) error {\n\to.mu.Lock()\n\to.state.Delivered.Stream = sseq\n\tbuf, err := o.encodeState()\n\to.mu.Unlock()\n\tif err != nil {\n\t\treturn err\n\t}\n\treturn o.writeState(buf)\n}\n\n// UpdateStarting updates our starting stream sequence.\nfunc (o *consumerFileStore) UpdateStarting(sseq uint64) {\n\to.mu.Lock()\n\tdefer o.mu.Unlock()\n\n\tif sseq > o.state.Delivered.Stream {\n\t\to.state.Delivered.Stream = sseq\n\t\t// For AckNone just update delivered and ackfloor at the same time.\n\t\tif o.cfg.AckPolicy == AckNone {\n\t\t\to.state.AckFloor.Stream = sseq\n\t\t}\n\t}\n\t// Make sure we flush to disk.\n\to.kickFlusher()\n}\n\n// HasState returns if this store has a recorded state.\nfunc (o *consumerFileStore) HasState() bool {\n\to.mu.Lock()\n\t// We have a running state, or stored on disk but not yet initialized.\n\tif o.state.Delivered.Consumer != 0 || o.state.Delivered.Stream != 0 {\n\t\to.mu.Unlock()\n\t\treturn true\n\t}\n\t_, err := os.Stat(o.ifn)\n\to.mu.Unlock()\n\treturn err == nil\n}\n\n// UpdateDelivered is called whenever a new message has been delivered.\nfunc (o *consumerFileStore) UpdateDelivered(dseq, sseq, dc uint64, ts int64) error {\n\to.mu.Lock()\n\tdefer o.mu.Unlock()\n\n\tif dc != 1 && o.cfg.AckPolicy == AckNone {\n\t\treturn ErrNoAckPolicy\n\t}\n\n\t// On restarts the old leader may get a replay from the raft logs that are old.\n\tif dseq <= o.state.AckFloor.Consumer {\n\t\treturn nil\n\t}\n\n\t// See if we expect an ack for this.\n\tif o.cfg.AckPolicy != AckNone {\n\t\t// Need to create pending records here.\n\t\tif o.state.Pending == nil {\n\t\t\to.state.Pending = make(map[uint64]*Pending)\n\t\t}\n\t\tvar p *Pending\n\t\t// Check for an update to a message already delivered.\n\t\tif sseq <= o.state.Delivered.Stream {\n\t\t\tif p = o.state.Pending[sseq]; p != nil {\n\t\t\t\t// Do not update p.Sequence, that should be the original delivery sequence.\n\t\t\t\tp.Timestamp = ts\n\t\t\t}\n\t\t} else {\n\t\t\t// Add to pending.\n\t\t\to.state.Pending[sseq] = &Pending{dseq, ts}\n\t\t}\n\t\t// Update delivered as needed.\n\t\tif dseq > o.state.Delivered.Consumer {\n\t\t\to.state.Delivered.Consumer = dseq\n\t\t}\n\t\tif sseq > o.state.Delivered.Stream {\n\t\t\to.state.Delivered.Stream = sseq\n\t\t}\n\n\t\tif dc > 1 {\n\t\t\tif maxdc := uint64(o.cfg.MaxDeliver); maxdc > 0 && dc > maxdc {\n\t\t\t\t// Make sure to remove from pending.\n\t\t\t\tdelete(o.state.Pending, sseq)\n\t\t\t}\n\t\t\tif o.state.Redelivered == nil {\n\t\t\t\to.state.Redelivered = make(map[uint64]uint64)\n\t\t\t}\n\t\t\t// Only update if greater than what we already have.\n\t\t\tif o.state.Redelivered[sseq] < dc-1 {\n\t\t\t\to.state.Redelivered[sseq] = dc - 1\n\t\t\t}\n\t\t}\n\t} else {\n\t\t// For AckNone just update delivered and ackfloor at the same time.\n\t\tif dseq > o.state.Delivered.Consumer {\n\t\t\to.state.Delivered.Consumer = dseq\n\t\t\to.state.AckFloor.Consumer = dseq\n\t\t}\n\t\tif sseq > o.state.Delivered.Stream {\n\t\t\to.state.Delivered.Stream = sseq\n\t\t\to.state.AckFloor.Stream = sseq\n\t\t}\n\t}\n\t// Make sure we flush to disk.\n\to.kickFlusher()\n\n\treturn nil\n}\n\n// UpdateAcks is called whenever a consumer with explicit ack or ack all acks a message.\nfunc (o *consumerFileStore) UpdateAcks(dseq, sseq uint64) error {\n\to.mu.Lock()\n\tdefer o.mu.Unlock()\n\n\tif o.cfg.AckPolicy == AckNone {\n\t\treturn ErrNoAckPolicy\n\t}\n\n\t// On restarts the old leader may get a replay from the raft logs that are old.\n\tif dseq <= o.state.AckFloor.Consumer {\n\t\treturn nil\n\t}\n\n\tif len(o.state.Pending) == 0 || o.state.Pending[sseq] == nil {\n\t\tdelete(o.state.Redelivered, sseq)\n\t\treturn ErrStoreMsgNotFound\n\t}\n\n\t// Check for AckAll here.\n\tif o.cfg.AckPolicy == AckAll {\n\t\tsgap := sseq - o.state.AckFloor.Stream\n\t\to.state.AckFloor.Consumer = dseq\n\t\to.state.AckFloor.Stream = sseq\n\t\tif sgap > uint64(len(o.state.Pending)) {\n\t\t\tfor seq := range o.state.Pending {\n\t\t\t\tif seq <= sseq {\n\t\t\t\t\tdelete(o.state.Pending, seq)\n\t\t\t\t\tdelete(o.state.Redelivered, seq)\n\t\t\t\t}\n\t\t\t}\n\t\t} else {\n\t\t\tfor seq := sseq; seq > sseq-sgap && len(o.state.Pending) > 0; seq-- {\n\t\t\t\tdelete(o.state.Pending, seq)\n\t\t\t\tdelete(o.state.Redelivered, seq)\n\t\t\t}\n\t\t}\n\t\to.kickFlusher()\n\t\treturn nil\n\t}\n\n\t// AckExplicit\n\n\t// First delete from our pending state.\n\tif p, ok := o.state.Pending[sseq]; ok {\n\t\tdelete(o.state.Pending, sseq)\n\t\tif dseq > p.Sequence && p.Sequence > 0 {\n\t\t\tdseq = p.Sequence // Use the original.\n\t\t}\n\t}\n\tif len(o.state.Pending) == 0 {\n\t\to.state.AckFloor.Consumer = o.state.Delivered.Consumer\n\t\to.state.AckFloor.Stream = o.state.Delivered.Stream\n\t} else if dseq == o.state.AckFloor.Consumer+1 {\n\t\to.state.AckFloor.Consumer = dseq\n\t\to.state.AckFloor.Stream = sseq\n\n\t\tif o.state.Delivered.Consumer > dseq {\n\t\t\tfor ss := sseq + 1; ss <= o.state.Delivered.Stream; ss++ {\n\t\t\t\tif p, ok := o.state.Pending[ss]; ok {\n\t\t\t\t\tif p.Sequence > 0 {\n\t\t\t\t\t\to.state.AckFloor.Consumer = p.Sequence - 1\n\t\t\t\t\t\to.state.AckFloor.Stream = ss - 1\n\t\t\t\t\t}\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\t// We do these regardless.\n\tdelete(o.state.Redelivered, sseq)\n\n\to.kickFlusher()\n\treturn nil\n}\n\nconst seqsHdrSize = 6*binary.MaxVarintLen64 + hdrLen\n\n// Encode our consumer state, version 2.\n// Lock should be held.\n\nfunc (o *consumerFileStore) EncodedState() ([]byte, error) {\n\to.mu.Lock()\n\tdefer o.mu.Unlock()\n\treturn o.encodeState()\n}\n\nfunc (o *consumerFileStore) encodeState() ([]byte, error) {\n\t// Grab reference to state, but make sure we load in if needed, so do not reference o.state directly.\n\tstate, err := o.stateWithCopyLocked(false)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn encodeConsumerState(state), nil\n}\n\nfunc (o *consumerFileStore) UpdateConfig(cfg *ConsumerConfig) error {\n\to.mu.Lock()\n\tdefer o.mu.Unlock()\n\n\t// This is mostly unchecked here. We are assuming the upper layers have done sanity checking.\n\tcsi := o.cfg\n\tcsi.ConsumerConfig = *cfg\n\n\treturn o.writeConsumerMeta()\n}\n\nfunc (o *consumerFileStore) Update(state *ConsumerState) error {\n\t// Sanity checks.\n\tif state.AckFloor.Consumer > state.Delivered.Consumer {\n\t\treturn fmt.Errorf(\"bad ack floor for consumer\")\n\t}\n\tif state.AckFloor.Stream > state.Delivered.Stream {\n\t\treturn fmt.Errorf(\"bad ack floor for stream\")\n\t}\n\n\t// Copy to our state.\n\tvar pending map[uint64]*Pending\n\tvar redelivered map[uint64]uint64\n\tif len(state.Pending) > 0 {\n\t\tpending = make(map[uint64]*Pending, len(state.Pending))\n\t\tfor seq, p := range state.Pending {\n\t\t\tpending[seq] = &Pending{p.Sequence, p.Timestamp}\n\t\t\tif seq <= state.AckFloor.Stream || seq > state.Delivered.Stream {\n\t\t\t\treturn fmt.Errorf(\"bad pending entry, sequence [%d] out of range\", seq)\n\t\t\t}\n\t\t}\n\t}\n\tif len(state.Redelivered) > 0 {\n\t\tredelivered = make(map[uint64]uint64, len(state.Redelivered))\n\t\tfor seq, dc := range state.Redelivered {\n\t\t\tredelivered[seq] = dc\n\t\t}\n\t}\n\n\t// Replace our state.\n\to.mu.Lock()\n\tdefer o.mu.Unlock()\n\n\t// Check to see if this is an outdated update.\n\tif state.Delivered.Consumer < o.state.Delivered.Consumer || state.AckFloor.Stream < o.state.AckFloor.Stream {\n\t\treturn fmt.Errorf(\"old update ignored\")\n\t}\n\n\to.state.Delivered = state.Delivered\n\to.state.AckFloor = state.AckFloor\n\to.state.Pending = pending\n\to.state.Redelivered = redelivered\n\n\to.kickFlusher()\n\n\treturn nil\n}\n\n// Will encrypt the state with our asset key. Will be a no-op if encryption not enabled.\n// Lock should be held.\nfunc (o *consumerFileStore) encryptState(buf []byte) ([]byte, error) {\n\tif o.aek == nil {\n\t\treturn buf, nil\n\t}\n\t// TODO(dlc) - Optimize on space usage a bit?\n\tnonce := make([]byte, o.aek.NonceSize(), o.aek.NonceSize()+len(buf)+o.aek.Overhead())\n\tif n, err := rand.Read(nonce); err != nil {\n\t\treturn nil, err\n\t} else if n != len(nonce) {\n\t\treturn nil, fmt.Errorf(\"not enough nonce bytes read (%d != %d)\", n, len(nonce))\n\t}\n\treturn o.aek.Seal(nonce, nonce, buf, nil), nil\n}\n\n// Used to limit number of disk IO calls in flight since they could all be blocking an OS thread.\n// https://github.com/nats-io/nats-server/issues/2742\nvar dios chan struct{}\n\n// Used to setup our simplistic counting semaphore using buffered channels.\n// golang.org's semaphore seemed a bit heavy.\nfunc init() {\n\t// Limit ourselves to a sensible number of blocking I/O calls. Range between\n\t// 4-16 concurrent disk I/Os based on CPU cores, or 50% of cores if greater\n\t// than 32 cores.\n\tmp := runtime.GOMAXPROCS(-1)\n\tnIO := min(16, max(4, mp))\n\tif mp > 32 {\n\t\t// If the system has more than 32 cores then limit dios to 50% of cores.\n\t\tnIO = max(16, min(mp, mp/2))\n\t}\n\tdios = make(chan struct{}, nIO)\n\t// Fill it up to start.\n\tfor i := 0; i < nIO; i++ {\n\t\tdios <- struct{}{}\n\t}\n}\n\nfunc (o *consumerFileStore) writeState(buf []byte) error {\n\t// Check if we have the index file open.\n\to.mu.Lock()\n\tif o.writing || len(buf) == 0 {\n\t\to.mu.Unlock()\n\t\treturn nil\n\t}\n\n\t// Check on encryption.\n\tif o.aek != nil {\n\t\tvar err error\n\t\tif buf, err = o.encryptState(buf); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\to.writing = true\n\to.dirty = false\n\tifn := o.ifn\n\to.mu.Unlock()\n\n\t// Lock not held here but we do limit number of outstanding calls that could block OS threads.\n\terr := o.fs.writeFileWithOptionalSync(ifn, buf, defaultFilePerms)\n\n\to.mu.Lock()\n\tif err != nil {\n\t\to.dirty = true\n\t}\n\to.writing = false\n\to.mu.Unlock()\n\n\treturn err\n}\n\n// Will upodate the config. Only used when recovering ephemerals.\nfunc (o *consumerFileStore) updateConfig(cfg ConsumerConfig) error {\n\to.mu.Lock()\n\tdefer o.mu.Unlock()\n\to.cfg = &FileConsumerInfo{ConsumerConfig: cfg}\n\treturn o.writeConsumerMeta()\n}\n\n// Write out the consumer meta data, i.e. state.\n// Lock should be held.\nfunc (cfs *consumerFileStore) writeConsumerMeta() error {\n\tmeta := filepath.Join(cfs.odir, JetStreamMetaFile)\n\tif _, err := os.Stat(meta); err != nil && !os.IsNotExist(err) {\n\t\treturn err\n\t}\n\n\tif cfs.prf != nil && cfs.aek == nil {\n\t\tfs := cfs.fs\n\t\tkey, _, _, encrypted, err := fs.genEncryptionKeys(fs.cfg.Name + tsep + cfs.name)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tcfs.aek = key\n\t\tkeyFile := filepath.Join(cfs.odir, JetStreamMetaFileKey)\n\t\tif _, err := os.Stat(keyFile); err != nil && !os.IsNotExist(err) {\n\t\t\treturn err\n\t\t}\n\t\terr = cfs.fs.writeFileWithOptionalSync(keyFile, encrypted, defaultFilePerms)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\tb, err := json.Marshal(cfs.cfg)\n\tif err != nil {\n\t\treturn err\n\t}\n\t// Encrypt if needed.\n\tif cfs.aek != nil {\n\t\tnonce := make([]byte, cfs.aek.NonceSize(), cfs.aek.NonceSize()+len(b)+cfs.aek.Overhead())\n\t\tif n, err := rand.Read(nonce); err != nil {\n\t\t\treturn err\n\t\t} else if n != len(nonce) {\n\t\t\treturn fmt.Errorf(\"not enough nonce bytes read (%d != %d)\", n, len(nonce))\n\t\t}\n\t\tb = cfs.aek.Seal(nonce, nonce, b, nil)\n\t}\n\n\terr = cfs.fs.writeFileWithOptionalSync(meta, b, defaultFilePerms)\n\tif err != nil {\n\t\treturn err\n\t}\n\tcfs.hh.Reset()\n\tcfs.hh.Write(b)\n\tchecksum := hex.EncodeToString(cfs.hh.Sum(nil))\n\tsum := filepath.Join(cfs.odir, JetStreamMetaFileSum)\n\n\terr = cfs.fs.writeFileWithOptionalSync(sum, []byte(checksum), defaultFilePerms)\n\tif err != nil {\n\t\treturn err\n\t}\n\treturn nil\n}\n\n// Consumer version.\nfunc checkConsumerHeader(hdr []byte) (uint8, error) {\n\tif len(hdr) < 2 || hdr[0] != magic {\n\t\treturn 0, errCorruptState\n\t}\n\tversion := hdr[1]\n\tswitch version {\n\tcase 1, 2:\n\t\treturn version, nil\n\t}\n\treturn 0, fmt.Errorf(\"unsupported version: %d\", version)\n}\n\nfunc (o *consumerFileStore) copyPending() map[uint64]*Pending {\n\tpending := make(map[uint64]*Pending, len(o.state.Pending))\n\tfor seq, p := range o.state.Pending {\n\t\tpending[seq] = &Pending{p.Sequence, p.Timestamp}\n\t}\n\treturn pending\n}\n\nfunc (o *consumerFileStore) copyRedelivered() map[uint64]uint64 {\n\tredelivered := make(map[uint64]uint64, len(o.state.Redelivered))\n\tfor seq, dc := range o.state.Redelivered {\n\t\tredelivered[seq] = dc\n\t}\n\treturn redelivered\n}\n\n// Type returns the type of the underlying store.\nfunc (o *consumerFileStore) Type() StorageType { return FileStorage }\n\n// State retrieves the state from the state file.\n// This is not expected to be called in high performance code, only on startup.\nfunc (o *consumerFileStore) State() (*ConsumerState, error) {\n\treturn o.stateWithCopy(true)\n}\n\n// This will not copy pending or redelivered, so should only be done under the\n// consumer owner's lock.\nfunc (o *consumerFileStore) BorrowState() (*ConsumerState, error) {\n\treturn o.stateWithCopy(false)\n}\n\nfunc (o *consumerFileStore) stateWithCopy(doCopy bool) (*ConsumerState, error) {\n\to.mu.Lock()\n\tdefer o.mu.Unlock()\n\treturn o.stateWithCopyLocked(doCopy)\n}\n\n// Lock should be held.\nfunc (o *consumerFileStore) stateWithCopyLocked(doCopy bool) (*ConsumerState, error) {\n\tif o.closed {\n\t\treturn nil, ErrStoreClosed\n\t}\n\n\tstate := &ConsumerState{}\n\n\t// See if we have a running state or if we need to read in from disk.\n\tif o.state.Delivered.Consumer != 0 || o.state.Delivered.Stream != 0 {\n\t\tstate.Delivered = o.state.Delivered\n\t\tstate.AckFloor = o.state.AckFloor\n\t\tif len(o.state.Pending) > 0 {\n\t\t\tif doCopy {\n\t\t\t\tstate.Pending = o.copyPending()\n\t\t\t} else {\n\t\t\t\tstate.Pending = o.state.Pending\n\t\t\t}\n\t\t}\n\t\tif len(o.state.Redelivered) > 0 {\n\t\t\tif doCopy {\n\t\t\t\tstate.Redelivered = o.copyRedelivered()\n\t\t\t} else {\n\t\t\t\tstate.Redelivered = o.state.Redelivered\n\t\t\t}\n\t\t}\n\t\treturn state, nil\n\t}\n\n\t// Read the state in here from disk..\n\t<-dios\n\tbuf, err := os.ReadFile(o.ifn)\n\tdios <- struct{}{}\n\n\tif err != nil && !os.IsNotExist(err) {\n\t\treturn nil, err\n\t}\n\n\tif len(buf) == 0 {\n\t\treturn state, nil\n\t}\n\n\t// Check on encryption.\n\tif o.aek != nil {\n\t\tns := o.aek.NonceSize()\n\t\tbuf, err = o.aek.Open(nil, buf[:ns], buf[ns:], nil)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\tstate, err = decodeConsumerState(buf)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Copy this state into our own.\n\to.state.Delivered = state.Delivered\n\to.state.AckFloor = state.AckFloor\n\tif len(state.Pending) > 0 {\n\t\tif doCopy {\n\t\t\to.state.Pending = make(map[uint64]*Pending, len(state.Pending))\n\t\t\tfor seq, p := range state.Pending {\n\t\t\t\to.state.Pending[seq] = &Pending{p.Sequence, p.Timestamp}\n\t\t\t}\n\t\t} else {\n\t\t\to.state.Pending = state.Pending\n\t\t}\n\t}\n\tif len(state.Redelivered) > 0 {\n\t\tif doCopy {\n\t\t\to.state.Redelivered = make(map[uint64]uint64, len(state.Redelivered))\n\t\t\tfor seq, dc := range state.Redelivered {\n\t\t\t\to.state.Redelivered[seq] = dc\n\t\t\t}\n\t\t} else {\n\t\t\to.state.Redelivered = state.Redelivered\n\t\t}\n\t}\n\n\treturn state, nil\n}\n\n// Lock should be held. Called at startup.\nfunc (o *consumerFileStore) loadState() {\n\tif _, err := os.Stat(o.ifn); err == nil {\n\t\t// This will load our state in from disk.\n\t\to.stateWithCopyLocked(false)\n\t}\n}\n\n// Decode consumer state.\nfunc decodeConsumerState(buf []byte) (*ConsumerState, error) {\n\tversion, err := checkConsumerHeader(buf)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tbi := hdrLen\n\t// Helpers, will set i to -1 on error.\n\treadSeq := func() uint64 {\n\t\tif bi < 0 {\n\t\t\treturn 0\n\t\t}\n\t\tseq, n := binary.Uvarint(buf[bi:])\n\t\tif n <= 0 {\n\t\t\tbi = -1\n\t\t\treturn 0\n\t\t}\n\t\tbi += n\n\t\treturn seq\n\t}\n\treadTimeStamp := func() int64 {\n\t\tif bi < 0 {\n\t\t\treturn 0\n\t\t}\n\t\tts, n := binary.Varint(buf[bi:])\n\t\tif n <= 0 {\n\t\t\tbi = -1\n\t\t\treturn -1\n\t\t}\n\t\tbi += n\n\t\treturn ts\n\t}\n\t// Just for clarity below.\n\treadLen := readSeq\n\treadCount := readSeq\n\n\tstate := &ConsumerState{}\n\tstate.AckFloor.Consumer = readSeq()\n\tstate.AckFloor.Stream = readSeq()\n\tstate.Delivered.Consumer = readSeq()\n\tstate.Delivered.Stream = readSeq()\n\n\tif bi == -1 {\n\t\treturn nil, errCorruptState\n\t}\n\tif version == 1 {\n\t\t// Adjust back. Version 1 also stored delivered as next to be delivered,\n\t\t// so adjust that back down here.\n\t\tif state.AckFloor.Consumer > 1 {\n\t\t\tstate.Delivered.Consumer += state.AckFloor.Consumer - 1\n\t\t}\n\t\tif state.AckFloor.Stream > 1 {\n\t\t\tstate.Delivered.Stream += state.AckFloor.Stream - 1\n\t\t}\n\t}\n\n\t// Protect ourselves against rolling backwards.\n\tconst hbit = 1 << 63\n\tif state.AckFloor.Stream&hbit != 0 || state.Delivered.Stream&hbit != 0 {\n\t\treturn nil, errCorruptState\n\t}\n\n\t// We have additional stuff.\n\tif numPending := readLen(); numPending > 0 {\n\t\tmints := readTimeStamp()\n\t\tstate.Pending = make(map[uint64]*Pending, numPending)\n\t\tfor i := 0; i < int(numPending); i++ {\n\t\t\tsseq := readSeq()\n\t\t\tvar dseq uint64\n\t\t\tif version == 2 {\n\t\t\t\tdseq = readSeq()\n\t\t\t}\n\t\t\tts := readTimeStamp()\n\t\t\t// Check the state machine for corruption, not the value which could be -1.\n\t\t\tif bi == -1 {\n\t\t\t\treturn nil, errCorruptState\n\t\t\t}\n\t\t\t// Adjust seq back.\n\t\t\tsseq += state.AckFloor.Stream\n\t\t\tif sseq == 0 {\n\t\t\t\treturn nil, errCorruptState\n\t\t\t}\n\t\t\tif version == 2 {\n\t\t\t\tdseq += state.AckFloor.Consumer\n\t\t\t}\n\t\t\t// Adjust the timestamp back.\n\t\t\tif version == 1 {\n\t\t\t\tts = (ts + mints) * int64(time.Second)\n\t\t\t} else {\n\t\t\t\tts = (mints - ts) * int64(time.Second)\n\t\t\t}\n\t\t\t// Store in pending.\n\t\t\tstate.Pending[sseq] = &Pending{dseq, ts}\n\t\t}\n\t}\n\n\t// We have redelivered entries here.\n\tif numRedelivered := readLen(); numRedelivered > 0 {\n\t\tstate.Redelivered = make(map[uint64]uint64, numRedelivered)\n\t\tfor i := 0; i < int(numRedelivered); i++ {\n\t\t\tif seq, n := readSeq(), readCount(); seq > 0 && n > 0 {\n\t\t\t\t// Adjust seq back.\n\t\t\t\tseq += state.AckFloor.Stream\n\t\t\t\tstate.Redelivered[seq] = n\n\t\t\t}\n\t\t}\n\t}\n\n\treturn state, nil\n}\n\n// Stop the processing of the consumers's state.\nfunc (o *consumerFileStore) Stop() error {\n\to.mu.Lock()\n\tif o.closed {\n\t\to.mu.Unlock()\n\t\treturn nil\n\t}\n\tif o.qch != nil {\n\t\tclose(o.qch)\n\t\to.qch = nil\n\t}\n\n\tvar err error\n\tvar buf []byte\n\n\tif o.dirty {\n\t\t// Make sure to write this out..\n\t\tif buf, err = o.encodeState(); err == nil && len(buf) > 0 {\n\t\t\tif o.aek != nil {\n\t\t\t\tif buf, err = o.encryptState(buf); err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\to.odir = _EMPTY_\n\to.closed = true\n\tifn, fs := o.ifn, o.fs\n\to.mu.Unlock()\n\n\tfs.RemoveConsumer(o)\n\n\tif len(buf) > 0 {\n\t\to.waitOnFlusher()\n\t\terr = o.fs.writeFileWithOptionalSync(ifn, buf, defaultFilePerms)\n\t}\n\treturn err\n}\n\nfunc (o *consumerFileStore) waitOnFlusher() {\n\tif !o.inFlusher() {\n\t\treturn\n\t}\n\n\ttimeout := time.Now().Add(100 * time.Millisecond)\n\tfor time.Now().Before(timeout) {\n\t\tif !o.inFlusher() {\n\t\t\treturn\n\t\t}\n\t\ttime.Sleep(10 * time.Millisecond)\n\t}\n}\n\n// Delete the consumer.\nfunc (o *consumerFileStore) Delete() error {\n\treturn o.delete(false)\n}\n\nfunc (o *consumerFileStore) StreamDelete() error {\n\treturn o.delete(true)\n}\n\nfunc (o *consumerFileStore) delete(streamDeleted bool) error {\n\to.mu.Lock()\n\tif o.closed {\n\t\to.mu.Unlock()\n\t\treturn nil\n\t}\n\tif o.qch != nil {\n\t\tclose(o.qch)\n\t\to.qch = nil\n\t}\n\n\tvar err error\n\todir := o.odir\n\to.odir = _EMPTY_\n\to.closed = true\n\tfs := o.fs\n\to.mu.Unlock()\n\n\t// If our stream was not deleted this will remove the directories.\n\tif odir != _EMPTY_ && !streamDeleted {\n\t\t<-dios\n\t\terr = os.RemoveAll(odir)\n\t\tdios <- struct{}{}\n\t}\n\n\tif !streamDeleted {\n\t\tfs.RemoveConsumer(o)\n\t}\n\n\treturn err\n}\n\nfunc (fs *fileStore) AddConsumer(o ConsumerStore) error {\n\tfs.cmu.Lock()\n\tdefer fs.cmu.Unlock()\n\tfs.cfs = append(fs.cfs, o)\n\treturn nil\n}\n\nfunc (fs *fileStore) RemoveConsumer(o ConsumerStore) error {\n\tfs.cmu.Lock()\n\tdefer fs.cmu.Unlock()\n\tfor i, cfs := range fs.cfs {\n\t\tif o == cfs {\n\t\t\tfs.cfs = append(fs.cfs[:i], fs.cfs[i+1:]...)\n\t\t\tbreak\n\t\t}\n\t}\n\treturn nil\n}\n\n////////////////////////////////////////////////////////////////////////////////\n// Templates\n////////////////////////////////////////////////////////////////////////////////\n\ntype templateFileStore struct {\n\tdir string\n\thh  hash.Hash64\n}\n\nfunc newTemplateFileStore(storeDir string) *templateFileStore {\n\ttdir := filepath.Join(storeDir, tmplsDir)\n\tkey := sha256.Sum256([]byte(\"templates\"))\n\thh, err := highwayhash.New64(key[:])\n\tif err != nil {\n\t\treturn nil\n\t}\n\treturn &templateFileStore{dir: tdir, hh: hh}\n}\n\nfunc (ts *templateFileStore) Store(t *streamTemplate) error {\n\tdir := filepath.Join(ts.dir, t.Name)\n\tif err := os.MkdirAll(dir, defaultDirPerms); err != nil {\n\t\treturn fmt.Errorf(\"could not create templates storage directory for %q- %v\", t.Name, err)\n\t}\n\tmeta := filepath.Join(dir, JetStreamMetaFile)\n\tif _, err := os.Stat(meta); (err != nil && !os.IsNotExist(err)) || err == nil {\n\t\treturn err\n\t}\n\tt.mu.Lock()\n\tb, err := json.Marshal(t)\n\tt.mu.Unlock()\n\tif err != nil {\n\t\treturn err\n\t}\n\tif err := os.WriteFile(meta, b, defaultFilePerms); err != nil {\n\t\treturn err\n\t}\n\t// FIXME(dlc) - Do checksum\n\tts.hh.Reset()\n\tts.hh.Write(b)\n\tchecksum := hex.EncodeToString(ts.hh.Sum(nil))\n\tsum := filepath.Join(dir, JetStreamMetaFileSum)\n\tif err := os.WriteFile(sum, []byte(checksum), defaultFilePerms); err != nil {\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc (ts *templateFileStore) Delete(t *streamTemplate) error {\n\treturn os.RemoveAll(filepath.Join(ts.dir, t.Name))\n}\n\n////////////////////////////////////////////////////////////////////////////////\n// Compression\n////////////////////////////////////////////////////////////////////////////////\n\ntype CompressionInfo struct {\n\tAlgorithm    StoreCompression\n\tOriginalSize uint64\n}\n\nfunc (c *CompressionInfo) MarshalMetadata() []byte {\n\tb := make([]byte, 14) // 4 + potentially up to 10 for uint64\n\tb[0], b[1], b[2] = 'c', 'm', 'p'\n\tb[3] = byte(c.Algorithm)\n\tn := binary.PutUvarint(b[4:], c.OriginalSize)\n\treturn b[:4+n]\n}\n\nfunc (c *CompressionInfo) UnmarshalMetadata(b []byte) (int, error) {\n\tc.Algorithm = NoCompression\n\tc.OriginalSize = 0\n\tif len(b) < 5 { // 4 + min 1 for uvarint uint64\n\t\treturn 0, nil\n\t}\n\tif b[0] != 'c' || b[1] != 'm' || b[2] != 'p' {\n\t\treturn 0, nil\n\t}\n\tvar n int\n\tc.Algorithm = StoreCompression(b[3])\n\tc.OriginalSize, n = binary.Uvarint(b[4:])\n\tif n <= 0 {\n\t\treturn 0, fmt.Errorf(\"metadata incomplete\")\n\t}\n\treturn 4 + n, nil\n}\n\nfunc (alg StoreCompression) Compress(buf []byte) ([]byte, error) {\n\tif len(buf) < checksumSize {\n\t\treturn nil, fmt.Errorf(\"uncompressed buffer is too short\")\n\t}\n\tbodyLen := int64(len(buf) - checksumSize)\n\tvar output bytes.Buffer\n\tvar writer io.WriteCloser\n\tswitch alg {\n\tcase NoCompression:\n\t\treturn buf, nil\n\tcase S2Compression:\n\t\twriter = s2.NewWriter(&output)\n\tdefault:\n\t\treturn nil, fmt.Errorf(\"compression algorithm not known\")\n\t}\n\n\tinput := bytes.NewReader(buf[:bodyLen])\n\tchecksum := buf[bodyLen:]\n\n\t// Compress the block content, but don't compress the checksum.\n\t// We will preserve it at the end of the block as-is.\n\tif n, err := io.CopyN(writer, input, bodyLen); err != nil {\n\t\treturn nil, fmt.Errorf(\"error writing to compression writer: %w\", err)\n\t} else if n != bodyLen {\n\t\treturn nil, fmt.Errorf(\"short write on body (%d != %d)\", n, bodyLen)\n\t}\n\tif err := writer.Close(); err != nil {\n\t\treturn nil, fmt.Errorf(\"error closing compression writer: %w\", err)\n\t}\n\n\t// Now add the checksum back onto the end of the block.\n\tif n, err := output.Write(checksum); err != nil {\n\t\treturn nil, fmt.Errorf(\"error writing checksum: %w\", err)\n\t} else if n != checksumSize {\n\t\treturn nil, fmt.Errorf(\"short write on checksum (%d != %d)\", n, checksumSize)\n\t}\n\n\treturn output.Bytes(), nil\n}\n\nfunc (alg StoreCompression) Decompress(buf []byte) ([]byte, error) {\n\tif len(buf) < checksumSize {\n\t\treturn nil, fmt.Errorf(\"compressed buffer is too short\")\n\t}\n\tbodyLen := int64(len(buf) - checksumSize)\n\tinput := bytes.NewReader(buf[:bodyLen])\n\n\tvar reader io.ReadCloser\n\tswitch alg {\n\tcase NoCompression:\n\t\treturn buf, nil\n\tcase S2Compression:\n\t\treader = io.NopCloser(s2.NewReader(input))\n\tdefault:\n\t\treturn nil, fmt.Errorf(\"compression algorithm not known\")\n\t}\n\n\t// Decompress the block content. The checksum isn't compressed so\n\t// we can preserve it from the end of the block as-is.\n\tchecksum := buf[bodyLen:]\n\toutput, err := io.ReadAll(reader)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"error reading compression reader: %w\", err)\n\t}\n\toutput = append(output, checksum...)\n\n\treturn output, reader.Close()\n}\n\n// writeFileWithOptionalSync is equivalent to os.WriteFile() but optionally\n// sets O_SYNC on the open file if SyncAlways is set. The dios semaphore is\n// handled automatically by this function, so don't wrap calls to it in dios.\nfunc (fs *fileStore) writeFileWithOptionalSync(name string, data []byte, perm fs.FileMode) error {\n\tif fs.fcfg.SyncAlways {\n\t\treturn writeFileWithSync(name, data, perm)\n\t}\n\t<-dios\n\tdefer func() {\n\t\tdios <- struct{}{}\n\t}()\n\treturn os.WriteFile(name, data, perm)\n}\n\nfunc writeFileWithSync(name string, data []byte, perm fs.FileMode) error {\n\t<-dios\n\tdefer func() {\n\t\tdios <- struct{}{}\n\t}()\n\tflags := os.O_WRONLY | os.O_CREATE | os.O_TRUNC | os.O_SYNC\n\tf, err := os.OpenFile(name, flags, perm)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif _, err = f.Write(data); err != nil {\n\t\t_ = f.Close()\n\t\treturn err\n\t}\n\treturn f.Close()\n}\n",
    "source_file": "server/filestore.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2012-2025 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage server\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n)\n\nvar (\n\t// ErrConnectionClosed represents an error condition on a closed connection.\n\tErrConnectionClosed = errors.New(\"connection closed\")\n\n\t// ErrAuthentication represents an error condition on failed authentication.\n\tErrAuthentication = errors.New(\"authentication error\")\n\n\t// ErrAuthTimeout represents an error condition on failed authorization due to timeout.\n\tErrAuthTimeout = errors.New(\"authentication timeout\")\n\n\t// ErrAuthExpired represents an expired authorization due to timeout.\n\tErrAuthExpired = errors.New(\"authentication expired\")\n\n\t// ErrMaxPayload represents an error condition when the payload is too big.\n\tErrMaxPayload = errors.New(\"maximum payload exceeded\")\n\n\t// ErrMaxControlLine represents an error condition when the control line is too big.\n\tErrMaxControlLine = errors.New(\"maximum control line exceeded\")\n\n\t// ErrReservedPublishSubject represents an error condition when sending to a reserved subject, e.g. _SYS.>\n\tErrReservedPublishSubject = errors.New(\"reserved internal subject\")\n\n\t// ErrBadPublishSubject represents an error condition for an invalid publish subject.\n\tErrBadPublishSubject = errors.New(\"invalid publish subject\")\n\n\t// ErrBadSubject represents an error condition for an invalid subject.\n\tErrBadSubject = errors.New(\"invalid subject\")\n\n\t// ErrBadQualifier is used to error on a bad qualifier for a transform.\n\tErrBadQualifier = errors.New(\"bad qualifier\")\n\n\t// ErrBadClientProtocol signals a client requested an invalid client protocol.\n\tErrBadClientProtocol = errors.New(\"invalid client protocol\")\n\n\t// ErrTooManyConnections signals a client that the maximum number of connections supported by the\n\t// server has been reached.\n\tErrTooManyConnections = errors.New(\"maximum connections exceeded\")\n\n\t// ErrTooManyAccountConnections signals that an account has reached its maximum number of active\n\t// connections.\n\tErrTooManyAccountConnections = errors.New(\"maximum account active connections exceeded\")\n\n\t// ErrLeafNodeLoop signals a leafnode is trying to register for a cluster we already have registered.\n\tErrLeafNodeLoop = errors.New(\"leafnode loop detected\")\n\n\t// ErrTooManySubs signals a client that the maximum number of subscriptions per connection\n\t// has been reached.\n\tErrTooManySubs = errors.New(\"maximum subscriptions exceeded\")\n\n\t// ErrTooManySubTokens signals a client that the subject has too many tokens.\n\tErrTooManySubTokens = errors.New(\"subject has exceeded number of tokens limit\")\n\n\t// ErrClientConnectedToRoutePort represents an error condition when a client\n\t// attempted to connect to the route listen port.\n\tErrClientConnectedToRoutePort = errors.New(\"attempted to connect to route port\")\n\n\t// ErrClientConnectedToLeafNodePort represents an error condition when a client\n\t// attempted to connect to the leaf node listen port.\n\tErrClientConnectedToLeafNodePort = errors.New(\"attempted to connect to leaf node port\")\n\n\t// ErrLeafNodeHasSameClusterName represents an error condition when a leafnode is a cluster\n\t// and it has the same cluster name as the hub cluster.\n\tErrLeafNodeHasSameClusterName = errors.New(\"remote leafnode has same cluster name\")\n\n\t// ErrLeafNodeDisabled is when we disable leafnodes.\n\tErrLeafNodeDisabled = errors.New(\"leafnodes disabled\")\n\n\t// ErrConnectedToWrongPort represents an error condition when a connection is attempted\n\t// to the wrong listen port (for instance a LeafNode to a client port, etc...)\n\tErrConnectedToWrongPort = errors.New(\"attempted to connect to wrong port\")\n\n\t// ErrAccountExists is returned when an account is attempted to be registered\n\t// but already exists.\n\tErrAccountExists = errors.New(\"account exists\")\n\n\t// ErrBadAccount represents a malformed or incorrect account.\n\tErrBadAccount = errors.New(\"bad account\")\n\n\t// ErrReservedAccount represents a reserved account that can not be created.\n\tErrReservedAccount = errors.New(\"reserved account\")\n\n\t// ErrMissingAccount is returned when an account does not exist.\n\tErrMissingAccount = errors.New(\"account missing\")\n\n\t// ErrMissingService is returned when an account does not have an exported service.\n\tErrMissingService = errors.New(\"service missing\")\n\n\t// ErrBadServiceType is returned when latency tracking is being applied to non-singleton response types.\n\tErrBadServiceType = errors.New(\"bad service response type\")\n\n\t// ErrBadSampling is returned when the sampling for latency tracking is not 1 >= sample <= 100.\n\tErrBadSampling = errors.New(\"bad sampling percentage, should be 1-100\")\n\n\t// ErrAccountValidation is returned when an account has failed validation.\n\tErrAccountValidation = errors.New(\"account validation failed\")\n\n\t// ErrAccountExpired is returned when an account has expired.\n\tErrAccountExpired = errors.New(\"account expired\")\n\n\t// ErrNoAccountResolver is returned when we attempt an update but do not have an account resolver.\n\tErrNoAccountResolver = errors.New(\"account resolver missing\")\n\n\t// ErrAccountResolverUpdateTooSoon is returned when we attempt an update too soon to last request.\n\tErrAccountResolverUpdateTooSoon = errors.New(\"account resolver update too soon\")\n\n\t// ErrAccountResolverSameClaims is returned when same claims have been fetched.\n\tErrAccountResolverSameClaims = errors.New(\"account resolver no new claims\")\n\n\t// ErrStreamImportAuthorization is returned when a stream import is not authorized.\n\tErrStreamImportAuthorization = errors.New(\"stream import not authorized\")\n\n\t// ErrStreamImportBadPrefix is returned when a stream import prefix contains wildcards.\n\tErrStreamImportBadPrefix = errors.New(\"stream import prefix can not contain wildcard tokens\")\n\n\t// ErrStreamImportDuplicate is returned when a stream import is a duplicate of one that already exists.\n\tErrStreamImportDuplicate = errors.New(\"stream import already exists\")\n\n\t// ErrServiceImportAuthorization is returned when a service import is not authorized.\n\tErrServiceImportAuthorization = errors.New(\"service import not authorized\")\n\n\t// ErrImportFormsCycle is returned when an import would form a cycle.\n\tErrImportFormsCycle = errors.New(\"import forms a cycle\")\n\n\t// ErrCycleSearchDepth is returned when we have exceeded our maximum search depth..\n\tErrCycleSearchDepth = errors.New(\"search cycle depth exhausted\")\n\n\t// ErrClientOrRouteConnectedToGatewayPort represents an error condition when\n\t// a client or route attempted to connect to the Gateway port.\n\tErrClientOrRouteConnectedToGatewayPort = errors.New(\"attempted to connect to gateway port\")\n\n\t// ErrWrongGateway represents an error condition when a server receives a connect\n\t// request from a remote Gateway with a destination name that does not match the server's\n\t// Gateway's name.\n\tErrWrongGateway = errors.New(\"wrong gateway\")\n\n\t// ErrGatewayNameHasSpaces signals that the gateway name contains spaces, which is not allowed.\n\tErrGatewayNameHasSpaces = errors.New(\"gateway name cannot contain spaces\")\n\n\t// ErrNoSysAccount is returned when an attempt to publish or subscribe is made\n\t// when there is no internal system account defined.\n\tErrNoSysAccount = errors.New(\"system account not setup\")\n\n\t// ErrRevocation is returned when a credential has been revoked.\n\tErrRevocation = errors.New(\"credentials have been revoked\")\n\n\t// ErrServerNotRunning is used to signal an error that a server is not running.\n\tErrServerNotRunning = errors.New(\"server is not running\")\n\n\t// ErrServerNameHasSpaces signals that the server name contains spaces, which is not allowed.\n\tErrServerNameHasSpaces = errors.New(\"server name cannot contain spaces\")\n\n\t// ErrBadMsgHeader signals the parser detected a bad message header\n\tErrBadMsgHeader = errors.New(\"bad message header detected\")\n\n\t// ErrMsgHeadersNotSupported signals the parser detected a message header\n\t// but they are not supported on this server.\n\tErrMsgHeadersNotSupported = errors.New(\"message headers not supported\")\n\n\t// ErrNoRespondersRequiresHeaders signals that a client needs to have headers\n\t// on if they want no responders behavior.\n\tErrNoRespondersRequiresHeaders = errors.New(\"no responders requires headers support\")\n\n\t// ErrClusterNameConfigConflict signals that the options for cluster name in cluster and gateway are in conflict.\n\tErrClusterNameConfigConflict = errors.New(\"cluster name conflicts between cluster and gateway definitions\")\n\n\t// ErrClusterNameRemoteConflict signals that a remote server has a different cluster name.\n\tErrClusterNameRemoteConflict = errors.New(\"cluster name from remote server conflicts\")\n\n\t// ErrClusterNameHasSpaces signals that the cluster name contains spaces, which is not allowed.\n\tErrClusterNameHasSpaces = errors.New(\"cluster name cannot contain spaces\")\n\n\t// ErrMalformedSubject is returned when a subscription is made with a subject that does not conform to subject rules.\n\tErrMalformedSubject = errors.New(\"malformed subject\")\n\n\t// ErrSubscribePermissionViolation is returned when processing of a subscription fails due to permissions.\n\tErrSubscribePermissionViolation = errors.New(\"subscribe permission violation\")\n\n\t// ErrNoTransforms signals no subject transforms are available to map this subject.\n\tErrNoTransforms = errors.New(\"no matching transforms available\")\n\n\t// ErrCertNotPinned is returned when pinned certs are set and the certificate is not in it\n\tErrCertNotPinned = errors.New(\"certificate not pinned\")\n\n\t// ErrDuplicateServerName is returned when processing a server remote connection and\n\t// the server reports that this server name is already used in the cluster.\n\tErrDuplicateServerName = errors.New(\"duplicate server name\")\n\n\t// ErrMinimumVersionRequired is returned when a connection is not at the minimum version required.\n\tErrMinimumVersionRequired = errors.New(\"minimum version required\")\n\n\t// ErrInvalidMappingDestination is used for all subject mapping destination errors\n\tErrInvalidMappingDestination = errors.New(\"invalid mapping destination\")\n\n\t// ErrInvalidMappingDestinationSubject is used to error on a bad transform destination mapping\n\tErrInvalidMappingDestinationSubject = fmt.Errorf(\"%w: invalid transform\", ErrInvalidMappingDestination)\n\n\t// ErrMappingDestinationNotUsingAllWildcards is used to error on a transform destination not using all of the token wildcards\n\tErrMappingDestinationNotUsingAllWildcards = fmt.Errorf(\"%w: not using all of the token wildcard(s)\", ErrInvalidMappingDestination)\n\n\t// ErrUnknownMappingDestinationFunction is returned when a subject mapping destination contains an unknown mustache-escaped mapping function.\n\tErrUnknownMappingDestinationFunction = fmt.Errorf(\"%w: unknown function\", ErrInvalidMappingDestination)\n\n\t// ErrMappingDestinationIndexOutOfRange is returned when the mapping destination function is passed an out of range wildcard index value for one of it's arguments\n\tErrMappingDestinationIndexOutOfRange = fmt.Errorf(\"%w: wildcard index out of range\", ErrInvalidMappingDestination)\n\n\t// ErrMappingDestinationNotEnoughArgs is returned when the mapping destination function is not passed enough arguments\n\tErrMappingDestinationNotEnoughArgs = fmt.Errorf(\"%w: not enough arguments passed to the function\", ErrInvalidMappingDestination)\n\n\t// ErrMappingDestinationInvalidArg is returned when the mapping destination function is passed and invalid argument\n\tErrMappingDestinationInvalidArg = fmt.Errorf(\"%w: function argument is invalid or in the wrong format\", ErrInvalidMappingDestination)\n\n\t// ErrMappingDestinationTooManyArgs is returned when the mapping destination function is passed too many arguments\n\tErrMappingDestinationTooManyArgs = fmt.Errorf(\"%w: too many arguments passed to the function\", ErrInvalidMappingDestination)\n\n\t// ErrMappingDestinationNotSupportedForImport is returned when you try to use a mapping function other than wildcard in a transform that needs to be reversible (i.e. an import)\n\tErrMappingDestinationNotSupportedForImport = fmt.Errorf(\"%w: the only mapping function allowed for import transforms is {{Wildcard()}}\", ErrInvalidMappingDestination)\n)\n\n// mappingDestinationErr is a type of subject mapping destination error\ntype mappingDestinationErr struct {\n\ttoken string\n\terr   error\n}\n\nfunc (e *mappingDestinationErr) Error() string {\n\treturn fmt.Sprintf(\"%s in %s\", e.err, e.token)\n}\n\nfunc (e *mappingDestinationErr) Is(target error) bool {\n\treturn target == ErrInvalidMappingDestination\n}\n\n// configErr is a configuration error.\ntype configErr struct {\n\ttoken  token\n\treason string\n}\n\n// Source reports the location of a configuration error.\nfunc (e *configErr) Source() string {\n\treturn fmt.Sprintf(\"%s:%d:%d\", e.token.SourceFile(), e.token.Line(), e.token.Position())\n}\n\n// Error reports the location and reason from a configuration error.\nfunc (e *configErr) Error() string {\n\tif e.token != nil {\n\t\treturn fmt.Sprintf(\"%s: %s\", e.Source(), e.reason)\n\t}\n\treturn e.reason\n}\n\n// unknownConfigFieldErr is an error reported in pedantic mode.\ntype unknownConfigFieldErr struct {\n\tconfigErr\n\tfield string\n}\n\n// Error reports that an unknown field was in the configuration.\nfunc (e *unknownConfigFieldErr) Error() string {\n\treturn fmt.Sprintf(\"%s: unknown field %q\", e.Source(), e.field)\n}\n\n// configWarningErr is an error reported in pedantic mode.\ntype configWarningErr struct {\n\tconfigErr\n\tfield string\n}\n\n// Error reports a configuration warning.\nfunc (e *configWarningErr) Error() string {\n\treturn fmt.Sprintf(\"%s: invalid use of field %q: %s\", e.Source(), e.field, e.reason)\n}\n\n// processConfigErr is the result of processing the configuration from the server.\ntype processConfigErr struct {\n\terrors   []error\n\twarnings []error\n}\n\n// Error returns the collection of errors separated by new lines,\n// warnings appear first then hard errors.\nfunc (e *processConfigErr) Error() string {\n\tvar msg string\n\tfor _, err := range e.Warnings() {\n\t\tmsg += err.Error() + \"\\n\"\n\t}\n\tfor _, err := range e.Errors() {\n\t\tmsg += err.Error() + \"\\n\"\n\t}\n\treturn msg\n}\n\n// Warnings returns the list of warnings.\nfunc (e *processConfigErr) Warnings() []error {\n\treturn e.warnings\n}\n\n// Errors returns the list of errors.\nfunc (e *processConfigErr) Errors() []error {\n\treturn e.errors\n}\n\n// errCtx wraps an error and stores additional ctx information for tracing.\n// Does not print or return it unless explicitly requested.\ntype errCtx struct {\n\terror\n\tctx string\n}\n\nfunc NewErrorCtx(err error, format string, args ...any) error {\n\treturn &errCtx{err, fmt.Sprintf(format, args...)}\n}\n\n// Unwrap implement to work with errors.Is and errors.As\nfunc (e *errCtx) Unwrap() error {\n\tif e == nil {\n\t\treturn nil\n\t}\n\treturn e.error\n}\n\n// Context for error\nfunc (e *errCtx) Context() string {\n\tif e == nil {\n\t\treturn \"\"\n\t}\n\treturn e.ctx\n}\n\n// UnpackIfErrorCtx return Error or, if type is right error and context\nfunc UnpackIfErrorCtx(err error) string {\n\tif e, ok := err.(*errCtx); ok {\n\t\tif _, ok := e.error.(*errCtx); ok {\n\t\t\treturn fmt.Sprint(UnpackIfErrorCtx(e.error), \": \", e.Context())\n\t\t}\n\t\treturn fmt.Sprint(e.Error(), \": \", e.Context())\n\t}\n\treturn err.Error()\n}\n\n// implements: go 1.13 errors.Unwrap(err error) error\n// TODO replace with native code once we no longer support go1.12\nfunc errorsUnwrap(err error) error {\n\tu, ok := err.(interface {\n\t\tUnwrap() error\n\t})\n\tif !ok {\n\t\treturn nil\n\t}\n\treturn u.Unwrap()\n}\n\n// ErrorIs implements: go 1.13 errors.Is(err, target error) bool\n// TODO replace with native code once we no longer support go1.12\nfunc ErrorIs(err, target error) bool {\n\t// this is an outright copy of go 1.13 errors.Is(err, target error) bool\n\t// removed isComparable\n\tif err == nil || target == nil {\n\t\treturn err == target\n\t}\n\n\tfor {\n\t\tif err == target {\n\t\t\treturn true\n\t\t}\n\t\tif x, ok := err.(interface{ Is(error) bool }); ok && x.Is(target) {\n\t\t\treturn true\n\t\t}\n\t\t// TODO: consider supporing target.Is(err). This would allow\n\t\t// user-definable predicates, but also may allow for coping with sloppy\n\t\t// APIs, thereby making it easier to get away with them.\n\t\tif err = errorsUnwrap(err); err == nil {\n\t\t\treturn false\n\t\t}\n\t}\n}\n",
    "source_file": "server/errors.go",
    "chunk_type": "code"
  },
  {
    "content": "#!/bin/bash\n\n# Ensure script is run at the root of a git repository\ngit rev-parse --is-inside-work-tree &>/dev/null || { echo \"Not inside a git repository\"; exit 1; }\n\n# Find all .go files tracked by git\ngit ls-files \"*.go\" | while read -r file; do\n    # Skip files that don't have a copyright belonging to \"The NATS Authors\"\n    current_copyright=$(grep -oE \"^// Copyright [0-9]{4}(-[0-9]{4})? The NATS Authors\" \"$file\" || echo \"\")\n    [[ -z \"$current_copyright\" ]] && continue\n\n    # Get the last commit year for the file, ignore commit messages containing the word \"copyright\"\n    last_year=$(git log --follow --format=\"%ad\" --date=format:%Y --grep=\"(C|c)opyright\" --invert-grep -n 1 -- \"$file\")\n    existing_years=$(echo \"$current_copyright\" | grep -oE \"[0-9]{4}(-[0-9]{4})?\")\n\n    # Determine the new copyright range\n    if [[ \"$existing_years\" =~ ^([0-9]{4})-([0-9]{4})$ ]]; then\n        first_year=${BASH_REMATCH[1]}\n        new_copyright=\"// Copyright $first_year-$last_year The NATS Authors\"\n    elif [[ \"$existing_years\" =~ ^([0-9]{4})$ ]]; then\n        first_year=${BASH_REMATCH[1]}\n        if [[ \"$first_year\" == \"$last_year\" ]]; then\n            new_copyright=\"// Copyright $first_year The NATS Authors\"\n        else\n            new_copyright=\"// Copyright $first_year-$last_year The NATS Authors\"\n        fi\n    else\n        continue # If the format is somehow incorrect, skip the file\n    fi\n\n    # Update the first line\n    if sed --version &>/dev/null; then\n        # Linux sed\n        sed -i \"1s|^// Copyright.*|$new_copyright|\" \"$file\"\n    else\n        # BSD/macOS sed, needs -i ''\n        sed -i '' \"1s|^// Copyright.*|$new_copyright|\" \"$file\"\n    fi\ndone\n",
    "source_file": "scripts/updateCopyrights.sh",
    "chunk_type": "unknown"
  },
  {
    "content": "#!/bin/bash\n# Run from directory above via ./scripts/cov.sh\n\ncheck_file () {\n    # If the content of the file is simply \"mode: atomic\", then it means that the\n    # code coverage did not complete due to a panic in one of the tests.\n    if [[ $(cat ./cov/$2) == \"mode: atomic\" ]]; then\n        echo \"#############################################\"\n        echo \"## Code coverage for $1 package failed ##\"\n        echo \"#############################################\"\n        exit 1\n    fi\n}\n\n# Do not globally set the -e flag because we don't a flapper to prevent the push to coverall.\n\nexport GO111MODULE=\"on\"\n\ngo install github.com/wadey/gocovmerge@latest\n# Fail fast by checking if we can run gocovmerge\ngocovmerge\nif [[ $? != 0 ]]; then\n    echo \"Unable to run gocovmerge\"\n    exit 1\nfi\n\nrm -rf ./cov\nmkdir cov\n#\n# Since it is difficult to get a full run without a flapper, do not use `-failfast`.\n# It is better to have one flapper or two and still get the report than have\n# to re-run the whole code coverage. One or two failed tests should not affect\n# so much the code coverage.\n#\n# However, we need to take into account that if there is a panic in one test, all\n# other tests in that package will not run, which then would cause the code coverage\n# to drastically be lowered. In that case, we don't want the code coverage to be\n# uploaded.\n#\ngo test -v -covermode=atomic -coverprofile=./cov/conf.out ./conf -timeout=1h -tags=skip_no_race_tests\ncheck_file \"conf\" \"conf.out\"\ngo test -v -covermode=atomic -coverprofile=./cov/internal.out ./internal/ldap -timeout=1h -tags=skip_no_race_tests\ncheck_file \"internal\" \"internal.out\"\ngo test -v -covermode=atomic -coverprofile=./cov/log.out ./logger -timeout=1h -tags=skip_no_race_tests\ncheck_file \"logger\" \"log.out\"\ngo test -v -covermode=atomic -coverprofile=./cov/server_avl.out ./server/avl -timeout=1h -tags=skip_no_race_tests\ncheck_file \"server_avl\" \"server_avl.out\"\ngo test -v -covermode=atomic -coverprofile=./cov/server.out ./server -timeout=1h -tags=skip_no_race_tests\ncheck_file \"server\" \"server.out\"\ngo test -v -covermode=atomic -coverprofile=./cov/test.out -coverpkg=./server ./test -timeout=1h -tags=skip_no_race_tests\ncheck_file \"test\" \"test.out\"\n\n# At this point, if that fails, we want the caller to know about the failure.\nset -e\ngocovmerge ./cov/*.out > acc.out\nrm -rf ./cov\n\n# If no argument passed, launch a browser to see the results.\nif [[ $1 == \"\" ]]; then\n    go tool cover -html=acc.out\nfi\nset +e\n",
    "source_file": "scripts/cov.sh",
    "chunk_type": "unknown"
  },
  {
    "content": "# Architecture Decision Records\n\nThe NATS ADR documents have moved to their [own repository](https://github.com/nats-io/nats-architecture-and-design/)",
    "source_file": "doc/README.md",
    "chunk_type": "doc"
  },
  {
    "content": "// Copyright 2013-2024 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n// Customized heavily from\n// https://github.com/BurntSushi/toml/blob/master/lex.go, which is based on\n// Rob Pike's talk: http://cuddle.googlecode.com/hg/talk/lex.html\n\n// The format supported is less restrictive than today's formats.\n// Supports mixed Arrays [], nested Maps {}, multiple comment types (# and //)\n// Also supports key value assignments using '=' or ':' or whiteSpace()\n//   e.g. foo = 2, foo : 2, foo 2\n// maps can be assigned with no key separator as well\n// semicolons as value terminators in key/value assignments are optional\n//\n// see lex_test.go for more examples.\n\npackage conf\n\nimport (\n\t\"encoding/hex\"\n\t\"fmt\"\n\t\"strings\"\n\t\"unicode\"\n\t\"unicode/utf8\"\n)\n\ntype itemType int\n\nconst (\n\titemError itemType = iota\n\titemNIL            // used in the parser to indicate no type\n\titemEOF\n\titemKey\n\titemText\n\titemString\n\titemBool\n\titemInteger\n\titemFloat\n\titemDatetime\n\titemArrayStart\n\titemArrayEnd\n\titemMapStart\n\titemMapEnd\n\titemCommentStart\n\titemVariable\n\titemInclude\n)\n\nconst (\n\teof               = 0\n\tmapStart          = '{'\n\tmapEnd            = '}'\n\tkeySepEqual       = '='\n\tkeySepColon       = ':'\n\tarrayStart        = '['\n\tarrayEnd          = ']'\n\tarrayValTerm      = ','\n\tmapValTerm        = ','\n\tcommentHashStart  = '#'\n\tcommentSlashStart = '/'\n\tdqStringStart     = '\"'\n\tdqStringEnd       = '\"'\n\tsqStringStart     = '\\''\n\tsqStringEnd       = '\\''\n\toptValTerm        = ';'\n\ttopOptStart       = '{'\n\ttopOptValTerm     = ','\n\ttopOptTerm        = '}'\n\tblockStart        = '('\n\tblockEnd          = ')'\n\tmapEndString      = string(mapEnd)\n)\n\ntype stateFn func(lx *lexer) stateFn\n\ntype lexer struct {\n\tinput string\n\tstart int\n\tpos   int\n\twidth int\n\tline  int\n\tstate stateFn\n\titems chan item\n\n\t// A stack of state functions used to maintain context.\n\t// The idea is to reuse parts of the state machine in various places.\n\t// For example, values can appear at the top level or within arbitrarily\n\t// nested arrays. The last state on the stack is used after a value has\n\t// been lexed. Similarly for comments.\n\tstack []stateFn\n\n\t// Used for processing escapable substrings in double-quoted and raw strings\n\tstringParts   []string\n\tstringStateFn stateFn\n\n\t// lstart is the start position of the current line.\n\tlstart int\n\n\t// ilstart is the start position of the line from the current item.\n\tilstart int\n}\n\ntype item struct {\n\ttyp  itemType\n\tval  string\n\tline int\n\tpos  int\n}\n\nfunc (lx *lexer) nextItem() item {\n\tfor {\n\t\tselect {\n\t\tcase item := <-lx.items:\n\t\t\treturn item\n\t\tdefault:\n\t\t\tlx.state = lx.state(lx)\n\t\t}\n\t}\n}\n\nfunc lex(input string) *lexer {\n\tlx := &lexer{\n\t\tinput:       input,\n\t\tstate:       lexTop,\n\t\tline:        1,\n\t\titems:       make(chan item, 10),\n\t\tstack:       make([]stateFn, 0, 10),\n\t\tstringParts: []string{},\n\t}\n\treturn lx\n}\n\nfunc (lx *lexer) push(state stateFn) {\n\tlx.stack = append(lx.stack, state)\n}\n\nfunc (lx *lexer) pop() stateFn {\n\tif len(lx.stack) == 0 {\n\t\treturn lx.errorf(\"BUG in lexer: no states to pop.\")\n\t}\n\tli := len(lx.stack) - 1\n\tlast := lx.stack[li]\n\tlx.stack = lx.stack[0:li]\n\treturn last\n}\n\nfunc (lx *lexer) emit(typ itemType) {\n\tval := strings.Join(lx.stringParts, \"\") + lx.input[lx.start:lx.pos]\n\t// Position of item in line where it started.\n\tpos := lx.pos - lx.ilstart - len(val)\n\tlx.items <- item{typ, val, lx.line, pos}\n\tlx.start = lx.pos\n\tlx.ilstart = lx.lstart\n}\n\nfunc (lx *lexer) emitString() {\n\tvar finalString string\n\tif len(lx.stringParts) > 0 {\n\t\tfinalString = strings.Join(lx.stringParts, \"\") + lx.input[lx.start:lx.pos]\n\t\tlx.stringParts = []string{}\n\t} else {\n\t\tfinalString = lx.input[lx.start:lx.pos]\n\t}\n\t// Position of string in line where it started.\n\tpos := lx.pos - lx.ilstart - len(finalString)\n\tlx.items <- item{itemString, finalString, lx.line, pos}\n\tlx.start = lx.pos\n\tlx.ilstart = lx.lstart\n}\n\nfunc (lx *lexer) addCurrentStringPart(offset int) {\n\tlx.stringParts = append(lx.stringParts, lx.input[lx.start:lx.pos-offset])\n\tlx.start = lx.pos\n}\n\nfunc (lx *lexer) addStringPart(s string) stateFn {\n\tlx.stringParts = append(lx.stringParts, s)\n\tlx.start = lx.pos\n\treturn lx.stringStateFn\n}\n\nfunc (lx *lexer) hasEscapedParts() bool {\n\treturn len(lx.stringParts) > 0\n}\n\nfunc (lx *lexer) next() (r rune) {\n\tif lx.pos >= len(lx.input) {\n\t\tlx.width = 0\n\t\treturn eof\n\t}\n\n\tif lx.input[lx.pos] == '\\n' {\n\t\tlx.line++\n\n\t\t// Mark start position of current line.\n\t\tlx.lstart = lx.pos\n\t}\n\tr, lx.width = utf8.DecodeRuneInString(lx.input[lx.pos:])\n\tlx.pos += lx.width\n\n\treturn r\n}\n\n// ignore skips over the pending input before this point.\nfunc (lx *lexer) ignore() {\n\tlx.start = lx.pos\n\tlx.ilstart = lx.lstart\n}\n\n// backup steps back one rune. Can be called only once per call of next.\nfunc (lx *lexer) backup() {\n\tlx.pos -= lx.width\n\tif lx.pos < len(lx.input) && lx.input[lx.pos] == '\\n' {\n\t\tlx.line--\n\t}\n}\n\n// peek returns but does not consume the next rune in the input.\nfunc (lx *lexer) peek() rune {\n\tr := lx.next()\n\tlx.backup()\n\treturn r\n}\n\n// errorf stops all lexing by emitting an error and returning `nil`.\n// Note that any value that is a character is escaped if it's a special\n// character (new lines, tabs, etc.).\nfunc (lx *lexer) errorf(format string, values ...any) stateFn {\n\tfor i, value := range values {\n\t\tif v, ok := value.(rune); ok {\n\t\t\tvalues[i] = escapeSpecial(v)\n\t\t}\n\t}\n\n\t// Position of error in current line.\n\tpos := lx.pos - lx.lstart\n\tlx.items <- item{\n\t\titemError,\n\t\tfmt.Sprintf(format, values...),\n\t\tlx.line,\n\t\tpos,\n\t}\n\treturn nil\n}\n\n// lexTop consumes elements at the top level of data structure.\nfunc lexTop(lx *lexer) stateFn {\n\tr := lx.next()\n\tif unicode.IsSpace(r) {\n\t\treturn lexSkip(lx, lexTop)\n\t}\n\n\tswitch r {\n\tcase topOptStart:\n\t\tlx.push(lexTop)\n\t\treturn lexSkip(lx, lexBlockStart)\n\tcase commentHashStart:\n\t\tlx.push(lexTop)\n\t\treturn lexCommentStart\n\tcase commentSlashStart:\n\t\trn := lx.next()\n\t\tif rn == commentSlashStart {\n\t\t\tlx.push(lexTop)\n\t\t\treturn lexCommentStart\n\t\t}\n\t\tlx.backup()\n\t\tfallthrough\n\tcase eof:\n\t\tif lx.pos > lx.start {\n\t\t\treturn lx.errorf(\"Unexpected EOF.\")\n\t\t}\n\t\tlx.emit(itemEOF)\n\t\treturn nil\n\t}\n\n\t// At this point, the only valid item can be a key, so we back up\n\t// and let the key lexer do the rest.\n\tlx.backup()\n\tlx.push(lexTopValueEnd)\n\treturn lexKeyStart\n}\n\n// lexTopValueEnd is entered whenever a top-level value has been consumed.\n// It must see only whitespace, and will turn back to lexTop upon a new line.\n// If it sees EOF, it will quit the lexer successfully.\nfunc lexTopValueEnd(lx *lexer) stateFn {\n\tr := lx.next()\n\tswitch {\n\tcase r == commentHashStart:\n\t\t// a comment will read to a new line for us.\n\t\tlx.push(lexTop)\n\t\treturn lexCommentStart\n\tcase r == commentSlashStart:\n\t\trn := lx.next()\n\t\tif rn == commentSlashStart {\n\t\t\tlx.push(lexTop)\n\t\t\treturn lexCommentStart\n\t\t}\n\t\tlx.backup()\n\t\tfallthrough\n\tcase isWhitespace(r):\n\t\treturn lexTopValueEnd\n\tcase isNL(r) || r == eof || r == optValTerm || r == topOptValTerm || r == topOptTerm:\n\t\tlx.ignore()\n\t\treturn lexTop\n\t}\n\treturn lx.errorf(\"Expected a top-level value to end with a new line, \"+\n\t\t\"comment or EOF, but got '%v' instead.\", r)\n}\n\nfunc lexBlockStart(lx *lexer) stateFn {\n\tr := lx.next()\n\tif unicode.IsSpace(r) {\n\t\treturn lexSkip(lx, lexBlockStart)\n\t}\n\n\tswitch r {\n\tcase topOptStart:\n\t\tlx.push(lexBlockEnd)\n\t\treturn lexSkip(lx, lexBlockStart)\n\tcase topOptTerm:\n\t\tlx.ignore()\n\t\treturn lx.pop()\n\tcase commentHashStart:\n\t\tlx.push(lexBlockStart)\n\t\treturn lexCommentStart\n\tcase commentSlashStart:\n\t\trn := lx.next()\n\t\tif rn == commentSlashStart {\n\t\t\tlx.push(lexBlockStart)\n\t\t\treturn lexCommentStart\n\t\t}\n\t\tlx.backup()\n\t\tfallthrough\n\tcase eof:\n\t\tif lx.pos > lx.start {\n\t\t\treturn lx.errorf(\"Unexpected EOF.\")\n\t\t}\n\t\tlx.emit(itemEOF)\n\t\treturn nil\n\t}\n\n\t// At this point, the only valid item can be a key, so we back up\n\t// and let the key lexer do the rest.\n\tlx.backup()\n\tlx.push(lexBlockValueEnd)\n\treturn lexKeyStart\n}\n\n// lexBlockValueEnd is entered whenever a block-level value has been consumed.\n// It must see only whitespace, and will turn back to lexBlockStart upon a new line.\n// If it sees EOF, it will quit the lexer successfully.\nfunc lexBlockValueEnd(lx *lexer) stateFn {\n\tr := lx.next()\n\tswitch {\n\tcase r == commentHashStart:\n\t\t// a comment will read to a new line for us.\n\t\tlx.push(lexBlockValueEnd)\n\t\treturn lexCommentStart\n\tcase r == commentSlashStart:\n\t\trn := lx.next()\n\t\tif rn == commentSlashStart {\n\t\t\tlx.push(lexBlockValueEnd)\n\t\t\treturn lexCommentStart\n\t\t}\n\t\tlx.backup()\n\t\tfallthrough\n\tcase isWhitespace(r):\n\t\treturn lexBlockValueEnd\n\tcase isNL(r) || r == optValTerm || r == topOptValTerm:\n\t\tlx.ignore()\n\t\treturn lexBlockStart\n\tcase r == topOptTerm:\n\t\tlx.backup()\n\t\treturn lexBlockEnd\n\t}\n\treturn lx.errorf(\"Expected a block-level value to end with a new line, \"+\n\t\t\"comment or EOF, but got '%v' instead.\", r)\n}\n\n// lexBlockEnd is entered whenever a block-level value has been consumed.\n// It must see only whitespace, and will turn back to lexTop upon a \"}\".\nfunc lexBlockEnd(lx *lexer) stateFn {\n\tr := lx.next()\n\tswitch {\n\tcase r == commentHashStart:\n\t\t// a comment will read to a new line for us.\n\t\tlx.push(lexBlockStart)\n\t\treturn lexCommentStart\n\tcase r == commentSlashStart:\n\t\trn := lx.next()\n\t\tif rn == commentSlashStart {\n\t\t\tlx.push(lexBlockStart)\n\t\t\treturn lexCommentStart\n\t\t}\n\t\tlx.backup()\n\t\tfallthrough\n\tcase isNL(r) || isWhitespace(r):\n\t\treturn lexBlockEnd\n\tcase r == optValTerm || r == topOptValTerm:\n\t\tlx.ignore()\n\t\treturn lexBlockStart\n\tcase r == topOptTerm:\n\t\tlx.ignore()\n\t\treturn lx.pop()\n\t}\n\treturn lx.errorf(\"Expected a block-level to end with a '}', but got '%v' instead.\", r)\n}\n\n// lexKeyStart consumes a key name up until the first non-whitespace character.\n// lexKeyStart will ignore whitespace. It will also eat enclosing quotes.\nfunc lexKeyStart(lx *lexer) stateFn {\n\tr := lx.peek()\n\tswitch {\n\tcase isKeySeparator(r):\n\t\treturn lx.errorf(\"Unexpected key separator '%v'\", r)\n\tcase unicode.IsSpace(r):\n\t\tlx.next()\n\t\treturn lexSkip(lx, lexKeyStart)\n\tcase r == dqStringStart:\n\t\tlx.next()\n\t\treturn lexSkip(lx, lexDubQuotedKey)\n\tcase r == sqStringStart:\n\t\tlx.next()\n\t\treturn lexSkip(lx, lexQuotedKey)\n\t}\n\tlx.ignore()\n\tlx.next()\n\treturn lexKey\n}\n\n// lexDubQuotedKey consumes the text of a key between quotes.\nfunc lexDubQuotedKey(lx *lexer) stateFn {\n\tr := lx.peek()\n\tif r == dqStringEnd {\n\t\tlx.emit(itemKey)\n\t\tlx.next()\n\t\treturn lexSkip(lx, lexKeyEnd)\n\t} else if r == eof {\n\t\tif lx.pos > lx.start {\n\t\t\treturn lx.errorf(\"Unexpected EOF.\")\n\t\t}\n\t\tlx.emit(itemEOF)\n\t\treturn nil\n\t}\n\tlx.next()\n\treturn lexDubQuotedKey\n}\n\n// lexQuotedKey consumes the text of a key between quotes.\nfunc lexQuotedKey(lx *lexer) stateFn {\n\tr := lx.peek()\n\tif r == sqStringEnd {\n\t\tlx.emit(itemKey)\n\t\tlx.next()\n\t\treturn lexSkip(lx, lexKeyEnd)\n\t} else if r == eof {\n\t\tif lx.pos > lx.start {\n\t\t\treturn lx.errorf(\"Unexpected EOF.\")\n\t\t}\n\t\tlx.emit(itemEOF)\n\t\treturn nil\n\t}\n\tlx.next()\n\treturn lexQuotedKey\n}\n\n// keyCheckKeyword will check for reserved keywords as the key value when the key is\n// separated with a space.\nfunc (lx *lexer) keyCheckKeyword(fallThrough, push stateFn) stateFn {\n\tkey := strings.ToLower(lx.input[lx.start:lx.pos])\n\tswitch key {\n\tcase \"include\":\n\t\tlx.ignore()\n\t\tif push != nil {\n\t\t\tlx.push(push)\n\t\t}\n\t\treturn lexIncludeStart\n\t}\n\tlx.emit(itemKey)\n\treturn fallThrough\n}\n\n// lexIncludeStart will consume the whitespace til the start of the value.\nfunc lexIncludeStart(lx *lexer) stateFn {\n\tr := lx.next()\n\tif isWhitespace(r) {\n\t\treturn lexSkip(lx, lexIncludeStart)\n\t}\n\tlx.backup()\n\treturn lexInclude\n}\n\n// lexIncludeQuotedString consumes the inner contents of a string. It assumes that the\n// beginning '\"' has already been consumed and ignored. It will not interpret any\n// internal contents.\nfunc lexIncludeQuotedString(lx *lexer) stateFn {\n\tr := lx.next()\n\tswitch {\n\tcase r == sqStringEnd:\n\t\tlx.backup()\n\t\tlx.emit(itemInclude)\n\t\tlx.next()\n\t\tlx.ignore()\n\t\treturn lx.pop()\n\tcase r == eof:\n\t\treturn lx.errorf(\"Unexpected EOF in quoted include\")\n\t}\n\treturn lexIncludeQuotedString\n}\n\n// lexIncludeDubQuotedString consumes the inner contents of a string. It assumes that the\n// beginning '\"' has already been consumed and ignored. It will not interpret any\n// internal contents.\nfunc lexIncludeDubQuotedString(lx *lexer) stateFn {\n\tr := lx.next()\n\tswitch {\n\tcase r == dqStringEnd:\n\t\tlx.backup()\n\t\tlx.emit(itemInclude)\n\t\tlx.next()\n\t\tlx.ignore()\n\t\treturn lx.pop()\n\tcase r == eof:\n\t\treturn lx.errorf(\"Unexpected EOF in double quoted include\")\n\t}\n\treturn lexIncludeDubQuotedString\n}\n\n// lexIncludeString consumes the inner contents of a raw string.\nfunc lexIncludeString(lx *lexer) stateFn {\n\tr := lx.next()\n\tswitch {\n\tcase isNL(r) || r == eof || r == optValTerm || r == mapEnd || isWhitespace(r):\n\t\tlx.backup()\n\t\tlx.emit(itemInclude)\n\t\treturn lx.pop()\n\tcase r == sqStringEnd:\n\t\tlx.backup()\n\t\tlx.emit(itemInclude)\n\t\tlx.next()\n\t\tlx.ignore()\n\t\treturn lx.pop()\n\t}\n\treturn lexIncludeString\n}\n\n// lexInclude will consume the include value.\nfunc lexInclude(lx *lexer) stateFn {\n\tr := lx.next()\n\tswitch {\n\tcase r == sqStringStart:\n\t\tlx.ignore() // ignore the \" or '\n\t\treturn lexIncludeQuotedString\n\tcase r == dqStringStart:\n\t\tlx.ignore() // ignore the \" or '\n\t\treturn lexIncludeDubQuotedString\n\tcase r == arrayStart:\n\t\treturn lx.errorf(\"Expected include value but found start of an array\")\n\tcase r == mapStart:\n\t\treturn lx.errorf(\"Expected include value but found start of a map\")\n\tcase r == blockStart:\n\t\treturn lx.errorf(\"Expected include value but found start of a block\")\n\tcase unicode.IsDigit(r), r == '-':\n\t\treturn lx.errorf(\"Expected include value but found start of a number\")\n\tcase r == '\\\\':\n\t\treturn lx.errorf(\"Expected include value but found escape sequence\")\n\tcase isNL(r):\n\t\treturn lx.errorf(\"Expected include value but found new line\")\n\t}\n\tlx.backup()\n\treturn lexIncludeString\n}\n\n// lexKey consumes the text of a key. Assumes that the first character (which\n// is not whitespace) has already been consumed.\nfunc lexKey(lx *lexer) stateFn {\n\tr := lx.peek()\n\tif unicode.IsSpace(r) {\n\t\t// Spaces signal we could be looking at a keyword, e.g. include.\n\t\t// Keywords will eat the keyword and set the appropriate return stateFn.\n\t\treturn lx.keyCheckKeyword(lexKeyEnd, nil)\n\t} else if isKeySeparator(r) || r == eof {\n\t\tlx.emit(itemKey)\n\t\treturn lexKeyEnd\n\t}\n\tlx.next()\n\treturn lexKey\n}\n\n// lexKeyEnd consumes the end of a key (up to the key separator).\n// Assumes that the first whitespace character after a key (or the '=' or ':'\n// separator) has NOT been consumed.\nfunc lexKeyEnd(lx *lexer) stateFn {\n\tr := lx.next()\n\tswitch {\n\tcase unicode.IsSpace(r):\n\t\treturn lexSkip(lx, lexKeyEnd)\n\tcase isKeySeparator(r):\n\t\treturn lexSkip(lx, lexValue)\n\tcase r == eof:\n\t\tlx.emit(itemEOF)\n\t\treturn nil\n\t}\n\t// We start the value here\n\tlx.backup()\n\treturn lexValue\n}\n\n// lexValue starts the consumption of a value anywhere a value is expected.\n// lexValue will ignore whitespace.\n// After a value is lexed, the last state on the next is popped and returned.\nfunc lexValue(lx *lexer) stateFn {\n\t// We allow whitespace to precede a value, but NOT new lines.\n\t// In array syntax, the array states are responsible for ignoring new lines.\n\tr := lx.next()\n\tif isWhitespace(r) {\n\t\treturn lexSkip(lx, lexValue)\n\t}\n\n\tswitch {\n\tcase r == arrayStart:\n\t\tlx.ignore()\n\t\tlx.emit(itemArrayStart)\n\t\treturn lexArrayValue\n\tcase r == mapStart:\n\t\tlx.ignore()\n\t\tlx.emit(itemMapStart)\n\t\treturn lexMapKeyStart\n\tcase r == sqStringStart:\n\t\tlx.ignore() // ignore the \" or '\n\t\treturn lexQuotedString\n\tcase r == dqStringStart:\n\t\tlx.ignore() // ignore the \" or '\n\t\tlx.stringStateFn = lexDubQuotedString\n\t\treturn lexDubQuotedString\n\tcase r == '-':\n\t\treturn lexNegNumberStart\n\tcase r == blockStart:\n\t\tlx.ignore()\n\t\treturn lexBlock\n\tcase unicode.IsDigit(r):\n\t\tlx.backup() // avoid an extra state and use the same as above\n\t\treturn lexNumberOrDateOrStringOrIPStart\n\tcase r == '.': // special error case, be kind to users\n\t\treturn lx.errorf(\"Floats must start with a digit\")\n\tcase isNL(r):\n\t\treturn lx.errorf(\"Expected value but found new line\")\n\t}\n\tlx.backup()\n\tlx.stringStateFn = lexString\n\treturn lexString\n}\n\n// lexArrayValue consumes one value in an array. It assumes that '[' or ','\n// have already been consumed. All whitespace and new lines are ignored.\nfunc lexArrayValue(lx *lexer) stateFn {\n\tr := lx.next()\n\tswitch {\n\tcase unicode.IsSpace(r):\n\t\treturn lexSkip(lx, lexArrayValue)\n\tcase r == commentHashStart:\n\t\tlx.push(lexArrayValue)\n\t\treturn lexCommentStart\n\tcase r == commentSlashStart:\n\t\trn := lx.next()\n\t\tif rn == commentSlashStart {\n\t\t\tlx.push(lexArrayValue)\n\t\t\treturn lexCommentStart\n\t\t}\n\t\tlx.backup()\n\t\tfallthrough\n\tcase r == arrayValTerm:\n\t\treturn lx.errorf(\"Unexpected array value terminator '%v'.\", arrayValTerm)\n\tcase r == arrayEnd:\n\t\treturn lexArrayEnd\n\t}\n\n\tlx.backup()\n\tlx.push(lexArrayValueEnd)\n\treturn lexValue\n}\n\n// lexArrayValueEnd consumes the cruft between values of an array. Namely,\n// it ignores whitespace and expects either a ',' or a ']'.\nfunc lexArrayValueEnd(lx *lexer) stateFn {\n\tr := lx.next()\n\tswitch {\n\tcase isWhitespace(r):\n\t\treturn lexSkip(lx, lexArrayValueEnd)\n\tcase r == commentHashStart:\n\t\tlx.push(lexArrayValueEnd)\n\t\treturn lexCommentStart\n\tcase r == commentSlashStart:\n\t\trn := lx.next()\n\t\tif rn == commentSlashStart {\n\t\t\tlx.push(lexArrayValueEnd)\n\t\t\treturn lexCommentStart\n\t\t}\n\t\tlx.backup()\n\t\tfallthrough\n\tcase r == arrayValTerm || isNL(r):\n\t\treturn lexSkip(lx, lexArrayValue) // Move onto next\n\tcase r == arrayEnd:\n\t\treturn lexArrayEnd\n\t}\n\treturn lx.errorf(\"Expected an array value terminator %q or an array \"+\n\t\t\"terminator %q, but got '%v' instead.\", arrayValTerm, arrayEnd, r)\n}\n\n// lexArrayEnd finishes the lexing of an array. It assumes that a ']' has\n// just been consumed.\nfunc lexArrayEnd(lx *lexer) stateFn {\n\tlx.ignore()\n\tlx.emit(itemArrayEnd)\n\treturn lx.pop()\n}\n\n// lexMapKeyStart consumes a key name up until the first non-whitespace\n// character.\n// lexMapKeyStart will ignore whitespace.\nfunc lexMapKeyStart(lx *lexer) stateFn {\n\tr := lx.peek()\n\tswitch {\n\tcase isKeySeparator(r):\n\t\treturn lx.errorf(\"Unexpected key separator '%v'.\", r)\n\tcase r == arrayEnd:\n\t\treturn lx.errorf(\"Unexpected array end '%v' processing map.\", r)\n\tcase unicode.IsSpace(r):\n\t\tlx.next()\n\t\treturn lexSkip(lx, lexMapKeyStart)\n\tcase r == mapEnd:\n\t\tlx.next()\n\t\treturn lexSkip(lx, lexMapEnd)\n\tcase r == commentHashStart:\n\t\tlx.next()\n\t\tlx.push(lexMapKeyStart)\n\t\treturn lexCommentStart\n\tcase r == commentSlashStart:\n\t\tlx.next()\n\t\trn := lx.next()\n\t\tif rn == commentSlashStart {\n\t\t\tlx.push(lexMapKeyStart)\n\t\t\treturn lexCommentStart\n\t\t}\n\t\tlx.backup()\n\tcase r == sqStringStart:\n\t\tlx.next()\n\t\treturn lexSkip(lx, lexMapQuotedKey)\n\tcase r == dqStringStart:\n\t\tlx.next()\n\t\treturn lexSkip(lx, lexMapDubQuotedKey)\n\tcase r == eof:\n\t\treturn lx.errorf(\"Unexpected EOF processing map.\")\n\t}\n\tlx.ignore()\n\tlx.next()\n\treturn lexMapKey\n}\n\n// lexMapQuotedKey consumes the text of a key between quotes.\nfunc lexMapQuotedKey(lx *lexer) stateFn {\n\tif r := lx.peek(); r == eof {\n\t\treturn lx.errorf(\"Unexpected EOF processing quoted map key.\")\n\t} else if r == sqStringEnd {\n\t\tlx.emit(itemKey)\n\t\tlx.next()\n\t\treturn lexSkip(lx, lexMapKeyEnd)\n\t}\n\tlx.next()\n\treturn lexMapQuotedKey\n}\n\n// lexMapDubQuotedKey consumes the text of a key between quotes.\nfunc lexMapDubQuotedKey(lx *lexer) stateFn {\n\tif r := lx.peek(); r == eof {\n\t\treturn lx.errorf(\"Unexpected EOF processing double quoted map key.\")\n\t} else if r == dqStringEnd {\n\t\tlx.emit(itemKey)\n\t\tlx.next()\n\t\treturn lexSkip(lx, lexMapKeyEnd)\n\t}\n\tlx.next()\n\treturn lexMapDubQuotedKey\n}\n\n// lexMapKey consumes the text of a key. Assumes that the first character (which\n// is not whitespace) has already been consumed.\nfunc lexMapKey(lx *lexer) stateFn {\n\tif r := lx.peek(); r == eof {\n\t\treturn lx.errorf(\"Unexpected EOF processing map key.\")\n\t} else if unicode.IsSpace(r) {\n\t\t// Spaces signal we could be looking at a keyword, e.g. include.\n\t\t// Keywords will eat the keyword and set the appropriate return stateFn.\n\t\treturn lx.keyCheckKeyword(lexMapKeyEnd, lexMapValueEnd)\n\t} else if isKeySeparator(r) {\n\t\tlx.emit(itemKey)\n\t\treturn lexMapKeyEnd\n\t}\n\tlx.next()\n\treturn lexMapKey\n}\n\n// lexMapKeyEnd consumes the end of a key (up to the key separator).\n// Assumes that the first whitespace character after a key (or the '='\n// separator) has NOT been consumed.\nfunc lexMapKeyEnd(lx *lexer) stateFn {\n\tr := lx.next()\n\tswitch {\n\tcase unicode.IsSpace(r):\n\t\treturn lexSkip(lx, lexMapKeyEnd)\n\tcase isKeySeparator(r):\n\t\treturn lexSkip(lx, lexMapValue)\n\t}\n\t// We start the value here\n\tlx.backup()\n\treturn lexMapValue\n}\n\n// lexMapValue consumes one value in a map. It assumes that '{' or ','\n// have already been consumed. All whitespace and new lines are ignored.\n// Map values can be separated by ',' or simple NLs.\nfunc lexMapValue(lx *lexer) stateFn {\n\tr := lx.next()\n\tswitch {\n\tcase unicode.IsSpace(r):\n\t\treturn lexSkip(lx, lexMapValue)\n\tcase r == mapValTerm:\n\t\treturn lx.errorf(\"Unexpected map value terminator %q.\", mapValTerm)\n\tcase r == mapEnd:\n\t\treturn lexSkip(lx, lexMapEnd)\n\t}\n\tlx.backup()\n\tlx.push(lexMapValueEnd)\n\treturn lexValue\n}\n\n// lexMapValueEnd consumes the cruft between values of a map. Namely,\n// it ignores whitespace and expects either a ',' or a '}'.\nfunc lexMapValueEnd(lx *lexer) stateFn {\n\tr := lx.next()\n\tswitch {\n\tcase isWhitespace(r):\n\t\treturn lexSkip(lx, lexMapValueEnd)\n\tcase r == commentHashStart:\n\t\tlx.push(lexMapValueEnd)\n\t\treturn lexCommentStart\n\tcase r == commentSlashStart:\n\t\trn := lx.next()\n\t\tif rn == commentSlashStart {\n\t\t\tlx.push(lexMapValueEnd)\n\t\t\treturn lexCommentStart\n\t\t}\n\t\tlx.backup()\n\t\tfallthrough\n\tcase r == optValTerm || r == mapValTerm || isNL(r):\n\t\treturn lexSkip(lx, lexMapKeyStart) // Move onto next\n\tcase r == mapEnd:\n\t\treturn lexSkip(lx, lexMapEnd)\n\t}\n\treturn lx.errorf(\"Expected a map value terminator %q or a map \"+\n\t\t\"terminator %q, but got '%v' instead.\", mapValTerm, mapEnd, r)\n}\n\n// lexMapEnd finishes the lexing of a map. It assumes that a '}' has\n// just been consumed.\nfunc lexMapEnd(lx *lexer) stateFn {\n\tlx.ignore()\n\tlx.emit(itemMapEnd)\n\treturn lx.pop()\n}\n\n// Checks if the unquoted string was actually a boolean\nfunc (lx *lexer) isBool() bool {\n\tstr := strings.ToLower(lx.input[lx.start:lx.pos])\n\treturn str == \"true\" || str == \"false\" ||\n\t\tstr == \"on\" || str == \"off\" ||\n\t\tstr == \"yes\" || str == \"no\"\n}\n\n// Check if the unquoted string is a variable reference, starting with $.\nfunc (lx *lexer) isVariable() bool {\n\tif lx.start >= len(lx.input) {\n\t\treturn false\n\t}\n\tif lx.input[lx.start] == '$' {\n\t\tlx.start += 1\n\t\treturn true\n\t}\n\treturn false\n}\n\n// lexQuotedString consumes the inner contents of a string. It assumes that the\n// beginning '\"' has already been consumed and ignored. It will not interpret any\n// internal contents.\nfunc lexQuotedString(lx *lexer) stateFn {\n\tr := lx.next()\n\tswitch {\n\tcase r == sqStringEnd:\n\t\tlx.backup()\n\t\tlx.emit(itemString)\n\t\tlx.next()\n\t\tlx.ignore()\n\t\treturn lx.pop()\n\tcase r == eof:\n\t\tif lx.pos > lx.start {\n\t\t\treturn lx.errorf(\"Unexpected EOF.\")\n\t\t}\n\t\tlx.emit(itemEOF)\n\t\treturn nil\n\t}\n\treturn lexQuotedString\n}\n\n// lexDubQuotedString consumes the inner contents of a string. It assumes that the\n// beginning '\"' has already been consumed and ignored. It will not interpret any\n// internal contents.\nfunc lexDubQuotedString(lx *lexer) stateFn {\n\tr := lx.next()\n\tswitch {\n\tcase r == '\\\\':\n\t\tlx.addCurrentStringPart(1)\n\t\treturn lexStringEscape\n\tcase r == dqStringEnd:\n\t\tlx.backup()\n\t\tlx.emitString()\n\t\tlx.next()\n\t\tlx.ignore()\n\t\treturn lx.pop()\n\tcase r == eof:\n\t\tif lx.pos > lx.start {\n\t\t\treturn lx.errorf(\"Unexpected EOF.\")\n\t\t}\n\t\tlx.emit(itemEOF)\n\t\treturn nil\n\t}\n\treturn lexDubQuotedString\n}\n\n// lexString consumes the inner contents of a raw string.\nfunc lexString(lx *lexer) stateFn {\n\tr := lx.next()\n\tswitch {\n\tcase r == '\\\\':\n\t\tlx.addCurrentStringPart(1)\n\t\treturn lexStringEscape\n\t// Termination of non-quoted strings\n\tcase isNL(r) || r == eof || r == optValTerm ||\n\t\tr == arrayValTerm || r == arrayEnd || r == mapEnd ||\n\t\tisWhitespace(r):\n\n\t\tlx.backup()\n\t\tif lx.hasEscapedParts() {\n\t\t\tlx.emitString()\n\t\t} else if lx.isBool() {\n\t\t\tlx.emit(itemBool)\n\t\t} else if lx.isVariable() {\n\t\t\tlx.emit(itemVariable)\n\t\t} else {\n\t\t\tlx.emitString()\n\t\t}\n\t\treturn lx.pop()\n\tcase r == sqStringEnd:\n\t\tlx.backup()\n\t\tlx.emitString()\n\t\tlx.next()\n\t\tlx.ignore()\n\t\treturn lx.pop()\n\t}\n\treturn lexString\n}\n\n// lexBlock consumes the inner contents as a string. It assumes that the\n// beginning '(' has already been consumed and ignored. It will continue\n// processing until it finds a ')' on a new line by itself.\nfunc lexBlock(lx *lexer) stateFn {\n\tr := lx.next()\n\tswitch {\n\tcase r == blockEnd:\n\t\tlx.backup()\n\t\tlx.backup()\n\n\t\t// Looking for a ')' character on a line by itself, if the previous\n\t\t// character isn't a new line, then break so we keep processing the block.\n\t\tif lx.next() != '\\n' {\n\t\t\tlx.next()\n\t\t\tbreak\n\t\t}\n\t\tlx.next()\n\n\t\t// Make sure the next character is a new line or an eof. We want a ')' on a\n\t\t// bare line by itself.\n\t\tswitch lx.next() {\n\t\tcase '\\n', eof:\n\t\t\tlx.backup()\n\t\t\tlx.backup()\n\t\t\tlx.emit(itemString)\n\t\t\tlx.next()\n\t\t\tlx.ignore()\n\t\t\treturn lx.pop()\n\t\t}\n\t\tlx.backup()\n\tcase r == eof:\n\t\treturn lx.errorf(\"Unexpected EOF processing block.\")\n\t}\n\treturn lexBlock\n}\n\n// lexStringEscape consumes an escaped character. It assumes that the preceding\n// '\\\\' has already been consumed.\nfunc lexStringEscape(lx *lexer) stateFn {\n\tr := lx.next()\n\tswitch r {\n\tcase 'x':\n\t\treturn lexStringBinary\n\tcase 't':\n\t\treturn lx.addStringPart(\"\\t\")\n\tcase 'n':\n\t\treturn lx.addStringPart(\"\\n\")\n\tcase 'r':\n\t\treturn lx.addStringPart(\"\\r\")\n\tcase '\"':\n\t\treturn lx.addStringPart(\"\\\"\")\n\tcase '\\\\':\n\t\treturn lx.addStringPart(\"\\\\\")\n\t}\n\treturn lx.errorf(\"Invalid escape character '%v'. Only the following \"+\n\t\t\"escape characters are allowed: \\\\xXX, \\\\t, \\\\n, \\\\r, \\\\\\\", \\\\\\\\.\", r)\n}\n\n// lexStringBinary consumes two hexadecimal digits following '\\x'. It assumes\n// that the '\\x' has already been consumed.\nfunc lexStringBinary(lx *lexer) stateFn {\n\tr := lx.next()\n\tif isNL(r) {\n\t\treturn lx.errorf(\"Expected two hexadecimal digits after '\\\\x', but hit end of line\")\n\t}\n\tr = lx.next()\n\tif isNL(r) {\n\t\treturn lx.errorf(\"Expected two hexadecimal digits after '\\\\x', but hit end of line\")\n\t}\n\toffset := lx.pos - 2\n\tbyteString, err := hex.DecodeString(lx.input[offset:lx.pos])\n\tif err != nil {\n\t\treturn lx.errorf(\"Expected two hexadecimal digits after '\\\\x', but got '%s'\", lx.input[offset:lx.pos])\n\t}\n\tlx.addStringPart(string(byteString))\n\treturn lx.stringStateFn\n}\n\n// lexNumberOrDateOrStringOrIPStart consumes either a (positive)\n// integer, a float, a datetime, or IP, or String that started with a\n// number.  It assumes that NO negative sign has been consumed, that\n// is triggered above.\nfunc lexNumberOrDateOrStringOrIPStart(lx *lexer) stateFn {\n\tr := lx.next()\n\tif !unicode.IsDigit(r) {\n\t\tif r == '.' {\n\t\t\treturn lx.errorf(\"Floats must start with a digit, not '.'.\")\n\t\t}\n\t\treturn lx.errorf(\"Expected a digit but got '%v'.\", r)\n\t}\n\treturn lexNumberOrDateOrStringOrIP\n}\n\n// lexNumberOrDateOrStringOrIP consumes either a (positive) integer,\n// float, datetime, IP or string without quotes that starts with a\n// number.\nfunc lexNumberOrDateOrStringOrIP(lx *lexer) stateFn {\n\tr := lx.next()\n\tswitch {\n\tcase r == '-':\n\t\tif lx.pos-lx.start != 5 {\n\t\t\treturn lx.errorf(\"All ISO8601 dates must be in full Zulu form.\")\n\t\t}\n\t\treturn lexDateAfterYear\n\tcase unicode.IsDigit(r):\n\t\treturn lexNumberOrDateOrStringOrIP\n\tcase r == '.':\n\t\t// Assume float at first, but could be IP\n\t\treturn lexFloatStart\n\tcase isNumberSuffix(r):\n\t\treturn lexConvenientNumber\n\tcase !(isNL(r) || r == eof || r == mapEnd || r == optValTerm || r == mapValTerm || isWhitespace(r) || unicode.IsDigit(r)):\n\t\t// Treat it as a string value once we get a rune that\n\t\t// is not a number.\n\t\tlx.stringStateFn = lexString\n\t\treturn lexString\n\t}\n\tlx.backup()\n\tlx.emit(itemInteger)\n\treturn lx.pop()\n}\n\n// lexConvenientNumber is when we have a suffix, e.g. 1k or 1Mb\nfunc lexConvenientNumber(lx *lexer) stateFn {\n\tr := lx.next()\n\tswitch {\n\tcase r == 'b' || r == 'B' || r == 'i' || r == 'I':\n\t\treturn lexConvenientNumber\n\t}\n\tlx.backup()\n\tif isNL(r) || r == eof || r == mapEnd || r == optValTerm || r == mapValTerm || isWhitespace(r) || unicode.IsDigit(r) {\n\t\tlx.emit(itemInteger)\n\t\treturn lx.pop()\n\t}\n\t// This is not a number, so treat it as a string.\n\tlx.stringStateFn = lexString\n\treturn lexString\n}\n\n// lexDateAfterYear consumes a full Zulu Datetime in ISO8601 format.\n// It assumes that \"YYYY-\" has already been consumed.\nfunc lexDateAfterYear(lx *lexer) stateFn {\n\tformats := []rune{\n\t\t// digits are '0'.\n\t\t// everything else is direct equality.\n\t\t'0', '0', '-', '0', '0',\n\t\t'T',\n\t\t'0', '0', ':', '0', '0', ':', '0', '0',\n\t\t'Z',\n\t}\n\tfor _, f := range formats {\n\t\tr := lx.next()\n\t\tif f == '0' {\n\t\t\tif !unicode.IsDigit(r) {\n\t\t\t\treturn lx.errorf(\"Expected digit in ISO8601 datetime, \"+\n\t\t\t\t\t\"but found '%v' instead.\", r)\n\t\t\t}\n\t\t} else if f != r {\n\t\t\treturn lx.errorf(\"Expected '%v' in ISO8601 datetime, \"+\n\t\t\t\t\"but found '%v' instead.\", f, r)\n\t\t}\n\t}\n\tlx.emit(itemDatetime)\n\treturn lx.pop()\n}\n\n// lexNegNumberStart consumes either an integer or a float. It assumes that a\n// negative sign has already been read, but that *no* digits have been consumed.\n// lexNegNumberStart will move to the appropriate integer or float states.\nfunc lexNegNumberStart(lx *lexer) stateFn {\n\t// we MUST see a digit. Even floats have to start with a digit.\n\tr := lx.next()\n\tif !unicode.IsDigit(r) {\n\t\tif r == '.' {\n\t\t\treturn lx.errorf(\"Floats must start with a digit, not '.'.\")\n\t\t}\n\t\treturn lx.errorf(\"Expected a digit but got '%v'.\", r)\n\t}\n\treturn lexNegNumber\n}\n\n// lexNegNumber consumes a negative integer or a float after seeing the first digit.\nfunc lexNegNumber(lx *lexer) stateFn {\n\tr := lx.next()\n\tswitch {\n\tcase unicode.IsDigit(r):\n\t\treturn lexNegNumber\n\tcase r == '.':\n\t\treturn lexFloatStart\n\tcase isNumberSuffix(r):\n\t\treturn lexConvenientNumber\n\t}\n\tlx.backup()\n\tlx.emit(itemInteger)\n\treturn lx.pop()\n}\n\n// lexFloatStart starts the consumption of digits of a float after a '.'.\n// Namely, at least one digit is required.\nfunc lexFloatStart(lx *lexer) stateFn {\n\tr := lx.next()\n\tif !unicode.IsDigit(r) {\n\t\treturn lx.errorf(\"Floats must have a digit after the '.', but got \"+\n\t\t\t\"'%v' instead.\", r)\n\t}\n\treturn lexFloat\n}\n\n// lexFloat consumes the digits of a float after a '.'.\n// Assumes that one digit has been consumed after a '.' already.\nfunc lexFloat(lx *lexer) stateFn {\n\tr := lx.next()\n\tif unicode.IsDigit(r) {\n\t\treturn lexFloat\n\t}\n\n\t// Not a digit, if its another '.', need to see if we falsely assumed a float.\n\tif r == '.' {\n\t\treturn lexIPAddr\n\t}\n\n\tlx.backup()\n\tlx.emit(itemFloat)\n\treturn lx.pop()\n}\n\n// lexIPAddr consumes IP addrs, like 127.0.0.1:4222\nfunc lexIPAddr(lx *lexer) stateFn {\n\tr := lx.next()\n\tif unicode.IsDigit(r) || r == '.' || r == ':' || r == '-' {\n\t\treturn lexIPAddr\n\t}\n\tlx.backup()\n\tlx.emit(itemString)\n\treturn lx.pop()\n}\n\n// lexCommentStart begins the lexing of a comment. It will emit\n// itemCommentStart and consume no characters, passing control to lexComment.\nfunc lexCommentStart(lx *lexer) stateFn {\n\tlx.ignore()\n\tlx.emit(itemCommentStart)\n\treturn lexComment\n}\n\n// lexComment lexes an entire comment. It assumes that '#' has been consumed.\n// It will consume *up to* the first new line character, and pass control\n// back to the last state on the stack.\nfunc lexComment(lx *lexer) stateFn {\n\tr := lx.peek()\n\tif isNL(r) || r == eof {\n\t\tlx.emit(itemText)\n\t\treturn lx.pop()\n\t}\n\tlx.next()\n\treturn lexComment\n}\n\n// lexSkip ignores all slurped input and moves on to the next state.\nfunc lexSkip(lx *lexer, nextState stateFn) stateFn {\n\treturn func(lx *lexer) stateFn {\n\t\tlx.ignore()\n\t\treturn nextState\n\t}\n}\n\n// Tests to see if we have a number suffix\nfunc isNumberSuffix(r rune) bool {\n\treturn r == 'k' || r == 'K' || r == 'm' || r == 'M' || r == 'g' || r == 'G' || r == 't' || r == 'T' || r == 'p' || r == 'P' || r == 'e' || r == 'E'\n}\n\n// Tests for both key separators\nfunc isKeySeparator(r rune) bool {\n\treturn r == keySepEqual || r == keySepColon\n}\n\n// isWhitespace returns true if `r` is a whitespace character according\n// to the spec.\nfunc isWhitespace(r rune) bool {\n\treturn r == '\\t' || r == ' '\n}\n\nfunc isNL(r rune) bool {\n\treturn r == '\\n' || r == '\\r'\n}\n\nfunc (itype itemType) String() string {\n\tswitch itype {\n\tcase itemError:\n\t\treturn \"Error\"\n\tcase itemNIL:\n\t\treturn \"NIL\"\n\tcase itemEOF:\n\t\treturn \"EOF\"\n\tcase itemText:\n\t\treturn \"Text\"\n\tcase itemString:\n\t\treturn \"String\"\n\tcase itemBool:\n\t\treturn \"Bool\"\n\tcase itemInteger:\n\t\treturn \"Integer\"\n\tcase itemFloat:\n\t\treturn \"Float\"\n\tcase itemDatetime:\n\t\treturn \"DateTime\"\n\tcase itemKey:\n\t\treturn \"Key\"\n\tcase itemArrayStart:\n\t\treturn \"ArrayStart\"\n\tcase itemArrayEnd:\n\t\treturn \"ArrayEnd\"\n\tcase itemMapStart:\n\t\treturn \"MapStart\"\n\tcase itemMapEnd:\n\t\treturn \"MapEnd\"\n\tcase itemCommentStart:\n\t\treturn \"CommentStart\"\n\tcase itemVariable:\n\t\treturn \"Variable\"\n\tcase itemInclude:\n\t\treturn \"Include\"\n\t}\n\tpanic(fmt.Sprintf(\"BUG: Unknown type '%s'.\", itype.String()))\n}\n\nfunc (item item) String() string {\n\treturn fmt.Sprintf(\"(%s, '%s', %d, %d)\", item.typ.String(), item.val, item.line, item.pos)\n}\n\nfunc escapeSpecial(c rune) string {\n\tswitch c {\n\tcase '\\n':\n\t\treturn \"\\\\n\"\n\t}\n\treturn string(c)\n}\n",
    "source_file": "conf/lex.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2020-2025 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n//go:build gofuzz\n\npackage conf\n\nfunc Fuzz(data []byte) int {\n\t_, err := Parse(string(data))\n\tif err != nil {\n\t\treturn 0\n\t}\n\treturn 1\n}\n",
    "source_file": "conf/fuzz.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2013-2025 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n// Package conf supports a configuration file format used by gnatsd. It is\n// a flexible format that combines the best of traditional\n// configuration formats and newer styles such as JSON and YAML.\npackage conf\n\n// The format supported is less restrictive than today's formats.\n// Supports mixed Arrays [], nested Maps {}, multiple comment types (# and //)\n// Also supports key value assignments using '=' or ':' or whiteSpace()\n//   e.g. foo = 2, foo : 2, foo 2\n// maps can be assigned with no key separator as well\n// semicolons as value terminators in key/value assignments are optional\n//\n// see parse_test.go for more examples.\n\nimport (\n\t\"crypto/sha256\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"strconv\"\n\t\"strings\"\n\t\"time\"\n\t\"unicode\"\n)\n\nconst _EMPTY_ = \"\"\n\ntype parser struct {\n\tmapping map[string]any\n\tlx      *lexer\n\n\t// The current scoped context, can be array or map\n\tctx any\n\n\t// stack of contexts, either map or array/slice stack\n\tctxs []any\n\n\t// Keys stack\n\tkeys []string\n\n\t// Keys stack as items\n\tikeys []item\n\n\t// The config file path, empty by default.\n\tfp string\n\n\t// pedantic reports error when configuration is not correct.\n\tpedantic bool\n}\n\n// Parse will return a map of keys to any, although concrete types\n// underly them. The values supported are string, bool, int64, float64, DateTime.\n// Arrays and nested Maps are also supported.\nfunc Parse(data string) (map[string]any, error) {\n\tp, err := parse(data, \"\", false)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn p.mapping, nil\n}\n\n// ParseWithChecks is equivalent to Parse but runs in pedantic mode.\nfunc ParseWithChecks(data string) (map[string]any, error) {\n\tp, err := parse(data, \"\", true)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn p.mapping, nil\n}\n\n// ParseFile is a helper to open file, etc. and parse the contents.\nfunc ParseFile(fp string) (map[string]any, error) {\n\tdata, err := os.ReadFile(fp)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"error opening config file: %v\", err)\n\t}\n\n\tp, err := parse(string(data), fp, false)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn p.mapping, nil\n}\n\n// ParseFileWithChecks is equivalent to ParseFile but runs in pedantic mode.\nfunc ParseFileWithChecks(fp string) (map[string]any, error) {\n\tdata, err := os.ReadFile(fp)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tp, err := parse(string(data), fp, true)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn p.mapping, nil\n}\n\n// cleanupUsedEnvVars will recursively remove all already used\n// environment variables which might be in the parsed tree.\nfunc cleanupUsedEnvVars(m map[string]any) {\n\tfor k, v := range m {\n\t\tt := v.(*token)\n\t\tif t.usedVariable {\n\t\t\tdelete(m, k)\n\t\t\tcontinue\n\t\t}\n\t\t// Cleanup any other env var that is still in the map.\n\t\tif tm, ok := t.value.(map[string]any); ok {\n\t\t\tcleanupUsedEnvVars(tm)\n\t\t}\n\t}\n}\n\n// ParseFileWithChecksDigest returns the processed config and a digest\n// that represents the configuration.\nfunc ParseFileWithChecksDigest(fp string) (map[string]any, string, error) {\n\tdata, err := os.ReadFile(fp)\n\tif err != nil {\n\t\treturn nil, _EMPTY_, err\n\t}\n\tp, err := parse(string(data), fp, true)\n\tif err != nil {\n\t\treturn nil, _EMPTY_, err\n\t}\n\t// Filter out any environment variables before taking the digest.\n\tcleanupUsedEnvVars(p.mapping)\n\tdigest := sha256.New()\n\te := json.NewEncoder(digest)\n\terr = e.Encode(p.mapping)\n\tif err != nil {\n\t\treturn nil, _EMPTY_, err\n\t}\n\treturn p.mapping, fmt.Sprintf(\"sha256:%x\", digest.Sum(nil)), nil\n}\n\ntype token struct {\n\titem         item\n\tvalue        any\n\tusedVariable bool\n\tsourceFile   string\n}\n\nfunc (t *token) MarshalJSON() ([]byte, error) {\n\treturn json.Marshal(t.value)\n}\n\nfunc (t *token) Value() any {\n\treturn t.value\n}\n\nfunc (t *token) Line() int {\n\treturn t.item.line\n}\n\nfunc (t *token) IsUsedVariable() bool {\n\treturn t.usedVariable\n}\n\nfunc (t *token) SourceFile() string {\n\treturn t.sourceFile\n}\n\nfunc (t *token) Position() int {\n\treturn t.item.pos\n}\n\nfunc parse(data, fp string, pedantic bool) (p *parser, err error) {\n\tp = &parser{\n\t\tmapping:  make(map[string]any),\n\t\tlx:       lex(data),\n\t\tctxs:     make([]any, 0, 4),\n\t\tkeys:     make([]string, 0, 4),\n\t\tikeys:    make([]item, 0, 4),\n\t\tfp:       filepath.Dir(fp),\n\t\tpedantic: pedantic,\n\t}\n\tp.pushContext(p.mapping)\n\n\tvar prevItem item\n\tfor {\n\t\tit := p.next()\n\t\tif it.typ == itemEOF {\n\t\t\t// Here we allow the final character to be a bracket '}'\n\t\t\t// in order to support JSON like configurations.\n\t\t\tif prevItem.typ == itemKey && prevItem.val != mapEndString {\n\t\t\t\treturn nil, fmt.Errorf(\"config is invalid (%s:%d:%d)\", fp, it.line, it.pos)\n\t\t\t}\n\t\t\tbreak\n\t\t}\n\t\tprevItem = it\n\t\tif err := p.processItem(it, fp); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\treturn p, nil\n}\n\nfunc (p *parser) next() item {\n\treturn p.lx.nextItem()\n}\n\nfunc (p *parser) pushContext(ctx any) {\n\tp.ctxs = append(p.ctxs, ctx)\n\tp.ctx = ctx\n}\n\nfunc (p *parser) popContext() any {\n\tif len(p.ctxs) == 0 {\n\t\tpanic(\"BUG in parser, context stack empty\")\n\t}\n\tli := len(p.ctxs) - 1\n\tlast := p.ctxs[li]\n\tp.ctxs = p.ctxs[0:li]\n\tp.ctx = p.ctxs[len(p.ctxs)-1]\n\treturn last\n}\n\nfunc (p *parser) pushKey(key string) {\n\tp.keys = append(p.keys, key)\n}\n\nfunc (p *parser) popKey() string {\n\tif len(p.keys) == 0 {\n\t\tpanic(\"BUG in parser, keys stack empty\")\n\t}\n\tli := len(p.keys) - 1\n\tlast := p.keys[li]\n\tp.keys = p.keys[0:li]\n\treturn last\n}\n\nfunc (p *parser) pushItemKey(key item) {\n\tp.ikeys = append(p.ikeys, key)\n}\n\nfunc (p *parser) popItemKey() item {\n\tif len(p.ikeys) == 0 {\n\t\tpanic(\"BUG in parser, item keys stack empty\")\n\t}\n\tli := len(p.ikeys) - 1\n\tlast := p.ikeys[li]\n\tp.ikeys = p.ikeys[0:li]\n\treturn last\n}\n\nfunc (p *parser) processItem(it item, fp string) error {\n\tsetValue := func(it item, v any) {\n\t\tif p.pedantic {\n\t\t\tp.setValue(&token{it, v, false, fp})\n\t\t} else {\n\t\t\tp.setValue(v)\n\t\t}\n\t}\n\n\tswitch it.typ {\n\tcase itemError:\n\t\treturn fmt.Errorf(\"Parse error on line %d: '%s'\", it.line, it.val)\n\tcase itemKey:\n\t\t// Keep track of the keys as items and strings,\n\t\t// we do this in order to be able to still support\n\t\t// includes without many breaking changes.\n\t\tp.pushKey(it.val)\n\n\t\tif p.pedantic {\n\t\t\tp.pushItemKey(it)\n\t\t}\n\tcase itemMapStart:\n\t\tnewCtx := make(map[string]any)\n\t\tp.pushContext(newCtx)\n\tcase itemMapEnd:\n\t\tsetValue(it, p.popContext())\n\tcase itemString:\n\t\t// FIXME(dlc) sanitize string?\n\t\tsetValue(it, it.val)\n\tcase itemInteger:\n\t\tlastDigit := 0\n\t\tfor _, r := range it.val {\n\t\t\tif !unicode.IsDigit(r) && r != '-' {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tlastDigit++\n\t\t}\n\t\tnumStr := it.val[:lastDigit]\n\t\tnum, err := strconv.ParseInt(numStr, 10, 64)\n\t\tif err != nil {\n\t\t\tif e, ok := err.(*strconv.NumError); ok &&\n\t\t\t\te.Err == strconv.ErrRange {\n\t\t\t\treturn fmt.Errorf(\"integer '%s' is out of the range\", it.val)\n\t\t\t}\n\t\t\treturn fmt.Errorf(\"expected integer, but got '%s'\", it.val)\n\t\t}\n\t\t// Process a suffix\n\t\tsuffix := strings.ToLower(strings.TrimSpace(it.val[lastDigit:]))\n\n\t\tswitch suffix {\n\t\tcase \"\":\n\t\t\tsetValue(it, num)\n\t\tcase \"k\":\n\t\t\tsetValue(it, num*1000)\n\t\tcase \"kb\", \"ki\", \"kib\":\n\t\t\tsetValue(it, num*1024)\n\t\tcase \"m\":\n\t\t\tsetValue(it, num*1000*1000)\n\t\tcase \"mb\", \"mi\", \"mib\":\n\t\t\tsetValue(it, num*1024*1024)\n\t\tcase \"g\":\n\t\t\tsetValue(it, num*1000*1000*1000)\n\t\tcase \"gb\", \"gi\", \"gib\":\n\t\t\tsetValue(it, num*1024*1024*1024)\n\t\tcase \"t\":\n\t\t\tsetValue(it, num*1000*1000*1000*1000)\n\t\tcase \"tb\", \"ti\", \"tib\":\n\t\t\tsetValue(it, num*1024*1024*1024*1024)\n\t\tcase \"p\":\n\t\t\tsetValue(it, num*1000*1000*1000*1000*1000)\n\t\tcase \"pb\", \"pi\", \"pib\":\n\t\t\tsetValue(it, num*1024*1024*1024*1024*1024)\n\t\tcase \"e\":\n\t\t\tsetValue(it, num*1000*1000*1000*1000*1000*1000)\n\t\tcase \"eb\", \"ei\", \"eib\":\n\t\t\tsetValue(it, num*1024*1024*1024*1024*1024*1024)\n\t\t}\n\tcase itemFloat:\n\t\tnum, err := strconv.ParseFloat(it.val, 64)\n\t\tif err != nil {\n\t\t\tif e, ok := err.(*strconv.NumError); ok &&\n\t\t\t\te.Err == strconv.ErrRange {\n\t\t\t\treturn fmt.Errorf(\"float '%s' is out of the range\", it.val)\n\t\t\t}\n\t\t\treturn fmt.Errorf(\"expected float, but got '%s'\", it.val)\n\t\t}\n\t\tsetValue(it, num)\n\tcase itemBool:\n\t\tswitch strings.ToLower(it.val) {\n\t\tcase \"true\", \"yes\", \"on\":\n\t\t\tsetValue(it, true)\n\t\tcase \"false\", \"no\", \"off\":\n\t\t\tsetValue(it, false)\n\t\tdefault:\n\t\t\treturn fmt.Errorf(\"expected boolean value, but got '%s'\", it.val)\n\t\t}\n\n\tcase itemDatetime:\n\t\tdt, err := time.Parse(\"2006-01-02T15:04:05Z\", it.val)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\n\t\t\t\t\"expected Zulu formatted DateTime, but got '%s'\", it.val)\n\t\t}\n\t\tsetValue(it, dt)\n\tcase itemArrayStart:\n\t\tvar array = make([]any, 0)\n\t\tp.pushContext(array)\n\tcase itemArrayEnd:\n\t\tarray := p.ctx\n\t\tp.popContext()\n\t\tsetValue(it, array)\n\tcase itemVariable:\n\t\tvalue, found, err := p.lookupVariable(it.val)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"variable reference for '%s' on line %d could not be parsed: %s\",\n\t\t\t\tit.val, it.line, err)\n\t\t}\n\t\tif !found {\n\t\t\treturn fmt.Errorf(\"variable reference for '%s' on line %d can not be found\",\n\t\t\t\tit.val, it.line)\n\t\t}\n\n\t\tif p.pedantic {\n\t\t\tswitch tk := value.(type) {\n\t\t\tcase *token:\n\t\t\t\t// Mark the looked up variable as used, and make\n\t\t\t\t// the variable reference become handled as a token.\n\t\t\t\ttk.usedVariable = true\n\t\t\t\tp.setValue(&token{it, tk.Value(), false, fp})\n\t\t\tdefault:\n\t\t\t\t// Special case to add position context to bcrypt references.\n\t\t\t\tp.setValue(&token{it, value, false, fp})\n\t\t\t}\n\t\t} else {\n\t\t\tp.setValue(value)\n\t\t}\n\tcase itemInclude:\n\t\tvar (\n\t\t\tm   map[string]any\n\t\t\terr error\n\t\t)\n\t\tif p.pedantic {\n\t\t\tm, err = ParseFileWithChecks(filepath.Join(p.fp, it.val))\n\t\t} else {\n\t\t\tm, err = ParseFile(filepath.Join(p.fp, it.val))\n\t\t}\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"error parsing include file '%s', %v\", it.val, err)\n\t\t}\n\t\tfor k, v := range m {\n\t\t\tp.pushKey(k)\n\n\t\t\tif p.pedantic {\n\t\t\t\tswitch tk := v.(type) {\n\t\t\t\tcase *token:\n\t\t\t\t\tp.pushItemKey(tk.item)\n\t\t\t\t}\n\t\t\t}\n\t\t\tp.setValue(v)\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// Used to map an environment value into a temporary map to pass to secondary Parse call.\nconst pkey = \"pk\"\n\n// We special case raw strings here that are bcrypt'd. This allows us not to force quoting the strings\nconst bcryptPrefix = \"2a$\"\n\n// lookupVariable will lookup a variable reference. It will use block scoping on keys\n// it has seen before, with the top level scoping being the environment variables. We\n// ignore array contexts and only process the map contexts..\n//\n// Returns true for ok if it finds something, similar to map.\nfunc (p *parser) lookupVariable(varReference string) (any, bool, error) {\n\t// Do special check to see if it is a raw bcrypt string.\n\tif strings.HasPrefix(varReference, bcryptPrefix) {\n\t\treturn \"$\" + varReference, true, nil\n\t}\n\n\t// Loop through contexts currently on the stack.\n\tfor i := len(p.ctxs) - 1; i >= 0; i-- {\n\t\tctx := p.ctxs[i]\n\t\t// Process if it is a map context\n\t\tif m, ok := ctx.(map[string]any); ok {\n\t\t\tif v, ok := m[varReference]; ok {\n\t\t\t\treturn v, ok, nil\n\t\t\t}\n\t\t}\n\t}\n\n\t// If we are here, we have exhausted our context maps and still not found anything.\n\t// Parse from the environment.\n\tif vStr, ok := os.LookupEnv(varReference); ok {\n\t\t// Everything we get here will be a string value, so we need to process as a parser would.\n\t\tif vmap, err := Parse(fmt.Sprintf(\"%s=%s\", pkey, vStr)); err == nil {\n\t\t\tv, ok := vmap[pkey]\n\t\t\treturn v, ok, nil\n\t\t} else {\n\t\t\treturn nil, false, err\n\t\t}\n\t}\n\treturn nil, false, nil\n}\n\nfunc (p *parser) setValue(val any) {\n\t// Test to see if we are on an array or a map\n\n\t// Array processing\n\tif ctx, ok := p.ctx.([]any); ok {\n\t\tp.ctx = append(ctx, val)\n\t\tp.ctxs[len(p.ctxs)-1] = p.ctx\n\t}\n\n\t// Map processing\n\tif ctx, ok := p.ctx.(map[string]any); ok {\n\t\tkey := p.popKey()\n\n\t\tif p.pedantic {\n\t\t\t// Change the position to the beginning of the key\n\t\t\t// since more useful when reporting errors.\n\t\t\tswitch v := val.(type) {\n\t\t\tcase *token:\n\t\t\t\tit := p.popItemKey()\n\t\t\t\tv.item.pos = it.pos\n\t\t\t\tv.item.line = it.line\n\t\t\t\tctx[key] = v\n\t\t\t}\n\t\t} else {\n\t\t\t// FIXME(dlc), make sure to error if redefining same key?\n\t\t\tctx[key] = val\n\t\t}\n\t}\n}\n",
    "source_file": "conf/parse.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2024 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n//go:build !windows\n\npackage tpm\n\nimport \"fmt\"\n\n// LoadJetStreamEncryptionKeyFromTPM here is a stub for unsupported platforms.\nfunc LoadJetStreamEncryptionKeyFromTPM(srkPassword, jsKeyFile, jsKeyPassword string, pcr int) (string, error) {\n\treturn \"\", fmt.Errorf(\"TPM functionality is not supported on this platform\")\n}\n",
    "source_file": "server/tpm/js_ek_tpm_other.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2024 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n//go:build windows\n\npackage tpm\n\nimport (\n\t\"encoding/base64\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"io\"\n\t\"os\"\n\t\"path/filepath\"\n\n\t\"github.com/google/go-tpm/legacy/tpm2\"\n\t\"github.com/google/go-tpm/tpmutil\"\n\t\"github.com/nats-io/nkeys\"\n)\n\nvar (\n\t// Version of the NATS TPM JS implmentation\n\tJsKeyTPMVersion = 1\n)\n\n// How this works:\n// Create a Storage Root Key (SRK) in the TPM.\n// If existing JS Encryption keys do not exist on disk.\n// \t  - Create a JetStream encryption key (js key) and seal it to the SRK\n//      using a provided js encryption key password.\n// \t  - Save the public and private blobs to a file on disk.\n//    - Return the new js encryption key (the private portion of the nkey)\n// Otherwise (keys exist on disk)\n//    - Read the public and private blobs from disk\n//    - Load them into the TPM\n//    - Unseal the js key using the TPM, and the provided js encryption keys password.\n//\n// Note: a SRK password for the SRK is supported but not tested here.\n\n// Gets/Regenerates the Storage Root Key (SRK) from the TPM. Caller MUST flush this handle when done.\nfunc regenerateSRK(rwc io.ReadWriteCloser, srkPassword string) (tpmutil.Handle, error) {\n\t// Default EK template defined in:\n\t// https://trustedcomputinggroup.org/wp-content/uploads/Credential_Profile_EK_V2.0_R14_published.pdf\n\t// Shared SRK template based off of EK template and specified in:\n\t// https://trustedcomputinggroup.org/wp-content/uploads/TCG-TPM-v2.0-Provisioning-Guidance-Published-v1r1.pdf\n\tsrkTemplate := tpm2.Public{\n\t\tType:       tpm2.AlgRSA,\n\t\tNameAlg:    tpm2.AlgSHA256,\n\t\tAttributes: tpm2.FlagFixedTPM | tpm2.FlagFixedParent | tpm2.FlagSensitiveDataOrigin | tpm2.FlagUserWithAuth | tpm2.FlagRestricted | tpm2.FlagDecrypt | tpm2.FlagNoDA,\n\t\tAuthPolicy: nil,\n\t\t// We must use RSA 2048 for the intel TSS2 stack\n\t\tRSAParameters: &tpm2.RSAParams{\n\t\t\tSymmetric: &tpm2.SymScheme{\n\t\t\t\tAlg:     tpm2.AlgAES,\n\t\t\t\tKeyBits: 128,\n\t\t\t\tMode:    tpm2.AlgCFB,\n\t\t\t},\n\t\t\tKeyBits:    2048,\n\t\t\tModulusRaw: make([]byte, 256),\n\t\t},\n\t}\n\t// Create the parent key against which to seal the data\n\tsrkHandle, _, err := tpm2.CreatePrimary(rwc, tpm2.HandleOwner, tpm2.PCRSelection{}, \"\", srkPassword, srkTemplate)\n\treturn srkHandle, err\n}\n\ntype natsTPMPersistedKeys struct {\n\tVersion    int    `json:\"version\"`\n\tPrivateKey []byte `json:\"private_key\"`\n\tPublicKey  []byte `json:\"public_key\"`\n}\n\n// Writes the private and public blobs to disk in a single file. If the directory does\n// not exist, it will be created. If the file already exists it will be overwritten.\nfunc writeTPMKeysToFile(filename string, privateBlob []byte, publicBlob []byte) error {\n\tkeyDir := filepath.Dir(filename)\n\tif err := os.MkdirAll(keyDir, 0750); err != nil {\n\t\treturn fmt.Errorf(\"unable to create/access directory %q: %v\", keyDir, err)\n\t}\n\n\t// Create a new set of persisted keys. Note that the private key doesn't necessarily\n\t// need to be protected as the TPM password is required to use unseal, although it's\n\t// a good idea to put this in a secure location accessible to the server.\n\ttpmKeys := natsTPMPersistedKeys{\n\t\tVersion:    JsKeyTPMVersion,\n\t\tPrivateKey: make([]byte, base64.StdEncoding.EncodedLen(len(privateBlob))),\n\t\tPublicKey:  make([]byte, base64.StdEncoding.EncodedLen(len(publicBlob))),\n\t}\n\tbase64.StdEncoding.Encode(tpmKeys.PrivateKey, privateBlob)\n\tbase64.StdEncoding.Encode(tpmKeys.PublicKey, publicBlob)\n\t// Convert to JSON\n\tkeysJSON, err := json.Marshal(tpmKeys)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"unable to marshal keys to JSON: %v\", err)\n\t}\n\t// Write the JSON to a file\n\tif err := os.WriteFile(filename, keysJSON, 0640); err != nil {\n\t\treturn fmt.Errorf(\"unable to write keys file to %q: %v\", filename, err)\n\t}\n\treturn nil\n}\n\n// Reads the private and public blobs from a single file. If the file does not exist,\n// or the file cannot be read and the keys decoded, an error is returned.\nfunc readTPMKeysFromFile(filename string) ([]byte, []byte, error) {\n\tkeysJSON, err := os.ReadFile(filename)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\n\tvar tpmKeys natsTPMPersistedKeys\n\tif err := json.Unmarshal(keysJSON, &tpmKeys); err != nil {\n\t\treturn nil, nil, fmt.Errorf(\"unable to unmarshal TPM file keys JSON from %s: %v\", filename, err)\n\t}\n\n\t// Placeholder for future-proofing. Here is where we would\n\t// check the current version against tpmKeys.Version and\n\t// handle any changes.\n\n\t// Base64 decode the private and public blobs.\n\tprivateBlob := make([]byte, base64.StdEncoding.DecodedLen(len(tpmKeys.PrivateKey)))\n\tpublicBlob := make([]byte, base64.StdEncoding.DecodedLen(len(tpmKeys.PublicKey)))\n\tprn, err := base64.StdEncoding.Decode(privateBlob, tpmKeys.PrivateKey)\n\tif err != nil {\n\t\treturn nil, nil, fmt.Errorf(\"unable to decode privateBlob from base64: %v\", err)\n\t}\n\tpun, err := base64.StdEncoding.Decode(publicBlob, tpmKeys.PublicKey)\n\tif err != nil {\n\t\treturn nil, nil, fmt.Errorf(\"unable to decode publicBlob from base64: %v\", err)\n\t}\n\treturn publicBlob[:pun], privateBlob[:prn], nil\n}\n\n// Creates a new JetStream encryption key, seals it to the TPM, and saves the public and\n// private blobs to disk in a JSON encoded file. The key is returned as a string.\nfunc createAndSealJsEncryptionKey(rwc io.ReadWriteCloser, srkHandle tpmutil.Handle, srkPassword, jsKeyFile, jsKeyPassword string, pcr int) (string, error) {\n\t// Get the authorization policy that will protect the data to be sealed\n\tsessHandle, policy, err := policyPCRPasswordSession(rwc, pcr)\n\tif err != nil {\n\t\treturn \"\", fmt.Errorf(\"unable to get policy: %v\", err)\n\t}\n\tif err := tpm2.FlushContext(rwc, sessHandle); err != nil {\n\t\treturn \"\", fmt.Errorf(\"unable to flush session: %v\", err)\n\t}\n\t// Seal the data to the parent key and the policy\n\tuser, err := nkeys.CreateUser()\n\tif err != nil {\n\t\treturn \"\", fmt.Errorf(\"unable to create seed: %v\", err)\n\t}\n\t// We'll use the seed to represent the encryption key.\n\tjsStoreKey, err := user.Seed()\n\tif err != nil {\n\t\treturn \"\", fmt.Errorf(\"unable to get seed: %v\", err)\n\t}\n\tprivateArea, publicArea, err := tpm2.Seal(rwc, srkHandle, srkPassword, jsKeyPassword, policy, jsStoreKey)\n\tif err != nil {\n\t\treturn \"\", fmt.Errorf(\"unable to seal data: %v\", err)\n\t}\n\terr = writeTPMKeysToFile(jsKeyFile, privateArea, publicArea)\n\tif err != nil {\n\t\treturn \"\", fmt.Errorf(\"unable to write key file: %v\", err)\n\t}\n\treturn string(jsStoreKey), nil\n}\n\n// Unseals the JetStream encryption key from the TPM with the provided keys.\n// The key is returned as a string.\nfunc unsealJsEncrpytionKey(rwc io.ReadWriteCloser, pcr int, srkHandle tpmutil.Handle, srkPassword, objectPassword string, publicBlob, privateBlob []byte) (string, error) {\n\t// Load the public/private blobs into the TPM for decryption.\n\tobjectHandle, _, err := tpm2.Load(rwc, srkHandle, srkPassword, publicBlob, privateBlob)\n\tif err != nil {\n\t\treturn \"\", fmt.Errorf(\"unable to load data: %v\", err)\n\t}\n\tdefer tpm2.FlushContext(rwc, objectHandle)\n\n\t// Create the authorization session with TPM.\n\tsessHandle, _, err := policyPCRPasswordSession(rwc, pcr)\n\tif err != nil {\n\t\treturn \"\", fmt.Errorf(\"unable to get auth session: %v\", err)\n\t}\n\tdefer func() {\n\t\ttpm2.FlushContext(rwc, sessHandle)\n\t}()\n\t// Unseal the data we've loaded into the TPM with the object (js key) password.\n\tunsealedData, err := tpm2.UnsealWithSession(rwc, sessHandle, objectHandle, objectPassword)\n\tif err != nil {\n\t\treturn \"\", fmt.Errorf(\"unable to unseal data: %v\", err)\n\t}\n\treturn string(unsealedData), nil\n}\n\n// Returns session handle and policy digest.\nfunc policyPCRPasswordSession(rwc io.ReadWriteCloser, pcr int) (sessHandle tpmutil.Handle, policy []byte, retErr error) {\n\tsessHandle, _, err := tpm2.StartAuthSession(\n\t\trwc,\n\t\ttpm2.HandleNull,  /*tpmKey*/\n\t\ttpm2.HandleNull,  /*bindKey*/\n\t\tmake([]byte, 16), /*nonceCaller*/\n\t\tnil,              /*secret*/\n\t\ttpm2.SessionPolicy,\n\t\ttpm2.AlgNull,\n\t\ttpm2.AlgSHA256)\n\tif err != nil {\n\t\treturn tpm2.HandleNull, nil, fmt.Errorf(\"unable to start session: %v\", err)\n\t}\n\tdefer func() {\n\t\tif sessHandle != tpm2.HandleNull && err != nil {\n\t\t\tif err := tpm2.FlushContext(rwc, sessHandle); err != nil {\n\t\t\t\tretErr = fmt.Errorf(\"%v\\nunable to flush session: %v\", retErr, err)\n\t\t\t}\n\t\t}\n\t}()\n\n\tpcrSelection := tpm2.PCRSelection{\n\t\tHash: tpm2.AlgSHA256,\n\t\tPCRs: []int{pcr},\n\t}\n\tif err := tpm2.PolicyPCR(rwc, sessHandle, nil, pcrSelection); err != nil {\n\t\treturn sessHandle, nil, fmt.Errorf(\"unable to bind PCRs to auth policy: %v\", err)\n\t}\n\tif err := tpm2.PolicyPassword(rwc, sessHandle); err != nil {\n\t\treturn sessHandle, nil, fmt.Errorf(\"unable to require password for auth policy: %v\", err)\n\t}\n\tpolicy, err = tpm2.PolicyGetDigest(rwc, sessHandle)\n\tif err != nil {\n\t\treturn sessHandle, nil, fmt.Errorf(\"unable to get policy digest: %v\", err)\n\t}\n\treturn sessHandle, policy, nil\n}\n\n// LoadJetStreamEncryptionKeyFromTPM loads the JetStream encryption key from the TPM.\n// If the keyfile does not exist, a key will be created and sealed. Public and private blobs\n// used to decrypt the key in future sessions will be saved to disk in the file provided.\n// The key will be unsealed and returned only with the correct password and PCR value.\nfunc LoadJetStreamEncryptionKeyFromTPM(srkPassword, jsKeyFile, jsKeyPassword string, pcr int) (string, error) {\n\trwc, err := tpm2.OpenTPM()\n\tif err != nil {\n\t\treturn \"\", fmt.Errorf(\"could not open the TPM: %v\", err)\n\t}\n\tdefer rwc.Close()\n\n\t// Load the key from the TPM\n\tsrkHandle, err := regenerateSRK(rwc, srkPassword)\n\tdefer func() {\n\t\ttpm2.FlushContext(rwc, srkHandle)\n\t}()\n\tif err != nil {\n\t\treturn \"\", fmt.Errorf(\"unable to regenerate SRK from the TPM: %v\", err)\n\t}\n\t// Read the keys from the key file. If the filed doesn't exist it means we need to create\n\t// a new js encrytpion key.\n\tpublicBlob, privateBlob, err := readTPMKeysFromFile(jsKeyFile)\n\tif err != nil {\n\t\tif os.IsNotExist(err) {\n\t\t\tjsek, err := createAndSealJsEncryptionKey(rwc, srkHandle, srkPassword, jsKeyFile, jsKeyPassword, pcr)\n\t\t\tif err != nil {\n\t\t\t\treturn \"\", fmt.Errorf(\"unable to generate new key from the TPM: %v\", err)\n\t\t\t}\n\t\t\t// we've created and sealed the JS Encryption key, now we just return it.\n\t\t\treturn jsek, nil\n\t\t}\n\t\treturn \"\", fmt.Errorf(\"unable to load key from TPM: %v\", err)\n\t}\n\n\t// Unseal the JetStream encryption key using the TPM.\n\tjsek, err := unsealJsEncrpytionKey(rwc, pcr, srkHandle, srkPassword, jsKeyPassword, publicBlob, privateBlob)\n\tif err != nil {\n\t\treturn \"\", fmt.Errorf(\"unable to unseal key from the TPM: %v\", err)\n\t}\n\treturn jsek, nil\n}\n",
    "source_file": "server/tpm/js_ek_tpm_windows.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2022-2025 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage certstore\n\nimport (\n\t\"crypto\"\n\t\"crypto/x509\"\n\t\"io\"\n\t\"runtime\"\n\t\"strings\"\n)\n\ntype StoreType int\n\nconst MATCHBYEMPTY = 0\nconst STOREEMPTY = 0\n\nconst (\n\twindowsCurrentUser StoreType = iota + 1\n\twindowsLocalMachine\n)\n\nvar StoreMap = map[string]StoreType{\n\t\"windowscurrentuser\":  windowsCurrentUser,\n\t\"windowslocalmachine\": windowsLocalMachine,\n}\n\nvar StoreOSMap = map[StoreType]string{\n\twindowsCurrentUser:  \"windows\",\n\twindowsLocalMachine: \"windows\",\n}\n\ntype MatchByType int\n\nconst (\n\tmatchByIssuer MatchByType = iota + 1\n\tmatchBySubject\n\tmatchByThumbprint\n)\n\nvar MatchByMap = map[string]MatchByType{\n\t\"issuer\":     matchByIssuer,\n\t\"subject\":    matchBySubject,\n\t\"thumbprint\": matchByThumbprint,\n}\n\nvar Usage = `\nIn place of cert_file and key_file you may use the windows certificate store:\n\n    tls {\n        cert_store:     \"WindowsCurrentUser\"\n        cert_match_by:  \"Subject\"\n        cert_match:     \"MyServer123\"\n    }\n`\n\nfunc ParseCertStore(certStore string) (StoreType, error) {\n\tcertStoreType, exists := StoreMap[strings.ToLower(certStore)]\n\tif !exists {\n\t\treturn 0, ErrBadCertStore\n\t}\n\tvalidOS, exists := StoreOSMap[certStoreType]\n\tif !exists || validOS != runtime.GOOS {\n\t\treturn 0, ErrOSNotCompatCertStore\n\t}\n\treturn certStoreType, nil\n}\n\nfunc ParseCertMatchBy(certMatchBy string) (MatchByType, error) {\n\tcertMatchByType, exists := MatchByMap[strings.ToLower(certMatchBy)]\n\tif !exists {\n\t\treturn 0, ErrBadMatchByType\n\t}\n\treturn certMatchByType, nil\n}\n\nfunc GetLeafIssuer(leaf *x509.Certificate, vOpts x509.VerifyOptions) (issuer *x509.Certificate) {\n\tchains, err := leaf.Verify(vOpts)\n\tif err != nil || len(chains) == 0 {\n\t\tissuer = nil\n\t} else {\n\t\tissuer = chains[0][1]\n\t}\n\treturn\n}\n\n// credential provides access to a public key and is a crypto.Signer.\ntype credential interface {\n\t// Public returns the public key corresponding to the leaf certificate.\n\tPublic() crypto.PublicKey\n\t// Sign signs digest with the private key.\n\tSign(rand io.Reader, digest []byte, opts crypto.SignerOpts) (signature []byte, err error)\n}\n",
    "source_file": "server/certstore/certstore.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2022-2025 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n//go:build !windows\n\npackage certstore\n\nimport (\n\t\"crypto\"\n\t\"crypto/tls\"\n\t\"io\"\n)\n\nvar _ = MATCHBYEMPTY\n\n// otherKey implements crypto.Signer and crypto.Decrypter to satisfy linter on platforms that don't implement certstore\ntype otherKey struct{}\n\nfunc TLSConfig(_ StoreType, _ MatchByType, _ string, _ []string, _ bool, _ *tls.Config) error {\n\treturn ErrOSNotCompatCertStore\n}\n\n// Public always returns nil public key since this is a stub on non-supported platform\nfunc (k otherKey) Public() crypto.PublicKey {\n\treturn nil\n}\n\n// Sign always returns a nil signature since this is a stub on non-supported platform\nfunc (k otherKey) Sign(rand io.Reader, digest []byte, opts crypto.SignerOpts) (signature []byte, err error) {\n\t_, _, _ = rand, digest, opts\n\treturn nil, nil\n}\n\n// Verify interface conformance.\nvar _ credential = &otherKey{}\n",
    "source_file": "server/certstore/certstore_other.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2022-2025 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n//\n// Adapted, updated, and enhanced from CertToStore, https://github.com/google/certtostore/releases/tag/v1.0.2\n// Apache License, Version 2.0, Copyright 2017 Google Inc.\n\npackage certstore\n\nimport (\n\t\"bytes\"\n\t\"crypto\"\n\t\"crypto/ecdsa\"\n\t\"crypto/elliptic\"\n\t\"crypto/rsa\"\n\t\"crypto/tls\"\n\t\"crypto/x509\"\n\t\"encoding/binary\"\n\t\"fmt\"\n\t\"io\"\n\t\"math/big\"\n\t\"reflect\"\n\t\"sync\"\n\t\"syscall\"\n\t\"unicode/utf16\"\n\t\"unsafe\"\n\n\t\"golang.org/x/crypto/cryptobyte\"\n\t\"golang.org/x/crypto/cryptobyte/asn1\"\n\t\"golang.org/x/sys/windows\"\n)\n\nconst (\n\t// wincrypt.h constants\n\twinAcquireCached         = windows.CRYPT_ACQUIRE_CACHE_FLAG\n\twinAcquireSilent         = windows.CRYPT_ACQUIRE_SILENT_FLAG\n\twinAcquireOnlyNCryptKey  = windows.CRYPT_ACQUIRE_ONLY_NCRYPT_KEY_FLAG\n\twinEncodingX509ASN       = windows.X509_ASN_ENCODING\n\twinEncodingPKCS7         = windows.PKCS_7_ASN_ENCODING\n\twinCertStoreProvSystem   = windows.CERT_STORE_PROV_SYSTEM\n\twinCertStoreCurrentUser  = windows.CERT_SYSTEM_STORE_CURRENT_USER\n\twinCertStoreLocalMachine = windows.CERT_SYSTEM_STORE_LOCAL_MACHINE\n\twinCertStoreReadOnly     = windows.CERT_STORE_READONLY_FLAG\n\twinInfoIssuerFlag        = windows.CERT_INFO_ISSUER_FLAG\n\twinInfoSubjectFlag       = windows.CERT_INFO_SUBJECT_FLAG\n\twinCompareNameStrW       = windows.CERT_COMPARE_NAME_STR_W\n\twinCompareShift          = windows.CERT_COMPARE_SHIFT\n\n\t// Reference https://learn.microsoft.com/en-us/windows/win32/api/wincrypt/nf-wincrypt-certfindcertificateinstore\n\twinFindIssuerStr  = windows.CERT_FIND_ISSUER_STR_W\n\twinFindSubjectStr = windows.CERT_FIND_SUBJECT_STR_W\n\twinFindHashStr    = windows.CERT_FIND_HASH_STR\n\n\twinNcryptKeySpec = windows.CERT_NCRYPT_KEY_SPEC\n\n\twinBCryptPadPKCS1   uintptr = 0x2\n\twinBCryptPadPSS     uintptr = 0x8 // Modern TLS 1.2+\n\twinBCryptPadPSSSalt uint32  = 32  // default 20, 32 optimal for typical SHA256 hash\n\n\twinRSA1Magic = 0x31415352 // \"RSA1\" BCRYPT_RSAPUBLIC_MAGIC\n\n\twinECS1Magic = 0x31534345 // \"ECS1\" BCRYPT_ECDSA_PUBLIC_P256_MAGIC\n\twinECS3Magic = 0x33534345 // \"ECS3\" BCRYPT_ECDSA_PUBLIC_P384_MAGIC\n\twinECS5Magic = 0x35534345 // \"ECS5\" BCRYPT_ECDSA_PUBLIC_P521_MAGIC\n\n\twinECK1Magic = 0x314B4345 // \"ECK1\" BCRYPT_ECDH_PUBLIC_P256_MAGIC\n\twinECK3Magic = 0x334B4345 // \"ECK3\" BCRYPT_ECDH_PUBLIC_P384_MAGIC\n\twinECK5Magic = 0x354B4345 // \"ECK5\" BCRYPT_ECDH_PUBLIC_P521_MAGIC\n\n\twinCryptENotFound = windows.CRYPT_E_NOT_FOUND\n\n\tproviderMSSoftware = \"Microsoft Software Key Storage Provider\"\n)\n\nvar (\n\twinBCryptRSAPublicBlob = winWide(\"RSAPUBLICBLOB\")\n\twinBCryptECCPublicBlob = winWide(\"ECCPUBLICBLOB\")\n\n\twinNCryptAlgorithmGroupProperty = winWide(\"Algorithm Group\") // NCRYPT_ALGORITHM_GROUP_PROPERTY\n\twinNCryptUniqueNameProperty     = winWide(\"Unique Name\")     // NCRYPT_UNIQUE_NAME_PROPERTY\n\twinNCryptECCCurveNameProperty   = winWide(\"ECCCurveName\")    // NCRYPT_ECC_CURVE_NAME_PROPERTY\n\n\twinCurveIDs = map[uint32]elliptic.Curve{\n\t\twinECS1Magic: elliptic.P256(), // BCRYPT_ECDSA_PUBLIC_P256_MAGIC\n\t\twinECS3Magic: elliptic.P384(), // BCRYPT_ECDSA_PUBLIC_P384_MAGIC\n\t\twinECS5Magic: elliptic.P521(), // BCRYPT_ECDSA_PUBLIC_P521_MAGIC\n\t\twinECK1Magic: elliptic.P256(), // BCRYPT_ECDH_PUBLIC_P256_MAGIC\n\t\twinECK3Magic: elliptic.P384(), // BCRYPT_ECDH_PUBLIC_P384_MAGIC\n\t\twinECK5Magic: elliptic.P521(), // BCRYPT_ECDH_PUBLIC_P521_MAGIC\n\t}\n\n\twinCurveNames = map[string]elliptic.Curve{\n\t\t\"nistP256\": elliptic.P256(), // BCRYPT_ECC_CURVE_NISTP256\n\t\t\"nistP384\": elliptic.P384(), // BCRYPT_ECC_CURVE_NISTP384\n\t\t\"nistP521\": elliptic.P521(), // BCRYPT_ECC_CURVE_NISTP521\n\t}\n\n\twinAlgIDs = map[crypto.Hash]*uint16{\n\t\tcrypto.SHA1:   winWide(\"SHA1\"),   // BCRYPT_SHA1_ALGORITHM\n\t\tcrypto.SHA256: winWide(\"SHA256\"), // BCRYPT_SHA256_ALGORITHM\n\t\tcrypto.SHA384: winWide(\"SHA384\"), // BCRYPT_SHA384_ALGORITHM\n\t\tcrypto.SHA512: winWide(\"SHA512\"), // BCRYPT_SHA512_ALGORITHM\n\t}\n\n\t// MY is well-known system store on Windows that holds personal certificates. Read\n\t// More about the CA locations here:\n\t// https://learn.microsoft.com/en-us/dotnet/framework/configure-apps/file-schema/wcf/certificate-of-clientcertificate-element?redirectedfrom=MSDN\n\t// https://superuser.com/questions/217719/what-are-the-windows-system-certificate-stores\n\t// https://docs.microsoft.com/en-us/windows/win32/seccrypto/certificate-stores\n\t// https://learn.microsoft.com/en-us/windows/win32/seccrypto/system-store-locations\n\t// https://stackoverflow.com/questions/63286085/which-x509-storename-refers-to-the-certificates-stored-beneath-trusted-root-cert#:~:text=4-,StoreName.,is%20%22Intermediate%20Certification%20Authorities%22.\n\twinMyStore             = winWide(\"MY\")\n\twinIntermediateCAStore = winWide(\"CA\")\n\twinRootStore           = winWide(\"Root\")\n\twinAuthRootStore       = winWide(\"AuthRoot\")\n\n\t// These DLLs must be available on all Windows hosts\n\twinCrypt32 = windows.NewLazySystemDLL(\"crypt32.dll\")\n\twinNCrypt  = windows.NewLazySystemDLL(\"ncrypt.dll\")\n\n\twinCertFindCertificateInStore        = winCrypt32.NewProc(\"CertFindCertificateInStore\")\n\twinCertVerifyTimeValidity            = winCrypt32.NewProc(\"CertVerifyTimeValidity\")\n\twinCryptAcquireCertificatePrivateKey = winCrypt32.NewProc(\"CryptAcquireCertificatePrivateKey\")\n\twinNCryptExportKey                   = winNCrypt.NewProc(\"NCryptExportKey\")\n\twinNCryptOpenStorageProvider         = winNCrypt.NewProc(\"NCryptOpenStorageProvider\")\n\twinNCryptGetProperty                 = winNCrypt.NewProc(\"NCryptGetProperty\")\n\twinNCryptSignHash                    = winNCrypt.NewProc(\"NCryptSignHash\")\n\n\twinFnGetProperty = winGetProperty\n)\n\nfunc init() {\n\tfor _, d := range []*windows.LazyDLL{\n\t\twinCrypt32, winNCrypt,\n\t} {\n\t\tif err := d.Load(); err != nil {\n\t\t\tpanic(err)\n\t\t}\n\t}\n\tfor _, p := range []*windows.LazyProc{\n\t\twinCertFindCertificateInStore, winCryptAcquireCertificatePrivateKey,\n\t\twinNCryptExportKey, winNCryptOpenStorageProvider,\n\t\twinNCryptGetProperty, winNCryptSignHash,\n\t} {\n\t\tif err := p.Find(); err != nil {\n\t\t\tpanic(err)\n\t\t}\n\t}\n}\n\ntype winPKCS1PaddingInfo struct {\n\tpszAlgID *uint16\n}\n\ntype winPSSPaddingInfo struct {\n\tpszAlgID *uint16\n\tcbSalt   uint32\n}\n\n// createCACertsPool generates a CertPool from the Windows certificate store,\n// adding all matching certificates from the caCertsMatch array to the pool.\n// All matching certificates (vs first) are added to the pool based on a user\n// request. If no certificates are found an error is returned.\nfunc createCACertsPool(cs *winCertStore, storeType uint32, caCertsMatch []string, skipInvalid bool) (*x509.CertPool, error) {\n\tvar errs []error\n\tcaPool := x509.NewCertPool()\n\tfor _, s := range caCertsMatch {\n\t\tlfs, err := cs.caCertsBySubjectMatch(s, storeType, skipInvalid)\n\t\tif err != nil {\n\t\t\terrs = append(errs, err)\n\t\t} else {\n\t\t\tfor _, lf := range lfs {\n\t\t\t\tcaPool.AddCert(lf)\n\t\t\t}\n\t\t}\n\t}\n\t// If every lookup failed return the errors.\n\tif len(errs) == len(caCertsMatch) {\n\t\treturn nil, fmt.Errorf(\"unable to match any CA certificate: %v\", errs)\n\t}\n\treturn caPool, nil\n}\n\n// TLSConfig fulfills the same function as reading cert and key pair from\n// pem files but sources the Windows certificate store instead. The\n// certMatchBy and certMatch fields search the \"MY\" certificate location\n// for the first certificate that matches the certMatch field. The\n// caCertsMatch field is used to search the Trusted Root, Third Party Root,\n// and Intermediate Certificate Authority locations for certificates with\n// Subjects matching the provided strings. If a match is found, the\n// certificate is added to the pool that is used to verify the certificate\n// chain.\nfunc TLSConfig(certStore StoreType, certMatchBy MatchByType, certMatch string, caCertsMatch []string, skipInvalid bool, config *tls.Config) error {\n\tvar (\n\t\tleaf     *x509.Certificate\n\t\tleafCtx  *windows.CertContext\n\t\tpk       *winKey\n\t\tvOpts    = x509.VerifyOptions{}\n\t\tchains   [][]*x509.Certificate\n\t\tchain    []*x509.Certificate\n\t\trawChain [][]byte\n\t)\n\n\t// By StoreType, open a store\n\tif certStore == windowsCurrentUser || certStore == windowsLocalMachine {\n\t\tvar scope uint32\n\t\tcs, err := winOpenCertStore(providerMSSoftware)\n\t\tif err != nil || cs == nil {\n\t\t\treturn err\n\t\t}\n\t\tif certStore == windowsCurrentUser {\n\t\t\tscope = winCertStoreCurrentUser\n\t\t}\n\t\tif certStore == windowsLocalMachine {\n\t\t\tscope = winCertStoreLocalMachine\n\t\t}\n\n\t\t// certByIssuer or certBySubject\n\t\tif certMatchBy == matchBySubject || certMatchBy == MATCHBYEMPTY {\n\t\t\tleaf, leafCtx, err = cs.certBySubject(certMatch, scope, skipInvalid)\n\t\t} else if certMatchBy == matchByIssuer {\n\t\t\tleaf, leafCtx, err = cs.certByIssuer(certMatch, scope, skipInvalid)\n\t\t} else if certMatchBy == matchByThumbprint {\n\t\t\tleaf, leafCtx, err = cs.certByThumbprint(certMatch, scope, skipInvalid)\n\t\t} else {\n\t\t\treturn ErrBadMatchByType\n\t\t}\n\t\tif err != nil {\n\t\t\t// pass through error from cert search\n\t\t\treturn err\n\t\t}\n\t\tif leaf == nil || leafCtx == nil {\n\t\t\treturn ErrFailedCertSearch\n\t\t}\n\t\tpk, err = cs.certKey(leafCtx)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif pk == nil {\n\t\t\treturn ErrNoPrivateKeyStoreRef\n\t\t}\n\t\t// Look for CA Certificates\n\t\tif len(caCertsMatch) != 0 {\n\t\t\tcaPool, err := createCACertsPool(cs, scope, caCertsMatch, skipInvalid)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tconfig.ClientCAs = caPool\n\t\t}\n\t} else {\n\t\treturn ErrBadCertStore\n\t}\n\n\t// Get intermediates in the cert store for the found leaf IFF there is a full chain of trust in the store\n\t// otherwise just use leaf as the final chain.\n\t//\n\t// Using std lib Verify as a reliable way to get valid chains out of the win store for the leaf; however,\n\t// using empty options since server TLS stanza could be TLS role as server identity or client identity.\n\tchains, err := leaf.Verify(vOpts)\n\tif err != nil || len(chains) == 0 {\n\t\tchains = append(chains, []*x509.Certificate{leaf})\n\t}\n\n\t// We have at least one verified chain so pop the first chain and remove the self-signed CA cert (if present)\n\t// from the end of the chain\n\tchain = chains[0]\n\tif len(chain) > 1 {\n\t\tchain = chain[:len(chain)-1]\n\t}\n\n\t// For tls.Certificate.Certificate need a [][]byte from []*x509.Certificate\n\t// Approximate capacity for efficiency\n\trawChain = make([][]byte, 0, len(chain))\n\tfor _, link := range chain {\n\t\trawChain = append(rawChain, link.Raw)\n\t}\n\n\ttlsCert := tls.Certificate{\n\t\tCertificate: rawChain,\n\t\tPrivateKey:  pk,\n\t\tLeaf:        leaf,\n\t}\n\tconfig.Certificates = []tls.Certificate{tlsCert}\n\n\t// note: pk is a windows pointer (not freed by Go) but needs to live the life of the server for Signing.\n\t// The cert context (leafCtx) windows pointer must not be freed underneath the pk so also life of the server.\n\treturn nil\n}\n\n// winWide returns a pointer to uint16 representing the equivalent\n// to a Windows LPCWSTR.\nfunc winWide(s string) *uint16 {\n\tw := utf16.Encode([]rune(s))\n\tw = append(w, 0)\n\treturn &w[0]\n}\n\n// winOpenProvider gets a provider handle for subsequent calls\nfunc winOpenProvider(provider string) (uintptr, error) {\n\tvar hProv uintptr\n\tpname := winWide(provider)\n\t// Open the provider, the last parameter is not used\n\tr, _, err := winNCryptOpenStorageProvider.Call(uintptr(unsafe.Pointer(&hProv)), uintptr(unsafe.Pointer(pname)), 0)\n\tif r == 0 {\n\t\treturn hProv, nil\n\t}\n\treturn hProv, fmt.Errorf(\"NCryptOpenStorageProvider returned %X: %v\", r, err)\n}\n\n// winFindCert wraps the CertFindCertificateInStore library call. Note that any cert context passed\n// into prev will be freed. If no certificate was found, nil will be returned.\nfunc winFindCert(store windows.Handle, enc, findFlags, findType uint32, para *uint16, prev *windows.CertContext) (*windows.CertContext, error) {\n\th, _, err := winCertFindCertificateInStore.Call(\n\t\tuintptr(store),\n\t\tuintptr(enc),\n\t\tuintptr(findFlags),\n\t\tuintptr(findType),\n\t\tuintptr(unsafe.Pointer(para)),\n\t\tuintptr(unsafe.Pointer(prev)),\n\t)\n\tif h == 0 {\n\t\t// Actual error, or simply not found?\n\t\tif errno, ok := err.(syscall.Errno); ok && errno == syscall.Errno(winCryptENotFound) {\n\t\t\treturn nil, ErrFailedCertSearch\n\t\t}\n\t\treturn nil, ErrFailedCertSearch\n\t}\n\t// nolint:govet\n\treturn (*windows.CertContext)(unsafe.Pointer(h)), nil\n}\n\n// winVerifyCertValid wraps the CertVerifyTimeValidity and simply returns true if the certificate is valid\nfunc winVerifyCertValid(timeToVerify *windows.Filetime, certInfo *windows.CertInfo) bool {\n\t// this function does not document returning errors / setting lasterror\n\tr, _, _ := winCertVerifyTimeValidity.Call(\n\t\tuintptr(unsafe.Pointer(timeToVerify)),\n\t\tuintptr(unsafe.Pointer(certInfo)),\n\t)\n\treturn r == 0\n}\n\n// winCertStore is a store implementation for the Windows Certificate Store\ntype winCertStore struct {\n\tProv     uintptr\n\tProvName string\n\tstores   map[string]*winStoreHandle\n\tmu       sync.Mutex\n}\n\n// winOpenCertStore creates a winCertStore\nfunc winOpenCertStore(provider string) (*winCertStore, error) {\n\tcngProv, err := winOpenProvider(provider)\n\tif err != nil {\n\t\t// pass through error from winOpenProvider\n\t\treturn nil, err\n\t}\n\n\twcs := &winCertStore{\n\t\tProv:     cngProv,\n\t\tProvName: provider,\n\t\tstores:   make(map[string]*winStoreHandle),\n\t}\n\n\treturn wcs, nil\n}\n\n// winCertContextToX509 creates an x509.Certificate from a Windows cert context.\nfunc winCertContextToX509(ctx *windows.CertContext) (*x509.Certificate, error) {\n\tvar der []byte\n\tslice := (*reflect.SliceHeader)(unsafe.Pointer(&der))\n\tslice.Data = uintptr(unsafe.Pointer(ctx.EncodedCert))\n\tslice.Len = int(ctx.Length)\n\tslice.Cap = int(ctx.Length)\n\treturn x509.ParseCertificate(der)\n}\n\n// certByIssuer matches and returns the first certificate found by passed issuer.\n// CertContext pointer returned allows subsequent key operations like Sign. Caller specifies\n// current user's personal certs or local machine's personal certs using storeType.\n// See CERT_FIND_ISSUER_STR description at https://learn.microsoft.com/en-us/windows/win32/api/wincrypt/nf-wincrypt-certfindcertificateinstore\nfunc (w *winCertStore) certByIssuer(issuer string, storeType uint32, skipInvalid bool) (*x509.Certificate, *windows.CertContext, error) {\n\treturn w.certSearch(winFindIssuerStr, issuer, winMyStore, storeType, skipInvalid)\n}\n\n// certBySubject matches and returns the first certificate found by passed subject field.\n// CertContext pointer returned allows subsequent key operations like Sign. Caller specifies\n// current user's personal certs or local machine's personal certs using storeType.\n// See CERT_FIND_SUBJECT_STR description at https://learn.microsoft.com/en-us/windows/win32/api/wincrypt/nf-wincrypt-certfindcertificateinstore\nfunc (w *winCertStore) certBySubject(subject string, storeType uint32, skipInvalid bool) (*x509.Certificate, *windows.CertContext, error) {\n\treturn w.certSearch(winFindSubjectStr, subject, winMyStore, storeType, skipInvalid)\n}\n\n// certByThumbprint matches and returns the first certificate found by passed SHA1 thumbprint.\n// CertContext pointer returned allows subsequent key operations like Sign. Caller specifies\n// current user's personal certs or local machine's personal certs using storeType.\n// See CERT_FIND_SUBJECT_STR description at https://learn.microsoft.com/en-us/windows/win32/api/wincrypt/nf-wincrypt-certfindcertificateinstore\nfunc (w *winCertStore) certByThumbprint(hash string, storeType uint32, skipInvalid bool) (*x509.Certificate, *windows.CertContext, error) {\n\treturn w.certSearch(winFindHashStr, hash, winMyStore, storeType, skipInvalid)\n}\n\n// caCertsBySubjectMatch matches and returns all matching certificates of the subject field.\n//\n// The following locations are searched:\n// 1) Root (Trusted Root Certification Authorities)\n// 2) AuthRoot (Third-Party Root Certification Authorities)\n// 3) CA (Intermediate Certification Authorities)\n//\n// Caller specifies current user's personal certs or local machine's personal certs using storeType.\n// See CERT_FIND_SUBJECT_STR description at https://learn.microsoft.com/en-us/windows/win32/api/wincrypt/nf-wincrypt-certfindcertificateinstore\nfunc (w *winCertStore) caCertsBySubjectMatch(subject string, storeType uint32, skipInvalid bool) ([]*x509.Certificate, error) {\n\tvar (\n\t\tleaf            *x509.Certificate\n\t\tsearchLocations = [3]*uint16{winRootStore, winAuthRootStore, winIntermediateCAStore}\n\t\trv              []*x509.Certificate\n\t)\n\t// surprisingly, an empty string returns a result. We'll treat this as an error.\n\tif subject == \"\" {\n\t\treturn nil, ErrBadCaCertMatchField\n\t}\n\tfor _, sr := range searchLocations {\n\t\tvar err error\n\t\tif leaf, _, err = w.certSearch(winFindSubjectStr, subject, sr, storeType, skipInvalid); err == nil {\n\t\t\trv = append(rv, leaf)\n\t\t} else {\n\t\t\t// Ignore the failed search from a single location. Errors we catch include\n\t\t\t// ErrFailedX509Extract (resulting from a malformed certificate) and errors\n\t\t\t// around invalid attributes, unsupported algorithms, etc. These are corner\n\t\t\t// cases as certificates with these errors shouldn't have been allowed\n\t\t\t// to be added to the store in the first place.\n\t\t\tif err != ErrFailedCertSearch {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t}\n\t}\n\t// Not found anywhere\n\tif len(rv) == 0 {\n\t\treturn nil, ErrFailedCertSearch\n\t}\n\treturn rv, nil\n}\n\n// certSearch is a helper function to lookup certificates based on search type and match value.\n// store is used to specify which store to perform the lookup in (system or user).\nfunc (w *winCertStore) certSearch(searchType uint32, matchValue string, searchRoot *uint16, store uint32, skipInvalid bool) (*x509.Certificate, *windows.CertContext, error) {\n\t// store handle to \"MY\" store\n\th, err := w.storeHandle(store, searchRoot)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\n\tvar prev *windows.CertContext\n\tvar cert *x509.Certificate\n\n\ti, err := windows.UTF16PtrFromString(matchValue)\n\tif err != nil {\n\t\treturn nil, nil, ErrFailedCertSearch\n\t}\n\n\t// pass 0 as the third parameter because it is not used\n\t// https://msdn.microsoft.com/en-us/library/windows/desktop/aa376064(v=vs.85).aspx\n\n\tfor {\n\t\tnc, err := winFindCert(h, winEncodingX509ASN|winEncodingPKCS7, 0, searchType, i, prev)\n\t\tif err != nil {\n\t\t\treturn nil, nil, err\n\t\t}\n\t\tif nc != nil {\n\t\t\t// certificate found\n\t\t\tprev = nc\n\n\t\t\tvar now *windows.Filetime\n\t\t\tif skipInvalid && !winVerifyCertValid(now, nc.CertInfo) {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// Extract the DER-encoded certificate from the cert context\n\t\t\txc, err := winCertContextToX509(nc)\n\t\t\tif err == nil {\n\t\t\t\tcert = xc\n\t\t\t\tbreak\n\t\t\t} else {\n\t\t\t\treturn nil, nil, ErrFailedX509Extract\n\t\t\t}\n\t\t} else {\n\t\t\treturn nil, nil, ErrFailedCertSearch\n\t\t}\n\t}\n\n\tif cert == nil {\n\t\treturn nil, nil, ErrFailedX509Extract\n\t}\n\n\treturn cert, prev, nil\n}\n\ntype winStoreHandle struct {\n\thandle *windows.Handle\n}\n\nfunc winNewStoreHandle(provider uint32, store *uint16) (*winStoreHandle, error) {\n\tvar s winStoreHandle\n\tif s.handle != nil {\n\t\treturn &s, nil\n\t}\n\tst, err := windows.CertOpenStore(\n\t\twinCertStoreProvSystem,\n\t\t0,\n\t\t0,\n\t\tprovider|winCertStoreReadOnly,\n\t\tuintptr(unsafe.Pointer(store)))\n\tif err != nil {\n\t\treturn nil, ErrBadCryptoStoreProvider\n\t}\n\ts.handle = &st\n\treturn &s, nil\n}\n\n// winKey implements crypto.Signer and crypto.Decrypter for key based operations.\ntype winKey struct {\n\thandle         uintptr\n\tpub            crypto.PublicKey\n\tContainer      string\n\tAlgorithmGroup string\n}\n\n// Public exports a public key to implement crypto.Signer\nfunc (k winKey) Public() crypto.PublicKey {\n\treturn k.pub\n}\n\n// Sign returns the signature of a hash to implement crypto.Signer\nfunc (k winKey) Sign(_ io.Reader, digest []byte, opts crypto.SignerOpts) ([]byte, error) {\n\tswitch k.AlgorithmGroup {\n\tcase \"ECDSA\", \"ECDH\":\n\t\treturn winSignECDSA(k.handle, digest)\n\tcase \"RSA\":\n\t\thf := opts.HashFunc()\n\t\talgID, ok := winAlgIDs[hf]\n\t\tif !ok {\n\t\t\treturn nil, ErrBadRSAHashAlgorithm\n\t\t}\n\t\tswitch opts.(type) {\n\t\tcase *rsa.PSSOptions:\n\t\t\treturn winSignRSAPSSPadding(k.handle, digest, algID)\n\t\tdefault:\n\t\t\treturn winSignRSAPKCS1Padding(k.handle, digest, algID)\n\t\t}\n\tdefault:\n\t\treturn nil, ErrBadSigningAlgorithm\n\t}\n}\n\nfunc winSignECDSA(kh uintptr, digest []byte) ([]byte, error) {\n\tvar size uint32\n\t// Obtain the size of the signature\n\tr, _, _ := winNCryptSignHash.Call(\n\t\tkh,\n\t\t0,\n\t\tuintptr(unsafe.Pointer(&digest[0])),\n\t\tuintptr(len(digest)),\n\t\t0,\n\t\t0,\n\t\tuintptr(unsafe.Pointer(&size)),\n\t\t0)\n\tif r != 0 {\n\t\treturn nil, ErrStoreECDSASigningError\n\t}\n\n\t// Obtain the signature data\n\tbuf := make([]byte, size)\n\tr, _, _ = winNCryptSignHash.Call(\n\t\tkh,\n\t\t0,\n\t\tuintptr(unsafe.Pointer(&digest[0])),\n\t\tuintptr(len(digest)),\n\t\tuintptr(unsafe.Pointer(&buf[0])),\n\t\tuintptr(size),\n\t\tuintptr(unsafe.Pointer(&size)),\n\t\t0)\n\tif r != 0 {\n\t\treturn nil, ErrStoreECDSASigningError\n\t}\n\tif len(buf) != int(size) {\n\t\treturn nil, ErrStoreECDSASigningError\n\t}\n\n\treturn winPackECDSASigValue(bytes.NewReader(buf[:size]), int(size/2))\n}\n\nfunc winPackECDSASigValue(r io.Reader, digestLength int) ([]byte, error) {\n\tsigR := make([]byte, digestLength)\n\tif _, err := io.ReadFull(r, sigR); err != nil {\n\t\treturn nil, ErrStoreECDSASigningError\n\t}\n\n\tsigS := make([]byte, digestLength)\n\tif _, err := io.ReadFull(r, sigS); err != nil {\n\t\treturn nil, ErrStoreECDSASigningError\n\t}\n\n\tvar b cryptobyte.Builder\n\tb.AddASN1(asn1.SEQUENCE, func(b *cryptobyte.Builder) {\n\t\tb.AddASN1BigInt(new(big.Int).SetBytes(sigR))\n\t\tb.AddASN1BigInt(new(big.Int).SetBytes(sigS))\n\t})\n\treturn b.Bytes()\n}\n\nfunc winSignRSAPKCS1Padding(kh uintptr, digest []byte, algID *uint16) ([]byte, error) {\n\t// PKCS#1 v1.5 padding for some TLS 1.2\n\tpadInfo := winPKCS1PaddingInfo{pszAlgID: algID}\n\tvar size uint32\n\t// Obtain the size of the signature\n\tr, _, _ := winNCryptSignHash.Call(\n\t\tkh,\n\t\tuintptr(unsafe.Pointer(&padInfo)),\n\t\tuintptr(unsafe.Pointer(&digest[0])),\n\t\tuintptr(len(digest)),\n\t\t0,\n\t\t0,\n\t\tuintptr(unsafe.Pointer(&size)),\n\t\twinBCryptPadPKCS1)\n\tif r != 0 {\n\t\treturn nil, ErrStoreRSASigningError\n\t}\n\n\t// Obtain the signature data\n\tsig := make([]byte, size)\n\tr, _, _ = winNCryptSignHash.Call(\n\t\tkh,\n\t\tuintptr(unsafe.Pointer(&padInfo)),\n\t\tuintptr(unsafe.Pointer(&digest[0])),\n\t\tuintptr(len(digest)),\n\t\tuintptr(unsafe.Pointer(&sig[0])),\n\t\tuintptr(size),\n\t\tuintptr(unsafe.Pointer(&size)),\n\t\twinBCryptPadPKCS1)\n\tif r != 0 {\n\t\treturn nil, ErrStoreRSASigningError\n\t}\n\n\treturn sig[:size], nil\n}\n\nfunc winSignRSAPSSPadding(kh uintptr, digest []byte, algID *uint16) ([]byte, error) {\n\t// PSS padding for TLS 1.3 and some TLS 1.2\n\tpadInfo := winPSSPaddingInfo{pszAlgID: algID, cbSalt: winBCryptPadPSSSalt}\n\n\tvar size uint32\n\t// Obtain the size of the signature\n\tr, _, _ := winNCryptSignHash.Call(\n\t\tkh,\n\t\tuintptr(unsafe.Pointer(&padInfo)),\n\t\tuintptr(unsafe.Pointer(&digest[0])),\n\t\tuintptr(len(digest)),\n\t\t0,\n\t\t0,\n\t\tuintptr(unsafe.Pointer(&size)),\n\t\twinBCryptPadPSS)\n\tif r != 0 {\n\t\treturn nil, ErrStoreRSASigningError\n\t}\n\n\t// Obtain the signature data\n\tsig := make([]byte, size)\n\tr, _, _ = winNCryptSignHash.Call(\n\t\tkh,\n\t\tuintptr(unsafe.Pointer(&padInfo)),\n\t\tuintptr(unsafe.Pointer(&digest[0])),\n\t\tuintptr(len(digest)),\n\t\tuintptr(unsafe.Pointer(&sig[0])),\n\t\tuintptr(size),\n\t\tuintptr(unsafe.Pointer(&size)),\n\t\twinBCryptPadPSS)\n\tif r != 0 {\n\t\treturn nil, ErrStoreRSASigningError\n\t}\n\n\treturn sig[:size], nil\n}\n\n// certKey wraps CryptAcquireCertificatePrivateKey. It obtains the CNG private\n// key of a known certificate and returns a pointer to a winKey which implements\n// both crypto.Signer. When a nil cert context is passed\n// a nil key is intentionally returned, to model the expected behavior of a\n// non-existent cert having no private key.\n// https://docs.microsoft.com/en-us/windows/win32/api/wincrypt/nf-wincrypt-cryptacquirecertificateprivatekey\nfunc (w *winCertStore) certKey(cert *windows.CertContext) (*winKey, error) {\n\t// Return early if a nil cert was passed.\n\tif cert == nil {\n\t\treturn nil, nil\n\t}\n\tvar (\n\t\tkh       uintptr\n\t\tspec     uint32\n\t\tmustFree int\n\t)\n\tr, _, _ := winCryptAcquireCertificatePrivateKey.Call(\n\t\tuintptr(unsafe.Pointer(cert)),\n\t\twinAcquireCached|winAcquireSilent|winAcquireOnlyNCryptKey,\n\t\t0, // Reserved, must be null.\n\t\tuintptr(unsafe.Pointer(&kh)),\n\t\tuintptr(unsafe.Pointer(&spec)),\n\t\tuintptr(unsafe.Pointer(&mustFree)),\n\t)\n\t// If the function succeeds, the return value is nonzero (TRUE).\n\tif r == 0 {\n\t\treturn nil, ErrNoPrivateKeyStoreRef\n\t}\n\tif mustFree != 0 {\n\t\treturn nil, ErrNoPrivateKeyStoreRef\n\t}\n\tif spec != winNcryptKeySpec {\n\t\treturn nil, ErrNoPrivateKeyStoreRef\n\t}\n\n\treturn winKeyMetadata(kh)\n}\n\nfunc winKeyMetadata(kh uintptr) (*winKey, error) {\n\t// uc is used to populate the unique container name attribute of the private key\n\tuc, err := winGetPropertyStr(kh, winNCryptUniqueNameProperty)\n\tif err != nil {\n\t\t// unable to determine key unique name\n\t\treturn nil, ErrExtractingPrivateKeyMetadata\n\t}\n\n\talg, err := winGetPropertyStr(kh, winNCryptAlgorithmGroupProperty)\n\tif err != nil {\n\t\t// unable to determine key algorithm\n\t\treturn nil, ErrExtractingPrivateKeyMetadata\n\t}\n\n\tvar pub crypto.PublicKey\n\n\tswitch alg {\n\tcase \"ECDSA\", \"ECDH\":\n\t\tbuf, err := winExport(kh, winBCryptECCPublicBlob)\n\t\tif err != nil {\n\t\t\t// failed to export ECC public key\n\t\t\treturn nil, ErrExtractingECCPublicKey\n\t\t}\n\t\tpub, err = unmarshalECC(buf, kh)\n\t\tif err != nil {\n\t\t\treturn nil, ErrExtractingECCPublicKey\n\t\t}\n\tcase \"RSA\":\n\t\tbuf, err := winExport(kh, winBCryptRSAPublicBlob)\n\t\tif err != nil {\n\t\t\treturn nil, ErrExtractingRSAPublicKey\n\t\t}\n\t\tpub, err = winUnmarshalRSA(buf)\n\t\tif err != nil {\n\t\t\treturn nil, ErrExtractingRSAPublicKey\n\t\t}\n\tdefault:\n\t\treturn nil, ErrBadPublicKeyAlgorithm\n\t}\n\n\treturn &winKey{handle: kh, pub: pub, Container: uc, AlgorithmGroup: alg}, nil\n}\n\nfunc winGetProperty(kh uintptr, property *uint16) ([]byte, error) {\n\tvar strSize uint32\n\tr, _, _ := winNCryptGetProperty.Call(\n\t\tkh,\n\t\tuintptr(unsafe.Pointer(property)),\n\t\t0,\n\t\t0,\n\t\tuintptr(unsafe.Pointer(&strSize)),\n\t\t0,\n\t\t0)\n\tif r != 0 {\n\t\treturn nil, ErrExtractPropertyFromKey\n\t}\n\n\tbuf := make([]byte, strSize)\n\tr, _, _ = winNCryptGetProperty.Call(\n\t\tkh,\n\t\tuintptr(unsafe.Pointer(property)),\n\t\tuintptr(unsafe.Pointer(&buf[0])),\n\t\tuintptr(strSize),\n\t\tuintptr(unsafe.Pointer(&strSize)),\n\t\t0,\n\t\t0)\n\tif r != 0 {\n\t\treturn nil, ErrExtractPropertyFromKey\n\t}\n\n\treturn buf, nil\n}\n\nfunc winGetPropertyStr(kh uintptr, property *uint16) (string, error) {\n\tbuf, err := winFnGetProperty(kh, property)\n\tif err != nil {\n\t\treturn \"\", ErrExtractPropertyFromKey\n\t}\n\tuc := bytes.ReplaceAll(buf, []byte{0x00}, []byte(\"\"))\n\treturn string(uc), nil\n}\n\nfunc winExport(kh uintptr, blobType *uint16) ([]byte, error) {\n\tvar size uint32\n\t// When obtaining the size of a public key, most parameters are not required\n\tr, _, _ := winNCryptExportKey.Call(\n\t\tkh,\n\t\t0,\n\t\tuintptr(unsafe.Pointer(blobType)),\n\t\t0,\n\t\t0,\n\t\t0,\n\t\tuintptr(unsafe.Pointer(&size)),\n\t\t0)\n\tif r != 0 {\n\t\treturn nil, ErrExtractingPublicKey\n\t}\n\n\t// Place the exported key in buf now that we know the size required\n\tbuf := make([]byte, size)\n\tr, _, _ = winNCryptExportKey.Call(\n\t\tkh,\n\t\t0,\n\t\tuintptr(unsafe.Pointer(blobType)),\n\t\t0,\n\t\tuintptr(unsafe.Pointer(&buf[0])),\n\t\tuintptr(size),\n\t\tuintptr(unsafe.Pointer(&size)),\n\t\t0)\n\tif r != 0 {\n\t\treturn nil, ErrExtractingPublicKey\n\t}\n\treturn buf, nil\n}\n\nfunc unmarshalECC(buf []byte, kh uintptr) (*ecdsa.PublicKey, error) {\n\t// BCRYPT_ECCKEY_BLOB from bcrypt.h\n\theader := struct {\n\t\tMagic uint32\n\t\tKey   uint32\n\t}{}\n\n\tr := bytes.NewReader(buf)\n\tif err := binary.Read(r, binary.LittleEndian, &header); err != nil {\n\t\treturn nil, ErrExtractingECCPublicKey\n\t}\n\n\tcurve, ok := winCurveIDs[header.Magic]\n\tif !ok {\n\t\t// Fix for b/185945636, where despite specifying the curve, nCrypt returns\n\t\t// an incorrect response with BCRYPT_ECDSA_PUBLIC_GENERIC_MAGIC.\n\t\tvar err error\n\t\tcurve, err = winCurveName(kh)\n\t\tif err != nil {\n\t\t\t// unsupported header magic or cannot match the curve by name\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\tkeyX := make([]byte, header.Key)\n\tif n, err := r.Read(keyX); n != int(header.Key) || err != nil {\n\t\t// failed to read key X\n\t\treturn nil, ErrExtractingECCPublicKey\n\t}\n\n\tkeyY := make([]byte, header.Key)\n\tif n, err := r.Read(keyY); n != int(header.Key) || err != nil {\n\t\t// failed to read key Y\n\t\treturn nil, ErrExtractingECCPublicKey\n\t}\n\n\tpub := &ecdsa.PublicKey{\n\t\tCurve: curve,\n\t\tX:     new(big.Int).SetBytes(keyX),\n\t\tY:     new(big.Int).SetBytes(keyY),\n\t}\n\treturn pub, nil\n}\n\n// winCurveName reads the curve name property and returns the corresponding curve.\nfunc winCurveName(kh uintptr) (elliptic.Curve, error) {\n\tcn, err := winGetPropertyStr(kh, winNCryptECCCurveNameProperty)\n\tif err != nil {\n\t\t// unable to determine the curve property name\n\t\treturn nil, ErrExtractPropertyFromKey\n\t}\n\tcurve, ok := winCurveNames[cn]\n\tif !ok {\n\t\t// unknown curve name\n\t\treturn nil, ErrBadECCCurveName\n\t}\n\treturn curve, nil\n}\n\nfunc winUnmarshalRSA(buf []byte) (*rsa.PublicKey, error) {\n\t// BCRYPT_RSA_BLOB from bcrypt.h\n\theader := struct {\n\t\tMagic         uint32\n\t\tBitLength     uint32\n\t\tPublicExpSize uint32\n\t\tModulusSize   uint32\n\t\tUnusedPrime1  uint32\n\t\tUnusedPrime2  uint32\n\t}{}\n\n\tr := bytes.NewReader(buf)\n\tif err := binary.Read(r, binary.LittleEndian, &header); err != nil {\n\t\treturn nil, ErrExtractingRSAPublicKey\n\t}\n\n\tif header.Magic != winRSA1Magic {\n\t\t// invalid header magic\n\t\treturn nil, ErrExtractingRSAPublicKey\n\t}\n\n\tif header.PublicExpSize > 8 {\n\t\t// unsupported public exponent size\n\t\treturn nil, ErrExtractingRSAPublicKey\n\t}\n\n\texp := make([]byte, 8)\n\tif n, err := r.Read(exp[8-header.PublicExpSize:]); n != int(header.PublicExpSize) || err != nil {\n\t\t// failed to read public exponent\n\t\treturn nil, ErrExtractingRSAPublicKey\n\t}\n\n\tmod := make([]byte, header.ModulusSize)\n\tif n, err := r.Read(mod); n != int(header.ModulusSize) || err != nil {\n\t\t// failed to read modulus\n\t\treturn nil, ErrExtractingRSAPublicKey\n\t}\n\n\tpub := &rsa.PublicKey{\n\t\tN: new(big.Int).SetBytes(mod),\n\t\tE: int(binary.BigEndian.Uint64(exp)),\n\t}\n\treturn pub, nil\n}\n\n// storeHandle returns a handle to a given cert store, opening the handle as needed.\nfunc (w *winCertStore) storeHandle(provider uint32, store *uint16) (windows.Handle, error) {\n\tw.mu.Lock()\n\tdefer w.mu.Unlock()\n\n\tkey := fmt.Sprintf(\"%d%s\", provider, windows.UTF16PtrToString(store))\n\tvar err error\n\tif w.stores[key] == nil {\n\t\tw.stores[key], err = winNewStoreHandle(provider, store)\n\t\tif err != nil {\n\t\t\treturn 0, ErrBadCryptoStoreProvider\n\t\t}\n\t}\n\treturn *w.stores[key].handle, nil\n}\n\n// Verify interface conformance.\nvar _ credential = &winKey{}\n",
    "source_file": "server/certstore/certstore_windows.go",
    "chunk_type": "code"
  },
  {
    "content": "package certstore\n\nimport (\n\t\"errors\"\n)\n\nvar (\n\t// ErrBadCryptoStoreProvider represents inablity to establish link with a certificate store\n\tErrBadCryptoStoreProvider = errors.New(\"unable to open certificate store or store not available\")\n\n\t// ErrBadRSAHashAlgorithm represents a bad or unsupported RSA hash algorithm\n\tErrBadRSAHashAlgorithm = errors.New(\"unsupported RSA hash algorithm\")\n\n\t// ErrBadSigningAlgorithm represents a bad or unsupported signing algorithm\n\tErrBadSigningAlgorithm = errors.New(\"unsupported signing algorithm\")\n\n\t// ErrStoreRSASigningError represents an error returned from store during RSA signature\n\tErrStoreRSASigningError = errors.New(\"unable to obtain RSA signature from store\")\n\n\t// ErrStoreECDSASigningError represents an error returned from store during ECDSA signature\n\tErrStoreECDSASigningError = errors.New(\"unable to obtain ECDSA signature from store\")\n\n\t// ErrNoPrivateKeyStoreRef represents an error getting a handle to a private key in store\n\tErrNoPrivateKeyStoreRef = errors.New(\"unable to obtain private key handle from store\")\n\n\t// ErrExtractingPrivateKeyMetadata represents a family of errors extracting metadata about the private key in store\n\tErrExtractingPrivateKeyMetadata = errors.New(\"unable to extract private key metadata\")\n\n\t// ErrExtractingECCPublicKey represents an error exporting ECC-type public key from store\n\tErrExtractingECCPublicKey = errors.New(\"unable to extract ECC public key from store\")\n\n\t// ErrExtractingRSAPublicKey represents an error exporting RSA-type public key from store\n\tErrExtractingRSAPublicKey = errors.New(\"unable to extract RSA public key from store\")\n\n\t// ErrExtractingPublicKey represents a general error exporting public key from store\n\tErrExtractingPublicKey = errors.New(\"unable to extract public key from store\")\n\n\t// ErrBadPublicKeyAlgorithm represents a bad or unsupported public key algorithm\n\tErrBadPublicKeyAlgorithm = errors.New(\"unsupported public key algorithm\")\n\n\t// ErrExtractPropertyFromKey represents a general failure to extract a metadata property field\n\tErrExtractPropertyFromKey = errors.New(\"unable to extract property from key\")\n\n\t// ErrBadECCCurveName represents an ECC signature curve name that is bad or unsupported\n\tErrBadECCCurveName = errors.New(\"unsupported ECC curve name\")\n\n\t// ErrFailedCertSearch represents not able to find certificate in store\n\tErrFailedCertSearch = errors.New(\"unable to find certificate in store\")\n\n\t// ErrFailedX509Extract represents not being able to extract x509 certificate from found cert in store\n\tErrFailedX509Extract = errors.New(\"unable to extract x509 from certificate\")\n\n\t// ErrBadMatchByType represents unknown CERT_MATCH_BY passed\n\tErrBadMatchByType = errors.New(\"cert match by type not implemented\")\n\n\t// ErrBadCertStore represents unknown CERT_STORE passed\n\tErrBadCertStore = errors.New(\"cert store type not implemented\")\n\n\t// ErrConflictCertFileAndStore represents ambiguous configuration of both file and store\n\tErrConflictCertFileAndStore = errors.New(\"'cert_file' and 'cert_store' may not both be configured\")\n\n\t// ErrBadCertStoreField represents malformed cert_store option\n\tErrBadCertStoreField = errors.New(\"expected 'cert_store' to be a valid non-empty string\")\n\n\t// ErrBadCertMatchByField represents malformed cert_match_by option\n\tErrBadCertMatchByField = errors.New(\"expected 'cert_match_by' to be a valid non-empty string\")\n\n\t// ErrBadCertMatchField represents malformed cert_match option\n\tErrBadCertMatchField = errors.New(\"expected 'cert_match' to be a valid non-empty string\")\n\n\t// ErrBadCaCertMatchField represents malformed cert_match option\n\tErrBadCaCertMatchField = errors.New(\"expected 'ca_certs_match' to be a valid non-empty string array\")\n\n\t// ErrBadCertMatchSkipInvalidField represents malformed cert_match_skip_invalid option\n\tErrBadCertMatchSkipInvalidField = errors.New(\"expected 'cert_match_skip_invalid' to be a boolean\")\n\n\t// ErrOSNotCompatCertStore represents cert_store passed that exists but is not valid on current OS\n\tErrOSNotCompatCertStore = errors.New(\"cert_store not compatible with current operating system\")\n)\n",
    "source_file": "server/certstore/errors.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2023-2025 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n//go:build zos\n\npackage pse\n\n// This is a placeholder for now.\nfunc ProcUsage(pcpu *float64, rss, vss *int64) error {\n\t*pcpu = 0.0\n\t*rss = 0\n\t*vss = 0\n\n\treturn nil\n}\n",
    "source_file": "server/pse/pse_zos.go",
    "chunk_type": "code"
  },
  {
    "content": "/*\n * Compile and run this as a C program to get the kinfo_proc offsets\n * for your architecture.\n * While FreeBSD works hard at binary-compatibility within an ABI, various\n * we can't say for sure that these are right for _all_ use on a hardware\n * platform.  The LP64 ifdef affects the offsets considerably.\n *\n * We use these offsets in hardware-specific files for FreeBSD, to avoid a cgo\n * compilation-time dependency, allowing us to cross-compile for FreeBSD from\n * other hardware platforms, letting us distribute binaries for FreeBSD.\n */\n\n#include <stddef.h>\n#include <stdint.h>\n#include <stdio.h>\n#include <sys/types.h>\n#include <sys/user.h>\n\n#define SHOW_OFFSET(FIELD) printf(\" KIP_OFF_%s = %zu\\n\", #FIELD, offsetof(struct kinfo_proc, ki_ ## FIELD))\n\nint main(int argc, char *argv[]) {\n\t/* Uncomment these if you want some extra debugging aids:\n\tSHOW_OFFSET(pid);\n\tSHOW_OFFSET(ppid);\n\tSHOW_OFFSET(uid);\n\t*/\n\tSHOW_OFFSET(size);\n\tSHOW_OFFSET(rssize);\n\tSHOW_OFFSET(pctcpu);\n}\n",
    "source_file": "server/pse/freebsd.txt",
    "chunk_type": "doc"
  },
  {
    "content": "// Copyright 2015-2021 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage pse\n\n// On macs after some studying it seems that typical tools like ps and activity monitor report MaxRss and not\n// current RSS. I wrote some C code to pull the real RSS and although it does not go down very often, when it does\n// that is not reflected in the typical tooling one might compare us to, so we can skip cgo and just use rusage imo.\n// We also do not use virtual memory in the upper layers at all, so ok to skip since rusage does not report vss.\n\nimport (\n\t\"math\"\n\t\"sync\"\n\t\"syscall\"\n\t\"time\"\n)\n\ntype lastUsage struct {\n\tsync.Mutex\n\tlast time.Time\n\tcpu  time.Duration\n\trss  int64\n\tpcpu float64\n}\n\n// To hold the last usage and call time.\nvar lu lastUsage\n\nfunc init() {\n\tupdateUsage()\n\tperiodic()\n}\n\n// Get our usage.\nfunc getUsage() (now time.Time, cpu time.Duration, rss int64) {\n\tvar ru syscall.Rusage\n\tsyscall.Getrusage(syscall.RUSAGE_SELF, &ru)\n\tnow = time.Now()\n\tcpu = time.Duration(ru.Utime.Sec)*time.Second + time.Duration(ru.Utime.Usec)*time.Microsecond\n\tcpu += time.Duration(ru.Stime.Sec)*time.Second + time.Duration(ru.Stime.Usec)*time.Microsecond\n\treturn now, cpu, ru.Maxrss\n}\n\n// Update last usage.\n// We need to have a prior sample to compute pcpu.\nfunc updateUsage() (pcpu float64, rss int64) {\n\tlu.Lock()\n\tdefer lu.Unlock()\n\n\tnow, cpu, rss := getUsage()\n\t// Don't skew pcpu by sampling too close to last sample.\n\tif elapsed := now.Sub(lu.last); elapsed < 500*time.Millisecond {\n\t\t// Always update rss.\n\t\tlu.rss = rss\n\t} else {\n\t\ttcpu := float64(cpu - lu.cpu)\n\t\tlu.last, lu.cpu, lu.rss = now, cpu, rss\n\t\t// Want to make this one decimal place and not count on upper layers.\n\t\t// Cores already taken into account via cpu time measurements.\n\t\tlu.pcpu = math.Round(tcpu/float64(elapsed)*1000) / 10\n\t}\n\treturn lu.pcpu, lu.rss\n}\n\n// Sampling function to keep pcpu relevant.\nfunc periodic() {\n\tupdateUsage()\n\ttime.AfterFunc(time.Second, periodic)\n}\n\n// ProcUsage returns CPU and memory usage.\n// Note upper layers do not use virtual memory size, so ok that it is not filled in here.\nfunc ProcUsage(pcpu *float64, rss, vss *int64) error {\n\t*pcpu, *rss = updateUsage()\n\treturn nil\n}\n",
    "source_file": "server/pse/pse_darwin.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2015-2025 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n// There are two FreeBSD implementations; one which uses cgo and should build\n// locally on any FreeBSD, and this one which uses sysctl but needs us to know\n// the offset constants for the fields we care about.\n//\n// The advantage of this one is that without cgo, it is much easier to\n// cross-compile to a target.  The official releases are all built with\n// cross-compilation.\n//\n// We've switched the other implementation to include '_cgo' in the filename,\n// to show that it's not the default.  This isn't an os or arch build tag,\n// so we have to use explicit build-tags within.\n// If lacking CGO support and targeting an unsupported arch, then before the\n// change you would have a compile failure for not being able to cross-compile.\n// After the change, you have a compile failure for not having the symbols\n// because no source file satisfies them.\n// Thus we are no worse off, and it's now much easier to extend support for\n// non-CGO to new architectures, just by editing this file.\n//\n// To generate for other architectures:\n//   1. Copy `freebsd.txt` to have a .c filename on a box running the target\n//      architecture, compile and run it.\n//   2. Update the init() function below to include a case for this architecture\n//   3. Update the build-tags in this file.\n\n//go:build !cgo && freebsd && (amd64 || arm64)\n\npackage pse\n\nimport (\n\t\"encoding/binary\"\n\t\"runtime\"\n\t\"syscall\"\n\n\t\"golang.org/x/sys/unix\"\n)\n\n// On FreeBSD, to get proc information we read it out of the kernel using a\n// binary sysctl.  The endianness of integers is thus explicitly \"host\", rather\n// than little or big endian.\nvar nativeEndian = binary.LittleEndian\n\nvar pageshift int // derived from getpagesize(3) in init() below\n\nvar (\n\t// These are populated in the init function, based on the current architecture.\n\t// (It's less file-count explosion than having one small file for each\n\t// FreeBSD architecture).\n\tKIP_OFF_size   int\n\tKIP_OFF_rssize int\n\tKIP_OFF_pctcpu int\n)\n\nfunc init() {\n\tswitch runtime.GOARCH {\n\t// These are the values which come from compiling and running\n\t// freebsd.txt as a C program.\n\t// Most recently validated: 2025-04 with FreeBSD 14.2R in AWS.\n\tcase \"amd64\":\n\t\tKIP_OFF_size = 256\n\t\tKIP_OFF_rssize = 264\n\t\tKIP_OFF_pctcpu = 308\n\tcase \"arm64\":\n\t\tKIP_OFF_size = 256\n\t\tKIP_OFF_rssize = 264\n\t\tKIP_OFF_pctcpu = 308\n\tdefault:\n\t\tpanic(\"code bug: server/pse FreeBSD support missing case for '\" + runtime.GOARCH + \"' but build-tags allowed us to build anyway?\")\n\t}\n\n\t// To get the physical page size, the C library checks two places:\n\t//   process ELF auxiliary info, AT_PAGESZ\n\t//   as a fallback, the hw.pagesize sysctl\n\t// In looking closely, I found that the Go runtime support is handling\n\t// this for us, and exposing that as syscall.Getpagesize, having checked\n\t// both in the same ways, at process start, so a call to that should return\n\t// a memory value without even a syscall bounce.\n\tpagesize := syscall.Getpagesize()\n\tpageshift = 0\n\tfor pagesize > 1 {\n\t\tpageshift += 1\n\t\tpagesize >>= 1\n\t}\n}\n\nfunc ProcUsage(pcpu *float64, rss, vss *int64) error {\n\trawdata, err := unix.SysctlRaw(\"kern.proc.pid\", unix.Getpid())\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tr_vss_bytes := nativeEndian.Uint32(rawdata[KIP_OFF_size:])\n\tr_rss_pages := nativeEndian.Uint32(rawdata[KIP_OFF_rssize:])\n\trss_bytes := r_rss_pages << pageshift\n\n\t// In C: fixpt_t ki_pctcpu\n\t// Doc: %cpu for process during ki_swtime\n\t// fixpt_t is __uint32_t\n\t// usr.bin/top uses pctdouble to convert to a double (float64)\n\t// define pctdouble(p) ((double)(p) / FIXED_PCTCPU)\n\t// FIXED_PCTCPU is _usually_ FSCALE (some architectures are special)\n\t// <sys/param.h> has:\n\t//   #define FSHIFT  11              /* bits to right of fixed binary point */\n\t//   #define FSCALE  (1<<FSHIFT)\n\tr_pcpu := nativeEndian.Uint32(rawdata[KIP_OFF_pctcpu:])\n\tf_pcpu := float64(r_pcpu) / float64(2048)\n\n\t*rss = int64(rss_bytes)\n\t*vss = int64(r_vss_bytes)\n\t*pcpu = f_pcpu\n\n\treturn nil\n}\n",
    "source_file": "server/pse/pse_freebsd_sysctl.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2015-2018 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n//\n// Copied from pse_darwin.go\n\npackage pse\n\nimport (\n\t\"fmt\"\n\t\"os\"\n\t\"os/exec\"\n)\n\n// ProcUsage returns CPU usage\nfunc ProcUsage(pcpu *float64, rss, vss *int64) error {\n\tpidStr := fmt.Sprintf(\"%d\", os.Getpid())\n\tout, err := exec.Command(\"ps\", \"o\", \"pcpu=,rss=,vsz=\", \"-p\", pidStr).Output()\n\tif err != nil {\n\t\t*rss, *vss = -1, -1\n\t\treturn fmt.Errorf(\"ps call failed:%v\", err)\n\t}\n\tfmt.Sscanf(string(out), \"%f %d %d\", pcpu, rss, vss)\n\t*rss *= 1024 // 1k blocks, want bytes.\n\t*vss *= 1024 // 1k blocks, want bytes.\n\treturn nil\n}\n",
    "source_file": "server/pse/pse_openbsd.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2015-2025 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage pse\n\nimport (\n\t\"bytes\"\n\t\"fmt\"\n\t\"os\"\n\t\"sync/atomic\"\n\t\"syscall\"\n\t\"time\"\n)\n\nvar (\n\tprocStatFile string\n\tticks        int64\n\tlastTotal    int64\n\tlastSeconds  int64\n\tipcpu        int64\n)\n\nconst (\n\tutimePos = 13\n\tstimePos = 14\n\tstartPos = 21\n\tvssPos   = 22\n\trssPos   = 23\n)\n\nfunc init() {\n\t// Avoiding to generate docker image without CGO\n\tticks = 100 // int64(C.sysconf(C._SC_CLK_TCK))\n\tprocStatFile = fmt.Sprintf(\"/proc/%d/stat\", os.Getpid())\n\tperiodic()\n}\n\n// Sampling function to keep pcpu relevant.\nfunc periodic() {\n\tcontents, err := os.ReadFile(procStatFile)\n\tif err != nil {\n\t\treturn\n\t}\n\tfields := bytes.Fields(contents)\n\n\t// PCPU\n\tpstart := parseInt64(fields[startPos])\n\tutime := parseInt64(fields[utimePos])\n\tstime := parseInt64(fields[stimePos])\n\ttotal := utime + stime\n\n\tvar sysinfo syscall.Sysinfo_t\n\tif err := syscall.Sysinfo(&sysinfo); err != nil {\n\t\treturn\n\t}\n\n\tseconds := int64(sysinfo.Uptime) - (pstart / ticks)\n\n\t// Save off temps\n\tlt := lastTotal\n\tls := lastSeconds\n\n\t// Update last sample\n\tlastTotal = total\n\tlastSeconds = seconds\n\n\t// Adjust to current time window\n\ttotal -= lt\n\tseconds -= ls\n\n\tif seconds > 0 {\n\t\tatomic.StoreInt64(&ipcpu, (total*1000/ticks)/seconds)\n\t}\n\n\ttime.AfterFunc(1*time.Second, periodic)\n}\n\n// ProcUsage returns CPU usage\nfunc ProcUsage(pcpu *float64, rss, vss *int64) error {\n\tcontents, err := os.ReadFile(procStatFile)\n\tif err != nil {\n\t\treturn err\n\t}\n\tfields := bytes.Fields(contents)\n\n\t// Memory\n\t*rss = (parseInt64(fields[rssPos])) << 12\n\t*vss = parseInt64(fields[vssPos])\n\n\t// PCPU\n\t// We track this with periodic sampling, so just load and go.\n\t*pcpu = float64(atomic.LoadInt64(&ipcpu)) / 10.0\n\n\treturn nil\n}\n\n// Ascii numbers 0-9\nconst (\n\tasciiZero = 48\n\tasciiNine = 57\n)\n\n// parseInt64 expects decimal positive numbers. We\n// return -1 to signal error\nfunc parseInt64(d []byte) (n int64) {\n\tif len(d) == 0 {\n\t\treturn -1\n\t}\n\tfor _, dec := range d {\n\t\tif dec < asciiZero || dec > asciiNine {\n\t\t\treturn -1\n\t\t}\n\t\tn = n*10 + (int64(dec) - asciiZero)\n\t}\n\treturn n\n}\n",
    "source_file": "server/pse/pse_linux.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2015-2025 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n//go:build windows\n\npackage pse\n\nimport (\n\t\"fmt\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"strings\"\n\t\"sync\"\n\t\"syscall\"\n\t\"time\"\n\t\"unsafe\"\n\n\t\"golang.org/x/sys/windows\"\n)\n\nvar (\n\tpdh                            = windows.NewLazySystemDLL(\"pdh.dll\")\n\twinPdhOpenQuery                = pdh.NewProc(\"PdhOpenQuery\")\n\twinPdhAddCounter               = pdh.NewProc(\"PdhAddCounterW\")\n\twinPdhCollectQueryData         = pdh.NewProc(\"PdhCollectQueryData\")\n\twinPdhGetFormattedCounterValue = pdh.NewProc(\"PdhGetFormattedCounterValue\")\n\twinPdhGetFormattedCounterArray = pdh.NewProc(\"PdhGetFormattedCounterArrayW\")\n)\n\nfunc init() {\n\tif err := pdh.Load(); err != nil {\n\t\tpanic(err)\n\t}\n\tfor _, p := range []*windows.LazyProc{\n\t\twinPdhOpenQuery, winPdhAddCounter, winPdhCollectQueryData,\n\t\twinPdhGetFormattedCounterValue, winPdhGetFormattedCounterArray,\n\t} {\n\t\tif err := p.Find(); err != nil {\n\t\t\tpanic(err)\n\t\t}\n\t}\n}\n\n// global performance counter query handle and counters\nvar (\n\tpcHandle                                       PDH_HQUERY\n\tpidCounter, cpuCounter, rssCounter, vssCounter PDH_HCOUNTER\n\tprevCPU                                        float64\n\tprevRss                                        int64\n\tprevVss                                        int64\n\tlastSampleTime                                 time.Time\n\tprocessPid                                     int\n\tpcQueryLock                                    sync.Mutex\n\tinitialSample                                  = true\n)\n\n// maxQuerySize is the number of values to return from a query.\n// It represents the maximum # of servers that can be queried\n// simultaneously running on a machine.\nconst maxQuerySize = 512\n\n// Keep static memory around to reuse; this works best for passing\n// into the pdh API.\nvar counterResults [maxQuerySize]PDH_FMT_COUNTERVALUE_ITEM_DOUBLE\n\n// PDH Types\ntype (\n\tPDH_HQUERY   syscall.Handle\n\tPDH_HCOUNTER syscall.Handle\n)\n\n// PDH constants used here\nconst (\n\tPDH_FMT_DOUBLE   = 0x00000200\n\tPDH_INVALID_DATA = 0xC0000BC6\n\tPDH_MORE_DATA    = 0x800007D2\n)\n\n// PDH_FMT_COUNTERVALUE_DOUBLE - double value\ntype PDH_FMT_COUNTERVALUE_DOUBLE struct {\n\tCStatus     uint32\n\tDoubleValue float64\n}\n\n// PDH_FMT_COUNTERVALUE_ITEM_DOUBLE is an array\n// element of a double value\ntype PDH_FMT_COUNTERVALUE_ITEM_DOUBLE struct {\n\tSzName   *uint16 // pointer to a string\n\tFmtValue PDH_FMT_COUNTERVALUE_DOUBLE\n}\n\nfunc pdhAddCounter(hQuery PDH_HQUERY, szFullCounterPath string, dwUserData uintptr, phCounter *PDH_HCOUNTER) error {\n\tptxt, _ := syscall.UTF16PtrFromString(szFullCounterPath)\n\tr0, _, _ := winPdhAddCounter.Call(\n\t\tuintptr(hQuery),\n\t\tuintptr(unsafe.Pointer(ptxt)),\n\t\tdwUserData,\n\t\tuintptr(unsafe.Pointer(phCounter)))\n\n\tif r0 != 0 {\n\t\treturn fmt.Errorf(\"pdhAddCounter failed. %d\", r0)\n\t}\n\treturn nil\n}\n\nfunc pdhOpenQuery(datasrc *uint16, userdata uint32, query *PDH_HQUERY) error {\n\tr0, _, _ := syscall.Syscall(winPdhOpenQuery.Addr(), 3, 0, uintptr(userdata), uintptr(unsafe.Pointer(query)))\n\tif r0 != 0 {\n\t\treturn fmt.Errorf(\"pdhOpenQuery failed - %d\", r0)\n\t}\n\treturn nil\n}\n\nfunc pdhCollectQueryData(hQuery PDH_HQUERY) error {\n\tr0, _, _ := winPdhCollectQueryData.Call(uintptr(hQuery))\n\tif r0 != 0 {\n\t\treturn fmt.Errorf(\"pdhCollectQueryData failed - %d\", r0)\n\t}\n\treturn nil\n}\n\n// pdhGetFormattedCounterArrayDouble returns the value of return code\n// rather than error, to easily check return codes\nfunc pdhGetFormattedCounterArrayDouble(hCounter PDH_HCOUNTER, lpdwBufferSize *uint32, lpdwBufferCount *uint32, itemBuffer *PDH_FMT_COUNTERVALUE_ITEM_DOUBLE) uint32 {\n\tret, _, _ := winPdhGetFormattedCounterArray.Call(\n\t\tuintptr(hCounter),\n\t\tuintptr(PDH_FMT_DOUBLE),\n\t\tuintptr(unsafe.Pointer(lpdwBufferSize)),\n\t\tuintptr(unsafe.Pointer(lpdwBufferCount)),\n\t\tuintptr(unsafe.Pointer(itemBuffer)))\n\n\treturn uint32(ret)\n}\n\nfunc getCounterArrayData(counter PDH_HCOUNTER) ([]float64, error) {\n\tvar bufSize uint32\n\tvar bufCount uint32\n\n\t// Retrieving array data requires two calls, the first which\n\t// requires an addressable empty buffer, and sets size fields.\n\t// The second call returns the data.\n\tinitialBuf := make([]PDH_FMT_COUNTERVALUE_ITEM_DOUBLE, 1)\n\tret := pdhGetFormattedCounterArrayDouble(counter, &bufSize, &bufCount, &initialBuf[0])\n\tif ret == PDH_MORE_DATA {\n\t\t// we'll likely never get here, but be safe.\n\t\tif bufCount > maxQuerySize {\n\t\t\tbufCount = maxQuerySize\n\t\t}\n\t\tret = pdhGetFormattedCounterArrayDouble(counter, &bufSize, &bufCount, &counterResults[0])\n\t\tif ret == 0 {\n\t\t\trv := make([]float64, bufCount)\n\t\t\tfor i := 0; i < int(bufCount); i++ {\n\t\t\t\trv[i] = counterResults[i].FmtValue.DoubleValue\n\t\t\t}\n\t\t\treturn rv, nil\n\t\t}\n\t}\n\tif ret != 0 {\n\t\treturn nil, fmt.Errorf(\"getCounterArrayData failed - %d\", ret)\n\t}\n\n\treturn nil, nil\n}\n\n// getProcessImageName returns the name of the process image, as expected by\n// the performance counter API.\nfunc getProcessImageName() (name string) {\n\tname = filepath.Base(os.Args[0])\n\tname = strings.TrimSuffix(name, \".exe\")\n\treturn\n}\n\n// initialize our counters\nfunc initCounters() (err error) {\n\n\tprocessPid = os.Getpid()\n\t// require an addressible nil pointer\n\tvar source uint16\n\tif err := pdhOpenQuery(&source, 0, &pcHandle); err != nil {\n\t\treturn err\n\t}\n\n\t// setup the performance counters, search for all server instances\n\tname := fmt.Sprintf(\"%s*\", getProcessImageName())\n\tpidQuery := fmt.Sprintf(\"\\\\Process(%s)\\\\ID Process\", name)\n\tcpuQuery := fmt.Sprintf(\"\\\\Process(%s)\\\\%% Processor Time\", name)\n\trssQuery := fmt.Sprintf(\"\\\\Process(%s)\\\\Working Set - Private\", name)\n\tvssQuery := fmt.Sprintf(\"\\\\Process(%s)\\\\Virtual Bytes\", name)\n\n\tif err = pdhAddCounter(pcHandle, pidQuery, 0, &pidCounter); err != nil {\n\t\treturn err\n\t}\n\tif err = pdhAddCounter(pcHandle, cpuQuery, 0, &cpuCounter); err != nil {\n\t\treturn err\n\t}\n\tif err = pdhAddCounter(pcHandle, rssQuery, 0, &rssCounter); err != nil {\n\t\treturn err\n\t}\n\tif err = pdhAddCounter(pcHandle, vssQuery, 0, &vssCounter); err != nil {\n\t\treturn err\n\t}\n\n\t// prime the counters by collecting once, and sleep to get somewhat\n\t// useful information the first request.  Counters for the CPU require\n\t// at least two collect calls.\n\tif err = pdhCollectQueryData(pcHandle); err != nil {\n\t\treturn err\n\t}\n\ttime.Sleep(50)\n\n\treturn nil\n}\n\n// ProcUsage returns process CPU and memory statistics\nfunc ProcUsage(pcpu *float64, rss, vss *int64) error {\n\tvar err error\n\n\t// For simplicity, protect the entire call.\n\t// Most simultaneous requests will immediately return\n\t// with cached values.\n\tpcQueryLock.Lock()\n\tdefer pcQueryLock.Unlock()\n\n\t// First time through, initialize counters.\n\tif initialSample {\n\t\tif err = initCounters(); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tinitialSample = false\n\t} else if time.Since(lastSampleTime) < (2 * time.Second) {\n\t\t// only refresh every two seconds as to minimize impact\n\t\t// on the server.\n\t\t*pcpu = prevCPU\n\t\t*rss = prevRss\n\t\t*vss = prevVss\n\t\treturn nil\n\t}\n\n\t// always save the sample time, even on errors.\n\tdefer func() {\n\t\tlastSampleTime = time.Now()\n\t}()\n\n\t// refresh the performance counter data\n\tif err = pdhCollectQueryData(pcHandle); err != nil {\n\t\treturn err\n\t}\n\n\t// retrieve the data\n\tvar pidAry, cpuAry, rssAry, vssAry []float64\n\tif pidAry, err = getCounterArrayData(pidCounter); err != nil {\n\t\treturn err\n\t}\n\tif cpuAry, err = getCounterArrayData(cpuCounter); err != nil {\n\t\treturn err\n\t}\n\tif rssAry, err = getCounterArrayData(rssCounter); err != nil {\n\t\treturn err\n\t}\n\tif vssAry, err = getCounterArrayData(vssCounter); err != nil {\n\t\treturn err\n\t}\n\t// find the index of the entry for this process\n\tidx := int(-1)\n\tfor i := range pidAry {\n\t\tif int(pidAry[i]) == processPid {\n\t\t\tidx = i\n\t\t\tbreak\n\t\t}\n\t}\n\t// no pid found...\n\tif idx < 0 {\n\t\treturn fmt.Errorf(\"could not find pid in performance counter results\")\n\t}\n\t// assign values from the performance counters\n\t*pcpu = cpuAry[idx]\n\t*rss = int64(rssAry[idx])\n\t*vss = int64(vssAry[idx])\n\n\t// save off cache values\n\tprevCPU = *pcpu\n\tprevRss = *rss\n\tprevVss = *vss\n\n\treturn nil\n}\n",
    "source_file": "server/pse/pse_windows.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2022 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n//\n// Copied from pse_openbsd.go\n\npackage pse\n\nimport (\n\t\"fmt\"\n\t\"os\"\n\t\"os/exec\"\n)\n\n// ProcUsage returns CPU usage\nfunc ProcUsage(pcpu *float64, rss, vss *int64) error {\n\tpidStr := fmt.Sprintf(\"%d\", os.Getpid())\n\tout, err := exec.Command(\"ps\", \"o\", \"pcpu=,rss=,vsz=\", \"-p\", pidStr).Output()\n\tif err != nil {\n\t\t*rss, *vss = -1, -1\n\t\treturn fmt.Errorf(\"ps call failed:%v\", err)\n\t}\n\tfmt.Sscanf(string(out), \"%f %d %d\", pcpu, rss, vss)\n\t*rss *= 1024 // 1k blocks, want bytes.\n\t*vss *= 1024 // 1k blocks, want bytes.\n\treturn nil\n}\n",
    "source_file": "server/pse/pse_netbsd.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2015-2025 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n//go:build cgo && freebsd\n\npackage pse\n\n/*\n#include <sys/types.h>\n#include <sys/sysctl.h>\n#include <sys/user.h>\n#include <stddef.h>\n#include <unistd.h>\n\nlong pagetok(long size)\n{\n    int pageshift, pagesize;\n\n    pagesize = getpagesize();\n    pageshift = 0;\n\n    while (pagesize > 1) {\n        pageshift++;\n        pagesize >>= 1;\n    }\n\n    return (size << pageshift);\n}\n\nint getusage(double *pcpu, unsigned int *rss, unsigned int *vss)\n{\n    int mib[4], ret;\n    size_t len;\n    struct kinfo_proc kp;\n\n    len = 4;\n    sysctlnametomib(\"kern.proc.pid\", mib, &len);\n\n    mib[3] = getpid();\n    len = sizeof(kp);\n\n    ret = sysctl(mib, 4, &kp, &len, NULL, 0);\n    if (ret != 0) {\n        return (errno);\n    }\n\n    *rss = pagetok(kp.ki_rssize);\n    *vss = kp.ki_size;\n    *pcpu = (double)kp.ki_pctcpu / FSCALE;\n\n    return 0;\n}\n\n*/\nimport \"C\"\n\nimport (\n\t\"syscall\"\n)\n\n// This is a placeholder for now.\nfunc ProcUsage(pcpu *float64, rss, vss *int64) error {\n\tvar r, v C.uint\n\tvar c C.double\n\n\tif ret := C.getusage(&c, &r, &v); ret != 0 {\n\t\treturn syscall.Errno(ret)\n\t}\n\n\t*pcpu = float64(c)\n\t*rss = int64(r)\n\t*vss = int64(v)\n\n\treturn nil\n}\n",
    "source_file": "server/pse/pse_freebsd_cgo.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2015-2025 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n//go:build rumprun\n\npackage pse\n\n// This is a placeholder for now.\nfunc ProcUsage(pcpu *float64, rss, vss *int64) error {\n\t*pcpu = 0.0\n\t*rss = 0\n\t*vss = 0\n\n\treturn nil\n}\n",
    "source_file": "server/pse/pse_rumprun.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2022-2025 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n//go:build wasm\n\npackage pse\n\n// This is a placeholder for now.\nfunc ProcUsage(pcpu *float64, rss, vss *int64) error {\n\t*pcpu = 0.0\n\t*rss = 0\n\t*vss = 0\n\n\treturn nil\n}\n",
    "source_file": "server/pse/pse_wasm.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2015-2023 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n//\n// Copied from pse_openbsd.go\n\npackage pse\n\nimport (\n\t\"fmt\"\n\t\"os\"\n\t\"os/exec\"\n)\n\n// ProcUsage returns CPU usage\nfunc ProcUsage(pcpu *float64, rss, vss *int64) error {\n\tpidStr := fmt.Sprintf(\"%d\", os.Getpid())\n\tout, err := exec.Command(\"ps\", \"o\", \"pcpu=,rss=,vsz=\", \"-p\", pidStr).Output()\n\tif err != nil {\n\t\t*rss, *vss = -1, -1\n\t\treturn fmt.Errorf(\"ps call failed:%v\", err)\n\t}\n\tfmt.Sscanf(string(out), \"%f %d %d\", pcpu, rss, vss)\n\t*rss *= 1024 // 1k blocks, want bytes.\n\t*vss *= 1024 // 1k blocks, want bytes.\n\treturn nil\n}\n",
    "source_file": "server/pse/pse_dragonfly.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2015-2018 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage pse\n\n// This is a placeholder for now.\nfunc ProcUsage(pcpu *float64, rss, vss *int64) error {\n\t*pcpu = 0.0\n\t*rss = 0\n\t*vss = 0\n\n\treturn nil\n}\n",
    "source_file": "server/pse/pse_solaris.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2019-2025 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n//go:build freebsd || openbsd || dragonfly || netbsd\n\npackage sysmem\n\nfunc Memory() int64 {\n\treturn sysctlInt64(\"hw.physmem\")\n}\n",
    "source_file": "server/sysmem/mem_bsd.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2019-2025 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n//go:build darwin\n\npackage sysmem\n\nfunc Memory() int64 {\n\treturn sysctlInt64(\"hw.memsize\")\n}\n",
    "source_file": "server/sysmem/mem_darwin.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2019-2025 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n//go:build windows\n\npackage sysmem\n\nimport (\n\t\"unsafe\"\n\n\t\"golang.org/x/sys/windows\"\n)\n\nvar winKernel32 = windows.NewLazySystemDLL(\"kernel32.dll\")\nvar winGlobalMemoryStatusEx = winKernel32.NewProc(\"GlobalMemoryStatusEx\")\n\nfunc init() {\n\tif err := winKernel32.Load(); err != nil {\n\t\tpanic(err)\n\t}\n\tif err := winGlobalMemoryStatusEx.Find(); err != nil {\n\t\tpanic(err)\n\t}\n}\n\n// https://docs.microsoft.com/en-us/windows/win32/api/sysinfoapi/ns-sysinfoapi-memorystatusex\ntype _memoryStatusEx struct {\n\tdwLength     uint32\n\tdwMemoryLoad uint32\n\tullTotalPhys uint64\n\tunused       [6]uint64 // ignore rest of struct\n}\n\nfunc Memory() int64 {\n\tmsx := &_memoryStatusEx{dwLength: 64}\n\tres, _, _ := winGlobalMemoryStatusEx.Call(uintptr(unsafe.Pointer(msx)))\n\tif res == 0 {\n\t\treturn 0\n\t}\n\treturn int64(msx.ullTotalPhys)\n}\n",
    "source_file": "server/sysmem/mem_windows.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2022-2025 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n//go:build zos\n\npackage sysmem\n\nfunc Memory() int64 {\n\t// TODO: We don't know the system memory\n\treturn 0\n}\n",
    "source_file": "server/sysmem/mem_zos.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2022-2025 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n//go:build wasm\n\npackage sysmem\n\nfunc Memory() int64 {\n\t// TODO: We don't know the system memory\n\treturn 0\n}\n",
    "source_file": "server/sysmem/mem_wasm.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2019-2025 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n//go:build linux\n\npackage sysmem\n\nimport \"syscall\"\n\nfunc Memory() int64 {\n\tvar info syscall.Sysinfo_t\n\terr := syscall.Sysinfo(&info)\n\tif err != nil {\n\t\treturn 0\n\t}\n\treturn int64(info.Totalram) * int64(info.Unit)\n}\n",
    "source_file": "server/sysmem/mem_linux.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2019-2025 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n//go:build darwin || freebsd || openbsd || dragonfly || netbsd\n\npackage sysmem\n\nimport (\n\t\"syscall\"\n\t\"unsafe\"\n)\n\nfunc sysctlInt64(name string) int64 {\n\ts, err := syscall.Sysctl(name)\n\tif err != nil {\n\t\treturn 0\n\t}\n\t// Make sure it's 8 bytes when we do the cast below.\n\t// We were getting fatal error: checkptr: converted pointer straddles multiple allocations in go 1.22.1 on darwin.\n\tvar b [8]byte\n\tcopy(b[:], s)\n\treturn *(*int64)(unsafe.Pointer(&b[0]))\n}\n",
    "source_file": "server/sysmem/sysctl.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2024-2025 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage thw\n\nimport (\n\t\"encoding/binary\"\n\t\"errors\"\n\t\"io\"\n\t\"math\"\n\t\"time\"\n)\n\n// Error for when we can not locate a task for removal or updates.\nvar ErrTaskNotFound = errors.New(\"thw: task not found\")\n\n// Error for when we try to decode a binary-encoded THW with an unknown version number.\nvar ErrInvalidVersion = errors.New(\"thw: encoded version not known\")\n\nconst (\n\ttickDuration = int64(time.Second) // Tick duration in nanoseconds.\n\twheelBits    = 12                 // 2^12 = 4096 slots.\n\twheelSize    = 1 << wheelBits     // Number of slots in the wheel.\n\twheelMask    = wheelSize - 1      // Mask for calculating position.\n\theaderLen    = 17                 // 1 byte magic + 2x uint64s\n)\n\n// slot represents a single slot in the wheel.\ntype slot struct {\n\tentries map[uint64]int64 // Map of sequence to expires.\n\tlowest  int64            // Lowest expiration time in this slot.\n}\n\n// HashWheel represents the timing wheel.\ntype HashWheel struct {\n\twheel  []*slot // Array of slots.\n\tlowest int64   // Track the lowest expiration time across all slots.\n\tcount  uint64  // How many entries are present?\n}\n\n// NewHashWheel initializes a new HashWheel.\nfunc NewHashWheel() *HashWheel {\n\treturn &HashWheel{\n\t\twheel:  make([]*slot, wheelSize),\n\t\tlowest: math.MaxInt64,\n\t}\n}\n\n// getPosition calculates the slot position for a given expiration time.\nfunc (hw *HashWheel) getPosition(expires int64) int64 {\n\treturn (expires / tickDuration) & wheelMask\n}\n\n// updateLowestExpires finds the new lowest expiration time across all slots.\nfunc (hw *HashWheel) updateLowestExpires() {\n\tlowest := int64(math.MaxInt64)\n\tfor _, s := range hw.wheel {\n\t\tif s != nil && s.lowest < lowest {\n\t\t\tlowest = s.lowest\n\t\t}\n\t}\n\thw.lowest = lowest\n}\n\n// newSlot creates a new slot.\nfunc newSlot() *slot {\n\treturn &slot{\n\t\tentries: make(map[uint64]int64),\n\t\tlowest:  math.MaxInt64,\n\t}\n}\n\n// Add schedules a new timer task.\nfunc (hw *HashWheel) Add(seq uint64, expires int64) error {\n\tpos := hw.getPosition(expires)\n\t// Initialize the slot lazily.\n\tif hw.wheel[pos] == nil {\n\t\thw.wheel[pos] = newSlot()\n\t}\n\tif _, ok := hw.wheel[pos].entries[seq]; !ok {\n\t\thw.count++\n\t}\n\thw.wheel[pos].entries[seq] = expires\n\n\t// Update slot's lowest expiration if this is earlier.\n\tif expires < hw.wheel[pos].lowest {\n\t\thw.wheel[pos].lowest = expires\n\t\t// Update global lowest if this is now the earliest.\n\t\tif expires < hw.lowest {\n\t\t\thw.lowest = expires\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// Remove removes a timer task.\nfunc (hw *HashWheel) Remove(seq uint64, expires int64) error {\n\tpos := hw.getPosition(expires)\n\ts := hw.wheel[pos]\n\tif s == nil {\n\t\treturn ErrTaskNotFound\n\t}\n\tif _, exists := s.entries[seq]; !exists {\n\t\treturn ErrTaskNotFound\n\t}\n\tdelete(s.entries, seq)\n\thw.count--\n\n\t// If the slot is empty, we can set it to nil to free memory.\n\tif len(s.entries) == 0 {\n\t\thw.wheel[pos] = nil\n\t} else if expires == s.lowest {\n\t\t// Find new lowest in this slot.\n\t\tlowest := int64(math.MaxInt64)\n\t\tfor _, exp := range s.entries {\n\t\t\tif exp < lowest {\n\t\t\t\tlowest = exp\n\t\t\t}\n\t\t}\n\t\ts.lowest = lowest\n\t}\n\n\t// If we removed the global lowest, find the new one.\n\tif expires == hw.lowest {\n\t\thw.updateLowestExpires()\n\t}\n\n\treturn nil\n}\n\n// Update updates the expiration time of an existing timer task.\nfunc (hw *HashWheel) Update(seq uint64, oldExpires int64, newExpires int64) error {\n\t// Remove from old position.\n\tif err := hw.Remove(seq, oldExpires); err != nil {\n\t\treturn err\n\t}\n\t// Add to new position.\n\treturn hw.Add(seq, newExpires)\n}\n\n// ExpireTasks processes all expired tasks using a callback, but only expires a task if the callback returns true.\nfunc (hw *HashWheel) ExpireTasks(callback func(seq uint64, expires int64) bool) {\n\tnow := time.Now().UnixNano()\n\n\t// Quick return if nothing is expired.\n\tif hw.lowest > now {\n\t\treturn\n\t}\n\n\t// Start from the slot containing the lowest expiration.\n\tstartPos, exitPos := hw.getPosition(hw.lowest), hw.getPosition(now+tickDuration)\n\tvar updateLowest bool\n\n\tfor offset := int64(0); ; offset++ {\n\t\tpos := (startPos + offset) & wheelMask\n\t\tif pos == exitPos {\n\t\t\tif updateLowest {\n\t\t\t\thw.updateLowestExpires()\n\t\t\t}\n\t\t\treturn\n\t\t}\n\t\t// Grab our slot.\n\t\tslot := hw.wheel[pos]\n\t\tif slot == nil || slot.lowest > now {\n\t\t\tcontinue\n\t\t}\n\n\t\t// Track new lowest while processing expirations\n\t\tnewLowest := int64(math.MaxInt64)\n\t\tfor seq, expires := range slot.entries {\n\t\t\tif expires <= now && callback(seq, expires) {\n\t\t\t\tdelete(slot.entries, seq)\n\t\t\t\thw.count--\n\t\t\t\tupdateLowest = true\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif expires < newLowest {\n\t\t\t\tnewLowest = expires\n\t\t\t}\n\t\t}\n\n\t\t// Nil out if we are empty.\n\t\tif len(slot.entries) == 0 {\n\t\t\thw.wheel[pos] = nil\n\t\t} else {\n\t\t\tslot.lowest = newLowest\n\t\t}\n\t}\n}\n\n// GetNextExpiration returns the earliest expiration time before the given time.\n// Returns math.MaxInt64 if no expirations exist before the specified time.\nfunc (hw *HashWheel) GetNextExpiration(before int64) int64 {\n\tif hw.lowest < before {\n\t\treturn hw.lowest\n\t}\n\treturn math.MaxInt64\n}\n\n// Count returns the amount of tasks in the THW.\nfunc (hw *HashWheel) Count() uint64 {\n\treturn hw.count\n}\n\n// AppendEncode writes out the contents of the THW into a binary snapshot\n// and returns it. The high seq number is included in the snapshot and will\n// be returned on decode.\nfunc (hw *HashWheel) Encode(highSeq uint64) []byte {\n\tb := make([]byte, 0, headerLen+(hw.count*(2*binary.MaxVarintLen64)))\n\tb = append(b, 1)                                  // Magic version\n\tb = binary.LittleEndian.AppendUint64(b, hw.count) // Entry count\n\tb = binary.LittleEndian.AppendUint64(b, highSeq)  // Stamp\n\tfor _, slot := range hw.wheel {\n\t\tif slot == nil || slot.entries == nil {\n\t\t\tcontinue\n\t\t}\n\t\tfor v, ts := range slot.entries {\n\t\t\tb = binary.AppendVarint(b, ts)\n\t\t\tb = binary.AppendUvarint(b, v)\n\t\t}\n\t}\n\treturn b\n}\n\n// Decode snapshots a binary-encoded THW and replaces the contents of this\n// THW with them. Returns the high seq number from the snapshot.\nfunc (hw *HashWheel) Decode(b []byte) (uint64, error) {\n\tif len(b) < headerLen {\n\t\treturn 0, io.ErrShortBuffer\n\t}\n\tif b[0] != 1 {\n\t\treturn 0, ErrInvalidVersion\n\t}\n\thw.wheel = make([]*slot, wheelSize)\n\thw.lowest = math.MaxInt64\n\tcount := binary.LittleEndian.Uint64(b[1:])\n\tstamp := binary.LittleEndian.Uint64(b[9:])\n\tb = b[headerLen:]\n\tfor i := uint64(0); i < count; i++ {\n\t\tts, tn := binary.Varint(b)\n\t\tif tn < 0 {\n\t\t\treturn 0, io.ErrUnexpectedEOF\n\t\t}\n\t\tv, vn := binary.Uvarint(b[tn:])\n\t\tif vn < 0 {\n\t\t\treturn 0, io.ErrUnexpectedEOF\n\t\t}\n\t\thw.Add(v, ts)\n\t\tb = b[tn+vn:]\n\t}\n\treturn stamp, nil\n}\n",
    "source_file": "server/thw/thw.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2025 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage gsl\n\nimport (\n\t\"errors\"\n\t\"sync\"\n\t\"unsafe\"\n\n\t\"github.com/nats-io/nats-server/v2/server/stree\"\n)\n\n// Sublist is a routing mechanism to handle subject distribution and\n// provides a facility to match subjects from published messages to\n// interested subscribers. Subscribers can have wildcard subjects to\n// match multiple published subjects.\n\n// Common byte variables for wildcards and token separator.\nconst (\n\tpwc     = '*'\n\tpwcs    = \"*\"\n\tfwc     = '>'\n\tfwcs    = \">\"\n\ttsep    = \".\"\n\tbtsep   = '.'\n\t_EMPTY_ = \"\"\n)\n\n// Sublist related errors\nvar (\n\tErrInvalidSubject    = errors.New(\"gsl: invalid subject\")\n\tErrNotFound          = errors.New(\"gsl: no matches found\")\n\tErrNilChan           = errors.New(\"gsl: nil channel\")\n\tErrAlreadyRegistered = errors.New(\"gsl: notification already registered\")\n)\n\n// A GenericSublist stores and efficiently retrieves subscriptions.\ntype GenericSublist[T comparable] struct {\n\tsync.RWMutex\n\troot  *level[T]\n\tcount uint32\n}\n\n// A node contains subscriptions and a pointer to the next level.\ntype node[T comparable] struct {\n\tnext *level[T]\n\tsubs map[T]string // value -> subject\n}\n\n// A level represents a group of nodes and special pointers to\n// wildcard nodes.\ntype level[T comparable] struct {\n\tnodes    map[string]*node[T]\n\tpwc, fwc *node[T]\n}\n\n// Create a new default node.\nfunc newNode[T comparable]() *node[T] {\n\treturn &node[T]{subs: make(map[T]string)}\n}\n\n// Create a new default level.\nfunc newLevel[T comparable]() *level[T] {\n\treturn &level[T]{nodes: make(map[string]*node[T])}\n}\n\n// NewSublist will create a default sublist with caching enabled per the flag.\nfunc NewSublist[T comparable]() *GenericSublist[T] {\n\treturn &GenericSublist[T]{root: newLevel[T]()}\n}\n\n// Insert adds a subscription into the sublist\nfunc (s *GenericSublist[T]) Insert(subject string, value T) error {\n\ttsa := [32]string{}\n\ttokens := tsa[:0]\n\tstart := 0\n\tfor i := 0; i < len(subject); i++ {\n\t\tif subject[i] == btsep {\n\t\t\ttokens = append(tokens, subject[start:i])\n\t\t\tstart = i + 1\n\t\t}\n\t}\n\ttokens = append(tokens, subject[start:])\n\n\ts.Lock()\n\n\tvar sfwc bool\n\tvar n *node[T]\n\tl := s.root\n\n\tfor _, t := range tokens {\n\t\tlt := len(t)\n\t\tif lt == 0 || sfwc {\n\t\t\ts.Unlock()\n\t\t\treturn ErrInvalidSubject\n\t\t}\n\n\t\tif lt > 1 {\n\t\t\tn = l.nodes[t]\n\t\t} else {\n\t\t\tswitch t[0] {\n\t\t\tcase pwc:\n\t\t\t\tn = l.pwc\n\t\t\tcase fwc:\n\t\t\t\tn = l.fwc\n\t\t\t\tsfwc = true\n\t\t\tdefault:\n\t\t\t\tn = l.nodes[t]\n\t\t\t}\n\t\t}\n\t\tif n == nil {\n\t\t\tn = newNode[T]()\n\t\t\tif lt > 1 {\n\t\t\t\tl.nodes[t] = n\n\t\t\t} else {\n\t\t\t\tswitch t[0] {\n\t\t\t\tcase pwc:\n\t\t\t\t\tl.pwc = n\n\t\t\t\tcase fwc:\n\t\t\t\t\tl.fwc = n\n\t\t\t\tdefault:\n\t\t\t\t\tl.nodes[t] = n\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tif n.next == nil {\n\t\t\tn.next = newLevel[T]()\n\t\t}\n\t\tl = n.next\n\t}\n\n\tn.subs[value] = subject\n\n\ts.count++\n\ts.Unlock()\n\n\treturn nil\n}\n\n// Match will match all entries to the literal subject.\n// It will return a set of results for both normal and queue subscribers.\nfunc (s *GenericSublist[T]) Match(subject string, cb func(T)) {\n\ts.match(subject, cb, true)\n}\n\n// MatchBytes will match all entries to the literal subject.\n// It will return a set of results for both normal and queue subscribers.\nfunc (s *GenericSublist[T]) MatchBytes(subject []byte, cb func(T)) {\n\ts.match(string(subject), cb, true)\n}\n\n// HasInterest will return whether or not there is any interest in the subject.\n// In cases where more detail is not required, this may be faster than Match.\nfunc (s *GenericSublist[T]) HasInterest(subject string) bool {\n\treturn s.hasInterest(subject, true, nil)\n}\n\n// NumInterest will return the number of subs interested in the subject.\n// In cases where more detail is not required, this may be faster than Match.\nfunc (s *GenericSublist[T]) NumInterest(subject string) (np int) {\n\ts.hasInterest(subject, true, &np)\n\treturn\n}\n\nfunc (s *GenericSublist[T]) match(subject string, cb func(T), doLock bool) {\n\ttsa := [32]string{}\n\ttokens := tsa[:0]\n\tstart := 0\n\tfor i := 0; i < len(subject); i++ {\n\t\tif subject[i] == btsep {\n\t\t\tif i-start == 0 {\n\t\t\t\treturn\n\t\t\t}\n\t\t\ttokens = append(tokens, subject[start:i])\n\t\t\tstart = i + 1\n\t\t}\n\t}\n\tif start >= len(subject) {\n\t\treturn\n\t}\n\ttokens = append(tokens, subject[start:])\n\n\tif doLock {\n\t\ts.RLock()\n\t\tdefer s.RUnlock()\n\t}\n\tmatchLevel(s.root, tokens, cb)\n}\n\nfunc (s *GenericSublist[T]) hasInterest(subject string, doLock bool, np *int) bool {\n\ttsa := [32]string{}\n\ttokens := tsa[:0]\n\tstart := 0\n\tfor i := 0; i < len(subject); i++ {\n\t\tif subject[i] == btsep {\n\t\t\tif i-start == 0 {\n\t\t\t\treturn false\n\t\t\t}\n\t\t\ttokens = append(tokens, subject[start:i])\n\t\t\tstart = i + 1\n\t\t}\n\t}\n\tif start >= len(subject) {\n\t\treturn false\n\t}\n\ttokens = append(tokens, subject[start:])\n\n\tif doLock {\n\t\ts.RLock()\n\t\tdefer s.RUnlock()\n\t}\n\treturn matchLevelForAny(s.root, tokens, np)\n}\n\nfunc matchLevelForAny[T comparable](l *level[T], toks []string, np *int) bool {\n\tvar pwc, n *node[T]\n\tfor i, t := range toks {\n\t\tif l == nil {\n\t\t\treturn false\n\t\t}\n\t\tif l.fwc != nil {\n\t\t\tif np != nil {\n\t\t\t\t*np += len(l.fwc.subs)\n\t\t\t}\n\t\t\treturn true\n\t\t}\n\t\tif pwc = l.pwc; pwc != nil {\n\t\t\tif match := matchLevelForAny(pwc.next, toks[i+1:], np); match {\n\t\t\t\treturn true\n\t\t\t}\n\t\t}\n\t\tn = l.nodes[t]\n\t\tif n != nil {\n\t\t\tl = n.next\n\t\t} else {\n\t\t\tl = nil\n\t\t}\n\t}\n\tif n != nil {\n\t\tif np != nil {\n\t\t\t*np += len(n.subs)\n\t\t}\n\t\treturn len(n.subs) > 0\n\t}\n\tif pwc != nil {\n\t\tif np != nil {\n\t\t\t*np += len(pwc.subs)\n\t\t}\n\t\treturn len(pwc.subs) > 0\n\t}\n\treturn false\n}\n\n// callbacksForResults will make the necessary callbacks for each\n// result in this node.\nfunc callbacksForResults[T comparable](n *node[T], cb func(T)) {\n\tfor sub := range n.subs {\n\t\tcb(sub)\n\t}\n}\n\n// matchLevel is used to recursively descend into the trie.\nfunc matchLevel[T comparable](l *level[T], toks []string, cb func(T)) {\n\tvar pwc, n *node[T]\n\tfor i, t := range toks {\n\t\tif l == nil {\n\t\t\treturn\n\t\t}\n\t\tif l.fwc != nil {\n\t\t\tcallbacksForResults(l.fwc, cb)\n\t\t}\n\t\tif pwc = l.pwc; pwc != nil {\n\t\t\tmatchLevel(pwc.next, toks[i+1:], cb)\n\t\t}\n\t\tn = l.nodes[t]\n\t\tif n != nil {\n\t\t\tl = n.next\n\t\t} else {\n\t\t\tl = nil\n\t\t}\n\t}\n\tif n != nil {\n\t\tcallbacksForResults(n, cb)\n\t}\n\tif pwc != nil {\n\t\tcallbacksForResults(pwc, cb)\n\t}\n}\n\n// lnt is used to track descent into levels for a removal for pruning.\ntype lnt[T comparable] struct {\n\tl *level[T]\n\tn *node[T]\n\tt string\n}\n\n// Raw low level remove, can do batches with lock held outside.\nfunc (s *GenericSublist[T]) remove(subject string, value T, shouldLock bool) error {\n\ttsa := [32]string{}\n\ttokens := tsa[:0]\n\tstart := 0\n\tfor i := 0; i < len(subject); i++ {\n\t\tif subject[i] == btsep {\n\t\t\ttokens = append(tokens, subject[start:i])\n\t\t\tstart = i + 1\n\t\t}\n\t}\n\ttokens = append(tokens, subject[start:])\n\n\tif shouldLock {\n\t\ts.Lock()\n\t\tdefer s.Unlock()\n\t}\n\n\tvar sfwc bool\n\tvar n *node[T]\n\tl := s.root\n\n\t// Track levels for pruning\n\tvar lnts [32]lnt[T]\n\tlevels := lnts[:0]\n\n\tfor _, t := range tokens {\n\t\tlt := len(t)\n\t\tif lt == 0 || sfwc {\n\t\t\treturn ErrInvalidSubject\n\t\t}\n\t\tif l == nil {\n\t\t\treturn ErrNotFound\n\t\t}\n\t\tif lt > 1 {\n\t\t\tn = l.nodes[t]\n\t\t} else {\n\t\t\tswitch t[0] {\n\t\t\tcase pwc:\n\t\t\t\tn = l.pwc\n\t\t\tcase fwc:\n\t\t\t\tn = l.fwc\n\t\t\t\tsfwc = true\n\t\t\tdefault:\n\t\t\t\tn = l.nodes[t]\n\t\t\t}\n\t\t}\n\t\tif n != nil {\n\t\t\tlevels = append(levels, lnt[T]{l, n, t})\n\t\t\tl = n.next\n\t\t} else {\n\t\t\tl = nil\n\t\t}\n\t}\n\n\tif !s.removeFromNode(n, value) {\n\t\treturn ErrNotFound\n\t}\n\n\ts.count--\n\n\tfor i := len(levels) - 1; i >= 0; i-- {\n\t\tl, n, t := levels[i].l, levels[i].n, levels[i].t\n\t\tif n.isEmpty() {\n\t\t\tl.pruneNode(n, t)\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// Remove will remove a subscription.\nfunc (s *GenericSublist[T]) Remove(subject string, value T) error {\n\treturn s.remove(subject, value, true)\n}\n\n// pruneNode is used to prune an empty node from the tree.\nfunc (l *level[T]) pruneNode(n *node[T], t string) {\n\tif n == nil {\n\t\treturn\n\t}\n\tif n == l.fwc {\n\t\tl.fwc = nil\n\t} else if n == l.pwc {\n\t\tl.pwc = nil\n\t} else {\n\t\tdelete(l.nodes, t)\n\t}\n}\n\n// isEmpty will test if the node has any entries. Used\n// in pruning.\nfunc (n *node[T]) isEmpty() bool {\n\treturn len(n.subs) == 0 && (n.next == nil || n.next.numNodes() == 0)\n}\n\n// Return the number of nodes for the given level.\nfunc (l *level[T]) numNodes() int {\n\tnum := len(l.nodes)\n\tif l.pwc != nil {\n\t\tnum++\n\t}\n\tif l.fwc != nil {\n\t\tnum++\n\t}\n\treturn num\n}\n\n// Remove the sub for the given node.\nfunc (s *GenericSublist[T]) removeFromNode(n *node[T], value T) (found bool) {\n\tif n == nil {\n\t\treturn false\n\t}\n\tif _, found = n.subs[value]; found {\n\t\tdelete(n.subs, value)\n\t}\n\treturn found\n}\n\n// Count returns the number of subscriptions.\nfunc (s *GenericSublist[T]) Count() uint32 {\n\ts.RLock()\n\tdefer s.RUnlock()\n\treturn s.count\n}\n\n// numLevels will return the maximum number of levels\n// contained in the Sublist tree.\nfunc (s *GenericSublist[T]) numLevels() int {\n\treturn visitLevel(s.root, 0)\n}\n\n// visitLevel is used to descend the Sublist tree structure\n// recursively.\nfunc visitLevel[T comparable](l *level[T], depth int) int {\n\tif l == nil || l.numNodes() == 0 {\n\t\treturn depth\n\t}\n\n\tdepth++\n\tmaxDepth := depth\n\n\tfor _, n := range l.nodes {\n\t\tif n == nil {\n\t\t\tcontinue\n\t\t}\n\t\tnewDepth := visitLevel(n.next, depth)\n\t\tif newDepth > maxDepth {\n\t\t\tmaxDepth = newDepth\n\t\t}\n\t}\n\tif l.pwc != nil {\n\t\tpwcDepth := visitLevel(l.pwc.next, depth)\n\t\tif pwcDepth > maxDepth {\n\t\t\tmaxDepth = pwcDepth\n\t\t}\n\t}\n\tif l.fwc != nil {\n\t\tfwcDepth := visitLevel(l.fwc.next, depth)\n\t\tif fwcDepth > maxDepth {\n\t\t\tmaxDepth = fwcDepth\n\t\t}\n\t}\n\treturn maxDepth\n}\n\n// IntersectStree will match all items in the given subject tree that\n// have interest expressed in the given sublist. The callback will only be called\n// once for each subject, regardless of overlapping subscriptions in the sublist.\nfunc IntersectStree[T1 any, T2 comparable](st *stree.SubjectTree[T1], sl *GenericSublist[T2], cb func(subj []byte, entry *T1)) {\n\tvar _subj [255]byte\n\tintersectStree(st, sl.root, _subj[:0], cb)\n}\n\nfunc intersectStree[T1 any, T2 comparable](st *stree.SubjectTree[T1], r *level[T2], subj []byte, cb func(subj []byte, entry *T1)) {\n\tnsubj := subj\n\tif len(nsubj) > 0 {\n\t\tnsubj = append(subj, '.')\n\t}\n\tswitch {\n\tcase r.fwc != nil:\n\t\t// We've reached a full wildcard, do a FWC match on the stree at this point\n\t\t// and don't keep iterating downward.\n\t\tnsubj := append(nsubj, '>')\n\t\tst.Match(nsubj, cb)\n\tcase r.pwc != nil:\n\t\t// We've found a partial wildcard. We'll keep iterating downwards, but first\n\t\t// check whether there's interest at this level (without triggering dupes) and\n\t\t// match if so.\n\t\tnsubj := append(nsubj, '*')\n\t\tif len(r.pwc.subs) > 0 {\n\t\t\tst.Match(nsubj, cb)\n\t\t}\n\t\tif r.pwc.next != nil && r.pwc.next.numNodes() > 0 {\n\t\t\tintersectStree(st, r.pwc.next, nsubj, cb)\n\t\t}\n\tdefault:\n\t\t// Normal node with subject literals, keep iterating.\n\t\tfor t, n := range r.nodes {\n\t\t\tnsubj := append(nsubj, t...)\n\t\t\tif len(n.subs) > 0 {\n\t\t\t\tif subjectHasWildcard(bytesToString(nsubj)) {\n\t\t\t\t\tst.Match(nsubj, cb)\n\t\t\t\t} else {\n\t\t\t\t\tif e, ok := st.Find(nsubj); ok {\n\t\t\t\t\t\tcb(nsubj, e)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tif n.next != nil && n.next.numNodes() > 0 {\n\t\t\t\tintersectStree(st, n.next, nsubj, cb)\n\t\t\t}\n\t\t}\n\t}\n}\n\n// Determine if a subject has any wildcard tokens.\nfunc subjectHasWildcard(subject string) bool {\n\t// This one exits earlier then !subjectIsLiteral(subject)\n\tfor i, c := range subject {\n\t\tif c == pwc || c == fwc {\n\t\t\tif (i == 0 || subject[i-1] == btsep) &&\n\t\t\t\t(i+1 == len(subject) || subject[i+1] == btsep) {\n\t\t\t\treturn true\n\t\t\t}\n\t\t}\n\t}\n\treturn false\n}\n\n// Note this will avoid a copy of the data used for the string, but it will also reference the existing slice's data pointer.\n// So this should be used sparingly when we know the encompassing byte slice's lifetime is the same.\nfunc bytesToString(b []byte) string {\n\tif len(b) == 0 {\n\t\treturn _EMPTY_\n\t}\n\tp := unsafe.SliceData(b)\n\treturn unsafe.String(p, len(b))\n}\n",
    "source_file": "server/gsl/gsl.go",
    "chunk_type": "code"
  },
  {
    "content": "-----BEGIN NATS USER JWT-----\neyJ0eXAiOiJKV1QiLCJhbGciOiJlZDI1NTE5LW5rZXkifQ.eyJqdGkiOiJHRUNQVEpISE1TM01DTUtMVFBHWUdBTzQ1R1E2TjZRUFlXUTRHUExBRUIzM1ZDUkpOUlZRIiwiaWF0IjoxNjE2MjQ3MjMyLCJpc3MiOiJBQlZSWktKNlo3TklNUElZSlJDSEVZRlJVTzdFTk42TldPS1FERkxGREZWUFNNMzZVUFgyVUNQUCIsIm5hbWUiOiJvbmUiLCJzdWIiOiJVRENJQkdHR0hDSkJRUE9PNFNDSkpCSFpEUjM3TlFHR0NJV01ORzJEREFLSjZXTUtCTUFLWElNTyIsIm5hdHMiOnsicHViIjp7fSwic3ViIjp7fSwic3VicyI6LTEsImRhdGEiOi0xLCJwYXlsb2FkIjotMSwidHlwZSI6InVzZXIiLCJ2ZXJzaW9uIjoyfX0.VLhSDtGZEF_jdvgmhgkdISXAt5wFMMZxxwm5w8UrsnlM1hkUvtxBlTe4IP0xJIf4xf8JOR2Bmf73xUGJKUZECQ\n------END NATS USER JWT------\n\n************************* IMPORTANT *************************\nNKEY Seed printed below can be used to sign and prove identity.\nNKEYs are sensitive and should be treated as secrets.\n\n-----BEGIN USER NKEY SEED-----\nSUAPCDMU5TSHHLWUUZSOUABJXP2GXRCZVEOVWPSVM5XRSXYGQMRRFDYNMY\n------END USER NKEY SEED------\n\n*************************************************************\n",
    "source_file": "server/configs/one.creds",
    "chunk_type": "unknown"
  },
  {
    "content": "// Copyright 2023-2024 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage stree\n\n// Node with 256 children\n// Order of struct fields for best memory alignment (as per govet/fieldalignment)\ntype node256 struct {\n\tchild [256]node\n\tmeta\n}\n\nfunc newNode256(prefix []byte) *node256 {\n\tnn := &node256{}\n\tnn.setPrefix(prefix)\n\treturn nn\n}\n\nfunc (n *node256) addChild(c byte, nn node) {\n\tn.child[c] = nn\n\tn.size++\n}\n\nfunc (n *node256) findChild(c byte) *node {\n\tif n.child[c] != nil {\n\t\treturn &n.child[c]\n\t}\n\treturn nil\n}\n\nfunc (n *node256) isFull() bool { return false }\nfunc (n *node256) grow() node   { panic(\"grow can not be called on node256\") }\n\n// Deletes a child from the node.\nfunc (n *node256) deleteChild(c byte) {\n\tif n.child[c] != nil {\n\t\tn.child[c] = nil\n\t\tn.size--\n\t}\n}\n\n// Shrink if needed and return new node, otherwise return nil.\nfunc (n *node256) shrink() node {\n\tif n.size > 48 {\n\t\treturn nil\n\t}\n\tnn := newNode48(nil)\n\tfor c, child := range n.child {\n\t\tif child != nil {\n\t\t\tnn.addChild(byte(c), n.child[c])\n\t\t}\n\t}\n\treturn nn\n}\n\n// Iterate over all children calling func f.\nfunc (n *node256) iter(f func(node) bool) {\n\tfor i := 0; i < 256; i++ {\n\t\tif n.child[i] != nil {\n\t\t\tif !f(n.child[i]) {\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}\n}\n\n// Return our children as a slice.\nfunc (n *node256) children() []node {\n\treturn n.child[:256]\n}\n",
    "source_file": "server/stree/node256.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2023-2024 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage stree\n\n// Node with 10 children\n// This node size is for the particular case that a part of the subject is numeric\n// in nature, i.e. it only needs to satisfy the range 0-9 without wasting bytes\n// Order of struct fields for best memory alignment (as per govet/fieldalignment)\ntype node10 struct {\n\tchild [10]node\n\tmeta\n\tkey [10]byte\n}\n\nfunc newNode10(prefix []byte) *node10 {\n\tnn := &node10{}\n\tnn.setPrefix(prefix)\n\treturn nn\n}\n\n// Currently we do not keep node10 sorted or use bitfields for traversal so just add to the end.\n// TODO(dlc) - We should revisit here with more detailed benchmarks.\nfunc (n *node10) addChild(c byte, nn node) {\n\tif n.size >= 10 {\n\t\tpanic(\"node10 full!\")\n\t}\n\tn.key[n.size] = c\n\tn.child[n.size] = nn\n\tn.size++\n}\n\nfunc (n *node10) findChild(c byte) *node {\n\tfor i := uint16(0); i < n.size; i++ {\n\t\tif n.key[i] == c {\n\t\t\treturn &n.child[i]\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc (n *node10) isFull() bool { return n.size >= 10 }\n\nfunc (n *node10) grow() node {\n\tnn := newNode16(n.prefix)\n\tfor i := 0; i < 10; i++ {\n\t\tnn.addChild(n.key[i], n.child[i])\n\t}\n\treturn nn\n}\n\n// Deletes a child from the node.\nfunc (n *node10) deleteChild(c byte) {\n\tfor i, last := uint16(0), n.size-1; i < n.size; i++ {\n\t\tif n.key[i] == c {\n\t\t\t// Unsorted so just swap in last one here, else nil if last.\n\t\t\tif i < last {\n\t\t\t\tn.key[i] = n.key[last]\n\t\t\t\tn.child[i] = n.child[last]\n\t\t\t\tn.key[last] = 0\n\t\t\t\tn.child[last] = nil\n\t\t\t} else {\n\t\t\t\tn.key[i] = 0\n\t\t\t\tn.child[i] = nil\n\t\t\t}\n\t\t\tn.size--\n\t\t\treturn\n\t\t}\n\t}\n}\n\n// Shrink if needed and return new node, otherwise return nil.\nfunc (n *node10) shrink() node {\n\tif n.size > 4 {\n\t\treturn nil\n\t}\n\tnn := newNode4(nil)\n\tfor i := uint16(0); i < n.size; i++ {\n\t\tnn.addChild(n.key[i], n.child[i])\n\t}\n\treturn nn\n}\n\n// Iterate over all children calling func f.\nfunc (n *node10) iter(f func(node) bool) {\n\tfor i := uint16(0); i < n.size; i++ {\n\t\tif !f(n.child[i]) {\n\t\t\treturn\n\t\t}\n\t}\n}\n\n// Return our children as a slice.\nfunc (n *node10) children() []node {\n\treturn n.child[:n.size]\n}\n",
    "source_file": "server/stree/node10.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2023-2025 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage stree\n\n// For subject matching.\nconst (\n\tpwc  = '*'\n\tfwc  = '>'\n\ttsep = '.'\n)\n\n// Determine index of common prefix. No match at all is 0, etc.\nfunc commonPrefixLen(s1, s2 []byte) int {\n\tlimit := min(len(s1), len(s2))\n\tvar i int\n\tfor ; i < limit; i++ {\n\t\tif s1[i] != s2[i] {\n\t\t\tbreak\n\t\t}\n\t}\n\treturn i\n}\n\n// Helper to copy bytes.\nfunc copyBytes(src []byte) []byte {\n\tif len(src) == 0 {\n\t\treturn nil\n\t}\n\tdst := make([]byte, len(src))\n\tcopy(dst, src)\n\treturn dst\n}\n\ntype position interface{ int | uint16 }\n\n// No pivot available.\nconst noPivot = byte(127)\n\n// Can return 127 (DEL) if we have all the subject as prefixes.\n// We used to use 0, but when that was in the subject would cause infinite recursion in some situations.\nfunc pivot[N position](subject []byte, pos N) byte {\n\tif int(pos) >= len(subject) {\n\t\treturn noPivot\n\t}\n\treturn subject[pos]\n}\n",
    "source_file": "server/stree/util.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2023-2024 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage stree\n\n// Node with 4 children\n// Order of struct fields for best memory alignment (as per govet/fieldalignment)\ntype node4 struct {\n\tchild [4]node\n\tmeta\n\tkey [4]byte\n}\n\nfunc newNode4(prefix []byte) *node4 {\n\tnn := &node4{}\n\tnn.setPrefix(prefix)\n\treturn nn\n}\n\n// Currently we do not need to keep sorted for traversal so just add to the end.\nfunc (n *node4) addChild(c byte, nn node) {\n\tif n.size >= 4 {\n\t\tpanic(\"node4 full!\")\n\t}\n\tn.key[n.size] = c\n\tn.child[n.size] = nn\n\tn.size++\n}\n\nfunc (n *node4) findChild(c byte) *node {\n\tfor i := uint16(0); i < n.size; i++ {\n\t\tif n.key[i] == c {\n\t\t\treturn &n.child[i]\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc (n *node4) isFull() bool { return n.size >= 4 }\n\nfunc (n *node4) grow() node {\n\tnn := newNode10(n.prefix)\n\tfor i := 0; i < 4; i++ {\n\t\tnn.addChild(n.key[i], n.child[i])\n\t}\n\treturn nn\n}\n\n// Deletes a child from the node.\nfunc (n *node4) deleteChild(c byte) {\n\tfor i, last := uint16(0), n.size-1; i < n.size; i++ {\n\t\tif n.key[i] == c {\n\t\t\t// Unsorted so just swap in last one here, else nil if last.\n\t\t\tif i < last {\n\t\t\t\tn.key[i] = n.key[last]\n\t\t\t\tn.child[i] = n.child[last]\n\t\t\t\tn.key[last] = 0\n\t\t\t\tn.child[last] = nil\n\t\t\t} else {\n\t\t\t\tn.key[i] = 0\n\t\t\t\tn.child[i] = nil\n\t\t\t}\n\t\t\tn.size--\n\t\t\treturn\n\t\t}\n\t}\n}\n\n// Shrink if needed and return new node, otherwise return nil.\nfunc (n *node4) shrink() node {\n\tif n.size == 1 {\n\t\treturn n.child[0]\n\t}\n\treturn nil\n}\n\n// Iterate over all children calling func f.\nfunc (n *node4) iter(f func(node) bool) {\n\tfor i := uint16(0); i < n.size; i++ {\n\t\tif !f(n.child[i]) {\n\t\t\treturn\n\t\t}\n\t}\n}\n\n// Return our children as a slice.\nfunc (n *node4) children() []node {\n\treturn n.child[:n.size]\n}\n",
    "source_file": "server/stree/node4.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2023-2024 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage stree\n\nimport (\n\t\"bytes\"\n)\n\n// Leaf node\n// Order of struct fields for best memory alignment (as per govet/fieldalignment)\ntype leaf[T any] struct {\n\tvalue T\n\t// This could be the whole subject, but most likely just the suffix portion.\n\t// We will only store the suffix here and assume all prior prefix paths have\n\t// been checked once we arrive at this leafnode.\n\tsuffix []byte\n}\n\nfunc newLeaf[T any](suffix []byte, value T) *leaf[T] {\n\treturn &leaf[T]{value, copyBytes(suffix)}\n}\n\nfunc (n *leaf[T]) isLeaf() bool                               { return true }\nfunc (n *leaf[T]) base() *meta                                { return nil }\nfunc (n *leaf[T]) match(subject []byte) bool                  { return bytes.Equal(subject, n.suffix) }\nfunc (n *leaf[T]) setSuffix(suffix []byte)                    { n.suffix = copyBytes(suffix) }\nfunc (n *leaf[T]) isFull() bool                               { return true }\nfunc (n *leaf[T]) matchParts(parts [][]byte) ([][]byte, bool) { return matchParts(parts, n.suffix) }\nfunc (n *leaf[T]) iter(f func(node) bool)                     {}\nfunc (n *leaf[T]) children() []node                           { return nil }\nfunc (n *leaf[T]) numChildren() uint16                        { return 0 }\nfunc (n *leaf[T]) path() []byte                               { return n.suffix }\n\n// Not applicable to leafs and should not be called, so panic if we do.\nfunc (n *leaf[T]) setPrefix(pre []byte)    { panic(\"setPrefix called on leaf\") }\nfunc (n *leaf[T]) addChild(_ byte, _ node) { panic(\"addChild called on leaf\") }\nfunc (n *leaf[T]) findChild(_ byte) *node  { panic(\"findChild called on leaf\") }\nfunc (n *leaf[T]) grow() node              { panic(\"grow called on leaf\") }\nfunc (n *leaf[T]) deleteChild(_ byte)      { panic(\"deleteChild called on leaf\") }\nfunc (n *leaf[T]) shrink() node            { panic(\"shrink called on leaf\") }\n",
    "source_file": "server/stree/leaf.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2023-2025 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage stree\n\nimport (\n\t\"bytes\"\n)\n\n// genParts will break a filter subject up into parts.\n// We need to break this up into chunks based on wildcards, either pwc '*' or fwc '>'.\n// We do not care about other tokens per se, just parts that are separated by wildcards with an optional end fwc.\nfunc genParts(filter []byte, parts [][]byte) [][]byte {\n\tvar start int\n\tfor i, e := 0, len(filter)-1; i < len(filter); i++ {\n\t\tif filter[i] == tsep {\n\t\t\t// See if next token is pwc. Either internal or end pwc.\n\t\t\tif i < e && filter[i+1] == pwc && (i+2 <= e && filter[i+2] == tsep || i+1 == e) {\n\t\t\t\tif i > start {\n\t\t\t\t\tparts = append(parts, filter[start:i+1])\n\t\t\t\t}\n\t\t\t\tparts = append(parts, filter[i+1:i+2])\n\t\t\t\ti++ // Skip pwc\n\t\t\t\tif i+2 <= e {\n\t\t\t\t\ti++ // Skip next tsep from next part too.\n\t\t\t\t}\n\t\t\t\tstart = i + 1\n\t\t\t} else if i < e && filter[i+1] == fwc && i+1 == e {\n\t\t\t\t// We have a fwc\n\t\t\t\tif i > start {\n\t\t\t\t\tparts = append(parts, filter[start:i+1])\n\t\t\t\t}\n\t\t\t\tparts = append(parts, filter[i+1:i+2])\n\t\t\t\ti++ // Skip fwc\n\t\t\t\tstart = i + 1\n\t\t\t}\n\t\t} else if filter[i] == pwc || filter[i] == fwc {\n\t\t\t// Wildcard must be at the start or preceded by tsep.\n\t\t\tif prev := i - 1; prev >= 0 && filter[prev] != tsep {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\t// Wildcard must be at the end or followed by tsep.\n\t\t\tif next := i + 1; next == e || next < e && filter[next] != tsep {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\t// We start with a pwc or fwc.\n\t\t\tparts = append(parts, filter[i:i+1])\n\t\t\tif i+1 <= e {\n\t\t\t\ti++ // Skip next tsep from next part too.\n\t\t\t}\n\t\t\tstart = i + 1\n\t\t}\n\t}\n\tif start < len(filter) {\n\t\t// Check to see if we need to eat a leading tsep.\n\t\tif filter[start] == tsep {\n\t\t\tstart++\n\t\t}\n\t\tparts = append(parts, filter[start:])\n\t}\n\treturn parts\n}\n\n// Match our parts against a fragment, which could be prefix for nodes or a suffix for leafs.\nfunc matchParts(parts [][]byte, frag []byte) ([][]byte, bool) {\n\tlf := len(frag)\n\tif lf == 0 {\n\t\treturn parts, true\n\t}\n\n\tvar si int\n\tlpi := len(parts) - 1\n\n\tfor i, part := range parts {\n\t\tif si >= lf {\n\t\t\treturn parts[i:], true\n\t\t}\n\t\tlp := len(part)\n\t\t// Check for pwc or fwc place holders.\n\t\tif lp == 1 {\n\t\t\tif part[0] == pwc {\n\t\t\t\tindex := bytes.IndexByte(frag[si:], tsep)\n\t\t\t\t// We are trying to match pwc and did not find our tsep.\n\t\t\t\t// Will need to move to next node from caller.\n\t\t\t\tif index < 0 {\n\t\t\t\t\tif i == lpi {\n\t\t\t\t\t\treturn nil, true\n\t\t\t\t\t}\n\t\t\t\t\treturn parts[i:], true\n\t\t\t\t}\n\t\t\t\tsi += index + 1\n\t\t\t\tcontinue\n\t\t\t} else if part[0] == fwc {\n\t\t\t\t// If we are here we should be good.\n\t\t\t\treturn nil, true\n\t\t\t}\n\t\t}\n\t\tend := min(si+lp, lf)\n\t\t// If part is bigger then the remaining fragment, adjust to a portion on the part.\n\t\tif si+lp > end {\n\t\t\t// Frag is smaller then part itself.\n\t\t\tpart = part[:end-si]\n\t\t}\n\t\tif !bytes.Equal(part, frag[si:end]) {\n\t\t\treturn parts, false\n\t\t}\n\t\t// If we still have a portion of the fragment left, update and continue.\n\t\tif end < lf {\n\t\t\tsi = end\n\t\t\tcontinue\n\t\t}\n\t\t// If we matched a partial, do not move past current part\n\t\t// but update the part to what was consumed. This allows upper layers to continue.\n\t\tif end < si+lp {\n\t\t\tif end >= lf {\n\t\t\t\t// Create a copy before modifying. Reuse slice capacity available at the\n\t\t\t\t// end of the parts slice, since this saves us additional allocations.\n\t\t\t\tlp := len(parts)\n\t\t\t\tparts = append(parts[lp:], parts[:lp]...)\n\t\t\t\tparts[i] = parts[i][lf-si:]\n\t\t\t} else {\n\t\t\t\ti++\n\t\t\t}\n\t\t\treturn parts[i:], true\n\t\t}\n\t\tif i == lpi {\n\t\t\treturn nil, true\n\t\t}\n\t\t// If we are here we are not the last part which means we have a wildcard\n\t\t// gap, so we need to match anything up to next tsep.\n\t\tsi += len(part)\n\t}\n\treturn parts, false\n}\n",
    "source_file": "server/stree/parts.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2023-2024 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage stree\n\n// Node with 48 children\n// Memory saving vs node256 comes from the fact that the child array is 16 bytes\n// per `node` entry, so node256's 256*16=4096 vs node48's 256+(48*16)=1024\n// Note that `key` is effectively 1-indexed, as 0 means no entry, so offset by 1\n// Order of struct fields for best memory alignment (as per govet/fieldalignment)\ntype node48 struct {\n\tchild [48]node\n\tmeta\n\tkey [256]byte\n}\n\nfunc newNode48(prefix []byte) *node48 {\n\tnn := &node48{}\n\tnn.setPrefix(prefix)\n\treturn nn\n}\n\nfunc (n *node48) addChild(c byte, nn node) {\n\tif n.size >= 48 {\n\t\tpanic(\"node48 full!\")\n\t}\n\tn.child[n.size] = nn\n\tn.key[c] = byte(n.size + 1) // 1-indexed\n\tn.size++\n}\n\nfunc (n *node48) findChild(c byte) *node {\n\ti := n.key[c]\n\tif i == 0 {\n\t\treturn nil\n\t}\n\treturn &n.child[i-1]\n}\n\nfunc (n *node48) isFull() bool { return n.size >= 48 }\n\nfunc (n *node48) grow() node {\n\tnn := newNode256(n.prefix)\n\tfor c := 0; c < len(n.key); c++ {\n\t\tif i := n.key[byte(c)]; i > 0 {\n\t\t\tnn.addChild(byte(c), n.child[i-1])\n\t\t}\n\t}\n\treturn nn\n}\n\n// Deletes a child from the node.\nfunc (n *node48) deleteChild(c byte) {\n\ti := n.key[c]\n\tif i == 0 {\n\t\treturn\n\t}\n\ti-- // Adjust for 1-indexing\n\tlast := byte(n.size - 1)\n\tif i < last {\n\t\tn.child[i] = n.child[last]\n\t\tfor ic := 0; ic < len(n.key); ic++ {\n\t\t\tif n.key[byte(ic)] == last+1 {\n\t\t\t\tn.key[byte(ic)] = i + 1\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t}\n\tn.child[last] = nil\n\tn.key[c] = 0\n\tn.size--\n}\n\n// Shrink if needed and return new node, otherwise return nil.\nfunc (n *node48) shrink() node {\n\tif n.size > 16 {\n\t\treturn nil\n\t}\n\tnn := newNode16(nil)\n\tfor c := 0; c < len(n.key); c++ {\n\t\tif i := n.key[byte(c)]; i > 0 {\n\t\t\tnn.addChild(byte(c), n.child[i-1])\n\t\t}\n\t}\n\treturn nn\n}\n\n// Iterate over all children calling func f.\nfunc (n *node48) iter(f func(node) bool) {\n\tfor _, c := range n.child {\n\t\tif c != nil && !f(c) {\n\t\t\treturn\n\t\t}\n\t}\n}\n\n// Return our children as a slice.\nfunc (n *node48) children() []node {\n\treturn n.child[:n.size]\n}\n",
    "source_file": "server/stree/node48.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2023-2025 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage stree\n\nimport (\n\t\"bytes\"\n\t\"slices\"\n)\n\n// SubjectTree is an adaptive radix trie (ART) for storing subject information on literal subjects.\n// Will use dynamic nodes, path compression and lazy expansion.\n// The reason this exists is to not only save some memory in our filestore but to greatly optimize matching\n// a wildcard subject to certain members, e.g. consumer NumPending calculations.\ntype SubjectTree[T any] struct {\n\troot node\n\tsize int\n}\n\n// NewSubjectTree creates a new SubjectTree with values T.\nfunc NewSubjectTree[T any]() *SubjectTree[T] {\n\treturn &SubjectTree[T]{}\n}\n\n// Size returns the number of elements stored.\nfunc (t *SubjectTree[T]) Size() int {\n\tif t == nil {\n\t\treturn 0\n\t}\n\treturn t.size\n}\n\n// Will empty out the tree, or if tree is nil create a new one.\nfunc (t *SubjectTree[T]) Empty() *SubjectTree[T] {\n\tif t == nil {\n\t\treturn NewSubjectTree[T]()\n\t}\n\tt.root, t.size = nil, 0\n\treturn t\n}\n\n// Insert a value into the tree. Will return if the value was updated and if so the old value.\nfunc (t *SubjectTree[T]) Insert(subject []byte, value T) (*T, bool) {\n\tif t == nil {\n\t\treturn nil, false\n\t}\n\n\t// Make sure we never insert anything with a noPivot byte.\n\tif bytes.IndexByte(subject, noPivot) >= 0 {\n\t\treturn nil, false\n\t}\n\n\told, updated := t.insert(&t.root, subject, value, 0)\n\tif !updated {\n\t\tt.size++\n\t}\n\treturn old, updated\n}\n\n// Find will find the value and return it or false if it was not found.\nfunc (t *SubjectTree[T]) Find(subject []byte) (*T, bool) {\n\tif t == nil {\n\t\treturn nil, false\n\t}\n\n\tvar si int\n\tfor n := t.root; n != nil; {\n\t\tif n.isLeaf() {\n\t\t\tif ln := n.(*leaf[T]); ln.match(subject[si:]) {\n\t\t\t\treturn &ln.value, true\n\t\t\t}\n\t\t\treturn nil, false\n\t\t}\n\t\t// We are a node type here, grab meta portion.\n\t\tif bn := n.base(); len(bn.prefix) > 0 {\n\t\t\tend := min(si+len(bn.prefix), len(subject))\n\t\t\tif !bytes.Equal(subject[si:end], bn.prefix) {\n\t\t\t\treturn nil, false\n\t\t\t}\n\t\t\t// Increment our subject index.\n\t\t\tsi += len(bn.prefix)\n\t\t}\n\t\tif an := n.findChild(pivot(subject, si)); an != nil {\n\t\t\tn = *an\n\t\t} else {\n\t\t\treturn nil, false\n\t\t}\n\t}\n\treturn nil, false\n}\n\n// Delete will delete the item and return its value, or not found if it did not exist.\nfunc (t *SubjectTree[T]) Delete(subject []byte) (*T, bool) {\n\tif t == nil {\n\t\treturn nil, false\n\t}\n\n\tval, deleted := t.delete(&t.root, subject, 0)\n\tif deleted {\n\t\tt.size--\n\t}\n\treturn val, deleted\n}\n\n// Match will match against a subject that can have wildcards and invoke the callback func for each matched value.\nfunc (t *SubjectTree[T]) Match(filter []byte, cb func(subject []byte, val *T)) {\n\tif t == nil || t.root == nil || len(filter) == 0 || cb == nil {\n\t\treturn\n\t}\n\t// We need to break this up into chunks based on wildcards, either pwc '*' or fwc '>'.\n\tvar raw [16][]byte\n\tparts := genParts(filter, raw[:0])\n\tvar _pre [256]byte\n\tt.match(t.root, parts, _pre[:0], cb)\n}\n\n// IterOrdered will walk all entries in the SubjectTree lexicographically. The callback can return false to terminate the walk.\nfunc (t *SubjectTree[T]) IterOrdered(cb func(subject []byte, val *T) bool) {\n\tif t == nil || t.root == nil {\n\t\treturn\n\t}\n\tvar _pre [256]byte\n\tt.iter(t.root, _pre[:0], true, cb)\n}\n\n// IterFast will walk all entries in the SubjectTree with no guarantees of ordering. The callback can return false to terminate the walk.\nfunc (t *SubjectTree[T]) IterFast(cb func(subject []byte, val *T) bool) {\n\tif t == nil || t.root == nil {\n\t\treturn\n\t}\n\tvar _pre [256]byte\n\tt.iter(t.root, _pre[:0], false, cb)\n}\n\n// Internal methods\n\n// Internal call to insert that can be recursive.\nfunc (t *SubjectTree[T]) insert(np *node, subject []byte, value T, si int) (*T, bool) {\n\tn := *np\n\tif n == nil {\n\t\t*np = newLeaf(subject, value)\n\t\treturn nil, false\n\t}\n\tif n.isLeaf() {\n\t\tln := n.(*leaf[T])\n\t\tif ln.match(subject[si:]) {\n\t\t\t// Replace with new value.\n\t\t\told := ln.value\n\t\t\tln.value = value\n\t\t\treturn &old, true\n\t\t}\n\t\t// Here we need to split this leaf.\n\t\tcpi := commonPrefixLen(ln.suffix, subject[si:])\n\t\tnn := newNode4(subject[si : si+cpi])\n\t\tln.setSuffix(ln.suffix[cpi:])\n\t\tsi += cpi\n\t\t// Make sure we have different pivot, normally this will be the case unless we have overflowing prefixes.\n\t\tif p := pivot(ln.suffix, 0); cpi > 0 && si < len(subject) && p == subject[si] {\n\t\t\t// We need to split the original leaf. Recursively call into insert.\n\t\t\tt.insert(np, subject, value, si)\n\t\t\t// Now add the update version of *np as a child to the new node4.\n\t\t\tnn.addChild(p, *np)\n\t\t} else {\n\t\t\t// Can just add this new leaf as a sibling.\n\t\t\tnl := newLeaf(subject[si:], value)\n\t\t\tnn.addChild(pivot(nl.suffix, 0), nl)\n\t\t\t// Add back original.\n\t\t\tnn.addChild(pivot(ln.suffix, 0), ln)\n\t\t}\n\t\t*np = nn\n\t\treturn nil, false\n\t}\n\n\t// Non-leaf nodes.\n\tbn := n.base()\n\tif len(bn.prefix) > 0 {\n\t\tcpi := commonPrefixLen(bn.prefix, subject[si:])\n\t\tif pli := len(bn.prefix); cpi >= pli {\n\t\t\t// Move past this node. We look for an existing child node to recurse into.\n\t\t\t// If one does not exist we can create a new leaf node.\n\t\t\tsi += pli\n\t\t\tif nn := n.findChild(pivot(subject, si)); nn != nil {\n\t\t\t\treturn t.insert(nn, subject, value, si)\n\t\t\t}\n\t\t\tif n.isFull() {\n\t\t\t\tn = n.grow()\n\t\t\t\t*np = n\n\t\t\t}\n\t\t\tn.addChild(pivot(subject, si), newLeaf(subject[si:], value))\n\t\t\treturn nil, false\n\t\t} else {\n\t\t\t// We did not match the prefix completely here.\n\t\t\t// Calculate new prefix for this node.\n\t\t\tprefix := subject[si : si+cpi]\n\t\t\tsi += len(prefix)\n\t\t\t// We will insert a new node4 and attach our current node below after adjusting prefix.\n\t\t\tnn := newNode4(prefix)\n\t\t\t// Shift the prefix for our original node.\n\t\t\tn.setPrefix(bn.prefix[cpi:])\n\t\t\tnn.addChild(pivot(bn.prefix[:], 0), n)\n\t\t\t// Add in our new leaf.\n\t\t\tnn.addChild(pivot(subject[si:], 0), newLeaf(subject[si:], value))\n\t\t\t// Update our node reference.\n\t\t\t*np = nn\n\t\t}\n\t} else {\n\t\tif nn := n.findChild(pivot(subject, si)); nn != nil {\n\t\t\treturn t.insert(nn, subject, value, si)\n\t\t}\n\t\t// No prefix and no matched child, so add in new leafnode as needed.\n\t\tif n.isFull() {\n\t\t\tn = n.grow()\n\t\t\t*np = n\n\t\t}\n\t\tn.addChild(pivot(subject, si), newLeaf(subject[si:], value))\n\t}\n\n\treturn nil, false\n}\n\n// internal function to recursively find the leaf to delete. Will do compaction if the item is found and removed.\nfunc (t *SubjectTree[T]) delete(np *node, subject []byte, si int) (*T, bool) {\n\tif t == nil || np == nil || *np == nil || len(subject) == 0 {\n\t\treturn nil, false\n\t}\n\tn := *np\n\tif n.isLeaf() {\n\t\tln := n.(*leaf[T])\n\t\tif ln.match(subject[si:]) {\n\t\t\t*np = nil\n\t\t\treturn &ln.value, true\n\t\t}\n\t\treturn nil, false\n\t}\n\t// Not a leaf node.\n\tif bn := n.base(); len(bn.prefix) > 0 {\n\t\t// subject could be shorter and would panic on bad index into subject slice.\n\t\tif len(subject) < si+len(bn.prefix) {\n\t\t\treturn nil, false\n\t\t}\n\t\tif !bytes.Equal(subject[si:si+len(bn.prefix)], bn.prefix) {\n\t\t\treturn nil, false\n\t\t}\n\t\t// Increment our subject index.\n\t\tsi += len(bn.prefix)\n\t}\n\tp := pivot(subject, si)\n\tnna := n.findChild(p)\n\tif nna == nil {\n\t\treturn nil, false\n\t}\n\tnn := *nna\n\tif nn.isLeaf() {\n\t\tln := nn.(*leaf[T])\n\t\tif ln.match(subject[si:]) {\n\t\t\tn.deleteChild(p)\n\n\t\t\tif sn := n.shrink(); sn != nil {\n\t\t\t\tbn := n.base()\n\t\t\t\t// Make sure to set cap so we force an append to copy below.\n\t\t\t\tpre := bn.prefix[:len(bn.prefix):len(bn.prefix)]\n\t\t\t\t// Need to fix up prefixes/suffixes.\n\t\t\t\tif sn.isLeaf() {\n\t\t\t\t\tln := sn.(*leaf[T])\n\t\t\t\t\t// Make sure to set cap so we force an append to copy.\n\t\t\t\t\tln.suffix = append(pre, ln.suffix...)\n\t\t\t\t} else {\n\t\t\t\t\t// We are a node here, we need to add in the old prefix.\n\t\t\t\t\tif len(pre) > 0 {\n\t\t\t\t\t\tbsn := sn.base()\n\t\t\t\t\t\tsn.setPrefix(append(pre, bsn.prefix...))\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\t*np = sn\n\t\t\t}\n\n\t\t\treturn &ln.value, true\n\t\t}\n\t\treturn nil, false\n\t}\n\treturn t.delete(nna, subject, si)\n}\n\n// Internal function which can be called recursively to match all leaf nodes to a given filter subject which\n// once here has been decomposed to parts. These parts only care about wildcards, both pwc and fwc.\nfunc (t *SubjectTree[T]) match(n node, parts [][]byte, pre []byte, cb func(subject []byte, val *T)) {\n\t// Capture if we are sitting on a terminal fwc.\n\tvar hasFWC bool\n\tif lp := len(parts); lp > 0 && len(parts[lp-1]) > 0 && parts[lp-1][0] == fwc {\n\t\thasFWC = true\n\t}\n\n\tfor n != nil {\n\t\tnparts, matched := n.matchParts(parts)\n\t\t// Check if we did not match.\n\t\tif !matched {\n\t\t\treturn\n\t\t}\n\t\t// We have matched here. If we are a leaf and have exhausted all parts or he have a FWC fire callback.\n\t\tif n.isLeaf() {\n\t\t\tif len(nparts) == 0 || (hasFWC && len(nparts) == 1) {\n\t\t\t\tln := n.(*leaf[T])\n\t\t\t\tcb(append(pre, ln.suffix...), &ln.value)\n\t\t\t}\n\t\t\treturn\n\t\t}\n\t\t// We have normal nodes here.\n\t\t// We need to append our prefix\n\t\tbn := n.base()\n\t\tif len(bn.prefix) > 0 {\n\t\t\t// Note that this append may reallocate, but it doesn't modify \"pre\" at the \"match\" callsite.\n\t\t\tpre = append(pre, bn.prefix...)\n\t\t}\n\n\t\t// Check our remaining parts.\n\t\tif len(nparts) == 0 && !hasFWC {\n\t\t\t// We are a node with no parts left and we are not looking at a fwc.\n\t\t\t// We could have a leafnode with no suffix which would be a match.\n\t\t\t// We could also have a terminal pwc. Check for those here.\n\t\t\tvar hasTermPWC bool\n\t\t\tif lp := len(parts); lp > 0 && len(parts[lp-1]) == 1 && parts[lp-1][0] == pwc {\n\t\t\t\t// If we are sitting on a terminal pwc, put the pwc back and continue.\n\t\t\t\tnparts = parts[len(parts)-1:]\n\t\t\t\thasTermPWC = true\n\t\t\t}\n\t\t\tfor _, cn := range n.children() {\n\t\t\t\tif cn == nil {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tif cn.isLeaf() {\n\t\t\t\t\tln := cn.(*leaf[T])\n\t\t\t\t\tif len(ln.suffix) == 0 {\n\t\t\t\t\t\tcb(append(pre, ln.suffix...), &ln.value)\n\t\t\t\t\t} else if hasTermPWC && bytes.IndexByte(ln.suffix, tsep) < 0 {\n\t\t\t\t\t\tcb(append(pre, ln.suffix...), &ln.value)\n\t\t\t\t\t}\n\t\t\t\t} else if hasTermPWC {\n\t\t\t\t\t// We have terminal pwc so call into match again with the child node.\n\t\t\t\t\tt.match(cn, nparts, pre, cb)\n\t\t\t\t}\n\t\t\t}\n\t\t\t// Return regardless.\n\t\t\treturn\n\t\t}\n\t\t// If we are sitting on a terminal fwc, put back and continue.\n\t\tif hasFWC && len(nparts) == 0 {\n\t\t\tnparts = parts[len(parts)-1:]\n\t\t}\n\n\t\t// Here we are a node type with a partial match.\n\t\t// Check if the first part is a wildcard.\n\t\tfp := nparts[0]\n\t\tp := pivot(fp, 0)\n\t\t// Check if we have a pwc/fwc part here. This will cause us to iterate.\n\t\tif len(fp) == 1 && (p == pwc || p == fwc) {\n\t\t\t// We need to iterate over all children here for the current node\n\t\t\t// to see if we match further down.\n\t\t\tfor _, cn := range n.children() {\n\t\t\t\tif cn != nil {\n\t\t\t\t\tt.match(cn, nparts, pre, cb)\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn\n\t\t}\n\t\t// Here we have normal traversal, so find the next child.\n\t\tnn := n.findChild(p)\n\t\tif nn == nil {\n\t\t\treturn\n\t\t}\n\t\tn, parts = *nn, nparts\n\t}\n}\n\n// Internal iter function to walk nodes in lexicographical order.\nfunc (t *SubjectTree[T]) iter(n node, pre []byte, ordered bool, cb func(subject []byte, val *T) bool) bool {\n\tif n.isLeaf() {\n\t\tln := n.(*leaf[T])\n\t\treturn cb(append(pre, ln.suffix...), &ln.value)\n\t}\n\t// We are normal node here.\n\tbn := n.base()\n\t// Note that this append may reallocate, but it doesn't modify \"pre\" at the \"iter\" callsite.\n\tpre = append(pre, bn.prefix...)\n\t// Not everything requires lexicographical sorting, so support a fast path for iterating in\n\t// whatever order the stree has things stored instead.\n\tif !ordered {\n\t\tfor _, cn := range n.children() {\n\t\t\tif cn == nil {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif !t.iter(cn, pre, false, cb) {\n\t\t\t\treturn false\n\t\t\t}\n\t\t}\n\t\treturn true\n\t}\n\t// Collect nodes since unsorted.\n\tvar _nodes [256]node\n\tnodes := _nodes[:0]\n\tfor _, cn := range n.children() {\n\t\tif cn != nil {\n\t\t\tnodes = append(nodes, cn)\n\t\t}\n\t}\n\t// Now sort.\n\tslices.SortStableFunc(nodes, func(a, b node) int { return bytes.Compare(a.path(), b.path()) })\n\t// Now walk the nodes in order and call into next iter.\n\tfor i := range nodes {\n\t\tif !t.iter(nodes[i], pre, true, cb) {\n\t\t\treturn false\n\t\t}\n\t}\n\treturn true\n}\n\n// LazyIntersect iterates the smaller of the two provided subject trees and\n// looks for matching entries in the other. It is lazy in that it does not\n// aggressively optimize against repeated walks, but is considerably faster\n// in most cases than intersecting against a potentially large sublist.\nfunc LazyIntersect[TL, TR any](tl *SubjectTree[TL], tr *SubjectTree[TR], cb func([]byte, *TL, *TR)) {\n\tif tl == nil || tr == nil || tl.root == nil || tr.root == nil {\n\t\treturn\n\t}\n\t// Iterate over the smaller tree to reduce the number of rounds.\n\tif tl.Size() <= tr.Size() {\n\t\ttl.IterFast(func(key []byte, v1 *TL) bool {\n\t\t\tif v2, ok := tr.Find(key); ok {\n\t\t\t\tcb(key, v1, v2)\n\t\t\t}\n\t\t\treturn true\n\t\t})\n\t} else {\n\t\ttr.IterFast(func(key []byte, v2 *TR) bool {\n\t\t\tif v1, ok := tl.Find(key); ok {\n\t\t\t\tcb(key, v1, v2)\n\t\t\t}\n\t\t\treturn true\n\t\t})\n\t}\n}\n",
    "source_file": "server/stree/stree.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2024 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage stree\n\nimport (\n\t\"fmt\"\n\t\"io\"\n\t\"strings\"\n)\n\n// For dumping out a text representation of a tree.\nfunc (t *SubjectTree[T]) Dump(w io.Writer) {\n\tt.dump(w, t.root, 0)\n\tfmt.Fprintln(w)\n}\n\n// Will dump out a node.\nfunc (t *SubjectTree[T]) dump(w io.Writer, n node, depth int) {\n\tif n == nil {\n\t\tfmt.Fprintf(w, \"EMPTY\\n\")\n\t\treturn\n\t}\n\tif n.isLeaf() {\n\t\tleaf := n.(*leaf[T])\n\t\tfmt.Fprintf(w, \"%s LEAF: Suffix: %q Value: %+v\\n\", dumpPre(depth), leaf.suffix, leaf.value)\n\t\tn = nil\n\t} else {\n\t\t// We are a node type here, grab meta portion.\n\t\tbn := n.base()\n\t\tfmt.Fprintf(w, \"%s %s Prefix: %q\\n\", dumpPre(depth), n.kind(), bn.prefix)\n\t\tdepth++\n\t\tn.iter(func(n node) bool {\n\t\t\tt.dump(w, n, depth)\n\t\t\treturn true\n\t\t})\n\t}\n}\n\n// For individual node/leaf dumps.\nfunc (n *leaf[T]) kind() string { return \"LEAF\" }\nfunc (n *node4) kind() string   { return \"NODE4\" }\nfunc (n *node10) kind() string  { return \"NODE10\" }\nfunc (n *node16) kind() string  { return \"NODE16\" }\nfunc (n *node48) kind() string  { return \"NODE48\" }\nfunc (n *node256) kind() string { return \"NODE256\" }\n\n// Calculates the indendation, etc.\nfunc dumpPre(depth int) string {\n\tif depth == 0 {\n\t\treturn \"-- \"\n\t} else {\n\t\tvar b strings.Builder\n\t\tfor i := 0; i < depth; i++ {\n\t\t\tb.WriteString(\"  \")\n\t\t}\n\t\tb.WriteString(\"|__ \")\n\t\treturn b.String()\n\t}\n}\n",
    "source_file": "server/stree/dump.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2023-2024 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage stree\n\n// Node with 16 children\n// Order of struct fields for best memory alignment (as per govet/fieldalignment)\ntype node16 struct {\n\tchild [16]node\n\tmeta\n\tkey [16]byte\n}\n\nfunc newNode16(prefix []byte) *node16 {\n\tnn := &node16{}\n\tnn.setPrefix(prefix)\n\treturn nn\n}\n\n// Currently we do not keep node16 sorted or use bitfields for traversal so just add to the end.\n// TODO(dlc) - We should revisit here with more detailed benchmarks.\nfunc (n *node16) addChild(c byte, nn node) {\n\tif n.size >= 16 {\n\t\tpanic(\"node16 full!\")\n\t}\n\tn.key[n.size] = c\n\tn.child[n.size] = nn\n\tn.size++\n}\n\nfunc (n *node16) findChild(c byte) *node {\n\tfor i := uint16(0); i < n.size; i++ {\n\t\tif n.key[i] == c {\n\t\t\treturn &n.child[i]\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc (n *node16) isFull() bool { return n.size >= 16 }\n\nfunc (n *node16) grow() node {\n\tnn := newNode48(n.prefix)\n\tfor i := 0; i < 16; i++ {\n\t\tnn.addChild(n.key[i], n.child[i])\n\t}\n\treturn nn\n}\n\n// Deletes a child from the node.\nfunc (n *node16) deleteChild(c byte) {\n\tfor i, last := uint16(0), n.size-1; i < n.size; i++ {\n\t\tif n.key[i] == c {\n\t\t\t// Unsorted so just swap in last one here, else nil if last.\n\t\t\tif i < last {\n\t\t\t\tn.key[i] = n.key[last]\n\t\t\t\tn.child[i] = n.child[last]\n\t\t\t\tn.key[last] = 0\n\t\t\t\tn.child[last] = nil\n\t\t\t} else {\n\t\t\t\tn.key[i] = 0\n\t\t\t\tn.child[i] = nil\n\t\t\t}\n\t\t\tn.size--\n\t\t\treturn\n\t\t}\n\t}\n}\n\n// Shrink if needed and return new node, otherwise return nil.\nfunc (n *node16) shrink() node {\n\tif n.size > 10 {\n\t\treturn nil\n\t}\n\tnn := newNode10(nil)\n\tfor i := uint16(0); i < n.size; i++ {\n\t\tnn.addChild(n.key[i], n.child[i])\n\t}\n\treturn nn\n}\n\n// Iterate over all children calling func f.\nfunc (n *node16) iter(f func(node) bool) {\n\tfor i := uint16(0); i < n.size; i++ {\n\t\tif !f(n.child[i]) {\n\t\t\treturn\n\t\t}\n\t}\n}\n\n// Return our children as a slice.\nfunc (n *node16) children() []node {\n\treturn n.child[:n.size]\n}\n",
    "source_file": "server/stree/node16.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2023-2024 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage stree\n\n// Internal node interface.\ntype node interface {\n\tisLeaf() bool\n\tbase() *meta\n\tsetPrefix(pre []byte)\n\taddChild(c byte, n node)\n\tfindChild(c byte) *node\n\tdeleteChild(c byte)\n\tisFull() bool\n\tgrow() node\n\tshrink() node\n\tmatchParts(parts [][]byte) ([][]byte, bool)\n\tkind() string\n\titer(f func(node) bool)\n\tchildren() []node\n\tnumChildren() uint16\n\tpath() []byte\n}\n\ntype meta struct {\n\tprefix []byte\n\tsize   uint16\n}\n\nfunc (n *meta) isLeaf() bool { return false }\nfunc (n *meta) base() *meta  { return n }\n\nfunc (n *meta) setPrefix(pre []byte) {\n\tn.prefix = append([]byte(nil), pre...)\n}\n\nfunc (n *meta) numChildren() uint16 { return n.size }\nfunc (n *meta) path() []byte        { return n.prefix }\n\n// Will match parts against our prefix.\nfunc (n *meta) matchParts(parts [][]byte) ([][]byte, bool) {\n\treturn matchParts(parts, n.prefix)\n}\n",
    "source_file": "server/stree/node.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2023-2025 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage avl\n\nimport (\n\t\"cmp\"\n\t\"encoding/binary\"\n\t\"errors\"\n\t\"math/bits\"\n\t\"slices\"\n)\n\n// SequenceSet is a memory and encoding optimized set for storing unsigned ints.\n//\n// SequenceSet is ~80-100 times more efficient memory wise than a map[uint64]struct{}.\n// SequenceSet is ~1.75 times slower at inserts than the same map.\n// SequenceSet is not thread safe.\n//\n// We use an AVL tree with nodes that hold bitmasks for set membership.\n//\n// Encoding will convert to a space optimized encoding using bitmasks.\ntype SequenceSet struct {\n\troot  *node // root node\n\tsize  int   // number of items\n\tnodes int   // number of nodes\n\t// Having this here vs on the stack in Insert/Delete\n\t// makes a difference in memory usage.\n\tchanged bool\n}\n\n// Insert will insert the sequence into the set.\n// The tree will be balanced inline.\nfunc (ss *SequenceSet) Insert(seq uint64) {\n\tif ss.root = ss.root.insert(seq, &ss.changed, &ss.nodes); ss.changed {\n\t\tss.changed = false\n\t\tss.size++\n\t}\n}\n\n// Exists will return true iff the sequence is a member of this set.\nfunc (ss *SequenceSet) Exists(seq uint64) bool {\n\tfor n := ss.root; n != nil; {\n\t\tif seq < n.base {\n\t\t\tn = n.l\n\t\t\tcontinue\n\t\t} else if seq >= n.base+numEntries {\n\t\t\tn = n.r\n\t\t\tcontinue\n\t\t}\n\t\treturn n.exists(seq)\n\t}\n\treturn false\n}\n\n// SetInitialMin should be used to set the initial minimum sequence when known.\n// This will more effectively utilize space versus self selecting.\n// The set should be empty.\nfunc (ss *SequenceSet) SetInitialMin(min uint64) error {\n\tif !ss.IsEmpty() {\n\t\treturn ErrSetNotEmpty\n\t}\n\tss.root, ss.nodes = &node{base: min, h: 1}, 1\n\treturn nil\n}\n\n// Delete will remove the sequence from the set.\n// Will optionally remove nodes and rebalance.\n// Returns where the sequence was set.\nfunc (ss *SequenceSet) Delete(seq uint64) bool {\n\tif ss == nil || ss.root == nil {\n\t\treturn false\n\t}\n\tss.root = ss.root.delete(seq, &ss.changed, &ss.nodes)\n\tif ss.changed {\n\t\tss.changed = false\n\t\tss.size--\n\t\tif ss.size == 0 {\n\t\t\tss.Empty()\n\t\t}\n\t\treturn true\n\t}\n\treturn false\n}\n\n// Size returns the number of items in the set.\nfunc (ss *SequenceSet) Size() int {\n\treturn ss.size\n}\n\n// Nodes returns the number of nodes in the tree.\nfunc (ss *SequenceSet) Nodes() int {\n\treturn ss.nodes\n}\n\n// Empty will clear all items from a set.\nfunc (ss *SequenceSet) Empty() {\n\tss.root = nil\n\tss.size = 0\n\tss.nodes = 0\n}\n\n// IsEmpty is a fast check of the set being empty.\nfunc (ss *SequenceSet) IsEmpty() bool {\n\tif ss == nil || ss.root == nil {\n\t\treturn true\n\t}\n\treturn false\n}\n\n// Range will invoke the given function for each item in the set.\n// They will range over the set in ascending order.\n// If the callback returns false we terminate the iteration.\nfunc (ss *SequenceSet) Range(f func(uint64) bool) {\n\tss.root.iter(f)\n}\n\n// Heights returns the left and right heights of the tree.\nfunc (ss *SequenceSet) Heights() (l, r int) {\n\tif ss.root == nil {\n\t\treturn 0, 0\n\t}\n\tif ss.root.l != nil {\n\t\tl = ss.root.l.h\n\t}\n\tif ss.root.r != nil {\n\t\tr = ss.root.r.h\n\t}\n\treturn l, r\n}\n\n// Returns min, max and number of set items.\nfunc (ss *SequenceSet) State() (min, max, num uint64) {\n\tif ss == nil || ss.root == nil {\n\t\treturn 0, 0, 0\n\t}\n\tmin, max = ss.MinMax()\n\treturn min, max, uint64(ss.Size())\n}\n\n// MinMax will return the minunum and maximum values in the set.\nfunc (ss *SequenceSet) MinMax() (min, max uint64) {\n\tif ss.root == nil {\n\t\treturn 0, 0\n\t}\n\tfor l := ss.root; l != nil; l = l.l {\n\t\tif l.l == nil {\n\t\t\tmin = l.min()\n\t\t}\n\t}\n\tfor r := ss.root; r != nil; r = r.r {\n\t\tif r.r == nil {\n\t\t\tmax = r.max()\n\t\t}\n\t}\n\treturn min, max\n}\n\nfunc clone(src *node, target **node) {\n\tif src == nil {\n\t\treturn\n\t}\n\tn := &node{base: src.base, bits: src.bits, h: src.h}\n\t*target = n\n\tclone(src.l, &n.l)\n\tclone(src.r, &n.r)\n}\n\n// Clone will return a clone of the given SequenceSet.\nfunc (ss *SequenceSet) Clone() *SequenceSet {\n\tif ss == nil {\n\t\treturn nil\n\t}\n\tcss := &SequenceSet{nodes: ss.nodes, size: ss.size}\n\tclone(ss.root, &css.root)\n\n\treturn css\n}\n\n// Union will union this SequenceSet with ssa.\nfunc (ss *SequenceSet) Union(ssa ...*SequenceSet) {\n\tfor _, sa := range ssa {\n\t\tsa.root.nodeIter(func(n *node) {\n\t\t\tfor nb, b := range n.bits {\n\t\t\t\tfor pos := uint64(0); b != 0; pos++ {\n\t\t\t\t\tif b&1 == 1 {\n\t\t\t\t\t\tseq := n.base + (uint64(nb) * uint64(bitsPerBucket)) + pos\n\t\t\t\t\t\tss.Insert(seq)\n\t\t\t\t\t}\n\t\t\t\t\tb >>= 1\n\t\t\t\t}\n\t\t\t}\n\t\t})\n\t}\n}\n\n// Union will return a union of all sets.\nfunc Union(ssa ...*SequenceSet) *SequenceSet {\n\tif len(ssa) == 0 {\n\t\treturn nil\n\t}\n\t// Sort so we can clone largest.\n\tslices.SortFunc(ssa, func(i, j *SequenceSet) int { return -cmp.Compare(i.Size(), j.Size()) }) // reverse order\n\tss := ssa[0].Clone()\n\n\t// Insert the rest through range call.\n\tfor i := 1; i < len(ssa); i++ {\n\t\tssa[i].Range(func(n uint64) bool {\n\t\t\tss.Insert(n)\n\t\t\treturn true\n\t\t})\n\t}\n\treturn ss\n}\n\nconst (\n\t// Magic is used to identify the encode binary state..\n\tmagic = uint8(22)\n\t// Version\n\tversion = uint8(2)\n\t// hdrLen\n\thdrLen = 2\n\t// minimum length of an encoded SequenceSet.\n\tminLen = 2 + 8 // magic + version + num nodes + num entries.\n)\n\n// EncodeLen returns the bytes needed for encoding.\nfunc (ss SequenceSet) EncodeLen() int {\n\treturn minLen + (ss.Nodes() * ((numBuckets+1)*8 + 2))\n}\n\nfunc (ss SequenceSet) Encode(buf []byte) ([]byte, error) {\n\tnn, encLen := ss.Nodes(), ss.EncodeLen()\n\n\tif cap(buf) < encLen {\n\t\tbuf = make([]byte, encLen)\n\t} else {\n\t\tbuf = buf[:encLen]\n\t}\n\n\t// TODO(dlc) - Go 1.19 introduced Append to not have to keep track.\n\t// Once 1.20 is out we could change this over.\n\t// Also binary.Write() is way slower, do not use.\n\n\tvar le = binary.LittleEndian\n\tbuf[0], buf[1] = magic, version\n\ti := hdrLen\n\tle.PutUint32(buf[i:], uint32(nn))\n\tle.PutUint32(buf[i+4:], uint32(ss.size))\n\ti += 8\n\tss.root.nodeIter(func(n *node) {\n\t\tle.PutUint64(buf[i:], n.base)\n\t\ti += 8\n\t\tfor _, b := range n.bits {\n\t\t\tle.PutUint64(buf[i:], b)\n\t\t\ti += 8\n\t\t}\n\t\tle.PutUint16(buf[i:], uint16(n.h))\n\t\ti += 2\n\t})\n\treturn buf[:i], nil\n}\n\n// ErrBadEncoding is returned when we can not decode properly.\nvar (\n\tErrBadEncoding = errors.New(\"ss: bad encoding\")\n\tErrBadVersion  = errors.New(\"ss: bad version\")\n\tErrSetNotEmpty = errors.New(\"ss: set not empty\")\n)\n\n// Decode returns the sequence set and number of bytes read from the buffer on success.\nfunc Decode(buf []byte) (*SequenceSet, int, error) {\n\tif len(buf) < minLen || buf[0] != magic {\n\t\treturn nil, -1, ErrBadEncoding\n\t}\n\n\tswitch v := buf[1]; v {\n\tcase 1:\n\t\treturn decodev1(buf)\n\tcase 2:\n\t\treturn decodev2(buf)\n\tdefault:\n\t\treturn nil, -1, ErrBadVersion\n\t}\n}\n\n// Helper to decode v2.\nfunc decodev2(buf []byte) (*SequenceSet, int, error) {\n\tvar le = binary.LittleEndian\n\tindex := 2\n\tnn := int(le.Uint32(buf[index:]))\n\tsz := int(le.Uint32(buf[index+4:]))\n\tindex += 8\n\n\texpectedLen := minLen + (nn * ((numBuckets+1)*8 + 2))\n\tif len(buf) < expectedLen {\n\t\treturn nil, -1, ErrBadEncoding\n\t}\n\n\tss, nodes := SequenceSet{size: sz}, make([]node, nn)\n\n\tfor i := 0; i < nn; i++ {\n\t\tn := &nodes[i]\n\t\tn.base = le.Uint64(buf[index:])\n\t\tindex += 8\n\t\tfor bi := range n.bits {\n\t\t\tn.bits[bi] = le.Uint64(buf[index:])\n\t\t\tindex += 8\n\t\t}\n\t\tn.h = int(le.Uint16(buf[index:]))\n\t\tindex += 2\n\t\tss.insertNode(n)\n\t}\n\n\treturn &ss, index, nil\n}\n\n// Helper to decode v1 into v2 which has fixed buckets of 32 vs 64 originally.\nfunc decodev1(buf []byte) (*SequenceSet, int, error) {\n\tvar le = binary.LittleEndian\n\tindex := 2\n\tnn := int(le.Uint32(buf[index:]))\n\tsz := int(le.Uint32(buf[index+4:]))\n\tindex += 8\n\n\tconst v1NumBuckets = 64\n\n\texpectedLen := minLen + (nn * ((v1NumBuckets+1)*8 + 2))\n\tif len(buf) < expectedLen {\n\t\treturn nil, -1, ErrBadEncoding\n\t}\n\n\tvar ss SequenceSet\n\tfor i := 0; i < nn; i++ {\n\t\tbase := le.Uint64(buf[index:])\n\t\tindex += 8\n\t\tfor nb := uint64(0); nb < v1NumBuckets; nb++ {\n\t\t\tn := le.Uint64(buf[index:])\n\t\t\t// Walk all set bits and insert sequences manually for this decode from v1.\n\t\t\tfor pos := uint64(0); n != 0; pos++ {\n\t\t\t\tif n&1 == 1 {\n\t\t\t\t\tseq := base + (nb * uint64(bitsPerBucket)) + pos\n\t\t\t\t\tss.Insert(seq)\n\t\t\t\t}\n\t\t\t\tn >>= 1\n\t\t\t}\n\t\t\tindex += 8\n\t\t}\n\t\t// Skip over encoded height.\n\t\tindex += 2\n\t}\n\n\t// Sanity check.\n\tif ss.Size() != sz {\n\t\treturn nil, -1, ErrBadEncoding\n\t}\n\n\treturn &ss, index, nil\n\n}\n\n// insertNode places a decoded node into the tree.\n// These should be done in tree order as defined by Encode()\n// This allows us to not have to calculate height or do rebalancing.\n// So much better performance this way.\nfunc (ss *SequenceSet) insertNode(n *node) {\n\tss.nodes++\n\n\tif ss.root == nil {\n\t\tss.root = n\n\t\treturn\n\t}\n\t// Walk our way to the insertion point.\n\tfor p := ss.root; p != nil; {\n\t\tif n.base < p.base {\n\t\t\tif p.l == nil {\n\t\t\t\tp.l = n\n\t\t\t\treturn\n\t\t\t}\n\t\t\tp = p.l\n\t\t} else {\n\t\t\tif p.r == nil {\n\t\t\t\tp.r = n\n\t\t\t\treturn\n\t\t\t}\n\t\t\tp = p.r\n\t\t}\n\t}\n}\n\nconst (\n\tbitsPerBucket = 64 // bits in uint64\n\tnumBuckets    = 32\n\tnumEntries    = numBuckets * bitsPerBucket\n)\n\ntype node struct {\n\t//v dvalue\n\tbase uint64\n\tbits [numBuckets]uint64\n\tl    *node\n\tr    *node\n\th    int\n}\n\n// Set the proper bit.\n// seq should have already been qualified and inserted should be non nil.\nfunc (n *node) set(seq uint64, inserted *bool) {\n\tseq -= n.base\n\ti := seq / bitsPerBucket\n\tmask := uint64(1) << (seq % bitsPerBucket)\n\tif (n.bits[i] & mask) == 0 {\n\t\tn.bits[i] |= mask\n\t\t*inserted = true\n\t}\n}\n\nfunc (n *node) insert(seq uint64, inserted *bool, nodes *int) *node {\n\tif n == nil {\n\t\tbase := (seq / numEntries) * numEntries\n\t\tn := &node{base: base, h: 1}\n\t\tn.set(seq, inserted)\n\t\t*nodes++\n\t\treturn n\n\t}\n\n\tif seq < n.base {\n\t\tn.l = n.l.insert(seq, inserted, nodes)\n\t} else if seq >= n.base+numEntries {\n\t\tn.r = n.r.insert(seq, inserted, nodes)\n\t} else {\n\t\tn.set(seq, inserted)\n\t}\n\n\tn.h = maxH(n) + 1\n\n\t// Don't make a function, impacts performance.\n\tif bf := balanceF(n); bf > 1 {\n\t\t// Left unbalanced.\n\t\tif balanceF(n.l) < 0 {\n\t\t\tn.l = n.l.rotateL()\n\t\t}\n\t\treturn n.rotateR()\n\t} else if bf < -1 {\n\t\t// Right unbalanced.\n\t\tif balanceF(n.r) > 0 {\n\t\t\tn.r = n.r.rotateR()\n\t\t}\n\t\treturn n.rotateL()\n\t}\n\treturn n\n}\n\nfunc (n *node) rotateL() *node {\n\tr := n.r\n\tif r != nil {\n\t\tn.r = r.l\n\t\tr.l = n\n\t\tn.h = maxH(n) + 1\n\t\tr.h = maxH(r) + 1\n\t} else {\n\t\tn.r = nil\n\t\tn.h = maxH(n) + 1\n\t}\n\treturn r\n}\n\nfunc (n *node) rotateR() *node {\n\tl := n.l\n\tif l != nil {\n\t\tn.l = l.r\n\t\tl.r = n\n\t\tn.h = maxH(n) + 1\n\t\tl.h = maxH(l) + 1\n\t} else {\n\t\tn.l = nil\n\t\tn.h = maxH(n) + 1\n\t}\n\treturn l\n}\n\nfunc balanceF(n *node) int {\n\tif n == nil {\n\t\treturn 0\n\t}\n\tvar lh, rh int\n\tif n.l != nil {\n\t\tlh = n.l.h\n\t}\n\tif n.r != nil {\n\t\trh = n.r.h\n\t}\n\treturn lh - rh\n}\n\nfunc maxH(n *node) int {\n\tif n == nil {\n\t\treturn 0\n\t}\n\tvar lh, rh int\n\tif n.l != nil {\n\t\tlh = n.l.h\n\t}\n\tif n.r != nil {\n\t\trh = n.r.h\n\t}\n\tif lh > rh {\n\t\treturn lh\n\t}\n\treturn rh\n}\n\n// Clear the proper bit.\n// seq should have already been qualified and deleted should be non nil.\n// Will return true if this node is now empty.\nfunc (n *node) clear(seq uint64, deleted *bool) bool {\n\tseq -= n.base\n\ti := seq / bitsPerBucket\n\tmask := uint64(1) << (seq % bitsPerBucket)\n\tif (n.bits[i] & mask) != 0 {\n\t\tn.bits[i] &^= mask\n\t\t*deleted = true\n\t}\n\tfor _, b := range n.bits {\n\t\tif b != 0 {\n\t\t\treturn false\n\t\t}\n\t}\n\treturn true\n}\n\nfunc (n *node) delete(seq uint64, deleted *bool, nodes *int) *node {\n\tif n == nil {\n\t\treturn nil\n\t}\n\n\tif seq < n.base {\n\t\tn.l = n.l.delete(seq, deleted, nodes)\n\t} else if seq >= n.base+numEntries {\n\t\tn.r = n.r.delete(seq, deleted, nodes)\n\t} else if empty := n.clear(seq, deleted); empty {\n\t\t*nodes--\n\t\tif n.l == nil {\n\t\t\tn = n.r\n\t\t} else if n.r == nil {\n\t\t\tn = n.l\n\t\t} else {\n\t\t\t// We have both children.\n\t\t\tn.r = n.r.insertNodePrev(n.l)\n\t\t\tn = n.r\n\t\t}\n\t}\n\n\tif n != nil {\n\t\tn.h = maxH(n) + 1\n\t}\n\n\t// Check balance.\n\tif bf := balanceF(n); bf > 1 {\n\t\t// Left unbalanced.\n\t\tif balanceF(n.l) < 0 {\n\t\t\tn.l = n.l.rotateL()\n\t\t}\n\t\treturn n.rotateR()\n\t} else if bf < -1 {\n\t\t// right unbalanced.\n\t\tif balanceF(n.r) > 0 {\n\t\t\tn.r = n.r.rotateR()\n\t\t}\n\t\treturn n.rotateL()\n\t}\n\n\treturn n\n}\n\n// Will insert nn into the node assuming it is less than all other nodes in n.\n// Will re-calculate height and balance.\nfunc (n *node) insertNodePrev(nn *node) *node {\n\tif n.l == nil {\n\t\tn.l = nn\n\t} else {\n\t\tn.l = n.l.insertNodePrev(nn)\n\t}\n\tn.h = maxH(n) + 1\n\n\t// Check balance.\n\tif bf := balanceF(n); bf > 1 {\n\t\t// Left unbalanced.\n\t\tif balanceF(n.l) < 0 {\n\t\t\tn.l = n.l.rotateL()\n\t\t}\n\t\treturn n.rotateR()\n\t} else if bf < -1 {\n\t\t// right unbalanced.\n\t\tif balanceF(n.r) > 0 {\n\t\t\tn.r = n.r.rotateR()\n\t\t}\n\t\treturn n.rotateL()\n\t}\n\treturn n\n}\n\nfunc (n *node) exists(seq uint64) bool {\n\tseq -= n.base\n\ti := seq / bitsPerBucket\n\tmask := uint64(1) << (seq % bitsPerBucket)\n\treturn n.bits[i]&mask != 0\n}\n\n// Return minimum sequence in the set.\n// This node can not be empty.\nfunc (n *node) min() uint64 {\n\tfor i, b := range n.bits {\n\t\tif b != 0 {\n\t\t\treturn n.base +\n\t\t\t\tuint64(i*bitsPerBucket) +\n\t\t\t\tuint64(bits.TrailingZeros64(b))\n\t\t}\n\t}\n\treturn 0\n}\n\n// Return maximum sequence in the set.\n// This node can not be empty.\nfunc (n *node) max() uint64 {\n\tfor i := numBuckets - 1; i >= 0; i-- {\n\t\tif b := n.bits[i]; b != 0 {\n\t\t\treturn n.base +\n\t\t\t\tuint64(i*bitsPerBucket) +\n\t\t\t\tuint64(bitsPerBucket-bits.LeadingZeros64(b>>1))\n\t\t}\n\t}\n\treturn 0\n}\n\n// This is done in tree order.\nfunc (n *node) nodeIter(f func(n *node)) {\n\tif n == nil {\n\t\treturn\n\t}\n\tf(n)\n\tn.l.nodeIter(f)\n\tn.r.nodeIter(f)\n}\n\n// iter will iterate through the set's items in this node.\n// If the supplied function returns false we terminate the iteration.\nfunc (n *node) iter(f func(uint64) bool) bool {\n\tif n == nil {\n\t\treturn true\n\t}\n\n\tif ok := n.l.iter(f); !ok {\n\t\treturn false\n\t}\n\tfor num := n.base; num < n.base+numEntries; num++ {\n\t\tif n.exists(num) {\n\t\t\tif ok := f(num); !ok {\n\t\t\t\treturn false\n\t\t\t}\n\t\t}\n\t}\n\tif ok := n.r.iter(f); !ok {\n\t\treturn false\n\t}\n\n\treturn true\n}\n",
    "source_file": "server/avl/seqset.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2025 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n// ats controls the go routines for the access time service.\n// This allows more efficient unixnano operations for cache access times.\n// We will have one per binary (usually per server).\npackage ats\n\nimport (\n\t\"sync/atomic\"\n\t\"time\"\n)\n\n// Update every 100ms for gathering access time in unix nano.\nconst TickInterval = 100 * time.Millisecond\n\nvar (\n\t// Our unix nano time.\n\tutime atomic.Int64\n\t// How may registered users do we have, controls lifetime of Go routine.\n\trefs atomic.Int64\n\t// To signal the shutdown of the Go routine.\n\tdone chan struct{}\n)\n\nfunc init() {\n\t// Initialize our done chan.\n\tdone = make(chan struct{}, 1)\n}\n\n// Register usage. This will happen on filestore creation.\nfunc Register() {\n\tif v := refs.Add(1); v == 1 {\n\t\t// This is the first to register (could also go up and down),\n\t\t// so spin up Go routine and grab initial time.\n\t\tutime.Store(time.Now().UnixNano())\n\n\t\tgo func() {\n\t\t\tticker := time.NewTicker(TickInterval)\n\t\t\tdefer ticker.Stop()\n\t\t\tfor {\n\t\t\t\tselect {\n\t\t\t\tcase <-ticker.C:\n\t\t\t\t\tutime.Store(time.Now().UnixNano())\n\t\t\t\tcase <-done:\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}\n\t\t}()\n\t}\n}\n\n// Unregister usage. We will shutdown the go routine if no more registered users.\nfunc Unregister() {\n\tif v := refs.Add(-1); v == 0 {\n\t\tdone <- struct{}{}\n\t} else if v < 0 {\n\t\trefs.Store(0)\n\t\tpanic(\"unbalanced unregister for access time state\")\n\t}\n}\n\n// Will load the access time from an atomic.\n// If no one has registered this will return 0 or stale data.\n// It is the responsibility of the user to properly register and unregister.\nfunc AccessTime() int64 {\n\t// Return last updated time.\n\tv := utime.Load()\n\tif v == 0 {\n\t\tpanic(\"access time service not running\")\n\t}\n\treturn v\n}\n",
    "source_file": "server/ats/ats.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2023-2024 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage certidp\n\nimport (\n\t\"crypto/sha256\"\n\t\"crypto/x509\"\n\t\"encoding/base64\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"net/url\"\n\t\"strings\"\n\t\"time\"\n\n\t\"golang.org/x/crypto/ocsp\"\n)\n\nconst (\n\tDefaultAllowedClockSkew     = 30 * time.Second\n\tDefaultOCSPResponderTimeout = 2 * time.Second\n\tDefaultTTLUnsetNextUpdate   = 1 * time.Hour\n)\n\ntype StatusAssertion int\n\nvar (\n\tStatusAssertionStrToVal = map[string]StatusAssertion{\n\t\t\"good\":    ocsp.Good,\n\t\t\"revoked\": ocsp.Revoked,\n\t\t\"unknown\": ocsp.Unknown,\n\t}\n\tStatusAssertionValToStr = map[StatusAssertion]string{\n\t\tocsp.Good:    \"good\",\n\t\tocsp.Revoked: \"revoked\",\n\t\tocsp.Unknown: \"unknown\",\n\t}\n\tStatusAssertionIntToVal = map[int]StatusAssertion{\n\t\t0: ocsp.Good,\n\t\t1: ocsp.Revoked,\n\t\t2: ocsp.Unknown,\n\t}\n)\n\n// GetStatusAssertionStr returns the corresponding string representation of the StatusAssertion.\nfunc GetStatusAssertionStr(sa int) string {\n\t// If the provided status assertion value is not found in the map (StatusAssertionIntToVal),\n\t// the function defaults to \"unknown\" to avoid defaulting to \"good,\" which is the default iota value\n\t// for the ocsp.StatusAssertion enumeration (https://pkg.go.dev/golang.org/x/crypto/ocsp#pkg-constants).\n\t// This ensures that we don't unintentionally default to \"good\" when there's no map entry.\n\tv, ok := StatusAssertionIntToVal[sa]\n\tif !ok {\n\t\t// set unknown as fallback\n\t\tv = ocsp.Unknown\n\t}\n\n\treturn StatusAssertionValToStr[v]\n}\n\nfunc (sa StatusAssertion) MarshalJSON() ([]byte, error) {\n\t// This ensures that we don't unintentionally default to \"good\" when there's no map entry.\n\t// (see more details in the GetStatusAssertionStr() comment)\n\tstr, ok := StatusAssertionValToStr[sa]\n\tif !ok {\n\t\t// set unknown as fallback\n\t\tstr = StatusAssertionValToStr[ocsp.Unknown]\n\t}\n\treturn json.Marshal(str)\n}\n\nfunc (sa *StatusAssertion) UnmarshalJSON(in []byte) error {\n\t// This ensures that we don't unintentionally default to \"good\" when there's no map entry.\n\t// (see more details in the GetStatusAssertionStr() comment)\n\tv, ok := StatusAssertionStrToVal[strings.ReplaceAll(string(in), \"\\\"\", \"\")]\n\tif !ok {\n\t\t// set unknown as fallback\n\t\tv = StatusAssertionStrToVal[\"unknown\"]\n\t}\n\t*sa = v\n\treturn nil\n}\n\ntype ChainLink struct {\n\tLeaf             *x509.Certificate\n\tIssuer           *x509.Certificate\n\tOCSPWebEndpoints *[]*url.URL\n}\n\n// OCSPPeerConfig holds the parsed OCSP peer configuration section of TLS configuration\ntype OCSPPeerConfig struct {\n\tVerify                 bool\n\tTimeout                float64\n\tClockSkew              float64\n\tWarnOnly               bool\n\tUnknownIsGood          bool\n\tAllowWhenCAUnreachable bool\n\tTTLUnsetNextUpdate     float64\n}\n\nfunc NewOCSPPeerConfig() *OCSPPeerConfig {\n\treturn &OCSPPeerConfig{\n\t\tVerify:                 false,\n\t\tTimeout:                DefaultOCSPResponderTimeout.Seconds(),\n\t\tClockSkew:              DefaultAllowedClockSkew.Seconds(),\n\t\tWarnOnly:               false,\n\t\tUnknownIsGood:          false,\n\t\tAllowWhenCAUnreachable: false,\n\t\tTTLUnsetNextUpdate:     DefaultTTLUnsetNextUpdate.Seconds(),\n\t}\n}\n\n// Log is a neutral method of passing server loggers to plugins\ntype Log struct {\n\tDebugf  func(format string, v ...any)\n\tNoticef func(format string, v ...any)\n\tWarnf   func(format string, v ...any)\n\tErrorf  func(format string, v ...any)\n\tTracef  func(format string, v ...any)\n}\n\ntype CertInfo struct {\n\tSubject     string `json:\"subject,omitempty\"`\n\tIssuer      string `json:\"issuer,omitempty\"`\n\tFingerprint string `json:\"fingerprint,omitempty\"`\n\tRaw         []byte `json:\"raw,omitempty\"`\n}\n\nvar OCSPPeerUsage = `\nFor client, leaf spoke (remotes), and leaf hub connections, you may enable OCSP peer validation:\n\n    tls {\n        ...\n        # mTLS must be enabled (with exception of Leaf remotes)\n        verify: true\n        ...\n        # short form enables peer verify and takes option defaults\n        ocsp_peer: true\n\n        # long form includes settable options\n        ocsp_peer {\n           # Enable OCSP peer validation (default false)\n           verify: true\n\n           # OCSP responder timeout in seconds (may be fractional, default 2 seconds)\n           ca_timeout: 2\n\n           # Allowed skew between server and OCSP responder time in seconds (may be fractional, default 30 seconds)\n           allowed_clockskew: 30\n\n           # Warn-only and never reject connections (default false)\n           warn_only: false\n\n           # Treat response Unknown status as valid certificate (default false)\n           unknown_is_good: false\n\n           # Warn-only if no CA response can be obtained and no cached revocation exists (default false)\n           allow_when_ca_unreachable: false\n\n           # If response NextUpdate unset by CA, set a default cache TTL in seconds from ThisUpdate (default 1 hour)\n           cache_ttl_when_next_update_unset: 3600\n        }\n        ...\n    }\n\nNote: OCSP validation for route and gateway connections is enabled using the 'ocsp' configuration option.\n`\n\n// GenerateFingerprint returns a base64-encoded SHA256 hash of the raw certificate\nfunc GenerateFingerprint(cert *x509.Certificate) string {\n\tdata := sha256.Sum256(cert.Raw)\n\treturn base64.StdEncoding.EncodeToString(data[:])\n}\n\nfunc getWebEndpoints(uris []string) []*url.URL {\n\tvar urls []*url.URL\n\tfor _, uri := range uris {\n\t\tendpoint, err := url.ParseRequestURI(uri)\n\t\tif err != nil {\n\t\t\t// skip invalid URLs\n\t\t\tcontinue\n\t\t}\n\t\tif endpoint.Scheme != \"http\" && endpoint.Scheme != \"https\" {\n\t\t\t// skip non-web URLs\n\t\t\tcontinue\n\t\t}\n\t\turls = append(urls, endpoint)\n\t}\n\treturn urls\n}\n\n// GetSubjectDNForm returns RDN sequence concatenation of the certificate's subject to be\n// used in logs, events, etc. Should never be used for reliable cache matching or other crypto purposes.\nfunc GetSubjectDNForm(cert *x509.Certificate) string {\n\tif cert == nil {\n\t\treturn \"\"\n\t}\n\treturn strings.TrimSuffix(fmt.Sprintf(\"%s+\", cert.Subject.ToRDNSequence()), \"+\")\n}\n\n// GetIssuerDNForm returns RDN sequence concatenation of the certificate's issuer to be\n// used in logs, events, etc. Should never be used for reliable cache matching or other crypto purposes.\nfunc GetIssuerDNForm(cert *x509.Certificate) string {\n\tif cert == nil {\n\t\treturn \"\"\n\t}\n\treturn strings.TrimSuffix(fmt.Sprintf(\"%s+\", cert.Issuer.ToRDNSequence()), \"+\")\n}\n\n// CertOCSPEligible checks if the certificate's issuer has populated AIA with OCSP responder endpoint(s)\n// and is thus eligible for OCSP validation\nfunc CertOCSPEligible(link *ChainLink) bool {\n\tif link == nil || link.Leaf.Raw == nil || len(link.Leaf.Raw) == 0 {\n\t\treturn false\n\t}\n\tif len(link.Leaf.OCSPServer) == 0 {\n\t\treturn false\n\t}\n\turls := getWebEndpoints(link.Leaf.OCSPServer)\n\tif len(urls) == 0 {\n\t\treturn false\n\t}\n\tlink.OCSPWebEndpoints = &urls\n\treturn true\n}\n\n// GetLeafIssuerCert returns the issuer certificate of the leaf (positional) certificate in the chain\nfunc GetLeafIssuerCert(chain []*x509.Certificate, leafPos int) *x509.Certificate {\n\tif len(chain) == 0 || leafPos < 0 {\n\t\treturn nil\n\t}\n\t// self-signed certificate or too-big leafPos\n\tif leafPos >= len(chain)-1 {\n\t\treturn nil\n\t}\n\t// returns pointer to issuer cert or nil\n\treturn (chain)[leafPos+1]\n}\n\n// OCSPResponseCurrent checks if the OCSP response is current (i.e. not expired and not future effective)\nfunc OCSPResponseCurrent(ocspr *ocsp.Response, opts *OCSPPeerConfig, log *Log) bool {\n\tskew := time.Duration(opts.ClockSkew * float64(time.Second))\n\tif skew < 0*time.Second {\n\t\tskew = DefaultAllowedClockSkew\n\t}\n\tnow := time.Now().UTC()\n\t// Typical effectivity check based on CA response ThisUpdate and NextUpdate semantics\n\tif !ocspr.NextUpdate.IsZero() && ocspr.NextUpdate.Before(now.Add(-1*skew)) {\n\t\tt := ocspr.NextUpdate.Format(time.RFC3339Nano)\n\t\tnt := now.Format(time.RFC3339Nano)\n\t\tlog.Debugf(DbgResponseExpired, t, nt, skew)\n\t\treturn false\n\t}\n\t// CA responder can assert NextUpdate unset, in which case use config option to set a default cache TTL\n\tif ocspr.NextUpdate.IsZero() {\n\t\tttl := time.Duration(opts.TTLUnsetNextUpdate * float64(time.Second))\n\t\tif ttl < 0*time.Second {\n\t\t\tttl = DefaultTTLUnsetNextUpdate\n\t\t}\n\t\texpiryTime := ocspr.ThisUpdate.Add(ttl)\n\t\tif expiryTime.Before(now.Add(-1 * skew)) {\n\t\t\tt := expiryTime.Format(time.RFC3339Nano)\n\t\t\tnt := now.Format(time.RFC3339Nano)\n\t\t\tlog.Debugf(DbgResponseTTLExpired, t, nt, skew)\n\t\t\treturn false\n\t\t}\n\t}\n\tif ocspr.ThisUpdate.After(now.Add(skew)) {\n\t\tt := ocspr.ThisUpdate.Format(time.RFC3339Nano)\n\t\tnt := now.Format(time.RFC3339Nano)\n\t\tlog.Debugf(DbgResponseFutureDated, t, nt, skew)\n\t\treturn false\n\t}\n\treturn true\n}\n\n// ValidDelegationCheck checks if the CA OCSP Response was signed by a valid CA Issuer delegate as per (RFC 6960, section 4.2.2.2)\n// If a valid delegate or direct-signed by CA Issuer, true returned.\nfunc ValidDelegationCheck(iss *x509.Certificate, ocspr *ocsp.Response) bool {\n\t// This call assumes prior successful parse and signature validation of the OCSP response\n\t// The Go OCSP library (as of x/crypto/ocsp v0.9) will detect and perform a 1-level delegate signature check but does not\n\t// implement the additional criteria for delegation specified in RFC 6960, section 4.2.2.2.\n\tif iss == nil || ocspr == nil {\n\t\treturn false\n\t}\n\t// not a delegation, no-op\n\tif ocspr.Certificate == nil {\n\t\treturn true\n\t}\n\t// delegate is self-same with CA Issuer, not a delegation although response issued in that form\n\tif ocspr.Certificate.Equal(iss) {\n\t\treturn true\n\t}\n\t// we need to verify CA Issuer stamped id-kp-OCSPSigning on delegate\n\tdelegatedSigner := false\n\tfor _, keyUseExt := range ocspr.Certificate.ExtKeyUsage {\n\t\tif keyUseExt == x509.ExtKeyUsageOCSPSigning {\n\t\t\tdelegatedSigner = true\n\t\t\tbreak\n\t\t}\n\t}\n\treturn delegatedSigner\n}\n",
    "source_file": "server/certidp/certidp.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2023 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage certidp\n\nvar (\n\t// Returned errors\n\tErrIllegalPeerOptsConfig              = \"expected map to define OCSP peer options, got [%T]\"\n\tErrIllegalCacheOptsConfig             = \"expected map to define OCSP peer cache options, got [%T]\"\n\tErrParsingPeerOptFieldGeneric         = \"error parsing tls peer config, unknown field [%q]\"\n\tErrParsingPeerOptFieldTypeConversion  = \"error parsing tls peer config, conversion error: %s\"\n\tErrParsingCacheOptFieldTypeConversion = \"error parsing OCSP peer cache config, conversion error: %s\"\n\tErrUnableToPlugTLSEmptyConfig         = \"unable to plug TLS verify connection, config is nil\"\n\tErrMTLSRequired                       = \"OCSP peer verification for client connections requires TLS verify (mTLS) to be enabled\"\n\tErrUnableToPlugTLSClient              = \"unable to register client OCSP verification\"\n\tErrUnableToPlugTLSServer              = \"unable to register server OCSP verification\"\n\tErrCannotWriteCompressed              = \"error writing to compression writer: %w\"\n\tErrCannotReadCompressed               = \"error reading compression reader: %w\"\n\tErrTruncatedWrite                     = \"short write on body (%d != %d)\"\n\tErrCannotCloseWriter                  = \"error closing compression writer: %w\"\n\tErrParsingCacheOptFieldGeneric        = \"error parsing OCSP peer cache config, unknown field [%q]\"\n\tErrUnknownCacheType                   = \"error parsing OCSP peer cache config, unknown type [%s]\"\n\tErrInvalidChainlink                   = \"invalid chain link\"\n\tErrBadResponderHTTPStatus             = \"bad OCSP responder http status: [%d]\"\n\tErrNoAvailOCSPServers                 = \"no available OCSP servers\"\n\tErrFailedWithAllRequests              = \"exhausted OCSP responders: %w\"\n\n\t// Direct logged errors\n\tErrLoadCacheFail          = \"Unable to load OCSP peer cache: %s\"\n\tErrSaveCacheFail          = \"Unable to save OCSP peer cache: %s\"\n\tErrBadCacheTypeConfig     = \"Unimplemented OCSP peer cache type [%v]\"\n\tErrResponseCompressFail   = \"Unable to compress OCSP response for key [%s]: %s\"\n\tErrResponseDecompressFail = \"Unable to decompress OCSP response for key [%s]: %s\"\n\tErrPeerEmptyNoEvent       = \"Peer certificate is nil, cannot send OCSP peer reject event\"\n\tErrPeerEmptyAutoReject    = \"Peer certificate is nil, rejecting OCSP peer\"\n\n\t// Debug information\n\tDbgPlugTLSForKind        = \"Plugging TLS OCSP peer for [%s]\"\n\tDbgNumServerChains       = \"Peer OCSP enabled: %d TLS server chain(s) will be evaluated\"\n\tDbgNumClientChains       = \"Peer OCSP enabled: %d TLS client chain(s) will be evaluated\"\n\tDbgLinksInChain          = \"Chain [%d]: %d total link(s)\"\n\tDbgSelfSignedValid       = \"Chain [%d] is self-signed, thus peer is valid\"\n\tDbgValidNonOCSPChain     = \"Chain [%d] has no OCSP eligible links, thus peer is valid\"\n\tDbgChainIsOCSPEligible   = \"Chain [%d] has %d OCSP eligible link(s)\"\n\tDbgChainIsOCSPValid      = \"Chain [%d] is OCSP valid for all eligible links, thus peer is valid\"\n\tDbgNoOCSPValidChains     = \"No OCSP valid chains, thus peer is invalid\"\n\tDbgCheckingCacheForCert  = \"Checking OCSP peer cache for [%s], key [%s]\"\n\tDbgCurrentResponseCached = \"Cached OCSP response is current, status [%s]\"\n\tDbgExpiredResponseCached = \"Cached OCSP response is expired, status [%s]\"\n\tDbgOCSPValidPeerLink     = \"OCSP verify pass for [%s]\"\n\tDbgCachingResponse       = \"Caching OCSP response for [%s], key [%s]\"\n\tDbgAchievedCompression   = \"OCSP response compression ratio: [%f]\"\n\tDbgCacheHit              = \"OCSP peer cache hit for key [%s]\"\n\tDbgCacheMiss             = \"OCSP peer cache miss for key [%s]\"\n\tDbgPreservedRevocation   = \"Revoked OCSP response for key [%s] preserved by cache policy\"\n\tDbgDeletingCacheResponse = \"Deleting OCSP peer cached response for key [%s]\"\n\tDbgStartingCache         = \"Starting OCSP peer cache\"\n\tDbgStoppingCache         = \"Stopping OCSP peer cache\"\n\tDbgLoadingCache          = \"Loading OCSP peer cache [%s]\"\n\tDbgNoCacheFound          = \"No OCSP peer cache found, starting with empty cache\"\n\tDbgSavingCache           = \"Saving OCSP peer cache [%s]\"\n\tDbgCacheSaved            = \"Saved OCSP peer cache successfully (%d bytes)\"\n\tDbgMakingCARequest       = \"Trying OCSP responder url [%s]\"\n\tDbgResponseExpired       = \"OCSP response NextUpdate [%s] is before now [%s] with clockskew [%s]\"\n\tDbgResponseTTLExpired    = \"OCSP response cache expiry [%s] is before now [%s] with clockskew [%s]\"\n\tDbgResponseFutureDated   = \"OCSP response ThisUpdate [%s] is before now [%s] with clockskew [%s]\"\n\tDbgCacheSaveTimerExpired = \"OCSP peer cache save timer expired\"\n\tDbgCacheDirtySave        = \"OCSP peer cache is dirty, saving\"\n\n\t// Returned to peer as TLS reject reason\n\tMsgTLSClientRejectConnection = \"client not OCSP valid\"\n\tMsgTLSServerRejectConnection = \"server not OCSP valid\"\n\n\t// Expected runtime errors (direct logged)\n\tErrCAResponderCalloutFail  = \"Attempt to obtain OCSP response from CA responder for [%s] failed: %s\"\n\tErrNewCAResponseNotCurrent = \"New OCSP CA response obtained for [%s] but not current\"\n\tErrCAResponseParseFailed   = \"Could not parse OCSP CA response for [%s]: %s\"\n\tErrOCSPInvalidPeerLink     = \"OCSP verify fail for [%s] with CA status [%s]\"\n\n\t// Policy override warnings (direct logged)\n\tMsgAllowWhenCAUnreachableOccurred             = \"Failed to obtain OCSP CA response for [%s] but AllowWhenCAUnreachable set; no cached revocation so allowing\"\n\tMsgAllowWhenCAUnreachableOccurredCachedRevoke = \"Failed to obtain OCSP CA response for [%s] but AllowWhenCAUnreachable set; cached revocation exists so rejecting\"\n\tMsgAllowWarnOnlyOccurred                      = \"OCSP verify fail for [%s] but WarnOnly is true so allowing\"\n\n\t// Info (direct logged)\n\tMsgCacheOnline  = \"OCSP peer cache online, type [%s]\"\n\tMsgCacheOffline = \"OCSP peer cache offline, type [%s]\"\n\n\t// OCSP cert invalid reasons (debug and event reasons)\n\tMsgFailedOCSPResponseFetch       = \"Failed OCSP response fetch\"\n\tMsgOCSPResponseNotEffective      = \"OCSP response not in effectivity window\"\n\tMsgFailedOCSPResponseParse       = \"Failed OCSP response parse\"\n\tMsgOCSPResponseInvalidStatus     = \"Invalid OCSP response status: %s\"\n\tMsgOCSPResponseDelegationInvalid = \"Invalid OCSP response delegation: %s\"\n\tMsgCachedOCSPResponseInvalid     = \"Invalid cached OCSP response for [%s] with fingerprint [%s]\"\n)\n",
    "source_file": "server/certidp/messages.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2023-2024 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage certidp\n\nimport (\n\t\"encoding/base64\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"net/http\"\n\t\"strings\"\n\t\"time\"\n\n\t\"golang.org/x/crypto/ocsp\"\n)\n\nfunc FetchOCSPResponse(link *ChainLink, opts *OCSPPeerConfig, log *Log) ([]byte, error) {\n\tif link == nil || link.Leaf == nil || link.Issuer == nil || opts == nil || log == nil {\n\t\treturn nil, errors.New(ErrInvalidChainlink)\n\t}\n\n\ttimeout := time.Duration(opts.Timeout * float64(time.Second))\n\tif timeout <= 0*time.Second {\n\t\ttimeout = DefaultOCSPResponderTimeout\n\t}\n\n\tgetRequestBytes := func(u string, hc *http.Client) ([]byte, error) {\n\t\tresp, err := hc.Get(u)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tdefer resp.Body.Close()\n\t\tif resp.StatusCode != http.StatusOK {\n\t\t\treturn nil, fmt.Errorf(ErrBadResponderHTTPStatus, resp.StatusCode)\n\t\t}\n\t\treturn io.ReadAll(resp.Body)\n\t}\n\n\t// Request documentation:\n\t// https://tools.ietf.org/html/rfc6960#appendix-A.1\n\n\treqDER, err := ocsp.CreateRequest(link.Leaf, link.Issuer, nil)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treqEnc := base64.StdEncoding.EncodeToString(reqDER)\n\n\tresponders := *link.OCSPWebEndpoints\n\n\tif len(responders) == 0 {\n\t\treturn nil, errors.New(ErrNoAvailOCSPServers)\n\t}\n\n\tvar raw []byte\n\thc := &http.Client{\n\t\tTimeout: timeout,\n\t}\n\tfor _, u := range responders {\n\t\turl := u.String()\n\t\tlog.Debugf(DbgMakingCARequest, url)\n\t\turl = strings.TrimSuffix(url, \"/\")\n\t\traw, err = getRequestBytes(fmt.Sprintf(\"%s/%s\", url, reqEnc), hc)\n\t\tif err == nil {\n\t\t\tbreak\n\t\t}\n\t}\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(ErrFailedWithAllRequests, err)\n\t}\n\n\treturn raw, nil\n}\n",
    "source_file": "server/certidp/ocsp_responder.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2022-2024 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n// This file is used iff the `enable_antithesis_sdk` build tag is not present\n//go:build !enable_antithesis_sdk\n\npackage antithesis\n\nimport (\n\t\"testing\"\n)\n\n// AssertUnreachable this implementation is a NOOP\nfunc AssertUnreachable(_ testing.TB, _ string, _ map[string]any) {}\n\n// Assert this implementation is a NOOP\nfunc Assert(_ testing.TB, _ bool, _ string, _ map[string]any) {}\n",
    "source_file": "internal/antithesis/noop.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright (c) 2011-2015 Michael Mitton (mmitton@gmail.com)\n// Portions copyright (c) 2015-2016 go-ldap Authors\npackage ldap\n\nimport (\n\t\"bytes\"\n\t\"crypto/x509/pkix\"\n\t\"encoding/asn1\"\n\tenchex \"encoding/hex\"\n\t\"errors\"\n\t\"fmt\"\n\t\"strings\"\n)\n\nvar attributeTypeNames = map[string]string{\n\t\"2.5.4.3\":  \"CN\",\n\t\"2.5.4.5\":  \"SERIALNUMBER\",\n\t\"2.5.4.6\":  \"C\",\n\t\"2.5.4.7\":  \"L\",\n\t\"2.5.4.8\":  \"ST\",\n\t\"2.5.4.9\":  \"STREET\",\n\t\"2.5.4.10\": \"O\",\n\t\"2.5.4.11\": \"OU\",\n\t\"2.5.4.17\": \"POSTALCODE\",\n\t// FIXME: Add others.\n\t\"0.9.2342.19200300.100.1.25\": \"DC\",\n}\n\n// AttributeTypeAndValue represents an attributeTypeAndValue from https://tools.ietf.org/html/rfc4514\ntype AttributeTypeAndValue struct {\n\t// Type is the attribute type\n\tType string\n\t// Value is the attribute value\n\tValue string\n}\n\n// RelativeDN represents a relativeDistinguishedName from https://tools.ietf.org/html/rfc4514\ntype RelativeDN struct {\n\tAttributes []*AttributeTypeAndValue\n}\n\n// DN represents a distinguishedName from https://tools.ietf.org/html/rfc4514\ntype DN struct {\n\tRDNs []*RelativeDN\n}\n\n// FromCertSubject takes a pkix.Name from a cert and returns a DN\n// that uses the same set.  Does not support multi value RDNs.\nfunc FromCertSubject(subject pkix.Name) (*DN, error) {\n\tdn := &DN{\n\t\tRDNs: make([]*RelativeDN, 0),\n\t}\n\tfor i := len(subject.Names) - 1; i >= 0; i-- {\n\t\tname := subject.Names[i]\n\t\toidString := name.Type.String()\n\t\ttypeName, ok := attributeTypeNames[oidString]\n\t\tif !ok {\n\t\t\treturn nil, fmt.Errorf(\"invalid type name: %+v\", name)\n\t\t}\n\t\tv, ok := name.Value.(string)\n\t\tif !ok {\n\t\t\treturn nil, fmt.Errorf(\"invalid type value: %+v\", v)\n\t\t}\n\t\trdn := &RelativeDN{\n\t\t\tAttributes: []*AttributeTypeAndValue{\n\t\t\t\t{\n\t\t\t\t\tType:  typeName,\n\t\t\t\t\tValue: v,\n\t\t\t\t},\n\t\t\t},\n\t\t}\n\t\tdn.RDNs = append(dn.RDNs, rdn)\n\t}\n\treturn dn, nil\n}\n\n// FromRawCertSubject takes a raw subject from a certificate\n// and uses asn1.Unmarshal to get the individual RDNs in the\n// original order, including multi-value RDNs.\nfunc FromRawCertSubject(rawSubject []byte) (*DN, error) {\n\tdn := &DN{\n\t\tRDNs: make([]*RelativeDN, 0),\n\t}\n\tvar rdns pkix.RDNSequence\n\t_, err := asn1.Unmarshal(rawSubject, &rdns)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfor i := len(rdns) - 1; i >= 0; i-- {\n\t\trdn := rdns[i]\n\t\tif len(rdn) == 0 {\n\t\t\tcontinue\n\t\t}\n\n\t\tr := &RelativeDN{}\n\t\tattrs := make([]*AttributeTypeAndValue, 0)\n\t\tfor j := len(rdn) - 1; j >= 0; j-- {\n\t\t\tatv := rdn[j]\n\n\t\t\ttypeName := \"\"\n\t\t\tname := atv.Type.String()\n\t\t\ttypeName, ok := attributeTypeNames[name]\n\t\t\tif !ok {\n\t\t\t\treturn nil, fmt.Errorf(\"invalid type name: %+v\", name)\n\t\t\t}\n\t\t\tvalue, ok := atv.Value.(string)\n\t\t\tif !ok {\n\t\t\t\treturn nil, fmt.Errorf(\"invalid type value: %+v\", atv.Value)\n\t\t\t}\n\t\t\tattr := &AttributeTypeAndValue{\n\t\t\t\tType:  typeName,\n\t\t\t\tValue: value,\n\t\t\t}\n\t\t\tattrs = append(attrs, attr)\n\t\t}\n\t\tr.Attributes = attrs\n\t\tdn.RDNs = append(dn.RDNs, r)\n\t}\n\n\treturn dn, nil\n}\n\n// ParseDN returns a distinguishedName or an error.\n// The function respects https://tools.ietf.org/html/rfc4514\nfunc ParseDN(str string) (*DN, error) {\n\tdn := new(DN)\n\tdn.RDNs = make([]*RelativeDN, 0)\n\trdn := new(RelativeDN)\n\trdn.Attributes = make([]*AttributeTypeAndValue, 0)\n\tbuffer := bytes.Buffer{}\n\tattribute := new(AttributeTypeAndValue)\n\tescaping := false\n\n\tunescapedTrailingSpaces := 0\n\tstringFromBuffer := func() string {\n\t\ts := buffer.String()\n\t\ts = s[0 : len(s)-unescapedTrailingSpaces]\n\t\tbuffer.Reset()\n\t\tunescapedTrailingSpaces = 0\n\t\treturn s\n\t}\n\n\tfor i := 0; i < len(str); i++ {\n\t\tchar := str[i]\n\t\tswitch {\n\t\tcase escaping:\n\t\t\tunescapedTrailingSpaces = 0\n\t\t\tescaping = false\n\t\t\tswitch char {\n\t\t\tcase ' ', '\"', '#', '+', ',', ';', '<', '=', '>', '\\\\':\n\t\t\t\tbuffer.WriteByte(char)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\t// Not a special character, assume hex encoded octet\n\t\t\tif len(str) == i+1 {\n\t\t\t\treturn nil, errors.New(\"got corrupted escaped character\")\n\t\t\t}\n\n\t\t\tdst := []byte{0}\n\t\t\tn, err := enchex.Decode([]byte(dst), []byte(str[i:i+2]))\n\t\t\tif err != nil {\n\t\t\t\treturn nil, fmt.Errorf(\"failed to decode escaped character: %s\", err)\n\t\t\t} else if n != 1 {\n\t\t\t\treturn nil, fmt.Errorf(\"expected 1 byte when un-escaping, got %d\", n)\n\t\t\t}\n\t\t\tbuffer.WriteByte(dst[0])\n\t\t\ti++\n\t\tcase char == '\\\\':\n\t\t\tunescapedTrailingSpaces = 0\n\t\t\tescaping = true\n\t\tcase char == '=':\n\t\t\tattribute.Type = stringFromBuffer()\n\t\t\t// Special case: If the first character in the value is # the following data\n\t\t\t// is BER encoded. Throw an error since not supported right now.\n\t\t\tif len(str) > i+1 && str[i+1] == '#' {\n\t\t\t\treturn nil, errors.New(\"unsupported BER encoding\")\n\t\t\t}\n\t\tcase char == ',' || char == '+':\n\t\t\t// We're done with this RDN or value, push it\n\t\t\tif len(attribute.Type) == 0 {\n\t\t\t\treturn nil, errors.New(\"incomplete type, value pair\")\n\t\t\t}\n\t\t\tattribute.Value = stringFromBuffer()\n\t\t\trdn.Attributes = append(rdn.Attributes, attribute)\n\t\t\tattribute = new(AttributeTypeAndValue)\n\t\t\tif char == ',' {\n\t\t\t\tdn.RDNs = append(dn.RDNs, rdn)\n\t\t\t\trdn = new(RelativeDN)\n\t\t\t\trdn.Attributes = make([]*AttributeTypeAndValue, 0)\n\t\t\t}\n\t\tcase char == ' ' && buffer.Len() == 0:\n\t\t\t// ignore unescaped leading spaces\n\t\t\tcontinue\n\t\tdefault:\n\t\t\tif char == ' ' {\n\t\t\t\t// Track unescaped spaces in case they are trailing and we need to remove them\n\t\t\t\tunescapedTrailingSpaces++\n\t\t\t} else {\n\t\t\t\t// Reset if we see a non-space char\n\t\t\t\tunescapedTrailingSpaces = 0\n\t\t\t}\n\t\t\tbuffer.WriteByte(char)\n\t\t}\n\t}\n\tif buffer.Len() > 0 {\n\t\tif len(attribute.Type) == 0 {\n\t\t\treturn nil, errors.New(\"DN ended with incomplete type, value pair\")\n\t\t}\n\t\tattribute.Value = stringFromBuffer()\n\t\trdn.Attributes = append(rdn.Attributes, attribute)\n\t\tdn.RDNs = append(dn.RDNs, rdn)\n\t}\n\treturn dn, nil\n}\n\n// Equal returns true if the DNs are equal as defined by rfc4517 4.2.15 (distinguishedNameMatch).\n// Returns true if they have the same number of relative distinguished names\n// and corresponding relative distinguished names (by position) are the same.\nfunc (d *DN) Equal(other *DN) bool {\n\tif len(d.RDNs) != len(other.RDNs) {\n\t\treturn false\n\t}\n\tfor i := range d.RDNs {\n\t\tif !d.RDNs[i].Equal(other.RDNs[i]) {\n\t\t\treturn false\n\t\t}\n\t}\n\treturn true\n}\n\n// RDNsMatch returns true if the individual RDNs of the DNs\n// are the same regardless of ordering.\nfunc (d *DN) RDNsMatch(other *DN) bool {\n\tif len(d.RDNs) != len(other.RDNs) {\n\t\treturn false\n\t}\n\nCheckNextRDN:\n\tfor _, irdn := range d.RDNs {\n\t\tfor _, ordn := range other.RDNs {\n\t\t\tif (len(irdn.Attributes) == len(ordn.Attributes)) &&\n\t\t\t\t(irdn.hasAllAttributes(ordn.Attributes) && ordn.hasAllAttributes(irdn.Attributes)) {\n\t\t\t\t// Found the RDN, check if next one matches.\n\t\t\t\tcontinue CheckNextRDN\n\t\t\t}\n\t\t}\n\n\t\t// Could not find a matching individual RDN, auth fails.\n\t\treturn false\n\t}\n\treturn true\n}\n\n// AncestorOf returns true if the other DN consists of at least one RDN followed by all the RDNs of the current DN.\n// \"ou=widgets,o=acme.com\" is an ancestor of \"ou=sprockets,ou=widgets,o=acme.com\"\n// \"ou=widgets,o=acme.com\" is not an ancestor of \"ou=sprockets,ou=widgets,o=foo.com\"\n// \"ou=widgets,o=acme.com\" is not an ancestor of \"ou=widgets,o=acme.com\"\nfunc (d *DN) AncestorOf(other *DN) bool {\n\tif len(d.RDNs) >= len(other.RDNs) {\n\t\treturn false\n\t}\n\t// Take the last `len(d.RDNs)` RDNs from the other DN to compare against\n\totherRDNs := other.RDNs[len(other.RDNs)-len(d.RDNs):]\n\tfor i := range d.RDNs {\n\t\tif !d.RDNs[i].Equal(otherRDNs[i]) {\n\t\t\treturn false\n\t\t}\n\t}\n\treturn true\n}\n\n// Equal returns true if the RelativeDNs are equal as defined by rfc4517 4.2.15 (distinguishedNameMatch).\n// Relative distinguished names are the same if and only if they have the same number of AttributeTypeAndValues\n// and each attribute of the first RDN is the same as the attribute of the second RDN with the same attribute type.\n// The order of attributes is not significant.\n// Case of attribute types is not significant.\nfunc (r *RelativeDN) Equal(other *RelativeDN) bool {\n\tif len(r.Attributes) != len(other.Attributes) {\n\t\treturn false\n\t}\n\treturn r.hasAllAttributes(other.Attributes) && other.hasAllAttributes(r.Attributes)\n}\n\nfunc (r *RelativeDN) hasAllAttributes(attrs []*AttributeTypeAndValue) bool {\n\tfor _, attr := range attrs {\n\t\tfound := false\n\t\tfor _, myattr := range r.Attributes {\n\t\t\tif myattr.Equal(attr) {\n\t\t\t\tfound = true\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\tif !found {\n\t\t\treturn false\n\t\t}\n\t}\n\treturn true\n}\n\n// Equal returns true if the AttributeTypeAndValue is equivalent to the specified AttributeTypeAndValue\n// Case of the attribute type is not significant\nfunc (a *AttributeTypeAndValue) Equal(other *AttributeTypeAndValue) bool {\n\treturn strings.EqualFold(a.Type, other.Type) && a.Value == other.Value\n}\n",
    "source_file": "internal/ldap/dn.go",
    "chunk_type": "code"
  },
  {
    "content": "Copyright (c) 2011 The LevelDB-Go Authors. All rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are\nmet:\n\n   * Redistributions of source code must retain the above copyright\nnotice, this list of conditions and the following disclaimer.\n   * Redistributions in binary form must reproduce the above\ncopyright notice, this list of conditions and the following disclaimer\nin the documentation and/or other materials provided with the\ndistribution.\n   * Neither the name of Google Inc. nor the names of its\ncontributors may be used to endorse or promote products derived from\nthis software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n\"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\nLIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\nA PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\nOWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\nSPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\nLIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\nDATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\nTHEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.",
    "source_file": "internal/fastrand/LICENSE",
    "chunk_type": "unknown"
  },
  {
    "content": "// Copyright 2020-2023 The LevelDB-Go, Pebble and NATS Authors. All rights reserved.\n// Use of this source code is governed by a BSD-style license that can be found in\n// the LICENSE file.\n\npackage fastrand\n\nimport _ \"unsafe\" // required by go:linkname\n\n// Uint32 returns a lock free uint32 value.\n//\n//go:linkname Uint32 runtime.fastrand\nfunc Uint32() uint32\n\n// Uint32n returns a lock free uint32 value in the interval [0, n).\n//\n//go:linkname Uint32n runtime.fastrandn\nfunc Uint32n(n uint32) uint32\n\n// Uint64 returns a lock free uint64 value.\nfunc Uint64() uint64 {\n\tv := uint64(Uint32())\n\treturn v<<32 | uint64(Uint32())\n}\n",
    "source_file": "internal/fastrand/fastrand.go",
    "chunk_type": "code"
  },
  {
    "content": "// Copyright 2019-2024 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage testhelper\n\nimport (\n\t\"crypto\"\n\t\"crypto/tls\"\n\t\"crypto/x509\"\n\t\"encoding/base64\"\n\t\"encoding/pem\"\n\t\"fmt\"\n\t\"io\"\n\t\"net/http\"\n\t\"os\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync\"\n\t\"testing\"\n\t\"time\"\n\n\t\"golang.org/x/crypto/ocsp\"\n)\n\nconst (\n\tdefaultResponseTTL = 4 * time.Second\n\tdefaultAddress     = \"127.0.0.1:8888\"\n)\n\nfunc NewOCSPResponderCustomAddress(t *testing.T, issuerCertPEM, issuerKeyPEM string, addr string) *http.Server {\n\tt.Helper()\n\treturn NewOCSPResponderBase(t, issuerCertPEM, issuerCertPEM, issuerKeyPEM, false, addr, defaultResponseTTL, \"\")\n}\n\nfunc NewOCSPResponder(t *testing.T, issuerCertPEM, issuerKeyPEM string) *http.Server {\n\tt.Helper()\n\treturn NewOCSPResponderBase(t, issuerCertPEM, issuerCertPEM, issuerKeyPEM, false, defaultAddress, defaultResponseTTL, \"\")\n}\n\nfunc NewOCSPResponderDesignatedCustomAddress(t *testing.T, issuerCertPEM, respCertPEM, respKeyPEM string, addr string) *http.Server {\n\tt.Helper()\n\treturn NewOCSPResponderBase(t, issuerCertPEM, respCertPEM, respKeyPEM, true, addr, defaultResponseTTL, \"\")\n}\n\nfunc NewOCSPResponderPreferringHTTPMethod(t *testing.T, issuerCertPEM, issuerKeyPEM, method string) *http.Server {\n\tt.Helper()\n\treturn NewOCSPResponderBase(t, issuerCertPEM, issuerCertPEM, issuerKeyPEM, false, defaultAddress, defaultResponseTTL, method)\n}\n\nfunc NewOCSPResponderCustomTimeout(t *testing.T, issuerCertPEM, issuerKeyPEM string, responseTTL time.Duration) *http.Server {\n\tt.Helper()\n\treturn NewOCSPResponderBase(t, issuerCertPEM, issuerCertPEM, issuerKeyPEM, false, defaultAddress, responseTTL, \"\")\n}\n\nfunc NewOCSPResponderBase(t *testing.T, issuerCertPEM, respCertPEM, respKeyPEM string, embed bool, addr string, responseTTL time.Duration, method string) *http.Server {\n\tt.Helper()\n\tvar mu sync.Mutex\n\tstatus := make(map[string]int)\n\n\tissuerCert := parseCertPEM(t, issuerCertPEM)\n\trespCert := parseCertPEM(t, respCertPEM)\n\trespKey := parseKeyPEM(t, respKeyPEM)\n\n\tmux := http.NewServeMux()\n\t// The \"/statuses/\" endpoint is for directly setting a key-value pair in\n\t// the CA's status database.\n\tmux.HandleFunc(\"/statuses/\", func(rw http.ResponseWriter, r *http.Request) {\n\t\tdefer r.Body.Close()\n\n\t\tkey := r.URL.Path[len(\"/statuses/\"):]\n\t\tswitch r.Method {\n\t\tcase \"GET\":\n\t\t\tmu.Lock()\n\t\t\tn, ok := status[key]\n\t\t\tif !ok {\n\t\t\t\tn = ocsp.Unknown\n\t\t\t}\n\t\t\tmu.Unlock()\n\n\t\t\tfmt.Fprintf(rw, \"%s %d\", key, n)\n\t\tcase \"POST\":\n\t\t\tdata, err := io.ReadAll(r.Body)\n\t\t\tif err != nil {\n\t\t\t\thttp.Error(rw, err.Error(), http.StatusBadRequest)\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\tn, err := strconv.Atoi(string(data))\n\t\t\tif err != nil {\n\t\t\t\thttp.Error(rw, err.Error(), http.StatusBadRequest)\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\tmu.Lock()\n\t\t\tstatus[key] = n\n\t\t\tmu.Unlock()\n\n\t\t\tfmt.Fprintf(rw, \"%s %d\", key, n)\n\t\tdefault:\n\t\t\thttp.Error(rw, \"Method Not Allowed\", http.StatusMethodNotAllowed)\n\t\t\treturn\n\t\t}\n\t})\n\t// The \"/\" endpoint is for normal OCSP requests. This actually parses an\n\t// OCSP status request and signs a response with a CA. Lightly based off:\n\t// https://www.ietf.org/rfc/rfc2560.txt\n\tmux.HandleFunc(\"/\", func(rw http.ResponseWriter, r *http.Request) {\n\t\tvar reqData []byte\n\t\tvar err error\n\n\t\tswitch {\n\t\tcase r.Method == \"GET\":\n\t\t\tif method != \"\" && r.Method != method {\n\t\t\t\thttp.Error(rw, \"\", http.StatusBadRequest)\n\t\t\t\treturn\n\t\t\t}\n\t\t\treqData, err = base64.StdEncoding.DecodeString(r.URL.Path[1:])\n\t\tcase r.Method == \"POST\":\n\t\t\tif method != \"\" && r.Method != method {\n\t\t\t\thttp.Error(rw, \"\", http.StatusBadRequest)\n\t\t\t\treturn\n\t\t\t}\n\t\t\treqData, err = io.ReadAll(r.Body)\n\t\t\tr.Body.Close()\n\t\tdefault:\n\t\t\thttp.Error(rw, \"Method Not Allowed\", http.StatusMethodNotAllowed)\n\t\t\treturn\n\t\t}\n\t\tif err != nil {\n\t\t\thttp.Error(rw, err.Error(), http.StatusBadRequest)\n\t\t\treturn\n\t\t}\n\n\t\tocspReq, err := ocsp.ParseRequest(reqData)\n\t\tif err != nil {\n\t\t\thttp.Error(rw, err.Error(), http.StatusBadRequest)\n\t\t\treturn\n\t\t}\n\n\t\tmu.Lock()\n\t\tn, ok := status[ocspReq.SerialNumber.String()]\n\t\tif !ok {\n\t\t\tn = ocsp.Unknown\n\t\t}\n\t\tmu.Unlock()\n\n\t\ttmpl := ocsp.Response{\n\t\t\tStatus:       n,\n\t\t\tSerialNumber: ocspReq.SerialNumber,\n\t\t\tThisUpdate:   time.Now(),\n\t\t}\n\t\tif responseTTL != 0 {\n\t\t\ttmpl.NextUpdate = tmpl.ThisUpdate.Add(responseTTL)\n\t\t}\n\t\tif embed {\n\t\t\ttmpl.Certificate = respCert\n\t\t}\n\t\trespData, err := ocsp.CreateResponse(issuerCert, respCert, tmpl, respKey)\n\t\tif err != nil {\n\t\t\thttp.Error(rw, err.Error(), http.StatusInternalServerError)\n\t\t\treturn\n\t\t}\n\n\t\trw.Header().Set(\"Content-Type\", \"application/ocsp-response\")\n\t\trw.Header().Set(\"Content-Length\", fmt.Sprint(len(respData)))\n\n\t\tfmt.Fprint(rw, string(respData))\n\t})\n\n\tsrv := &http.Server{\n\t\tAddr:        addr,\n\t\tHandler:     mux,\n\t\tReadTimeout: time.Second * 5,\n\t}\n\tgo srv.ListenAndServe()\n\ttime.Sleep(1 * time.Second)\n\treturn srv\n}\n\nfunc parseCertPEM(t *testing.T, certPEM string) *x509.Certificate {\n\tt.Helper()\n\tblock := parsePEM(t, certPEM)\n\n\tcert, err := x509.ParseCertificate(block.Bytes)\n\tif err != nil {\n\t\tt.Fatalf(\"failed to parse cert '%s': %s\", certPEM, err)\n\t}\n\treturn cert\n}\n\nfunc parseKeyPEM(t *testing.T, keyPEM string) crypto.Signer {\n\tt.Helper()\n\tblock := parsePEM(t, keyPEM)\n\n\tkey, err := x509.ParsePKCS8PrivateKey(block.Bytes)\n\tif err != nil {\n\t\tkey, err = x509.ParsePKCS1PrivateKey(block.Bytes)\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"failed to parse ikey %s: %s\", keyPEM, err)\n\t\t}\n\t}\n\tkeyc := key.(crypto.Signer)\n\treturn keyc\n}\n\nfunc parsePEM(t *testing.T, pemPath string) *pem.Block {\n\tt.Helper()\n\tdata, err := os.ReadFile(pemPath)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tblock, _ := pem.Decode(data)\n\tif block == nil {\n\t\tt.Fatalf(\"failed to decode PEM %s\", pemPath)\n\t}\n\treturn block\n}\n\nfunc GetOCSPStatus(s tls.ConnectionState) (*ocsp.Response, error) {\n\tif len(s.VerifiedChains) == 0 {\n\t\treturn nil, fmt.Errorf(\"missing TLS verified chains\")\n\t}\n\tchain := s.VerifiedChains[0]\n\n\tif got, want := len(chain), 2; got < want {\n\t\treturn nil, fmt.Errorf(\"incomplete cert chain, got %d, want at least %d\", got, want)\n\t}\n\tleaf, issuer := chain[0], chain[1]\n\n\tresp, err := ocsp.ParseResponseForCert(s.OCSPResponse, leaf, issuer)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to parse OCSP response: %w\", err)\n\t}\n\tif err := resp.CheckSignatureFrom(issuer); err != nil {\n\t\treturn resp, err\n\t}\n\treturn resp, nil\n}\n\nfunc SetOCSPStatus(t *testing.T, ocspURL, certPEM string, status int) {\n\tt.Helper()\n\n\tcert := parseCertPEM(t, certPEM)\n\n\thc := &http.Client{Timeout: 10 * time.Second}\n\tresp, err := hc.Post(\n\t\tfmt.Sprintf(\"%s/statuses/%s\", ocspURL, cert.SerialNumber),\n\t\t\"\",\n\t\tstrings.NewReader(fmt.Sprint(status)),\n\t)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer resp.Body.Close()\n\n\tdata, err := io.ReadAll(resp.Body)\n\tif err != nil {\n\t\tt.Fatalf(\"failed to read OCSP HTTP response body: %s\", err)\n\t}\n\n\tif got, want := resp.Status, \"200 OK\"; got != want {\n\t\tt.Error(strings.TrimSpace(string(data)))\n\t\tt.Fatalf(\"unexpected OCSP HTTP set status, got %q, want %q\", got, want)\n\t}\n}\n",
    "source_file": "internal/ocsp/ocsp.go",
    "chunk_type": "code"
  }
]